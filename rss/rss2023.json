[
    {
        "id": "f19c6d77f6",
        "title": "A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training",
        "site": "https://www.roboticsproceedings.org/rss19/p076.html",
        "author": "Jingnan Shi; Rajat Talak; Dominic Maggio; Luca Carlone",
        "abstract": "Real-world robotics applications demand object pose estimation methods that work reliably across a variety of scenarios. Modern learning-based approaches require large labeled datasets and tend to perform poorly outside the training domain. Our first contribution is to develop a robust corrector module that corrects pose estimates using depth information, thus enabling existing methods to better generalize to new test domains; the corrector operates on semantic keypoints (but is also applicable to other pose estimators) and is fully differentiable. Our second contribution is an ensemble self-training approach that simultaneously trains multiple pose estimators in a self-supervised manner. Our ensemble self-training architecture uses the robust corrector to refine the output of each pose estimator; then, it evaluates the quality of the outputs using observable correctness certificates; finally, it uses the observably correct outputs for further training, without requiring external supervision. As an additional contribution, we propose small improvements to a regression-based keypoint detection architecture, to enhance its robustness to outliers; these improvements include a robust pooling scheme and a robust centroid computation. Experiments on the YCBV and TLESS datasets show the proposed ensemble self-training performs on par or better than fully supervised baselines while not requiring 3D annotations on real data.",
        "bibtex": "@INPROCEEDINGS{Shi-RSS-23, \r\n    AUTHOR    = {Jingnan Shi AND Rajat Talak AND Dominic Maggio AND Luca Carlone}, \r\n    TITLE     = {{A Correct-and-Certify Approach to Self-Supervise Object Pose Estimators via Ensemble Self-Training}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.076} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p076.pdf",
        "supp": "",
        "pdf_size": 10615413,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10219221336227485985&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "Laboratory for Information & Decision Systems (LIDS), Massachusetts Institute of Technology, Cambridge, MA 02139, USA; Laboratory for Information & Decision Systems (LIDS), Massachusetts Institute of Technology, Cambridge, MA 02139, USA; Laboratory for Information & Decision Systems (LIDS), Massachusetts Institute of Technology, Cambridge, MA 02139, USA; Laboratory for Information & Decision Systems (LIDS), Massachusetts Institute of Technology, Cambridge, MA 02139, USA",
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "https://web.mit.edu/sparklab/research/ensemble_self_training/",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Laboratory for Information & Decision Systems (LIDS)",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "da96ad8e31",
        "title": "A Sampling-Based Approach for Heterogeneous Coalition Scheduling with Temporal Uncertainty",
        "site": "https://www.roboticsproceedings.org/rss19/p107.html",
        "author": "Andrew Messing; Jacopo Banfi; Martina Stadler; Ethan Stump; Harish Ravichandar; Nicholas Roy; Seth Hutchinson",
        "abstract": "Scheduling algorithms for real-world heterogeneous multi-robot teams must be able to reason about temporal uncertainty in the world model in order to create plans that are tolerant to the risk of unexpected delays. To this end, we present a novel sampling-based risk-aware approach for solving Heterogeneous Coalition Scheduling with Temporal Uncertainty (HCSTU) problems, which does not require any assumptions regarding the specific underlying cause of the temporal uncertainty or the specific duration distributions. Our approach computes a schedule which obeys the temporal constraints of a small number of heuristically-selected sample scenarios by solving a Mixed-Integer Linear Program, along with an upper bound on the schedule execution time. Then, it uses a hypothesis testing method, the Sequential Probability Ratio Test, to provide a probabilistic guarantee that the upper bound on the execution time will be respected for a user-specified risk tolerance. With extensive experiments, we demonstrate that our approach empirically respects the risk tolerance, and generates solutions of comparable or better quality than state-of-the-art approaches while being an order of magnitude faster to compute on average. Finally, we demonstrate how robust schedules generated by our approach can be incorporated as solutions to subproblems within the broader Simultaneous Task Allocation and Planning with Spatiotemporal Constraints problem to both guide and expedite the search for solutions of higher quality and lower risk.",
        "bibtex": "@INPROCEEDINGS{Messing-RSS-23, \r\n    AUTHOR    = {Andrew Messing AND Jacopo Banfi AND Martina Stadler AND Ethan Stump AND Harish Ravichandar AND Nicholas Roy AND Seth Hutchinson}, \r\n    TITLE     = {{A Sampling-Based Approach for Heterogeneous Coalition Scheduling with Temporal Uncertainty}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.107} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p107.pdf",
        "supp": "",
        "pdf_size": 2123300,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=798483843962987614&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "IRIM, Georgia Institute of Technology, Atlanta (GA) 30332, USA+CSAIL, Massachusetts Institute of Technology, Cambridge (MA) 02139, USA; CSAIL, Massachusetts Institute of Technology, Cambridge (MA) 02139, USA; CSAIL, Massachusetts Institute of Technology, Cambridge (MA) 02139, USA; DEVCOM ARL, Adelphi (MD) 20783, USA; IRIM, Georgia Institute of Technology, Atlanta (GA) 30332, USA; CSAIL, Massachusetts Institute of Technology, Cambridge (MA) 02139, USA; IRIM, Georgia Institute of Technology, Atlanta (GA) 30332, USA",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;2;0;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Massachusetts Institute of Technology;U.S. Army Research Laboratory",
        "aff_unique_dep": "IRIM;Computer Science and Artificial Intelligence Laboratory;DEVCOM",
        "aff_unique_url": "https://www.gatech.edu;https://www.csail.mit.edu;https://www.arl.army.mil",
        "aff_unique_abbr": "Georgia Tech;MIT;ARL",
        "aff_campus_unique_index": "0+1;1;1;2;0;1;0",
        "aff_campus_unique": "Atlanta;Cambridge;Adelphi",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d3945835a9",
        "title": "Active Collaborative Localization in Heterogeneous Robot Teams",
        "site": "https://www.roboticsproceedings.org/rss19/p112.html",
        "author": "Igor Spasojevic; Xu Liu; Alejandro Ribeiro; George J. Pappas; Vijay Kumar",
        "abstract": "Accurate and robust state estimation is critical for autonomous navigation of robot teams. This task is especially challenging for large groups of size, weight, and power (SWAP) constrained aerial robots operating in perceptually-degraded GPS-denied environments. We can, however, actively increase the amount of perceptual information available to such robots by augmenting them with a small number of more expensive, but less resource-constrained, agents. Specifically, the latter can serve as sources of perceptual information themselves. In this paper, we study the problem of optimally positioning (and potentially navigating) a small number of more capable agents to enhance the perceptual environment for their lightweight, inexpensive, teammates that only need to rely on cameras and IMUs. We propose a numerically robust, computationally efficient approach to solve this problem via nonlinear optimization. Our method outperforms the standard approach based on the greedy algorithm, while matching the accuracy of a heuristic evolutionary scheme for global optimization at a fraction of its running time. Ultimately, we validate our solution in both photorealistic simulations and real-world experiments. In these experiments, we use lidar-based autonomous ground vehicles as the more capable agents, and vision-based aerial robots as their SWAP-constrained teammates. Our method is able to reduce drift in visual-inertial odometry by as much as 90%, and it outperforms random positioning of lidar-equipped agents by a significant margin. Furthermore, our method can be generalized to different types of robot teams with heterogeneous perception capabilities. It has a wide range of applications, such as surveying and mapping challenging, dynamic, environments, and enabling resilience to large-scale perturbations that can be caused by earthquakes or storms.",
        "bibtex": "@INPROCEEDINGS{Spasojevic-RSS-23, \r\n    AUTHOR    = {Igor Spasojevic AND Xu Liu AND Alejandro Ribeiro AND George J. Pappas AND Vijay Kumar}, \r\n    TITLE     = {{Active Collaborative Localization in Heterogeneous Robot Teams}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.112} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p112.pdf",
        "supp": "",
        "pdf_size": 7876220,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11413569575941134465&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "GRASP Laboratory, University of Pennsylvania; GRASP Laboratory, University of Pennsylvania; GRASP Laboratory, University of Pennsylvania; GRASP Laboratory, University of Pennsylvania; GRASP Laboratory, University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "GRASP Laboratory",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Philadelphia",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a97d4b63f4",
        "title": "Active Velocity Estimation using Light Curtains via Self-Supervised Multi-Armed Bandits",
        "site": "https://www.roboticsproceedings.org/rss19/p097.html",
        "author": "Siddharth  Ancha; Gaurav Pathak; Ji Zhang; Srinivasa Narasimhan; David Held",
        "abstract": "To navigate in an environment safely and autonomously, robots must accurately estimate where obstacles are and how they move. Instead of using expensive traditional 3D sensors, we explore the use of a much cheaper, faster, and higher resolution alternative: programmable light curtains. Light curtains are a controllable depth sensor that sense only along a surface that the user selects. We adapt a probabilistic method based on particle filters and occupancy grids to explicitly estimate the position and velocity of 3D points in the scene using partial measurements made by light curtains. The central challenge is to decide where to place the light curtain to accurately perform this task. We propose multiple curtain placement strategies guided by maximizing information gain and verifying predicted object locations. Then, we combine these strategies using an online learning framework. We propose a novel self-supervised reward function that evaluates the accuracy of current velocity estimates using future light curtain placements. We use a multi-armed bandit framework to intelligently switch between placement policies in real time, outperforming fixed policies. We develop a full-stack navigation system that uses position and velocity estimates from light curtains for downstream tasks such as localization, mapping, path-planning, and obstacle avoidance. This work paves the way for controllable light curtains to accurately, efficiently, and purposefully perceive and navigate complex and dynamic environments.",
        "bibtex": "@INPROCEEDINGS{Ancha-RSS-23, \r\n    AUTHOR    = {Siddharth  Ancha AND Gaurav Pathak AND Ji Zhang AND Srinivasa Narasimhan AND David Held}, \r\n    TITLE     = {{Active Velocity Estimation using Light Curtains via Self-Supervised Multi-Armed Bandits}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.097} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p097.pdf",
        "supp": "",
        "pdf_size": 5560198,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12763647816327535151&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Massachusetts Institute of Technology; Adobe; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "mit.edu; ; ; ; ",
        "email": "mit.edu; ; ; ; ",
        "github": "https://github.com/siddancha",
        "project": "https://siddancha.github.io/projects/active-velocity-estimation1",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;2",
        "aff_unique_norm": "Massachusetts Institute of Technology;Adobe;Carnegie Mellon University",
        "aff_unique_dep": ";Adobe Inc.;",
        "aff_unique_url": "https://web.mit.edu;https://www.adobe.com;https://www.cmu.edu",
        "aff_unique_abbr": "MIT;Adobe;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "547c2a562a",
        "title": "Adaptive Tracking Control of Dielectric Elastomer Soft Actuators with Viscoelastic Hysteresis Compensation",
        "site": "https://www.roboticsproceedings.org/rss19/p093.html",
        "author": "Yunhua Zhao; Li Wen",
        "abstract": "This paper proposes a new adaptive control method with viscoelastic hysteresis compensation for high-precision tracking control of dielectric elastomer actuators (DEAs). A direct inverse feedforward compensator is constructed by using a modified Prandtl-Ishlinskii model for compensating hysteresis nonlinearities. The dynamics effects of DEAs and disturbances are coped with the adaptive inverse controller using filtered-x normalized least mean square algorithm. A series of real-time tracking experiments are carried out on a DEA made of commercial acrylic elastomers. The proposed control method achieves accurate tracking of various trajectories with the relative root-mean-square tracking error ranging from 1.37% to a maximum of 4.37% over the whole operating frequency range, and outperforms previously proposed methods in terms of accuracy. The excellent tracking results demonstrate the effectiveness of the developed control method for dielectric elastomer artificial muscles based soft actuators.",
        "bibtex": "@INPROCEEDINGS{Zhao-RSS-23, \r\n    AUTHOR    = {Yunhua Zhao AND Li Wen}, \r\n    TITLE     = {{Adaptive Tracking Control of Dielectric Elastomer Soft Actuators with Viscoelastic Hysteresis Compensation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.093} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p093.pdf",
        "supp": "",
        "pdf_size": 4968704,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:EeSVyDRLu34J:scholar.google.com/&scioq=Adaptive+Tracking+Control+of+Dielectric+Elastomer+Soft+Actuators+with+Viscoelastic+Hysteresis+Compensation&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "School of Mechanical Engineering and Automation, Beihang University, Beijing, China; School of Mechanical Engineering and Automation, Beihang University, Beijing, China",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Beihang University",
        "aff_unique_dep": "School of Mechanical Engineering and Automation",
        "aff_unique_url": "http://www.buaa.edu.cn",
        "aff_unique_abbr": "Beihang",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "8b606936e0",
        "title": "An Efficient Multi-solution Solver for the Inverse Kinematics of 3-Section Constant-Curvature Robots",
        "site": "https://www.roboticsproceedings.org/rss19/p091.html",
        "author": "Ke Qiu; Jingyu Zhang; Danying Sun; Rong Xiong; Haojian LU; Yue Wang",
        "abstract": "Piecewise constant curvature is a popular kinematics framework for continuum robots. Computing the model parameters from the desired end pose, known as the inverse kinematics problem, is fundamental in manipulation, tracking and planning tasks. In this paper, we propose an efficient multi-solution solver to address the inverse kinematics problem of 3-section constant-curvature robots by bridging both the theoretical reduction and numerical correction. We derive analytical conditions to simplify the original problem into a one-dimensional problem. Further, the equivalence of the two problems is formalised. In addition, we introduce an approximation with bounded error so that the one dimension becomes traversable while the remaining parameters analytically solvable. With the theoretical results, the global search and numerical correction are employed to implement the solver. The experiments validate the better efficiency and higher success rate of our solver than the numerical methods when one solution is required, and demonstrate the ability of obtaining multiple solutions with optimal path planning in a space with obstacles.",
        "bibtex": "@INPROCEEDINGS{Qiu-RSS-23, \r\n    AUTHOR    = {Ke Qiu AND Jingyu Zhang AND Danying Sun AND Rong Xiong AND Haojian LU AND Yue Wang}, \r\n    TITLE     = {{An Efficient Multi-solution Solver for the Inverse Kinematics of 3-Section Constant-Curvature Robots}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.091} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p091.pdf",
        "supp": "",
        "pdf_size": 4248203,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9848804170188516531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "State Key Laboratory of Industrial Control and Technology, Zhejiang University, China; State Key Laboratory of Industrial Control and Technology, Zhejiang University, China; State Key Laboratory of Industrial Control and Technology, Zhejiang University, China; State Key Laboratory of Industrial Control and Technology, Zhejiang University, China; State Key Laboratory of Industrial Control and Technology, Zhejiang University, China; State Key Laboratory of Industrial Control and Technology, Zhejiang University, China",
        "aff_domain": "zju.edu.cn;zju.edu.cn; ; ; ; ",
        "email": "zju.edu.cn;zju.edu.cn; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "State Key Laboratory of Industrial Control and Technology",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "34bc9bbffc",
        "title": "AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System",
        "site": "https://www.roboticsproceedings.org/rss19/p015.html",
        "author": "Yuzhe Qin; Wei Yang; Binghao Huang; Karl Van Wyk; Hao Su; Xiaolong Wang; Yu-Wei Chao; Dieter Fox",
        "abstract": "Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expanded and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop\r\ncan outperform a previous system that was designed for the specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better imitation learning performance, compared with a previous system that is particularly designed for that simulator.",
        "bibtex": "@INPROCEEDINGS{Qin-RSS-23, \r\n    AUTHOR    = {Yuzhe Qin AND Wei Yang AND Binghao Huang AND Karl Van Wyk AND Hao Su AND Xiaolong Wang AND Yu-Wei Chao AND Dieter Fox}, \r\n    TITLE     = {{AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.015} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p015.pdf",
        "supp": "",
        "pdf_size": 8136264,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13934093999634933027&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "UC San Diego; NVIDIA; UC San Diego; NVIDIA; UC San Diego; UC San Diego; NVIDIA; NVIDIA",
        "aff_domain": "; ; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ; ",
        "github": "",
        "project": "http://anyteleop.com",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;0;0;1;1",
        "aff_unique_norm": "University of California, San Diego;NVIDIA",
        "aff_unique_dep": ";NVIDIA Corporation",
        "aff_unique_url": "https://www.ucsd.edu;https://www.nvidia.com",
        "aff_unique_abbr": "UCSD;NVIDIA",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3debaa3ae7",
        "title": "Autonomous Justification for Enabling Explainable Decision Support in Human-Robot Teaming",
        "site": "https://www.roboticsproceedings.org/rss19/p002.html",
        "author": "Matthew Luebbers; Aaquib Tabrez; Kyler Ruvane; Bradley Hayes",
        "abstract": "Justification is an important facet of policy explanation, a process for describing the behavior of an autonomous system. In human-robot collaboration, an autonomous agent can attempt to justify distinctly important decisions by offering explanations as to why those decisions are right or reasonable, leveraging a snapshot of its internal reasoning to do so. Without sufficient insight into a robot's decision-making process, it becomes challenging for users to trust or comply with those important decisions, especially when they are viewed as confusing or contrary to the user's expectations (e.g., when decisions change as new information is introduced to the agent's decision-making process). In this work we characterize the benefits of justification within the context of decision-support during human-robot teaming (i.e., agents giving recommendations to human teammates). We introduce a formal framework using value of information theory to strategically time justifications during periods of misaligned expectations for greater effect. We also characterize four different types of counterfactual justification derived from established explainable AI literature and evaluate them against each other in a human-subjects study involving a collaborative, partially observable search task. Based on our findings, we present takeaways on the effective use of different types of justifications in human-robot teaming scenarios, to improve user compliance and decision-making by strategically influencing human teammate thinking patterns. Finally, we present an augmented reality system incorporating these findings into a real-world decision-support system for human-robot teaming.",
        "bibtex": "@INPROCEEDINGS{Luebbers-RSS-23, \r\n    AUTHOR    = {Matthew Luebbers AND Aaquib Tabrez AND Kyler Ruvane AND Bradley Hayes}, \r\n    TITLE     = {{Autonomous Justification for Enabling Explainable Decision Support in Human-Robot Teaming}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.002} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p002.pdf",
        "supp": "",
        "pdf_size": 6181871,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14370918769126872679&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder",
        "aff_domain": "colorado.edu;colorado.edu;colorado.edu;colorado.edu",
        "email": "colorado.edu;colorado.edu;colorado.edu;colorado.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Colorado",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.colorado.edu",
        "aff_unique_abbr": "CU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Boulder",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8ed92bf493",
        "title": "Autonomous Navigation, Mapping and Exploration with Gaussian Processes",
        "site": "https://www.roboticsproceedings.org/rss19/p104.html",
        "author": "Mahmoud Ali; Hassan Jardali; Nicholas Roy; Lantao Liu",
        "abstract": "Navigating and exploring an unknown environment is a challenging task for autonomous robots, especially in complex and unstructured environments. We propose a new framework that can simultaneously accomplish multiple objectives that are essential to robot autonomy including identifying free space for navigation, building a metric-topological representation for mapping, and ensuring good spatial coverage for unknown space exploration. Different from existing work that model these critical objectives separately, we show that navigation, mapping, and exploration can be derived with the same foundation modeled with a sparse variant of Gaussian Process. Specifically, in our framework the robot navigates by following frontiers computed from a local Gaussian Process perception model, and along the way builds a map in a metric-topological form where nodes are adaptively selected from important perception frontiers. The topology expands towards unexplored areas by assessing a low-cost global uncertainty map also computed from a sparse Gaussian Process. Through evaluations in various cluttered and unstructured environments, we validate that the proposed framework can explore unknown environments faster and with a traveled distance less than the start-of-art frontier exploration approaches. Through field demonstration, we have begun to lay the groundwork for field robots to explore challenging environments such as forests that humans have yet to set foot in.",
        "bibtex": "@INPROCEEDINGS{Ali-RSS-23, \r\n    AUTHOR    = {Mahmoud Ali AND Hassan Jardali AND Nicholas Roy AND Lantao Liu}, \r\n    TITLE     = {{Autonomous Navigation, Mapping and Exploration with Gaussian Processes}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.104} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p104.pdf",
        "supp": "",
        "pdf_size": 10903050,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17356429298476160101&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Indiana University; Indiana University; MIT; Indiana University",
        "aff_domain": "iu.edu;iu.edu;csail.mit.edu;iu.edu",
        "email": "iu.edu;iu.edu;csail.mit.edu;iu.edu",
        "github": "",
        "project": "https://youtu.be/WcbngSewXBw",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Indiana University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.indiana.edu;https://web.mit.edu",
        "aff_unique_abbr": "IU;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9db6d3e490",
        "title": "Bandit Submodular Maximization for Multi-Robot Coordination in Unpredictable and Partially Observable Environments",
        "site": "https://www.roboticsproceedings.org/rss19/p109.html",
        "author": "Zirui Xu; Xiaofeng Lin; Vasileios Tzoumas",
        "abstract": "We study the problem of multi-agent coordination in unpredictable and partially observable environments, that is, environments whose future evolution is unknown a priori and that can only be partially observed. We are motivated by the future of autonomy that involves multiple robots coordinating actions in dynamic, unstructured, and partially observable environments to complete complex tasks such as target tracking, environmental mapping, and area monitoring. Such tasks are often modeled as submodular maximization coordination problems due to the information overlap among the robots. We introduce the first submodular coordination algorithm with bandit feedback and bounded tracking regret \u2014bandit feedback is the robots\u2019 ability to compute in hindsight only the effect of their chosen actions, instead of all the alternative actions that they could have chosen instead, due to the partial observability; and tracking regret is the algorithm\u2019s suboptimality with respect to the optimal time-varying actions that fully know the future a priori. The bound gracefully degrades with the environments\u2019 capacity to change adversarially, quantifying how often the robots should re-select actions to learn to coordinate as if they fully knew the future a priori. The algorithm generalizes the seminal Sequential Greedy algorithm by Fisher et al. to the bandit setting, by leveraging submodularity and algorithms for the problem of tracking the best action. We validate our algorithm in simulated scenarios of multi-target tracking.",
        "bibtex": "@INPROCEEDINGS{Xu-RSS-23, \r\n    AUTHOR    = {Zirui Xu AND Xiaofeng Lin AND Vasileios Tzoumas}, \r\n    TITLE     = {{Bandit Submodular Maximization for Multi-Robot Coordination in Unpredictable and Partially Observable Environments}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.109} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p109.pdf",
        "supp": "",
        "pdf_size": 1128509,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10652617685658988240&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Aerospace Engineering; Department of Robotics; Department of Aerospace Engineering",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University Affiliation Not Specified;Robotics Department",
        "aff_unique_dep": "Department of Aerospace Engineering;Department of Robotics",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "0d8a473c90",
        "title": "Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets",
        "site": "https://www.roboticsproceedings.org/rss19/p011.html",
        "author": "Maximilian Du; Suraj Nair; Dorsa Sadigh; Chelsea Finn",
        "abstract": "Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert interventions or demonstrations to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our method learns to query only the relevant transitions to the task, filtering out sub-optimal or task-irrelevant data. By doing so, it is able to learn more effectively from the mix of task-specific and offline data compared to naively mixing the data or only using the task-specific data. \r\nFurthermore, we find that our simple querying approach outperforms more complex goal-conditioned methods by 20% across simulated and real robotic manipulation tasks from images.",
        "bibtex": "@INPROCEEDINGS{Du-RSS-23, \r\n    AUTHOR    = {Maximilian Du AND Suraj Nair AND Dorsa Sadigh AND Chelsea Finn}, \r\n    TITLE     = {{Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.011} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p011.pdf",
        "supp": "",
        "pdf_size": 3775102,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17942596415659695591&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu; ; ",
        "email": "stanford.edu;stanford.edu; ; ",
        "github": "",
        "project": "https://sites.google.com/view/behaviorretrieval",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e53ac2b5a2",
        "title": "Beyond Flat GelSight Sensors: Simulation of Optical Tactile Sensors of Complex Morphologies for Sim2Real Learning",
        "site": "https://www.roboticsproceedings.org/rss19/p035.html",
        "author": "Daniel Fernandes Gomes; Shan Luo; Paolo Paoletti",
        "abstract": "Recently, several morphologies, each with its advantages, have been proposed for the GelSight high-resolution tactile sensors. However, existing simulation methods are limited to flat-surface sensors, which prevents its usage with the newer sensors of non-flat morphologies in Sim2Real experiments. In this paper, we extend a previously proposed GelSight simulation method, which was developed for flat-surface sensors, and propose a novel method for curved sensors. In particular, we address the simulation of light rays travelling through a curved tactile membrane in the form of geodesic paths. The method is validated by simulating the finger-shaped GelTip sensor and comparing the generated synthetic tactile images against the corresponding real images. Our extensive experiments show that combining the illumination generated from the geodesic paths, with a background image from the real sensor, produces the best results when compared to the lighting generated by direct linear paths in the same conditions. As the method is parameterized by the sensor mesh, it can be applied in principle to simulate a tactile sensor of any morphology. The proposed method not only unlocks simulating existing optical tactile sensors of complex morphologies, but also enables experimenting with sensors of novel morphologies, before the fabrication of the real sensor.\r\nProject website: https://danfergo.github.io/geltip-sim",
        "bibtex": "@INPROCEEDINGS{Gomes-RSS-23, \r\n    AUTHOR    = {Daniel Fernandes Gomes AND Shan Luo AND Paolo Paoletti}, \r\n    TITLE     = {{Beyond Flat GelSight Sensors: Simulation of Optical Tactile Sensors of Complex Morphologies for Sim2Real Learning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.035} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p035.pdf",
        "supp": "",
        "pdf_size": 2990554,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7116078084212873404&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of Liverpool + Department of Engineering, King\u2019s College London; School of Engineering, University of Liverpool; Department of Engineering, King\u2019s College London",
        "aff_domain": "kcl.ac.uk;liverpool.ac.uk;kcl.ac.uk",
        "email": "kcl.ac.uk;liverpool.ac.uk;kcl.ac.uk",
        "github": "",
        "project": "https://danfergo.github.io/geltip-sim",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "University of Liverpool;King\u2019s College London",
        "aff_unique_dep": "Department of Computer Science;Department of Engineering",
        "aff_unique_url": "https://www.liverpool.ac.uk;https://www.kcl.ac.uk",
        "aff_unique_abbr": "Liv Uni;KCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "dda487358a",
        "title": "Bridging Active Exploration and Uncertainty-Aware Deployment Using Probabilistic Ensemble Neural Network Dynamics",
        "site": "https://www.roboticsproceedings.org/rss19/p086.html",
        "author": "Taekyung Kim; Jungwi Mun; Junwon Seo; Beomsu Kim; Seongil Hong",
        "abstract": "In recent years, learning-based control in robotics has gained significant attention due to its capability to address complex tasks in real-world environments. With the advances in machine learning algorithms and computational capabilities, this approach is becoming increasingly important for solving challenging control problems in robotics by learning unknown or partially known robot dynamics. Active exploration, in which a robot directs itself to states that yield the highest information gain, is essential for efficient data collection and minimizing human supervision. Similarly, uncertainty-aware deployment has been a growing concern in robotic control, as uncertain actions informed by the learned model can lead to unstable motions or failure. However, active exploration and uncertainty-aware deployment have been studied independently, and there is limited literature that seamlessly integrates them. This paper presents a unified model-based reinforcement learning framework that bridges these two tasks in the robotics control domain. Our framework uses a probabilistic ensemble neural network for dynamics learning, allowing the quantification of epistemic uncertainty via Jensen-R\u00e9nyi Divergence. The two opposing tasks of exploration and deployment are optimized through state-of-the-art sampling-based MPC, resulting in efficient collection of training data and successful avoidance of uncertain state-action spaces. We conduct experiments on both autonomous vehicles and wheeled robots, showing promising results for both exploration and deployment.",
        "bibtex": "@INPROCEEDINGS{Kim-RSS-23, \r\n    AUTHOR    = {Taekyung Kim AND Jungwi Mun AND Junwon Seo AND Beomsu Kim AND Seongil Hong}, \r\n    TITLE     = {{Bridging Active Exploration and Uncertainty-Aware Deployment Using Probabilistic Ensemble Neural Network Dynamics}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.086} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p086.pdf",
        "supp": "",
        "pdf_size": 13578180,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10816969232278826646&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "AI Autonomy Technology Center, Agency for Defense Development; AI Autonomy Technology Center, Agency for Defense Development; AI Autonomy Technology Center, Agency for Defense Development; AI Autonomy Technology Center, Agency for Defense Development; AI Autonomy Technology Center, Agency for Defense Development",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://taekyung.me/rss2023-bridging",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Agency for Defense Development",
        "aff_unique_dep": "AI Autonomy Technology Center",
        "aff_unique_url": "https://www.add.re.kr",
        "aff_unique_abbr": "ADD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "1729581ca1",
        "title": "CCIL: Context-conditioned imitation learning for urban driving",
        "site": "https://www.roboticsproceedings.org/rss19/p101.html",
        "author": "Ke Guo; Wei Jing; Junbo Chen; Jia Pan",
        "abstract": "Imitation learning holds great promise for addressing the complex task of autonomous urban driving, as experienced human drivers can navigate highly challenging scenarios with ease. While behavior cloning is a widely used imitation learning approach in autonomous driving due to its exemption from risky online interactions, it suffers from the covariate shift issue. To address this limitation, we propose a context-conditioned imitation learning approach that employs a policy to map the context state into the ego vehicle's future trajectory, rather than relying on the traditional formulation of both ego and context states to predict the ego action. Additionally, to reduce the implicit ego information in the coordinate system, we design an ego-perturbed goal-oriented coordinate system. The origin of this coordinate system is the ego vehicle's position plus a zero mean Gaussian perturbation, and the x-axis direction points towards its goal position. Our experiments on the real-world large-scale Lyft and nuPlan datasets show that our method significantly outperforms state-of-the-art approaches.",
        "bibtex": "@INPROCEEDINGS{Guo-RSS-23, \r\n    AUTHOR    = {Ke Guo AND Wei Jing AND Junbo Chen AND Jia Pan}, \r\n    TITLE     = {{CCIL: Context-conditioned imitation learning for urban driving}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.101} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p101.pdf",
        "supp": "",
        "pdf_size": 2959800,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4566459378223926244&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "The University of Hong Kong + Alibaba Group; Alibaba Group; Alibaba Group; The University of Hong Kong",
        "aff_domain": "cs.hku.hk;gmail.com;zju.edu.cn;cs.hku.hk",
        "email": "cs.hku.hk;gmail.com;zju.edu.cn;cs.hku.hk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;1;0",
        "aff_unique_norm": "University of Hong Kong;Alibaba Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.hku.hk;https://www.alibaba.com",
        "aff_unique_abbr": "HKU;Alibaba",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "b13ce3a92a",
        "title": "CHSEL: Producing Diverse Plausible Pose Estimates from Contact and Free Space Data",
        "site": "https://www.roboticsproceedings.org/rss19/p077.html",
        "author": "Sheng Zhong; Dmitry Berenson; Nima Fazeli",
        "abstract": "This paper proposes a novel method for estimating the set of plausible poses of a rigid object from a set of points with volumetric information, such as whether each point is in free space or on the surface of the object. In particular, we study how pose can be estimated\r\nfrom force and tactile data arising from contact. \r\nUsing data derived from contact is challenging because it is inherently less information-dense than visual data, and thus the pose estimation problem is severely under-constrained when there are few contacts. \r\nRather than attempting to estimate the true pose of the object, which is not tractable without a large number of contacts, we seek to estimate a plausible set of poses which obey the constraints imposed by the sensor data. Existing methods struggle to estimate this set because they are either designed for single pose estimates or require informative priors to be effective. Our approach to this problem, Constrained pose Hypothesis Set Elimination (CHSEL), has three key attributes: 1) It considers volumetric information, which allows us to account for known free space; 2) It uses a novel differentiable volumetric cost function to take advantage of powerful gradient-based optimization tools; and 3) It uses methods from the Quality Diversity (QD) optimization literature to produce a diverse set of high-quality poses. To our knowledge, QD methods have not been used previously for pose registration. We also show how to update our plausible pose estimates online as more data is gathered by the robot. \r\nOur experiments suggest that CHSEL shows large performance improvements over several baseline methods for both simulated and real-world data.",
        "bibtex": "@INPROCEEDINGS{Zhong-RSS-23, \r\n    AUTHOR    = {Sheng Zhong AND Dmitry Berenson AND Nima Fazeli}, \r\n    TITLE     = {{CHSEL: Producing Diverse Plausible Pose Estimates from Contact and Free Space Data}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.077} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p077.pdf",
        "supp": "",
        "pdf_size": 10575655,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8748927443870299265&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "University of Michigan, Ann Arbor, MI 48109; University of Michigan, Ann Arbor, MI 48109; University of Michigan, Ann Arbor, MI 48109",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "https://github.com/UM-ARM-Lab/chsel",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "954a2f6376",
        "title": "CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory",
        "site": "https://www.roboticsproceedings.org/rss19/p074.html",
        "author": "Nur Muhammad (Mahi)Shafiullah; Chris Paxton; Lerrel Pinto; Soumith Chintala; Arthur Szlam",
        "abstract": "We propose CLIP-Fields, an implicit scene model that can be used for a variety of tasks, such as segmentation, instance identification, semantic search over space, and view localization. CLIP-Fields learns a mapping from spatial locations to semantic embedding vectors. Importantly, we show that this mapping can be trained with supervision coming only from web-image and web-text trained models such as CLIP, Detic, and Sentence-BERT; and thus uses no direct human supervision. When compared to baselines like Mask-RCNN, our method outperforms on few-shot instance identification or semantic segmentation on the HM3D dataset with only a fraction of the examples. Finally, we show that using CLIP-Fields as a scene memory, robots can perform semantic navigation in real-world environments. Our code and demonstration videos are available here: https://clip-fields.github.io",
        "bibtex": "@INPROCEEDINGS{Shafiullah-RSS-23, \r\n    AUTHOR    = {Nur Muhammad (Mahi)Shafiullah AND Chris Paxton AND Lerrel Pinto AND Soumith Chintala AND Arthur Szlam}, \r\n    TITLE     = {{CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.074} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p074.pdf",
        "supp": "",
        "pdf_size": 10040724,
        "gs_citation": 152,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13312297201416247036&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "1; 2; 1; 2; 2",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://mahis.life/clip-fields",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "",
        "aff_unique_norm": "",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "a66a7ff3f5",
        "title": "Causal Policy Gradient for Whole-Body Mobile Manipulation",
        "site": "https://www.roboticsproceedings.org/rss19/p049.html",
        "author": "Jiaheng Hu; Peter Stone; Roberto Mart\u00edn-Mart\u00edn",
        "abstract": "Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion\r\nby manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subspace of the robot\u2019s action space to address each sub-objective. Causal MoMa automatically discovers the causal dependencies between actions and terms of the reward function and exploits these dependencies in a causal policy learning procedure that reduces gradient variance compared to previous state-of-the-art policy gradient algorithms, improving convergence and results. We evaluate the performance of Causal MoMa on three types of simulated robots across different MoMa tasks and demonstrate success in transferring the policies trained in simulation directly to a real robot, where our agent is able to follow moving goals and react to dynamic obstacles while simultaneously and synergistically controlling the whole-body: base, arm, and head. More information at https://sites.google.com/view/causal-moma",
        "bibtex": "@INPROCEEDINGS{Hu-RSS-23, \r\n    AUTHOR    = {Jiaheng Hu AND Peter Stone AND Roberto Mart\u00edn-Mart\u00edn}, \r\n    TITLE     = {{Causal Policy Gradient for Whole-Body Mobile Manipulation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.049} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p049.pdf",
        "supp": "",
        "pdf_size": 4340265,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4521954468462340030&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "The University of Texas at Austin; The University of Texas at Austin + SonyAI; The University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "https://sites.google.com/view/causal-moma",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "University of Texas at Austin;Sony",
        "aff_unique_dep": ";Sony AI",
        "aff_unique_url": "https://www.utexas.edu;https://www.sony.com",
        "aff_unique_abbr": "UT Austin;SonyAI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "3204b83858",
        "title": "Centralized Model Predictive Control for Collaborative Loco-Manipulation",
        "site": "https://www.roboticsproceedings.org/rss19/p050.html",
        "author": "Flavio De Vincenti; Stelian Coros",
        "abstract": "In this work, we extend the model predictive control methods developed in the legged robotics literature to collaborative loco-manipulation settings. The systems we study entail a payload collectively carried by multiple quadruped robots equipped with a mechanical arm. We use a direct multiple shooting method to solve the resulting high-dimensional, optimal control problems for trajectories of ground reaction forces, manipulation wrenches, and stepping locations. To capture the dominant dynamics of the system, we model each agent and the shared payload as single rigid bodies. We demonstrate the versatility of our framework in a series of simulation experiments involving collaborative manipulation over challenging terrains.",
        "bibtex": "@INPROCEEDINGS{Vincenti-RSS-23, \r\n    AUTHOR    = {Flavio De Vincenti AND Stelian Coros}, \r\n    TITLE     = {{Centralized Model Predictive Control for Collaborative Loco-Manipulation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.050} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p050.pdf",
        "supp": "",
        "pdf_size": 8451176,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10669291984126081852&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Computational Robotics Lab, ETH Zurich, Switzerland; Computational Robotics Lab, ETH Zurich, Switzerland",
        "aff_domain": "inf.ethz.ch; ",
        "email": "inf.ethz.ch; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Computational Robotics Lab",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "8d50d56996",
        "title": "Cherry-Picking with Reinforcement Learning",
        "site": "https://www.roboticsproceedings.org/rss19/p021.html",
        "author": "Yunchu Zhang; Liyiming Ke; Abhay Deshpande; Abhishek Gupta; Siddhartha Srinivasa",
        "abstract": "Grasping small objects surrounded by unstable or non-rigid material plays a crucial role in applications such as surgery, harvesting, construction, disaster recovery, and assisted feeding. This task is especially difficult when fine manipulation is required in the presence of sensor noise and perception errors; errors inevitably trigger dynamic motion, which is challenging to model precisely. Circumventing the difficulty to build accurate models for contacts and dynamics, data-driven methods like reinforcement learning (RL) can optimize task performance via trial and error, reducing the need for accurate models of contacts and dynamics. Applying RL methods to real robots, however, has been hindered by factors such as prohibitively high sample complexity or the high training infrastructure cost for providing resets on hardware. This work presents CherryBot, an RL system that uses chopsticks for fine manipulation that surpasses human reactiveness for some dynamic grasping tasks. By integrating imprecise simulators, suboptimal demonstrations and external state estimation, we study how to make a real-world robot learning system sample efficient and general while reducing the human effort required for supervision. Our system shows continual improvement through 30 minutes of real-world interaction: through reactive retry, it achieves an almost 100% success rate on the demanding task of using chopsticks to grasp small objects swinging in the air. We demonstrate the reactiveness, robustness and generalizability of CherryBot to varying object shapes and dynamics (e.g., external disturbances like wind and human perturbations). Videos are available at https://goodcherrybot.github.io/.",
        "bibtex": "@INPROCEEDINGS{Zhang-RSS-23, \r\n    AUTHOR    = {Yunchu Zhang AND Liyiming Ke AND Abhay Deshpande AND Abhishek Gupta AND Siddhartha Srinivasa}, \r\n    TITLE     = {{Cherry-Picking with Reinforcement Learning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.021} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p021.pdf",
        "supp": "",
        "pdf_size": 1165138,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15886496866536784650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "The Robotics Institute, Carnegie Mellon University, Pittsburgh, USA+Paul G Allen School of Computer Science and Engineering, Seattle, USA; The Robotics Institute, Carnegie Mellon University, Pittsburgh, USA+Paul G Allen School of Computer Science and Engineering, Seattle, USA; Paul G Allen School of Computer Science and Engineering, Seattle, USA; Paul G Allen School of Computer Science and Engineering, Seattle, USA; Paul G Allen School of Computer Science and Engineering, Seattle, USA",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://goodcherrybot.github.io/",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1;1;1;1",
        "aff_unique_norm": "Carnegie Mellon University;University of Washington",
        "aff_unique_dep": "The Robotics Institute;Paul G Allen School of Computer Science and Engineering",
        "aff_unique_url": "https://www.cmu.edu;https://www.cs.washington.edu",
        "aff_unique_abbr": "CMU;UW CSE",
        "aff_campus_unique_index": "0+1;0+1;1;1;1",
        "aff_campus_unique": "Pittsburgh;Seattle",
        "aff_country_unique_index": "0+0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fdf8cc9362",
        "title": "Co-optimization of Morphology and Behavior of Modular Robots via Hierarchical Deep Reinforcement Learning",
        "site": "https://www.roboticsproceedings.org/rss19/p096.html",
        "author": "Jieqiang Sun; Meibao Yao; Xueming Xiao; Zhibing Xie; Bo Zheng",
        "abstract": "Modular robots hold the promise of changing their shape and even dimension to adapt to various tasks and environments. To realize this superiority, it is essential to find the appropriate morphology and its corresponding behavior simultaneously to ensure optimality of the reconfiguration. However, achieving co-optimization is challenging because robotic configuration and motion are interactive and coupled with each other, as well as their optimization processes. To this end, we proposed a co-optimization framework based on hierarchical Deep Reinforcement Learning (DRL), consisting of a configuration model and a motion model based on the Twin Delayed Deep Deterministic policy gradient algorithm (TD3). The two network models update asynchronously with a shared reward to ensure co-optimality. We conduct simulations and experiments with the Webots platform to validate the proposed framework, and the preliminary results show that it yields high quality optimization schemes and thus allows modular robots to be more adaptive to dynamic and multi-task scenarios.",
        "bibtex": "@INPROCEEDINGS{Sun-RSS-23, \r\n    AUTHOR    = {Jieqiang Sun AND Meibao Yao AND Xueming Xiao AND Zhibing Xie AND Bo Zheng}, \r\n    TITLE     = {{Co-optimization of Morphology and Behavior of Modular Robots via Hierarchical Deep Reinforcement Learning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.096} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p096.pdf",
        "supp": "",
        "pdf_size": 2756175,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6674952020267497383&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Intelligent Robotics Lab (IRL), School of Artificial Intelligence, Jilin University, Changchun, 130012, China + Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, Ministry of Education, China; CVIR Lab, Changchun University of Science and Technology, Changchun, 130012, China; Intelligent Robotics Lab (IRL), School of Artificial Intelligence, Jilin University, Changchun, 130012, China; Intelligent Robotics Lab (IRL), School of Artificial Intelligence, Jilin University, Changchun, 130012, China; Shanghai Aerospace Control Technology Institute, Shanghai, 201109, China",
        "aff_domain": "jlu.edu.cn;jlu.edu.cn;cust.edu.cn;jlu.edu.cn;sacti.com.cn",
        "email": "jlu.edu.cn;jlu.edu.cn;cust.edu.cn;jlu.edu.cn;sacti.com.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;0;0;3",
        "aff_unique_norm": "Jilin University;Engineering Research Center of Knowledge-Driven Human-Machine Intelligence;Changchun University of Science and Technology;Shanghai Aerospace Control Technology Institute",
        "aff_unique_dep": "School of Artificial Intelligence;Ministry of Education;CVIR Lab;",
        "aff_unique_url": "https://www.jlu.edu.cn;;;",
        "aff_unique_abbr": "JLU;;;",
        "aff_campus_unique_index": "0;0;0;0;2",
        "aff_campus_unique": "Changchun;;Shanghai",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "a491fb7a38",
        "title": "CoDEPS: Online Continual Learning for Depth Estimation and Panoptic Segmentation",
        "site": "https://www.roboticsproceedings.org/rss19/p073.html",
        "author": "Niclas V\u00f6disch; K\u00fcrsat Petek; Wolfram Burgard; Abhinav Valada",
        "abstract": "Operating a robot in the open world requires a high level of robustness with respect to previously unseen environments. Optimally, the robot is able to adapt by itself to new conditions without human supervision, e.g., automatically adjusting its perception system to changing lighting conditions. In this work, we address the task of continual learning for deep learning-based monocular depth estimation and panoptic segmentation in new environments in an online manner. We introduce CoDEPS to perform continual learning involving multiple real-world domains while mitigating catastrophic forgetting by leveraging experience replay. In particular, we propose a novel domain-mixing strategy to generate pseudo-labels to adapt panoptic segmentation. Furthermore, we explicitly address the limited storage capacity of robotic systems by leveraging sampling strategies for constructing a fixed-size replay buffer based on rare semantic class sampling and image diversity. We perform extensive evaluations of CoDEPS on various real-world datasets demonstrating that it successfully adapts to unseen environments without sacrificing performance on previous domains while achieving state-of-the-art results. The code of our work is publicly available at http://codeps.cs.uni-freiburg.de.",
        "bibtex": "@INPROCEEDINGS{V\u00f6disch-RSS-23, \r\n    AUTHOR    = {Niclas V\u00f6disch AND K\u00fcrsat Petek AND Wolfram Burgard AND Abhinav Valada}, \r\n    TITLE     = {{CoDEPS: Online Continual Learning for Depth Estimation and Panoptic Segmentation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.073} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p073.pdf",
        "supp": "",
        "pdf_size": 8196660,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10577692462836428249&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Freiburg; University of Freiburg; University of Technology Nuremberg; University of Freiburg",
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "github": "",
        "project": "http://codeps.cs.uni-freiburg.de",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Freiburg;Nuremberg University of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-freiburg.de;https://www.tu-nuernberg.de",
        "aff_unique_abbr": "UoF;TUN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "1159362940",
        "title": "ConceptFusion: Open-set multimodal 3D mapping",
        "site": "https://www.roboticsproceedings.org/rss19/p066.html",
        "author": "Krishna Murthy Jatavallabhula; Alihusein Kuwajerwala; Qiao Gu; Mohd Omama; Ganesh Iyer; Soroush Saryazdi; Tao Chen; Alaa Maalouf; Shuang Li; Nikhil Varma Keetha; Ayush Tewari; Joshua Tenenbaum; Celso de Melo; Madhava Krishna; Liam Paull; Florian Shkurti; Antonio Torralba",
        "abstract": "Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. To address this issue, we propose ConceptFusion, a scene representation that is: (i) fundamentally open-set, enabling reasoning beyond a closed set of concepts (ii) inherently multi-modal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today\u2019s foundation models that have been pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.",
        "bibtex": "@INPROCEEDINGS{Jatavallabhula-RSS-23, \r\n    AUTHOR    = {Krishna Murthy Jatavallabhula AND Alihusein Kuwajerwala AND Qiao Gu AND Mohd Omama AND Ganesh Iyer AND Soroush Saryazdi AND Tao Chen AND Alaa Maalouf AND Shuang Li AND Nikhil Varma Keetha AND Ayush Tewari AND Joshua Tenenbaum AND Celso de Melo AND Madhava Krishna AND Liam Paull AND Florian Shkurti AND Antonio Torralba}, \r\n    TITLE     = {{ConceptFusion: Open-set multimodal 3D mapping}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.066} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p066.pdf",
        "supp": "",
        "pdf_size": 16748334,
        "gs_citation": 249,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2509961475764126578&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;",
        "github": "",
        "project": "https://concept-fusion.github.io/",
        "author_num": 17,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2c42cb1c98",
        "title": "Concurrent Constrained Optimization of Unknown Rewards for Multi-Robot Task Allocation",
        "site": "https://www.roboticsproceedings.org/rss19/p108.html",
        "author": "Sukriti Singh; Anusha Srikanthan; Vivek Mallampati; Harish Ravichandar",
        "abstract": "Task allocation can enable effective coordination of multi-robot teams to accomplish tasks that are intractable for individual robots. However, existing approaches to task allocation often assume that task requirements or reward functions are known and explicitly specified by the user. In this work, we consider the challenge of forming effective coalitions for a given heterogeneous multi-robot team when task reward functions are unknown. To this end, we first formulate a new class of problems, dubbed COncurrent Constrained Online optimization of Allocation (COCOA). The COCOA problem requires online optimization of coalitions such that the unknown rewards of all the tasks are simultaneously maximized using a given multi-robot team with constrained resources. To address the COCOA problem, we introduce an online optimization algorithm, named Concurrent Multi-Task Adaptive Bandits (CMTAB), that leverages and builds upon continuum-armed bandit algorithms. Experiments involving detailed numerical simulations and a simulated emergency response task reveal that CMTAB can effectively trade-off exploration and exploitation to simultaneously and efficiently optimize the unknown task rewards while respecting the team's resource constraints.",
        "bibtex": "@INPROCEEDINGS{Singh-RSS-23, \r\n    AUTHOR    = {Sukriti Singh AND Anusha Srikanthan AND Vivek Mallampati AND Harish Ravichandar}, \r\n    TITLE     = {{Concurrent Constrained Optimization of Unknown Rewards for Multi-Robot Task Allocation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.108} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p108.pdf",
        "supp": "",
        "pdf_size": 1274807,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13045867278737106319&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Georgia Institute of Technology; University of Pennsylvania; Georgia Institute of Technology; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;seas.upenn.edu;gatech.edu;gatech.edu",
        "email": "gatech.edu;seas.upenn.edu;gatech.edu;gatech.edu",
        "github": "https://github.com/GT-STAR-Lab/CMTAB",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Georgia Institute of Technology;University of Pennsylvania",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://www.upenn.edu",
        "aff_unique_abbr": "Georgia Tech;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3bdffd3650",
        "title": "Convex Geometric Motion Planning on Lie Groups via Moment Relaxation",
        "site": "https://www.roboticsproceedings.org/rss19/p058.html",
        "author": "Sangli Teng; Ashkan Jasour; Ram Vasudevan; Maani Ghaffari Jadidi",
        "abstract": "This paper reports a novel result: with proper robot models on matrix Lie groups, one can formulate the kinodynamic motion planning problem for rigid body systems as \\emph{exact} polynomial optimization problems that can be relaxed as semidefinite programming (SDP). Due to the nonlinear rigid body dynamics, the motion planning problem for rigid body systems is nonconvex. Existing global optimization-based methods do not properly deal with the configuration space of the 3D rigid body; thus, they do not scale well to long-horizon planning problems. We use Lie groups as the configuration space in our formulation and apply the variational integrator to formulate the forced rigid body systems as quadratic polynomials. Then we leverage Lasserre's hierarchy to obtain the globally optimal solution via SDP. By constructing the motion planning problem in a sparse manner, the results show that the proposed algorithm has \\emph{linear} complexity with respect to the planning horizon. This paper demonstrates the proposed method can provide rank-one optimal solutions at relaxation order two for most of the testing cases of 1) 3D drone landing using the full dynamics model and 2) inverse kinematics for serial manipulators.",
        "bibtex": "@INPROCEEDINGS{Teng-RSS-23, \r\n    AUTHOR    = {Sangli Teng AND Ashkan Jasour AND Ram Vasudevan AND Maani Ghaffari Jadidi}, \r\n    TITLE     = {{Convex Geometric Motion Planning on Lie Groups via Moment Relaxation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.058} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p058.pdf",
        "supp": "",
        "pdf_size": 2175014,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11402948016163681969&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Michigan, Ann Arbor, MI 48109, USA; Team 347T-Robotic Aerial Mobility, Jet Propulsion Lab, Pasadena, CA, 91109; University of Michigan, Ann Arbor, MI 48109, USA; University of Michigan, Ann Arbor, MI 48109, USA",
        "aff_domain": "umich.edu;jpl.caltech.edu;umich.edu;umich.edu",
        "email": "umich.edu;jpl.caltech.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Michigan;Jet Propulsion Lab",
        "aff_unique_dep": ";Robotic Aerial Mobility",
        "aff_unique_url": "https://www.umich.edu;https://www.jpl.nasa.gov",
        "aff_unique_abbr": "UM;JPL",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Ann Arbor;Pasadena",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d410a46c98",
        "title": "Decentralization and Acceleration Enables Large-Scale Bundle Adjustment",
        "site": "https://www.roboticsproceedings.org/rss19/p111.html",
        "author": "Taosha Fan; Joseph Ortiz; Ming Hsiao; Maurizio Monge; Jing Dong; Todd Murphey; Mustafa Mukadam",
        "abstract": "Scaling to arbitrarily large bundle adjustment problems requires data and compute to be distributed across multiple devices. Centralized methods in prior works are only able to solve small or medium size problems due to overhead in computation and communication. In this paper, we present a fully decentralized method that alleviates computation and communication bottlenecks to solve arbitrarily large bundle adjustment problems. We achieve this by reformulating the reprojection error and deriving a novel surrogate function that decouples optimization variables from different devices. This function makes it possible to use majorization minimization techniques and reduces bundle adjustment to independent optimization subproblems that can be solved in parallel. We further apply Nesterov's acceleration and adaptive restart to improve convergence while maintaining its theoretical guarantees. Despite limited peer-to-peer communication, our method has provable convergence to first-order critical points under mild conditions. On extensive benchmarks with public datasets, our method converges much faster than decentralized baselines with similar memory usage and communication load. Compared to centralized baselines using a single device, our method, while being decentralized, yields more accurate solutions with significant speedups of up to 953.7x over Ceres and 174.6x over DeepLM. Code: https://github.com/facebookresearch/DABA.",
        "bibtex": "@INPROCEEDINGS{Fan-RSS-23, \r\n    AUTHOR    = {Taosha Fan AND Joseph Ortiz AND Ming Hsiao AND Maurizio Monge AND Jing Dong AND Todd Murphey AND Mustafa Mukadam}, \r\n    TITLE     = {{Decentralization and Acceleration Enables Large-Scale Bundle Adjustment}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.111} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p111.pdf",
        "supp": "",
        "pdf_size": 6883541,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9075972333164995940&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "aaa7476392",
        "title": "Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators",
        "site": "https://www.roboticsproceedings.org/rss19/p022.html",
        "author": "Alexander Herzog; Kanishka Rao; Karol Hausman; Yao Lu; Paul Wohlhart; Mengyuan Yan; Jessica Lin; Montserrat Gonzalez Arenas; Ted Xiao; Daniel Kappler; Daniel Ho; Jarek Rettinghouse; Yevgen Chebotar; Kuang-Huei Lee; Keerthana Gopalakrishnan; Ryan Julian; Adrian Li; Chuyuan Fu; Bob Wei; Sangeetha Ramesh; Khem Holden; Kim Kleiven; David J Rendleman; Sean Kirmani; Jeffrey Bingham; Jonathan Weisz; Ying Xu; Wenlong Lu; Matthew Bennice; Cody Fong; David Do; Jessica Lam; Yunfei Bai; Benjie Holson; Michael Quinlan; Noah Brown; Mrinal Kalakrishnan; Julian Ibarz; Peter Pastor; Sergey Levine",
        "abstract": "We describe a system for deep reinforcement learning of robotic manipulation skills applied to a large-scale real-world task: sorting recyclables and trash in office buildings. Real-world deployment of deep RL policies requires not only effective training algorithms, but the ability to bootstrap real-world training and enable broad generalization. To this end, our system combines scalable deep RL from real-world data with bootstrapping from training in simulation, and incorporates auxiliary inputs from existing computer vision systems as a way to boost generalization to novel objects, while retaining the benefits of end-to-end training. We analyze the tradeoffs of different design decisions in our system, and present a large-scale empirical validation that includes training on real-world data gathered over the course of 24\r\nmonths of experimentation, across a fleet of 23 robots in three office buildings, with a total training set of 9527 hours of robotic experience. Our final validation also consists of 4800 evaluation trials across 240 waste station configurations, in order to evaluate in detail the impact of the design decisions in our system, the scaling effects of including more real-world data, and the performance of the method on novel objects.",
        "bibtex": "@INPROCEEDINGS{Herzog-RSS-23, \r\n    AUTHOR    = {Alexander Herzog AND Kanishka Rao AND Karol Hausman AND Yao Lu AND Paul Wohlhart AND Mengyuan Yan AND Jessica Lin AND Montserrat Gonzalez Arenas AND Ted Xiao AND Daniel Kappler AND Daniel Ho AND Jarek Rettinghouse AND Yevgen Chebotar AND Kuang-Huei Lee AND Keerthana Gopalakrishnan AND Ryan Julian AND Adrian Li AND Chuyuan Fu AND Bob Wei AND Sangeetha Ramesh AND Khem Holden AND Kim Kleiven AND David J Rendleman AND Sean Kirmani AND Jeffrey Bingham AND Jonathan Weisz AND Ying Xu AND Wenlong Lu AND Matthew Bennice AND Cody Fong AND David Do AND Jessica Lam AND Yunfei Bai AND Benjie Holson AND Michael Quinlan AND Noah Brown AND Mrinal Kalakrishnan AND Julian Ibarz AND Peter Pastor AND Sergey Levine}, \r\n    TITLE     = {{Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.022} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p022.pdf",
        "supp": "",
        "pdf_size": 18086569,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15322994731025705577&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Everyday Robots\u2020; Robotics at Google\u2021; Everyday Robots\u2020; Robotics at Google\u2021; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Robotics at Google\u2021; Robotics at Google\u2021; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Robotics at Google\u2021; Robotics at Google\u2021; Robotics at Google\u2021; Robotics at Google\u2021; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Robotics at Google\u2021; Everyday Robots\u2020; Robotics at Google\u2021; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Everyday Robots\u2020; Robotics at Google\u2021; Robotics at Google\u2021; Everyday Robots\u2020; Robotics at Google\u2021",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "rl-at-scale.github.io",
        "author_num": 40,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;0;0;0;1;1;0;0;0;1;1;1;1;0;0;0;0;0;1;0;1;0;0;0;0;0;0;0;0;0;0;0;0;1;1;0;1",
        "aff_unique_norm": "Everyday Robots;Google",
        "aff_unique_dep": ";Robotics",
        "aff_unique_url": "https://everydayrobots.com;https://www.google.com",
        "aff_unique_abbr": "Everyday Robots;Google Robotics",
        "aff_campus_unique_index": "1;1;1;1;1;1;1;1;1;1;1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ea54b86154",
        "title": "Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning",
        "site": "https://www.roboticsproceedings.org/rss19/p056.html",
        "author": "Ilya Kostrikov; Laura M Smith; Sergey Levine",
        "abstract": "Deep reinforcement learning is a promising approach to learning policies in unstructured environments. Due to its sample inefficiency, though, deep RL applications have primarily focused on simulated environments. In this work, we demonstrate that the recent advancements in machine learning algorithms and libraries combined with careful MDP formulation lead to learning quadruped locomotion in only 20 minutes in the real world. We evaluate our approach on several indoor and outdoor terrains that are known to be challenging for classical, model-based controllers and observe that the robot consistently learns a walking gait on all of these terrains. Finally, we evaluate our design decisions in a simulated environment.",
        "bibtex": "@INPROCEEDINGS{Kostrikov-RSS-23, \r\n    AUTHOR    = {Ilya Kostrikov AND Laura M Smith AND Sergey Levine}, \r\n    TITLE     = {{Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.056} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p056.pdf",
        "supp": "",
        "pdf_size": 5784212,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7853311438449005281&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Berkeley AI Research, UC Berkeley; Berkeley AI Research, UC Berkeley; Berkeley AI Research, UC Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;eecs.berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "https://sites.google.com/berkeley.edu/walk-in-the-park",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Berkeley AI Research",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c1a12889bf",
        "title": "Demonstrating Arena-Web: A Web-based Development and Benchmarking Platform for Autonomous Navigation Approaches",
        "site": "https://www.roboticsproceedings.org/rss19/p088.html",
        "author": "Linh K\u00e4stner; Reyk Carstens; Lena Nahrwold; Christopher  Liebig; Volodymyr Shcherbyna; Subhin Lee; Jens Lambrecht",
        "abstract": "In recent years, mobile robot navigation approaches have become increasingly important due to various application areas ranging from healthcare to warehouse logistics. In particular, Deep Reinforcement Learning approaches have gained popularity for robot navigation but are not easily accessible to non-experts and complex to develop. In recent years, efforts have been made to make these sophisticated approaches accessible to a wider audience. In this paper, we present Arena-Web, a web-based development and evaluation suite for developing, training, and testing DRL-based navigation planners for various robotic platforms and scenarios. The interface is designed to be intuitive and engaging to appeal to non-experts and make the technology accessible to a wider audience. With Arena-Web and its interface, training and developing Deep Reinforcement Learning agents is simplified and made easy without a single line of code. The web-app is free to use and openly available under the link stated in the supplementary materials.",
        "bibtex": "@INPROCEEDINGS{K\u00e4stner-RSS-23, \r\n    AUTHOR    = {Linh K\u00e4stner AND Reyk Carstens AND Lena Nahrwold AND Christopher  Liebig AND Volodymyr Shcherbyna AND Subhin Lee AND Jens Lambrecht}, \r\n    TITLE     = {{Demonstrating Arena-Web: A Web-based Development and Benchmarking Platform for Autonomous Navigation Approaches}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.088} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p088.pdf",
        "supp": "",
        "pdf_size": 1049306,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14244772794385949585&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany; Chair Industry Grade Networks and Clouds, Faculty of Electrical Engineering, and Computer Science, Berlin Institute of Technology, Berlin, Germany",
        "aff_domain": "tu-berlin.de; ; ; ; ; ; ",
        "email": "tu-berlin.de; ; ; ; ; ; ",
        "github": "",
        "project": "https://app.arena-rosnav.com",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Berlin Institute of Technology",
        "aff_unique_dep": "Faculty of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.tu-berlin.de",
        "aff_unique_abbr": "TU Berlin",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Berlin",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "59c7fc5e9a",
        "title": "Demonstrating Large Language Models on Robots",
        "site": "https://www.roboticsproceedings.org/rss19/p024.html",
        "author": "Andy Zeng; Brian Ichter; Fei Xia; Ted Xiao; Vikas Sindhwani",
        "abstract": "Robots may benefit from large language models (LLMs), which have demonstrated strong reasoning capabilities across various domains. This demonstration includes several systems based on recent methods that integrate LLMs on robots: SayCan, Socratic Models, Inner Monologue, and Code as Policies. While each algorithm highlights a different mode of grounding, they all share a common system-level structure in that they use LLMs to take as input natural language instructions and generate robot plans in the form of step-by-step procedures or code. This structure provides several practical perks for demonstration in that (i) we can use existing video chat interfaces to instruct the robot by typing commands and broadcasting its movements in action via video streaming, (ii) one can seamlessly switch between interfaces that communicate with different robots, and (iii) this can all be done remotely on a laptop, where the robots on real hardware can be held on standby in the lab ready to run on command. Our tentative plan is to show at least one system running on real hardware remotely -- Inner Monologue or Code as Policies, and solicit task instructions from a live audience. Time-permitting we may also demonstrate the other systems available to run on real hardware. Otherwise, we will present recorded videos of past runs. We will link to open-source code, and conclude with a discussion of open research questions in the area.",
        "bibtex": "@INPROCEEDINGS{Zeng-RSS-23, \r\n    AUTHOR    = {Andy Zeng AND Brian Ichter AND Fei Xia AND Ted Xiao AND Vikas Sindhwani}, \r\n    TITLE     = {{Demonstrating Large Language Models on Robots}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.024} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p024.pdf",
        "supp": "",
        "pdf_size": 2228050,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d6160d9c6d",
        "title": "Demonstrating Large-Scale Package Manipulation via Learned Metrics of Pick Success",
        "site": "https://www.roboticsproceedings.org/rss19/p023.html",
        "author": "Shuai Li; Azarakhsh Keipour; Kevin Jamieson; Nicolas Hudson; Charles Swan; Kostas Bekris",
        "abstract": "Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.\r\n\r\nThis paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.\r\n\r\nThe developed learned pick quality measure ranks various pick alternatives in real-time and prioritizes the most promising ones for execution. The pick success predictor aims to estimate from prior experience the success probability of a desired pick by the deployed industrial robotic arms in cluttered scenes containing deformable and rigid objects with partially known properties. It is a shallow machine learning model, which allows us to evaluate which features are most important for the prediction. An online pick ranker leverages the learned success predictor to prioritize the most promising picks for the robotic arm, which are then assessed for collision avoidance. This learned ranking process is demonstrated to overcome the limitations and outperform the performance of manually engineered and heuristic alternatives.\r\n\r\nTo the best of the authors' knowledge, this paper presents the first large-scale deployment of learned pick quality estimation methods in a real production system.",
        "bibtex": "@INPROCEEDINGS{Li-RSS-23, \r\n    AUTHOR    = {Shuai Li AND Azarakhsh Keipour AND Kevin Jamieson AND Nicolas Hudson AND Charles Swan AND Kostas Bekris}, \r\n    TITLE     = {{Demonstrating Large-Scale Package Manipulation via Learned Metrics of Pick Success}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.023} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p023.pdf",
        "supp": "",
        "pdf_size": 6710968,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17494979471544779663&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "fcad886a17",
        "title": "Demonstrating Mobile Manipulation in the Wild: A Metrics-Driven Approach",
        "site": "https://www.roboticsproceedings.org/rss19/p055.html",
        "author": "Max Bajracharya; James Borders; Richard Cheng; Dan Helmick; Lukas Kaul; Dan Kruse; John Leichty; Jeremy Ma; Carolyn Matl; Frank Michel; Chavdar Papazov; Josh Petersen; Krishna Shankar; Mark Tjersland",
        "abstract": "We present our general-purpose mobile manipulation system consisting of a custom robot platform and key algorithms spanning perception and planning. To extensively test the system in the wild and benchmark its performance, we choose a grocery shopping scenario in an actual, unmodified grocery store. We derive key performance metrics from detailed robot log data collected during six week-long field tests, spread across 18 months. These objective metrics, gained from complex yet repeatable tests, drive the direction of our research efforts and let us continuously improve our system\u2019s performance. We find that thorough end-to-end system-level testing of a complex mobile manipulation system can serve as a reality-check for state-of-the-art methods in robotics. This effectively grounds robotics research efforts in real world needs and challenges, which we deem highly useful for the advancement of the field. To this end, we share our key insights and takeaways to inspire and accelerate similar system-level research projects.",
        "bibtex": "@INPROCEEDINGS{Bajracharya-RSS-23, \r\n    AUTHOR    = {Max Bajracharya AND James Borders AND Richard Cheng AND Dan Helmick AND Lukas Kaul AND Dan Kruse AND John Leichty AND Jeremy Ma AND Carolyn Matl AND Frank Michel AND Chavdar Papazov AND Josh Petersen AND Krishna Shankar AND Mark Tjersland}, \r\n    TITLE     = {{Demonstrating Mobile Manipulation in the Wild: A Metrics-Driven Approach}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.055} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p055.pdf",
        "supp": "",
        "pdf_size": 14473359,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17896900675565223594&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 14,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f3ab4244ee",
        "title": "Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI",
        "site": "https://www.roboticsproceedings.org/rss19/p087.html",
        "author": "Haoyuan Fu; Wenqiang Xu; Ruolin Ye; Han Xue; Zhenjun Yu; Tutian Tang; Yutong Li; Wenxin Du; Jieyi Zhang; Cewu Lu",
        "abstract": "Multiphysics phenomena, the coupling effects involving different aspects of physics laws, are pervasive in the real world and can often be encountered when performing everyday household tasks. Intelligent agents which seek to assist\r\nor replace human laborers will need to learn to cope with such phenomena in household task settings. To equip the agents with such kind of abilities, the research community needs a simulation environment, which will have the capability to serve as the testbed for the training process of these intelligent agents, to have the ability to support multiphysics coupling effects.\r\n\r\nThough many mature simulation software for multiphysics simulation have been adopted in industrial production, such techniques have not been applied to robot learning or embodied AI research. To bridge the gap, we propose a novel simulation environment named RFUniverse. This simulator can not only compute rigid and multi-body dynamics, but also multiphysics coupling effects commonly observed in daily life, such as air-solid interaction, fluid-solid interaction, and heat transfer.\r\n\r\nBecause of the unique multiphysics capacities of this simulator, we can benchmark tasks that involve complex dynamics due to multiphysics coupling effects in a simulation environment before deploying to the real world. RFUniverse provides multiple interfaces to let the users interact with the virtual world in various ways, which is helpful and essential for learning, planning, and control. We benchmark three tasks with reinforcement learning, including food cutting, water pushing, and towel catching. We also evaluate butter pushing with a classic planning-control paradigm. This simulator offers an enhancement of physics simulation in terms of the computation of multiphysics coupling effects. The simulation environment, videos, and other supplementary materials can be viewed on the website: https: //sites.google.com/view/rfuniverse.",
        "bibtex": "@INPROCEEDINGS{Fu-RSS-23, \r\n    AUTHOR    = {Haoyuan Fu AND Wenqiang Xu AND Ruolin Ye AND Han Xue AND Zhenjun Yu AND Tutian Tang AND Yutong Li AND Wenxin Du AND Jieyi Zhang AND Cewu Lu}, \r\n    TITLE     = {{Demonstrating RFUniverse: A Multiphysics Simulation Platform for Embodied AI}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.087} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p087.pdf",
        "supp": "",
        "pdf_size": 3809189,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; Cornell University + Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;cornell.edu;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;cornell.edu;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn",
        "github": "",
        "project": "https://sites.google.com/view/rfuniverse",
        "author_num": 10,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1+0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;Cornell University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.cornell.edu",
        "aff_unique_abbr": "SJTU;Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0;0;0;0;0;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "cb35ef79cf",
        "title": "DexPBT: Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training",
        "site": "https://www.roboticsproceedings.org/rss19/p037.html",
        "author": "Aleksei Petrenko; Arthur Allshire; Gavriel State; Ankur Handa; Viktor Makoviychuk",
        "abstract": "In this work, we propose algorithms and methods that enable learning dexterous object manipulation using simulated one- or two-armed robots equipped with multi-fingered hand end-effectors. Using a parallel GPU-accelerated physics simulator (Isaac Gym), we implement challenging tasks for these robots, including regrasping, grasp-and-throw, and object reorientation. To solve these problems we introduce a decentralized Population-Based Training (PBT) algorithm that allows us to massively amplify the exploration capabilities of deep reinforcement learning. We find that this method significantly outperforms regular end-to-end learning and is able to discover robust control policies in challenging tasks. Video demonstrations of learned behaviors and the code can be found at https://sites.google.com/view/dexpbt",
        "bibtex": "@INPROCEEDINGS{Petrenko-RSS-23, \r\n    AUTHOR    = {Aleksei Petrenko AND Arthur Allshire AND Gavriel State AND Ankur Handa AND Viktor Makoviychuk}, \r\n    TITLE     = {{DexPBT: Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.037} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p037.pdf",
        "supp": "",
        "pdf_size": 3624774,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5550747502415772616&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "64ca5505bd",
        "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
        "site": "https://www.roboticsproceedings.org/rss19/p026.html",
        "author": "Cheng Chi; Siyuan Feng; Yilun Du; Zhenjia Xu; Eric Cousineau; Benjamin CM Burchfiel; Shuran Song",
        "abstract": "This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the score function of the action distribution and optimizes with respect to this gradient field iteratively during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details will be publicly available.",
        "bibtex": "@INPROCEEDINGS{Chi-RSS-23, \r\n    AUTHOR    = {Cheng Chi AND Siyuan Feng AND Yilun Du AND Zhenjia Xu AND Eric Cousineau AND Benjamin CM Burchfiel AND Shuran Song}, \r\n    TITLE     = {{Diffusion Policy: Visuomotor Policy Learning via Action Diffusion}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.026} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p026.pdf",
        "supp": "",
        "pdf_size": 5811288,
        "gs_citation": 834,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17610759778971274244&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0048060da3",
        "title": "Distributed Hierarchical Distribution Control for Very-Large-Scale Clustered Multi-Agent Systems",
        "site": "https://www.roboticsproceedings.org/rss19/p110.html",
        "author": "Augustinos D Saravanos; Yihui Li; Evangelos Theodorou",
        "abstract": "As the scale and complexity of multi-agent robotic systems are subject to a continuous increase, this paper considers a class of systems labeled as Very-Large-Scale Multi-Agent Systems (VLMAS) with dimensionality that can scale up to the order of millions of agents. In particular, we consider the problem of steering the state distributions of all agents of a VLMAS to prescribed target distributions while satisfying probabilistic safety guarantees. Based on the key assumption that such systems often admit a multi-level hierarchical clustered structure - where the agents are organized into cliques of different levels - we associate the control of such cliques with the control of distributions, and introduce the Distributed Hierarchical Distribution Control (DHDC) framework. The proposed approach consists of two sub-frameworks. The first one, Distributed Hierarchical Distribution Estimation (DHDE), is a bottom-up hierarchical decentralized algorithm which links the initial and target configurations of the cliques of all levels with suitable Gaussian distributions. The second part, Distributed Hierarchical Distribution Steering (DHDS), is a top-down hierarchical distributed method that steers the distributions of all cliques and agents from the initial to the targets ones assigned by DHDE. Simulation results that scale up to two million agents demonstrate the effectiveness and scalability of the proposed framework. The increased computational efficiency and safety performance of DHDC against related methods is also illustrated. The results of this work indicate the importance of hierarchical distribution control approaches towards achieving safe and scalable solutions for the control of VLMAS.",
        "bibtex": "@INPROCEEDINGS{Saravanos-RSS-23, \r\n    AUTHOR    = {Augustinos D Saravanos AND Yihui Li AND Evangelos Theodorou}, \r\n    TITLE     = {{Distributed Hierarchical Distribution Control for Very-Large-Scale Clustered Multi-Agent Systems}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.110} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p110.pdf",
        "supp": "",
        "pdf_size": 2747733,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6389155236358234642&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Georgia Institute of Technology, GA, USA; Georgia Institute of Technology, GA, USA; Georgia Institute of Technology, GA, USA",
        "aff_domain": "gatech.edu; ; ",
        "email": "gatech.edu; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Georgia",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "29ca71d4ce",
        "title": "Dynamic-Resolution Model Learning for Object Pile Manipulation",
        "site": "https://www.roboticsproceedings.org/rss19/p047.html",
        "author": "Yixuan Wang; Yunzhu Li; Katherine Driggs-Campbell; Li Fei-Fei; Jiajun Wu",
        "abstract": "Dynamics models learned from visual observations have shown to be effective in various robotic manipulation tasks. One of the key questions for learning such dynamics models is what scene representation to use. Prior works typically assume representation at a fixed dimension or resolution, which may be inefficient for simple tasks and ineffective for more complicated tasks. In this work, we investigate how to learn dynamic and adaptive representations at different levels of abstraction to achieve the optimal trade-off between efficiency and effectiveness. Specifically, we construct dynamic-resolution particle representations of the environment and learn a unified dynamics model using graph neural networks (GNNs) that allows continuous selection of the abstraction level. During test time, the agent can adaptively determine the optimal resolution at each model-predictive control (MPC) step. We evaluate our method in object pile manipulation, a task we commonly encounter in cooking, agriculture, manufacturing, and pharmaceutical applications. Through comprehensive evaluations both in the simulation and the real world, we show that our method achieves significantly better performance than state-of-the-art fixed-resolution baselines at the gathering, sorting, and redistribution of granular object piles made with various instances like coffee beans, almonds, corn, etc.",
        "bibtex": "@INPROCEEDINGS{Wang-RSS-23, \r\n    AUTHOR    = {Yixuan Wang AND Yunzhu Li AND Katherine Driggs-Campbell AND Li Fei-Fei AND Jiajun Wu}, \r\n    TITLE     = {{Dynamic-Resolution Model Learning for Object Pile Manipulation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.047} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p047.pdf",
        "supp": "",
        "pdf_size": 8386580,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15785372038608128524&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; Stanford University; Stanford University",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "https://RoboPIL.github.io/dyn-res-pile-manip",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.stanford.edu",
        "aff_unique_abbr": "UIUC;Stanford",
        "aff_campus_unique_index": "0;0;0;1;1",
        "aff_campus_unique": "Urbana-Champaign;Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3a2f90386a",
        "title": "ERASOR2: Instance-Aware Robust 3D Mapping of the Static World in Dynamic Scenes",
        "site": "https://www.roboticsproceedings.org/rss19/p067.html",
        "author": "Hyungtae Lim; Lucas Nunes; Benedikt Mersch; Xieyuanli Chen; Jens Behley; Hyun Myung; Cyrill Stachniss",
        "abstract": "A map of the environment is an essential component for robotic navigation. In the majority of cases, a map of the static part of the world is the basis for localization, planning, and navigation. However, dynamic objects that are presented in the scenes during mapping leave undesirable traces in the map, which can impede mobile robots from achieving successful robotic navigation. To remove the artifacts caused by dynamic objects in the map, we propose a novel instance-aware map building method. Our approach rejects dynamic points at an instance-level while preserving most static points by exploiting instance segmentation estimates. Furthermore, we propose effective ways to consider the erroneous estimates of instance segmentation, enabling our proposed method to be robust even under imprecise instance segmentation. As demonstrated in our experimental evaluation, our approach shows substantial performance increases in terms of both, the preservation of static points and rejection of dynamic points. Our code will be made available on publication.",
        "bibtex": "@INPROCEEDINGS{Lim-RSS-23, \r\n    AUTHOR    = {Hyungtae Lim AND Lucas Nunes AND Benedikt Mersch AND Xieyuanli Chen AND Jens Behley AND Hyun Myung AND Cyrill Stachniss}, \r\n    TITLE     = {{ERASOR2: Instance-Aware Robust 3D Mapping of the Static World in Dynamic Scenes}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.067} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p067.pdf",
        "supp": "",
        "pdf_size": 22251192,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7746710174646160293&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Urban Robotics Lab., School of Electrical Engineering and KI-AI, KAIST, Republic of Korea; Photogrammetry & Robotics Lab, University of Bonn, Germany; Photogrammetry & Robotics Lab, University of Bonn, Germany; Photogrammetry & Robotics Lab, University of Bonn, Germany; Photogrammetry & Robotics Lab, University of Bonn, Germany; Urban Robotics Lab., School of Electrical Engineering and KI-AI, KAIST, Republic of Korea + Department of Engineering Science at the University of Oxford, UK + Lamarr Institute for Machine Learning and Artificial Intelligence, Germany; Photogrammetry & Robotics Lab, University of Bonn, Germany + Department of Engineering Science at the University of Oxford, UK + Lamarr Institute for Machine Learning and Artificial Intelligence, Germany",
        "aff_domain": "kaist.ac.kr;uni-bonn.de;uni-bonn.de;uni-bonn.de;uni-bonn.de;kaist.ac.kr;uni-bonn.de",
        "email": "kaist.ac.kr;uni-bonn.de;uni-bonn.de;uni-bonn.de;uni-bonn.de;kaist.ac.kr;uni-bonn.de",
        "github": "https://github.com/url-kaist/ERASOR2",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1;0+2+3;1+2+3",
        "aff_unique_norm": "KAIST;University of Bonn;University of Oxford;Lamarr Institute for Machine Learning and Artificial Intelligence",
        "aff_unique_dep": "School of Electrical Engineering and KI-AI;Photogrammetry & Robotics Lab;Department of Engineering Science;",
        "aff_unique_url": "https://www.kaist.edu;https://www.uni-bonn.de;https://www.ox.ac.uk;",
        "aff_unique_abbr": "KAIST;;Oxford;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Oxford",
        "aff_country_unique_index": "0;1;1;1;1;0+2+1;1+2+1",
        "aff_country_unique": "South Korea;Germany;United Kingdom"
    },
    {
        "id": "e696e6faf6",
        "title": "Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors",
        "site": "https://www.roboticsproceedings.org/rss19/p102.html",
        "author": "Letian Wang; Jie Liu; Hao Shao; Wenshuo Wang; Ruobing Chen; Yu Liu; Steven L Waslander",
        "abstract": "When autonomous vehicles are deployed on public roads, they will encounter countless and diverse driving situations. Many manually designed driving policies are difficult to scale to the real world. Fortunately, reinforcement learning has shown great success in many tasks by automatic trial and error. However, when it comes to autonomous driving in interactive dense traffic, RL agents either fail to learn reasonable performance or necessitate a large amount of data. Our insight is that when humans learn to drive, they will 1) make decisions over the high-level skill space instead of the low-level control space and 2) leverage expert prior knowledge rather than learning from scratch. Inspired by this, we propose ASAP-RL, an efficient reinforcement learning algorithm for autonomous driving that simultaneously leverages motion skills and expert priors. We first parameterized motion skills, which are diverse enough to cover various complex driving scenarios and situations. A skill parameter inverse recovery method is proposed to convert expert demonstrations from control space to skill space. A simple but effective double initialization technique is proposed to leverage expert priors while bypassing the issue of expert suboptimality and early performance degradation. We validate our proposed method on interactive dense-traffic driving tasks given simple and sparse rewards. Experimental results show that our method can lead to higher learning efficiency and better driving performance relative to previous methods that exploit skills and priors differently. Code is open-sourced to facilitate further research.",
        "bibtex": "@INPROCEEDINGS{Wang-RSS-23, \r\n    AUTHOR    = {Letian Wang AND Jie Liu AND Hao Shao AND Wenshuo Wang AND Ruobing Chen AND Yu Liu AND Steven L Waslander}, \r\n    TITLE     = {{Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.102} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p102.pdf",
        "supp": "",
        "pdf_size": 3352236,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14136133443046679583&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Toronto; Shanghai Artificial Intelligence Laboratory; SenseTime Research; McGill University; SenseTime Research; Shanghai Artificial Intelligence Laboratory; University of Toronto",
        "aff_domain": "mail.utoronto.ca; ; ; ; ;gmail.com; ",
        "email": "mail.utoronto.ca; ; ; ; ;gmail.com; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;2;1;0",
        "aff_unique_norm": "University of Toronto;Shanghai Artificial Intelligence Laboratory;SenseTime;McGill University",
        "aff_unique_dep": ";;SenseTime Research;",
        "aff_unique_url": "https://www.utoronto.ca;http://www.shailab.org/;https://www.sensetime.com;https://www.mcgill.ca",
        "aff_unique_abbr": "U of T;Shanghai AI Lab;SenseTime;McGill",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;1;1;0",
        "aff_country_unique": "Canada;China"
    },
    {
        "id": "1810399acc",
        "title": "Efficient volumetric mapping of multi-scale environments using wavelet-based compression",
        "site": "https://www.roboticsproceedings.org/rss19/p065.html",
        "author": "Victor Reijgwart; Cesar Cadena; Roland Siegwart; Lionel Ott",
        "abstract": "Volumetric maps are widely used in robotics due to their desirable properties in applications such as path planning, exploration, and manipulation. Constant advances in mapping technologies are needed to keep up with the improvements in sensor technology, generating increasingly vast amounts of precise measurements. Handling this data in a computationally and memory-efficient manner is paramount to representing the environment at the desired scales and resolutions. In this work, we express the desirable properties of a volumetric mapping framework through the lens of multi-resolution analysis. This shows that wavelets are a natural foundation for hierarchical and multi-resolution volumetric mapping. Based on this insight we design an efficient mapping system that uses wavelet decomposition. The efficiency of the system enables the use of uncertainty-aware sensor models, improving the quality of the maps. Experiments on both synthetic and real-world data provide mapping accuracy and runtime performance comparisons with state-of-the-art methods on both RGB-D and 3D LiDAR data. The framework is open-sourced to allow the robotics community at large to explore this approach.",
        "bibtex": "@INPROCEEDINGS{Reijgwart-RSS-23, \r\n    AUTHOR    = {Victor Reijgwart AND Cesar Cadena AND Roland Siegwart AND Lionel Ott}, \r\n    TITLE     = {{Efficient volumetric mapping of multi-scale environments using wavelet-based compression}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.065} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p065.pdf",
        "supp": "",
        "pdf_size": 4286965,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13711918124470859333&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4256117f76",
        "title": "Enabling Team of Teams: A Trust Inference and Propagation (TIP) Model in Multi-Human Multi-Robot Teams",
        "site": "https://www.roboticsproceedings.org/rss19/p003.html",
        "author": "Yaohui Guo; X. Jessie Yang; Cong Shi",
        "abstract": "Trust has been identified as a central factor for effective human-robot teaming. Existing literature on trust modeling predominantly focuses on dyadic human-autonomy teams where one human agent interacts with one robot. There is little, if not no, research on trust modeling in teams consisting of multiple human agents and multiple robotic agents.\r\n\r\nTo fill this research gap, we present the trust inference and propagation (TIP) model for trust modeling in multi-human multi-robot teams. In a multi-human multi-robot team, we postulate that there exist two types of experiences that a human agent has with a robot: direct and indirect experiences. The TIP model presents a novel mathematical framework that explicitly accounts for both types of experiences. To evaluate the model, we conducted a human-subject experiment with 15 pairs of participants (${N=30}$). Each pair performed a search and detection task with two drones. Results show that our TIP model successfully captured the underlying trust dynamics and significantly outperformed a baseline model. To the best of our knowledge, the TIP model is the first mathematical framework for computational trust modeling in multi-human multi-robot teams.",
        "bibtex": "@INPROCEEDINGS{Guo-RSS-23, \r\n    AUTHOR    = {Yaohui Guo AND X. Jessie Yang AND Cong Shi}, \r\n    TITLE     = {{Enabling Team of Teams: A Trust Inference and Propagation (TIP) Model in Multi-Human Multi-Robot Teams}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.003} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p003.pdf",
        "supp": "",
        "pdf_size": 949775,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Industrial and Operations Engineering, University of Michigan; Industrial and Operations Engineering, University of Michigan; Industrial and Operations Engineering, University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Industrial and Operations Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f2668240f9",
        "title": "Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement",
        "site": "https://www.roboticsproceedings.org/rss19/p030.html",
        "author": "Nikolaos Gkanatsios; Ayush Jain; Zhou Xian; Yunchu Zhang; Christopher G Atkeson; Katerina Fragkiadaki",
        "abstract": "Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene-rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. \r\nWe propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then re-locate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model can execute highly compositional instructions zero-shot in simulation and in the real world. It outperforms language-to-action reactive policies and Large Language Model planners by a large margin, especially for long instructions that involve compositions of multiple spatial concepts. Simulation and real-world robot execution videos, as well as our code and datasets are publicly available on our website: https://ebmplanner.github.io.",
        "bibtex": "@INPROCEEDINGS{Gkanatsios-RSS-23, \r\n    AUTHOR    = {Nikolaos Gkanatsios AND Ayush Jain AND Zhou Xian AND Yunchu Zhang AND Christopher G Atkeson AND Katerina Fragkiadaki}, \r\n    TITLE     = {{Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.030} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p030.pdf",
        "supp": "",
        "pdf_size": 5403615,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3704571982902552076&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;cs.cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "https://ebmplanner.github.io",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "98cfb6d12c",
        "title": "Fast Monocular Visual-Inertial Initialization Leveraging Learned Single-View Depth",
        "site": "https://www.roboticsproceedings.org/rss19/p072.html",
        "author": "Nathaniel  W Merrill; Patrick Geneva; Saimouli Katragadda; Chuchu Chen; Guoquan  Huang",
        "abstract": "In monocular visual-inertial navigation systems, it is\r\nideal to initialize as quickly and robustly as possible. State-of-the-\r\nart initialization methods typically make linear approximations\r\nusing the image features and inertial information in order\r\nto initialize in closed-form, and then refine the states with a\r\nnonlinear optimization. While the standard methods typically\r\nwait for a 2sec data window, a recent work has shown that it\r\nis possible to initialize faster (0.5sec) by adding constraints from\r\na robust but only up-to-scale monocular depth network in the\r\nnonlinear optimization. To further expedite the initialization, in\r\nthis work, we leverage the scale-less depth measurements instead\r\nin the linear initialization step that is performed prior to the\r\nnonlinear one, which only requires a single depth image for\r\nthe first frame. We show that the typical estimation of each\r\nfeature state independently in the closed-form solution can be\r\nreplaced by just estimating the scale and offset parameters of\r\nthe learned depth map. Interestingly, our formulation makes it\r\npossible to construct small minimal problems in a RANSAC loop,\r\nwhereas the typical linear system\u2019s minimal problem is quite\r\nlarge and includes every feature state. Experiments show that\r\nour method can improve the overall initialization performance on\r\npopular public datasets (EuRoC MAV and TUM-VI) over state-\r\nof-the-art methods. For the TUM-VI dataset, we show superior\r\ninitialization performance with only a 300ms window of data,\r\nwhich is the smallest ever reported, and show that our method\r\ncan initialize more often, robustly, and accurately in different\r\nchallenging scenarios.",
        "bibtex": "@INPROCEEDINGS{Merrill-RSS-23, \r\n    AUTHOR    = {Nathaniel  W Merrill AND Patrick Geneva AND Saimouli Katragadda AND Chuchu Chen AND Guoquan  Huang}, \r\n    TITLE     = {{Fast Monocular Visual-Inertial Initialization Leveraging Learned Single-View Depth}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.072} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p072.pdf",
        "supp": "",
        "pdf_size": 748104,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15091774753348299924&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Delaware; University of Delaware; University of Delaware; University of Delaware; University of Delaware",
        "aff_domain": "udel.edu;udel.edu;udel.edu;udel.edu;udel.edu",
        "email": "udel.edu;udel.edu;udel.edu;udel.edu;udel.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Delaware",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.udel.edu",
        "aff_unique_abbr": "UD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9dc2e539ad",
        "title": "Fast Traversability Estimation for Wild Visual Navigation",
        "site": "https://www.roboticsproceedings.org/rss19/p054.html",
        "author": "Jonas Frey; Matias Mattamala; Nived Chebrolu; Cesar Cadena; Maurice Fallon; Marco Hutter",
        "abstract": "Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we propose Wild Visual Navigation (WVN), an online self-supervised learning system for traversability estimation which uses only vision. The system is able to continuously adapt from a short human demonstration in the field. It leverages high-dimensional features from self-supervised visual transformer models, with an online scheme for supervision generation that runs in real-time on the robot. We demonstrate the advantages of our approach with experiments and ablation studies in challenging environments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex outdoor terrains - negotiating obstacles in high grass as well as a 1.4 km footpath following. While our experiments were executed with a quadruped robot, ANYmal, the approach presented can generalize to any ground robot. Project page: https://bit.ly/3M6nMHH",
        "bibtex": "@INPROCEEDINGS{Frey-RSS-23, \r\n    AUTHOR    = {Jonas Frey AND Matias Mattamala AND Nived Chebrolu AND Cesar Cadena AND Maurice Fallon AND Marco Hutter}, \r\n    TITLE     = {{Fast Traversability Estimation for Wild Visual Navigation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.054} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p054.pdf",
        "supp": "",
        "pdf_size": 8249394,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17752136971242033813&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "ETH Zurich + Max Planck Institute for Intelligent Systems; University of Oxford + Max Planck Institute for Intelligent Systems; University of Oxford; ETH Zurich; University of Oxford; ETH Zurich",
        "aff_domain": "ethz.ch;robots.ox.ac.uk; ;ethz.ch;eng.ox.ac.uk;ethz.ch",
        "email": "ethz.ch;robots.ox.ac.uk; ;ethz.ch;eng.ox.ac.uk;ethz.ch",
        "github": "",
        "project": "bit.ly/3M6nMHH",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+1;2;0;2;0",
        "aff_unique_norm": "ETH Zurich;Max Planck Institute for Intelligent Systems;University of Oxford",
        "aff_unique_dep": ";Intelligent Systems;",
        "aff_unique_url": "https://www.ethz.ch;https://www.mpi-is.mpg.de;https://www.ox.ac.uk",
        "aff_unique_abbr": "ETHZ;MPI-IS;Oxford",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;2+1;2;0;2;0",
        "aff_country_unique": "Switzerland;Germany;United Kingdom"
    },
    {
        "id": "9f6ed8971a",
        "title": "Few-shot Adaptation for Manipulating Granular Materials Under Domain Shift",
        "site": "https://www.roboticsproceedings.org/rss19/p048.html",
        "author": "Yifan Zhu; Pranay Thangeda; Melkior Ornik; Kris Hauser",
        "abstract": "Autonomous lander missions on extraterrestrial bodies will need to sample granular material while coping with domain shift, no matter how well a sampling strategy is tuned on Earth. This paper proposes an adaptive scooping strategy that uses deep Gaussian process method trained with meta-learning to learn on-line from very limited experience on the target terrains. It introduces a novel meta-training approach, Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa), that explicitly trains the deep kernel to predict scooping volume robustly under large domain shifts. Employed in a Bayesian Optimization sequential decision-making framework, the proposed method allows the robot to use vision and very little on-line experience to achieve high-quality scooping actions on out-of-distribution terrains, significantly outperforming non-adaptive methods proposed in the excavation literature as well as other state-of-the-art meta-learning methods. Moreover, a dataset of 6,700 executed scoops collected on a diverse set of materials, terrain topography, and compositions is made available for future research in granular material manipulation and meta-learning.",
        "bibtex": "@INPROCEEDINGS{Zhu-RSS-23, \r\n    AUTHOR    = {Yifan Zhu AND Pranay Thangeda AND Melkior Ornik AND Kris Hauser}, \r\n    TITLE     = {{Few-shot Adaptation for Manipulating Granular Materials Under Domain Shift}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.048} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p048.pdf",
        "supp": "",
        "pdf_size": 16805721,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15075827798803485668&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6f0c15cbb8",
        "title": "Follow my Advice: Assume-Guarantee Approach to Task Planning with Human in the Loop",
        "site": "https://www.roboticsproceedings.org/rss19/p001.html",
        "author": "Georg Schuppe; Ilaria Torre; Iolanda Leite; Jana Tumova",
        "abstract": "We focus on correct-by-design robot task planning from finite Linear Temporal Logic (LTLf) specifications with a human in the loop. Since provable guarantees are difficult to obtain unconditionally, we take an assume-guarantee perspective. Along with guarantees on the robot's task satisfaction, we compute the weakest sufficient assumptions on the human's behavior. We approach the problem via a stochastic game and leverage algorithmic synthesis of the weakest sufficient assumptions. We turn the assumptions into runtime advice to be communicated to the human. We conducted an online user study and showed that the robot is perceived as safer, more intelligent and more compliant with our approach than a robot giving more frequent advice corresponding to stronger assumptions. In addition, we show that our approach leads to less violations of the specification than not communicating with the participant at all.",
        "bibtex": "@INPROCEEDINGS{Schuppe-RSS-23, \r\n    AUTHOR    = {Georg Schuppe AND Ilaria Torre AND Iolanda Leite AND Jana Tumova}, \r\n    TITLE     = {{Follow my Advice: Assume-Guarantee Approach to Task Planning with Human in the Loop}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.001} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p001.pdf",
        "supp": "",
        "pdf_size": 401815,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:sJzZoJj-jY4J:scholar.google.com/&scioq=Follow+my+Advice:+Assume-Guarantee+Approach+to+Task+Planning+with+Human+in+the+Loop&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f2a7ce838d",
        "title": "FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation",
        "site": "https://www.roboticsproceedings.org/rss19/p041.html",
        "author": "Minho Heo; Youngwoon Lee; Doohyun Lee; Joseph J Lim",
        "abstract": "Reinforcement learning (RL), imitation learning (IL), and task and motion planning (TAMP) have demonstrated impressive performance across various robotic manipulation tasks. However, these approaches have been limited to learning simple behaviors in current real-world manipulation benchmarks, such as pushing or pick-and-place. To enable more complex, long-horizon behaviors of an autonomous robot, we propose to focus on real-world furniture assembly, a complex, long-horizon robot manipulation task that requires addressing many current robotic manipulation challenges to solve. We present FurnitureBench, a reproducible real-world furniture assembly benchmark aimed at providing a low barrier for entry and being easily reproducible, so that researchers across the world can reliably test their algorithms and compare them against prior work. For ease of use, we provide 200+ hours of pre-collected data (5000+ demonstrations), 3D printable furniture models, a robotic environment setup guide, and systematic task initialization. Furthermore, we provide FurnitureSim, a fast and realistic simulator of FurnitureBench. We benchmark the performance of offline RL and IL algorithms on our assembly tasks and demonstrate the need to improve such algorithms to be able to solve our tasks in the real world, providing ample opportunities for future research.",
        "bibtex": "@INPROCEEDINGS{Heo-RSS-23, \r\n    AUTHOR    = {Minho Heo AND Youngwoon Lee AND Doohyun Lee AND Joseph J Lim}, \r\n    TITLE     = {{FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.041} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p041.pdf",
        "supp": "",
        "pdf_size": 10142170,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2749249325086651253&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "KAIST; UC Berkeley; KAIST; KAIST",
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "github": "",
        "project": "https://clvrai.com/furniture-bench",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.berkeley.edu",
        "aff_unique_abbr": "KAIST;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "815f2d7d25",
        "title": "G*: A New Approach to Bounding Curvature Constrained Shortest Paths through Dubins Gates",
        "site": "https://www.roboticsproceedings.org/rss19/p059.html",
        "author": "Satyanarayana  Gupta Manyam; Abhishek Nayak; Sivakumar  Rathinam",
        "abstract": "We consider a Curvature-constrained Shortest Path (CSP) problem on a 2D plane for a robot with minimum turning radius constraints in the presence of obstacles. We introduce a new bounding technique called Gate* (G*) that provides optimality guarantees to the CSP. Our approach relies on relaxing the obstacle avoidance constraints but allows a path to travel through some restricted sets of configurations called gates which are informed by the obstacles. We also let the path to be discontinuous when it reaches a gate. This approach allows us to pose the bounding problem as a least-cost problem in a graph where the cost of traveling an edge requires us to solve a new motion planning problem called the Dubins gate problem. In addition to the theoretical results, our numerical tests show that G* can significantly improve the lower bounds with respect to the baseline approaches, and by more than 60% in some instances.",
        "bibtex": "@INPROCEEDINGS{Manyam-RSS-23, \r\n    AUTHOR    = {Satyanarayana  Gupta Manyam AND Abhishek Nayak AND Sivakumar  Rathinam}, \r\n    TITLE     = {{G*: A New Approach to Bounding Curvature Constrained Shortest Paths through Dubins Gates}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.059} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p059.pdf",
        "supp": "",
        "pdf_size": 1714583,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Infoscitex Corp.; Texas A&M University; Texas A&M University",
        "aff_domain": "gmail.com;tamu.edu;tamu.edu",
        "email": "gmail.com;tamu.edu;tamu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Infoscitex Corporation;Texas A&M University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.infoscitex.com;https://www.tamu.edu",
        "aff_unique_abbr": "Infoscitex;TAMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "395a9145cf",
        "title": "Gait design for limbless obstacle aided locomotion using geometric mechanics",
        "site": "https://www.roboticsproceedings.org/rss19/p094.html",
        "author": "Baxi Chong; Tianyu Wang; Daniel Irvine; Velin Kojouharov ; Bo Lin; Howie Choset; Daniel Goldman; Grigoriy Blekherman",
        "abstract": "Limbless robots have the potential to maneuver through cluttered environments that conventional robots cannot traverse. As illustrated in their biological counterparts such as snakes and nematodes, limbless locomotors can benefit from interactions with obstacles, yet such obstacle-aided locomotion (OAL) requires properly coordinated high-level self-deformation patterns (gait templates) as well as low-level body adaptation to environments. Most prior work on OAL utilized stereotyped traveling-wave gait templates and relied on local body deformations (e.g., passive body mechanics or decentralized controller parameter adaptation based on force feedback) for obstacle navigation, while gait template design for OAL remains less studied. In this paper, we explore novel gait templates for OAL based on tools derived from geometric mechanics (GM), which thus far has been limited to homogeneous environments. Here, we expand the scope of GM to obstacle-rich environments. Specifically, we establish a model that maps the presence of an obstacle to directional constraints in optimization. In doing so, we identify novel gait templates suitable for sparsely and densely distributed obstacle-rich environments respectively. Open-loop robophysical experiments verify the effectiveness of our identified OAL gaits in obstacle-rich environments. We posit that when such OAL gait templates are augmented with appropriate sensing and feedback controls, limbless locomotors will gain robust function in obstacle rich environments.",
        "bibtex": "@INPROCEEDINGS{Chong-RSS-23, \r\n    AUTHOR    = {Baxi Chong AND Tianyu Wang AND Daniel Irvine AND Velin Kojouharov  AND Bo Lin AND Howie Choset AND Daniel Goldman AND Grigoriy Blekherman}, \r\n    TITLE     = {{Gait design for limbless obstacle aided locomotion using geometric mechanics}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.094} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p094.pdf",
        "supp": "",
        "pdf_size": 2918477,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6771501534653972710&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "7090d0d804",
        "title": "GenAug: Retargeting behaviors to unseen situations via Generative Augmentation",
        "site": "https://www.roboticsproceedings.org/rss19/p010.html",
        "author": "Qiuyu Chen; Shosuke C Kiami; Abhishek Gupta; Vikash Kumar",
        "abstract": "Robot learning methods have the potential for widespread generalization across tasks, environments, and objects. However, these methods are severely limited by the amount of data that they are provided or are able to collect. Robots in the real world are likely to only be able to collect a small dataset, both in terms of data quantity and diversity. For robot learning to generalize, we must be able to leverage sources of data or priors beyond the robot\u2019s own experience. In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data, can serve as such a source of data. We show that despite these generative models being trained on largely non-robotics data, they can serve as effective ways to impart priors into the process of robot learning in the\r\nreal world in a way that enables widespread generalization. In particular, we show how pre-trained generative models for in- painting can serve as effective tools for semantically meaningful data augmentation. By leveraging these pre-trained models for generating appropriate \u201cfunctional\u201d data augmentations, we\r\npropose a system GenAug that is able to significantly improve policy generalization. We apply GenAug to tabletop manipulation tasks, showing the ability to retarget behavior to novel scenarios, while only requiring marginal amounts of real-world data. We demonstrate the efficacy of this system on a number of object manipulation problems in the real world, showing a 40% improvement in generalization to novel scenes and objects.",
        "bibtex": "@INPROCEEDINGS{Chen-RSS-23, \r\n    AUTHOR    = {Qiuyu Chen AND Shosuke C Kiami AND Abhishek Gupta AND Vikash Kumar}, \r\n    TITLE     = {{GenAug: Retargeting behaviors to unseen situations via Generative Augmentation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.010} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p010.pdf",
        "supp": "",
        "pdf_size": 48268417,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12999894384742418986&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Washington; University of Washington; University of Washington; Meta AI",
        "aff_domain": "cs.washington.edu;cs.washington.edu;cs.washington.edu;meta.com",
        "email": "cs.washington.edu;cs.washington.edu;cs.washington.edu;meta.com",
        "github": "",
        "project": "genaug.github.io",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Washington;Meta",
        "aff_unique_dep": ";Meta AI",
        "aff_unique_url": "https://www.washington.edu;https://meta.com",
        "aff_unique_abbr": "UW;Meta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a72898d492",
        "title": "Goal-Conditioned Imitation Learning using Score-based Diffusion Policies",
        "site": "https://www.roboticsproceedings.org/rss19/p028.html",
        "author": "Moritz Reuss; Maximilian Li; Xiaogang Jia; Rudolf Lioutikov",
        "abstract": "We propose a new policy representation based on score-based diffusion models (SDMs). We apply our new policy representation in the domain of Goal-Conditioned Imitation Learning (GCIL) to learn general-purpose goal-specified policies from large uncurated datasets without rewards. Our new goal-conditioned policy architecture \"BEhavior generation with ScOre-based Diffusion Policies\" (BESO) leverages a generative, score-based diffusion model as its policy. BESO decouples the learning of the score model from the inference sampling process, and, hence allows for fast sampling strategies to generate goal-specified behavior in just 3 inference steps, compared to 30+ inference steps of other diffusion based policies. Furthermore, BESO is highly expressive and can effectively capture multi-modality present in the solution space of the play data. Unlike previous methods such as Latent Plans or C-Bet, BESO does not rely on complex hierarchical policies or additional clustering for effective goal-conditioned behavior learning. Finally, we show how BESO can even be used to learn a goal-independent policy from play-data using classifier-free guidance. To the best of our knowledge this is the first work that a) represents a behavior policy based on such a decoupled SDM b) learns an SDM based policy in the domain of GCIL and c) provides a way to simultaneously learn a goal-dependent and a goal-independent policy from play-data. We evaluate BESO through detailed simulation and show that it consistently outperforms several state-of-the-art goal-conditioned imitation learning methods on challenging benchmarks. We additionally provide extensive ablation studies and experiments to demonstrate the effectiveness of our method for goal-conditioned behavior generation. Demonstrations and Code are available at https://intuitive-robots.github.io/beso-website.",
        "bibtex": "@INPROCEEDINGS{Reuss-RSS-23, \r\n    AUTHOR    = {Moritz Reuss AND Maximilian Li AND Xiaogang Jia AND Rudolf Lioutikov}, \r\n    TITLE     = {{Goal-Conditioned Imitation Learning using Score-based Diffusion Policies}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.028} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p028.pdf",
        "supp": "",
        "pdf_size": 3511550,
        "gs_citation": 175,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8969442101952326635&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0d484ebd7c",
        "title": "GranularGym: High Performance Simulation for Robotic Tasks with Granular Materials",
        "site": "https://www.roboticsproceedings.org/rss19/p034.html",
        "author": "David R Millard; Daniel Pastor; Joseph Bowkett; Paul Backes; Gaurav S Sukhatme",
        "abstract": "Granular materials are of critical interest to many robotic tasks in planetary science, construction, and manufacturing. However, the dynamics of granular materials are complex and often computationally very expensive to simulate. We propose a set of methodologies and a system for the fast simulation of granular materials on Graphics Processing Units (GPUs), and show that this simulation is fast enough for basic training with Reinforcement Learning algorithms, which currently require many dynamics samples to achieve acceptable performance. Our method models granular material dynamics using implicit timestepping methods for multibody rigid contacts, as well as algorithmic techniques for efficient parallel collision detection between pairs of particles and between particle and arbitrarily shaped rigid bodies, and programming techniques for minimizing warp divergence on Single-Instruction, Multiple-Thread (SIMT) chip architectures. We showcase our simulation system on several environments targeted toward robotic tasks, and release our simulator as an open-source tool.",
        "bibtex": "@INPROCEEDINGS{Millard-RSS-23, \r\n    AUTHOR    = {David R Millard AND Daniel Pastor AND Joseph Bowkett AND Paul Backes AND Gaurav S Sukhatme}, \r\n    TITLE     = {{GranularGym: High Performance Simulation for Robotic Tasks with Granular Materials}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.034} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p034.pdf",
        "supp": "",
        "pdf_size": 881761,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17929150264021726964&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Southern California, Los Angeles, USA; Jet Propulsion Lab, Pasadena, USA; Jet Propulsion Lab, Pasadena, USA; Jet Propulsion Lab, Pasadena, USA; University of Southern California, Los Angeles, USA",
        "aff_domain": "usc.edu; ; ; ;usc.edu",
        "email": "usc.edu; ; ; ;usc.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of Southern California;Jet Propulsion Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.usc.edu;https://www.jpl.nasa.gov",
        "aff_unique_abbr": "USC;JPL",
        "aff_campus_unique_index": "0;1;1;1;0",
        "aff_campus_unique": "Los Angeles;Pasadena",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3186d988ed",
        "title": "Graph Attention Multi-Agent Fleet Autonomy for Advanced Air Mobility",
        "site": "https://www.roboticsproceedings.org/rss19/p105.html",
        "author": "Malintha Fernando; Ransalu Senanayake; Heeyoul Choi; Martin Swany",
        "abstract": "Autonomous mobility is emerging as a new disruptive mode of urban transportation for moving cargo and passengers. \r\nHowever, designing scalable autonomous fleet coordination schemes to accommodate fast-growing mobility systems is challenging primarily due to the increasing heterogeneity of the fleets, time-varying demand patterns, service area expansions, and communication limitations. We introduce the concept of partially observable advanced air mobility games to coordinate a fleet of aerial vehicles by accounting for the heterogeneity of the interacting agents and the self-interested nature inherent to commercial mobility fleets. To model the complex interactions among the agents and the observation uncertainty in the mobility networks, we propose a novel heterogeneous graph attention encoder-decoder (HetGAT Enc-Dec) neural network-based stochastic policy. We train the policy by leveraging deep multi-agent reinforcement learning, allowing decentralized decision-making for the agents using their local observations. Through extensive experimentation, we show that the learned policy generalizes to various fleet compositions, demand patterns, and observation topologies. Further, fleets operating under the HetGAT Enc-Dec policy outperform other state-of-the-art graph neural network policies by achieving the highest fleet reward and fulfillment ratios in on-demand mobility networks.",
        "bibtex": "@INPROCEEDINGS{Fernando-RSS-23, \r\n    AUTHOR    = {Malintha Fernando AND Ransalu Senanayake AND Heeyoul Choi AND Martin Swany}, \r\n    TITLE     = {{Graph Attention Multi-Agent Fleet Autonomy for Advanced Air Mobility}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.105} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p105.pdf",
        "supp": "",
        "pdf_size": 3627856,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10096291208518809252&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Luddy School of Informatics, Computing, and Engineering at Indiana University, Bloomington, IN, 47401, USA + Stanford University, CA, 94305, USA; Stanford University, CA, 94305, USA; Luddy School of Informatics, Computing, and Engineering at Indiana University, Bloomington, IN, 47401, USA; Luddy School of Informatics, Computing, and Engineering at Indiana University, Bloomington, IN, 47401, USA",
        "aff_domain": "iu.edu;stanford.edu;handong.edu;iu.edu",
        "email": "iu.edu;stanford.edu;handong.edu;iu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0;0",
        "aff_unique_norm": "Indiana University;Stanford University",
        "aff_unique_dep": "Luddy School of Informatics, Computing, and Engineering;",
        "aff_unique_url": "https://www.indiana.edu;https://www.stanford.edu",
        "aff_unique_abbr": "IU;Stanford",
        "aff_campus_unique_index": "0+1;1;0;0",
        "aff_campus_unique": "Bloomington;Stanford",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0c0106f6b5",
        "title": "HDVIO: Improving Localization and Disturbance Estimation with Hybrid Dynamics VIO",
        "site": "https://www.roboticsproceedings.org/rss19/p071.html",
        "author": "Giovanni Cioffi; Leonard Bauersfeld; Davide Scaramuzza",
        "abstract": "Visual-inertial odometry (VIO) is the most common approach for estimating the state of autonomous micro aerial vehicles using only onboard sensors. Existing methods improve VIO performance by including a dynamics model in the estimation pipeline. However, such methods degrade in the presence of low-fidelity vehicle models and continuous external disturbances, such as wind. Our proposed method, HDVIO, overcomes these limitations by using a hybrid dynamics model that combines a point-mass vehicle model with a learning-based component that captures complex aerodynamic effects. HDVIO estimates the external force and the full robot state by leveraging the discrepancy between the actual motion and the predicted motion of the hybrid dynamics model. Our hybrid dynamics model uses a history of thrust and IMU measurements to predict the vehicle dynamics. To demonstrate the performance of our method, we present results on both public and novel drone dynamics datasets and show real-world experiments of a quadrotor flying in strong winds up to 25 km/h. The results show that our approach improves the motion and external force estimation compared to the state-of-the-art by up to 33% and 40%, respectively. Furthermore, differently from existing methods, we show that it is possible to predict the vehicle dynamics accurately while having no explicit knowledge of its full state.",
        "bibtex": "@INPROCEEDINGS{Cioffi-RSS-23, \r\n    AUTHOR    = {Giovanni Cioffi AND Leonard Bauersfeld AND Davide Scaramuzza}, \r\n    TITLE     = {{HDVIO: Improving Localization and Disturbance Estimation with Hybrid Dynamics VIO}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.071} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p071.pdf",
        "supp": "",
        "pdf_size": 2067671,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9155314315703837395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8bbfe7f88e",
        "title": "Hindsight States: Blending Sim & Real Task Elements for Efficient Reinforcement Learning",
        "site": "https://www.roboticsproceedings.org/rss19/p038.html",
        "author": "Simon Guist; Jan Schneider; Vincent Berenz; Alexander Dittrich; Bernhard Sch\u00f6lkopf; Dieter B\u00fcchler",
        "abstract": "Reinforcement learning has shown great potential in solving complex tasks when large amounts of data can be generated with little effort. In robotics, one approach to generate training data builds on simulations or models. However, for many tasks, such as with complex soft robots, devising such models is substantially more challenging. Recent successes in soft robotics indicate that employing complex robots can lead to performance boosts. Here, we leverage the imbalance in complexity of the dynamics to learn more sample-efficiently. We (i) abstract the task into distinct components, (ii) off-load the simple dynamics parts into the simulation, and (iii) multiply these virtual parts to generate more data in hindsight. Our new method, Hindsight States (HiS), uses this data and selects the most useful transitions for training. It can be used with an arbitrary off-policy algorithm. \r\nWe validate our method on several challenging simulated tasks and demonstrate that it improves learning both on its own and when combined with an existing hindsight algorithm, Hindsight Experience Replay (HER). Finally, we evaluate HiS on a physical system and show that it boosts performance on a complex table tennis task with a muscular robot.",
        "bibtex": "@INPROCEEDINGS{Guist-RSS-23, \r\n    AUTHOR    = {Simon Guist AND Jan Schneider AND Vincent Berenz AND Alexander Dittrich AND Bernhard Sch\u00f6lkopf AND Dieter B\u00fcchler}, \r\n    TITLE     = {{Hindsight States: Blending Sim & Real Task Elements for Efficient Reinforcement Learning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.038} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p038.pdf",
        "supp": "",
        "pdf_size": 1790373,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12733159459329217115&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "MPI for Intelligent Systems; MPI for Intelligent Systems; MPI for Intelligent Systems; MPI for Intelligent Systems; MPI for Intelligent Systems; MPI for Intelligent Systems",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "webdav.tuebingen.mpg.de/his/",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "23e51172c9",
        "title": "How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers",
        "site": "https://www.roboticsproceedings.org/rss19/p075.html",
        "author": "Junting Chen; Guohao Li; Suryansh Kumar; Bernard Ghanem; Fisher Yu",
        "abstract": "Object goal navigation is an important problem in Embodied AI that involves guiding the agent to navigate to an instance of the object category in an unknown environment---typically an indoor scene. Unfortunately, current state-of-the-art methods for this problem rely heavily on data-driven approaches, e.g., end-to-end reinforcement learning, imitation learning, and others. Moreover, such methods are typically costly to train and difficult to debug, leading to a lack of transferability and explainability. Inspired by recent successes in combining classical and learning methods, we present a modular and training-free solution, which embraces more classic approaches, to tackle the object goal navigation problem. Our method builds a structured scene representation based on the classic visual simultaneous localization and mapping (V-SLAM) framework. We then inject semantics into geometric-based frontier exploration to reason about promising areas to search for a goal object. Our structured scene representation comprises a 2D occupancy map, semantic point cloud, and spatial scene graph. Our method propagates semantics on the scene graphs based on language priors and scene statistics to introduce semantic knowledge to the geometric frontiers. With injected semantic priors, the agent can reason about the most promising frontier to explore. The proposed pipeline shows strong experimental performance for object goal navigation on the Gibson benchmark dataset, outperforming the previous state-of-the-art. We also perform comprehensive ablation studies to identify the current bottleneck in the object navigation task.",
        "bibtex": "@INPROCEEDINGS{Chen-RSS-23, \r\n    AUTHOR    = {Junting Chen AND Guohao Li AND Suryansh Kumar AND Bernard Ghanem AND Fisher Yu}, \r\n    TITLE     = {{How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.075} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p075.pdf",
        "supp": "",
        "pdf_size": 1610469,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3056764070401364581&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "ETH Z\u00fcrich; King Abdullah University of Science and Technology (KAUST); ETH Z\u00fcrich; King Abdullah University of Science and Technology (KAUST); ETH Z\u00fcrich",
        "aff_domain": "gmail.com; ; ; ; ",
        "email": "gmail.com; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "ETH Zurich;King Abdullah University of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.kaust.edu.sa",
        "aff_unique_abbr": "ETHZ;KAUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "Switzerland;Saudi Arabia"
    },
    {
        "id": "c830e2779e",
        "title": "Incremental Nonlinear Dynamic Inversion based Optical Flow Control for Flying Robots: An Efficient Data-driven Approach",
        "site": "https://www.roboticsproceedings.org/rss19/p081.html",
        "author": "Hann Woei Ho; Ye Zhou",
        "abstract": "This paper presents a novel approach for optical flow control of Micro Air Vehicles (MAVs). The task is challenging due to the nonlinearity of optical flow observables. Our proposed Incremental Nonlinear Dynamic Inversion (INDI) control scheme incorporates an efficient data-driven method to address the nonlinearity. It directly estimates the inverse of the time-varying control effectiveness in real-time, eliminating the need for the constant assumption and avoiding high computation in traditional INDI. This approach effectively handles fast-changing system dynamics commonly encountered in optical flow control, particularly height-dependent changes. We demonstrate the robustness and efficiency of the proposed control scheme in numerical simulations and also real-world flight tests: multiple landings of an MAV on a static and flat surface with various tracking setpoints, hovering and landings on moving and undulating surfaces. Despite being challenged with the presence of noisy optical flow estimates and the lateral and vertical movement of the landing surfaces, the MAV is able to successfully track or land on the surface with an exponential decay of both height and vertical velocity at almost the same time, as desired.",
        "bibtex": "@INPROCEEDINGS{Ho-RSS-23, \r\n    AUTHOR    = {Hann Woei Ho AND Ye Zhou}, \r\n    TITLE     = {{Incremental Nonlinear Dynamic Inversion based Optical Flow Control for Flying Robots: An Efficient Data-driven Approach}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.081} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p081.pdf",
        "supp": "",
        "pdf_size": 1923410,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6020265105295426638&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Aerospace Engineering, Engineering Campus, Universiti Sains Malaysia, Malaysia; Faculty of Aerospace Engineering, Delft University of Technology, The Netherlands",
        "aff_domain": "usm.my;usm.my",
        "email": "usm.my;usm.my",
        "github": "",
        "project": "https://youtu.be/i2xFJNGhfxs",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universiti Sains Malaysia;Delft University of Technology",
        "aff_unique_dep": "School of Aerospace Engineering;Faculty of Aerospace Engineering",
        "aff_unique_url": "https://www.usm.my;https://www.tudelft.nl",
        "aff_unique_abbr": "USM;TU Delft",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Engineering Campus;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Malaysia;Netherlands"
    },
    {
        "id": "a68ceaf994",
        "title": "IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality",
        "site": "https://www.roboticsproceedings.org/rss19/p039.html",
        "author": "Bingjie Tang; Michael A Lin; Iretiayo A Akinola; Ankur Handa; Gaurav S Sukhatme; Fabio Ramos; Dieter Fox; Yashraj S Narang",
        "abstract": "Robotic assembly is a longstanding challenge, requiring contact-rich interaction and high precision and accuracy. Many applications also require adaptivity to diverse parts, poses, and environments, as well as low cycle times. In other areas of robotics, simulation is a powerful tool to develop algorithms, generate datasets, and train agents. However, simulation has had a more limited impact on assembly. We present IndustReal, a set of algorithms, systems, and tools that solve assembly tasks in simulation with reinforcement learning (RL) and successfully achieve policy transfer to the real world. Specifically, we propose 1) simulation-aware policy updates, 2) signed-distance-field rewards, and 3) sampling-based curricula for robotic RL agents. We use these algorithms to enable robots to solve contact-rich pick, place, and insertion tasks in simulation. We then propose 4) a policy-level action integrator to minimize error at policy deployment time. We build and demonstrate a real-world robotic assembly system that uses the trained policies and action integrator to achieve repeatable performance in the real world. Finally, we present hardware and software tools that allow other researchers to reproduce our system and results. For videos and additional details, please see our project website at https://sites.google.com/nvidia.com/industreal.",
        "bibtex": "@INPROCEEDINGS{Tang-RSS-23, \r\n    AUTHOR    = {Bingjie Tang AND Michael A Lin AND Iretiayo A Akinola AND Ankur Handa AND Gaurav S Sukhatme AND Fabio Ramos AND Dieter Fox AND Yashraj S Narang}, \r\n    TITLE     = {{IndustReal: Transferring Contact-Rich Assembly Tasks from Simulation to Reality}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.039} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p039.pdf",
        "supp": "",
        "pdf_size": 9328638,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2330253169214054353&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "University of Southern California; Stanford University; NVIDIA Corporation; NVIDIA Corporation; University of Southern California; University of Sydney; University of Washington; NVIDIA Corporation",
        "aff_domain": "; ; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;2;0;3;4;2",
        "aff_unique_norm": "University of Southern California;Stanford University;NVIDIA;University of Sydney;University of Washington",
        "aff_unique_dep": ";;NVIDIA Corporation;;",
        "aff_unique_url": "https://www.usc.edu;https://www.stanford.edu;https://www.nvidia.com;https://www.sydney.edu.au;https://www.washington.edu",
        "aff_unique_abbr": "USC;Stanford;NVIDIA;USYD;UW",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Los Angeles;Stanford;",
        "aff_country_unique_index": "0;0;0;0;0;1;0;0",
        "aff_country_unique": "United States;Australia"
    },
    {
        "id": "72403d2c77",
        "title": "InstaLoc: One-shot Global Lidar Localisation in Indoor Environments through Instance Learning",
        "site": "https://www.roboticsproceedings.org/rss19/p070.html",
        "author": "Lintong Zhang; Sundara Tejaswi Digumarti; Georgi Tinchev; Maurice Fallon",
        "abstract": "Localization for autonomous robots in prior maps is crucial for their functionality. This paper offers a solution to this problem for indoor environments called InstaLoc, which operates on an individual lidar scan to localize it within a prior map. We draw on inspiration from how humans navigate and position themselves by recognizing the layout of distinctive objects and structures. Mimicking the human approach, InstaLoc identifies and matches object instances in the scene with those from a prior map. As far as we know, this is the first method to use panoptic segmentation directly inferring on 3D lidar scans for indoor localization. InstaLoc operates through two networks based on spatially sparse tensors to directly infer dense 3D lidar point clouds. The first network is a panoptic segmentation network that produces object instances and their semantic classes. The second smaller network produces a descriptor for each object instance. A consensus based matching algorithm then matches the instances to the prior map and estimates a six degrees of freedom (DoF) pose for the input cloud in the prior map. InstaLoc utilizes two efficient networks, requires only one to two hours of training on a mobile GPU, and runs in real-time at 1 Hz. Our method achieves between two and four times more detections when localizing, as compared to baseline methods, and achieves higher precision on these detections.",
        "bibtex": "@INPROCEEDINGS{Zhang-RSS-23, \r\n    AUTHOR    = {Lintong Zhang AND Sundara Tejaswi Digumarti AND Georgi Tinchev AND Maurice Fallon}, \r\n    TITLE     = {{InstaLoc: One-shot Global Lidar Localisation in Indoor Environments through Instance Learning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.070} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p070.pdf",
        "supp": "",
        "pdf_size": 7016733,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11197848772265167716&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "acb47bd078",
        "title": "Integrated Object Deformation and Contact Patch Estimation from Visuo-Tactile Feedback",
        "site": "https://www.roboticsproceedings.org/rss19/p080.html",
        "author": "Mark J Van der Merwe; Youngsun Wi; Dmitry Berenson; Nima Fazeli",
        "abstract": "Reasoning over the interplay between object deformation and force transmission through contact is central to the manipulation of compliant objects. In this paper, we propose Neural Deforming Contact Field (NDCF), a representation that jointly models object deformations and contact patches from visuo-tactile feedback using implicit representations. Representing the object geometry and contact with the environment implicitly allows a single model to predict contact patches of varying complexity. Additionally, learning geometry and contact simultaneously allows us to enforce physical priors, such as ensuring contacts lie on the surface of the object. We propose a neural network architecture to learn a NDCF, and train it using simulated data. We then demonstrate that the learned NDCF transfers directly to the real-world without the need for fine-tuning. We benchmark our proposed approach against a baseline representing geometry and contact patches with point clouds. We find that NDCF performs better on simulated data and in transfer to the real-world. More details and video results can be found at https://www.mmintlab.com/ndcf/.",
        "bibtex": "@INPROCEEDINGS{Merwe-RSS-23, \r\n    AUTHOR    = {Mark J Van der Merwe AND Youngsun Wi AND Dmitry Berenson AND Nima Fazeli}, \r\n    TITLE     = {{Integrated Object Deformation and Contact Patch Estimation from Visuo-Tactile Feedback}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.080} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p080.pdf",
        "supp": "",
        "pdf_size": 5717368,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12263859034670289951&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Michigan, Ann Arbor, MI 48109; University of Michigan, Ann Arbor, MI 48109; University of Michigan, Ann Arbor, MI 48109; University of Michigan, Ann Arbor, MI 48109",
        "aff_domain": "umich.edu;umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "https://www.mmintlab.com/ndcf/",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6ccec2f216",
        "title": "Investigating the Impact of Experience on a User's Ability to Perform Hierarchical Abstraction",
        "site": "https://www.roboticsproceedings.org/rss19/p004.html",
        "author": "Nina M Moorman; Nakul Gopalan; Aman Singh; Erin Botti; Mariah Schrum; Chuxuan Yang; Lakshmi Seelam; Matthew Gombolay",
        "abstract": "The field of Learning from Demonstration enables end-users, who are not robotics experts, to shape robot behavior. However, using human demonstrations to teach robots to solve long-horizon problems by leveraging the hierarchical structure of the task is still an unsolved problem. Prior work has yet to show that human users can provide sufficient demonstrations in novel domains without showing the demonstrators explicit teaching strategies for each domain. In this work, we investigate whether non-expert demonstrators can generalize robot teaching strategies to provide necessary and sufficient demonstrations to robots zero-shot in novel domains. We find that increasing participant experience with providing demonstrations improves their demonstration's degree of sub-task abstraction (p<.001), teaching efficiency (p<.001), and sub-task redundancy (p<.05) in novel domains, allowing generalization in robot teaching. Our findings demonstrate for the first time that non-expert demonstrators can transfer knowledge from a series of training experiences to novel domains without the need for explicit instruction, such that they can provide necessary and sufficient demonstrations when programming robots to complete task and motion planning problems.",
        "bibtex": "@INPROCEEDINGS{Moorman-RSS-23, \r\n    AUTHOR    = {Nina M Moorman AND Nakul Gopalan AND Aman Singh AND Erin Botti AND Mariah Schrum AND Chuxuan Yang AND Lakshmi Seelam AND Matthew Gombolay}, \r\n    TITLE     = {{Investigating the Impact of Experience on a User's Ability to Perform Hierarchical Abstraction}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.004} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p004.pdf",
        "supp": "",
        "pdf_size": 1991460,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14724814162582788115&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e5ff0b06a9",
        "title": "LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning",
        "site": "https://www.roboticsproceedings.org/rss19/p089.html",
        "author": "Kenneth Shaw; Ananye Agarwal; Deepak Pathak",
        "abstract": "Dexterous manipulation has been a long-standing challenge in robotics. While machine learning techniques have shown some promise, results have largely been currently limited to simulation. This can be mostly attributed to the lack of suitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous and anthropomorphic hand for machine learning research. In contrast to previous hands, LEAP Hand has a novel kinematic structure that allows maximal dexterity regardless of finger pose. LEAP Hand is low-cost and can be assembled in 4 hours at a cost of 2000 USD from readily available parts. It is capable of consistently exerting large torques over long durations of time. We show that LEAP Hand can be used to perform several manipulation tasks in the real world---from visual teleoperation to learning from passive video data and sim2real. LEAP Hand significantly outperforms its closest competitor Allegro Hand in all our experiments while being 1/8th of the cost. We release the URDF model, 3D CAD files, tuned simulation environment, and a development platform with useful APIs on our website at https://leap-hand.github.io/ .",
        "bibtex": "@INPROCEEDINGS{Shaw-RSS-23, \r\n    AUTHOR    = {Kenneth Shaw AND Ananye Agarwal AND Deepak Pathak}, \r\n    TITLE     = {{LEAP Hand: Low-Cost, Efficient, and Anthropomorphic Hand for Robot Learning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.089} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p089.pdf",
        "supp": "",
        "pdf_size": 6084698,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16952631117046692524&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ab7d429758",
        "title": "Language-Driven Representation Learning for Robotics",
        "site": "https://www.roboticsproceedings.org/rss19/p032.html",
        "author": "Siddharth Karamcheti; Suraj Nair; Annie S Chen; Thomas Kollar; Chelsea Finn; Dorsa Sadigh; Percy Liang",
        "abstract": "Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite. We then introduce Voltron, a framework for language-driven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems \u2013 a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find that Voltron\u2019s language-driven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.",
        "bibtex": "@INPROCEEDINGS{Karamcheti-RSS-23, \r\n    AUTHOR    = {Siddharth Karamcheti AND Suraj Nair AND Annie S Chen AND Thomas Kollar AND Chelsea Finn AND Dorsa Sadigh AND Percy Liang}, \r\n    TITLE     = {{Language-Driven Representation Learning for Robotics}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.032} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p032.pdf",
        "supp": "",
        "pdf_size": 23226568,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12659072539892457898&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8128a0ef47",
        "title": "Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware",
        "site": "https://www.roboticsproceedings.org/rss19/p016.html",
        "author": "Tony Z. Zhao; Vikash Kumar; Sergey Levine; Chelsea Finn",
        "abstract": "Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: the error of the policy can compound over time, drifting out of the training distribution. To address this challenge, we develop a simple yet novel algorithm Action Chunking with Transformers (ACT) which reduces the effective horizon by predicting actions in chunks. This allows us to learn difficult tasks such as opening a translucent condiment cup and slotting a battery with 80-90% success, with only 10 minutes worth of demonstration data. Project website: https://tonyzhaozh.github.io/aloha/",
        "bibtex": "@INPROCEEDINGS{Zhao-RSS-23, \r\n    AUTHOR    = {Tony Z. Zhao AND Vikash Kumar AND Sergey Levine AND Chelsea Finn}, \r\n    TITLE     = {{Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.016} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p016.pdf",
        "supp": "",
        "pdf_size": 5781160,
        "gs_citation": 674,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1799330372553040473&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Stanford University; Meta; UC Berkeley; Stanford University",
        "aff_domain": "stanford.edu;meta.com;berkeley.edu;stanford.edu",
        "email": "stanford.edu;meta.com;berkeley.edu;stanford.edu",
        "github": "",
        "project": "tonyzhaozh.github.io/aloha",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Stanford University;Meta;University of California, Berkeley",
        "aff_unique_dep": ";Meta Platforms, Inc.;",
        "aff_unique_url": "https://www.stanford.edu;https://meta.com;https://www.berkeley.edu",
        "aff_unique_abbr": "Stanford;Meta;UC Berkeley",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Stanford;;Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d09f7e6b2d",
        "title": "Learning and Adapting Agile Locomotion Skills by Transferring Experience",
        "site": "https://www.roboticsproceedings.org/rss19/p051.html",
        "author": "Laura M Smith; J. Chase Kew; Tianyu Li; Linda Luu; Xue Bin Peng; Sehoon Ha; Jie Tan; Sergey Levine",
        "abstract": "Legged robots have enormous potential in their range of capabilities, from navigating unstructured terrains to high-speed running. However, these capabilities bring with them difficult control problems, and designing controllers for highly agile dynamic motions remains a substantial challenge for roboticists. Reinforcement learning (RL) offers a promising data-driven approach for automatically training such controllers. However, exploration in these high-dimensional, underactuated systems remains a significant hurdle for enabling legged robots to learn performant, naturalistic, and versatile agility skills. We propose a framework for training complex robotic skills by transferring experience from existing controllers to jumpstart learning new tasks. To leverage controllers we can acquire in practice, we design this framework to be flexible in terms of their source---that is, the controllers may have been optimized for a different objective under different dynamics, or may require different knowledge of the surroundings---and thus may be highly suboptimal for the target task. We show that our method enables learning complex agile jumping behaviors, navigating to goal locations while walking on hind legs, and adapting to new environments. We also demonstrate that the agile behaviors learned in this way are graceful and safe enough to deploy in the real world.",
        "bibtex": "@INPROCEEDINGS{Smith-RSS-23, \r\n    AUTHOR    = {Laura M Smith AND J. Chase Kew AND Tianyu Li AND Linda Luu AND Xue Bin Peng AND Sehoon Ha AND Jie Tan AND Sergey Levine}, \r\n    TITLE     = {{Learning and Adapting Agile Locomotion Skills by Transferring Experience}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.051} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p051.pdf",
        "supp": "",
        "pdf_size": 15281820,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4038361930369766278&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Berkeley AI Research, UC Berkeley; Google Research; Georgia Institute of Technology; Simon Fraser University; Google Research; Georgia Institute of Technology; Google Research; Berkeley AI Research, UC Berkeley+Google Research",
        "aff_domain": "berkeley.edu; ; ; ; ; ; ; ",
        "email": "berkeley.edu; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://sites.google.com/berkeley.edu/twirl",
        "author_num": 8,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;1;2;1;0+1",
        "aff_unique_norm": "University of California, Berkeley;Google;Georgia Institute of Technology;Simon Fraser University",
        "aff_unique_dep": "Berkeley AI Research;Google Research;;",
        "aff_unique_url": "https://www.berkeley.edu;https://research.google;https://www.gatech.edu;https://www.sfu.ca",
        "aff_unique_abbr": "UC Berkeley;Google Research;Georgia Tech;SFU",
        "aff_campus_unique_index": "0;1;1;1;0+1",
        "aff_campus_unique": "Berkeley;Mountain View;",
        "aff_country_unique_index": "0;0;0;1;0;0;0;0+0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "c6dc60fe10",
        "title": "Learning-Free Grasping of Unknown Objects Using Hidden Superquadrics",
        "site": "https://www.roboticsproceedings.org/rss19/p042.html",
        "author": "Yuwei Wu; Weixiao Liu; Zhiyang Liu; Gregory S Chirikjian",
        "abstract": "Robotic grasping is an essential and fundamental task and has been studied extensively over the past several decades. Traditional work analyzes physical models of the objects and computes force-closure grasps. Such methods require pre-knowledge of the complete 3D model of an object, which can be hard to obtain. Recently with significant progress in machine learning, data-driven methods have dominated the area. Although impressive improvements have been achieved, those methods require a vast amount of training data and suffer from limited generalizability. In this paper, we propose a novel two-stage approach to predicting and synthesizing grasping poses directly from the point cloud of an object without database knowledge or learning. Firstly, multiple superquadrics are recovered at different positions within the object, representing the local geometric features of the object surface. Subsequently, our algorithm exploits the tri-symmetry feature of superquadrics and synthesizes a list of antipodal grasps from each recovered superquadric. An evaluation model is designed to assess and quantify the quality of each grasp candidate. The grasp candidate with the highest score is then selected as the final grasping pose. We conduct experiments on isolated and packed scenes to corroborate the effectiveness of our method. The results indicate that our method demonstrates competitive performance compared with the state-of-the-art without the need for either a full model or prior training.",
        "bibtex": "@INPROCEEDINGS{Wu-RSS-23, \r\n    AUTHOR    = {Yuwei Wu AND Weixiao Liu AND Zhiyang Liu AND Gregory S Chirikjian}, \r\n    TITLE     = {{Learning-Free Grasping of Unknown Objects Using Hidden Superquadrics}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.042} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p042.pdf",
        "supp": "",
        "pdf_size": 4000280,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11684703949427725849&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "National University of Singapore; National University of Singapore + Johns Hopkins University; National University of Singapore; National University of Singapore",
        "aff_domain": "nus.edu.sg;nus.edu.sg;nus.edu.sg;nus.edu.sg",
        "email": "nus.edu.sg;nus.edu.sg;nus.edu.sg;nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "National University of Singapore;Johns Hopkins University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.jhu.edu",
        "aff_unique_abbr": "NUS;JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "7f26b81665",
        "title": "Local object crop collision network for efficient simulation of non-convex objects in GPU-based simulators",
        "site": "https://www.roboticsproceedings.org/rss19/p033.html",
        "author": "Dongwon Son; Beomjoon Kim",
        "abstract": "Our goal is to develop an efficient contact detection algorithm for large-scale GPU-based simulation of non-convex objects. Current GPU-based simulators such as IsaacGym [16] and Brax [11] must trade-off speed with fidelity, generality, or both when simulating non-convex objects. Their main issue lies in contact detection (CD): existing CD algorithms, such as Gilbert\u2013Johnson\u2013Keerthi (GJK), must trade off their computational speed with accuracy which becomes expensive as the number of collisions among non-convex objects increases. We propose a data-driven approach for CD, whose accuracy depends only on the quality and quantity of offline dataset rather than online computation time. Unlike GJK, our method inherently has a uniform computational flow, which facilitates efficient GPU usage based on advanced compilers such as XLA (Accelerated Linear Algebra) [2]. Further, we offer a data-efficient solution by learning the patterns of colliding local crop object shapes, rather than global object shapes which are harder to learn. We demonstrate our approach improves the efficiency of existing CD methods by a factor of 5-10 for nonconvex objects with comparable accuracy. Using the previous work on contact resolution for a neural-network-based contact detector [23], we integrate our CD algorithm into the open-source GPU-based simulator, Brax, and show that we can improve the efficiency over IsaacGym and generality over standard Brax. We highly recommend the videos of our simulator included in the supplementary materials. (https://sites.google.com/view/locc-rss2023/home)",
        "bibtex": "@INPROCEEDINGS{Son-RSS-23, \r\n    AUTHOR    = {Dongwon Son AND Beomjoon Kim}, \r\n    TITLE     = {{Local object crop collision network for efficient simulation of non-convex objects in GPU-based simulators}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.033} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p033.pdf",
        "supp": "",
        "pdf_size": 20247709,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17389915455684464925&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Graduate School of AI, KAIST; Graduate School of AI, KAIST",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr",
        "github": "",
        "project": "https://sites.google.com/view/locc-rss2023/home",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "KAIST",
        "aff_unique_dep": "Graduate School of AI",
        "aff_unique_url": "https://www.kaist.edu",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "c48813791f",
        "title": "Metric-Free Exploration for Topological Mapping by Task and Motion Imitation in Feature Space",
        "site": "https://www.roboticsproceedings.org/rss19/p099.html",
        "author": "Yuhang He; Irving Fang; Yiming Li; Rushi Bhavesh Shah; Chen Feng",
        "abstract": "We propose DeepExplorer, a simple and lightweight metric-free exploration method for topological mapping of unknown environments. It performs task and motion planning (TAMP) entirely in image feature space. The task planner is a recurrent network using the latest image observation sequence to hallucinate a feature as the next-best exploration goal. The motion planner then utilizes the current and the hallucinated features to generate an action taking the agent towards that goal. Our novel feature hallucination enables imitation learning with deep supervision to jointly train the two planners more efficiently than baseline methods. During exploration, we iteratively call the two planners to predict the next action, and the topological map is built by constantly appending the latest image observation and action to the map and using visual place recognition (VPR) for loop closing. \r\nThe resulting topological map efficiently represents an environment's connectivity and traversability, so it can be used for tasks such as visual navigation. We show DeepExplorer's exploration efficiency and strong sim2sim generalization capability on large-scale simulation datasets like Gibson and MP3D. Its effectiveness is further validated via the image-goal navigation performance on the resulting topological map. We further show its strong zero-shot sim2real generalization capability in real-world experiments. The source code is available at https://ai4ce.github.io/DeepExplorer/.",
        "bibtex": "@INPROCEEDINGS{He-RSS-23, \r\n    AUTHOR    = {Yuhang He AND Irving Fang AND Yiming Li AND Rushi Bhavesh Shah AND Chen Feng}, \r\n    TITLE     = {{Metric-Free Exploration for Topological Mapping by Task and Motion Imitation in Feature Space}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.099} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p099.pdf",
        "supp": "",
        "pdf_size": 6578832,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14765503516083921562&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Oxford, Oxford, United Kingdom; Tandon School of Engineering, New York University, New York, United States; Tandon School of Engineering, New York University, New York, United States; Tandon School of Engineering, New York University, New York, United States; Tandon School of Engineering, New York University, New York, United States",
        "aff_domain": "cs.ox.ac.uk;nyu.edu;nyu.edu;nyu.edu;nyu.edu",
        "email": "cs.ox.ac.uk;nyu.edu;nyu.edu;nyu.edu;nyu.edu",
        "github": "",
        "project": "https://ai4ce.github.io/DeepExplorer/",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "University of Oxford;New York University",
        "aff_unique_dep": "Department of Computer Science;Tandon School of Engineering",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.nyu.edu",
        "aff_unique_abbr": "Oxford;NYU",
        "aff_campus_unique_index": "0;1;1;1;1",
        "aff_campus_unique": "Oxford;New York",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "d72aff3718",
        "title": "Motion Planning (In)feasibility Detection using a Prior Roadmap via Path and Cut Search",
        "site": "https://www.roboticsproceedings.org/rss19/p060.html",
        "author": "Yoonchang Sung; Peter Stone",
        "abstract": "Motion planning seeks a collision-free path in a configuration space (C-space), representing all possible robot configurations in the environment. As it is challenging to construct a C-space explicitly for a high-dimensional robot, we generally build a graph structure called a roadmap, a discrete approximation of a complex continuous C-space, to reason about connectivity. Checking collision-free connectivity in the roadmap requires expensive edge-evaluation computations, and thus, reducing the number of evaluations has become a significant research objective. However, in practice, we often face infeasible problems: those in which there is no collision-free path in the roadmap between the start and the goal locations. Existing studies often overlook the possibility of infeasibility, becoming highly inefficient by performing many edge evaluations. \r\n\r\nIn this work, we address this oversight in scenarios where a prior roadmap is available; that is, the edges of the roadmap contain the probability of being a collision-free edge learned from past experience. To this end, we propose an algorithm called iterative path and cut finding (IPC) that iteratively searches for a path and a cut in a prior roadmap to detect infeasibility while reducing expensive edge evaluations as much as possible. We further improve the efficiency of IPC by introducing a second algorithm, iterative decomposition and path and cut finding (IDPC), that leverages the fact that cut-finding algorithms partition the roadmap into smaller subgraphs. We analyze the theoretical properties of IPC and IDPC, such as completeness and computational complexity, and evaluate their performance in terms of completion time and the number of edge evaluations in large-scale simulations.",
        "bibtex": "@INPROCEEDINGS{Sung-RSS-23, \r\n    AUTHOR    = {Yoonchang Sung AND Peter Stone}, \r\n    TITLE     = {{Motion Planning (In)feasibility Detection using a Prior Roadmap via Path and Cut Search}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.060} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p060.pdf",
        "supp": "",
        "pdf_size": 7379233,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5144166116638069050&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, The University of Texas at Austin, USA + Sony AI; Department of Computer Science, The University of Texas at Austin, USA + Sony AI",
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Texas at Austin;Sony",
        "aff_unique_dep": "Department of Computer Science;Sony AI",
        "aff_unique_url": "https://www.utexas.edu;https://www.sony.com",
        "aff_unique_abbr": "UT Austin;Sony AI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0+1;0+1",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "70921331f0",
        "title": "MultiSCOPE: Disambiguating In-Hand Object Poses with Proprioception and Tactile Feedback",
        "site": "https://www.roboticsproceedings.org/rss19/p078.html",
        "author": "Andrea Sipos; Nima Fazeli",
        "abstract": "In this paper, we propose a method for estimating in-hand object poses using proprioception and tactile feedback from a bimanual robotic system. Our method addresses the problem of reducing pose uncertainty through a sequence of frictional contact interactions between the grasped objects. As part of our method, we propose 1) a tool segmentation routine that facilitates contact location and object pose estimation, 2) a loss that allows reasoning over solution consistency between interactions, and 3) a loss to promote converging to object poses and contact locations that explain the external force-torque experienced by each arm. We demonstrate the efficacy of our method in a task-based demonstration both in simulation and on a real-world bimanual platform and show significant improvement in object pose estimation over single interactions.",
        "bibtex": "@INPROCEEDINGS{Sipos-RSS-23, \r\n    AUTHOR    = {Andrea Sipos AND Nima Fazeli}, \r\n    TITLE     = {{MultiSCOPE: Disambiguating In-Hand Object Poses with Proprioception and Tactile Feedback}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.078} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p078.pdf",
        "supp": "",
        "pdf_size": 9128386,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3271257774102523133&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Robotics Department, University of Michigan, Ann Arbor, USA; Robotics Department, University of Michigan, Ann Arbor, USA",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "www.mmintlab.com/multiscope/",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Robotics Department",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a1002c8891",
        "title": "NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects",
        "site": "https://www.roboticsproceedings.org/rss19/p068.html",
        "author": "Jiahui Fu; Yilun Du; Kurran Singh; Joshua Tenenbaum; John Leonard",
        "abstract": "We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and illustrate how it supports object SLAM for consistent spatial understanding with longterm scene changes. NeuSE is a set of latent object embeddings created from partial object observations. It serves as a compact point cloud surrogate for complete object models, encoding full shape information while transforming SE(3)-equivariantly in tandem with the object in the physical world. With NeuSE, relative frame transforms can be directly derived from inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape and pose characterization, can operate independently or in conjunction with typical SLAM systems. It directly infers SE(3) camera pose constraints that are compatible with general SLAM pose graph optimization, while also maintaining a lightweight object-centric map that adapts to real-world changes. Our approach is evaluated on synthetic and real-world sequences featuring changed objects and shows improved localization accuracy and change-aware mapping capability, when working either standalone or jointly with a common SLAM pipeline.",
        "bibtex": "@INPROCEEDINGS{Fu-RSS-23, \r\n    AUTHOR    = {Jiahui Fu AND Yilun Du AND Kurran Singh AND Joshua Tenenbaum AND John Leonard}, \r\n    TITLE     = {{NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.068} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p068.pdf",
        "supp": "",
        "pdf_size": 12154097,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7887144390160328384&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "MIT CSAIL; MIT CSAIL; MIT CSAIL; MIT CSAIL; MIT CSAIL",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/neuse-slam/neuse",
        "project": "https://neuse-slam.github.io/neuse/",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT CSAIL",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "af88431f97",
        "title": "Non-Euclidean Motion Planning with Graphs of Geodesically-Convex Sets",
        "site": "https://www.roboticsproceedings.org/rss19/p057.html",
        "author": "Thomas B Cohn; Mark Petersen; Max Simchowitz; Russ Tedrake",
        "abstract": "Computing optimal, collision-free trajectories for high-dimensional systems is a challenging problem. Sampling-based planners struggle with the dimensionality, whereas trajectory optimizers may get stuck in local minima due to inherent nonconvexities in the optimization landscape. The use of mixed-integer programming to encapsulate these nonconvexities and find globally optimal trajectories has recently shown great promise, thanks in part to tight convex relaxations and efficient approximation strategies that greatly reduce runtimes. These approaches were previously limited to Euclidean configuration spaces, precluding their use with mobile bases or continuous revolute joints. In this paper, we handle such scenarios by modeling configuration spaces as Riemannian manifolds, and we describe a reduction procedure for the zero-curvature case to a mixed-integer convex optimization problem. We demonstrate our results on various robot platforms, including producing efficient collision-free trajectories for a PR2 bimanual mobile manipulator.",
        "bibtex": "@INPROCEEDINGS{Cohn-RSS-23, \r\n    AUTHOR    = {Thomas B Cohn AND Mark Petersen AND Max Simchowitz AND Russ Tedrake}, \r\n    TITLE     = {{Non-Euclidean Motion Planning with Graphs of Geodesically-Convex Sets}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.057} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p057.pdf",
        "supp": "",
        "pdf_size": 3474695,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5913467251082696122&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science and Artificial Intelligence Laboratory (CSAIL) Massachusetts Institute of Technology; School of Engineering and Applied Sciences (SEAS) Harvard University + Computer Science and Artificial Intelligence Laboratory (CSAIL) Massachusetts Institute of Technology; Computer Science and Artificial Intelligence Laboratory (CSAIL) Massachusetts Institute of Technology; Computer Science and Artificial Intelligence Laboratory (CSAIL) Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;g.harvard.edu;mit.edu;mit.edu",
        "email": "mit.edu;g.harvard.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Harvard University",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.mit.edu;https://www.harvard.edu",
        "aff_unique_abbr": "MIT;Harvard",
        "aff_campus_unique_index": "0;0+0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6a05c59f0f",
        "title": "On discrete symmetries of robotics systems: A group-theoretic and data-driven analysis",
        "site": "https://www.roboticsproceedings.org/rss19/p053.html",
        "author": "Daniel F Ordonez-Apraez; Martin; Mario; Antonio Agudo; Francesc Moreno",
        "abstract": "We present a comprehensive study on discrete morphological symmetries of dynamical systems, which are commonly observed in biological and artificial locomoting systems, such as legged, swimming, and flying animals/robots/virtual characters. These symmetries arise from the presence of one or more planes/axis of symmetry in the system's morphology, resulting in harmonious duplication and distribution of body parts. Significantly, we characterize how morphological symmetries extend to symmetries in the system's dynamics, optimal control policies, and in all proprioceptive and exteroceptive measurements related to the system's dynamics evolution. In the context of data-driven methods, symmetry represents an inductive bias that justifies the use of data augmentation or symmetric function approximators. To tackle this, we present a theoretical and practical framework for identifying the system's morphological symmetry group $\\G$ and characterizing the symmetries in proprioceptive and exteroceptive data measurements. We then exploit these symmetries using data augmentation and $\\G$-equivariant neural networks. Our experiments on both synthetic and real-world applications provide empirical evidence of the advantageous outcomes resulting from the exploitation of these symmetries, including improved sample efficiency, enhanced generalization, and reduction of trainable parameters.",
        "bibtex": "@INPROCEEDINGS{Ordonez-Apraez-RSS-23, \r\n    AUTHOR    = {Daniel F Ordonez-Apraez AND Martin, Mario AND Antonio Agudo AND Francesc Moreno}, \r\n    TITLE     = {{On discrete symmetries of robotics systems: A group-theoretic and data-driven analysis}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.053} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p053.pdf",
        "supp": "",
        "pdf_size": 3317942,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13142199566877590517&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ac0e090104",
        "title": "One Policy to Dress Them All: Learning to Dress People with Diverse Poses and Garments",
        "site": "https://www.roboticsproceedings.org/rss19/p008.html",
        "author": "Yufei Wang; Zhanyi Sun; Zackory Erickson; David Held",
        "abstract": "Robot-assisted dressing could benefit the lives of many people such as older adults and individuals with disabilities. Despite such potential, robot-assisted dressing remains a challenging task for robotics as it involves complex manipulation of deformable cloth in 3D space. Many prior works aim to solve the robot-assisted dressing task, but they make certain assumptions such as a fixed garment and a fixed arm pose that limit their ability to generalize. In this work, we develop a robot-assisted dressing system that is able to dress different garments on people with diverse poses from partial point cloud observations, based on a learned policy. We show that with proper design of the policy architecture and Q function, reinforcement learning (RL) can be used to learn effective policies with partial point cloud observations that work well for dressing diverse garments. We further leverage policy distillation to combine multiple policies trained on different ranges of human arm poses into a single policy that works over a wide range of different arm poses. We conduct comprehensive real-world evaluations of our system with 510 dressing trials in a human study with 17 participants with different arm poses and dressed garments. Our system is able to dress 86% of the length of the participants' arms on average. Videos can be found on our project webpage: https://sites.google.com/view/one-policy-dress.",
        "bibtex": "@INPROCEEDINGS{Wang-RSS-23, \r\n    AUTHOR    = {Yufei Wang AND Zhanyi Sun AND Zackory Erickson AND David Held}, \r\n    TITLE     = {{One Policy to Dress Them All: Learning to Dress People with Diverse Poses and Garments}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.008} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p008.pdf",
        "supp": "",
        "pdf_size": 3296112,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2005113805122118484&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "github": "",
        "project": "https://sites.google.com/view/one-policy-dress",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "98e07f0908",
        "title": "PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection",
        "site": "https://www.roboticsproceedings.org/rss19/p013.html",
        "author": "Shivin Dass; Karl Pertsch; Hejia Zhang; Youngwoon Lee; Joseph J Lim; Stefanos Nikolaidis",
        "abstract": "Large-scale data is an essential component of machine learning as demonstrated in recent advances in natural language processing and computer vision research. However, collecting large-scale robotic data is much more expensive and slower as each operator can control only a single robot at a time. To make this costly data collection process efficient and scalable, we propose Policy Assisted TeleOperation (PATO), a system which automates part of the demonstration collection process using a learned assistive policy. PATO autonomously executes repetitive behaviors in data collection and asks for human input only when it is uncertain about which subtask or behavior to execute. We conduct teleoperation user studies both with a real robot and a simulated robot fleet and demonstrate that our assisted teleoperation system reduces human operators' mental load while improving data collection efficiency. Further, it enables a single operator to control multiple robots in parallel, which is a first step towards scalable robotic data collection. For code and video results, see https://clvrai.com/pato",
        "bibtex": "@INPROCEEDINGS{Dass-RSS-23, \r\n    AUTHOR    = {Shivin Dass AND Karl Pertsch AND Hejia Zhang AND Youngwoon Lee AND Joseph J Lim AND Stefanos Nikolaidis}, \r\n    TITLE     = {{PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.013} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p013.pdf",
        "supp": "",
        "pdf_size": 6147736,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6403411268174471889&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "USC\u2020; USC\u2020; USC\u2020; UC Berkeley\u2021; KAIST\u00a7; USC\u2020",
        "aff_domain": "usc.edu;usc.edu;usc.edu;berkeley.edu;kaist.ac.kr;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu;berkeley.edu;kaist.ac.kr;usc.edu",
        "github": "",
        "project": "clvrai.com/pato",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1;2;0",
        "aff_unique_norm": "University of Southern California;University of California, Berkeley;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.usc.edu;https://www.berkeley.edu;https://www.kaist.ac.kr",
        "aff_unique_abbr": "USC;UC Berkeley;KAIST",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0;0;1;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "14da7122ee",
        "title": "POV-SLAM: Probabilistic Object-Aware Variational SLAM in Semi-Static Environments",
        "site": "https://www.roboticsproceedings.org/rss19/p069.html",
        "author": "Jingxing Qian; Veronica Chatrath; James Servos; Aaron Mavrinac; Wolfram Burgard; Steven L Waslander; Angela Schoellig",
        "abstract": "Simultaneous localization and mapping (SLAM) in slowly varying scenes is important for long-term robot task completion in GPS-denied environments. Failing to detect scene changes may lead to inaccurate maps and, ultimately, lost robots. Classical SLAM algorithms assume static scenes, and recent works take dynamics into account, but require scene changes to be observed in consecutive frames. Semi-static scenes, wherein objects appear, disappear, or move slowly over time, are often overlooked, yet are critical for long-term operation. We propose an object-aware, factor-graph SLAM framework that tracks and reconstructs semi-static object-level changes. Our novel variational expectation-maximization strategy is used to optimize factor graphs involving a Gaussian-Uniform bimodal measurement likelihood for potentially-changing objects. We evaluate our approach alongside the state-of-the-art SLAM solutions in simulation and on our novel real-world SLAM dataset captured in a warehouse over four months. Our method improves the robustness of localization in the presence of semi-static changes, providing object-level reasoning about the scene.",
        "bibtex": "@INPROCEEDINGS{Qian-RSS-23, \r\n    AUTHOR    = {Jingxing Qian AND Veronica Chatrath AND James Servos AND Aaron Mavrinac AND Wolfram Burgard AND Steven L Waslander AND Angela Schoellig}, \r\n    TITLE     = {{POV-SLAM: Probabilistic Object-Aware Variational SLAM in Semi-Static Environments}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.069} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p069.pdf",
        "supp": "",
        "pdf_size": 83437067,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10993147318912464155&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The University of Toronto Institute for Aerospace Studies and the University of Toronto Robotics Institute; The University of Toronto Institute for Aerospace Studies and the University of Toronto Robotics Institute + The Technical University of Munich; Clearpath Robotics, Waterloo, Canada; Clearpath Robotics, Waterloo, Canada; The Technical University of Nuremberg; The University of Toronto Institute for Aerospace Studies and the University of Toronto Robotics Institute; The University of Toronto Institute for Aerospace Studies and the University of Toronto Robotics Institute + The Technical University of Munich",
        "aff_domain": "robotics.utias.utoronto.ca;robotics.utias.utoronto.ca;clearpath.ai;clearpath.ai;utn.de;robotics.utias.utoronto.ca;tum.de",
        "email": "robotics.utias.utoronto.ca;robotics.utias.utoronto.ca;clearpath.ai;clearpath.ai;utn.de;robotics.utias.utoronto.ca;tum.de",
        "github": "https://github.com/Viky397/TorWICDataset",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0+1;2;2;3;0;0+1",
        "aff_unique_norm": "University of Toronto;Technical University of Munich;Clearpath Robotics;Technical University of Nuremberg",
        "aff_unique_dep": "Institute for Aerospace Studies;;;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.tum.de;https://www.clearpathrobotics.com;https://www.tu-nuernberg.de/",
        "aff_unique_abbr": "U of T;TUM;;TUN",
        "aff_campus_unique_index": "0;0;2;2;0;0",
        "aff_campus_unique": "Toronto;;Waterloo",
        "aff_country_unique_index": "0;0+1;0;0;1;0;0+1",
        "aff_country_unique": "Canada;Germany"
    },
    {
        "id": "0383e62f75",
        "title": "Path Planning for Multiple Tethered Robots Using Topological Braids",
        "site": "https://www.roboticsproceedings.org/rss19/p106.html",
        "author": "Muqing Cao; Kun Cao; Shenghai Yuan; Kangcheng Liu; Yan Loi Wong; Lihua Xie",
        "abstract": "Path planning for multiple tethered robots is a challenging problem due to the complex interactions among the cables and the possibility of severe entanglements. Previous works on this problem either consider idealistic cable models or provide no guarantee for entanglement-free paths. In this work, we present a new approach to address this problem using the theory of braids. By establishing a topological equivalence between the physical cables and the space-time trajectories of the robots, and identifying particular braid patterns that emerge from the entangled trajectories, we obtain the key finding that all complex entanglements stem from a finite number of interaction patterns between 2 or 3 robots. Hence, non-entanglement can be guaranteed by avoiding these interaction patterns in the trajectories of the robots. Based on this finding, we present a graph search algorithm using the permutation grid to efficiently search for a feasible topology of paths and reject braid patterns that result in an entanglement. \r\nWe demonstrate that the proposed algorithm can achieve 100% goal-reaching capability without entanglement for up to 10 drones with a slack cable model in a high-fidelity simulation platform.\r\nThe practicality of the proposed approach is verified using three small tethered UAVs in indoor flight experiments.",
        "bibtex": "@INPROCEEDINGS{Cao-RSS-23, \r\n    AUTHOR    = {Muqing Cao AND Kun Cao AND Shenghai Yuan AND Kangcheng Liu AND Yan Loi Wong AND Lihua Xie}, \r\n    TITLE     = {{Path Planning for Multiple Tethered Robots Using Topological Braids}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.106} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p106.pdf",
        "supp": "",
        "pdf_size": 10497316,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9387650729751428851&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Department of Mathematics, Faculty of Science, National University of Singapore, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "aff_domain": "ntu.edu.sg; ; ; ; ;ntu.edu.sg",
        "email": "ntu.edu.sg; ; ; ; ;ntu.edu.sg",
        "github": "https://github.com/caomuqing/tethered robots path planning",
        "project": "https://youtu.be/igP7eaOyZuc",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "Nanyang Technological University;National University of Singapore",
        "aff_unique_dep": "School of Electrical and Electronic Engineering;Department of Mathematics",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.nus.edu.sg",
        "aff_unique_abbr": "NTU;NUS",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "59aa57e2f1",
        "title": "Pre-Training for Robots: Offline RL Enables Learning New Tasks in a Handful of Trials",
        "site": "https://www.roboticsproceedings.org/rss19/p019.html",
        "author": "Aviral Kumar; Anikait Singh; Frederik D Ebert; Mitsuhiko Nakamoto; Yanlai Yang; Chelsea Finn; Sergey Levine",
        "abstract": "Progress in deep learning highlights the tremendous potential of utilizing diverse datasets for attaining effective generalization and makes it enticing to consider leveraging broad datasets for attaining robust generalization in robotic learning as well. However, in practice we often want to learn a new skill in a new environment that is unlikely to be contained in the prior data. Therefore we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data? In this paper, we demonstrate that end-to-end offline RL can be an effective approach for doing this, without the need for any representation learning or vision-based pre-training. We present pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as a few as 10 demonstrations. PTR utilizes an existing offline RL method, conservative Q-learning (CQL), but extends it to include several crucial design decisions that enable PTR to actually work and outperform a variety of prior methods. To our knowledge, PTR is the first RL method that succeeds at learning new tasks in a new domain on a real WidowX robot with as few as 10 task demonstrations, by effectively leveraging an existing dataset of diverse multi-task robot data collected in a variety of toy kitchens. We also demonstrate that the PTR approach can enable effective autonomous fine-tuning and improvement in a handful of trials, without needing any demonstrations. An accompanying overview video can be found at this anonymous URL:",
        "bibtex": "@INPROCEEDINGS{Kumar-RSS-23, \r\n    AUTHOR    = {Aviral Kumar AND Anikait Singh AND Frederik D Ebert AND Mitsuhiko Nakamoto AND Yanlai Yang AND Chelsea Finn AND Sergey Levine}, \r\n    TITLE     = {{Pre-Training for Robots: Offline RL Enables Learning New Tasks in a Handful of Trials}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.019} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p019.pdf",
        "supp": "",
        "pdf_size": 30870911,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10192077347540070986&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; New York University; Stanford University; UC Berkeley",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "https://sites.google.com/view/ptr-rss",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;2;0",
        "aff_unique_norm": "University of California, Berkeley;New York University;Stanford University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.nyu.edu;https://www.stanford.edu",
        "aff_unique_abbr": "UC Berkeley;NYU;Stanford",
        "aff_campus_unique_index": "0;0;0;0;2;0",
        "aff_campus_unique": "Berkeley;;Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d22e3dc620",
        "title": "Precise Object Sliding with Top Contact via Asymmetric Dual Limit Surfaces",
        "site": "https://www.roboticsproceedings.org/rss19/p045.html",
        "author": "Xili Yi; Nima Fazeli",
        "abstract": "In this paper, we discuss the mechanics and planning algorithms to slide an object on a horizontal planar surface via frictional patch contact made with its top surface. Here, we propose an asymmetric dual limit surface model to determine slip boundary conditions for both the top and bottom contact. With this model, we obtain a range of twists that can keep the object in sticking contact with the robot end-effector while slipping on the supporting plane. Based on these constraints, we derive a planning algorithm to slide objects with only top contact to arbitrary goal poses without slippage between end effector and the object. We validate the proposed model empirically and demonstrate its predictive accuracy on a variety of object geometries and motions. We also evaluate the planning algorithm over a variety of objects and goals demonstrate an orientation error improvement of 90% when compared to methods naive to linear path planners.",
        "bibtex": "@INPROCEEDINGS{Yi-RSS-23, \r\n    AUTHOR    = {Xili Yi AND Nima Fazeli}, \r\n    TITLE     = {{Precise Object Sliding with Top Contact via Asymmetric Dual Limit Surfaces}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.045} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p045.pdf",
        "supp": "",
        "pdf_size": 9932691,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5073223392255727405&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Robotics, University of Michigan; Department of Robotics, University of Michigan",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "https://www.mmintlab.com/dual-limit-surfaces/",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Robotics",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "466b66e11f",
        "title": "Predefined-Time Convergent Motion Control for Heterogeneous Continuum Robots",
        "site": "https://www.roboticsproceedings.org/rss19/p092.html",
        "author": "Ning Tan; Peng Yu; Kai Huang",
        "abstract": "As research into continuum robots flourishes, there are more and more types of continuum robots, which require researchers to tirelessly design robot-specific motion control algorithms. Besides, the convergence time of control systems for continuum robots has received very little attention. In this paper, we propose a novel predefined-time convergent zeroing dynamics (PTCZD) model, which ensures that the associated error-monitoring function converges to zero in predefined-time. Based on the PTCZD model, we design an inverse kinematics solver and a state estimator for continuum robots, thereby obtaining a generic predefined-time convergent control method for heterogeneous continuum robots for the first time. Simulations and experiments based on cable-driven continuum robots and concentric tube continuum robots are performed to verify the efficacy, robustness and adaptability of the proposed control method. In addition, comparative studies are carried out to demonstrate its advantages against existing control methods for continuum robots.",
        "bibtex": "@INPROCEEDINGS{Tan-RSS-23, \r\n    AUTHOR    = {Ning Tan AND Peng Yu AND Kai Huang}, \r\n    TITLE     = {{Predefined-Time Convergent Motion Control for Heterogeneous Continuum Robots}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.092} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p092.pdf",
        "supp": "",
        "pdf_size": 5102046,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7916921022769551574&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China",
        "aff_domain": "mail.sysu.edu.cn; ; ",
        "email": "mail.sysu.edu.cn; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sun Yat-sen University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.sysu.edu.cn",
        "aff_unique_abbr": "SYSU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Guangzhou",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "62f9640611",
        "title": "Progressive Learning for Physics-informed Neural Motion Planning",
        "site": "https://www.roboticsproceedings.org/rss19/p063.html",
        "author": "Ruiqi Ni; Ahmed H Qureshi",
        "abstract": "Motion planning (MP) is one of the core robotics problems requiring fast methods for finding a collision-free robot motion path connecting the given start and goal states. Neural motion planners (NMPs) demonstrate fast computational speed in finding path solutions but require a huge amount of expert trajectories for learning, thus adding a significant training computational load. In contrast, recent advancements have also led to a physics-informed NMP approach that directly solves the Eikonal equation for motion planning and does not require expert demonstrations for learning. However, experiments show that the physics-informed NMP approach performs poorly in complex environments and lacks scalability in multiple scenarios and high-dimensional real robot settings. To overcome these limitations, this paper presents a novel and tractable Eikonal equation formulation and introduces a new progressive learning strategy to train neural networks without expert data in complex, cluttered, multiple high-dimensional robot motion planning scenarios. The results demonstrate that our method outperforms state-of-the-art traditional MP, data-driven NMP, and physics-informed NMP methods by a significant margin in terms of computational planning speed, path quality, and success rates. We also show that our approach scales to multiple complex, cluttered scenarios and the real robot set up in a narrow passage environment. The proposed method's videos and code implementations are available at https://github.com/ruiqini/P-NTFields.",
        "bibtex": "@INPROCEEDINGS{Ni-RSS-23, \r\n    AUTHOR    = {Ruiqi Ni AND Ahmed H Qureshi}, \r\n    TITLE     = {{Progressive Learning for Physics-informed Neural Motion Planning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.063} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p063.pdf",
        "supp": "",
        "pdf_size": 7814136,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8379526857960023412&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, Purdue University; Department of Computer Science, Purdue University",
        "aff_domain": "purdue.edu;purdue.edu",
        "email": "purdue.edu;purdue.edu",
        "github": "https://github.com/ruiqini/P-NTFields",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "86f5e3fe6b",
        "title": "RADIUS: Risk-Aware, Real-Time, Reachability-Based Motion Planning",
        "site": "https://www.roboticsproceedings.org/rss19/p083.html",
        "author": "Challen Enninful Adu; Jinsun Liu; Lucas Lymburner; Vishrut Kaushik; Lena Trang; Ram Vasudevan",
        "abstract": "Deterministic methods for motion planning guarantee safety amidst uncertainty in obstacle locations by trying to restrict the robot from operating in any possible location that an obstacle could be in. Unfortunately, this can result in overly conservative behavior. Chance-constrained optimization can be applied to improve the performance of motion planning algorithms by allowing for a user-specified amount of bounded constraint violation. However, state-of-the-art methods rely either on moment-based inequalities, which can be overly conservative, or make it difficult to satisfy assumptions about the class of probability distributions used to model uncertainty. To address these challenges, this work proposes a real-time, risk-aware reachability-based motion planning framework called RADIUS. The method first generates a reachable set of parameterized trajectories for the robot offline. At run time, RADIUS computes a closed-form over-approximation of the risk of a collision with an obstacle. This is done without restricting the probability distribution used to model uncertainty to a simple class (e.g., Gaussian). Then, RADIUS performs real-time optimization to construct a trajectory that can be followed by the robot in a manner that is certified to have a risk of collision that is less than or equal to a user-specified threshold. The proposed algorithm is compared to several state-of-the-art chance-constrained and deterministic methods in simulation, and is shown to consistently outperform them in a variety of driving scenarios. A demonstration of the proposed framework on hardware is also provided.",
        "bibtex": "@INPROCEEDINGS{Adu-RSS-23, \r\n    AUTHOR    = {Challen Enninful Adu AND Jinsun Liu AND Lucas Lymburner AND Vishrut Kaushik AND Lena Trang AND Ram Vasudevan}, \r\n    TITLE     = {{RADIUS: Risk-Aware, Real-Time, Reachability-Based Motion Planning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.083} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p083.pdf",
        "supp": "",
        "pdf_size": 2079338,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16572984549543290715&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Robotics, University of Michigan, Ann Arbor, MI; Robotics, University of Michigan, Ann Arbor, MI; Robotics, University of Michigan, Ann Arbor, MI; Robotics, University of Michigan, Ann Arbor, MI; College of Engineering, University of Michigan, Ann Arbor, MI; Robotics, University of Michigan, Ann Arbor, MI",
        "aff_domain": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu;umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "https://roahmlab.github.io/RADIUS/",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Robotics",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c7bf1f6473",
        "title": "ROSE: Rotation-based Squeezing Robotic Gripper toward Universal Handling of Objects",
        "site": "https://www.roboticsproceedings.org/rss19/p090.html",
        "author": "Son Tien Bui; Van Anh Ho",
        "abstract": "Robotics hand/grippers nowadays are not limited within manufacturing lines; instead, widely utilized in cluttered environments, such as restaurants, farms, and warehouses. In such scenarios, they need to deal with high uncertainty of the grasped objects\u2019 shapes, postures, surfaces, and material properties, which requires complex integration of sensing and decision-making process. On the other hand, integrating soft materials into the gripper\u2019s design may tolerate the above uncertainties and reduce complexity in control. In this paper, we introduce ROSE, a novel soft gripper that can embrace the object and squeeze it by buckling a funnel-liked thin-walled soft membrane around the object by simple rotation of the base. Thanks to this design, ROSE hand can adapt to a wide range of objects that can fall within the funnel, and handle with pleasant gripping force. Regardless this, ROSE can generate a high lift force (up to 328.7 N) while significantly reducing the normal pressure on the gripped objects. In our experiment, a 198 g ROSE can be integrated into a robot arm with a single actuation, and successfully lift various types of objects, even after 400,000 trials. The embracing mechanism helps reduce the dependence of friction between the object and the membrane, as ROSE could pick up a chicken egg submerged inside an olive oil tank. We also report a feasible design for equipping the ROSE hand with tactile sensing, while appealing to the scalability of the design to fit a wide range of objects.",
        "bibtex": "@INPROCEEDINGS{Bui-RSS-23, \r\n    AUTHOR    = {Son Tien Bui AND Van Anh Ho}, \r\n    TITLE     = {{ROSE: Rotation-based Squeezing Robotic Gripper toward Universal Handling of Objects}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.090} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p090.pdf",
        "supp": "",
        "pdf_size": 12344228,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17808339930630843992&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "https://youtu.be/E1wAI09LaoY",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9a46826499",
        "title": "RT-1: Robotics Transformer for Real-World Control at Scale",
        "site": "https://www.roboticsproceedings.org/rss19/p025.html",
        "author": "Anthony Brohan; Noah Brown; Justice Carbajal; Yevgen Chebotar; Joseph Dabis; Chelsea Finn; Keerthana Gopalakrishnan; Karol Hausman; Alexander Herzog; Jasmine Hsu; Julian Ibarz; Brian Ichter; Alex Irpan; Tomas Jackson; Sally Jesmonth; Nikhil Joshi; Ryan Julian; Dmitry Kalashnikov; Yuheng Kuang; Isabel Leal; Kuang-Huei Lee; Sergey Levine; Yao Lu; Utsav Malla; Deeksha Manjunath; Igor Mordatch; Ofir Nachum; Carolina Parada; Jodilyn  Peralta; Emily Perez; Karl Pertsch; Jornell  Quiambao; Kanishka Rao; Michael S Ryoo; Grecia  Salazar; Pannag R Sanketi; Kevin  Sayed; Jaspiar  Singh; Sumedh  Sontakke; Austin  Stone; Clayton  Tan; Huong  Tran; Vincent Vanhoucke; Steve  Vega; Quan H Vuong; Fei Xia; Ted Xiao; Peng Xu; Sichun Xu; Tianhe Yu; Brianna  Zitkovich",
        "abstract": "By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks.",
        "bibtex": "@INPROCEEDINGS{Brohan-RSS-23, \r\n    AUTHOR    = {Anthony Brohan AND Noah Brown AND Justice Carbajal AND Yevgen Chebotar AND Joseph Dabis AND Chelsea Finn AND Keerthana Gopalakrishnan AND Karol Hausman AND Alexander Herzog AND Jasmine Hsu AND Julian Ibarz AND Brian Ichter AND Alex Irpan AND Tomas Jackson AND Sally Jesmonth AND Nikhil Joshi AND Ryan Julian AND Dmitry Kalashnikov AND Yuheng Kuang AND Isabel Leal AND Kuang-Huei Lee AND Sergey Levine AND Yao Lu AND Utsav Malla AND Deeksha Manjunath AND Igor Mordatch AND Ofir Nachum AND Carolina Parada AND Jodilyn  Peralta AND Emily Perez AND Karl Pertsch AND Jornell  Quiambao AND Kanishka Rao AND Michael S Ryoo AND Grecia  Salazar AND Pannag R Sanketi AND Kevin  Sayed AND Jaspiar  Singh AND Sumedh  Sontakke AND Austin  Stone AND Clayton  Tan AND Huong  Tran AND Vincent Vanhoucke AND Steve  Vega AND Quan H Vuong AND Fei Xia AND Ted Xiao AND Peng Xu AND Sichun Xu AND Tianhe Yu AND Brianna  Zitkovich}, \r\n    TITLE     = {{RT-1: Robotics Transformer for Real-World Control at Scale}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.025} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p025.pdf",
        "supp": "",
        "pdf_size": 13330974,
        "gs_citation": 1231,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9461015925810580693&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 51,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5929d79b64",
        "title": "Reachability-based Trajectory Design with Neural Implicit Safety Constraints",
        "site": "https://www.roboticsproceedings.org/rss19/p062.html",
        "author": "Jonathan B Michaux; Yong Seok Kwon; Qingyi Chen; Ram Vasudevan",
        "abstract": "Generating safe motion plans in real-time is a key requirement for deploying robot manipulators to assist humans in collaborative settings.\r\nIn particular, robots must satisfy strict safety requirements to avoid damaging itself or harming nearby humans.\r\nThis is particularly challenging if the robot must also operate in real-time to quickly adjust to changes in its environment.\r\nThis paper addresses these challenges by proposing Reachability-based Signed Distance Functions (RDFs) as a neural implicit representation for robot safety.\r\nRDF, trained using supervised learning, accurately predicts the distance between the swept volume of a robot arm and an obstacle.\r\nRDF's inference and gradient computations are fast and scale linearly with the dimension of the system; these features enables its use within a novel real-time trajectory planning framework as a continuous-time collision-avoidance constraint.\r\nThe planning method here is compared to state-of-the-art methods and is demonstrated to successfully solve challenging motion planning tasks for high-dimensional systems under a limited planning time horizon.",
        "bibtex": "@INPROCEEDINGS{Michaux-RSS-23, \r\n    AUTHOR    = {Jonathan B Michaux AND Yong Seok Kwon AND Qingyi Chen AND Ram Vasudevan}, \r\n    TITLE     = {{Reachability-based Trajectory Design with Neural Implicit Safety Constraints}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.062} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p062.pdf",
        "supp": "",
        "pdf_size": 19487149,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13431672859044410137&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://roahmlab.github.io/RDF/",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1411ac7326",
        "title": "Reconfigurable Robot Control Using Flexible Coupling Mechanisms",
        "site": "https://www.roboticsproceedings.org/rss19/p095.html",
        "author": "Sha Yi; Katia Sycara; Zeynep Temel",
        "abstract": "Reconfigurable robot swarms are capable of connecting with each other to form complex structures. Current mechanical or magnetic connection mechanisms can be complicated to manufacture, consume high power, have a limited load-bearing capacity, or can only form rigid structures. In this paper, we present our low-cost soft anchor design that enables flexible coupling and decoupling between robots. Our asymmetric anchor requires minimal force to be pushed into the opening of another robot while having a strong pulling force so that the connection between robots can be secured. To maintain this flexible coupling mechanism as an assembled structure, we present our Model Predictive Control (MPC) frameworks with polygon constraints to model the geometric relationship between robots. We conducted experiments on the soft anchor to obtain its force profile, which informed the three-bar linkage model of the anchor in the simulations. We show that the proposed mechanism and MPC frameworks enable the robots to couple, decouple, and perform various behaviors in both the simulation environment and hardware platform. Our code is available at https://github.com/ZoomLabCMU/puzzlebot_anchor.",
        "bibtex": "@INPROCEEDINGS{Yi-RSS-23, \r\n    AUTHOR    = {Sha Yi AND Katia Sycara AND Zeynep Temel}, \r\n    TITLE     = {{Reconfigurable Robot Control Using Flexible Coupling Mechanisms}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.095} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p095.pdf",
        "supp": "",
        "pdf_size": 3390137,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1161540255199238808&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Robotics Institute, Carnegie Mellon University, Pittsburgh, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, USA",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/ZoomLabCMU/puzzlebot_anchor",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b1a1bc38d5",
        "title": "RoboNinja: Learning an Adaptive Cutting Policy for Multi-Material Objects",
        "site": "https://www.roboticsproceedings.org/rss19/p046.html",
        "author": "Zhenjia Xu; Zhou Xian; Xingyu Lin; Cheng Chi; Zhiao Huang; Chuang Gan; Shuran Song",
        "abstract": "We introduce RoboNinja, a learning-based cutting system for multi-material objects (i.e., soft objects with rigid cores such as avocados or mangos). In contrast to prior works using open-loop cutting actions to cut through single-material objects (e.g., slicing a cucumber), RoboNinja aims to remove the soft part of an object while preserving the rigid core, thereby maximizing the yield. To achieve this, our system closes the perception-action loop by utilizing an interactive state estimator and an adaptive cutting policy. The system first employs sparse collision information to iteratively estimate the position and geometry of an object's core and then generates closed-loop cutting actions based on the estimated state and a tolerance value. The \"adaptiveness\" of the policy is achieved through the tolerance value, which modulates the policy's conservativeness when encountering collisions, maintaining an adaptive safety distance from the estimated core. Learning such cutting skills directly on a real-world robot is challenging. Yet, existing simulators are limited in simulating multi-material objects or computing the energy consumption during the cutting process. To address this issue, we develop a differentiable cutting simulator that supports multi-material coupling and allows for the generation of optimized trajectories as demonstrations for policy learning. Furthermore, by using a low-cost force sensor to capture collision feedback, we were able to successfully deploy the learned model in real-world scenarios, including objects with diverse core geometries and soft materials.",
        "bibtex": "@INPROCEEDINGS{Xu-RSS-23, \r\n    AUTHOR    = {Zhenjia Xu AND Zhou Xian AND Xingyu Lin AND Cheng Chi AND Zhiao Huang AND Chuang Gan AND Shuran Song}, \r\n    TITLE     = {{RoboNinja: Learning an Adaptive Cutting Policy for Multi-Material Objects}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.046} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p046.pdf",
        "supp": "",
        "pdf_size": 18683691,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16938780326689304620&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Columbia University; CMU; UC Berkeley; UC San Diego; UMass Amherst & MIT-IBM AI Lab; UMass Amherst & MIT-IBM AI Lab; Columbia University",
        "aff_domain": "cs.columbia.edu;andrew.cmu.edu;berkeley.edu;cs.ucsd.edu;eng.ucsd.edu;ibm.com;cs.columbia.edu",
        "email": "cs.columbia.edu;andrew.cmu.edu;berkeley.edu;cs.ucsd.edu;eng.ucsd.edu;ibm.com;cs.columbia.edu",
        "github": "",
        "project": "https://roboninja.cs.columbia.edu/",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;3;4;4;0",
        "aff_unique_norm": "Columbia University;Carnegie Mellon University;University of California, Berkeley;University of California, San Diego;University of Massachusetts Amherst",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.columbia.edu;https://www.cmu.edu;https://www.berkeley.edu;https://www.ucsd.edu;https://www.umass.edu",
        "aff_unique_abbr": "Columbia;CMU;UC Berkeley;UCSD;UMass Amherst",
        "aff_campus_unique_index": "1;2;3;3",
        "aff_campus_unique": ";Berkeley;San Diego;Amherst",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "cec3b448fc",
        "title": "Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment",
        "site": "https://www.roboticsproceedings.org/rss19/p005.html",
        "author": "Huihan Liu; Soroush Nasiriany; Lance Zhang; Zhiyao Bao; Yuke Zhu",
        "abstract": "With the rapid growth of computing powers and recent advances in deep learning, we have witnessed impressive demonstrations of novel robot capabilities in research settings. Nonetheless, these learning systems exhibit brittle generalization and require excessive training data for practical tasks. To harness the capabilities of state-of-the-art robot learning models while embracing their imperfections, we present Sirius, a principled framework for humans and robots to collaborate through a division of work. In this framework, partially autonomous robots are tasked with handling a major portion of decision-making where they work reliably; meanwhile, human operators monitor the process and intervene in challenging situations. Such a human-robot team ensures safe deployments in complex tasks. Further, we introduce a new learning algorithm to improve the policy's performance on the data collected from the task executions. The core idea is re-weighing training samples with approximated human trust and optimizing the policies with weighted behavioral cloning. We evaluate Sirius in simulation and on real hardware, showing that Sirius consistently outperforms baselines over a collection of contact-rich manipulation tasks, achieving an 8% boost in simulation and 27% on real hardware than the state-of-the-art methods in policy success rate, with twice faster convergence and 85% memory size reduction. Videos and more details are available at https://ut-austin-rpl.github.io/sirius/",
        "bibtex": "@INPROCEEDINGS{Liu-RSS-23, \r\n    AUTHOR    = {Huihan Liu AND Soroush Nasiriany AND Lance Zhang AND Zhiyao Bao AND Yuke Zhu}, \r\n    TITLE     = {{Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.005} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p005.pdf",
        "supp": "",
        "pdf_size": 9351074,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13738092807784463374&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "1; 1; 1; 1; 1",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://ut-austin-rpl.github.io/sirius/",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "",
        "aff_unique_norm": "",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "83a921b585",
        "title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models",
        "site": "https://www.roboticsproceedings.org/rss19/p029.html",
        "author": "Ted Xiao; Harris Chan; Pierre Sermanet; Ayzaan Wahid; Anthony Brohan; Karol Hausman; Sergey Levine; Jonathan Tompson",
        "abstract": "Robotic manipulation policies that follow natural language instructions are typically trained from corpora of robot-language data that were either collected with specific tasks in mind or expensively relabeled by humans with varied language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? For example, if the original annotations contained simple task descriptions such as \"pick up the apple\", a pretrained VLM-based labeler could significantly expand the number of semantic concepts available in the data and introduce spatial concepts such as \"the apple on the right side of the table\" or alternative phrasings such as \"the red colored fruit\". To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabeled demonstration data and then train language-conditioned policies on the augmented datasets. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations do not contain crowd-sourced language annotations. Through a large-scale study of over 1,300 real world evaluations, we find that DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.",
        "bibtex": "@INPROCEEDINGS{Xiao-RSS-23, \r\n    AUTHOR    = {Ted Xiao AND Harris Chan AND Pierre Sermanet AND Ayzaan Wahid AND Anthony Brohan AND Karol Hausman AND Sergey Levine AND Jonathan Tompson}, \r\n    TITLE     = {{Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.029} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p029.pdf",
        "supp": "",
        "pdf_size": 11259644,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17972997523614766130&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3dc34b12a3",
        "title": "Robotic Table Tennis: A Case Study into a High Speed Learning System",
        "site": "https://www.roboticsproceedings.org/rss19/p006.html",
        "author": "David B D'Ambrosio; Navdeep  Jaitly; Vikas Sindhwani; Ken Oslund; Peng Xu; Nevena Lazic; Anish Shankar; Tianli Ding; Jonathan Abelian; Erwin Coumans; Gus Kouretas; Thinh Nguyen; Justin Boyd; Atil Iscen; Reza Mahjourian; Vincent Vanhoucke; Alex Bewley; Yuheng Kuang; Michael Ahn; Deepali Jain; Satoshi Kataoka; Omar E Cortes; Pierre Sermanet; Corey Lynch; Pannag R Sanketi; Krzysztof Choromanski; Wenbo Gao; Juhana Kangaspunta; Krista Reymann; Grace Vesom; Sherry Q Moore; Avi Singh; Saminda W Abeyruwan; Laura Graesser",
        "abstract": "We present a deep-dive into a real-world robotic learning system that, in previous work, was shown to be capable of hundreds of table tennis rallies with a human and has the ability to precisely return the ball to desired targets. This system puts together a highly optimized perception subsystem, a high-speed low-latency robot controller, a simulation paradigm that can prevent damage in the real world and also train policies for zero-shot transfer, and automated real world environment resets that enable autonomous training and evaluation on physical robots. We complement a complete system description, including numerous design decisions that are typically not widely disseminated, with a collection of studies that clarify the importance of mitigating various sources of latency, accounting for training and deployment distribution shifts, robustness of the perception system, sensitivity to policy hyper-parameters, and choice of action space. A video demonstrating the components of the system and details of experimental results can be found at https://youtu.be/uFcnWjB42I0.",
        "bibtex": "@INPROCEEDINGS{D'Ambrosio-RSS-23, \r\n    AUTHOR    = {David B D'Ambrosio AND Navdeep  Jaitly AND Vikas Sindhwani AND Ken Oslund AND Peng Xu AND Nevena Lazic AND Anish Shankar AND Tianli Ding AND Jonathan Abelian AND Erwin Coumans AND Gus Kouretas AND Thinh Nguyen AND Justin Boyd AND Atil Iscen AND Reza Mahjourian AND Vincent Vanhoucke AND Alex Bewley AND Yuheng Kuang AND Michael Ahn AND Deepali Jain AND Satoshi Kataoka AND Omar E Cortes AND Pierre Sermanet AND Corey Lynch AND Pannag R Sanketi AND Krzysztof Choromanski AND Wenbo Gao AND Juhana Kangaspunta AND Krista Reymann AND Grace Vesom AND Sherry Q Moore AND Avi Singh AND Saminda W Abeyruwan AND Laura Graesser}, \r\n    TITLE     = {{Robotic Table Tennis: A Case Study into a High Speed Learning System}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.006} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p006.pdf",
        "supp": "",
        "pdf_size": 2425294,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12358843505771294908&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "https://youtu.be/HbortM1wpAA",
        "author_num": 34,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ff8c64609f",
        "title": "Robust Safety under Stochastic Uncertainty with Discrete-Time Control Barrier Functions",
        "site": "https://www.roboticsproceedings.org/rss19/p084.html",
        "author": "Ryan Cosner; Preston Culbertson; Andrew Taylor; Aaron Ames",
        "abstract": "Robots deployed in unstructured, real-world environments operate under considerable uncertainty due to imperfect state estimates, model error, and disturbances. The goal of this paper is to develop controllers that are provably safe under uncertainties. To this end, we leverage Control Barrier Functions (CBFs) which guarantee that a robot remains in a ``safe set'' during its operation---yet CBFs (and their guarantees) are traditionally studied in the context of continuous-time, deterministic systems with bounded uncertainties. In this work, we study the safety properties of discrete-time CBFs (DTCBFs) for systems with discrete-time dynamics and unbounded stochastic disturbances. Using tools from martingale theory, we develop bounds for the finite-time safety of systems whose dynamics satisfy the discrete-time barrier function condition in expectation, and analyze the effect of Jensen's inequality on DTCBF-based controllers. Finally we present several examples of our method synthesizing safe control inputs for systems subject to significant process noise, including an inverted pendulum, a double integrator, and a quadruped locomoting on a narrow path.",
        "bibtex": "@INPROCEEDINGS{Cosner-RSS-23, \r\n    AUTHOR    = {Ryan Cosner AND Preston Culbertson AND Andrew Taylor AND Aaron Ames}, \r\n    TITLE     = {{Robust Safety under Stochastic Uncertainty with Discrete-Time Control Barrier Functions}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.084} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p084.pdf",
        "supp": "",
        "pdf_size": 4230324,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10360664154735283725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Mechanical and Civil Engineering; Department of Mechanical and Civil Engineering; Computing and Mathematical Sciences Department; Department of Mechanical and Civil Engineering",
        "aff_domain": "caltech.edu;caltech.edu;caltech.edu;caltech.edu",
        "email": "caltech.edu;caltech.edu;caltech.edu;caltech.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "Department of Mechanical and Civil Engineering",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7abfbca58f",
        "title": "Robust and Versatile Bipedal Jumping Control through Reinforcement Learning",
        "site": "https://www.roboticsproceedings.org/rss19/p052.html",
        "author": "Zhongyu Li; Xue Bin Peng; Pieter Abbeel; Sergey Levine; Glen Berseth; Koushil Sreenath",
        "abstract": "This work aims to push the limits of agility for bipedal robots by enabling a torque-controlled bipedal robot to perform robust and versatile dynamic jumps in the real world. We present a reinforcement learning framework for training a robot to accomplish a large variety of jumping tasks, such as jumping to different locations and directions. To improve performance on these challenging tasks, we develop a new policy structure that encodes the robot\u2019s long-term input/output (I/O) history while also providing direct access to a short-term I/O history. In order to train a versatile jumping policy, we utilize a multi-stage training scheme that includes different training stages for different objectives. After multi-stage training, the policy can be directly transferred to a real bipedal Cassie robot. Training on different tasks and exploring more diverse scenarios lead to highly robust policies that can exploit the diverse set of learned maneuvers to recover from perturbations or poor landings during real-world deployment. Such robustness in the proposed policy enables Cassie to succeed in completing a variety of challenging jump tasks in the real world, such as standing long jumps, jumping onto elevated platforms, and multi-axes jumps.",
        "bibtex": "@INPROCEEDINGS{Li-RSS-23, \r\n    AUTHOR    = {Zhongyu Li AND Xue Bin Peng AND Pieter Abbeel AND Sergey Levine AND Glen Berseth AND Koushil Sreenath}, \r\n    TITLE     = {{Robust and Versatile Bipedal Jumping Control through Reinforcement Learning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.052} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p052.pdf",
        "supp": "",
        "pdf_size": 5650155,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3113596517696048926&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of California, Berkeley; Simon Fraser University; University of California, Berkeley; University of California, Berkeley; Universit \u00b4e de Montr \u00b4eal + Mila; University of California, Berkeley",
        "aff_domain": "berkeley.edu;sfu.ca;cs.berkeley.edu;eecs.berkeley.edu;mila.quebec;berkeley.edu",
        "email": "berkeley.edu;sfu.ca;cs.berkeley.edu;eecs.berkeley.edu;mila.quebec;berkeley.edu",
        "github": "",
        "project": "https://youtu.be/aAPSZ2QFB-E",
        "author_num": 6,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0;2+3;0",
        "aff_unique_norm": "University of California, Berkeley;Simon Fraser University;Universit\u00e9 de Montr\u00e9al;Mila",
        "aff_unique_dep": ";;;Quebec Artificial Intelligence Institute",
        "aff_unique_url": "https://www.berkeley.edu;https://www.sfu.ca;https://www.umontreal.ca;https://mila.quebec",
        "aff_unique_abbr": "UC Berkeley;SFU;UdeM;Mila",
        "aff_campus_unique_index": "0;0;0;;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;1;0;0;1+1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "a1f24776b8",
        "title": "Rotating without Seeing: Towards In-hand Dexterity through Touch",
        "site": "https://www.roboticsproceedings.org/rss19/p036.html",
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7803452156447166936&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "author": "",
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5b80e02e69",
        "title": "SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering",
        "site": "https://www.roboticsproceedings.org/rss19/p040.html",
        "author": "Jun Lv; Yunhai Feng; Cheng Zhang; Shuang Zhao; Lin Shao; Cewu Lu",
        "abstract": "Model-based reinforcement learning (MBRL) is recognized with the potential to be significantly more sample efficient than model-free RL. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of MBRL in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectiveness of SAM-RL via extensive experiments. Videos are available on our project webpage at https://sites.google.com/view/rss-sam-rl.",
        "bibtex": "@INPROCEEDINGS{Lv-RSS-23, \r\n    AUTHOR    = {Jun Lv AND Yunhai Feng AND Cheng Zhang AND Shuang Zhao AND Lin Shao AND Cewu Lu}, \r\n    TITLE     = {{SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.040} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p040.pdf",
        "supp": "",
        "pdf_size": 2022413,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17702244661207530083&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "https://sites.google.com/view/rss-sam-rl",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "16b85b255f",
        "title": "SAR: Generalization of Physiological Dexterity via Synergistic Action Representation",
        "site": "https://www.roboticsproceedings.org/rss19/p007.html",
        "author": "Cameron H Berg; Vittorio Caggiano; Vikash Kumar",
        "abstract": "Learning effective continuous control policies in high-dimensional systems, including musculoskeletal agents, remains a significant challenge. Over the course of biological evolution, organisms have developed robust mechanisms for overcoming this complexity to learn highly sophisticated strategies for motor control. What accounts for this robust behavioral flexibility? Modular control via muscle synergies, i.e. coordinated muscle co-contractions, is considered to be one putative mechanism that enables organisms to learn muscle control in a simplified and generalizable action space. Drawing inspiration from this evolved motor control strategy, we use a physiologically accurate hand model to investigate whether leveraging a Synergistic Action Representation (SAR) acquired from simpler manipulation tasks improves learning and generalization on more complex tasks. We find that SAR-exploiting policies trained on a complex, 100-object randomized reorientation task significantly outperformed (> 70% success) baseline approaches (< 20% success). Notably, SAR-exploiting policies were also found to zero-shot generalize to thousands of unseen objects with out-of-domain size variations, while policies that did not adopt SAR failed to generalize. SAR also enabled significantly improved transfer learning on real-world objects. Finally, using a robotic manipulation task set and a full-body humanoid locomotion task, we establish the generality of SAR on broader high-dimensional control problems, achieving SOTA performance with an order of magnitude improved sample efficiency. To the best of our knowledge, this investigation is the first of its kind to present an end-to-end pipeline for discovering synergies and using this representation to learn high dimensional continuous control across a wide diversity of tasks.",
        "bibtex": "@INPROCEEDINGS{Berg-RSS-23, \r\n    AUTHOR    = {Cameron H Berg AND Vittorio Caggiano AND Vikash Kumar}, \r\n    TITLE     = {{SAR: Generalization of Physiological Dexterity via Synergistic Action Representation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.007} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p007.pdf",
        "supp": "",
        "pdf_size": 21683374,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "bfafa6dff7",
        "title": "Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation",
        "site": "https://www.roboticsproceedings.org/rss19/p020.html",
        "author": "Gagan Khandate; Siqi Shang; Eric T Chang; Tristan L Saidi; Johnson Adams; Matei Ciocarlie",
        "abstract": "In this paper, we present a novel method for achieving dexterous manipulation of complex objects, while simultaneously securing the object without the use of passive support surfaces. We posit that a key difficulty for training such policies in a Reinforcement Learning framework is the difficulty of exploring the problem state space, as the accessible regions of this space form a complex structure along manifolds of a high-dimensional space. To address this challenge, we use two versions of the non-holonomic Rapidly-Exploring Random Trees algorithm; one version is more general, but requires explicit use of the environment\u2019s transition function, while the second version uses manipulation-specific kinematic constraints to attain better sample efficiency. In both cases, we use states found via sampling-based exploration to generate reset distributions that enable training control policies under full dynamic constraints via model-free Reinforcement Learning. We show that these policies are effective at manipulation problems of higher difficulty than previously shown, and also transfer effectively to real robots.",
        "bibtex": "@INPROCEEDINGS{Khandate-RSS-23, \r\n    AUTHOR    = {Gagan Khandate AND Siqi Shang AND Eric T Chang AND Tristan L Saidi AND Johnson Adams AND Matei Ciocarlie}, \r\n    TITLE     = {{Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.020} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p020.pdf",
        "supp": "",
        "pdf_size": 3159811,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13136966290055594749&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "sbrl.cs.columbia.edu",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9a21a405a6",
        "title": "Scaling Robot Learning with Semantically Imagined Experience",
        "site": "https://www.roboticsproceedings.org/rss19/p027.html",
        "author": "Tianhe Yu; Ted Xiao; Jonathan Tompson; Austin Stone; Su Wang; Anthony Brohan; Jaspiar Singh; Clayton Tan; Dee M; Jodilyn Peralta; Karol Hausman; Brian Ichter; Fei Xia",
        "abstract": "Recent advances in robot learning have shown promise in enabling robots to perform a variety of manipulation tasks and generalize to novel scenarios. \r\nOne of the key contributing factors to this progress is the scale of robot data used to train the models. To obtain large-scale datasets, prior approaches have relied on either demonstrations requiring high human involvement or engineering-heavy autonomous data collection schemes, both of which being challenging in scaling up the space of new tasks and skills needed for building generalist robots. \r\nTo mitigate this issue, we propose to take an alternative route and leverage text-to-image foundation models widely used in computer vision and natural language processing to obtain meaningful data for robot learning without requiring additional robot data. Specifically, we make use of the state of the art text-to-image diffusion models and perform aggressive data augmentation on top of our existing robotic manipulation datasets via inpainting of various unseen objects for manipulation, backgrounds, and distractors with pure text guidance. Through extensive real-world experiments, we show that manipulation policies trained on the augmented data are able to solve completely unseen tasks with new objects and can behave more robustly w.r.t. novel distractors. In addition, we also find that we can improve the robustness and generalization of high-level robot learning tasks such as success detection through training with the diffusion-based data augmentation.",
        "bibtex": "@INPROCEEDINGS{Yu-RSS-23, \r\n    AUTHOR    = {Tianhe Yu AND Ted Xiao AND Jonathan Tompson AND Austin Stone AND Su Wang AND Anthony Brohan AND Jaspiar Singh AND Clayton Tan AND Dee M AND Jodilyn Peralta AND Karol Hausman AND Brian Ichter AND Fei Xia}, \r\n    TITLE     = {{Scaling Robot Learning with Semantically Imagined Experience}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.027} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p027.pdf",
        "supp": "",
        "pdf_size": 9869270,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3751425054893366447&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Google Research; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google; Robotics at Google",
        "aff_domain": ";;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;",
        "github": "",
        "project": "https://diffusion-rosie.github.io",
        "author_num": 13,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Robotics",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google Robotics",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2988a433af",
        "title": "Self-Supervised Lidar Place Recognition in Overhead Imagery Using Unpaired Data",
        "site": "https://www.roboticsproceedings.org/rss19/p098.html",
        "author": "Tim Y. Tang; Daniele De Martini; Paul M Newman",
        "abstract": "As much as place recognition is crucial for navigation, mapping and collecting training ground truth, namely sensor data pairs across different locations, are costly and time-consuming.\r\nThis paper tackles these by learning lidar place recognition on public overhead imagery and in a self-supervised fashion, with no need for paired lidar and overhead imagery data.\r\nWe learn the cross-modal data comparison between lidar and overhead imagery with a multi-step framework.\r\nFirst, images are transformed into synthetic lidar data and a latent projection is learned.\r\nNext, we discover pseudo pairs of lidar and satellite data from unpaired and asynchronous sequences, and use them for training a final embedding space projection in a cross-modality place recognition framework.\r\nWe train and test our approach on real data from various environments and show performances approaching a supervised method using paired data.",
        "bibtex": "@INPROCEEDINGS{Tang-RSS-23, \r\n    AUTHOR    = {Tim Y. Tang AND Daniele De Martini AND Paul M Newman}, \r\n    TITLE     = {{Self-Supervised Lidar Place Recognition in Overhead Imagery Using Unpaired Data}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.098} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p098.pdf",
        "supp": "",
        "pdf_size": 7185290,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6352410073216347397&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Mobile Robotics Group, Oxford Robotics Institute, University of Oxford; Mobile Robotics Group, Oxford Robotics Institute, University of Oxford; Mobile Robotics Group, Oxford Robotics Institute, University of Oxford",
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Oxford Robotics Institute",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "99e1eaa9c4",
        "title": "Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction",
        "site": "https://www.roboticsproceedings.org/rss19/p017.html",
        "author": "Yangxiao Lu; Ninad A Khargonkar; Zesheng Xu; Charles Averill; Kamalesh Palanisamy; Kaiyu Hang; Yunhui Guo; Nicholas Ruozzi; Yu Xiang",
        "abstract": "We introduce a novel robotic system for improving unseen object instance segmentation in the real world by leveraging long-term robot interaction with objects. Previous approaches either grasp or push an object and then obtain the segmentation mask of the grasped or pushed object after one action. Instead, our system defers the decision on segmenting objects after a sequence of robot pushing actions. By applying multi-object tracking and video object segmentation on the images collected via robot pushing, our system can generate segmentation masks of all the objects in these images in a self-supervised way. These include images where objects are very close to each other, and segmentation errors usually occur on these images for existing object segmentation networks. We demonstrate the usefulness of our system by fine-tuning segmentation networks trained on synthetic data with real-world data collected by our system. We show that, after fine-tuning, the segmentation accuracy of the networks is significantly improved both in the same domain and across different domains. In addition, we verify that the fine-tuned networks improve top-down robotic grasping of unseen objects in the real world.",
        "bibtex": "@INPROCEEDINGS{Lu-RSS-23, \r\n    AUTHOR    = {Yangxiao Lu AND Ninad A Khargonkar AND Zesheng Xu AND Charles Averill AND Kamalesh Palanisamy AND Kaiyu Hang AND Yunhui Guo AND Nicholas Ruozzi AND Yu Xiang}, \r\n    TITLE     = {{Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.017} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p017.pdf",
        "supp": "",
        "pdf_size": 4381461,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7377171025957555904&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "The University of Texas at Dallas; The University of Texas at Dallas; The University of Texas at Dallas; The University of Texas at Dallas; The University of Texas at Dallas; Rice University; The University of Texas at Dallas; The University of Texas at Dallas; The University of Texas at Dallas",
        "aff_domain": "utdallas.edu;utdallas.edu;utdallas.edu;utdallas.edu;utdallas.edu;rice.edu;utdallas.edu;utdallas.edu;utdallas.edu",
        "email": "utdallas.edu;utdallas.edu;utdallas.edu;utdallas.edu;utdallas.edu;rice.edu;utdallas.edu;utdallas.edu;utdallas.edu",
        "github": "",
        "project": "https://irvlutd.github.io/SelfSupervisedSegmentation",
        "author_num": 9,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;1;0;0;0",
        "aff_unique_norm": "University of Texas at Dallas;Rice University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utdallas.edu;https://www.rice.edu",
        "aff_unique_abbr": "UT Dallas;Rice",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Dallas;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0dd169e0b1",
        "title": "Self-Supervised Visuo-Tactile Pretraining to Locate and Follow Garment Features",
        "site": "https://www.roboticsproceedings.org/rss19/p018.html",
        "author": "Justin Kerr; Huang Huang; Albert Wilcox; Ryan I Hoque; Jeffrey Ichnowski; Roberto Calandra; Ken Goldberg",
        "abstract": "Humans make extensive use of vision and touch as complementary senses, with vision providing global information about the scene and touch measuring local information during manipulation without suffering from occlusions. While prior work demonstrates the efficacy of tactile sensing for precise manipulation of deformables, they typically rely on supervised, human-labeled datasets. We propose Self-Supervised Visuo-Tactile Pretraining (SSVTP), a framework for learning multi-task visuo-tactile representations in a self-supervised manner through cross-modal supervision. We design a mechanism that enables a robot to autonomously collect precisely spatially-aligned visual and tactile image pairs, then train visual and tactile encoders to embed these pairs into a shared latent space using cross- modal contrastive loss. We apply this latent space to downstream perception and control of deformable garments on flat surfaces, and evaluate the flexibility of the learned representations without fine-tuning on 5 tasks: feature classification, contact localization, anomaly detection, feature search from a visual query (e.g., garment feature localization under occlusion), and edge following along cloth edges. The pretrained representations achieve a 73-100% success rate on these 5 tasks.",
        "bibtex": "@INPROCEEDINGS{Kerr-RSS-23, \r\n    AUTHOR    = {Justin Kerr AND Huang Huang AND Albert Wilcox AND Ryan I Hoque AND Jeffrey Ichnowski AND Roberto Calandra AND Ken Goldberg}, \r\n    TITLE     = {{Self-Supervised Visuo-Tactile Pretraining to Locate and Follow Garment Features}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.018} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p018.pdf",
        "supp": "",
        "pdf_size": 41633793,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1810005682634647218&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley; The AUTOLab at UC Berkeley + Carnegie Mellon University; Learning, Adaptive Systems, and Robotics (LASR) Lab, TU Dresden + The Centre for Tactile Internet with Human-in-the-Loop (CeTI); The AUTOLab at UC Berkeley",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0+1;2+3;0",
        "aff_unique_norm": "University of California, Berkeley;Carnegie Mellon University;Technische Universit\u00e4t Dresden;Centre for Tactile Internet with Human-in-the-Loop",
        "aff_unique_dep": "The AUTOLab;;Learning, Adaptive Systems, and Robotics (LASR) Lab;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.cmu.edu;https://www.tu-dresden.de;",
        "aff_unique_abbr": "UC Berkeley;CMU;TU Dresden;CeTI",
        "aff_campus_unique_index": "0;0;0;0;0;;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0+0;1;0",
        "aff_country_unique": "United States;Germany;"
    },
    {
        "id": "40744f5e9c",
        "title": "Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning",
        "site": "https://www.roboticsproceedings.org/rss19/p061.html",
        "author": "Zhutian  Yang; Caelan R Garrett; Tomas Lozano-Perez; Leslie Kaelbling; Dieter Fox",
        "abstract": "We present a learning-enabled Task and Motion Planning (TAMP) algorithm for solving mobile manipulation problems in environments with many articulated and movable obstacles. Our idea is to bias the search procedure of a traditional TAMP planner with a learned plan feasibility predictor. The core of our algorithm is PIGINet, a novel Transformer-based learning method that takes in a task plan, the goal, and the initial state, and predicts the probability of finding motion trajectories associated with the task plan. We integrate PIGINet within a TAMP planner that generates a diverse set of high-level task plans, sorts them by their predicted likelihood of feasibility, and refines them in that order. We evaluate the runtime of our TAMP algorithm on seven families of kitchen rearrangement problems, comparing its performance to that of non-learning baselines. Our experiments show that PIGINet substantially improves planning efficiency, cutting down runtime by 80% on problems with small state spaces and 10%-50% on larger ones, after being trained on only 150-600 problems. Finally, it also achieves zero-shot generalization to problems with unseen object categories thanks to its visual encoding of objects.",
        "bibtex": "@INPROCEEDINGS{Yang-RSS-23, \r\n    AUTHOR    = {Zhutian  Yang AND Caelan R Garrett AND Tomas Lozano-Perez AND Leslie Kaelbling AND Dieter Fox}, \r\n    TITLE     = {{Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.061} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p061.pdf",
        "supp": "",
        "pdf_size": 10167021,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18435396823065382850&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/piginet",
        "project": "https://piginet.github.io/",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e6f64791da",
        "title": "Simultaneous Trajectory Optimization and Contact Selection for Multi-Modal Manipulation Planning",
        "site": "https://www.roboticsproceedings.org/rss19/p044.html",
        "author": "Mengchao Zhang; Devesh K Jha; Arvind U Raghunathan; Kris Hauser",
        "abstract": "Complex dexterous manipulations require switching between prehensile and non-prehensile grasps, and sliding and pivoting the object against the environment. This paper presents a manipulation planner that is able to reason about diverse changes of contacts to discover such plans. It implements a hybrid approach that performs contact-implicit trajectory optimization for pivoting and sliding manipulation primitives and sampling-based planning to change between manipulation primitives and target object poses. The optimization method, simultaneous trajectory optimization and contact selection (STOCS), introduces an infinite programming framework to dynamically select from contact points and support forces between the object and environment during a manipulation primitive. To sequence manipulation primitives, a sampling-based tree-growing planner uses STOCS to construct a manipulation tree.\r\nWe show that by using a powerful trajectory optimizer, the proposed planner can discover multi-modal manipulation trajectories involving grasping, sliding, and pivoting within a few dozen samples. The resulting trajectories are verified to enable a 6 DoF manipulator to manipulate physical objects successfully.",
        "bibtex": "@INPROCEEDINGS{Zhang-RSS-23, \r\n    AUTHOR    = {Mengchao Zhang AND Devesh K Jha AND Arvind U Raghunathan AND Kris Hauser}, \r\n    TITLE     = {{Simultaneous Trajectory Optimization and Contact Selection for Multi-Modal Manipulation Planning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.044} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p044.pdf",
        "supp": "",
        "pdf_size": 15714237,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17000089121526782482&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f6af55c417",
        "title": "Solving Stabilize-Avoid via Epigraph Form Optimal Control using Deep Reinforcement Learning",
        "site": "https://www.roboticsproceedings.org/rss19/p085.html",
        "author": "Oswin So; Chuchu Fan",
        "abstract": "Tasks for autonomous robotic systems commonly require stabilization to a desired region while maintaining safety specifications. However, solving this multi-objective problem is challenging when the dynamics are nonlinear and high-dimensional, as traditional methods do not scale well and are often limited to specific problem structures. To address this issue, we propose a novel approach to solve the stabilize-avoid problem via the solution of an infinite-horizon constrained optimal control problem (OCP). We transform the constrained OCP into epigraph form and obtain a two-stage optimization problem that optimizes over the policy in the inner problem and over an auxiliary variable in the outer problem. We then propose a new method for this formulation that combines an on-policy deep reinforcement learning algorithm with neural network regression. Our method yields better stability during training, avoids instabilities caused by saddle-point finding, and is not restricted to specific requirements on the problem structure compared to more traditional methods. We validate our approach on different benchmark tasks, ranging from low-dimensional toy examples to an F16 fighter jet with a 17-dimensional state space. Simulation results show that our approach consistently yields controllers that match or exceed the safety of existing methods while providing ten-fold increases in stability performance from larger regions of attraction.",
        "bibtex": "@INPROCEEDINGS{So-RSS-23, \r\n    AUTHOR    = {Oswin So AND Chuchu Fan}, \r\n    TITLE     = {{Solving Stabilize-Avoid via Epigraph Form Optimal Control using Deep Reinforcement Learning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.085} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p085.pdf",
        "supp": "",
        "pdf_size": 7117106,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16028071543694607756&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c7f44c0522",
        "title": "StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects",
        "site": "https://www.roboticsproceedings.org/rss19/p031.html",
        "author": "Weiyu Liu; Yilun Du; Tucker Hermans; Sonia Chernova; Chris Paxton",
        "abstract": "Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures given partial-view point clouds and high-level language goals, such as \"set the table\". Our method can perform multiple challenging language-conditioned multi-step 3D planning tasks using one model. StructDiffusion even improves the success rate of assembling physically-valid structures out of unseen objects by on average 16% over an existing multi-modal transformer model trained on specific structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. Importantly, we show how integrating both a diffusion model and a collision-discriminator model allows for improved generalization over other methods when rearranging previously-unseen objects.",
        "bibtex": "@INPROCEEDINGS{Liu-RSS-23, \r\n    AUTHOR    = {Weiyu Liu AND Yilun Du AND Tucker Hermans AND Sonia Chernova AND Chris Paxton}, \r\n    TITLE     = {{StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.031} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p031.pdf",
        "supp": "",
        "pdf_size": 8016380,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3873618469739832611&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Georgia Tech; MIT; University of Utah and NVIDIA; Georgia Tech; Meta AI",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "",
        "project": "https://structdiffusion.github.io/",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0;3",
        "aff_unique_norm": "Georgia Institute of Technology;Massachusetts Institute of Technology;University of Utah;Meta",
        "aff_unique_dep": ";;;Meta AI",
        "aff_unique_url": "https://www.gatech.edu;https://web.mit.edu;https://www.utah.edu;https://meta.com",
        "aff_unique_abbr": "Georgia Tech;MIT;U of U;Meta",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Salt Lake City",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "813d784a51",
        "title": "Structured World Models from Human Videos",
        "site": "https://www.roboticsproceedings.org/rss19/p012.html",
        "author": "Russell Mendonca; Shikhar Bahl; Deepak Pathak",
        "abstract": "In this paper, we tackle the problem of learning complex, general behaviors directly in the real world. We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision. We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction. Videos can be found at https://human-world-model.github.io",
        "bibtex": "@INPROCEEDINGS{Mendonca-RSS-23, \r\n    AUTHOR    = {Russell Mendonca AND Shikhar Bahl AND Deepak Pathak}, \r\n    TITLE     = {{Structured World Models from Human Videos}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.012} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p012.pdf",
        "supp": "",
        "pdf_size": 3424681,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8830779778402694004&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "https://human-world-model.github.io",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "339cf2c00e",
        "title": "Tactile-Filter: Interactive Tactile Perception for Part Mating",
        "site": "https://www.roboticsproceedings.org/rss19/p079.html",
        "author": "Kei Ota; Devesh K Jha; Hsiao-Yu Tung; Joshua Tenenbaum",
        "abstract": "Humans rely on touch and tactile sensing for a lot of dexterous manipulation tasks. Our tactile sensing provides us with a lot of information regarding contact formations as well as geometric information about objects during any interaction. With this motivation, vision-based tactile sensors are being widely used for various robotic perception and control tasks. In this paper, we present a method for interactive perception using vision-based tactile sensors for a part mating task, where a robot can use tactile sensors and a feedback mechanism using a particle filter to incrementally improve its estimate of objects (pegs and holes) that fit together. To do this, we first train a deep neural network that makes use of tactile images to predict the probabilistic correspondence between arbitrarily shaped objects that fit together. The trained model is used to design a particle filter which is used twofold. First, given one partial (or non-unique) observation of the hole, it incrementally improves the estimate of the correct peg by sampling more tactile observations. Second, it selects the next action for the robot to sample the next touch (and thus image) which results in maximum uncertainty reduction to minimize the number of interactions during the perception task. We evaluate our method on several part-mating tasks with novel objects using a robot equipped with a vision-based tactile sensor. We also show the efficiency of the proposed action selection method against a naive method. See supplementary video at \\url{https://www.youtube.com/watch?v=jMVBg_e3gLw}.",
        "bibtex": "@INPROCEEDINGS{Ota-RSS-23, \r\n    AUTHOR    = {Kei Ota AND Devesh K Jha AND Hsiao-Yu Tung AND Joshua Tenenbaum}, \r\n    TITLE     = {{Tactile-Filter: Interactive Tactile Perception for Part Mating}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.079} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p079.pdf",
        "supp": "",
        "pdf_size": 3502303,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4191870840333417867&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a1710ae07d",
        "title": "Task-Aware Risk Estimation of Perception Failures for Autonomous Vehicles",
        "site": "https://www.roboticsproceedings.org/rss19/p100.html",
        "author": "Pasquale Antonante; Sushant Veer; Karen Leung; Xinshuo Weng; Luca Carlone; Marco Pavone",
        "abstract": "Safety and performance are key enablers for autonomous driving: on the one hand we want our autonomous vehicles (AVs) to be safe, while at the same time their performance (e.g., comfort or progression) is key to adoption. To effectively walk the tightrope between safety and performance, AVs need to be risk-averse, but not entirely risk-avoidant. To facilitate safe-yet-performant driving, in this paper, we develop a task-aware risk estimator that assesses the risk a perception failure poses to the AV\u2019s motion plan. If the failure has no bearing on the safety of the AV\u2019s motion plan, then regardless of how egregious the perception failure is, our task-aware risk estimator considers the failure to have a low risk; on the other hand, if a seemingly benign perception failure severely impacts the motion plan, then our estimator considers it to have a high risk. In this paper, we propose a task-aware risk estimator to decide whether a safety maneuver needs to be triggered. To estimate the task-aware risk, first, we leverage the perception failure \u2014 detected by a perception monitor\u2014 to synthesize an alternative plausible model for the vehicle\u2019s surroundings. The risk due to the perception failure is then formalized as the \u201crelative\u201d risk to the AV\u2019s motion plan between the perceived and the alternative plausible scenario. We employ a statistical tool called copula, which models tail dependencies between distributions, to estimate this risk. The theoretical properties of the copula allow us to compute probably approximately correct (PAC) estimates of the risk. We evaluate our task-aware risk estimator using NuPlan and compare it with established baselines, showing that the proposed risk estimator achieves the best F1-score (doubling the score of the best baseline) and exhibits a good balance between recall and precision, i.e., a good balance of safety and performance.",
        "bibtex": "@INPROCEEDINGS{Antonante-RSS-23, \r\n    AUTHOR    = {Pasquale Antonante AND Sushant Veer AND Karen Leung AND Xinshuo Weng AND Luca Carlone AND Marco Pavone}, \r\n    TITLE     = {{Task-Aware Risk Estimation of Perception Failures for Autonomous Vehicles}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.100} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p100.pdf",
        "supp": "",
        "pdf_size": 2392584,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11621855070967198286&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ad285e6e37",
        "title": "Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations",
        "site": "https://www.roboticsproceedings.org/rss19/p009.html",
        "author": "Siddhant Haldar; Jyothish Pari; Anant Rai; Lerrel Pinto",
        "abstract": "While imitation learning provides us with an efficient toolkit to train robots, learning skills that are robust to environment variations remains a significant challenge. Current approaches address this challenge by relying either on large amounts of demonstrations that span environment variations or on handcrafted reward functions that require state estimates. Both directions are not scalable to fast imitation. In this work, we present Fast Imitation of Skills from Humans (FISH), a new imitation learning approach that can learn robust visual skills with less than a minute of human demonstrations. Given a weak base-policy trained by offline imitation of demonstrations, FISH computes rewards that correspond to the \u201cmatch\u201d between the robot\u2019s behavior and the demonstrations. These rewards are then used to adaptively update a residual policy that adds on to the base-policy. Across all tasks, FISH requires at most twenty minutes of interactive learning to imitate demonstrations on object configurations that were not seen in the demonstrations. Importantly, FISH is constructed to be versatile, which allows it to be used across robot morphologies (e.g. xArm, Allegro, Stretch) and camera configurations (e.g. third-person, eye-in-hand). Our experimental evaluations on 9 different tasks show that FISH achieves an average success rate of 93%, which is around 3.8\u00d7 higher than prior state-of-the-art methods.",
        "bibtex": "@INPROCEEDINGS{Haldar-RSS-23, \r\n    AUTHOR    = {Siddhant Haldar AND Jyothish Pari AND Anant Rai AND Lerrel Pinto}, \r\n    TITLE     = {{Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.009} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p009.pdf",
        "supp": "",
        "pdf_size": 51180694,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2040116458547080910&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "New York University; New York University; New York University; New York University",
        "aff_domain": "nyu.edu;nyu.edu; ; ",
        "email": "nyu.edu;nyu.edu; ; ",
        "github": "https://github.com/fast-imitation",
        "project": "https://fast-imitation.github.io",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "bfbe7adde1",
        "title": "TerrainNet: Visual Modeling of Complex Terrain for High-speed, Off-road Navigation",
        "site": "https://www.roboticsproceedings.org/rss19/p103.html",
        "author": "Xiangyun Meng; Nathan Hatch; Alexander Lambert; Anqi Li; Nolan Wagener; Matthew Schmittle; JoonHo Lee; Wentao Yuan; Zoey Chen; Sameul Deng; Greg Okopal; Dieter Fox; Byron Boots; Amirreza Shaban",
        "abstract": "Effective use of camera-based vision systems is essential for robust performance in autonomous off-road driving, particularly in the high-speed regime. Despite success in structured, on-road settings, current end-to-end approaches for scene prediction have yet to be successfully adapted for complex outdoor terrain. To this end, we present TerrainNet, a vision-based terrain perception system for semantic and geometric terrain prediction for aggressive, off-road navigation. The approach relies on several key insights and practical considerations for achieving reliable terrain modeling. The network includes a multi-headed output representation to capture fine- and coarse-grained terrain features necessary for estimating traversability. Accurate depth estimation is achieved using self-supervised depth completion with multi-view RGB and stereo inputs. Requirements for real-time performance and fast inference speeds are met using efficient, learned image feature projections. Furthermore, the model is trained on a large-scale, real-world off-road dataset collected across a variety of diverse outdoor environments. We show how TerrainNet can also be used for costmap prediction and provide a detailed framework for integration into a planning module. We demonstrate the performance of TerrainNet through extensive comparison to current state-of-the-art baselines for camera-only scene prediction. Finally, we showcase the effectiveness of integrating TerrainNet within a complete autonomous-driving stack by conducting a real-world vehicle test in a challenging off-road scenario.",
        "bibtex": "@INPROCEEDINGS{Meng-RSS-23, \r\n    AUTHOR    = {Xiangyun Meng AND Nathan Hatch AND Alexander Lambert AND Anqi Li AND Nolan Wagener AND Matthew Schmittle AND JoonHo Lee AND Wentao Yuan AND Zoey Chen AND Sameul Deng AND Greg Okopal AND Dieter Fox AND Byron Boots AND Amirreza Shaban}, \r\n    TITLE     = {{TerrainNet: Visual Modeling of Complex Terrain for High-speed, Off-road Navigation}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.103} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p103.pdf",
        "supp": "",
        "pdf_size": 15403215,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2164897916655523708&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Washington; University of Washington; University of Washington; University of Washington; Georgia Institute of Technology; University of Washington; University of Washington; University of Washington; University of Washington; University of Washington; University of Washington; University of Washington; University of Washington; University of Washington",
        "aff_domain": ";;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;",
        "github": "",
        "project": "https://sites.google.com/view/visual-terrain-modeling",
        "author_num": 14,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "University of Washington;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://www.gatech.edu",
        "aff_unique_abbr": "UW;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d45da02731",
        "title": "Time Optimal Ergodic Search",
        "site": "https://www.roboticsproceedings.org/rss19/p082.html",
        "author": "Dayi E Dong; Henry P Berger; Ian Abraham",
        "abstract": "Robots with the ability to balance time against the thoroughness of search have the potential to provide time-critical assistance in applications such as search and rescue. Current advances in ergodic coverage-based search methods have enabled robots to completely explore and search an area in a fixed amount of time. However, optimizing time against the quality of autonomous ergodic search has yet to be demonstrated. In this paper, we investigate solutions to the time-optimal ergodic search problem for fast and adaptive robotic search and exploration. We pose the problem as a minimum time problem with an ergodic inequality constraint whose upper bound regulates and balances the granularity of search against time. Solutions to the problem are presented analytically using Pontryagin's conditions of optimality and demonstrated numerically through a direct transcription optimization approach. We show the efficacy of the approach in generating time-optimal ergodic search trajectories in simulation and with drone experiments in a cluttered environment. Obstacle avoidance is shown to be readily integrated into our formulation, and we perform ablation studies that investigate parameter dependence on optimized time and trajectory sensitivity for search.",
        "bibtex": "@INPROCEEDINGS{Dong-RSS-23, \r\n    AUTHOR    = {Dayi E Dong AND Henry P Berger AND Ian Abraham}, \r\n    TITLE     = {{Time Optimal Ergodic Search}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.082} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p082.pdf",
        "supp": "",
        "pdf_size": 7903934,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8580493086768863623&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Yale University, New Haven, CT, USA; Yale University, New Haven, CT, USA; Yale University, New Haven, CT, USA",
        "aff_domain": "yale.edu;yale.edu;yale.edu",
        "email": "yale.edu;yale.edu;yale.edu",
        "github": "https://github.com/ialab-yale/time optimal ergodic search",
        "project": "https://sites.google.com/view/time-optimal-ergodic-search",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Yale University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yale.edu",
        "aff_unique_abbr": "Yale",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New Haven",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6fbbb93166",
        "title": "To the Noise and Back: Diffusion for Shared Autonomy",
        "site": "https://www.roboticsproceedings.org/rss19/p014.html",
        "author": "Takuma Yoneda; Luzhe Sun; Ge Yang; Bradly  C Stadie; Matthew R Walter",
        "abstract": "Shared autonomy is an operational concept in which a user and an autonomous agent collaboratively control a robotic system. It provides a number of advantages over the extremes of full-teleoperation and full-autonomy in many settings. Traditional approaches to shared autonomy rely on knowledge of the environment dynamics, a discrete space of user goals that is known a priori, or knowledge of the user's policy -- assumptions that are unrealistic in many domains.\r\nRecent works relax some of these assumptions by formulating shared autonomy with model-free deep reinforcement learning (RL).\r\nIn particular, they no longer need knowledge of the goal space (e.g., that the goals are discrete or constrained) or environment dynamics. However, they need knowledge of a task-specific reward function to train the policy. Unfortunately, such reward specification can be a difficult and brittle process. On top of that, the formulations inherently rely on human-in-the-loop training, and that necessitates them to prepare a policy that mimics users' behavior.\r\nIn this paper, we present a new approach to shared autonomy that employs a modulation of the forward and reverse diffusion process of diffusion models. Our approach does not assume known environment dynamics or the space of user goals, and in contrast to previous work, it does not require any reward feedback, nor does it require access to the user's policy during training. Instead, our framework learns a distribution over a space of desired behaviors. It then employs a diffusion model to translate the user's actions to a sample from this distribution. Crucially, we show that it is possible to carry out this process in a manner that preserves the user's control authority. We evaluate our framework on a series of challenging continuous control tasks, and analyze its ability to effectively correct user actions while maintaining their autonomy.",
        "bibtex": "@INPROCEEDINGS{Yoneda-RSS-23, \r\n    AUTHOR    = {Takuma Yoneda AND Luzhe Sun AND Ge Yang AND Bradly  C Stadie AND Matthew R Walter}, \r\n    TITLE     = {{To the Noise and Back: Diffusion for Shared Autonomy}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.014} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p014.pdf",
        "supp": "",
        "pdf_size": 7755770,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=250268104051209873&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Toyota Technological Institute at Chicago (TTIC); Department of Computer Science, University of Chicago; Institute of Artificial Intelligence and Fundamental Interactions (IAIFI)+Computer Science and Artificial Intelligence Laboratory (CSAIL), MIT; Department of Statistics and Data Science, Northwestern University; Toyota Technological Institute at Chicago (TTIC)",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+3;4;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;University of Chicago;Institute of Artificial Intelligence and Fundamental Interactions;Massachusetts Institute of Technology;Northwestern University",
        "aff_unique_dep": ";Department of Computer Science;Artificial Intelligence and Fundamental Interactions;Computer Science and Artificial Intelligence Laboratory;Department of Statistics and Data Science",
        "aff_unique_url": "https://www.ttic.edu;https://www.uchicago.edu;;https://www.csail.mit.edu;https://www.northwestern.edu",
        "aff_unique_abbr": "TTIC;UChicago;IAIFI;MIT;NU",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Chicago;;Cambridge",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c2312bf0a2",
        "title": "Uncertain Pose Estimation during Contact Tasks using Differentiable Contact Features",
        "site": "https://www.roboticsproceedings.org/rss19/p043.html",
        "author": "Dongjun Lee; Jeongmin Lee; Minji Lee",
        "abstract": "For many robotic manipulation and contact tasks, it is crucial to accurately estimate uncertain object poses, for which certain geometry and sensor information are fused in some optimal fashion. Previous results for this problem primarily adopt sampling-based or end-to-end learning methods, which yet often suffer from the issues of efficiency and generalizability.\r\nIn this paper, we propose a novel differentiable framework for this uncertain pose estimation during contact, so that it can be solved in an efficient and accurate manner with gradient-based solver. \r\nTo achieve this, we introduce a new geometric definition that is highly adaptable and capable of providing differentiable contact features. \r\nThen we approach the problem from a bi-level perspective and utilize the gradient of these contact features along with differentiable optimization to efficiently solve for the uncertain pose.\r\nSeveral scenarios are implemented to demonstrate how the proposed framework can improve existing methods.",
        "bibtex": "@INPROCEEDINGS{Lee-RSS-23, \r\n    AUTHOR    = {Dongjun Lee AND Jeongmin Lee AND Minji Lee}, \r\n    TITLE     = {{Uncertain Pose Estimation during Contact Tasks using Differentiable Contact Features}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.043} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p043.pdf",
        "supp": "",
        "pdf_size": 1668023,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15889300685399915140&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Mechanical Engineering, IAMD and IOER, Seoul National University; Department of Mechanical Engineering, IAMD and IOER, Seoul National University; Department of Mechanical Engineering, IAMD and IOER, Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Department of Mechanical Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "52e37b7d88",
        "title": "iPlanner: Imperative Path Planning",
        "site": "https://www.roboticsproceedings.org/rss19/p064.html",
        "author": "Fan Yang; Chen Wang; Cesar Cadena; Marco Hutter",
        "abstract": "The problem of path planning has been studied for years. Classic planning pipelines, including perception, mapping, and path searching, can result in latency and compounding errors between modules. While recent studies have demonstrated the effectiveness of end-to-end learning methods in achieving high planning efficiency, these methods often struggle to match the generalization abilities of classic approaches in handling different environments. Moreover, end-to-end training of policies often requires a large number of labeled data or training iterations to reach convergence. In this paper, we present a novel Imperative Learning (IL) approach. This approach leverages a differentiable cost map to provide implicit supervision during policy training, eliminating the need for demonstrations or labeled trajectories. Furthermore, the policy training adopts a Bi-Level Optimization (BLO) process, which combines network update and metric-based trajectory optimization, to generate a smooth and collision-free path toward the goal based on a single depth measurement. The proposed method allows task-level costs of predicted trajectories to be backpropagated through all components to update the network through direct gradient descent. In our experiments, the method demonstrates around 4x faster planning than the classic approach and robustness against localization noise. Additionally, the IL approach enables the planner to generalize to various unseen environments, resulting in an overall 26-87% improvement in SPL performance compared to baseline learning methods.",
        "bibtex": "@INPROCEEDINGS{Yang-RSS-23, \r\n    AUTHOR    = {Fan Yang AND Chen Wang AND Cesar Cadena AND Marco Hutter}, \r\n    TITLE     = {{iPlanner: Imperative Path Planning}}, \r\n    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, \r\n    YEAR      = {2023}, \r\n    ADDRESS   = {Daegu, Republic of Korea}, \r\n    MONTH     = {July}, \r\n    DOI       = {10.15607/RSS.2023.XIX.064} \r\n}",
        "pdf": "https://www.roboticsproceedings.org/rss19/p064.pdf",
        "supp": "",
        "pdf_size": 25427068,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13191628377900906314&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Robotic Systems Lab, ETH Zurich, 8092 Z\u00fcrich, Switzerland; Spatial AI and Robotics Lab, State University of New York at Buffalo, NY 14260, USA; Robotic Systems Lab, ETH Zurich, 8092 Z\u00fcrich, Switzerland; Robotic Systems Lab, ETH Zurich, 8092 Z\u00fcrich, Switzerland",
        "aff_domain": "ethz.ch;dr.com;ethz.ch;ethz.ch",
        "email": "ethz.ch;dr.com;ethz.ch;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "ETH Zurich;State University of New York at Buffalo",
        "aff_unique_dep": "Robotic Systems Lab;Spatial AI and Robotics Lab",
        "aff_unique_url": "https://www.ethz.ch;https://www.buffalo.edu",
        "aff_unique_abbr": "ETHZ;SUNY Buffalo",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Z\u00fcrich;Buffalo",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Switzerland;United States"
    }
]