[
    {
        "title": "A Biconvex Method for Minimum-Time Motion Planning Through Sequences of Convex Sets",
        "session": "Planning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "44",
        "author": "Tobia Marcucci; Mathew Halm; William Yang; Dongchan Lee; Andrew Marchese",
        "abstract": "We consider the problem of designing a smooth trajectory that traverses a sequence of convex sets in minimum time, while satisfying given velocity and acceleration constraints. This problem is naturally formulated as a nonconvex program. To solve it, we propose a biconvex method that quickly produces an initial trajectory and iteratively refines it by solving two convex subproblems in alternation. This method converges quickly to low-cost trajectories, returns a feasible solution even if stopped early, and does not require the selection of any line-search or trust-region parameter. Exhaustive experiments show that our method can find high-quality trajectories in a fraction of the time of state-of-the-art solvers for nonconvex optimization. Additionally, tested on the problem of transferring packages between bins using two robot arms, our method achieves a fifty percent increase in throughput compared to waypoint-based motion planners that are common in industry.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p044.pdf",
        "supp": "",
        "pdf_size": 1699100,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2642173193147221159&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Amazon Robotics; Amazon Robotics; Amazon Robotics; Amazon Robotics; Amazon Robotics",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/44/",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon Robotics",
        "aff_unique_url": "https://www.amazonrobotics.com",
        "aff_unique_abbr": "Amazon Robotics",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Generic Continuous Multi-Joint Spinal Robotic System for Agile and Accurate Behaviors with GNN-MPC method",
        "session": "Control and Dynamics",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "117",
        "author": "Ying Wu; Zida Zhou; Renmingliu; Lanxiang Zheng; Hui Cheng",
        "abstract": "The biomimetic research of vertebrates is challenging in both mechanism design and control methods. Motivated by natural acrobatics exhibited by cats and humans, this paper presents a generic multi-joint continuous spinal system and a learning-based algorithm for agile and accurate control. The spinal system combines flexibility with a high load-bearing capacity, rendering it suitable for various types of bionic robots. It features a chain-like structure formed by multiple pairs of spherical gear joints, which endow it with the ability to bend in all directions. Then, to realize dynamic and precious control, a universal control framework integrating online and offline learning is proposed. In this framework, Graph Neural Networks are employed to learn the dynamic model parameters of the spine offline, while the parameterized Model Predictive Control (GNN-MPC) can update the dynamic constraints online and select the optimal control strategy. In the aerial flipping task of the spinal column, a dynamic constraint analysis of the angular momentum of the spinal structure is conducted to derive the most efficient flipping strategy. It allows the spinal structure to execute flips in the air without relying on external forces or mechanical structures. Quantitative analyses of high-load applications on the spine reveal that the spinal column can maintain strength, precision and flexibility simultaneously. A series of aerial flipping experiments prove the designed spine\u2019s scalability, flexibility and high load capacity. With GNN-MPC, the spine system can realistically mimic biological spine behavior, validating the algorithm\u2019s effectiveness and robustness.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p117.pdf",
        "supp": "",
        "pdf_size": 20008822,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou 510006, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou 510006, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou 510006, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou 510006, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou 510006, China",
        "aff_domain": "sysu.edu.cn;sysu.edu.cn;sysu.edu.cn;sysu.edu.cn;mail.sysu.edu.cn",
        "email": "sysu.edu.cn;sysu.edu.cn;sysu.edu.cn;sysu.edu.cn;mail.sysu.edu.cn",
        "github": "",
        "project": "https://youtu.be/DDwp12vfYg4",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/117/",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Sun Yat-sen University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.sysu.edu.cn",
        "aff_unique_abbr": "SYSU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Guangzhou",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Probabilistic Measure of Multi-Robot Connectivity and Ergodic Optimal Control",
        "session": "Multi-Robot Systems",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "100",
        "author": "Yongce Liu; Zhongqiang Ren",
        "abstract": "This paper considers multi-robot trajectory planning for information gathering with intermittent connectivity maintenance. For information gathering, ergodic search provides a framework to inherently balance between exploration (visit all locations for information) and exploitation (greedily search high information regions), by planning trajectories such that the amount of time the robots spend in a region is proportional to the amount of information in that region. Although ergodic search was studied in different ways, most of them ignore or over-simplify the connectivity maintenance requirement among the robots, which is crucial for information exchange in missions without global communication. This paper introduces a novel probabilistic measure of inter-robot connectivity based on the time-averaged statistics of the robots\u2019 trajectories. Such a measure provides a new way to impose intermittent connectivity constraints during the ergodic search, which leads to an optimal control problem (OCP). We derive the theoretical condition for optimality based on the Pontryagin principle, and develop iLQR and augmented Lagrangian method-based algorithms that can numerically solve this OCP. Our experimental results validate the effectiveness of the proposed probabilistic measure and demonstrate that the ergodic search combined with this measure achieves better ergodic metrics compared to baseline approaches. We also showcase the use of our planner on a multi-drone system.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p100.pdf",
        "supp": "",
        "pdf_size": 2549213,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:UagdrjaiEE8J:scholar.google.com/&scioq=A+Probabilistic+Measure+of+Multi-Robot+Connectivity+and+Ergodic+Optimal+Control&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "UM-SJTU Joint Institute; UM-SJTU Joint Institute + Department of Automation Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn",
        "github": "https://github.com/rap-lab-org/public pymec",
        "project": "",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/100/",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "UM-SJTU Joint Institute;Shanghai Jiao Tong University",
        "aff_unique_dep": ";Department of Automation",
        "aff_unique_url": "http://ji.sjtu.edu.cn/;https://www.sjtu.edu.cn",
        "aff_unique_abbr": ";SJTU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Robot-Assisted Approach to Small Talk Training for Adults with ASD",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "88",
        "author": "Rebecca Ramnauth; Dra\u017een Br\u0161\u010di\u0107; Brian Scassellati",
        "abstract": "From dating to job interviews, making new friends or simply chatting with the cashier at checkout, engaging in small talk is a vital, everyday social skill. For adults with Autism Spectrum Disorder (ASD), small talk can be particularly challenging, yet it is essential for social integration, building relationships, and accessing professional opportunities. In this study, we present our development and evaluation of an in-home autonomous robot system that allows users to practice small talk. Results from the week-long study show that adults with ASD enjoyed the training, made notable progress in initiating conversations and improving eye contact, and viewed the system as a valuable tool for enhancing their conversational skills.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p088.pdf",
        "supp": "",
        "pdf_size": 7182149,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VFMuBm1RWuQJ:scholar.google.com/&scioq=A+Robot-Assisted+Approach+to+Small+Talk+Training+for+Adults+with+ASD&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, Yale University, USA; Graduate School of Informatics, Kyoto University, Japan; Department of Computer Science, Yale University, USA",
        "aff_domain": "yale.edu; ; ",
        "email": "yale.edu; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/88/",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Yale University;Kyoto University",
        "aff_unique_dep": "Department of Computer Science;Graduate School of Informatics",
        "aff_unique_url": "https://www.yale.edu;https://www.kyoto-u.ac.jp",
        "aff_unique_abbr": "Yale;Kyoto U",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Kyoto",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "title": "A Unified and General Humanoid Whole-Body Controller for Fine-Grained Locomotion",
        "session": "Humanoids",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "67",
        "author": "Yufei Xue; Wentao Dong; Minghuan Liu; Weinan Zhang; Jiangmiao Pang",
        "abstract": "Locomotion is a fundamental skill for humanoid robots. However, most existing works made locomotion a single, tedious, unextendable, and passive movement. This limits the kinematic capabilities of humanoid robots. In contrast, humans possess versatile athletic abilities\u2014running, jumping, hopping, and finely adjusting walking parameters such as frequency, and foot height. In this paper, we investigate solutions to bring such versatility into humanoid locomotion and thereby propose HUGWBC: a unified and general humanoid whole-body controller for fine-grained locomotion. By designing a general command space in the aspect of tasks and behaviors, along with advanced techniques like symmetrical loss and intervention training for learning a whole-body humanoid controlling policy in simulation, HUGWBC enables real-world humanoid robots to produce various natural gaits, including walking (running), jumping, standing, and hopping, with customizable parameters such as frequency, foot swing height, further combined with different body height, waist rotation, and body pitch, all in one single policy. Beyond locomotion, HUGWBC also supports real-time interventions from external upper-body controllers like teleoperation, enabling loco-manipulation while maintaining precise control under any locomotive behavior. Our experiments validate the high tracking accuracy and robustness of HUGWBC with/without upper-body intervention for all commands, and we further provide an in-depth analysis of how the various commands affect humanoid movement and offer insights into the relationships between these commands. To our knowledge, HUGWBC is the first humanoid whole-body controller that supports such fine-grained locomotion behaviors with high robustness and flexibility.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p067.pdf",
        "supp": "",
        "pdf_size": 9199597,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=856200997248583871&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Shanghai Jiao Tong University+Shanghai AI Lab; Shanghai Jiao Tong University+Shanghai AI Lab; Shanghai Jiao Tong University; Shanghai Jiao Tong University+Shanghai AI Lab; Shanghai AI Lab",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;gmail.com",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;gmail.com",
        "github": "https://hugwbc.github.io",
        "project": "",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/67/",
        "aff_unique_index": "0+1;0+1;0;0+1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Shanghai AI Lab",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.shanghaiailab.com",
        "aff_unique_abbr": "SJTU;SAIL",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A low-cost and lightweight 6 DoF bimanual arm for dynamic and contact-rich manipulation",
        "session": "Manipulation I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "55",
        "author": "Jaehyung Kim; Jiho Kim; Dongryung Lee; Yujin Jang; Beomjoon Kim",
        "abstract": "Dynamic and contact-rich object manipulation, such as striking, snatching, or hammering, remains challenging for robotic systems due to hardware limitations. Most existing robots are constrained by high-inertia design, limited compliance, and reliance on expensive torque sensors. To address this, we introduce ARMADA (Affordable Robot for Manipulation and Dynamic Actions), a 6 degrees-of-freedom bimanual robot designed for dynamic manipulation research. ARMADA combines low-inertia, back-drivable actuators with a lightweight design, using readily available components and 3D-printed links for ease of assembly in research labs.  The entire system, including both arms, is built for just $6,100. Each arm achieves speeds up to 6.16m/s, almost twice that of most collaborative robots, with a comparable payload of 2.5kg. We demonstrate ARMADA can perform dynamic manipulation like snatching, hammering, and bimanual throwing in real-world environments. We also showcase its effectiveness in reinforcement learning (RL) by training a non-prehensile manipulation policy in simulation and transferring it zero-shot to the real world, as well as human motion shadowing for dynamic bimanual object throwing. ARMADA is fully open-sourced with detailed assembly instructions, CAD models, URDFs, simulation, and learning codes.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p055.pdf",
        "supp": "",
        "pdf_size": 15901517,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:xPXf6H3AxBsJ:scholar.google.com/&scioq=A+low-cost+and+lightweight+6+DoF+bimanual+arm+for+dynamic+and+contact-rich+manipulation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Kim Jaechul Graduate School of AI, KAIST; Department of Mechanical System Design Engineering, SEOULTECH; Department of Mechanical System Design Engineering, SEOULTECH; Kim Jaechul Graduate School of AI, KAIST; Kim Jaechul Graduate School of AI, KAIST",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://sites.google.com/view/im2-humanoid-arm",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/55/",
        "aff_unique_index": "0;1;1;0;0",
        "aff_unique_norm": "KAIST;SeoulTech",
        "aff_unique_dep": "Kim Jaechul Graduate School of AI;Department of Mechanical System Design Engineering",
        "aff_unique_url": "https://www.kaist.edu;https://www.seoultech.ac.kr",
        "aff_unique_abbr": "KAIST;SEOULTECH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control",
        "session": "Humanoids",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "61",
        "author": "Jialong Li; Xuxin Cheng; Tianshu Huang; Shiqi Yang; Ri-Zhao Qiu; Xiaolong Wang",
        "abstract": "Humanoid robots derive much of their dexterity from hyper-dexterous whole-body movements, enabling tasks that require a large operational workspace\u2014such as picking objects off the ground. However, achieving these capabilities on real humanoids remains challenging due to their high degrees of freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization (AMO), a framework that integrates sim-to-real reinforcement learning (RL) with trajectory optimization for real-time, adaptive whole-body control. To mitigate distribution bias in motion imitation RL, we construct a hybrid AMO dataset and train a network capable of robust, on-demand adaptation to potentially O.O.D. commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid robot, demonstrating superior stability and an expanded workspace compared to strong baselines. Finally, we show that AMO\u2019s consistent performance supports autonomous task execution via imitation learning, underscoring the system\u2019s versatility and robustness.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p061.pdf",
        "supp": "",
        "pdf_size": 9390876,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:nRyEtml5zYUJ:scholar.google.com/&scioq=AMO:+Adaptive+Motion+Optimization+for+Hyper-Dexterous+Humanoid+Whole-Body+Control&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/61/"
    },
    {
        "title": "APEX-MR: Multi-Robot Asynchronous Planning and Execution for Cooperative Assembly",
        "session": "Multi-Robot Systems",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "98",
        "author": "Philip Huang; Ruixuan Liu; Shobhit Aggarwal; Changliu Liu; Jiaoyang Li",
        "abstract": "Compared to a single-robot workstation, a multi-robot system offers several advantages: 1) it expands the system\u2019s workspace, 2) improves task efficiency, and more importantly, 3) enables robots to achieve significantly more complex and dexterous tasks, such as cooperative assembly. However, coordinating the tasks and motions of multiple robots is challenging due to issues, e.g., system uncertainty, task efficiency, algorithm scalability, and safety concerns. To address these challenges, this paper studies multi-robot coordination and proposes APEX-MR, an asynchronous planning and execution framework designed to safely and efficiently coordinate multiple robots to achieve cooperative assembly, e.g., LEGO assembly. In particular, APEX-MR provides a systematic approach to postprocess multi-robot tasks and motion plans to enable robust asynchronous execution under uncertainty. Experimental results demonstrate that APEX-MR can significantly speed up the execution time of many Lego assembly tasks by 48% compared to sequential planning, and 36% compared to synchronous planning on average for a set of long-horizon LEGO assembly tasks. To further demonstrate the performance, we deploy APEX-MR to a dual-arm system to perform physical LEGO assembly. To our knowledge, this is the first robotic system capable of performing customized LEGO assembly using commercial LEGO bricks. The experiment results demonstrate that the dual-arm system, with APEX-MR, can safely coordinate robot motions, efficiently collaborate, and construct complex LEGO structures.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p098.pdf",
        "supp": "",
        "pdf_size": 15171600,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17878820023435911970&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://intelligent-control-lab.github.io/APEX-MR/",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/98/",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills",
        "session": "Humanoids",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "66",
        "author": "Tairan He; Jiawei Gao; Wenli Xiao; Yuanhang Zhang; Zi Wang; Jiashun Wang; Zhengyi Luo; Guanqi He; Nikhil Sobanbabu; Chaoyi Pan; Zeji Yi; Guannan Qu; Kris Kitani; Jessica K. Hodgins; Linxi Fan; Yuke Zhu; Changliu Liu; Guanya Shi",
        "abstract": "Humanoid robots hold the potential for unparalleled versatility by performing human-like, whole-body skills. However, achieving agile and coordinated whole-body motions remains a significant challenge due to the dynamics mismatch between simulation and real-world physics. Existing approaches, such as system identification (SysID) and sim-to-real (Sim2Real) methods, often rely on labor-intensive parameter tuning or result in overly conservative policies that sacrifice agility. In this paper, we present ASAP (Aligning Simulation and Real Physics), a two-stage framework designed to tackle the dynamics mismatch and enable agile whole-body skills for humanoid robots. In the second stage, we deploy the policies in the real world and collect real-world data to train a residual action model that compensates for the dynamics mismatch, reducing tracking errors and improving agility. Then ASAP fine-tunes pre-trained policies with the residual action model integrated into the simulator to align effectively with real-world dynamics. We evaluate ASAP across three transfer scenarios\u2014IsaacGym to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1 humanoid robot. Our approach achieves significant improvements in agility and whole-body coordination across a variety of dynamic motions, reducing tracking error compared to SysID, Sim2Real baselines. ASAP enables highly agile motions previously unattainable, showcasing the effectiveness of residual action learning for bridging simulation and real-world dynamics. These results highlight a promising path toward more expressive and agile humanoid robots.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p066.pdf",
        "supp": "",
        "pdf_size": 36060369,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8259620770088743010&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University+NVIDIA; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University+NVIDIA; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; NVIDIA; NVIDIA; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": ";;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;",
        "github": "https://github.com/LeCAR-Lab/ASAP",
        "project": "https://agile.human2humanoid.com",
        "author_num": 18,
        "oa": "https://roboticsconference.org/program/papers/66/",
        "aff_unique_index": "0;0;0+1;0;0;0;0+1;0;0;0;0;0;0;0;1;1;0;0",
        "aff_unique_norm": "Carnegie Mellon University;NVIDIA",
        "aff_unique_dep": ";NVIDIA Corporation",
        "aff_unique_url": "https://www.cmu.edu;https://www.nvidia.com",
        "aff_unique_abbr": "CMU;NVIDIA",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0+0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ASTRID: A Robotic Tutor for Nurse Training to Reduce Healthcare-Associated Infections",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "90",
        "author": "Peizhu Qian; Filip Bajraktari; Carlos Quintero-Pena; Qingxi Meng; Shannan Hamlin; Lydia E. Kavraki; Vaibhav V. Unhelkar",
        "abstract": "The central line dressing change is a life-critical procedure performed by nurses to provide patients with rapid infusion of fluids, such as blood and medications.  Due to their complexity and the heavy workloads nurses face, dressing changes are prone to preventable errors that can result in central line-associated bloodstream infections (CLABSIs), leading to serious health complications or, in the worst cases, patient death. In the post-COVID-19 era, CLABSI rates have increased, partly due to the heightened nursing workload caused by shortages of both registered nurses and nurse educators. To address this challenge, healthcare facilities and educators are seeking innovative solutions to complement expert nurse educators. In response, we present a robotic tutoring system,",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p090.pdf",
        "supp": "",
        "pdf_size": 4247354,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15649881002345419001&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, Rice University, Houston, Texas USA; Department of Computer Science, Rice University, Houston, Texas USA; Department of Computer Science, Rice University, Houston, Texas USA; Department of Computer Science, Rice University, Houston, Texas USA; Center for Nursing Research, Education and Practice, Houston Methodist, Houston, Texas USA; Ken Kennedy Institute, Rice University, Houston, Texas USA; Ken Kennedy Institute, Rice University, Houston, Texas USA",
        "aff_domain": "rice.edu;rice.edu;rice.edu;rice.edu;houstonmethodist.org;rice.edu;rice.edu",
        "email": "rice.edu;rice.edu;rice.edu;rice.edu;houstonmethodist.org;rice.edu;rice.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://roboticsconference.org/program/papers/90/",
        "aff_unique_index": "0;0;0;0;1;0;0",
        "aff_unique_norm": "Rice University;Houston Methodist",
        "aff_unique_dep": "Department of Computer Science;Center for Nursing Research, Education and Practice",
        "aff_unique_url": "https://www.rice.edu;https://www.houstonmethodist.org",
        "aff_unique_abbr": "Rice;Houston Methodist",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Houston",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Action Flow Matching for Lifelong Learning",
        "session": "Scaling Robot Learning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "26",
        "author": "Alejandro Murillo-Gonz\u00e1lez; Lantao Liu",
        "abstract": "Lifelong learning in robotics promises systems that can continually improve and adapt to changing environments, mirroring human adaptability. For robots, this capability is crucial in refining dynamics models, which are foundational for planning and control. However, enabling such adaptability faces significant challenges: safely incorporating new experience while avoiding catastrophic forgetting, managing outliers, reconciling exploration with exploitation and operating within the constraints of onboard resources. Towards this goal, we introduce a framework leveraging flow matching for online correction of misaligned robot models. Our method transforms the actions proposed by a model-based planner by mapping it to the action the robot would have ideally executed given a well-aligned dynamics model, based on observed deviations caused by model inaccuracies. This allows the robot to bridge the gap between planned actions and actual outcomes, resulting in faster adaptation to changes in dynamics such as altered surface friction or actuator degradation. We find that by transforming the actions themselves rather than exploring with a misaligned model, the robot collects informative data more efficiently, thereby accelerating robot adaptation. Moreover, we validate that the method can handle an evolving and possibly imperfect model while eliminating the dependency on replay buffers or legacy model snapshots. We validate our approach using an unmanned ground vehicle in both simulation and real-world settings. The results highlight the method\u2019s adaptability and efficiency, demonstrating its potential towards enabling robot lifelong learning.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p026.pdf",
        "supp": "",
        "pdf_size": 4883527,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Indiana University\u2013Bloomington; Indiana University\u2013Bloomington",
        "aff_domain": "iu.edu;iu.edu",
        "email": "iu.edu;iu.edu",
        "github": "https://github.com/AlejandroMllo/action flow matching",
        "project": "",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/26/",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indiana University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.indiana.edu",
        "aff_unique_abbr": "IU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bloomington",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adaptive Locomotion on Mud through Proprioceptive Sensing of Substrate Properties",
        "session": "Mobile Manipulation and Locomotion",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "126",
        "author": "Shipeng Liu; Jiaze Tang; Siyuan Meng; Feifei Qian",
        "abstract": "Muddy terrains present significant challenges for terrestrial robots, as subtle changes in composition and water content can lead to large variations in substrate strength and force responses, causing robot to slip or stuck. This paper presents a method to estimate mud properties using proprioceptive sensing, enabling a flipper-driven robot to adapt its locomotion through muddy substrates of varying strength. First, we characterize mud reaction forces through actuator current and position signals from a statically-mounted robotic flipper, and use the measured force to determine key coefficients that characterize intrinsic mud properties. The proprioceptively estimated coefficients match closely with measurements from a lab-grade load cell, validating the effectiveness of the proposed method. Next, we extend the method to a locomoting robot, to estimate mud properties online as it crawls across different mud mixtures. Experimental data reveals that mud reaction forces depend sensitively on robot motion, requiring joint analysis of robot movement with proprioceptive force to correctly determine mud property. Lastly, we deploy this method in a flipper-driven robot moving across muddy substrates of varying strengths, and demonstrate that the proposed method allow the robot to use the estimated mud properties to adapt its locomotion strategy, and successfully avoid locomotion failures. Our findings highlight the potential of proprioception-based terrain sensing to enhance robot mobility in complex, deformable natural environments, paving the way for more robust field exploration capabilities.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p126.pdf",
        "supp": "",
        "pdf_size": 11288789,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:JKD3ibkmm3MJ:scholar.google.com/&scioq=Adaptive+Locomotion+on+Mud+through+Proprioceptive+Sensing+of+Substrate+Properties&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, California, USA; Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, California, USA; Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, California, USA; Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, California, USA",
        "aff_domain": "usc.edu;usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/126/",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation",
        "session": "Imitation Learning II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "156",
        "author": "Yufei Wang; Ziyu Wang; Mino Nakura; Pratik Bhowal; Chia-Liang Kuo; Yi-Ting Chen; Zackory Erickson; David Held",
        "abstract": "This paper presents ArticuBot, in which a single learned policy enables a robotics system to open diverse categories of unseen articulated objects in the real world.  This task has long been challenging for robotics due to the large variations in the geometry, size, and articulation types of such objects.  Our system, ArticuBot, consists of three parts: generating a large number of demonstrations in physics-based simulation, distilling all generated demonstrations into a point cloud-based neural policy via imitation learning, and performing zero-shot sim2real transfer to real robotics systems. Utilizing sampling-based grasping and motion planning, our demonstration generalization pipeline is fast and effective, generating a total of 42.3k demonstrations over 322 training articulated objects. For policy learning, we propose a novel hierarchical policy representation, in which the high-level policy learns the sub-goal for the end-effector, and the low-level policy learns how to move the end-effector conditioned on the predicted goal.  We demonstrate that this hierarchical approach achieves much better object-level generalization compared to the non-hierarchical version.  We further propose a novel weighted displacement model for the high-level policy that grounds the prediction into the existing 3D structure of the scene, outperforming alternative policy representations.  We show that our learned policy can zero-shot transfer to three different real robot settings: a fixed table-top Franka arm across two different labs, and an X-Arm on a mobile base, opening multiple unseen articulated objects across two labs, real lounges, and kitchens.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p156.pdf",
        "supp": "",
        "pdf_size": 4326994,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:AnSC880BaPIJ:scholar.google.com/&scioq=ArticuBot:+Learning+Universal+Articulated+Object+Manipulation+Policy+via+Large+Scale+Simulation&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://roboticsconference.org/program/papers/156/"
    },
    {
        "title": "BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds",
        "session": "Humanoids",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "68",
        "author": "Huayi Wang; Zirui Wang; Junli Ren; Qingwei Ben; Tao Huang; Weinan Zhang; Jiangmiao Pang",
        "abstract": "Traversing risky terrains with sparse footholds poses a significant challenge for humanoid robots, requiring precise foot placements and stable locomotion. Existing approaches designed for quadrupedal robots often fail to generalize to humanoid robots due to differences in foot geometry and unstable morphology, while learning-based approaches for humanoid locomotion still struggle with complex terrains. In this paper, we introduce BeamDojo, a novel learning-based control framework that first enables agile humanoid locomotion on sparse beams. BeamDojo leverages a two-stage reinforcement learning (RL) approach that emphasizes fully trial-and-error exploration, and incorporates a newly designed foothold reward function tailored for polygonal feet and a double-head critic for sparse foothold reward learning. Extensive simulation and real-world experiments demonstrate that BeamDojo achieves efficient learning in simulation and enables agile locomotion with precise foot placement on sparse footholds in the real world, maintaining a high success rate even under significant external disturbances.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p068.pdf",
        "supp": "",
        "pdf_size": 36678703,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13279244876130197814&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Shanghai AI Laboratory + Shanghai Jiao Tong University; Shanghai AI Laboratory + Zhejiang University; Shanghai AI Laboratory + The University of Hong Kong; Shanghai AI Laboratory + The Chinese University of Hong Kong; Shanghai AI Laboratory + Shanghai Jiao Tong University; Shanghai AI Laboratory\u2020; Shanghai AI Laboratory\u2020",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://why618188.github.io/beamdojo",
        "project": "",
        "author_num": 7,
        "oa": "https://roboticsconference.org/program/papers/68/",
        "aff_unique_index": "0+1;0+2;0+3;0+4;0+1;0;0",
        "aff_unique_norm": "Shanghai AI Laboratory;Shanghai Jiao Tong University;Zhejiang University;University of Hong Kong;Chinese University of Hong Kong",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.shanghai-ai-lab.com;https://www.sjtu.edu.cn;https://www.zju.edu.cn;https://www.hku.hk;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "SAIL;SJTU;ZJU;HKU;CUHK",
        "aff_campus_unique_index": ";;1;1;",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Behavior Synthesis via Contact-Aware Fisher Information Maximization",
        "session": "Control and Dynamics",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "118",
        "author": "Hrishikesh Sathyanarayan; Ian Abraham",
        "abstract": "Contact dynamics hold immense amounts of information that can improve a robot\u2019s ability to characterize and learn about objects in their environment through interactions. However, collecting information-rich contact data is challenging due to its inherent sparsity and non-smooth nature, requiring an active approach to maximize the utility of contacts for learning. In this work, we investigate an optimal experimental design approach to synthesize robot behaviors that produce contact-rich data for learning. Our approach derives a contact-aware Fisher information measure that characterizes information-rich contact behaviors that improve learning. We observe emergent robot behaviors that are able to excite contact interactions that efficiently learns object parameters across a range of examples. Last, we demonstrate the utility of contact-awareness for learning contact-seeking behavior on several robotic experiments.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p118.pdf",
        "supp": "",
        "pdf_size": 5656272,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:K6jSSu3ut3kJ:scholar.google.com/&scioq=Behavior+Synthesis+via+Contact-Aware+Fisher+Information+Maximization&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Yale University, New Haven, CT, USA; Yale University, New Haven, CT, USA",
        "aff_domain": "yale.edu;yale.edu",
        "email": "yale.edu;yale.edu",
        "github": "https://github.com/ialab-yale/contact aware active learning.git",
        "project": "",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/118/",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Yale University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yale.edu",
        "aff_unique_abbr": "Yale",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "New Haven",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Bilevel Learning for Bilevel Planning",
        "session": "Planning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "43",
        "author": "Bowen Li; Tom Silver; Sebastian Scherer; Alexander G. Gray",
        "abstract": "A robot that learns from demonstrations should not just imitate what it sees\u2014it should understand the high-level concepts that are being demonstrated and generalize them to new tasks. Bilevel planning is a hierarchical model-based approach where predicates (relational state abstractions) are leveraged to achieve compositional generalization. However, previous bilevel planning approaches depend on predicates that are either hand-engineered or restricted to very simple forms, limiting their scalability to sophisticated, high-dimensional state spaces. To address this limitation, we present IVNTR, the first bilevel planning approach capable of learning neural predicates directly from demonstrations. Our key innovation is a neuro-symbolic bilevel learning framework that mirrors the structure of bilevel planning. In IVNTR, symbolic learning of the predicate \u201ceffects\u201d and neural learning of the predicate \u201cfunctions\u201d alternate, with each providing guidance for the other. We evaluate IVNTR in six diverse robot planning domains, demonstrating its efficacy in abstracting various continuous and high-dimensional states. While most existing approaches struggle to generalize (with <35% success rate), our IVNTR achieves an average of 76.8% success rate on unseen tasks. Additionally, we showcase IVNTR on a Spot robot, where it learns to perform real-world mobile manipulation tasks and generalizes to unseen test scenarios that feature new objects, new states, and longer task horizons. Our findings underscore the promise of learning and planning with abstractions as a path towards high-level generalization.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p043.pdf",
        "supp": "",
        "pdf_size": 9577440,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7353592421599047462&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Carnegie Mellon University+Centaur AI Institute; Princeton University; Carnegie Mellon University; Centaur AI Institute",
        "aff_domain": "andrew.cmu.edu; ;andrew.cmu.edu; ",
        "email": "andrew.cmu.edu; ;andrew.cmu.edu; ",
        "github": "",
        "project": "https://jaraxxus-me.github.io/IVNTR/",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/43/",
        "aff_unique_index": "0+1;2;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Centaur AI Institute;Princeton University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.centaur.ai;https://www.princeton.edu",
        "aff_unique_abbr": "CMU;Centaur AI;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Boxi: Design Decisions in the Context of Algorithmic Performance for Robotics",
        "session": "Robot Design",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "134",
        "author": "Jonas Frey; Turcan Tuna; Lanke Frank Tarimo Fu; Cedric Weibel; Katharine Patterson; Benjamin Krummenacher; Matthias M\u00fcller; Julian Nubert; Maurice Fallon; Cesar Cadena; Marco Hutter",
        "abstract": "Achieving robust autonomy in mobile robots operating in complex, unstructured environments requires a multimodal sensor suite capable of capturing diverse and complementary information. However, designing such a sensor suite involves multiple critical design decisions, such as sensor selection, component placement, thermal and power limitations, compute requirements, networking, synchronization, and calibration. While the importance of these key aspects is widely recognized, they are often overlooked in academia or retained as proprietary knowledge within large corporations. To improve this situation, we present",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p134.pdf",
        "supp": "",
        "pdf_size": 11102749,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6943363690311001478&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "ETH Zurich; ETH Zurich; University of Oxford + Max Planck Institute for Intelligent Systems; ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich + Max Planck Institute for Intelligent Systems; University of Oxford; ETH Zurich; ETH Zurich",
        "aff_domain": "ethz.ch; ; ; ; ; ; ; ; ; ; ",
        "email": "ethz.ch; ; ; ; ; ; ; ; ; ; ",
        "github": "https://github.com/leggedrobotics/grand_tour_boxI",
        "project": "",
        "author_num": 11,
        "oa": "https://roboticsconference.org/program/papers/134/",
        "aff_unique_index": "0;0;1+2;0;0;0;0;0+2;1;0;0",
        "aff_unique_norm": "ETH Zurich;University of Oxford;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";;Intelligent Systems",
        "aff_unique_url": "https://www.ethz.ch;https://www.ox.ac.uk;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "ETHZ;Oxford;MPI-IS",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+2;0;0;0;0;0+2;1;0;0",
        "aff_country_unique": "Switzerland;United Kingdom;Germany"
    },
    {
        "title": "Bridging Model Predictive Control and Deep Learning for Scalable Reachability Analysis",
        "session": "Control and Dynamics",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "114",
        "author": "Zeyuan Feng; Le Qiu; Somil Bansal",
        "abstract": "Hamilton-Jacobi (HJ) reachability analysis is a widely used method for ensuring the safety of robotic systems. Traditional approaches compute reachable sets by numerically solving an HJ Partial Differential Equation (PDE) over a grid, which is computationally prohibitive due to the curse of dimensionality. Recent learning-based methods have sought to address this challenge by approximating reachability solutions using neural networks trained with PDE residual error. However, these approaches often suffer from unstable training dynamics and suboptimal solutions due to the weak learning signal provided by the residual loss. In this work, we propose a novel approach that leverages model predictive control (MPC) techniques to guide and accelerate the reachability learning process. Observing that HJ reachability is inherently rooted in optimal control, we utilize MPC to generate approximate reachability solutions at key collocation points, which are then used to tactically guide the neural network training by ensuring compliance with these approximations. Moreover, we iteratively refine the MPC-generated solutions using the learned reachability solution, mitigating convergence to local optima. Case studies on a 2D vertical drone, a 13D quadrotor, and a 7D F1-tenth car demonstrate that bridging MPC with deep learning yields significant improvements in the robustness and accuracy of reachable sets, as well as corresponding safety assurances, compared to existing methods.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p114.pdf",
        "supp": "",
        "pdf_size": 3764656,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ddfrJGVPtHYJ:scholar.google.com/&scioq=Bridging+Model+Predictive+Control+and+Deep+Learning+for+Scalable+Reachability+Analysis&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Department of Aeronautics and Astronautics at Stanford University; Department of Electrical Engineering at Tsinghua University; Department of Aeronautics and Astronautics at Stanford University",
        "aff_domain": "stanford.edu;gmail.com;stanford.edu",
        "email": "stanford.edu;gmail.com;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/114/",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Stanford University;Tsinghua University",
        "aff_unique_dep": "Department of Aeronautics and Astronautics;Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Stanford;Tsinghua",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Bridging Perception and Action: Spatially-Grounded Mid-Level Representations for Robot Generalization",
        "session": "Imitation Learning II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "155",
        "author": "Jonathan Heewon Yang; Chuyuan Fu; Dhruv Shah; Dorsa Sadigh; Fei Xia; Tingnan Zhang",
        "abstract": "In this work, we investigate how spatially-grounded auxiliary representations can provide both broad, high-level grounding, as well as direct, actionable information, and help policy learning performance and generalization. We study these mid-level representations across three critical dimensions: object-centricity, pose-awareness, and depth-awareness. We use these interpretable mid-level representations to train specialist encoders via supervised learning, and train a diffusion policy to solve dexterous bimanual manipulation tasks in the real-world. We propose a novel mixture-of-experts policy architecture that can combine multiple specialized expert models, each trained on a distinct mid-level representation, to improve the generalization of the policy. This method achieves an average of 15.5% increase in success rate over a language-grounded baseline for our evaluation tasks. Furthermore, we find that leveraging mid-level representations as supervision signals for policy actions within a weighted imitation learning algorithm improves the precision with which the policy follows these representations, leading to an additional performance increase of 8.5%. Our findings highlight the importance of grounding robot policies with not only broad, perceptual tasks, but also more granular, actionable representations.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p155.pdf",
        "supp": "",
        "pdf_size": 8152177,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/155/"
    },
    {
        "title": "Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation",
        "session": "Mobile Manipulation and Locomotion",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "125",
        "author": "Nolan Fey; Gabriel B. Margolis; Martin Peticco; Pulkit Agrawal",
        "abstract": "Training controllers via reinforcement learning (RL) in simulation has emerged as a powerful approach for synthesizing robust and agile robotic behaviors evaluated in reality. We push the envelope of the simulation training paradigm by exposing problems encountered when learning agile behaviors only made possible by dynamic coordination between many joints, such as in the whole-body control of a quadruped robot. We find that training athletic whole-body control behaviors from scratch often fails, and the sim-to-real gap is greatly pronounced, especially on commodity hardware using complex-to-model harmonic drive actuators with limited sensing. We propose general solutions to overcome these issues: (i) leveraging a pre-trained whole-body controller as a robust foundation that can be fine-tuned with RL for a highly dynamic task (ii) a framework for modeling complex actuation mechanisms without requiring access to torque sensors. Along with several other design decisions that we elaborate, we achieve highly-dynamic whole-body control behaviors such as ball throwing, lifting heavy weights, and others.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p125.pdf",
        "supp": "",
        "pdf_size": 5118572,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:dng8veqY_z8J:scholar.google.com/&scioq=Bridging+the+Sim-to-Real+Gap+for+Athletic+Loco-Manipulation&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Improbable AI Lab, Massachusetts Institute of Technology, Cambridge, MA 02139 + Computer Science and Artificial Lab-oratory (CSAIL), the Laboratory for Information and Decision Systems (LIDS), and the MIT-IBM Watson AI Lab at MIT; Improbable AI Lab, Massachusetts Institute of Technology, Cambridge, MA 02139 + Computer Science and Artificial Lab-oratory (CSAIL), the Laboratory for Information and Decision Systems (LIDS), and the MIT-IBM Watson AI Lab at MIT; Improbable AI Lab, Massachusetts Institute of Technology, Cambridge, MA 02139 + Computer Science and Artificial Lab-oratory (CSAIL), the Laboratory for Information and Decision Systems (LIDS), and the MIT-IBM Watson AI Lab at MIT; Improbable AI Lab, Massachusetts Institute of Technology, Cambridge, MA 02139 + Computer Science and Artificial Lab-oratory (CSAIL), the Laboratory for Information and Decision Systems (LIDS), and the MIT-IBM Watson AI Lab at MIT",
        "aff_domain": "mit.edu; ; ; ",
        "email": "mit.edu; ; ; ",
        "github": "",
        "project": "https://uan.csail.mit.edu/",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/125/",
        "aff_unique_index": "0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Improbable AI Lab",
        "aff_unique_url": "https://www.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Building Rome with Convex Optimization",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "32",
        "author": "Haoyu Han; Heng Yang",
        "abstract": "Global bundle adjustment is made easy by depth prediction and convex optimization. We (i) propose a scaled bundle adjustment (SBA) formulation that lifts 2D keypoint measurements to 3D with learned depth, (ii) design an empirically tight convex semidefinite program (SDP) relaxation that solves SBA to certifiable global optimality, (iii) solve the SDP relaxations at extreme scale with Burer-Monteiro factorization and a CUDA-based trust-region Riemannian optimizer (dubbed XM), (iv) build a structure from motion (SfM) pipeline with XM as the optimization engine and show that XM-SfM dominates or compares favorably with existing SfM pipelines in terms of reconstruction quality while being faster, more scalable, and initialization-free.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p032.pdf",
        "supp": "",
        "pdf_size": 39295627,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10171437159809572440&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Engineering and Applied Sciences, Harvard University; School of Engineering and Applied Sciences, Harvard University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "https://computationalrobotics.seas.harvard.edu/XM",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/32/",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harvard University",
        "aff_unique_dep": "School of Engineering and Applied Sciences",
        "aff_unique_url": "https://www.harvard.edu",
        "aff_unique_abbr": "Harvard",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision",
        "session": "VLA Models",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "16",
        "author": "Gi-Cheon Kang; Junghyun Kim; Kyuhwan Shim; Jun Ki Lee; Byoung-Tak Zhang",
        "abstract": "Teaching robots desired skills in real-world environments remains challenging, especially for non-experts. Current robot learning methods often require expert demonstrations or complex programming, limiting their accessibility to non-experts. We posit that natural language offers an intuitive and accessible interface for robot learning. To this end, we study two aspects: (1) enabling non-experts to collect robotic data through natural language supervision (e.g., \u201cmove the arm up\u201d) and (2) learning robotic policies directly from this supervision. Specifically, we introduce a data collection framework that collects robot demonstrations based on natural language supervision and further augments these demonstrations. We then present CLIP-RT, a vision-language-action (VLA) model that learns language-conditioned visuomotor policies from this supervision. CLIP-RT adapts pre-trained CLIP models and learns to predict language-based motion primitives via contrastive imitation learning. We train CLIP-RT on the Open X-Embodiment dataset and finetune it on in-domain data collected by our framework to learn diverse skills. CLIP-RT demonstrates strong capabilities in acquiring novel manipulation skills, outperforming the state-of-the-art model, OpenVLA (7B parameters), by 24% in average success rates, while using 7x fewer parameters (1B). We further observe that CLIP-RT shows significant improvements in few-shot imitation learning. Finally, CLIP-RT demonstrates its adaptability by collaborating with humans through corrections or incorporating predictions from foundation models for improved generalization.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p016.pdf",
        "supp": "",
        "pdf_size": 10028696,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5165900103746885576&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Seoul National University; Seoul National University; Seoul National University; Seoul National University; Seoul National University+Tommoro Robotics",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://clip-rt.github.io",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/16/",
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "Seoul National University;Tommoro Robotics",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.snu.ac.kr;",
        "aff_unique_abbr": "SNU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "title": "CREStE: Scalable Mapless Navigation with Internet Scale Priors and Counterfactual Guidance",
        "session": "Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "136",
        "author": "Arthur Zhang; Harshit Sikchi; Joydeep Biswas; Amy Zhang",
        "abstract": "We address the long-horizon mapless navigation problem: enabling robots to traverse novel environments without relying on high-definition maps or precise waypoints that specify exactly where to navigate. Two major challenges arise: (1) learning robust, generalizable perceptual representations of the environment\u2014where it is impossible to pre-enumerate all relevant factors and ensure robustness to perceptual aliasing\u2014and (2) planning human-aligned navigation paths using these learned features. Existing solutions often struggle to generalize due to their reliance on: (a) hand-curated object lists, which overlook new, unforeseen factors; (b) end-to-end learning of navigation-relevant features, which is constrained by the limited availability of real robot data; (c) large sets of expert demonstrations, which provide insufficient guidance on the most critical perceptual cues; or (d) handcrafted reward functions for learning, which are difficult to design and adapt for new scenarios. To overcome these limitations, we propose CREStE, a framework for representation and policy learning that does not require large-scale robot datasets or manually specified feature sets. First, CREStE  leverages visual foundation models trained on internet-scale data to learn continuous bird\u2019s-eye-view representations capturing elevation, semantics, and instance-level features. Second, it incorporates a counterfactual-based loss and active learning procedure to focus on the perceptual cues that matter most by querying humans for counterfactual trajectory annotations in challenging scenes. We evaluate CREStE in kilometer-scale navigation tasks across six distinct urban environments. Our experiments demonstrate that CREStE achieves more efficient navigation and requires fewer human interventions than existing approaches, showcasing its robustness and effectiveness for long-horizon mapless navigation.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p136.pdf",
        "supp": "",
        "pdf_size": 19746927,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16561121033979120822&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/136/"
    },
    {
        "title": "Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies",
        "session": "Imitation Learning I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "73",
        "author": "Chen Xu; Tony Khuong Nguyen; Emma Dixon; Christopher Rodriguez; Patrick Miller; Robert Lee; Paarth Shah; Rares Andrei Ambrus; Haruki Nishimura; Masha Itkina",
        "abstract": "Recent years have witnessed impressive robotic manipulation systems driven by advances in imitation learning and generative modeling, such as diffusion- and flow-based approaches. As robot policy performance increases, so does the complexity and time horizon of achievable tasks, inducing unexpected and diverse failure modes that are difficult to predict a priori. To enable trustworthy policy deployment in safety-critical human environments, reliable runtime failure detection becomes important during policy inference. However, most existing failure detection approaches rely on prior knowledge of failure modes and require failure data during training, which imposes a significant challenge in practicality and scalability. In response to these limitations, we present FAIL-Detect, a modular two-stage approach for failure detection in imitation learning-based robotic manipulation. To accurately identify failures from successful training data, we frame the problem as sequential out-of-distribution (OOD) detection. We first distill policy inputs and outputs into scalar signals that correlate with policy failures and capture epistemic uncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile framework for uncertainty quantification with statistical guarantees. Empirically, we thoroughly investigate both learned and post-hoc scalar signal candidates on diverse robotic manipulation tasks. Our experiments show learned signals based on adapted random networks and a novel flow-based density estimator to be most effective. Furthermore, our method detects failures more accurately and faster than state-of-the-art (SOTA) failure detection baselines. These results highlight the potential of FAIL-Detect to enhance the safety and reliability of imitation learning-based robotic systems as they progress toward real-world deployment.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p073.pdf",
        "supp": "",
        "pdf_size": 6199777,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13327785187115495398&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Toyota Research Institute (TRI); Toyota Research Institute (TRI); Toyota Research Institute (TRI); Toyota Research Institute (TRI); Toyota Research Institute (TRI); Woven by Toyota (WbyT); Toyota Research Institute (TRI); Toyota Research Institute (TRI); Toyota Research Institute (TRI); Toyota Research Institute (TRI)",
        "aff_domain": "tri.global; ; ; ; ; ; ; ; ;",
        "email": "tri.global; ; ; ; ; ; ; ; ;",
        "github": "",
        "project": "https://cxu-tri.github.io/FAILDetect-Website/",
        "author_num": 10,
        "oa": "https://roboticsconference.org/program/papers/73/",
        "aff_unique_index": "0;0;0;0;0;1;0;0;0;0",
        "aff_unique_norm": "Toyota Research Institute;Toyota",
        "aff_unique_dep": ";Woven by Toyota",
        "aff_unique_url": "https://www.tri.toyota.com/;https://www.toyota-global.com",
        "aff_unique_abbr": "TRI;WbyT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;1;0;0;0;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "title": "Certifiably-Correct Mapping for Safe Navigation Despite Odometry Drift",
        "session": "Perception and Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "7",
        "author": "Devansh R. Agrawal; Rajiv Govindjee; Taekyung Kim; Trushant Adeshara; Jiangbo Yu; Anurekha Ravikumar; Dimitra Panagou",
        "abstract": "Accurate perception, state estimation and mapping are essential for safe robotic navigation as planners and controllers rely on these components for safety critical decisions. However, existing mapping approaches often assume perfect pose estimates, an unrealistic assumption that can lead to incorrect obstacle maps and therefore collisions. This paper introduces a framework for certifiably-correct mapping that ensures that the obstacle map correctly classifies obstacle-free regions despite the odometry drift in vision-based localization systems (VIO/SLAM). By deflating the safe region based on the incremental odometry error at each timestep, we ensure that the map remains accurate and reliable locally around the robot, even as the overall odometry error with respect to the inertial frame grows unbounded.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p007.pdf",
        "supp": "",
        "pdf_size": 7557170,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:wR8Yp7lrvHIJ:scholar.google.com/&scioq=Certifiably-Correct+Mapping+for+Safe+Navigation+Despite+Odometry+Drift&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor",
        "aff_domain": "umich.edu; ; ; ; ; ; ",
        "email": "umich.edu; ; ; ; ; ; ",
        "github": "https://github.com/dasc-lab/certifiably-correct-mapping",
        "project": "https://youtu.be/qMlDK7Iou48",
        "author_num": 7,
        "oa": "https://roboticsconference.org/program/papers/7/",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity",
        "session": "Imitation Learning I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "72",
        "author": "Guang Yin; Yitong Li; Yixuan Wang; Dale Mcconachie; Paarth Shah; Kunimatsu Hashimoto; Huan Zhang; Katherine Liu; Yunzhu Li",
        "abstract": "Natural language instructions for robotic manipulation tasks often exhibit ambiguity and vagueness. For instance, the instruction \u201cHang a mug on the mug tree\u201d may involve multiple valid actions if there are several mugs and branches to choose from. Existing language-conditioned policies typically rely on end-to-end models that jointly handle high-level semantic understanding and low-level action generation, which can result in suboptimal performance due to their lack of modularity and interpretability. To address these challenges, we introduce a novel robotic manipulation framework that can accomplish tasks specified by potentially ambiguous natural language. This framework employs a Vision-Language Model (VLM) to interpret abstract concepts in natural language instructions and generates task-specific code \u2014 an interpretable and executable intermediate representation. The generated code interfaces with the perception module to produce 3D attention maps that highlight task-relevant regions by integrating spatial and semantic information, effectively resolving ambiguities in instructions. Through extensive experiments, we identify key limitations of current imitation learning methods, such as poor adaptation to language and environmental variations. We show that our approach excels across challenging manipulation tasks involving language ambiguity, contact-rich manipulation, and multi-object interactions.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p072.pdf",
        "supp": "",
        "pdf_size": 2199755,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ivwbCZyeS9gJ:scholar.google.com/&scioq=CodeDiffuser:+Attention-Enhanced+Diffusion+Policy+via+VLM-Generated+Code+for+Instruction+Ambiguity&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "University of Illinois Urbana-Champaign; Tsinghua University; Columbia University; Toyota Research Institute; Toyota Research Institute; Toyota Research Institute; University of Illinois Urbana-Champaign; Toyota Research Institute; Columbia University",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "https://robopil.github.io/code-diffuser/",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/72/",
        "aff_unique_index": "0;1;2;3;3;3;0;3;2",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Tsinghua University;Columbia University;Toyota Research Institute",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://illinois.edu;https://www.tsinghua.edu.cn;https://www.columbia.edu;https://www.tri.global",
        "aff_unique_abbr": "UIUC;THU;Columbia;TRI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;1;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Coherance-based Approximate Derivatives via Web of Affine Spaces Optimization",
        "session": "Scaling Robot Learning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "24",
        "author": "Daniel Rakita; Chen Liang; Qian Wang",
        "abstract": "Computing derivatives is a crucial subroutine in computer science and related fields as it provides a local characterization of a function\u2019s steepest directions of ascent or descent.  In this work, we recognize that derivatives are often not computed in isolation; conversely, it is quite common to compute a sequence of derivatives, each one somewhat related to the last.  Thus, we propose accelerating derivative computation by reusing information from previous, related calculations\u2014a general strategy known as coherence.  We introduce the first instantiation of this strategy through a novel approach called the Web of Affine Spaces (WASP) Optimization.  This approach provides an accurate approximation of a function\u2019s derivative object (i.e. gradient, Jacobian matrix, etc.) at the current input within a sequence.  Each derivative within the sequence only requires a small number of forward passes through the function (typically two), regardless of the number of function inputs and outputs.  We demonstrate the efficacy of our approach through several numerical experiments, comparing it with alternative derivative computation methods on benchmark functions.  We show that our method significantly improves the performance of derivative computation on small to medium-sized functions, i.e., functions with approximately fewer than 500 combined inputs and outputs.  Furthermore, we show that this method can be effectively applied in a robotics optimization context. We conclude with a discussion of the limitations and implications of our work.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p024.pdf",
        "supp": "",
        "pdf_size": 1628885,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:YzHwy_DCLL4J:scholar.google.com/&scioq=Coherance-based+Approximate+Derivatives+via+Web+of+Affine+Spaces+Optimization&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, Yale University; Department of Computer Science, Yale University; Department of Computer Science, Yale University",
        "aff_domain": "yale.edu;yale.edu;yale.edu",
        "email": "yale.edu;yale.edu;yale.edu",
        "github": "",
        "project": "https://apollo-lab-yale.github.io/25-RSS-WASP-website/",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/24/",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Yale University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.yale.edu",
        "aff_unique_abbr": "Yale",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Collaborative Object Transportation in Space via Impact Interactions",
        "session": "Multi-Robot Systems",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "97",
        "author": "Joris Verhagen; Jana Tumova",
        "abstract": "We present a planning and control approach for collaborative transportation of objects in space by a team of robots. Object and robots in microgravity environments are not subject to friction but are instead free-floating. This property is key to how we approach the transportation problem: the passive objects are controlled by impact interactions with the controlled robots. In particular, given a high-level Signal Temporal Logic (STL) specification of the transportation task, we synthesize motion plans for the robots to maximize the specification satisfaction in terms of spatial STL robustness. Given that the physical impact interactions are complex and hard to model precisely, we also present an alternative formulation maximizing the permissible uncertainty in a simplified kinematic impact model. We define the full planning and control stack required to solve the object transportation problem; an offline planner, an online replanner, and a low-level model-predictive control scheme for each of the robots.  We show the method in a high-fidelity simulator for a variety of scenarios and present experimental validation of 2-robot, 1-object scenarios on a freeflyer platform.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p097.pdf",
        "supp": "",
        "pdf_size": 32986394,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:WCajONwiSwIJ:scholar.google.com/&scioq=Collaborative+Object+Transportation+in+Space+via+Impact+Interactions&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden",
        "aff_domain": "kth.se;kth.se",
        "email": "kth.se;kth.se",
        "github": "https://joris997.github.io/impact stl/",
        "project": "",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/97/",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "KTH Royal Institute of Technology",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.kth.se",
        "aff_unique_abbr": "KTH",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stockholm",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "title": "Complementarity-Free Multi-Contact Modeling and Optimization for Dexterous Manipulation",
        "session": "Manipulation II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "111",
        "author": "Wanxin Jin",
        "abstract": "A significant barrier preventing model-based methods from achieving real-time and versatile dexterous robotic manipulation is the inherent complexity of multi-contact dynamics. Traditionally formulated as complementarity models, multi-contact dynamics introduces  non-smoothness and combinatorial complexity, complicating contact-rich planning and  optimization. In this paper, we circumvent these challenges by introducing a lightweight yet capable multi-contact model. Our new model, derived from the duality of optimization-based contact models, dispenses with the complementarity constructs entirely, providing computational advantages such as closed-form time stepping, differentiability, automatic satisfaction with Coulomb\u2019s friction law, and minimal hyperparameter tuning. We demonstrate the model\u2019s effectiveness and efficiency for planning and control in a range of challenging dexterous manipulation tasks, including fingertip 3D in-air  manipulation, TriFinger in-hand manipulation, and Allegro hand on-palm reorientation, all performed with diverse objects. Our method consistently achieves state-of-the-art results: (I) a 96.5% average success rate across all objects and tasks, (II) high manipulation accuracy with an average reorientation error of 11\u00b0 and position error of 7.8 mm, and (III) contact-implicit model predictive control running at 50-100 Hz for all objects and tasks. These results are achieved  with minimal hyperparameter tuning.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p111.pdf",
        "supp": "",
        "pdf_size": 14984215,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3873638951218165535&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Arizona State University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://roboticsconference.org/program/papers/111/",
        "aff_unique_index": "0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy",
        "session": "VLA Models",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "19",
        "author": "Yuhui Chen; Shuai Tian; Shugao Liu; Yingting Zhou; Haoran Li; Dongbin Zhao",
        "abstract": "Vision-Language-Action (VLA) models have shown substantial potential in real-world robotic manipulation. However, fine-tuning these models through supervised learning struggles to achieve robust performance due to limited, inconsistent demonstrations, especially in contact-rich environments. In this paper, we propose a reinforced fine-tuning approach for VLA models, named ConRFT, which consists of offline and online fine-tuning with a unified consistency-based training objective, to address these challenges. In the offline stage, our method integrates behavior cloning and Q-learning to effectively extract policy from limited demonstrations and stabilize value estimating. In the online stage, the VLA model is further fine-tuned via consistency policy, with human interventions to ensure safe exploration and high sample efficiency. We evaluate our approach on eight diverse real-world manipulation tasks. It achieves an average success rate of 96.3 % within 45\u201390 minutes of online fine-tuning, outperforming prior supervised methods with a 144 % improvement in success rate and 1.9x shorter episode length. This work highlights the potential of integrating reinforcement learning to enhance the performance of VLA models for real-world robotic applications.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p019.pdf",
        "supp": "",
        "pdf_size": 13593230,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4775884256553924030&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "SKL-MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China + School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; SKL-MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China + School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; SKL-MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China + School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; SKL-MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China + School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; SKL-MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China + School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; SKL-MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China + School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "ia.ac.cn;ia.ac.cn;ia.ac.cn;ia.ac.cn;ia.ac.cn;ia.ac.cn",
        "email": "ia.ac.cn;ia.ac.cn;ia.ac.cn;ia.ac.cn;ia.ac.cn;ia.ac.cn",
        "github": "",
        "project": "https://cccedric.github.io/conrft/",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/19/",
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World",
        "session": "Manipulation II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "110",
        "author": "Yankai Fu; Ning Chen; Qiuxuan Feng; Zichen Zhou; Mengzhen Liu; Mingdong Wu; Tianxing Chen; Shanyu Rong; Jiaming Liu; Hao Dong; Shanghang Zhang",
        "abstract": "Achieving human-level dexterity in robots is a key objective in the field of robotic manipulation. Recent advancements in 3D-based imitation learning have shown promising results, providing an effective pathway to achieve this goal. However, obtaining high-quality 3D representations presents two key problems:  (1) the quality of point clouds captured by a single-view camera is significantly affected by factors such as camera resolution, positioning, and occlusions caused by the dexterous hand; (2) the global point clouds lack crucial contact information and spatial correspondences, which are necessary for fine-grained dexterous manipulation tasks. To eliminate these limitations, we propose  CordViP, a novel framework that constructs and learns correspondences by leveraging the robust 6D pose estimation of objects and robot proprioception. Specifically, we first introduce the interaction-aware point clouds, which establish correspondences between the object and the hand. These point clouds are then used for our pretraining strategy, where we also incorporate object-centric contact maps and hand-arm coordination information, effectively capturing both spatial and temporal dynamics.  Our method demonstrates exceptional dexterous manipulation capabilities with an average success rate of 90% in four real-world tasks, surpassing other baselines by a large margin. Experimental results also highlight the superior generalization and robustness of CordViP to different objects, viewpoints, and scenarios.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p110.pdf",
        "supp": "",
        "pdf_size": 13995460,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1250653898385854242&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; The University of Hong Kong; State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University+Beijing Academy of Artificial Intelligence",
        "aff_domain": ";;;;;;;;;;",
        "email": ";;;;;;;;;;",
        "github": "",
        "project": "https://aureleopku.github.io/CordViP",
        "author_num": 11,
        "oa": "https://roboticsconference.org/program/papers/110/",
        "aff_unique_index": "0;0;0;0;0;0;1;0;0;0;0+2",
        "aff_unique_norm": "Peking University;University of Hong Kong;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "School of Computer Science;;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.hku.hk;https://www.baaic.cn",
        "aff_unique_abbr": "PKU;HKU;BAAI",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Curating Demonstrations using Online Experience",
        "session": "Imitation Learning I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "71",
        "author": "Annie S. Chen; Alec M. Lessing; Yuejiang Liu; Chelsea Finn",
        "abstract": "Many robot demonstration datasets contain heterogeneous demonstrations of varying quality. This heterogeneity may benefit policy pre-training, but can hinder robot performance when used with a final imitation learning objective. In particular, some strategies in the data may be less reliable than others or may be underrepresented in the data, leading to poor performance when such strategies are sampled at test time. Moreover, such unreliable or underrepresented strategies can be difficult even for people to discern, and sifting through demonstration datasets is time-consuming and costly. On the other hand, policy performance when trained on such demonstrations can reflect the reliability of different strategies. We thus propose for robots to self-curate based on online robot experience (Demo-SCORE). More specifically, we train and cross-validate a classifier to discern successful policy roll-outs from unsuccessful ones and use the classifier to filter heterogeneous demonstration datasets. Our experiments on simulated and real world manipulation tasks show that Demo-SCORE can effectively identify suboptimal demonstrations without manual curation. Notably, Demo-SCORE achieves over 15-35% higher absolute success rate in the resulting policy compared to the base policy trained with all original demonstrations.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p071.pdf",
        "supp": "",
        "pdf_size": 5146745,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15935271266907867698&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu; ; ; ",
        "email": "stanford.edu; ; ; ",
        "github": "",
        "project": "https://anniesch.github.io/demo-score/",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/71/",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot Trajectories",
        "session": "Imitation Learning I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "78",
        "author": "Jean-Baptiste Bouvier; Kanghyun Ryu; Qiayuan Liao; Koushil Sreenath; Negar Mehr",
        "abstract": "Diffusion models excel at creating images and videos thanks to their multimodal generative capabilities, which have also attracted the interest of roboticists for trajectory planning and policy learning. However, the stochastic nature of diffusion models is fundamentally at odds with the precise dynamical equations describing the feasible motion of robots. Hence, generating dynamically admissible robot trajectories is a challenge for diffusion models. To alleviate this issue, we introduce DDAT: Diffusion policies for Dynamically Admissible Trajectories to generate provably admissible trajectories of black-box robotic systems using diffusion models. A sequence of states is a dynamically admissible trajectory if each state of the sequence belongs to the reachable set of its predecessor by the robot\u2019s equations of motion. To generate such trajectories our diffusion policies project their predictions onto a dynamically admissible manifold during both training and inference to align the objective of the denoiser neural network with the dynamical admissibility constraint. These projections are challenging due to their autoregressive character and because the black-box nature of the dynamics prevents an exact characterization of the reachable sets. We thus enforce admissibility by iteratively sampling a polytopic underapproximation of the reachable set of a state onto which we project its predicted successor, before iterating this process with the projected successor. By generating accurate trajectories, this projection rids diffusion models of their unceasing replanning to enable one-shot long-horizon trajectory planning. We demonstrate that our proposed framework generates higher quality dynamically admissible robot trajectories through extensive simulations on a quadcopter and various MuJoCo environments, along with real-world experiments on a Unitree GO1.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p078.pdf",
        "supp": "",
        "pdf_size": 2802375,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15310689776116970636&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://iconlab.negarmehr.com/DDAT/",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/78/"
    },
    {
        "title": "DOGlove: Dexterous Manipulation with a Low-Cost Open-Source Haptic Force Feedback Glove",
        "session": "Manipulation II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "104",
        "author": "Han Zhang; Songbo Hu; Zhecheng Yuan; Huazhe Xu",
        "abstract": "Dexterous hand teleoperation plays a pivotal role in enabling robots to achieve human-level manipulation dexterity. However, current teleoperation systems often rely on expensive equipment and lack multi-modal sensory feedback, restricting human operators\u2019 ability to perceive object properties and perform complex manipulation tasks. To address these limitations, we present DOGlove, a low-cost, precise, and haptic force feedback glove system for teleoperation and manipulation. DOGlove can be assembled in hours at a cost under 600 USD. It features a customized joint structure for 21-DoF motion capture, a compact cable-driven torque transmission mechanism for 5-DoF multidirectional force feedback, and a linear resonate actuator for 5-DoF fingertip haptic feedback. Leveraging action and haptic force retargeting, DOGlove enables precise and immersive teleoperation of dexterous robotic hands, achieving high success rates in complex, contact-rich tasks. We further evaluate DOGlove in scenarios without visual feedback, demonstrating the critical role of haptic force feedback in task performance. In addition, we utilize the collected demonstrations to train imitation learning policies, highlighting the potential and effectiveness of DOGlove. DOGlove\u2019s hardware and software system is fully open-sourced.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p104.pdf",
        "supp": "",
        "pdf_size": 5444587,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15245281268933553689&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Tsinghua University+Shanghai Qi Zhi Institute+Shanghai AI Lab; Tsinghua University+Shanghai Qi Zhi Institute; Tsinghua University+Shanghai Qi Zhi Institute+Shanghai AI Lab; Tsinghua University+Shanghai Qi Zhi Institute+Shanghai AI Lab",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://do-glove.github.io/",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/104/",
        "aff_unique_index": "0+1+2;0+1;0+1+2;0+1+2",
        "aff_unique_norm": "Tsinghua University;Shanghai Qi Zhi Institute;Shanghai AI Lab",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.qz.io;https://www.shanghaiailab.com",
        "aff_unique_abbr": "THU;;SAIL",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "DRO: Doppler-Aware Direct Radar Odometry with Gyroscope",
        "session": "Perception and Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "6",
        "author": "Cedric Le Gentil; Leonardo Brizi; Daniil Lisus; Xinyuan Qiao; Giorgio Grisetti; Timothy Barfoot",
        "abstract": "A renaissance in radar-based sensing for mobile robotic applications is underway. Compared to cameras or lidars, millimetre-wave radars have the ability to `see\u2019 through thin walls, vegetation, and adversarial weather conditions such as heavy rain, fog, snow, and dust. In this paper, we propose a novel SE(2) odometry approach for spinning frequency-modulated continuous-wave radars. With the aid of a gyroscope, our method performs scan-to-local-map registration of the incoming radar data in a direct manner using all the radar intensity information, and without the need for feature or point cloud extraction. The method performs locally continuous trajectory estimation and accounts for both motion and Doppler distortion of the radar scans. If the radar used possesses a specific frequency modulation pattern that makes radial Doppler velocities observable, an additional Doppler-based constraint is formulated to improve the velocity estimate and enable odometry in geometrically degenerated scenarios (e.g., featureless tunnels). Our method has been validated on over 250km of on-road data, sourced from public datasets (Boreas and MulRan) as well as data collected using our automotive platform. It outperforms state-of-the-art approaches on the Boreas leaderboard with an average translation error of 0.46% without the Doppler-based velocity constraint. When using data with the appropriate Doppler-enabling frequency modulation pattern, the translation error is reduced to 0.29% in similar environments thanks to our novel velocity constraint. We also benchmarked our algorithm using 1.5 hours of data collected with a mobile robot in off-road environments with various levels of structure to demonstrate its versatility.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p006.pdf",
        "supp": "",
        "pdf_size": 2820929,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Autonomous Space Robotics Lab, University of Toronto Institute for Aerospace Studies (UTIAS), Toronto, Ontario, Canada+Department of Computer, Control, and Management Engineering \u201cAntonio Ruberti\u201d, Sapienza University of Rome; Department of Computer, Control, and Management Engineering \u201cAntonio Ruberti\u201d, Sapienza University of Rome; Autonomous Space Robotics Lab, University of Toronto Institute for Aerospace Studies (UTIAS), Toronto, Ontario, Canada; Autonomous Space Robotics Lab, University of Toronto Institute for Aerospace Studies (UTIAS), Toronto, Ontario, Canada; Autonomous Space Robotics Lab, University of Toronto Institute for Aerospace Studies (UTIAS), Toronto, Ontario, Canada+Department of Computer, Control, and Management Engineering \u201cAntonio Ruberti\u201d, Sapienza University of Rome; Autonomous Space Robotics Lab, University of Toronto Institute for Aerospace Studies (UTIAS), Toronto, Ontario, Canada",
        "aff_domain": "utoronto.ca;diag.uniroma1.it;utoronto.ca;utoronto.ca;diag.uniroma1.it;utoronto.ca",
        "email": "utoronto.ca;diag.uniroma1.it;utoronto.ca;utoronto.ca;diag.uniroma1.it;utoronto.ca",
        "github": "https://github.com/utiasASRL/dro",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/6/",
        "aff_unique_index": "0+1;1;0;0;0+1;0",
        "aff_unique_norm": "University of Toronto Institute for Aerospace Studies;Sapienza University of Rome",
        "aff_unique_dep": "Autonomous Space Robotics Lab;Department of Computer, Control, and Management Engineering \"Antonio Ruberti\"",
        "aff_unique_url": "https://www.ias.uToronto.ca;https://www.uniroma1.it",
        "aff_unique_abbr": "UTIAS;Sapienza",
        "aff_campus_unique_index": "0+1;1;0;0;0+1;0",
        "aff_campus_unique": "Toronto;Rome",
        "aff_country_unique_index": "0+1;1;0;0;0+1;0",
        "aff_country_unique": "Canada;Italy"
    },
    {
        "title": "DVS: Dynamic Virtual-Real Simulation Platform for Mobile Robotic Tasks",
        "session": "Mobile Manipulation and Locomotion",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "129",
        "author": "Zijie Zheng; Zeshun Li; Yunpeng Wang; Qinghongbing Xie; Long Zeng",
        "abstract": "With the development of Embodied AI, robotic research has increasingly focused on complex tasks. Existing simulation platforms, however, are often limited to idealized environments, single-task scenarios and lack data interoperability. This restricts task decomposition and multi-task learning. Additionally, current Simulation Platforms face challenges in dynamic pedestrian modeling, scene editability, and synchronization between virtual and real assets. These limitations hinder real-world robot deployment and feedback. To address these challenges, we propose DVS (Dynamic Virtual-Real Simulation Platform), a platform for dynamic virtual-real synchronization in mobile robotic tasks. DVS integrates a random pedestrian behavior modeling plugin and large-scale, customizable indoor scenes for generating annotated training datasets. It features a optical motion capture system, synchronizing object poses and coordinates between virtual and real worlds to support dynamic task benchmarking. Experimental validation shows that DVS supports tasks such as pedestrian trajectory prediction, robot path planning, and robotic arm  grasping, with potential for both simulation and real-world deployment. In this way, DVS represents more than just a versatile robotic platform; it paves the way for research in human intervention in robot execution tasks and real-time feedback algorithms in virtual-real fusion environments.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p129.pdf",
        "supp": "",
        "pdf_size": 2254707,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:8gV16lPaiGoJ:scholar.google.com/&scioq=DVS:+Dynamic+Virtual-Real+Simulation+Platform+for+Mobile+Robotic+Tasks&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://immvlab.github.io/DVS/",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/129/",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Debiasing 6-DOF IMU via Hierarchical Learning of Continuous Bias Dynamics",
        "session": "Perception and Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "9",
        "author": "Ben Liu; Tzu-Yuan Lin; Wei Zhang; Maani Ghaffari",
        "abstract": "This paper develops a deep learning approach to the online debiasing of IMU gyroscopes and accelerometers. Most existing methods rely on implicitly learning a bias term to compensate for raw IMU data. Explicit bias learning has recently shown its potential as a more interpretable and motion-independent alternative. However, it remains underexplored and faces challenges, particularly the need for ground truth bias data, which is rarely available. To address this, we propose a neural ordinary differential equation (NODE) framework that explicitly models continuous bias dynamics, requiring only pose ground truth, often available in datasets. This is achieved by extending the canonical NODE framework to the matrix Lie group for IMU kinematics with a hierarchical training strategy. The validation on two public datasets and one real-world experiment demonstrates significant accuracy improvements in IMU measurements, reducing errors in both pure IMU integration and visual-inertial odometry.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p009.pdf",
        "supp": "",
        "pdf_size": 5633184,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:kyIsDvPDOC4J:scholar.google.com/&scioq=Debiasing+6-DOF+IMU+via+Hierarchical+Learning+of+Continuous+Bias+Dynamics&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Southern University of Science and Technology; University of Michigan; Southern University of Science and Technology\u2020; University of Michigan",
        "aff_domain": "mail.sustech.edu.cn;umich.edu;sustech.edu.cn;umich.edu",
        "email": "mail.sustech.edu.cn;umich.edu;sustech.edu.cn;umich.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/9/",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Southern University of Science and Technology;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sustech.edu.cn;https://www.umich.edu",
        "aff_unique_abbr": "SUSTech;UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning",
        "session": "Imitation Learning II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "157",
        "author": "Zhengrong Xue; Shuying Deng; Zhenyang Chen; Yixuan Wang; Zhecheng Yuan; Huazhe Xu",
        "abstract": "Visuomotor policies have shown great promise in robotic manipulation but often require substantial amounts of human-collected data for effective performance. A key reason underlying the data demands is their limited spatial generalization capability, which necessitates extensive data collection across different object configurations. In this work, we present",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p157.pdf",
        "supp": "",
        "pdf_size": 19070693,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10079568588957508059&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/157/"
    },
    {
        "title": "Demonstrating Arena 5.0: A Photorealistic ROS2 Simulation Framework for Developing and Benchmarking Social Navigation",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "92",
        "author": "Linh K\u00e4stner; Volodymyr Shcherbyna; Harold Soh; Giang Nguyen Huu Truong; Do Duc Anh; Ton Manh Kien; Tim Seeger; Ahmed Martban; Vu Thanh Lam; Nguyen Quoc Hung; Pham Thai Hoang Tung; Tran Dang An; Eva Wiese; Maximilian Ho-Kyoung Schreff",
        "abstract": "Building upon the foundations laid by our previous work, this paper introducesArena 5.0, the fifth iteration of our framework for robotics social navigation development and benchmarking. Arena 5.0 provides three main contributions: 1) The complete integration of NVIDIA Isaac Gym, enabling photorealistic simulations and more efficient training. It seamlessly incorporates Isaac Gym into the Arena platform, allowing the use of existing modules such as randomized environment generation, evaluation tools, ROS2 support, and the integration of planners, robot models, and APIs within Isaac Gym. 2) A comprehensive benchmark of state-of-the-art social navigation strategies, evaluated on a diverse set of generated and customized worlds and scenarios of varying difficulty levels. These benchmarks provide a detailed assessment of navigation planners using a wide range of social navigation metrics. 3) An extensive set of modules for specified and highly customizable scenario generation and task planning facilitating improved and customizable generation of social navigation scenarios, such as emergency and rescue situations. The platform\u2019s performance was evaluated by generating the aforementioned benchmark and through a comprehensive user study, demonstrating significant improvements in usability and efficiency compared to previous versions. Arena 5.0 is open source and available at https://github.com/Arena-Rosnav.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p092.pdf",
        "supp": "",
        "pdf_size": 26904265,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;",
        "github": "https://github.com/Arena-Rosnav",
        "project": "",
        "author_num": 14,
        "oa": "https://roboticsconference.org/program/papers/92/"
    },
    {
        "title": "Demonstrating Berkeley Humanoid Lite: An Open-source, Accessible, and Customizable 3D-printed Humanoid Robot",
        "session": "Humanoids",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "62",
        "author": "Yufeng Chi; Qiayuan Liao; Junfeng Long; Xiaoyu Huang; Sophia Shao; Borivoje Nikolic; Zhongyu Li; Koushil Sreenath",
        "abstract": "Despite significant interest and advancements in humanoid robotics, most existing commercially available hardware remains high-cost, closed-source, and non-transparent within the robotics community. This lack of accessibility and customization hinders the growth of the field and the broader development of humanoid technologies. To address these challenges and promote democratization in humanoid robotics, we demonstrate Berkeley Humanoid Lite, an open-source humanoid robot designed to be accessible, customizable, and beneficial for the entire community. The core of this design is a modular 3D-printed gearbox for the actuators and robot body. All components can be sourced from widely available e-commerce platforms and fabricated using standard desktop 3D printers, keeping the total hardware cost under $6,000 (based on U.S. market prices). The design emphasizes modularity and ease of fabrication. To address the inherent limitations of 3D-printed gearboxes, such as reduced strength and durability compared to metal alternatives, we adopted a cycloidal gear design, which provides an optimal form factor in this context. Extensive testing was conducted on the 3D-printed actuators to validate their durability and alleviate concerns about the reliability of plastic components. To demonstrate the capabilities of Berkeley Humanoid Lite, we conducted a series of experiments, including the development of a locomotion controller using reinforcement learning. These experiments successfully showcased zero-shot policy transfer from simulation to hardware, highlighting the platform\u2019s suitability for research validation. By making the hardware design, embedded code, and training and deployment frameworks fully open-source and globally accessible, we aim for Berkeley Humanoid Lite to serve as a pivotal step toward democratizing the development of humanoid robotics.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p062.pdf",
        "supp": "",
        "pdf_size": 29074124,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:DYYpWok8GCAJ:scholar.google.com/&scioq=Demonstrating+Berkeley+Humanoid+Lite:+An+Open-source,+Accessible,+and+Customizable+3D-printed+Humanoid+Robot&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://roboticsconference.org/program/papers/62/"
    },
    {
        "title": "Demonstrating CavePI: Autonomous Exploration of Underwater Caves by Semantic Guidance",
        "session": "Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "141",
        "author": "Alankrit Gupta; Adnan Abdullah; Xianyao Li; Vaishnav Ramesh; Ioannis Rekleitis; Md Jahidul Islam",
        "abstract": "Enabling autonomous robots to navigate, explore, and map underwater caves safely and efficiently is of significant importance to marine robotics and archaeology. In this work, we demonstrate the system design and algorithmic integration of a visual servoing capability for semantically guided autonomous underwater cave exploration. We present the hardware and edge-AI design considerations to enable this feature on a novel 6-DOF robot named CavePI. The guided navigation is driven by a computationally light yet robust AI-based perception module, delivering a rich semantic understanding of the environment. Subsequently, a robust control mechanism enables CavePI to track the semantic guides and navigate inside complex cave environments. We evaluate the CavePI system through field experiments in natural underwater caves and spring-water sites, and further validate its ROS (Robot Operating System)-based digital twin in a simulation environment. Our results highlight how these integrated design choices facilitate reliable navigation under feature-deprived, GPS-denied, and low-visibility conditions. The system design, code, and data are available on the project website: truncated for blind review.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p141.pdf",
        "supp": "",
        "pdf_size": 19903057,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:indKc3LlgesJ:scholar.google.com/&scioq=Demonstrating+CavePI:+Autonomous+Exploration+of+Underwater+Caves+by+Semantic+Guidance&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "RoboPI laboratory, Department of ECE, University of Florida, FL 32611, USA; RoboPI laboratory, Department of ECE, University of Florida, FL 32611, USA; RoboPI laboratory, Department of ECE, University of Florida, FL 32611, USA; RoboPI laboratory, Department of ECE, University of Florida, FL 32611, USA; Department of ME, University of Delaware, Newark, DE 19716, USA; RoboPI laboratory, Department of ECE, University of Florida, FL 32611, USA",
        "aff_domain": "ufl.edu;ufl.edu;ufl.edu;ufl.edu;udel.edu;ece.ufl.edu",
        "email": "ufl.edu;ufl.edu;ufl.edu;ufl.edu;udel.edu;ece.ufl.edu",
        "github": "https://github.com/uf-robopi/CavePI AUV/",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/141/",
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "University of Florida;University of Delaware",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Mechanical Engineering",
        "aff_unique_url": "https://www.ufl.edu;https://www.udel.edu",
        "aff_unique_abbr": "UF;UD",
        "aff_campus_unique_index": "0;0;0;0;1;0",
        "aff_campus_unique": "FL;Newark",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Demonstrating GPU Parallelized Robot Simulation and Rendering for Generalizable Embodied AI with ManiSkill3",
        "session": "Scaling Robot Learning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "21",
        "author_site": "Stone Tao; Fanbo Xiang; Arth Shukla; Yuzhe Qin; Xander Hinrichsen; Xiaodi Yuan; Chen Bao; Xinsong Lin; Yulin Liu; Tse-Kai Chan; Yuan Gao; Xuanlin Li; Tongzhou Mu; Nan Xiao; Arnav Gurha; Viswesh N.; Yong Woo Choi; Yen-Ru Chen; Zhiao Huang; Roberto",
        "author": "Stone Tao; Fanbo Xiang; Arth Shukla; Yuzhe Qin; Xander Hinrichsen; Xiaodi Yuan; Chen Bao; Xinsong Lin; Yulin Liu; Tse-Kai Chan; Yuan Gao; Xuanlin Li; Tongzhou Mu; Nan Xiao; Arnav Gurha; Viswesh N.; Yong Woo Choi; Yen-Ru Chen; Zhiao Huang; Roberto Calandra; Rui Chen; Shan Luo; Hao Su",
        "abstract": "Simulation has enabled unprecedented compute-scalable approaches to robot learning. However, many existing simulation frameworks typically support a narrow range of scenes/tasks and lack features critical for scaling generalizable robotics and sim2real. We introduce and open source ManiSkill3, the fastest state-visual GPU parallelized robotics simulator with contact-rich physics targeting generalizable manipulation. ManiSkill3 supports GPU parallelization of many aspects including simulation+rendering, heterogeneous simulation, pointclouds/voxels visual input, and more. GPU Simulation with rendering on ManiSkill3 uses 2-3x less GPU memory usage than other platforms and achieves up to 30,000+ FPS in benchmarked environments due to minimal python/pytorch overhead in the system, simulation on the GPU, and the use of the SAPIEN parallel rendering system. Tasks that used to take hours to train can now take minutes. We further provide the most comprehensive range of GPU parallelized environments/tasks spanning 12 distinct domains including but not limited to mobile manipulation, drawing, humanoids, and dextrous manipulation in realistic scenes designed by artists or real-world digital twins. In addition, millions of demonstration frames are provided from motion planning, RL, and teleoperation. ManiSkill3 also provides a comprehensive set of baselines that span popular RL and learning-from-demonstrations algorithms.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p021.pdf",
        "supp": "",
        "pdf_size": 24942116,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; Carnegie Mellon University; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; University of California San Diego; TU Dresden; Tsinghua University; King\u2019s College London; University of California San Diego",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "maniskill.ai/",
        "author_num": 23,
        "oa": "https://roboticsconference.org/program/papers/21/",
        "aff_unique_index": "0;0;0;0;0;0;1;0;0;0;0;0;0;0;0;0;0;0;0;2;3;4;0",
        "aff_unique_norm": "University of California, San Diego;Carnegie Mellon University;Technische Universit\u00e4t Dresden;Tsinghua University;King's College London",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://ucsd.edu;https://www.cmu.edu;https://www.tu-dresden.de;https://www.tsinghua.edu.cn;https://www.kcl.ac.uk",
        "aff_unique_abbr": "UCSD;CMU;TUD;THU;KCL",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;1;2;3;0",
        "aff_country_unique": "United States;Germany;China;United Kingdom"
    },
    {
        "title": "Demonstrating LEAP Hand V3: Low-Cost, Easy-to-Assemble, High-Performance Hand for Robot Learning",
        "session": "Robot Design",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "132",
        "author": "Kenneth Shaw; Deepak Pathak",
        "abstract": "Replicating human-like dexterity in robotic hands has been a long-standing challenge in robotics. Recently, with the rise of robot learning and humanoids, the demand for dexterous robot hands to be reliable, affordable, and easy to reproduce has grown significantly. To address these needs, we present LEAP Hand V3, a $200 8-DOF highly dexterous robotic hand designed for robot learning research. It is strong yet compliant, using a hybrid rigid-soft structure that is very durable. Its universal dexterous MCP joint provides exceptional finger mobility, enabling a variety of different grasps. The parts are all 3D printed and can be assembled very easily in under two hours using our instructions. Importantly, we offer a suite of advanced open-source software tools to support robot learning research.  This includes human video retargeting code from MANO and Vision Pro, motion capture teleoperation code using the Manus Glove, and a URDF with sim2real examples for various simulation engines.  We will showcase LEAP Hand V3\u2014designed specifically for this demonstration\u2014alongside our previous robot hands with real robot interactive demos. Following our successful demos at RSS 2023 and 2024, we will again offer an engaging opportunity for attendees to get hands-on experience and information about the accessibility of low-cost, open-source robotic hands.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p132.pdf",
        "supp": "",
        "pdf_size": 10663513,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "https://leaphand.com",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/132/",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Demonstrating MOSART: Opening Articulated Structures in the Real World",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "33",
        "author": "Arjun Gupta; Michelle Zhang; Rishik Sathua; Saurabh Gupta",
        "abstract": "What does it take to build mobile manipulation systems that can competently operate on previously unseen objects in previously unseen environments? This work answers this question using opening of articulated structures as a mobile manipulation testbed. Specifically, our focus is on the end-to-end performance on this task without any privileged information, ie the robot starts at a location with the novel target articulated object in view, and has to approach the object and successfully open it. We first develop a system for this task, and then conduct 100+ end-to-end system tests across 13 real world test sites. Our large-scale study reveals a number of surprising findings: a) modular systems outperform end-to-end learned systems for this task, even when the end-to-end learned systems are trained on 1000+ demonstrations, b) perception, and not precise end-effector control, is the primary bottleneck to task success, and c) state-of-the-art articulation parameter estimation models developed in isolation struggle when faced with robot-centric viewpoints. Overall, our findings highlight the limitations of developing components of the pipeline in isolation and underscore the need for system-level research, providing a pragmatic roadmap for building generalizable mobile manipulation systems. Videos, code, and models are available on the project website: https://arjung128.github.io/opening-articulated-structures/.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p033.pdf",
        "supp": "",
        "pdf_size": 25596442,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/33/"
    },
    {
        "title": "Demonstrating MuJoCo Playground",
        "session": "Scaling Robot Learning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "20",
        "author": "Kevin Zakka; Baruch Tabanpour; Qiayuan Liao; Mustafa Haiderbhai; Samuel Holt; Jing Yuan Luo; Arthur Allshire; Erik Frey; Koushil Sreenath; Lueder Alexander Kahrs; Carmelo Sferrazza; Yuval Tassa; Pieter Abbeel",
        "abstract": "We introduce MuJoCo Playground, a fully open-source framework for robot learning built with MJX, with the express goal of streamlining simulation, training, and sim-to-real transfer onto robots. With a simple installation process, researchers can train policies in minutes on a single GPU. Playground supports diverse robotic platforms, including quadrupeds, humanoids, dexterous hands, and robotic arms, enabling zero-shot sim-to-real transfer from both state and pixel inputs. This is achieved through an integrated stack comprising a physics engine, batch renderer, and training environments.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p020.pdf",
        "supp": "",
        "pdf_size": 27010137,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "UC Berkeley+Google DeepMind; Google DeepMind+University of Toronto; UC Berkeley+Google DeepMind; University of Toronto+University of Cambridge; University of Cambridge; Google DeepMind; UC Berkeley; Google DeepMind; UC Berkeley; University of Toronto; UC Berkeley+Google DeepMind; Google DeepMind; UC Berkeley+Google DeepMind",
        "aff_domain": "; ; ; ; ; ; ; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "playground.mujoco.org",
        "author_num": 13,
        "oa": "https://roboticsconference.org/program/papers/20/",
        "aff_unique_index": "0+1;1+2;0+1;2+3;3;1;0;1;0;2;0+1;1;0+1",
        "aff_unique_norm": "University of California, Berkeley;Google;University of Toronto;University of Cambridge",
        "aff_unique_dep": ";Google DeepMind;;",
        "aff_unique_url": "https://www.berkeley.edu;https://deepmind.com;https://www.utoronto.ca;https://www.cam.ac.uk",
        "aff_unique_abbr": "UC Berkeley;DeepMind;U of T;Cambridge",
        "aff_campus_unique_index": "0;;0;2;2;0;0;0;0",
        "aff_campus_unique": "Berkeley;;Cambridge",
        "aff_country_unique_index": "0+1;1+2;0+1;2+1;1;1;0;1;0;2;0+1;1;0+1",
        "aff_country_unique": "United States;United Kingdom;Canada"
    },
    {
        "title": "Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success",
        "session": "Manipulation II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "107",
        "author": "Che Wang; Jeroen Vanbaar; Chaitanya Mitash; Shuai Li; Dylan Randle; Weiyao Wang; Sumedh Anand Sontakke; Kostas Bekris; Kapil Katyal",
        "abstract": "This work demonstrates how autonomously learning aspects of robotic operation from sparsely-labeled, real-world data of deployed, engineered solutions at industrial scale can provide with solutions that achieve improved performance.  Specifically, it focuses on multi-suction robot picking and performs a comprehensive study on the application of multi-modal visual encoders for predicting the success of candidate robotic picks.  Picking diverse items from unstructured piles is an important and challenging task for robot manipulation in real-world settings, such as warehouses. Methods for picking from clutter must work for an open set of items while simultaneously meeting latency constraints to achieve high throughput. The demonstrated approach utilizes multiple input modalities, such as RGB, depth and semantic segmentation, to estimate the quality of candidate multi-suction picks. The strategy is trained from real-world experience, i.e., given examples of successful and failed attempts to pick items. The training picks have been generated by an engineered strategy. A real-world limitation when learning in such live, industrial setups is that only a single or a few picks can be attempted per scene. The learning strategy first pretrains multi-modal visual models in a self-supervised manner to effectively reconstruct the input modalities in the target domain. A downstream model is then trained to evaluate the quality of multi-suction picks given the learned multi-modal embedding, while the multi-modal model is further fine-tuned. The manuscript provides comprehensive experimental evaluation performed over a large item-picking dataset, an item-picking dataset targeted to include partial occlusions, and a package-picking dataset, which focuses on containers, such as boxes and envelopes, instead of unpackaged items. The evaluation measures performance for different item configurations, pick scenes, and object types. Ablations help to understand the effects of in-domain pretraining, the impact of different modalities and the importance of finetuning. These ablations reveal both the importance of training over multiple modalities but also the ability of models to learn during pretraining the relationship between modalities so that during finetuning and inference, only a subset of them can be used as input.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p107.pdf",
        "supp": "",
        "pdf_size": 19666013,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/107/"
    },
    {
        "title": "Demonstrating REASSEMBLE: A Multimodal Dataset for Contact-rich Robotic Assembly and Disassembly",
        "session": "Manipulation I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "59",
        "author": "Daniel Sliwowski; Shail Jadav; Sergej Stanovcic; J\u0119drzej Orbik; Johannes Heidersberger; Dongheui Lee",
        "abstract": "Robotic manipulation remains a core challenge in robotics, particularly for contact-rich tasks such as industrial assembly and disassembly. Existing datasets have significantly advanced learning in manipulation but are primarily focused on simpler tasks like object rearrangement, falling short of capturing the complexity and physical dynamics involved in assembly and disassembly. To bridge this gap, we present REASSEMBLE (Robotic assEmbly disASSEMBLy datasEt), a new dataset designed specifically for contact-rich manipulation tasks. Built around the NIST Assembly Task Board 1 benchmark, REASSEMBLE includes four actions (pick, insert, remove, and place) involving 17 objects. The dataset contains 4,551 demonstrations, of which 4,035 were successful, spanning a total of 781 minutes. Our dataset features multimodal sensor data including event cameras, force-torque sensors, microphones, and multi-view RGB cameras. This diverse dataset supports research in areas such as learning contact-rich manipulation, task condition identification, action segmentation, and more. We believe REASSEMBLE will be a valuable resource for advancing robotic manipulation in complex, real-world scenarios. The dataset is publicly available on our project website: placeholder_URL.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p059.pdf",
        "supp": "",
        "pdf_size": 13628064,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=726477599916547248&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "Autonomous Systems Lab, Institute of Computer Technology, TU Wien, Vienna, Austria; Autonomous Systems Lab, Institute of Computer Technology, TU Wien, Vienna, Austria; Autonomous Systems Lab, Institute of Computer Technology, TU Wien, Vienna, Austria; Autonomous Systems Lab, Institute of Computer Technology, TU Wien, Vienna, Austria; Autonomous Systems Lab, Institute of Computer Technology, TU Wien, Vienna, Austria; Autonomous Systems Lab, Institute of Computer Technology, TU Wien, Vienna, Austria + Institute of Robotics and Mechatronics (DLR), German Aerospace Center, Wessling, Germany",
        "aff_domain": "tuwien.ac.at;tuwien.ac.at;tuwien.ac.at;tuwien.ac.at;tuwien.ac.at;tuwien.ac.at",
        "email": "tuwien.ac.at;tuwien.ac.at;tuwien.ac.at;tuwien.ac.at;tuwien.ac.at;tuwien.ac.at",
        "github": "",
        "project": "https://tuwien-asl.github.io/REASSEMBLE_page/",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/59/",
        "aff_unique_index": "0;0;0;0;0;0+1",
        "aff_unique_norm": "TU Wien;German Aerospace Center",
        "aff_unique_dep": "Institute of Computer Technology;Institute of Robotics and Mechatronics",
        "aff_unique_url": "https://www.tuwien.ac.at;https://www.dlr.de",
        "aff_unique_abbr": "TU Wien;DLR",
        "aff_campus_unique_index": "0;0;0;0;0;0+1",
        "aff_campus_unique": "Vienna;Wessling",
        "aff_country_unique_index": "0;0;0;0;0;0+1",
        "aff_country_unique": "Austria;Germany"
    },
    {
        "title": "Demonstrating Shared Force-Language Embeddings for Natural Human-Robot Communication",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "86",
        "author": "Ravi Tejwani; Karl Angel Velazquez; John Payne; Paolo Bonato; Haruhiko Asada",
        "abstract": "As robots increasingly collaborate with humans, natural language provides an intuitive interface for communication about physical actions. However, bridging the gap between linguistic descriptions and physical forces remains challenging for enabling robots to interpret movement instructions and communicate their intended actions. We address learning a shared embedding space between time-series force data and natural language motion descriptions for human-robot interaction. Our framework maps both force curves and phrases into a common latent space using data augmentation, feature engineering, and multitask learning to enable bidirectional translation. Evaluation with 10 participants performing motions with a robot arm demonstrates our model learns meaningful embeddings that effectively translate between forces and language descriptions. This will help robots learn appropriate verbal communication patterns while physically interacting with humans during collaborative tasks.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p086.pdf",
        "supp": "",
        "pdf_size": 3556028,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Electrical Engineering and Computer Science, Massachusetts Institute of Technology; Electrical Engineering and Computer Science, Massachusetts Institute of Technology; Electrical Engineering and Computer Science, Massachusetts Institute of Technology; Department of Physical Medicine, Harvard Medical School + Rehabilitation, Spaulding Rehabilitation Hospital; Mechanical Engineering, Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;mit.edu;mgh.harvard.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu;mgh.harvard.edu;mit.edu",
        "github": "",
        "project": "https://shared-language-force-embedding.github.io/therapy-sessions/",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/86/",
        "aff_unique_index": "0;0;0;1+2;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Harvard Medical School;Spaulding Rehabilitation Hospital",
        "aff_unique_dep": "Electrical Engineering and Computer Science;Department of Physical Medicine;Rehabilitation",
        "aff_unique_url": "https://web.mit.edu;https://hms.harvard.edu;https://www.spauldingrehab.org",
        "aff_unique_abbr": "MIT;HMS;",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Cambridge;Boston;",
        "aff_country_unique_index": "0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid",
        "session": "Perception and Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "2",
        "author": "Parv Kapoor; Ian Higgins; Nikhil Varma Keetha; Jay Patrikar; Brady Moon; Zelin Ye; Yao He; Ivan Cisneros; Changliu Liu; Eunsuk Kang; Sebastian Scherer",
        "abstract": "Maintaining visual separation is crucial to achieving safe and seamless high-density operation of airborne vehicles in shared airspace, where pilots currently shoulder this responsibility. To automate this, we present ViSafe, a high-speed airborne vision-only collision avoidance system.  Designed under SWaP-C constraints, ViSafe is built using a tightly integrated learning-enabled edge-AI framework deployed on a custom multi-camera hardware prototype, offering a full-stack solution to the Detect and Avoid (DAA) problem.  By leveraging perceptual input-focused control barrier functions (CBF) to design, encode, and enforce safety thresholds, ViSafe can provide provably safe runtime guarantees on self-separation for high-speed aerial operations. We evaluate ViSafe\u2019s performance through an extensive test campaign involving both simulated digital-twin and real-world flight scenarios.  By independently varying agent types, closure rates, interaction geometries, and environmental conditions (e.g., weather and lighting), we demonstrate that ViSafe consistently ensures self-separation across diverse scenarios.  In first-of-its-kind real-world high-speed collision avoidance tests with closure rates reaching 144 km/hr, ViSafe sets a new benchmark for vision-only autonomous collision avoidance, establishing a new standard for safety in high-speed aerial navigation.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p002.pdf",
        "supp": "",
        "pdf_size": 32459737,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ULL0mltapT8J:scholar.google.com/&scioq=Demonstrating+ViSafe:+Vision-enabled+Safety+for+High-speed+Detect+and+Avoid&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": ";;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;",
        "email": ";;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 11,
        "oa": "https://roboticsconference.org/program/papers/2/"
    },
    {
        "title": "Demonstrating a Control Framework for Physical Human-Robot Interaction Toward Industrial Applications",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "84",
        "author": "Bastien Muraccioli; Mathieu Celerier; Mehdi Benallegue; Gentiane Venture",
        "abstract": "Physical Human-Robot Interaction (pHRI) is critical for implementing Industry 5.0 which focuses on human-centric approaches. However few studies explore the practical alignment of pHRI to industrial grade performance. This paper introduces a versatile control framework designed to bridge this gap by incorporating the torque-based control modes: compliance control, null-space compliance,  dual compliance, all in static and dynamic scenarios. Thanks to our second-order Quadratic Programming (QP) formulation, strict kinematic and collisions constraints are integrated into the system as safety features, and weighted hierarchy guarantees singularity-robust task tracking performance. The framework is implemented on a Kinova Gen3 collaborative robot (cobot) equipped with a Bota force/torque sensor. A DualShock 4 game controller is integrated at the robot\u2019s end-effector to demonstrate the framework\u2019s capabilities. This setup enables seamless dynamic switching between the modes, and real-time adjustment of parameters, such as transitioning between position and torque control or selecting a more robust custom-developed low-level torque controller over the default one. Built on the open-source robotic control software mc_rtc, ensuring reproducibility for both research and industrial deployment, this framework demonstrates industrial-grade performance and repeatability, showcasing its potential as a robust pHRI control system for industrial environments.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p084.pdf",
        "supp": "",
        "pdf_size": 6139791,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:MUtDrXckTu0J:scholar.google.com/&scioq=Demonstrating+a+Control+Framework+for+Physical+Human-Robot+Interaction+Toward+Industrial+Applications&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "National Institute of Advanced Industrial Science and Technology (AIST), CNRS-AIST Joint Robotics Laboratory (JRL), Tsukuba, Ibaraki 305-8560, Japan+Department of Mechanical Engineering, The University of Tokyo, Tokyo 113-8654, Japan; National Institute of Advanced Industrial Science and Technology (AIST), CNRS-AIST Joint Robotics Laboratory (JRL), Tsukuba, Ibaraki 305-8560, Japan+Department of Mechanical Engineering, The University of Tokyo, Tokyo 113-8654, Japan; National Institute of Advanced Industrial Science and Technology (AIST), CNRS-AIST Joint Robotics Laboratory (JRL), Tsukuba, Ibaraki 305-8560, Japan; National Institute of Advanced Industrial Science and Technology (AIST), CNRS-AIST Joint Robotics Laboratory (JRL), Tsukuba, Ibaraki 305-8560, Japan+Department of Mechanical Engineering, The University of Tokyo, Tokyo 113-8654, Japan",
        "aff_domain": "aist.go.jp;aist.go.jp;aist.go.jp;g.ecc.u-tokyo.ac.jp",
        "email": "aist.go.jp;aist.go.jp;aist.go.jp;g.ecc.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/84/",
        "aff_unique_index": "0+1;0+1;0;0+1",
        "aff_unique_norm": "National Institute of Advanced Industrial Science and Technology;University of Tokyo",
        "aff_unique_dep": "CNRS-AIST Joint Robotics Laboratory;Department of Mechanical Engineering",
        "aff_unique_url": "https://www.aist.go.jp;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "AIST;UTokyo",
        "aff_campus_unique_index": "0+1;0+1;0;0+1",
        "aff_campus_unique": "Tsukuba;Tokyo",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model",
        "session": "Manipulation I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "58",
        "author": "Samson Yu; Lin Kelvin; Harold Soh",
        "abstract": "Touch is recognized as a vital sense for humans and an equally important modality for robots, especially for dexterous manipulation, material identification, and scenarios involving visual occlusion. Building upon very recent work in touch foundation models, this demonstration will feature Octopi-1.5, our latest visual-tactile-language model. Compared to its predecessor, Octopi-1.5 introduces the ability to process tactile signals from multiple object parts and employs a simple retrieval-augmented generation (RAG) module to improve performance on tasks and potentially learn new objects on-the-fly. The system can be experienced live through a new handheld tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile sensors. This convenient and accessible setup allows users to interact with Octopi-1.5 without requiring a robot. During the demonstration, we will showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5 will identify objects being grasped and respond to follow-up queries about how to handle it (e.g., recommending careful handling for soft fruits). We also plan to demonstrate Octopi-1.5\u2019s RAG capabilities by teaching it new items. With live interactions, this demonstration aims to highlight both the progress and limitations of VTLMs such as Octopi-1.5  and to foster further interest in this exciting field. All code for Octopi-1.5 and design files for the TMI gripper will be released as open-source resources.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p058.pdf",
        "supp": "",
        "pdf_size": 893926,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:X2gbAqbZ2KYJ:scholar.google.com/&scioq=Demonstrating+the+Octopi-1.5+Visual-Tactile-Language+Model&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Dept. of Computer Science, National University of Singapore; Dept. of Computer Science, National University of Singapore; Dept. of Computer Science, National University of Singapore + NUS Smart Systems Institute",
        "aff_domain": "u.nus.edu; ;comp.nus.edu.sg",
        "email": "u.nus.edu; ;comp.nus.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/58/",
        "aff_unique_index": "0;0;0+0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation",
        "session": "Manipulation II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "106",
        "author": "Jianglong Ye; Keyi Wang; Chengjing Yuan; Ruihan Yang; Yiquan Li; Jiyue Zhu; Yuzhe Qin; Xueyan Zou; Xiaolong Wang",
        "abstract": "Generating large-scale demonstrations for dexterous manipulation remains a challenging problem, and various approaches have been proposed in recent years to address it. Among these, generative models have emerged as a promising paradigm, enabling the efficient generation of diverse and plausible demonstrations. In this paper, we introduce Dex1B, a large-scale, diverse, and high-quality demonstration dataset created using generative models. The dataset includes 1 billion demonstrations and focuses on two fundamental tasks: grasping and articulation. To achieve this, we propose a unified generative model that incorporates diverse conditions, such as contact points and hand orientation, to synthesize actions and other essential properties that can be utilized for both data generation and policy deployment. We validate the proposed model on both established and newly introduced simulation benchmarks, demonstrating significant improvements over previous state-of-the-art methods. Furthermore, we showcase the model\u2019s effectiveness and robustness through real-world robot experiments.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p106.pdf",
        "supp": "",
        "pdf_size": 24944657,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "UC San Diego; UC San Diego; UC San Diego; UC San Diego; UC San Diego; UC San Diego; UC San Diego; UC San Diego; UC San Diego",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "https://jianglongye.com/dex1b",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/106/",
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies",
        "session": "Imitation Learning I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "75",
        "author": "Tony Tao; Mohan Kumar Srirama; Jason Jingzhou Liu; Kenneth Shaw; Deepak Pathak",
        "abstract": "Many believe that large-scale datasets for robotics could be a key enabler of dexterous robotic policies that can generalize across diverse environments. While teleoperation provides high-fidelity datasets, its high cost limits its scalability. Instead, what if people could use their own hands, just as they do in everyday life, to collect data? In DexWild, a diverse team of data collectors uses their hands to collect hours of interactions across a multitude of environments and objects. To record this data, we create DW-Mocap, a low-cost, mobile, and easy-to-use system. The DexWild learning framework co-trains on both human and robot demonstrations, leading to improved performance compared to training on each dataset individually. Our large-scale dataset enables robots with only a handful of teleoperation demonstrations to generalize across many different environments, objects, and embodiments. The software, hardware, and the dataset used in this paper will be released on our website upon acceptance of the paper.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p075.pdf",
        "supp": "",
        "pdf_size": 12050882,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12669679265339587012&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://dexwild.github.io",
        "project": "",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/75/",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy",
        "session": "Manipulation II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "105",
        "author": "Jiayi Chen; Yubin Ke; Lin Peng; He Wang",
        "abstract": "Generalizable dexterous grasping is a fundamental skill for intelligent robots. To develop such skills, a large-scale, high-quality, and diverse dataset of robotic dexterous grasps\u2014covering the GRASP taxonomy\u2014is essential but extremely challenging to collect. Previous dexterous grasp synthesis methods are often limited to specific grasp types or object categories, and tend to suffer from issues like penetration and unnatural poses. In this work, we address these challenges by proposing an efficient method capable of synthesizing physically plausible, contact-rich, and penetration-free dexterous grasps for any grasp type, object, and articulated robotic hand. Starting from only one human-annotated template per hand and grasp type, our pipeline first uses a lightweight global alignment stage to optimize the object pose and then a simulation-based local refinement stage to adjust the hand pose. Next, to validate the synthesized grasps, we introduce a contact-aware control strategy that applies desired forces to each contact point on the object. The validated grasps can further enrich the grasp template library and facilitate future synthesis. Experimental results demonstrate the superiority of our pipeline over existing grasp synthesis approaches for both fingertip and other grasp types. Furthermore, using our synthesized grasps, we show that a type-conditional generative model can successfully learn and perform the desired grasp type in both simulation and the real world.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p105.pdf",
        "supp": "",
        "pdf_size": 15105464,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:wV5yeHeGBBsJ:scholar.google.com/&scioq=Dexonomy:+Synthesizing+All+Dexterous+Grasp+Types+in+a+Grasp+Taxonomy&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/105/"
    },
    {
        "title": "DexterityGen:  Foundation Controller for Unprecedented Dexterity",
        "session": "Manipulation II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "103",
        "author": "Zhao-Heng Yin; Changhao Wang; Luis Pineda; Francois Robert Hogan; Chaithanya Krishna Bodduluri; Akash Sharma; Patrick Lancaster; Ishita Prasad; Mrinal Kalakrishnan; Jitendra Malik; Mike Lambeta; Tingfan Wu; Pieter Abbeel; Mustafa Mukadam",
        "abstract": "Teaching robots dexterous manipulation skills, such as tool use, presents a significant challenge.  Current approaches can be broadly categorized into two strategies: human teleoperation (for imitation learning) and sim-to-real reinforcement learning. The first approach is difficult as it is hard for humans to produce safe and dexterous motions on a different embodiment without touch feedback. The second RL-based approach struggles with the domain gap and involves highly task-specific reward engineering on complex tasks. Our key insight is that RL is effective at learning low-level motion primitives, while humans excel at providing coarse motion commands for complex, long-horizon tasks. Therefore, the optimal solution might be a combination of both approaches. In this paper, we introduce DexterityGen (DexGen), which uses RL to pretrain large-scale dexterous motion primitives, such as in-hand rotation or translation. We then leverage this learned dataset to train a dexterous foundational controller. In the real world, we use human teleoperation as a prompt to the controller to produce highly dexterous behavior. We evaluate the effectiveness of DexGen in both simulation and real world, demonstrating that it is a general-purpose controller that can realize input dexterous manipulation commands and significantly improves stability by 10-100x measured as duration of holding objects across diverse tasks. Notably, with DexGen we demonstrate unprecedented dexterous skills including diverse object reorientation and dexterous tool use such as pen, syringe, and screwdriver for the first time.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p103.pdf",
        "supp": "",
        "pdf_size": 36844580,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11546830291692880936&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "BAIR, UC Berkeley+FAIR at Meta; FAIR at Meta; FAIR at Meta; FAIR at Meta; FAIR at Meta; FAIR at Meta; FAIR at Meta; FAIR at Meta; FAIR at Meta; FAIR at Meta; FAIR at Meta; FAIR at Meta; BAIR, UC Berkeley; FAIR at Meta",
        "aff_domain": ";;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;",
        "github": "",
        "project": "zhaohengyin.github.io/dexteritygen",
        "author_num": 14,
        "oa": "https://roboticsconference.org/program/papers/103/",
        "aff_unique_index": "0+1;1;1;1;1;1;1;1;1;1;1;1;0;1",
        "aff_unique_norm": "University of California, Berkeley;Meta",
        "aff_unique_dep": "Berkeley Artificial Intelligence Research (BAIR);AI Research",
        "aff_unique_url": "https://www.berkeley.edu;https://ai.facebook.com",
        "aff_unique_abbr": "UC Berkeley;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Diffeomorphic Obstacle Avoidance for Contractive Dynamical Systems via Implicit Representations",
        "session": "Imitation Learning II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "162",
        "author": "Ken-Joel Simmoteit; Philipp Schillinger; Leonel Rozo",
        "abstract": "Ensuring safety and robustness of robot skills is becoming crucial as robots are required to perform increasingly complex and dynamic tasks. The former is essential when performing tasks in cluttered environments, while the latter is relevant to overcome unseen task situations. This paper addresses the challenge of ensuring both safety and robustness in dynamic robot skills learned from demonstrations. Specifically, we build on neural contractive dynamical systems to provide robust extrapolation of the learned skills, while designing a full-body obstacle avoidance strategy that preserves contraction stability via diffeomorphic transforms. This is particularly crucial in cluttered environments where implicit scene representations, such as Signed Distance Fields (SDFs), are necessary. To this end, our framework called Signed Distance Field Diffeomorphic Transform, leverages SDFs and flow-based diffeomorphisms to achieve contraction-preserving obstacle avoidance. We thoroughly evaluate our framework on synthetic datasets and several real-world robotic tasks in a kitchen environment. Our results show that our approach locally adapts the learned contractive vector field while staying close to the learned dynamics and without introducing highly-curved motion paths, thus outperforming several state-of-the-art methods.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p162.pdf",
        "supp": "",
        "pdf_size": 13674475,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:IGkFeJesuBsJ:scholar.google.com/&scioq=Diffeomorphic+Obstacle+Avoidance+for+Contractive+Dynamical+Systems+via+Implicit+Representations&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Karlsruhe Institute of Technology; Bosch Center for Artificial Intelligence; Bosch Center for Artificial Intelligence",
        "aff_domain": "student.kit.edu;de.bosch.com;de.bosch.com",
        "email": "student.kit.edu;de.bosch.com;de.bosch.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/162/",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Karlsruhe Institute of Technology;Bosch Center for Artificial Intelligence",
        "aff_unique_dep": ";Center for Artificial Intelligence",
        "aff_unique_url": "https://www.kit.edu;https://www.bosch-ai.com",
        "aff_unique_abbr": "KIT;BCAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Differentiable GPU-Parallelized Task and Motion Planning",
        "session": "Planning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "50",
        "author": "William Shen; Caelan Reed Garrett; Nishanth Kumar; Ankit Goyal; Tucker Hermans; Leslie Pack Kaelbling; Tom\u00e1s Lozano-P\u00e9rez; Fabio Ramos",
        "abstract": "Planning long-horizon robot manipulation requires making discrete decisions about which objects to interact with and continuous decisions about how to interact with them. A robot planner must select grasps, placements, and motions that are feasible and safe. This class of problems falls under Task and Motion Planning (TAMP) and poses significant computational challenges in terms of algorithm runtime and solution quality, particularly when the solution space is highly constrained. To address these challenges, we propose a new bilevel TAMP algorithm that leverages GPU parallelism to efficiently explore thousands of candidate continuous solutions simultaneously. Our approach uses GPU parallelism to sample an initial batch of solution seeds for a plan skeleton and to apply differentiable optimization on this batch to satisfy plan constraints and minimize solution cost. We demonstrate that our algorithm can effectively solve highly constrained problems with non-convex constraints in just seconds, substantially outperforming serial TAMP approaches, and validate our approach on multiple real-world robots.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p050.pdf",
        "supp": "",
        "pdf_size": 5633068,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Sexob6OC9jIJ:scholar.google.com/&scioq=Differentiable+GPU-Parallelized+Task+and+Motion+Planning&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "MIT CSAIL+University of Utah; NVIDIA Research; MIT CSAIL+NVIDIA Research; NVIDIA Research; University of Utah+NVIDIA Research; MIT CSAIL; MIT CSAIL; NVIDIA Research+University of Sydney",
        "aff_domain": "mit.edu;nvidia.com; ; ; ; ; ; ",
        "email": "mit.edu;nvidia.com; ; ; ; ; ; ",
        "github": "",
        "project": "cutamp.github.io",
        "author_num": 8,
        "oa": "https://roboticsconference.org/program/papers/50/",
        "aff_unique_index": "0+1;2;0+2;2;1+2;0;0;2+3",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Utah;NVIDIA;University of Sydney",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;;NVIDIA Research;",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.utah.edu;https://www.nvidia.com/research;https://www.sydney.edu.au",
        "aff_unique_abbr": "MIT CSAIL;Utah;NVIDIA;USYD",
        "aff_campus_unique_index": "0;0;;0;0;",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0+0;0;0+0;0;0+0;0;0;0+1",
        "aff_country_unique": "United States;Australia"
    },
    {
        "title": "Discrete-Time Hybrid Automata Learning: Legged Locomotion Meets Skateboarding",
        "session": "Mobile Manipulation and Locomotion",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "127",
        "author": "Hang Liu; Sangli Teng; Ben Liu; Wei Zhang; Maani Ghaffari",
        "abstract": "This paper introduces Discrete-time Hybrid Automata Learning (DHAL), a framework using on-policy Reinforcement Learning to identify and execute mode-switching without trajectory segmentation or event function learning. Hybrid dynamical systems, which include continuous flow and discrete mode switching, can model robotics tasks like legged robot locomotion. Model-based methods depend on predefined gaits, while model-free approaches lack explicit mode-switching knowledge. Current methods identify discrete modes via segmentation before regressing continuous flow, but learning high-dimensional complex rigid body dynamics without trajectory labels or segmentation is a challenging open problem. Our approach incorporates a beta policy distribution and a multi-critic architecture to model contact-guided motions, exemplified by a challenging quadrupedal robot skateboard task. We validate our method through simulations and real-world tests, demonstrating robust performance in hybrid dynamical systems.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p127.pdf",
        "supp": "",
        "pdf_size": 18910391,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15655500166738840768&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Michigan; University of Michigan; Southern University of Science and Technology; Southern University of Science and Technology; University of Michigan",
        "aff_domain": "umich.edu;umich.edu;sustech.edu.cn;sustech.edu.cn;umich.edu",
        "email": "umich.edu;umich.edu;sustech.edu.cn;sustech.edu.cn;umich.edu",
        "github": "https://umich-curly.github.io/DHAL/",
        "project": "",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/127/",
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "University of Michigan;Southern University of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umich.edu;https://www.sustech.edu.cn",
        "aff_unique_abbr": "UM;SUSTech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Distilling Contact Planning for Fast Trajectory Optimization in Robot Air Hockey",
        "session": "Control and Dynamics",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "115",
        "author": "Julius Jankowski; Ante Mari\u0107; Puze Liu; Davide Tateo; Jan Peters; Sylvain Calinon",
        "abstract": "Robot control through contact is challenging as it requires reasoning over long horizons and discontinuous system dynamics. Highly dynamic tasks such as Air Hockey additionally require agile behavior, making the corresponding optimal control problems intractable for planning in realtime. Learning-based approaches address this issue by shifting computationally expensive reasoning through contacts to an offline learning phase. However, learning low-level motor policies subject to kinematic and dynamic constraints can be challenging if operating in proximity to such constraints is desired. This paper explores the combination of distilling a stochastic optimal control policy for high-level contact planning and online model-predictive control for low-level constrained motion planning. Our system learns to balance shooting accuracy and resulting puck speed by leveraging bank shots and the robot\u2019s kinematic structure. We show that the proposed framework outperforms purely control-based and purely learning-based techniques in both simulated and real-world games of Robot Air Hockey.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p115.pdf",
        "supp": "",
        "pdf_size": 9281890,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Idiap Research Institute, Martigny, Switzerland+\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), Switzerland; Idiap Research Institute, Martigny, Switzerland+\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), Switzerland; Intelligent Autonomous Systems, TU Darmstadt, Germany+German Research Center for AI (DFKI); Intelligent Autonomous Systems, TU Darmstadt, Germany; Intelligent Autonomous Systems, TU Darmstadt, Germany+German Research Center for AI (DFKI)+Centre for Cognitive Science, Hessian.AI; Idiap Research Institute, Martigny, Switzerland+\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), Switzerland",
        "aff_domain": "idiap.ch; ; ; ; ; ",
        "email": "idiap.ch; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/115/",
        "aff_unique_index": "0+1;0+1;2+3;2;2+3+4;0+1",
        "aff_unique_norm": "Idiap Research Institute;EPFL;Technische Universit\u00e4t Darmstadt;German Research Center for Artificial Intelligence;Hessian.AI",
        "aff_unique_dep": ";;Intelligent Autonomous Systems;Research Center;Centre for Cognitive Science",
        "aff_unique_url": "https://www.idiap.ch;https://www.epfl.ch;https://www.tu-darmstadt.de;https://www.dFKI.de;",
        "aff_unique_abbr": "Idiap;EPFL;TU Darmstadt;DFKI;",
        "aff_campus_unique_index": "0;0;;;0",
        "aff_campus_unique": "Martigny;",
        "aff_country_unique_index": "0+0;0+0;1+1;1;1+1+1;0+0",
        "aff_country_unique": "Switzerland;Germany"
    },
    {
        "title": "Doppler Correspondence: Non-Iterative Scan Matching With Doppler Velocity-Based Correspondence",
        "session": "Perception and Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "5",
        "author": "Jiwoo Kim; Geunsik Bae; Changseung Kim; Jinwoo Lee; Woojae Shin; Hyondong Oh",
        "abstract": "Achieving successful scan matching is essential for LiDAR odometry. However, in challenging environments with adverse weather conditions or repetitive geometric patterns,  LiDAR odometry performance is degraded due to incorrect scan matching. Recently, the emergence of frequency-modulated continuous wave 4D LiDAR and 4D radar technologies has provided the potential to address these unfavorable conditions. The term 4D refers to point cloud data characterized by range, azimuth, and elevation along with Doppler velocity. Although 4D data is available, most scan matching methods for 4D LiDAR and 4D radar still establish correspondence by repeatedly identifying the closest points between consecutive scans, overlooking the Doppler information. This paper introduces, for the first time, a simple Doppler velocity-based correspondence\u2014Doppler Correspondence\u2014that is invariant to translation and small rotation, with its geometric and kinematic foundations. Extensive experiments demonstrate that the proposed method enables the direct matching of consecutive point clouds without an iterative process, making it computationally efficient. Additionally, it provides a more robust correspondence estimation in environments with repetitive geometric patterns. The implementation is publicly available upon acceptance.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p005.pdf",
        "supp": "",
        "pdf_size": 20020152,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:FYN-6rp0U7kJ:scholar.google.com/&scioq=Doppler+Correspondence:+Non-Iterative+Scan+Matching+With+Doppler+Velocity-Based+Correspondence&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea",
        "aff_domain": "unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr",
        "email": "unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr",
        "github": "https://github.com/Tars0523/Doppler Correspondence",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/5/",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Ulsan National Institute of Science and Technology",
        "aff_unique_dep": "Department of Mechanical Engineering",
        "aff_unique_url": "https://www.unist.ac.kr",
        "aff_unique_abbr": "UNIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Dynamic Rank Adjustment in Diffusion Policies for Efficient and Flexible Training",
        "session": "Imitation Learning II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "159",
        "author": "Xiatao Sun; Shuo Yang; Yinxing Chen; Francis Fan; Yiyan (Edgar) Liang; Daniel Rakita",
        "abstract": "Diffusion policies trained via offline behavioral cloning have recently gained traction in robotic motion generation. While effective, these policies typically require a large number of trainable parameters. This model size affords powerful representations but also incurs high computational cost during training. Ideally, it would be beneficial to",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p159.pdf",
        "supp": "",
        "pdf_size": 1939492,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12363554861620235977&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Yale University; University of Pennsylvania; Yale University; Yale University; University of Pennsylvania; Yale University",
        "aff_domain": "yale.edu; ; ; ; ;yale.edu",
        "email": "yale.edu; ; ; ; ;yale.edu",
        "github": "",
        "project": "https://apollo-lab-yale.github.io/25-RSS-DRIFT-website/",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/159/",
        "aff_unique_index": "0;1;0;0;1;0",
        "aff_unique_norm": "Yale University;University of Pennsylvania",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.yale.edu;https://www.upenn.edu",
        "aff_unique_abbr": "Yale;UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Dynamic Safety in Complex Environments: Synthesizing Safety Filters with Poisson\u2019s Equation",
        "session": "Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "137",
        "author": "Gilbert Bahati; Ryan M. Bena; Aaron Ames",
        "abstract": "Synthesizing safe sets for robotic systems operating in complex and dynamically changing environments is a challenging problem. Solving this problem can enable the construction of safety filters that guarantee safe control actions\u2014most notably by employing Control Barrier Functions (CBFs). This paper presents an algorithm for generating safe sets from perception data by leveraging elliptic partial differential equations, specifically Poisson\u2019s equation. Given a local occupancy map, we solve Poisson\u2019s equation subject to Dirichlet boundary conditions, with a novel forcing function. Specifically, we design a smooth guidance vector field, which encodes gradient information required for safety. The result is a variational problem for which the unique minimizer\u2014a safety function\u2014characterizes the safe set. After establishing our theoretical result, we illustrate how safety functions can be used in CBF-based safety filtering. The real-time utility of our synthesis method is highlighted through hardware demonstrations on a legged robot navigating a dynamically changing obstacle-filled environment.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p137.pdf",
        "supp": "",
        "pdf_size": 35109836,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12677148908545618496&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Mechanical and Civil Engineering, California Institute of Technology, Pasadena, CA, USA; Department of Mechanical and Civil Engineering, California Institute of Technology, Pasadena, CA, USA; Department of Mechanical and Civil Engineering, California Institute of Technology, Pasadena, CA, USA",
        "aff_domain": "caltech.edu;caltech.edu;caltech.edu",
        "email": "caltech.edu;caltech.edu;caltech.edu",
        "github": "",
        "project": "https://youtu.be/fBRdkAJGixI",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/137/",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "Department of Mechanical and Civil Engineering",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Effective Sampling for Robot Motion Planning Through the Lens of Lattices",
        "session": "Planning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "48",
        "author": "Itai Panasoff; Kiril Solovey",
        "abstract": "Sampling-based methods for motion planning, which capture the structure of the robot\u2019s free space via (typically random) sampling, have gained popularity due to their scalability, simplicity, and for offering global guarantees, such as probabilistic completeness and asymptotic optimality. Unfortunately, the practicality of those guarantees remains limited as they do not provide insights into the behavior of motion planners for a finite number of samples (i.e., a finite running time). In this work, we harness lattice theory and the recently-introduced concept of (\u03b4,\u03b5)-completeness by Tsao et al. (2020) to construct deterministic sample sets that endow their planners with strong finite-time guarantees while minimizing running time. In particular, we introduce a highly-efficient deterministic sampling approach based on the A_d^* lattice, which  is the best-known geometric covering in dimensions \u2264 21. Using our new  sampling approach we obtain at least an order-of-magnitude speedup over existing deterministic and uniform random sampling methods in complex motion-planning problems. Overall, our work provides deep mathematical insights while advancing the practical applicability of sampling-based motion planning.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p048.pdf",
        "supp": "",
        "pdf_size": 2918408,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:kfVLz17QajUJ:scholar.google.com/&scioq=Effective+Sampling+for+Robot+Motion+Planning+Through+the+Lens+of+Lattices&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Viterbi Faculty of Electrical and Computer Engineering, Technion\u2013Israel Institute of Technology, Haifa, Israel; Viterbi Faculty of Electrical and Computer Engineering, Technion\u2013Israel Institute of Technology, Haifa, Israel",
        "aff_domain": "campus.technion.ac.il;technion.ac.il",
        "email": "campus.technion.ac.il;technion.ac.il",
        "github": "https://github.com/MRSTechnion/lattice-sampling-mp",
        "project": "",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/48/",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion\u2013Israel Institute of Technology",
        "aff_unique_dep": "Viterbi Faculty of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Efficient Hierarchical Any-Angle Path Planning on Multi-Resolution 3D Grids",
        "session": "Planning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "49",
        "author": "Victor Reijgwart; Cesar Cadena; Roland Siegwart; Lionel Ott",
        "abstract": "Hierarchical, multi-resolution volumetric mapping approaches are widely used to represent large and complex environments as they can efficiently capture their occupancy and connectivity information. Yet widely used path planning methods such as sampling and trajectory optimization do not exploit this explicit connectivity information, and search-based methods such as A* suffer from scalability issues in large-scale high-resolution maps. In many applications, Euclidean shortest paths form the underpinning of the navigation system. For such applications, any-angle planning methods, which find optimal paths by connecting corners of obstacles with straight-line segments, provide a simple and efficient solution. In this paper, we present a method that has the optimality and completeness properties of any-angle planners while overcoming computational tractability issues common to search-based methods by exploiting multi-resolution representations. Extensive experiments on real and synthetic environments demonstrate the proposed approach\u2019s solution quality and speed, outperforming even sampling-based methods. The framework is open-sourced to allow the robotics and planning community to build on our research.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p049.pdf",
        "supp": "",
        "pdf_size": 6022342,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:DtKVT3BeraoJ:scholar.google.com/&scioq=Efficient+Hierarchical+Any-Angle+Path+Planning+on+Multi-Resolution+3D+Grids&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Autonomous Systems Lab, ETH Z\u00fcrich, Switzerland; Autonomous Systems Lab, ETH Z\u00fcrich, Switzerland; Autonomous Systems Lab, ETH Z\u00fcrich, Switzerland; Autonomous Systems Lab, ETH Z\u00fcrich, Switzerland",
        "aff_domain": "rai-inst.com;ethz.ch;ethz.ch;ethz.ch",
        "email": "rai-inst.com;ethz.ch;ethz.ch;ethz.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/49/",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Autonomous Systems Lab",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models",
        "session": "Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "140",
        "author": "Nicolas Baumann; Cheng Hu; Paviththiren Sivasothilingam; Haotong Qin; Lei Xie; Michele Magno; Luca Benini",
        "abstract": "Neural Networks (NNs) trained through supervised learning, struggle with managing edge-case scenarios common in real-world driving due to the intractability of exhaustive datasets covering all edge-cases, making knowledge-driven approaches,  akin to how humans intuitively detect unexpected driving behavior, a suitable complement to data-driven methods. This work proposes a hybrid architecture combining low- level Model Predictive Controller (MPC) with locally deployed Large Language Models (LLMs) to enhance decision-making and Human Machine Interaction (HMI). The DecisionxLLM module evaluates robotic state information against natural language instructions to ensure adherence to desired driving behavior. The MPCxLLM module then adjusts MPC parameters based on LLM-generated insights, achieving control adaptability while preserving the safety and constraint guarantees of traditional MPC systems. Further, to enable efficient on-board deployment and to eliminate dependency on cloud connectivity, we shift processing to the on-board computing platform: We propose an approach that exploits Retrieval Augmented Generation (RAG), Low Rank Adaptation (LoRA) fine-tuning, and quantization. Experimental results demonstrate that these enhancements yield significant improvements in reasoning accuracy by up to 10.45%, control adaptability by as much as 52.2%, and up to 10.5\u00d7 increase in computational efficiency (tokens/s), validating the proposed framework\u2019s practicality for real-time deployment even on down- scaled robotic platforms. This work bridges high-level decision- making with low-level control adaptability, offering a sinergistic  framework for knowledge-driven and adaptive Autonomous Driving Systems (ADS).",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p140.pdf",
        "supp": "",
        "pdf_size": 1121134,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:LqVMdHdPCLYJ:scholar.google.com/&scioq=Enhancing+Autonomous+Driving+Systems+with+On-Board+Deployed+Large+Language+Models&hl=en&as_sdt=0,14",
        "gs_version_total": 3,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://roboticsconference.org/program/papers/140/"
    },
    {
        "title": "FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning",
        "session": "Imitation Learning I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "79",
        "author": "Jason Jingzhou Liu; Yulong Li; Kenneth Shaw; Tony Tao; Russ Salakhutdinov; Deepak Pathak",
        "abstract": "Many contact-rich tasks humans perform, such as box pickup or hammering, rely on force feedback for reliable execution. However, this force information, which is readily available in most robot arms, is not commonly used in teleoperation and policy learning. Consequently, robot behavior is often limited to quasi-static kinematic tasks that do not require intricate force-feedback. In this paper, we first present a low-cost, intuitive, bilateral teleoperation setup that relays external forces of the follower arm back to the teacher arm, facilitating data collection for complex, contact-rich tasks. We then introduce FACTR, a policy learning method that employs a curriculum which corrupts the visual input with decreasing intensity throughout training. The curriculum prevents our transformer-based policy from over-fitting to the visual input and guides the policy to properly attend to the force modality. We demonstrate that by fully utilizing the force information, our method significantly improves generalization to unseen objects compared to baseline approaches without a curriculum and exhibits more reactive recovery behaviors. All components of our system will be open-sourced upon the paper\u2019s acceptance.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p079.pdf",
        "supp": "",
        "pdf_size": 4613850,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12970098698448097427&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/79/"
    },
    {
        "title": "FAST: Efficient Action Tokenization for Vision-Language-Action Models",
        "session": "VLA Models",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "12",
        "author": "Karl Pertsch; Kyle Stachowicz; Brian Ichter; Danny Driess; Suraj Nair; Quan Vuong; Oier Mees; Chelsea Finn; Sergey Levine",
        "abstract": "Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the pi_0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p012.pdf",
        "supp": "",
        "pdf_size": 7266931,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2318538005626370572&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Physical Intelligence+UC Berkeley+Stanford; UC Berkeley; Physical Intelligence; Physical Intelligence; Physical Intelligence; Physical Intelligence; UC Berkeley; Stanford+Physical Intelligence; Physical Intelligence+UC Berkeley",
        "aff_domain": "physicalintelligence.company; ; ; ; ; ; ; ; ",
        "email": "physicalintelligence.company; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://pi.website/research/fast",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/12/",
        "aff_unique_index": "0+1+2;1;0;0;0;0;1;2+0;0+1",
        "aff_unique_norm": "Physical Intelligence;University of California, Berkeley;Stanford University",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.berkeley.edu;https://www.stanford.edu",
        "aff_unique_abbr": ";UC Berkeley;Stanford",
        "aff_campus_unique_index": "1+2;1;1;2;1",
        "aff_campus_unique": ";Berkeley;Stanford",
        "aff_country_unique_index": "1+1;1;1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "83",
        "author": "Rajat Kumar Jenamani; Tom Silver; Ben Dodson; Shiqin Tong; Anthony Song; Yuting Yang; Ziang Liu; Benjamin Howe; Aimee Whitneck; Tapomayukh Bhattacharjee",
        "abstract": "Physical caregiving robots hold promise for improving the quality of life of millions worldwide who require assistance with feeding. However, in-home meal assistance remains challenging due to the diversity of activities (e.g., eating, drinking, mouth wiping), contexts (e.g., socializing, watching TV), food items, and user preferences that arise during deployment. In this work, we propose FEAST, a flexible mealtime-assistance system that can be personalized in-the-wild to meet the unique needs of individual care recipients. Developed in collaboration with two community researchers and informed by a formative study with a diverse group of care recipients, our system is guided by three key tenets for in-the-wild personalization: adaptability, transparency, and safety. FEAST embodies these principles through: (i) modular hardware that enables switching between assisted feeding, drinking, and mouth-wiping, (ii) diverse interaction methods, including a web interface, head gestures, and physical buttons, to accommodate diverse functional abilities and preferences, and (iii) parameterized behavior trees that can be safely and transparently adapted using a large language model. We evaluate our system based on the personalization requirements identified in our formative study, demonstrating that FEAST offers a wide range of transparent and safe adaptations and outperforms a state-of-the-art baseline limited to fixed customizations. Finally, we conduct an in-home user study with two care recipients (who are community researchers), feeding them three meals each across three diverse scenarios. In all cases, they successfully personalize FEAST to meet their individual needs and preferences.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p083.pdf",
        "supp": "",
        "pdf_size": 39822871,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Cornell University; Cornell University; Cornell University; Cornell University; Cornell University; University of Michigan; Cornell University; Independent researcher; Independent researcher; Cornell University",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "",
        "project": "emprise.cs.cornell.edu/feast",
        "author_num": 10,
        "oa": "https://roboticsconference.org/program/papers/83/",
        "aff_unique_index": "0;0;0;0;0;1;0;2;2;0",
        "aff_unique_norm": "Cornell University;University of Michigan;Independent Researcher",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cornell.edu;https://www.umich.edu;",
        "aff_unique_abbr": "Cornell;UM;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "FERMI: Flexible Radio Mapping with a Hybrid Propagation Model and Scalable Autonomous Data Collection",
        "session": "Multi-Robot Systems",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "95",
        "author": "Yiming Luo; Yunfei Wang; Hongming Chen; Chengkai Wu; Ximin Lyu; Jinni Zhou; Jun Ma; Fu Zhang; Boyu Zhou",
        "abstract": "Communication is fundamental for multi-robot collaboration, with accurate radio mapping playing a crucial role in predicting signal strength between robots. However, modeling radio signal propagation in large and occluded environments is challenging due to complex interactions between signals and obstacles. Existing methods face two key limitations: they struggle to predict signal strength for transmitter-receiver pairs not present in the training set, while also requiring extensive manual data collection for modeling, making them impractical for large, obstacle-rich scenarios. To overcome these limitations, we propose FERMI, a flexible radio mapping framework. FERMI combines physics-based modeling of direct signal paths with a neural network to capture environmental interactions with radio signals. This hybrid model learns radio signal propagation more efficiently, requiring only sparse training data. Additionally, FERMI introduces a scalable planning method for autonomous data collection using a multi-robot team. By increasing parallelism in data collection and minimizing robot travel costs between regions, overall data collection efficiency is significantly improved. Experiments in both simulation and real-world scenarios demonstrate that FERMI enables accurate signal prediction and generalizes well to unseen positions in complex environments. It also supports fully autonomous data collection and scales to different team sizes, offering a flexible solution for creating radio maps.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p095.pdf",
        "supp": "",
        "pdf_size": 12770912,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:gshw3_GJonIJ:scholar.google.com/&scioq=FERMI:+Flexible+Radio+Mapping+with+a+Hybrid+Propagation+Model+and+Scalable+Autonomous+Data+Collection&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Southern University of Science and Technology+The University of Hong Kong; Hong Kong University of Science and Technology (Guangzhou); Sun Yat-Sen University; Hong Kong University of Science and Technology (Guangzhou); Sun Yat-Sen University; Hong Kong University of Science and Technology (Guangzhou); Hong Kong University of Science and Technology (Guangzhou); The University of Hong Kong; Southern University of Science and Technology",
        "aff_domain": "ust.hk;ust.hk;mail.sysu.edu.cn;ust.hk;ust.hk;ust.hk;ust.hk;hku.hk;sustech.edu.cn",
        "email": "ust.hk;ust.hk;mail.sysu.edu.cn;ust.hk;ust.hk;ust.hk;ust.hk;hku.hk;sustech.edu.cn",
        "github": "https://github.com/ymLuo1214/Flexible-Radio-Mapping",
        "project": "",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/95/",
        "aff_unique_index": "0+1;2;3;2;3;2;2;1;0",
        "aff_unique_norm": "Southern University of Science and Technology;University of Hong Kong;Hong Kong University of Science and Technology;Sun Yat-sen University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.sustech.edu.cn;https://www.hku.hk;https://www.ust.hk;http://www.sysu.edu.cn/",
        "aff_unique_abbr": "SUSTech;HKU;HKUST;SYSU",
        "aff_campus_unique_index": "1;1;1;1;1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success",
        "session": "VLA Models",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "17",
        "author": "Moo Jin Kim; Chelsea Finn; Percy Liang",
        "abstract": "Recent vision-language-action models (VLAs), which build upon pretrained vision-language models and leverage diverse robot datasets, have demonstrated strong task execution, language-following ability, and out-of-distribution generalization. Despite their success, VLAs struggle with novel robot setups and must be fine-tuned to achieve optimal performance. However, existing fine-tuning methods yield suboptimal speed and task performance, and systematic investigations of alternative adaptation strategies and controlled evaluations of their effects remain largely underexplored. In this work, we conduct a comprehensive study of adaptation design choices for the recently released OpenVLA model, examining different action decoding schemes, action representations, and learning objectives for fine-tuning. Based on our findings, we propose OpenVLA-OFT, an instantiation of our Optimized Fine-Tuning recipe that integrates parallel decoding, action chunking, continuous action representations, and a simple L1 regression-based learning objective to altogether improve inference efficiency, policy performance, and model input/output flexibility. OpenVLA-OFT sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA\u2019s average success rate across four task suites from 76% to 97% while increasing action generation throughput by 26\u00d7. In real-world evaluation, OpenVLA-OFT successfully performs dexterous, high-frequency control tasks on a dual-arm ALOHA robot and matches or outperforms strong imitation learning methods trained from scratch as well as other fine-tuned VLAs. We will release code for the optimized fine-tuning recipe, pretrained model checkpoints, and datasets upon publication.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p017.pdf",
        "supp": "",
        "pdf_size": 26883878,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14511679643312711139&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Stanford University; Stanford University; Stanford University",
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "https://openvla-oft.github.io",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/17/",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Flow Matching Ergodic Coverage",
        "session": "Planning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "51",
        "author": "Max Muchen Sun; Allison Pinosky; Todd Murphey",
        "abstract": "Ergodic coverage provides a robust framework for generating exploratory behaviors in embodied agents by aligning the spatial distribution of an agent\u2019s trajectory with a target distribution. However, solving this optimization problem is challenging due to the need to minimize the difference between distributions while respecting the agent\u2019s dynamic constraints. In this work, we propose an alternative approach to ergodic coverage through an unconstrained optimization paradigm based on flow matching, a technique widely used in variational inference and generative modeling for efficient and scalable sampling. We formally show that the flow matching problem for ergodic coverage is equivalent to a linear quadratic optimal control problem, enabling a closed-form solution. Numerical benchmarks demonstrate that our method offers two key advantages: (1) it enables unconstrained optimization under the standard Fourier ergodic metric with comparable coverage performance and improved computational efficiency compared to existing trajectory optimization methods, and (2) it facilitates the use of alternative metrics to address the limitations of existing methods. Specifically, incorporating Stein variational gradient flow enables ergodic coverage over unnormalized distributions without compromising coverage performance or computational efficiency, and incorporating optimal transport-based flow significantly improves coverage performance for non-smooth distributions. Finally, we validate the effectiveness of our method through hardware demonstrations on a Franka Emika Panda robot.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p051.pdf",
        "supp": "",
        "pdf_size": 18641610,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4802147107672282708&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Center for Robotics and Biosystems, Northwestern University, Evanston, IL 60208; Center for Robotics and Biosystems, Northwestern University, Evanston, IL 60208; Center for Robotics and Biosystems, Northwestern University, Evanston, IL 60208",
        "aff_domain": "u.northwestern.edu; ; ",
        "email": "u.northwestern.edu; ; ",
        "github": "",
        "project": "https://murpheylab.github.io/lqr-flow-matching/",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/51/",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Northwestern University",
        "aff_unique_dep": "Center for Robotics and Biosystems",
        "aff_unique_url": "https://www.northwestern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Evanston",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Flying Hand: End-Effector-Centric Framework for Versatile Aerial Manipulation Teleoperation and Policy Learning",
        "session": "Mobile Manipulation and Locomotion",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "130",
        "author": "Guanqi He; Xiaofeng Guo; Luyi Tang; Yuanhang Zhang; Mohammadreza Mousaei; Jiahe Xu; Junyi Geng; Sebastian Scherer; Guanya Shi",
        "abstract": "Aerial manipulation has recently attracted increasing interest from both industry and academia. Previous approaches have demonstrated success in various specific tasks. However, their hardware design and control frameworks are often tightly coupled with particular tasks, limiting the development of cross-task and cross-platform algorithms. Inspired by the success of robot learning in tabletop manipulation, we propose a unified aerial manipulation framework with an end-effector-centric interface that decouples high-level platform-agnostic decision-making from task-agnostic low-level control. Our framework consists of a fully-actuated hexarotor with a 4-DoF robotic arm, a whole-body model predictive controller, and an end-effector-centric interface to receive commands from the high-level policy. The high-precision end-effector controller enables efficient and intuitive aerial teleoperation for versatile tasks and facilitates the development of imitation learning policies. Real-world experiments show that the proposed framework significantly improves end-effector tracking accuracy, and can handle multiple aerial teleoperation and imitation learning tasks, including writing, peg-in-hole, pick and place, light bulb changing, etc. We believe the proposed framework provides one way to standardize and unify aerial manipulation into the general manipulation community and to advance the field.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p130.pdf",
        "supp": "",
        "pdf_size": 4458330,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13559960850304244109&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA; Department of Aerospace Engineering, Pennsylvania State University, University Park, PA 16802 USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA",
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;psu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;psu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "github": "",
        "project": "https://lecar-lab.github.io/flying hand/",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/130/",
        "aff_unique_index": "0;0;0;0;0;0;1;0;0",
        "aff_unique_norm": "Carnegie Mellon University;Pennsylvania State University",
        "aff_unique_dep": "Robotics Institute;Department of Aerospace Engineering",
        "aff_unique_url": "https://www.cmu.edu;https://www.psu.edu",
        "aff_unique_abbr": "CMU;PSU",
        "aff_campus_unique_index": "0;0;0;0;0;0;1;0;0",
        "aff_campus_unique": "Pittsburgh;University Park",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment",
        "session": "Imitation Learning I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "76",
        "author": "Yilin Wu; Thomas Tian; Gokul Swamy; Andrea Bajcsy",
        "abstract": "While generative robot policies have demonstrated significant potential in learning complex, multimodal behaviors from demonstrations, they still exhibit diverse failures at deployment-time. Policy steering offers an elegant solution to reducing the chance of failure by using an external verifier to select from low-level actions proposed by an imperfect generative policy. Here, one might hope to use a Vision Language Model (VLM) as a verifier, leveraging their open-world reasoning capabilities. However, off-the-shelf VLMs struggle to understand the consequences of low-level robot actions as they are represented fundamentally differently than the text and images the VLM was trained on. In response, we propose FOREWARN, a novel framework to unlock the potential of VLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is to decouple the VLM\u2019s burden of predicting action outcomes (",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p076.pdf",
        "supp": "",
        "pdf_size": 1805132,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8747053422788995471&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Carnegie Mellon University; UC Berkeley; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;berkeley.edu;andrew.cmu.edu;andrew.cmu.edu",
        "email": "andrew.cmu.edu;berkeley.edu;andrew.cmu.edu;andrew.cmu.edu",
        "github": "",
        "project": "https://yilin-wu98.github.io/forewarn/",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/76/",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Carnegie Mellon University;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "CMU;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Gain Tuning Is Not What You Need: Reward Gain Adaptation for Constrained Locomotion Learning",
        "session": "Mobile Manipulation and Locomotion",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "123",
        "author": "Arthicha Srisuchinnawong; Poramate Manoonpong",
        "abstract": "Existing robot locomotion learning techniques rely heavily on the offline selection of proper reward weighting gains and cannot guarantee constraint satisfaction (i.e., constraint violation) during training. Thus, this work aims to address both issues by proposing Reward-Oriented Gains via Embodied Interaction (ROGER), adapting reward-weighting gains online based on penalties received throughout the embodied interaction process. The ratio between the positive reward (primary reward) and negative reward (penalty) gains is automatically reduced as the learning approaches the constraint thresholds to avoid violation. Conversely, the ratio is increased when the learning is in safe states to prioritize performance. With a 60-kg quadruped robot, ROGER achieved near-zero constraint violation throughout multiple learning trials. It also achieved up to 50% more primary reward than the equivalent state-of-the-art techniques. In MuJoCo continuous locomotion benchmarks, including a single-leg hopper, ROGER exhibited comparable or up to 100% higher performance and 60% less torque usage and orientation deviation, compared to those trained with the default reward function. Finally, real-world locomotion learning of a physical quadruped robot was achieved from scratch within one hour without any falling. Therefore, this work contributes to constraint-satisfying real-world continual robot locomotion learning and the simplification of reward weighting gain tuning, potentially facilitating the development of physical robots and those that learn in the real world.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p123.pdf",
        "supp": "",
        "pdf_size": 55407510,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand; Vidyasirimedhi Institute of Science and Technology, Rayong, Thailand + The University of Southern Denmark, Odense, Denmark",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "https://youtu.be/Cqu7vLTPiw?si=jtzJCpRubbFHx06w",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/123/",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Vidyasirimedhi Institute of Science and Technology;University of Southern Denmark",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.sdu.dk",
        "aff_unique_abbr": ";SDU",
        "aff_campus_unique_index": "0;0+1",
        "aff_campus_unique": "Rayong;Odense",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "Thailand;Denmark"
    },
    {
        "title": "Gait-Net-augmented Implicit Kino-dynamic MPC for Dynamic Variable-frequency Humanoid Locomotion over Discrete Terrains",
        "session": "Humanoids",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "69",
        "author": "Junheng Li; Ziwei Duan; Junchao Ma; Quan Nguyen",
        "abstract": "Current optimization-based control techniques for humanoid locomotion struggle to adapt step duration and placement simultaneously in dynamic walking gaits due to their reliance on fixed-time discretization, which limits responsiveness to terrain conditions and results in suboptimal performance in challenging environments. In this work, we propose a Gait-Net-augmented implicit kino-dynamic model-predictive control (MPC) to simultaneously optimize step location, step duration, and contact forces for natural variable-frequency locomotion. The proposed method incorporates a Gait-Net-augmented Sequential Convex MPC algorithm to solve multi-linearly constrained variables by their step sizes iteratively. At its core, a lightweight Gait-frequency Network (Gait-Net) determines the preferred step duration in terms of variable MPC sampling times, simplifying step duration optimization to the parameter level. Additionally, it enhances and updates the spatial momentum reference trajectory estimation within each sequential iteration by incorporating local solutions, allowing the projection of kinematic constraints to the design of reference trajectories. We validate the proposed algorithm in high-fidelity simulations and on in-house humanoid hardware, demonstrating its capability for variable-frequency and 3-D discrete terrain locomotion with only a one-step preview of terrain data.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p069.pdf",
        "supp": "",
        "pdf_size": 4131245,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7171867122225560080&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "University of Southern California, USA; University of Southern California, USA; University of Southern California, USA; University of Southern California, USA",
        "aff_domain": "usc.edu;usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu;usc.edu",
        "github": "",
        "project": "https://youtu.be/UqLDYHGL5EA",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/69/",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "30",
        "author": "Yuhan Xie; Yixi Cai; Yinqiang Zhang; Lei Yang; Jia Pan",
        "abstract": "This research tackles the challenge of real-time active view selection and uncertainty quantification on visual quality for active 3D reconstruction. Visual quality is a critical aspect of 3D reconstruction. Recent advancements such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have notably enhanced the image rendering quality of reconstruction models. Nonetheless, the efficient and effective acquisition of input images for reconstruction\u2014specifically, the selection of the most informative viewpoint\u2014remains an open challenge, which is crucial for active reconstruction. Existing studies have primarily focused on evaluating geometric completeness and exploring unobserved or unknown regions, without direct evaluation of the visual uncertainty within the reconstruction model. To address this gap, this paper introduces a probabilistic model that quantifies visual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we formulate a criterion, Gaussian Splatting Shannon Mutual Information (GauSS-MI), for real-time assessment of visual mutual information from novel viewpoints, facilitating the selection of next best view. GauSS-MI is implemented within an active reconstruction system integrated with a view and motion planner. Extensive experiments across various simulated scenes showcase the superior visual quality and reconstruction efficiency performance of the proposed system.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p030.pdf",
        "supp": "",
        "pdf_size": 6705118,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:q7puX4DgovgJ:scholar.google.com/&scioq=GauSS-MI:+Gaussian+Splatting+Shannon+Mutual+Information+for+Active+3D+Reconstruction&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "School of Computing and Data Science, The University of Hong Kong, Hong Kong SAR, China+Centre for Transformative Garment Production, Hong Kong SAR, China; Division of Robotics, Perception, and Learning, KTH Royal Institute of Technology, Stockholm, Sweden; School of Computing and Data Science, The University of Hong Kong, Hong Kong SAR, China+Centre for Transformative Garment Production, Hong Kong SAR, China; Faculty of Engineering, The University of Hong Kong, Hong Kong SAR, China+Centre for Transformative Garment Production, Hong Kong SAR, China; School of Computing and Data Science, The University of Hong Kong, Hong Kong SAR, China+Centre for Transformative Garment Production, Hong Kong SAR, China",
        "aff_domain": "connect.hku.hk;connect.hku.hk;cs.hku.hk;kth.se;hku.hk",
        "email": "connect.hku.hk;connect.hku.hk;cs.hku.hk;kth.se;hku.hk",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/30/",
        "aff_unique_index": "0+1;2;0+1;0+1;0+1",
        "aff_unique_norm": "University of Hong Kong;Centre for Transformative Garment Production;KTH Royal Institute of Technology",
        "aff_unique_dep": "School of Computing and Data Science;;Division of Robotics, Perception, and Learning",
        "aff_unique_url": "https://www.hku.hk;;https://www.kth.se",
        "aff_unique_abbr": "HKU;;KTH",
        "aff_campus_unique_index": "0;2;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;;Stockholm",
        "aff_country_unique_index": "0+0;1;0+0;0+0;0+0",
        "aff_country_unique": "China;Sweden"
    },
    {
        "title": "Generalizing Safety Beyond Collision-Avoidance via Latent-Space Reachability Analysis",
        "session": "Control and Dynamics",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "113",
        "author": "Kensuke Nakamura; Lasse Peters; Andrea Bajcsy",
        "abstract": "Hamilton-Jacobi (HJ) reachability is a rigorous mathematical framework that enables robots to simultaneously detect unsafe states and generate actions that prevent future failures. While in theory, HJ reachability can synthesize safe controllers for nonlinear systems and nonconvex constraints, in practice, it has been limited to hand-engineered collision- avoidance constraints modeled via low-dimensional state-space representations and first-principles dynamics. In this work, our goal is to generalize safe robot controllers to prevent failures that are hard\u2014if not impossible\u2014to write down by hand, but can be intuitively identified from high-dimensional observations: for example, spilling the contents of a bag. We propose Latent Safety Filters, a latent-space generalization of HJ reachability that tractably operates directly on raw observation data (e.g., RGB images) by performing safety analysis in the latent embedding space of a generative world model. This transforms nuanced constraint specification to a classification problem in latent space and enables reasoning about dynamical consequences that are hard to simulate. In simulation and hardware experiments, we use Latent Safety Filters to safeguard arbitrary policies (from generative policies to direct teleoperation) from complex safety hazards, like preventing a Franka Research 3 manipulator from spilling the contents of a bag or toppling cluttered objects.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p113.pdf",
        "supp": "",
        "pdf_size": 7110754,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=622600443454262235&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Carnegie Mellon University; Delft University of Technology; Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;tudelft.nl;andrew.cmu.edu",
        "email": "andrew.cmu.edu;tudelft.nl;andrew.cmu.edu",
        "github": "",
        "project": "https://kensukenk.github.io/latent-safety/",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/113/",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Delft University of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.tudelft.nl",
        "aff_unique_abbr": "CMU;TU Delft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Netherlands"
    },
    {
        "title": "GeoDEx: A Unified Geometric Framework for Tactile Dexterous and Extrinsic Manipulation under Force Uncertainty",
        "session": "Manipulation I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "57",
        "author": "Sirui Chen; Sergio Francisco Aguilera Marinovic; Soshi Iba; Rana Soltani Zarrin",
        "abstract": "Sense of touch that allows robots to detect contact and measure interaction forces enables them to perform challenging tasks such as grasping fragile objects or using tools. Tactile sensors in theory can equip the robots with such capabilities. However, accuracy of the measured forces is not on a par with those of the force sensors due to the potential calibration challenges and noise. This has limited the values these sensors can offer in manipulation applications that require force control. In this paper, we introduce GeoDEx, a unified estimation, planning, and control framework using geometric primitives such as plane, cone and ellipsoid, which enables dexterous as well as extrinsic manipulation in the presence of uncertain force readings. Through various experimental results, we show that while relying on direct inaccurate and noisy force readings from tactile sensors results in unstable or failed manipulation, our method enables successful grasping and extrinsic manipulation of different objects. Additionally, compared to directly running optimization using SOCP (Second Order Cone Programming), planning and force estimation using our framework achieves a 14x speed-up.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p057.pdf",
        "supp": "",
        "pdf_size": 7149057,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:5QjP2CCVz28J:scholar.google.com/&scioq=GeoDEx:+A+Unified+Geometric+Framework+for+Tactile+Dexterous+and+Extrinsic+Manipulation+under+Force+Uncertainty&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Computer Science Department, Stanford University, Stanford, CA 94305, USA; Honda Research Institute USA, San Jose, CA 95134, USA; Honda Research Institute USA, San Jose, CA 95134, USA; Honda Research Institute USA, San Jose, CA 95134, USA",
        "aff_domain": "stanford.edu; ; ;honda-ri.com",
        "email": "stanford.edu; ; ;honda-ri.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/57/",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Stanford University;Honda Research Institute USA",
        "aff_unique_dep": "Computer Science Department;",
        "aff_unique_url": "https://www.stanford.edu;https://honda-ri.com",
        "aff_unique_abbr": "Stanford;HRI USA",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Stanford;San Jose",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Geometric Gait Optimization for Kinodynamic Systems Using a Lie Group Integrator",
        "session": "Control and Dynamics",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "119",
        "author": "Yanhao Yang; Ross Hatton",
        "abstract": "This paper presents a gait optimization and motion planning framework for a class of locomoting systems with mixed kinematic and dynamic properties. Using Lagrangian reduction and differential geometry, we derive a general dynamic model that incorporates second-order dynamics and nonholonomic constraints, applicable to kinodynamic systems such as wheeled robots with nonholonomic constraints as well as swimming robots with nonisotropic fluid-added inertia and viscous drag. Building on Lie group integrators and group symmetries, we develop a variational gait optimization method for kinodynamic systems. By integrating multiple gaits and their transitions, we construct comprehensive motion plans that enable a wide range of motions for these systems. We evaluate our framework on three representative examples: roller racer, snakeboard, and swimmer. Simulation and hardware experiments demonstrate diverse motions, including acceleration, steady-state maintenance, gait transitions, and turning. The results highlight the effectiveness of the proposed method and its potential for generalization to other biological and robotic locomoting systems.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p119.pdf",
        "supp": "",
        "pdf_size": 7717410,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_vrDMizTJg8J:scholar.google.com/&scioq=Geometric+Gait+Optimization+for+Kinodynamic+Systems+Using+a+Lie+Group+Integrator&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Collaborative Robotics and Intelligent Systems (CoRIS) Institute at Oregon State University; Collaborative Robotics and Intelligent Systems (CoRIS) Institute at Oregon State University",
        "aff_domain": "oregonstate.edu;oregonstate.edu",
        "email": "oregonstate.edu;oregonstate.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/119/",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "Collaborative Robotics and Intelligent Systems (CoRIS) Institute",
        "aff_unique_url": "https://oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Corvallis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Global Contact-Rich Planning with Sparsity-Rich Semidefinite Relaxations",
        "session": "Planning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "46",
        "author": "Shucheng Kang; Guorui Liu; Heng Yang",
        "abstract": "We show that contact-rich motion planning is also",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p046.pdf",
        "supp": "",
        "pdf_size": 21378901,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12080359592254273067&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Engineering and Applied Sciences, Harvard University; School of Mathematical Sciences, University of Science and Technology of China; School of Engineering and Applied Sciences, Harvard University",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "https://computationalrobotics.seas.harvard.edu/project-spot/",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/46/",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Harvard University;University of Science and Technology of China",
        "aff_unique_dep": "School of Engineering and Applied Sciences;School of Mathematical Sciences",
        "aff_unique_url": "https://www.harvard.edu;http://www.ustc.edu.cn",
        "aff_unique_abbr": "Harvard;USTC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Gripper Pose and Object Pointflow as Interfaces for Robotic Bimanual Manipulation",
        "session": "Imitation Learning II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "160",
        "author_site": "Yuyin Yang; Zetao Cai; Yang Tian; Jia Zeng; Jiangmiao Pang",
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "author": "",
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://roboticsconference.org/program/papers/160/"
    },
    {
        "title": "HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit",
        "session": "Humanoids",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "70",
        "author": "Qingwei Ben; Feiyu Jia; Jia Zeng; Junting Dong; Dahua Lin; Jiangmiao Pang",
        "abstract": "Current humanoid teleoperation systems either lack reliable low-level control policies, or struggle to acquire accurate whole-body control commands, making it difficult to teleoperate humanoids for loco-manipulation tasks. To solve these issues, we propose HOMIE, a novel humanoid teleoperation system that integrates a humanoid loco-manipulation policy and a low-cost exoskeleton-based cockpit. The policy enables humanoid robots to walk and squat to specific heights while accommodating arbitrary upper-body poses. This is achieved through our novel RL-based training framework that incorporates upper-body poses curriculum, height-tracking reward, and symmetry utilization, without relying on any motion priors. Complementing the policy, the cockpit integrates isomorphic exoskeleton arms, hands, and a pedal, allowing a single operator to achieve full control of the humanoid robot. Our experiments show our system facilitates more stable, rapid, and precise humanoid loco-manipulation teleoperation, accelerating task completion and eliminating retargeting errors compared to inverse kinematics-based methods. We also validate the effectiveness of the data collected by our system for imitation learning.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p070.pdf",
        "supp": "",
        "pdf_size": 21003115,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18175957527239044492&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/70/"
    },
    {
        "title": "Hierarchical Temporal Logic Task and Motion Planning for Multi-Robot Systems",
        "session": "Multi-Robot Systems",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "99",
        "author": "Zhongqi Wei; Xusheng Luo; Changliu Liu",
        "abstract": "Task and motion planning (TAMP) for multi-robot systems, combining discrete task planning with continuous motion planning, presents a significant challenge in robotics. Current TAMP methodologies, which typically rely on sampling-based or optimization-based approaches, often fail to effectively scale to multi-robot systems with complex specifications, resulting in infeasible solutions and extended solve times. In this paper, we address the hierarchical temporal logic TAMP problem for multi-robot systems, where complex tasks are defined using expressive hierarchical temporal logic specifications, and task assignments to individual robots are not predetermined. We introduce an efficient convex-optimization-based method that incorporates hierarchical temporal logic TAMP within a hybrid Graph of Convex Sets (GCS) framework. This framework enables simultaneous consideration of both the discrete task level and the continuous motion level and is proven to be sound and complete. To reduce the complexity of the GCS framework, we apply multiple heuristics to prune the graph, thereby reducing optimization time. Using a multi-robot cooperative pick-and-place task as a case study, we incorporate handover constraints into the hybrid GCS framework and address the optimization using mixed-integer convex programming (MICP). Our evaluations, conducted on various high-dimensional multi-robot systems in both simulated and real-world environments\u2014including quadrupeds, robotic arms, and automated conveyor systems\u2014demonstrate that our method surpasses existing approaches in terms of execution time and optimality. We also demonstrate that our method scales effectively with the complexity of tasks.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p099.pdf",
        "supp": "",
        "pdf_size": 13249153,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:g9jOKGTu5s0J:scholar.google.com/&scioq=Hierarchical+Temporal+Logic+Task+and+Motion+Planning+for+Multi-Robot+Systems&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA",
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/99/",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Hierarchical and Modular Network on Non-prehensile Manipulation in General Environments",
        "session": "Manipulation III",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "154",
        "author": "Yoonyoung Cho; Junhyek Han; Jisu Han; Beomjoon Kim",
        "abstract": "For robots to operate in general environments like households, they must be able to perform non-prehensile manipulation actions such as toppling and rolling to manipulate ungraspable objects. However, prior works on non-prehensile manipulation cannot yet generalize across environments with diverse geometries. The main challenge lies in adapting to varying environmental constraints: within a cabinet, the robot must avoid walls and ceilings; to lift objects to the top of a step, the robot must account for the step\u2019s pose and extent. While deep reinforcement learning (RL) has demonstrated impressive success in non-prehensile manipulation, accounting for such variability presents a challenge for the generalist policy, as it must learn diverse strategies for each new combination of constraints. To address this, we propose a modular and reconfigurable architecture that adaptively reconfigures network modules based on task requirements. To capture the geometric variability in environments, we extend the contact-based object representation (CORN) to environment geometries, and propose a procedural algorithm for generating diverse environments to train our agent. Taken together, the resulting policy can zero-shot transfer to novel real-world environments and objects despite training entirely within a simulator. We additionally release a simulation-based benchmark featuring nine digital twins of real-world scenes with 353 objects to facilitate non-prehensile manipulation research in realistic domains.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p154.pdf",
        "supp": "",
        "pdf_size": 15234642,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:JQh6dcq0fMkJ:scholar.google.com/&scioq=Hierarchical+and+Modular+Network+on+Non-prehensile+Manipulation+in+General+Environments&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/154/"
    },
    {
        "title": "How to Coordinate UAVs and UGVs for Efficient Mission Planning? Optimizing Energy-Constrained Cooperative Routing with a DRL Framework",
        "session": "Multi-Robot Systems",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "101",
        "author": "Mohammad Safwan Mondal; Subramanian Ramasamy; Pranav Bhounsule",
        "abstract": "Efficient mission planning for cooperative systems involving Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) requires addressing energy constraints, scalability, and coordination challenges between agents. UAVs excel in rapidly covering large areas but are constrained by limited battery life, while UGVs, with their extended operational range and capability to serve as mobile recharging stations, are hindered by slower speeds. This heterogeneity makes coordination between UAVs and UGVs critical for achieving optimal mission outcomes. In this work, we propose a scalable deep reinforcement learning (DRL) framework to address the energy-constrained cooperative routing problem for multi-agent UAV-UGV teams, aiming to visit a set of task points in minimal time with UAVs relying on UGVs for recharging during the mission. The framework incorporates sortie-wise agent switching to efficiently manage multiple agents, by allocating task points and coordinating actions. Using an encoder-decoder transformer architecture, it optimizes routes and recharging rendezvous for the UAV-UGV team in the task scenario. Extensive computational experiments demonstrate the framework\u2019s superior performance over heuristic methods and a DRL baseline, delivering significant improvements in solution quality and runtime efficiency across diverse scenarios. Generalization studies validate its robustness, while dynamic scenario analyses highlight its adaptability to real-time changes with a case study. This work advances UAV-UGV cooperative routing by providing a scalable, efficient, and robust solution for multi-agent mission planning.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p101.pdf",
        "supp": "",
        "pdf_size": 8546603,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:AtCKTi5GKdMJ:scholar.google.com/&scioq=How+to+Coordinate+UAVs+and+UGVs+for+Efficient+Mission+Planning%3F+Optimizing+Energy-Constrained+Cooperative+Routing+with+a+DRL+Framework&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Department of Mechanical and Industrial Engineering, University of Illinois Chicago, IL, 60607 USA; Department of Mechanical and Industrial Engineering, University of Illinois Chicago, IL, 60607 USA; Department of Mechanical and Industrial Engineering, University of Illinois Chicago, IL, 60607 USA",
        "aff_domain": "uic.edu;uic.edu;uic.edu",
        "email": "uic.edu;uic.edu;uic.edu",
        "github": "",
        "project": "https://sites.google.com/view/muavugvdrl",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/101/",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois Chicago",
        "aff_unique_dep": "Department of Mechanical and Industrial Engineering",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining",
        "session": "Mobile Manipulation and Locomotion",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "122",
        "author": "Yaru Niu; Yunzhe Zhang; Mingyang Yu; Changyi Lin; Chenhao Li; Yikai Wang; Yuxiang Yang; Wenhao Yu; Tingnan Zhang; Zhenzhen Li; Jonathan Francis; Bingqing Chen; Jie Tan; Ding Zhao",
        "abstract": "Quadrupedal robots have demonstrated impressive locomotion capabilities in complex environments, but equipping them with autonomous versatile manipulation skills in a scalable way remains a significant challenge. In this work, we introduce a system that integrates data collection and imitation learning from both humans and LocoMan, a quadrupedal robot with multiple manipulation modes. Specifically, we introduce a teleoperation and data collection pipeline, supported by dedicated hardware, which unifies and modularizes the observation and action spaces of the human and the robot. To effectively leverage the collected data, we propose an efficient learning architecture that supports co-training and pretraining with multimodal data across different embodiments. Additionally, we construct the first manipulation dataset for the LocoMan robot, covering various household tasks in both unimanual and bimanual modes, supplemented by a corresponding human dataset. Experimental results demonstrate that our data collection and training framework significantly improves the efficiency and effectiveness of imitation learning, enabling more versatile quadrupedal manipulation capabilities. Our hardware, data, and code are open-sourced at: https://human2bots.github.io.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p122.pdf",
        "supp": "",
        "pdf_size": 14671818,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:WcfJmk4Z7AoJ:scholar.google.com/&scioq=Human2LocoMan:+Learning+Versatile+Quadrupedal+Manipulation+with+Human+Pretraining&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Google DeepMind; Google DeepMind; Google DeepMind; Bosch Center for AI; Carnegie Mellon University+Bosch Center for AI; Bosch Center for AI; Google DeepMind; Carnegie Mellon University",
        "aff_domain": ";;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;",
        "github": "",
        "project": "https://human2bots.github.io",
        "author_num": 14,
        "oa": "https://roboticsconference.org/program/papers/122/",
        "aff_unique_index": "0;0;0;0;0;0;1;1;1;2;0+2;2;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Google;Bosch Center for AI",
        "aff_unique_dep": ";Google DeepMind;Center for AI",
        "aff_unique_url": "https://www.cmu.edu;https://deepmind.com;https://www.bosch-ai.com",
        "aff_unique_abbr": "CMU;DeepMind;BCAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;1;1;1;2;0+2;2;1;0",
        "aff_country_unique": "United States;United Kingdom;Germany"
    },
    {
        "title": "IMLE Policy: Fast and Sample Efficient Visuomotor Policy Learning via Implicit Maximum Likelihood Estimation",
        "session": "Imitation Learning II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "158",
        "author": "Krishan Rana; Robert Lee; David Pershouse; Niko Suenderhauf",
        "abstract": "Recent advances in imitation learning, particularly using generative modelling techniques like diffusion, have enabled policies to capture complex multi-modal action distributions. However, these methods often require large datasets and multiple inference steps for action generation, posing challenges in robotics due to the high cost of data collection and limited computational resources. To address this, we introduce IMLE Policy, a novel behaviour cloning approach based on Rejection-Sampling Implicit Maximum Likelihood Estimation (RS-IMLE). IMLE Policy excels in low-data regimes, effectively learning from minimal demonstrations and requiring 38% less data on average to match the performance of baseline methods in learning complex multi-modal behaviours. Its simple generator-based architecture enables single-step action generation, improving inference speed by 97.3% compared to Diffusion Policy, while outperforming single-step Flow Matching. We validate our approach across diverse manipulation tasks in simulated and real-world environments, showcasing its ability to capture complex behaviours under data constraints.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p158.pdf",
        "supp": "",
        "pdf_size": 2890519,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:4bX6XvnlL-AJ:scholar.google.com/&scioq=IMLE+Policy:+Fast+and+Sample+Efficient+Visuomotor+Policy+Learning+via+Implicit+Maximum+Likelihood+Estimation&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "QUT Centre for Robotics; QUT Centre for Robotics; QUT Centre for Robotics; QUT Centre for Robotics",
        "aff_domain": "qut.edu.au; ; ; ",
        "email": "qut.edu.au; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/158/",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Queensland University of Technology",
        "aff_unique_dep": "Centre for Robotics",
        "aff_unique_url": "https://www.qut.edu.au",
        "aff_unique_abbr": "QUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Implicit Neural-Representation Learning for Elastic Deformable-Object Manipulations",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "35",
        "author": "Jeongho Ha; Minseok Song; Bonggyeong Park; Daehyung Park",
        "abstract": "We aim to solve the problem of manipulating deformable objects, particularly elastic bands, in real-world scenarios. However, deformable object manipulation (DOM) requires a policy that works on a large state space, due to the unlimited degree of freedom (DoF) of deformable objects. Further, their dense but partial observations (e.g., images or point clouds) may lower the sampling complexity and uncertainty in policy learning. To figure it out, we propose a novel implicit neural-representation (INR) learning for elastic DOMs, called INR-DOM. Our method learns consistent state representations associated with partially observable elastic objects reconstructing a complete and implicit surface represented as a signed distance function. Furthermore, we perform exploratory representation fine-tunning through reinforcement learning (RL) that enables RL algorithms to effectively learn exploitable representations while efficiently obtaining a DOM policy. We perform quantitative and qualitative analysis building three simulated environments and real-world manipulation studies with a Franka Emika Panda arm.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p035.pdf",
        "supp": "",
        "pdf_size": 10387812,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:O8YFMP6rzAcJ:scholar.google.com/&scioq=Implicit+Neural-Representation+Learning+for+Elastic+Deformable-Object+Manipulations&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "KAIST; KAIST; KAIST; KAIST",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "github": "",
        "project": "http://inr-dom.github.io",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/35/",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems",
        "session": "Multi-Robot Systems",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "102",
        "author": "Anoop Kiran; Nora Ayanian; Kenneth Breuer",
        "abstract": "Flying multiple quadrotors in close proximity presents a significant challenge due to complex aerodynamic interactions, particularly downwash effects that are known to destabilize vehicles and degrade performance. Traditionally, multi-quadrotor systems rely on conservative strategies, such as collision avoidance zones around the robot volume, to circumvent this effect. This restricts their capabilities by requiring a large volume for the operation of a multi-quadrotor system, limiting their applicability in dense environments. This work provides a comprehensive, data-driven analysis of the downwash effect, with a focus on characterizing, analyzing, and understanding forces, moments, and velocities in both single and multi-quadrotor configurations. We use measurements of forces and torques to characterize vehicle interactions, and particle image velocimetry (PIV) to quantify the spatial features of the downwash wake for a single quadrotor and an interacting pair of quadrotors. This data can be used to inform physics-based strategies for coordination, leverage downwash for optimized formations, expand the envelope of operation, and improve the robustness of multi-quadrotor control.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p102.pdf",
        "supp": "",
        "pdf_size": 10150295,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "School of Engineering, Brown University, Providence, Rhode Island, USA; School of Engineering, Brown University, Providence, Rhode Island, USA; School of Engineering, Brown University, Providence, Rhode Island, USA",
        "aff_domain": "brown.edu;brown.edu;brown.edu",
        "email": "brown.edu;brown.edu;brown.edu",
        "github": "",
        "project": "https://doi.org/10.26300/64d4-wa17",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/102/",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "School of Engineering",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Providence",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Interface-level Intent Inference for Environment-agnostic Robot Teleoperation Assistance",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "81",
        "author": "Larisa Y.c. Loke; Brenna Argall",
        "abstract": "In robot teleoperation, humans often issue control signals through an interface that requires physical actuation. This interface-level interaction largely goes unmodeled within the field, yet the robot\u2019s interpretation of an interface-level command can differ from what was intended by the user, as a result of diminished human ability or inadequate mappings from raw interface signals to robot control signals. Interface-aware systems aim to address this limitation in robot teleoperation by explicitly considering the impact of a control interface on user input quality when interpreting interface signals for robot control. This work presents an interface-aware formulation for the direct inference of intended interface-level commands given known interaction characteristics of a control interface using data-driven modeling, allowing for teleoperation assistance without knowledge of the human\u2019s policy. In our specific implementation, we tailor the formulation to model a user\u2019s operation of a sip/puff interface using a network of Gated Recurrent Units, chosen for their ability to model temporal patterns and suitability for data-scarce domains. The resulting model is agnostic to the robot being controlled, which allows for its use in task- and environment-agnostic robot teleoperation assistance. We deploy this model in two variations of assisted  eleoperation frameworks using a sip/puff to control a 7-DoF robotic arm, and conduct a human subjects study with spinal cord injured participants to evaluate the efficacy of our method. Our proposed task- and environment-agnostic formulation is effective in reducing collisions during teleoperation, and is preferred by users over teleoperation baselines for ease and intuitiveness of robot operation.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p081.pdf",
        "supp": "",
        "pdf_size": 1149398,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:8gNPerhvwrcJ:scholar.google.com/&scioq=Interface-level+Intent+Inference+for+Environment-agnostic+Robot+Teleoperation+Assistance&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Department of Mechanical Engineering, Northwestern University, Evanston, IL+Shirley Ryan AbilityLab, Chicago, IL, USA; Department of Computer Science, Northwestern University, Evanston, IL+Department of Physical Medicine and Rehabilitation, Northwestern University, Chicago, IL+Shirley Ryan AbilityLab, Chicago, IL, USA",
        "aff_domain": "u.northwestern.edu;northwestern.edu",
        "email": "u.northwestern.edu;northwestern.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/81/",
        "aff_unique_index": "0+1;0+0+1",
        "aff_unique_norm": "Northwestern University;Shirley Ryan AbilityLab",
        "aff_unique_dep": "Department of Mechanical Engineering;",
        "aff_unique_url": "https://www.northwestern.edu;https://abilitylab.org",
        "aff_unique_abbr": "NU;",
        "aff_campus_unique_index": "0+1;0+1+1",
        "aff_campus_unique": "Evanston;Chicago",
        "aff_country_unique_index": "0+0;0+0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Interruption Handling for Conversational Robots",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "89",
        "author": "Shiye Cao; Jiwon Moon; Amama Mahmood; Victor Nikhil Antony; Ziang Xiao; Anqi Liu; Chien-Ming Huang",
        "abstract": "Interruptions, a fundamental component of human communication, can enhance the dynamics and effectiveness of conversations, but only when effectively managed by all parties involved. Despite advancements in robotic systems, state-of-the-art systems still have limited capabilities in handling user-initiated interruptions in real-time. Prior research has primarily focused on post hoc analysis of interruptions. To address this gap, we present a system that detects user-initiated interruptions and manages them in real-time based on the interrupter\u2019s intent (i.e., cooperative agreement, cooperative assistance, cooperative clarification, or disruptive interruption). The system was designed based on interaction patterns identified from human-human interaction data. We integrated our system into an LLM-powered social robot and validated its effectiveness through a timed decision-making task and a contentious discussion task with 21 participants. Our system successfully handled 93.69% (n=104/111) of user-initiated interruptions. We discuss our learnings and their implications for designing interruption-handling behaviors in conversational robots.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p089.pdf",
        "supp": "",
        "pdf_size": 1352962,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9011559535309876473&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://roboticsconference.org/program/papers/89/"
    },
    {
        "title": "Is Your Imitation Learning Policy Better than Mine? Policy Comparison with Near-Optimal Stopping",
        "session": "Imitation Learning I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "77",
        "author": "David Snyder; Asher James Hancock; Apurva Badithela; Emma Dixon; Patrick Miller; Rares Andrei Ambrus; Anirudha Majumdar; Masha Itkina; Haruki Nishimura",
        "abstract": "Imitation learning has enabled robots to perform complex, long-horizon tasks in challenging dexterous manipulation settings.  As new methods are developed, they must be rigorously evaluated and compared against corresponding baselines through repeated evaluation trials. However, policy comparison is fundamentally constrained by a small feasible sample size (e.g., 10 or 50) due to significant human effort and limited inference throughput of policies. This paper proposes a novel statistical framework for rigorously comparing two policies in the small sample size regime. Prior work in statistical policy comparison relies on batch testing, which requires a fixed, pre-determined number of trials and lacks flexibility in adapting the sample size to the observed evaluation data. Furthermore, extending the test with additional trials risks inducing inadvertent p-hacking, undermining statistical assurances. In contrast, our proposed statistical test is",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p077.pdf",
        "supp": "",
        "pdf_size": 12579610,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17016896321368572899&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Toyota Research Institute (TRI)+Princeton University; Princeton University; Princeton University; Toyota Research Institute (TRI); Toyota Research Institute (TRI); Toyota Research Institute (TRI); Princeton University; Toyota Research Institute (TRI); Toyota Research Institute (TRI)",
        "aff_domain": "princeton.edu; ; ; ; ; ; ; ; ",
        "email": "princeton.edu; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/77/",
        "aff_unique_index": "0+1;1;1;0;0;0;1;0;0",
        "aff_unique_norm": "Toyota Research Institute;Princeton University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tri.toyota.com/;https://www.princeton.edu",
        "aff_unique_abbr": "TRI;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Joint State and Noise Covariance Estimation",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "38",
        "author": "Kasra Khosoussi; Iman Shames",
        "abstract": "This paper tackles the problem of jointly estimating the noise covariance matrix alongside states (parameters such as poses and points) from measurements corrupted by Gaussian noise and, if available, prior information. In such settings, the noise covariance matrix determines the weights assigned to individual measurements in the least squares problem. We show that the joint problem exhibits a convex structure and provide a full characterization of the optimal noise covariance estimate (with analytical solutions) within joint maximum a posteriori and likelihood frameworks and several variants. Leveraging this theoretical result, we propose two novel algorithms that jointly estimate the primary parameters and the noise covariance matrix. Our BCD algorithm can be easily integrated into existing nonlinear least squares solvers, with negligible per-iteration computational overhead. To validate our approach, we conduct extensive experiments across diverse scenarios and offer practical insights into their application in robotics and computer vision estimation problems with a particular focus on SLAM.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p038.pdf",
        "supp": "",
        "pdf_size": 1705759,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9804356071722774287&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of Electrical Engineering and Computer Science, The University of Queensland, St Lucia, QLD, Australia + CSIRO Robotics, Data61; School of Engineering, The Australian National University, Canberra, ACT, Australia",
        "aff_domain": "uq.edu.au;anu.edu.au",
        "email": "uq.edu.au;anu.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/38/",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "University of Queensland;CSIRO;Australian National University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science;Data61;School of Engineering",
        "aff_unique_url": "https://www.uq.edu.au;https://www.csiro.au;https://www.anu.edu.au",
        "aff_unique_abbr": "UQ;CSIRO;ANU",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "St Lucia;;Canberra",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Kinodynamic Trajectory Following with STELA: Simultaneous Trajectory Estimation & Local Adaptation",
        "session": "Perception and Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "8",
        "author": "Edgar Granados; Sumanth Tangirala; Kostas Bekris",
        "abstract": "State estimation and control are often addressed separately, which can lead to unsafe execution due to sensing noise, execution errors and discrepancies between the planning model and reality. Simultaneous control and trajectory estimation using probabilistic graphical models has been proposed as a unified solution to these challenges. Previous work, however, relies heavily on appropriate Gaussian priors and is limited to holonomic robots with linear time-varying models. The current research extends graphical optimization methods to vehicles with arbitrary dynamical models via Simultaneous Trajectory Estimation and Local Adaptation (STELA). The overall approach first initializes feasible trajectories using a kinodynamic, sampling-based motion planner. Then, STELA simultaneously: (i) estimates the past trajectory based on noisy observations, and (ii) adapts the controls to be executed so as to minimize deviations from the planned, feasible trajectory, while avoiding collisions. The proposed factor graph representation of trajectories in STELA can be applied for any dynamical system given access to first or second-order state update equations, and introduces the duration of execution between two states in the trajectory discretization as another variable to be optimized. These features provide both generalization and flexibility in trajectory following. In addition, and targeting computational efficiency, the proposed strategy allows for the use of incremental updates of the factor graph by using the iSAM algorithm and also introduces a time-window mechanism. This mechanism allows the factor graph to be dynamically updated so as to operate over a limited past history and forward horizon of the planned trajectory. This enables the online update of controls at a minimum of 10Hz. Experiments first demonstrate that STELA achieves at least comparable performance to previous frameworks on idealized vehicles with linear dynamics. More critically, STELA directly applies to and successfully solves trajectory following problems for more complex dynamical models. Beyond generalization, simulations assess STELA\u2019s robustness under varying levels of sensing and execution noise, while ablation studies highlight the importance of different components of the approach. Real-world experiments validate STELA\u2019s practical applicability on a low-cost MuSHR robot, which exhibits high noise and non-trivial dynamics.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p008.pdf",
        "supp": "",
        "pdf_size": 3806325,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:KrEfcI0ZaVcJ:scholar.google.com/&scioq=Kinodynamic+Trajectory+Following+with+STELA:+Simultaneous+Trajectory+Estimation+%26+Local+Adaptation&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Dept. of Computer Science, Rutgers University, NJ, USA; Dept. of Computer Science, Rutgers University, NJ, USA; Dept. of Computer Science, Rutgers University, NJ, USA",
        "aff_domain": "cs.rutgers.edu;cs.rutgers.edu;cs.rutgers.edu",
        "email": "cs.rutgers.edu;cs.rutgers.edu;cs.rutgers.edu",
        "github": "",
        "project": "https://go.rutgers.edu/46618xjt",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/8/",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Rutgers University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rutgers.edu",
        "aff_unique_abbr": "Rutgers",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New Brunswick",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "LangWBC: Language-directed Humanoid Whole-Body Control via End-to-end Learning",
        "session": "Humanoids",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "65",
        "author": "Yiyang Shao; Bike Zhang; Qiayuan Liao; Xiaoyu Huang; Yuman Gao; Yufeng Chi; Zhongyu Li; Sophia Shao; Koushil Sreenath",
        "abstract": "General-purpose humanoid robots are expected to interact intuitively with humans, enabling seamless integration into daily life. Natural language provides the most accessible medium for this purpose. However, translating languages into humanoid whole-body motions remains a significant challenge, primarily due to the gap between linguistic understanding and physical actions. In this work, we present an end-to-end, language-directed policy for real-world humanoid control. Our approach combines reinforcement learning with policy distillation, allowing a single neural network to interpret language commands and execute corresponding physical actions directly. To enhance motion diversity and compositionality, we incorporate a Conditional Variational Autoencoder (CVAE) structure. The resulting policy achieves agile and versatile whole-body behaviors conditioned on language inputs, with smooth transitions between various motions, enabling iterative and adaptable control. We validate the efficacy and generalizability of our method through extensive simulations and real-world experiments, demonstrating robust whole-body control.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p065.pdf",
        "supp": "",
        "pdf_size": 14639176,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-akBafN2jksJ:scholar.google.com/&scioq=LangWBC:+Language-directed+Humanoid+Whole-Body+Control+via+End-to-end+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "https://github.com/LangWBC",
        "project": "https://youtu.be/9AN0GulqWwc",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/65/",
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation",
        "session": "Perception and Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "1",
        "author": "Pascal Roth; Jonas Frey; Cesar Cadena; Marco Hutter",
        "abstract": "Ensuring safe navigation in complex environments requires accurate real-time traversability assessment and understanding of environmental interactions relative to the robot\u2019s capabilities. Traditional methods, which assume simplified dynamics, often require designing and tuning cost functions to safely guide paths or actions toward the goal. This process is tedious, environment-dependent, and not generalizable. To overcome these issues, we propose a novel learned perceptive Forward Dynamics Model (FDM) that predicts the robot\u2019s future state conditioned on the surrounding geometry and history of proprioceptive measurements, proposing a more scalable, safer, and heuristic-free solution. The FDM is trained on multiple years of simulated navigation experience, including high-risk maneuvers and real-world interactions to incorporate the full system dynamics beyond rigid body simulation. We integrate our perceptive FDM into a zero-shot Model Predictive Path Integral (MPPI) planning framework, leveraging the learned mapping between actions, future states, and failure probability. This allows for optimizing a simplified cost function, eliminating the need for extensive cost-tuning to ensure safety. On the legged robot ANYmal, the proposed perceptive FDM improves the position estimation over competitive baselines, which translates into a significantly higher navigation success rate in rough simulation environments.  Moreover, we demonstrate effective sim-to-real transfer and showcase the benefit of training on synthetic and real data. Code and models are made publicly available.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p001.pdf",
        "supp": "",
        "pdf_size": 14775575,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:dmaqwwEh7YEJ:scholar.google.com/&scioq=Learned+Perceptive+Forward+Dynamics+Model+for+Safe+and+Platform-aware+Robotic+Navigation&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "ETH Zurich\u2020NVIDIA; ETH Zurich\u2021Max Planck Institute for Intelligent Systems; ETH Zurich; ETH Zurich",
        "aff_domain": "ethz.ch;ethz.ch;ethz.ch;ethz.ch",
        "email": "ethz.ch;ethz.ch;ethz.ch;ethz.ch",
        "github": "https://github.com/leggedrobotics/fdm",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/1/",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Learning Getting-Up Policies for Real-World Humanoid Robots",
        "session": "Humanoids",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "63",
        "author": "Xialin He; Runpei Dong; Zixuan Chen; Saurabh Gupta",
        "abstract": "Automatic recovery from falls is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Different from previous successful applications of humanoid locomotion learning, the getting-up task involves complex contact patterns, the necessity to accurately model collision geometry, and sparser rewards. We circumvent these challenges through a two-phase approach that follows a curriculum. The first stage focuses on discovering a good get up trajectory under minimal constraints on smoothness or speed / torque limits. The second stage then refines the discovered motions into deployable (",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p063.pdf",
        "supp": "",
        "pdf_size": 33974762,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16003489314336260943&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/63/"
    },
    {
        "title": "Learning Humanoid Standing-up Control across Diverse Postures",
        "session": "Humanoids",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "64",
        "author": "Tao Huang; Junli Ren; Huayi Wang; Zirui Wang; Qingwei Ben; Muning Wen; Xiao Chen; Jianan Li; Jiangmiao Pang",
        "abstract": "Standing-up control is crucial for humanoid robots, with the potential for integration into current locomotion and loco-manipulation systems. Existing approaches are either limited to simulations that neglect hardware constraints or rely on predefined ground-specific motion trajectories, failing to enable standing-up across diverse postures in the real world. To bridge this gap, we present HoST (",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p064.pdf",
        "supp": "",
        "pdf_size": 29436699,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9280410877223640034&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Shanghai AI Laboratory; Shanghai Jiao Tong University; The University of Hong Kong; Zhejiang University; The Chinese University of Hong Kong; Shanghai AI Laboratory; Shanghai Jiao Tong University; The Chinese University of Hong Kong; Shanghai AI Laboratory",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "https://github.com/OpenRobotLab/HoST",
        "project": "humanoid-standingup.github.io",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/64/",
        "aff_unique_index": "0;1;2;3;4;0;1;4;0",
        "aff_unique_norm": "Shanghai AI Laboratory;Shanghai Jiao Tong University;University of Hong Kong;Zhejiang University;Chinese University of Hong Kong",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.shanghai-ai-lab.com;https://www.sjtu.edu.cn;https://www.hku.hk;https://www.zju.edu.cn;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "SAIL;SJTU;HKU;ZJU;CUHK",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Interpretable Features from Interventions",
        "session": "Imitation Learning II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "163",
        "author": "Erin Hedlund-Botti; Julianna Schalkwyk; Nina Marie Moorman; Chuxuan Yang; Lakshmi Seelam; Sanne Van Waveren; Russell Perkins; Paul Robinette; Matthew Craig Gombolay",
        "abstract": "The behavior of in-home robots must be adaptable to end-users to adequately address individual users\u2019 needs and preferences. Learning from Demonstration (LfD) is a common approach for customizing robot behavior, enabling non-expert users to teach robots how to perform tasks according to their preferences. While LfD allows users to teach robots tasks, it can be difficult for users to specify their individual needs a priori. Therefore, we propose Learning Interpretable Features from Interventions (LIFI), a user-friendly and streamlined method for personalizing robot behavior through interventions. This approach allows users to easily prompt the robot to adapt its behavior by intervening when the robot\u2019s behavior against user expectations. With LIFI, 1) the user intervenes to communicate that the robot is making a mistake, 2) the robot then learns an explanatory feature that describes the failure and 3) uses it to adjust its policy to correct the mistake, aligning with user-specific needs. In a between-subjects evaluation experiment with 48 participants, where the robot attempts household manipulation tasks, we demonstrate that adding features via LIFI improves objective performance and subjective measures, i.e., perceived workload, usability, and trust, compared to a no-feature baseline.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p163.pdf",
        "supp": "",
        "pdf_size": 1683933,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:yjsKLcEx7WYJ:scholar.google.com/&scioq=Learning+Interpretable+Features+from+Interventions&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; University of Massachusettes Lowell; University of Massachusettes Lowell; Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;student.uml.edu;uml.edu;cc.gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;student.uml.edu;uml.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/163/",
        "aff_unique_index": "0;0;0;0;0;0;1;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;University of Massachusetts Lowell",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://www.uml.edu",
        "aff_unique_abbr": "Georgia Tech;UMass Lowell",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Lowell",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning to Act Anywhere with Task-centric Latent Actions",
        "session": "VLA Models",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "14",
        "author": "Qingwen Bu; Yanting Yang; Jisong Cai; Shenyuan Gao; Guanghui Ren; Maoqing Yao; Ping Luo; Hongyang Li",
        "abstract": "The advancement of generalist robotic models capable of executing diverse tasks across varied environments and embodiments has been impeded by the dependence on large-scale, labeled datasets and the inherent heterogeneity of action and observation spaces. To address these challenges, we introduce UniVLA, a framework designed to develop omni-purpose vision-language-action (VLA) policies that facilitate scalable and efficient planning across diverse environments and tasks. Our methodology comprises three pivotal stages: 1) Task-Centric Latent Action Learning, where we derive task-relevant action representations from extensive cross-embodiment videos in an unsupervised manner, utilizing DINOv2 features and language instructions to filter out task-irrelevant dynamics; 2) Latent Action Pretraining, where we train an auto-regressive vision-language model with discretized latent action tokens to enable embodiment-agnostic planning; and 3) Latent Action Decoding, where we translate latent plans into executable behaviors for deployment across diverse and heterogeneous robotic systems. UniVLA achieves state-of-the-art performance on multiple manipulation and navigation benchmarks, surpassing existing VLAs while requiring reduced computational cost. Extensive evaluations underscore the efficiency, scalability, and generalizability of UniVLA, presenting a promising pathway toward the development of next-generation generalist policies.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p014.pdf",
        "supp": "",
        "pdf_size": 5604977,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16158036315340498176&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "The University of Hong Kong; OpenDriveLab; AgiBot; The University of Hong Kong; OpenDriveLab; AgiBot; The University of Hong Kong; OpenDriveLab",
        "aff_domain": "; ; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ; ",
        "github": "https://github.com/OpenDriveLab/UniVLA",
        "project": "",
        "author_num": 8,
        "oa": "https://roboticsconference.org/program/papers/14/",
        "aff_unique_index": "0;1;2;0;1;2;0;1",
        "aff_unique_norm": "University of Hong Kong;OpenDriveLab;AgiBot",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.hku.hk;;",
        "aff_unique_abbr": "HKU;;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "title": "Leveling the Playing Field: Carefully Comparing Classical and Learned Controllers for Quadrotor Trajectory Tracking",
        "session": "Control and Dynamics",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "116",
        "author": "Pratik Kunapuli; Jake Welde; Dinesh Jayaraman; Vijay Kumar",
        "abstract": "Learning-based control approaches like reinforcement learning (RL) have recently produced a slew of impressive results for tasks like quadrotor trajectory tracking and drone racing. Naturally, it is common to demonstrate the advantages of these new controllers against established methods like analytical controllers. We observe, however, that reliably comparing the performance of these very different classes of controllers is more complicated than might appear at first sight. As a case study, we take up the problem of agile tracking of an end-effector for a quadrotor with a fixed-arm. We develop a set of best practices for synthesizing the best RL and Geometric controllers for benchmarking. In the process, we fix widely prevalent RL-favoring biases in prior studies that provide asymmetric access to: (1) the task definition in the form of objective functions, (2) datasets for parameter optimization, and (3) \u201cfeed-forward\u201d controller inputs revealing the desired future trajectory. The resulting contributions are threefold: first, our improved robust experimental protocol reveals that the gaps between the two controller classes are much smaller than expected from previously published findings. Geometric control performs on par or better than RL in most practical settings, while RL fares better in transient performance at the expense of steady-state error. Second, our improvements to the experimental protocol for comparing learned and classical controller synthesis approaches are critical: each of the above asymmetries can yield misleading conclusions, and we show evidence that suggests that they indeed have in prior quadrotor studies. Finally, we open-source implementations of Geometric and RL controllers for these aerial vehicles implementing best practices for future development.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p116.pdf",
        "supp": "",
        "pdf_size": 1437682,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZERv7ocs9DEJ:scholar.google.com/&scioq=Leveling+the+Playing+Field:+Carefully+Comparing+Classical+and+Learned+Controllers+for+Quadrotor+Trajectory+Tracking&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "https://pratikkunapuli.github.io/rl-vs-gc/",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/116/",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "LiDAR Registration with Visual Foundation Models",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "31",
        "author": "Niclas V\u00f6disch; Giovanni Cioffi; Marco Cannici; Wolfram Burgard; Davide Scaramuzza",
        "abstract": "LiDAR registration is a fundamental task in robotic mapping and localization. A critical component of aligning two point clouds is identifying robust point correspondences using point descriptors, which becomes particularly challenging in scenarios involving domain shifts, seasonal changes, and variations in point cloud structures. These factors substantially impact both handcrafted and learning-based approaches. In this paper, we address these problems by proposing to use DINOv2 features, obtained from surround-view images, as point descriptors. We demonstrate that coupling these descriptors with traditional registration algorithms, such as RANSAC or ICP, facilitates robust 6DoF alignment of LiDAR scans with 3D maps, even when the map was recorded more than a year before. Although conceptually straightforward, our method substantially outperforms more complex baseline techniques. In contrast to previous learning-based point descriptors, our method does not require domain-specific retraining and is agnostic to the point cloud structure, effectively handling both sparse LiDAR scans and dense 3D maps. We show that leveraging the additional camera data enables our method to outperform the best-performing baseline by +24.8 and +17.3 registration recall on the NCLT and Oxford RobotCar datasets. We will publicly release the code of our work upon acceptance of this manuscript (code is available for reviewers in the supplementary material).",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p031.pdf",
        "supp": "",
        "pdf_size": 11848533,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11582860862691445107&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Freiburg+University of Zurich; University of Zurich; University of Zurich; University of Technology Nuremberg; University of Zurich",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://vfm-registration.cs.uni-freiburg.de",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/31/",
        "aff_unique_index": "0+1;1;1;2;1",
        "aff_unique_norm": "University of Freiburg;University of Zurich;Nuremberg University of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uni-freiburg.de;https://www.unizh.ch;https://www.tu-nuernberg.de",
        "aff_unique_abbr": "UoF;UZH;TUN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;0;1",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "title": "MISO: Multiresolution Submap Optimization for Efficient Globally Consistent Neural Implicit Reconstruction",
        "session": "Perception and Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "4",
        "author": "Yulun Tian; Hanwen Cao; Sunghwan Kim; Nikolay Atanasov",
        "abstract": "Neural implicit representations have had significant impact on simultaneous localization and mapping (SLAM) by enabling robots to build continuous, differentiable, and high-fidelity 3D maps from sensor data. However, as the scale and complexity of the environment grow, neural SLAM approaches face renewed challenges in the back-end optimization process to keep up with runtime requirements and maintain global consistency. We introduce MISO, a hierarchical optimization framework that leverages multiresolution submaps to achieve efficient and scalable neural implicit reconstruction. For local SLAM within each submap, we develop a learned hierarchical optimization scheme that substantially reduces the time needed to optimize the implicit submap features. Further, to correct estimation drift globally, we develop a hierarchical method to align and fuse the multiresolution submap features directly, leading to significant acceleration by avoiding the need to decode full scene geometry. MISO significantly improves computational efficiency and estimation accuracy of neural signed distance function (SDF) SLAM on large-scale real-world benchmarks.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p004.pdf",
        "supp": "",
        "pdf_size": 13177724,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-JwuwjMXckMJ:scholar.google.com/&scioq=MISO:+Multiresolution+Submap+Optimization+for+Efficient+Globally+Consistent+Neural+Implicit+Reconstruction&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/4/",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Manual2Skill: Learning to Read Manuals and Acquire Robotic Skills for Furniture Assembly Using Vision-Language Models",
        "session": "Manipulation III",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "150",
        "author": "Chenrui Tie; Shengxiang Sun; Jinxuan Zhu; Yiwei Liu; Jingxiang Guo; Yue Hu; Haonan Chen; Junting Chen; Ruihai Wu; Lin Shao",
        "abstract": "Humans possess an extraordinary ability to understand and execute complex manipulation tasks by interpreting abstract instruction manuals. For robots, however, this capability remains a substantial challenge, as they lack the ability to interpret abstract instructions and translate them into executable actions. In this paper, we present Manual2Skill, a novel framework that enables robots to perform complex assembly tasks guided by high-level manual instructions. Our approach leverages Vision-Language Models (VLMs) to extract structured information from instructional images, which is then used to construct hierarchical assembly graphs. These graphs represent parts, subassemblies, and the relationships between them. To facilitate task execution, a pose estimation model predicts the relative 6D poses of components at each assembly step, while a motion planning module generates actionable sequences for real-world robotic implementation. We demonstrate the effectiveness of Manual2Skill by successfully assembling multiple real-world IKEA furniture items. This application highlights its ability to manage long-horizon manipulation tasks with both efficiency and precision, significantly enhancing the practicality of robot learning from instruction manuals. This work marks a step forward in advancing robotic systems capable of understanding and executing complex manipulation tasks in a manner akin to human capabilities.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p150.pdf",
        "supp": "",
        "pdf_size": 11938671,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9614838092604368753&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "National University of Singapore; University of Toronto; National University of Singapore; Sichuan University; National University of Singapore; Zhejiang University; National University of Singapore; National University of Singapore; Peking University; National University of Singapore",
        "aff_domain": "; ; ; ; ; ; ; ; ;",
        "email": "; ; ; ; ; ; ; ; ;",
        "github": "",
        "project": "https://owensun2004.github.io/Furniture-Assembly-Web/",
        "author_num": 10,
        "oa": "https://roboticsconference.org/program/papers/150/",
        "aff_unique_index": "0;1;0;2;0;3;0;0;4;0",
        "aff_unique_norm": "National University of Singapore;University of Toronto;Sichuan University;Zhejiang University;Peking University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.utoronto.ca;https://www.scu.edu.cn;https://www.zju.edu.cn;http://www.pku.edu.cn",
        "aff_unique_abbr": "NUS;U of T;SCU;ZJU;Peking U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2;0;2;0;0;2;0",
        "aff_country_unique": "Singapore;Canada;China"
    },
    {
        "title": "Map Space Belief Prediction for Manipulation-Enhanced Mapping",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "39",
        "author": "Joao Marcos Correia Marques; Nils Dengler; Jesper M\u00fccke; Tobias Zaenker; Shenlong Wang; Maren Bennewitz; Kris Hauser",
        "abstract": "Searching for objects in cluttered environments requires selecting efficient viewpoints and manipulation actions to remove occlusions and reduce uncertainty in object locations, shapes, and categories. In this work, we address the problem of manipulation-enhanced semantic mapping, where a robot has to efficiently identify all objects in a cluttered shelf. Although Partially Observable Markov Decision Processes~(POMDPs) are standard for decision-making under uncertainty, representing unstructured interactive worlds remains challenging in this formalism. To tackle this, we define a POMDP whose belief is summarized by a metric-semantic grid map and propose a novel framework that uses neural networks to perform map-space belief updates to reason efficiently and simultaneously about object geometries, locations, categories, occlusions, and manipulation physics.  Further, to enable accurate information gain analysis, the learned belief updates should maintain calibrated estimates of uncertainty.  Therefore, we propose Calibrated Neural-Accelerated Belief Updates (CNABUs) to learn a belief propagation model that generalizes to novel scenarios and provides confidence-calibrated predictions for unknown areas.  Our experiments show that our novel POMDP planner improves map completeness and accuracy over existing methods in challenging simulations and successfully transfers to real-world cluttered shelves in zero-shot fashion.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p039.pdf",
        "supp": "",
        "pdf_size": 4474617,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3154408691944102642&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Illinois at Urbana-Champaign, IL, USA; Humanoid Robots Lab, University of Bonn, Germany + The Lamarr Institute, Bonn, Germany + The Center for Robotics, University of Bonn, Germany; Humanoid Robots Lab, University of Bonn, Germany + The Lamarr Institute, Bonn, Germany + The Center for Robotics, University of Bonn, Germany; Humanoid Robots Lab, University of Bonn, Germany; University of Illinois at Urbana-Champaign, IL, USA; Humanoid Robots Lab, University of Bonn, Germany + The Lamarr Institute, Bonn, Germany + The Center for Robotics, University of Bonn, Germany; University of Illinois at Urbana-Champaign, IL, USA",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://roboticsconference.org/program/papers/39/",
        "aff_unique_index": "0;1+2+1;1+2+1;1;0;1+2+1;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;University of Bonn;Lamarr Institute",
        "aff_unique_dep": ";Humanoid Robots Lab;",
        "aff_unique_url": "https://illinois.edu;https://www.uni-bonn.de;",
        "aff_unique_abbr": "UIUC;;",
        "aff_campus_unique_index": "0;2;2;0;2;0",
        "aff_campus_unique": "Urbana-Champaign;;Bonn",
        "aff_country_unique_index": "0;1+1+1;1+1+1;1;0;1+1+1;0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving",
        "session": "Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "139",
        "author": "Jacob Levy; Jason Gibson; Bogdan Vlahov; Erica Tevere; Evangelos Theodorou; David Fridovich-Keil; Patrick Spieler",
        "abstract": "High-speed off-road autonomous driving presents unique challenges due to complex, evolving terrain characteristics and the difficulty of accurately modeling terrain-vehicle interactions. While dynamics models used in model-based control can be learned from real-world data, they often struggle to generalize to unseen terrain, making real-time adaptation essential. We propose a novel framework that combines a Kalman filter-based online adaptation scheme with meta-learned parameters to address these challenges. Offline meta-learning optimizes the basis functions along which adaptation occurs, as well as the adaptation parameters, while online adaptation dynamically adjusts the onboard dynamics model in real time for model-based control. We validate our approach through extensive experiments, including real-world testing on a full-scale autonomous off-road vehicle, demonstrating that our method outperforms baseline approaches in prediction accuracy, performance, and safety metrics, particularly in safety-critical scenarios. Our results underscore the effectiveness of meta-learned dynamics model adaptation, advancing the development of reliable autonomous systems capable of navigating diverse and unseen environments.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p139.pdf",
        "supp": "",
        "pdf_size": 2119991,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:MfWeeT3ZGlsJ:scholar.google.com/&scioq=Meta-Learning+Online+Dynamics+Model+Adaptation+in+Off-Road+Autonomous+Driving&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://roboticsconference.org/program/papers/139/"
    },
    {
        "title": "Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "80",
        "author": "Zongzheng Zhang; Jiawen Yang; Ziqiao Peng; Meng Yang; Jianzhu Ma; Lin Cheng; Huazhe Xu; Hang Zhao; Hao Zhao",
        "abstract": "Previous animatronic faces struggle to effectively express emotions due to both hardware and software limitations. On the hardware side, earlier approaches either used rigid-driven mechanisms, which provide precise control but are difficult to design within constrained spaces, or tendon-driven mechanisms, which are more space-efficient but challenging to control. In contrast, we propose a hybrid actuation approach that combines the best of both worlds. The eyes and mouth\u2014key areas for emotional expression\u2014are controlled using rigid mechanisms for precise movement, while the nose and cheeks, which convey subtle facial microexpressions, are driven by strings. This design allows us to build a compact yet versatile hardware platform capable of expressing a wide range of emotions. On the algorithmic side, our method introduces a self-modelling network that maps motor actions to facial landmarks, allowing us to automatically establish the relationship between blendshape primitives for different facial expressions and the corresponding motor control signals through gradient backpropagation. We then train a neural network to map speech input to corresponding blendshape controls. With our method, we can generate distinct emotional expressions (happy, fear, disgust, and anger) from any given sentence, each with nuanced, emotion-specific control signals\u2014a feature that has not been demonstrated in earlier systems.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p080.pdf",
        "supp": "",
        "pdf_size": 89424001,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Institute for AI Industry Research (AIR), Tsinghua University+Beijing Academy of Artificial Intelligence (BAAI); Institute for AI Industry Research (AIR), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University; MGI Tech, Shenzhen, China; Institute for AI Industry Research (AIR), Tsinghua University; Beihang University; Institute for Interdisciplinary Information Sciences(IIIS), Tsinghua University; Institute for Interdisciplinary Information Sciences(IIIS), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University+Beijing Academy of Artificial Intelligence (BAAI)",
        "aff_domain": "air.tsinghua.edu.cn; ; ; ; ; ; ; ;air.tsinghua.edu.cn",
        "email": "air.tsinghua.edu.cn; ; ; ; ; ; ; ;air.tsinghua.edu.cn",
        "github": "https://github.com/ZZongzheng0918/Morpheus-Hardware; https://github.com/ZZongzheng0918/Morpheus-Software",
        "project": "",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/80/",
        "aff_unique_index": "0+1;0;0;2;0;3;0;0;0+1",
        "aff_unique_norm": "Tsinghua University;Beijing Academy of Artificial Intelligence;MGI Tech;Beihang University",
        "aff_unique_dep": "Institute for AI Industry Research (AIR);;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.baai.ac.cn;;http://www.buaa.edu.cn/",
        "aff_unique_abbr": "Tsinghua;BAAI;;BUAA",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "NaVILA: Legged Robot Vision-Language-Action Model for Navigation",
        "session": "VLA Models",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "18",
        "author": "An-Chieh Cheng; Yandong Ji; Zhaojing Yang; Zaitian Gongye; Xueyan Zou; Jan Kautz; Erdem Biyik; Hongxu Yin; Sifei Liu; Xiaolong Wang",
        "abstract": "This paper proposes to solve the problem of Vision-and-Language Navigation with legged robots, which not only provides a flexible way for humans to command but also allows the robot to navigate through more challenging and cluttered scenes. However, it is non-trivial to translate human language instructions all the way to low-level leg joint actions. We propose NaVILA, a 2-level framework that unifies a Vision-Language-Action model (VLA) with locomotion skills. Instead of directly predicting low-level actions from VLA, NaVILA first generates mid-level actions with spatial information in the form of language, (e.g., \u201cmoving forward 75cm\u201d), which serves as an input for a visual locomotion RL policy for execution. NaVILA substantially improves previous approaches on existing benchmarks. The same advantages are demonstrated in our newly developed benchmarks with IsaacLab, featuring more realistic scenes, low-level controls, and real-world robot experiments.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p018.pdf",
        "supp": "",
        "pdf_size": 9743002,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9485429853238170935&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "UC San Diego; UC San Diego; USC; UC San Diego; UC San Diego; NVIDIA; USC; NVIDIA; NVIDIA; UC San Diego+NVIDIA",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "",
        "project": "https://navila-bot.github.io",
        "author_num": 10,
        "oa": "https://roboticsconference.org/program/papers/18/",
        "aff_unique_index": "0;0;1;0;0;2;1;2;2;0+2",
        "aff_unique_norm": "University of California, San Diego;University of Southern California;NVIDIA",
        "aff_unique_dep": ";;NVIDIA Corporation",
        "aff_unique_url": "https://www.ucsd.edu;https://www.usc.edu;https://www.nvidia.com",
        "aff_unique_abbr": "UCSD;USC;NVIDIA",
        "aff_campus_unique_index": "0;0;1;0;0;1;0",
        "aff_campus_unique": "San Diego;Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Neural Inertial Odometry from Lie Events",
        "session": "Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "143",
        "author": "Royina Karegoudra Jayanth; Yinshuang Xu; Evangelos Chatzipantazis; Kostas Daniilidis; Daniel Gehrig",
        "abstract": "Neural displacement priors (NDPs) can reduce the drift in inertial odometry and provide uncertainty estimates that can be readily fused with off-the-shelf filters. However, they fail to generalize to different IMU sampling rates and trajectory profiles, which limits their robustness in diverse settings. To address this challenge, we replace the traditional NDP inputs comprising raw IMU data with",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p143.pdf",
        "supp": "",
        "pdf_size": 1126762,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:cnMMF7H8y3sJ:scholar.google.com/&scioq=Neural+Inertial+Odometry+from+Lie+Events&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/RoyinaJayanth/NIO LieEvents",
        "project": "",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/143/"
    },
    {
        "title": "Novel Demonstration Generation with Gaussian Splatting Enables Robust One-Shot Manipulation",
        "session": "Manipulation III",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "146",
        "author": "Sizhe Yang; Wenye Yu; Jia Zeng; Jun Lv; Kerui Ren; Cewu Lu; Dahua Lin; Jiangmiao Pang",
        "abstract": "Visuomotor policies learned through imitation learning methods often struggle to generalize to new visual domains due to the limited diversity of expert demonstrations, and collecting extensive real-world data is exhaustive.  To address this challenge, we propose a novel demonstration generation approach leveraging 3D Gaussian Splatting (3DGS), an explicit and interpretable means of 3D scene representation.  Our method reconstructs manipulation scenes with high fidelity and enables autonomous scene editing, giving rise to novel scene configurations. Stemming from a single expert demonstration, diversified data are generated across various visual domains, including different object poses, object types, camera views, scene appearance, lighting conditions, and robot embodiments.  Comprehensive real-world experiments suggest that our demonstration generation pipeline significantly enhances the generalization of visuomotor policies when confronting multiple disturbances. Specifically, while policies trained on real-world demonstrations achieve an average success rate of less than 10%, our method lifts this number to 85.9% across various task settings and scenarios.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p146.pdf",
        "supp": "",
        "pdf_size": 9741144,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10534308416062831881&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Shanghai AI Laboratory + The Chinese University of Hong Kong; Shanghai AI Laboratory + Shanghai Jiao Tong University; Shanghai AI Laboratory; Shanghai Jiao Tong University; Shanghai AI Laboratory + Shanghai Jiao Tong University; Shanghai Jiao Tong University; The Chinese University of Hong Kong; Shanghai AI Laboratory",
        "aff_domain": "; ; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://yangsizhe.github.io/robosplat/",
        "author_num": 8,
        "oa": "https://roboticsconference.org/program/papers/146/",
        "aff_unique_index": "0+1;0+2;0;2;0+2;2;1;0",
        "aff_unique_norm": "Shanghai AI Laboratory;Chinese University of Hong Kong;Shanghai Jiao Tong University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.shanghai-ai-lab.com;https://www.cuhk.edu.hk;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SAIL;CUHK;SJTU",
        "aff_campus_unique_index": "1;;;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "On the Surprising Robustness of Sequential Convex Optimization for Contact-Implicit Motion Planning",
        "session": "Planning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "47",
        "author": "Yulin Li; Haoyu Han; Shucheng Kang; Jun Ma; Heng Yang",
        "abstract": "Contact-implicit motion planning\u2014embedding contact sequencing as implicit complementarity constraints\u2014holds the promise of leveraging continuous optimization to discover new contact patterns online. Nevertheless, the resulting optimization, being an instance of Mathematical Programming with Complementary Constraints, fails the classical constraint qualifications that are crucial for the convergence of popular numerical solvers. We present robust contact-implicit motion planning with sequential convex programming (CRISP), a solver that departs from the usual primal-dual algorithmic framework but instead only focuses on the primal problem. CRISP solves a convex quadratic program with an adaptive trust region radius at each iteration, and its convergence is evaluated by a merit function using weighted l_1 penalty. We (i) provide sufficient conditions on CRISP\u2019s convergence to first-order stationary points of the merit function; (ii) release a high-performance C++ implementation of CRISP with a generic nonlinear programming interface; and (iii) demonstrate CRISP\u2019s surprising robustness in solving contact-implicit planning with naive initialization. In fact, CRISP solves several contact-implicit problems with all-zero initialization.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p047.pdf",
        "supp": "",
        "pdf_size": 28151969,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:yIbtbPDZ2ywJ:scholar.google.com/&scioq=On+the+Surprising+Robustness+of+Sequential+Convex+Optimization+for+Contact-Implicit+Motion+Planning&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "The Hong Kong University of Science and Technology+Harvard University; Harvard University; Harvard University; The Hong Kong University of Science and Technology; Harvard University",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://computationalrobotics.seas.harvard.edu/CRISP",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/47/",
        "aff_unique_index": "0+1;1;1;0;1",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Harvard University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ust.hk;https://www.harvard.edu",
        "aff_unique_abbr": "HKUST;Harvard",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0+1;1;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Online Competitive Information Gathering for Partially Observable Trajectory Games",
        "session": "Multi-Robot Systems",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "96",
        "author": "Mel Krusniak; Hang Xu; Forrest John Laine",
        "abstract": "To cooperate or compete rationally in continuous, partially observable multi-agent spaces, game theoretic agents must make plans that optimally gather information about their opponents. These problems are modeled by partially observable stochastic games (POSGs), but planning in fully continuous POSGs is intractable without heavy offline computation or assumptions on the order of belief players maintain. We formulate a finite history/horizon refinement of POSGs which admits competitive information gathering behavior in trajectory space, and through a series of approximations, we present an online method for computing rational trajectory plans in these games, leveraging particle-based estimations of the joint state space to perform stochastic gradient play. We also provide the necessary adjustments required to deploy this method on individual agents. The method is tested in continuous pursuit-evasion and warehouse-pickup scenarios (alongside extensions to N>2 players and to more complex environments with visual and physical obstacles), demonstrating evidence of active information gathering and outperforming passive competitors.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p096.pdf",
        "supp": "",
        "pdf_size": 542526,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:tfME9ju4dqIJ:scholar.google.com/&scioq=Online+Competitive+Information+Gathering+for+Partially+Observable+Trajectory+Games&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/96/"
    },
    {
        "title": "Optimal Interactive Learning on the Job via Facility Location Planning",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "87",
        "author": "Shivam Vats; Michelle D. Zhao; Patrick Callaghan; Mingxi Jia; Maxim Likhachev; Oliver Kroemer; George Konidaris",
        "abstract": "Collaborative robots have the ability to adapt and improve their behavior by learning from their human users. By interactively learning on the job, these robots can both acquire new motor skills and customize their behavior to personal user preferences. However, for this paradigm to be viable, there must be a balance between teaching the robot necessary skills, minimizing user burden, and maintaining task progress. We propose COIL, a novel polynomial-time interaction planner that explicitly minimizes human effort while ensuring the completion of a given sequence of tasks according to hidden user preferences. When user preferences are known, we formulate this planning-to-learn problem as an uncapacitated facility location problem. COIL utilizes efficient approximation algorithms for facility location to plan in the case of unknown preferences in polynomial time. In contrast, prior methods do not guarantee minimization of human effort nor consider the inherently collaborative nature of learning on the job, in which timely task execution may require the robot to forego learning and instead request human contributions. Simulated and physical experiments on manipulation tasks show that our framework significantly reduces the amount of work allocated to the human while maintaining successful task completion.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p087.pdf",
        "supp": "",
        "pdf_size": 7246192,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:sposXnB_-sAJ:scholar.google.com/&scioq=Optimal+Interactive+Learning+on+the+Job+via+Facility+Location+Planning&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Brown University; Carnegie Mellon University; Carnegie Mellon University; Brown University; Carnegie Mellon University; Carnegie Mellon University; Brown University",
        "aff_domain": "brown.edu; ; ; ; ; ; ",
        "email": "brown.edu; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://roboticsconference.org/program/papers/87/",
        "aff_unique_index": "0;1;1;0;1;1;0",
        "aff_unique_norm": "Brown University;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.brown.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Brown;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation",
        "session": "Manipulation III",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "153",
        "author": "Wenxuan Li; Hang Zhao; Zhiyuan Yu; Yu Du; Qin Zou; Ruizhen Hu; Kai Xu",
        "abstract": "Non-prehensile manipulation, such as pushing and poking, involves moving objects without grasping, offering cost-effective solutions in constrained environments. However, it presents challenges due to sensitivity to complex physics like friction and restitution. Existing approaches either rely on expensive expert demonstrations, limiting scalability, or use simulation-based trial-and-error, suffering from gaps between simulation and reality. We propose PIN-WM, a Physics-INformed World Model that efficiently identifies physical parameters for rigid bodies from visual observations, serving as an interactive environment for deployable policy learning. PIN-WM leverages differentiable physics and rendering to achieve system identification with minimal task-agnostic interactions. Built on 3D simulation, it comprehensively considers properties including mass, friction, restitution, and moment of inertia, supporting vision-based learning of 6DOF manipulation skills. To bridge gaps between the identified model and the target domain, we introduce Identified Digital Cousins, which perturbs physics and rendering parameters to generate diverse, meaningful variations for enhancing policy transfer. Evaluations across diverse task scenarios demonstrate that our method achieves zero-shot policy transfer with an impressive task success rate, significantly outperforming recent Real2Sim2Real state-of-the-arts.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p153.pdf",
        "supp": "",
        "pdf_size": 22489656,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5506932520977386445&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "National University of Defense Technology; Wuhan University; Wuhan University; National University of Defense Technology; Wuhan University+Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ); Shenzhen University; National University of Defense Technology",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "https://pinwm.github.io",
        "author_num": 7,
        "oa": "https://roboticsconference.org/program/papers/153/",
        "aff_unique_index": "0;1;1;0;1+2;3;0",
        "aff_unique_norm": "National University of Defense Technology;Wuhan University;Guangdong Laboratory of Artificial Intelligence and Digital Economy;Shenzhen University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "http://www.nudt.edu.cn/;http://www.whu.edu.cn/;;https://www.szu.edu.cn",
        "aff_unique_abbr": "NUDT;WHU;GD-LAB;SZU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based Implicit Neural Map",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "40",
        "author": "Yue Pan; Xingguang Zhong; Liren Jin; Louis Wiesmann; Marija Popovic; Jens Behley; Cyrill Stachniss",
        "abstract": "Robots require high-fidelity reconstructions of their environment for effective operation. Such scene representations should be both, geometrically accurate and photorealistic to support downstream tasks. While this can be achieved by building distance fields from range sensors and radiance fields from cameras, the scalable incremental mapping of both fields consistently and at the same time with high quality remains challenging. In this paper, we propose a novel map representation that unifies a continuous signed distance field and a Gaussian splatting radiance field within an elastic and compact point-based implicit neural map. By enforcing geometric consistency between these fields, we achieve mutual improvements by exploiting both modalities. We devise a LiDAR-visual SLAM system called PINGS using the proposed map representation and evaluate it on several challenging large-scale datasets. Experimental results demonstrate that PINGS can incrementally build globally consistent distance and radiance fields encoded with a compact set of neural points. Compared to the state-of-the-art methods, PINGS achieves superior photometric and geometric rendering at novel views by leveraging the constraints from the distance field. Furthermore, by utilizing dense photometric cues and multi-view consistency from the radiance field, PINGS produces more accurate distance fields, leading to improved odometry estimation and mesh reconstruction.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p040.pdf",
        "supp": "",
        "pdf_size": 11560395,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7255794031966383128&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "oa": "https://roboticsconference.org/program/papers/40/"
    },
    {
        "title": "PP-Tac: Paper Picking Using Omnidirectional Tactile Feedback in Dexterous Robotic Hands",
        "session": "Manipulation I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "56",
        "author": "Pei Lin; Yuzhe Huang; Jianpeng Ma; Chenxi Xiao; Wanlin Li; Ziyuan Jiao",
        "abstract": "Robots are increasingly envisioned as human companions, assisting with everyday tasks that often involve manipulating deformable objects. Recent advancements in robotic hardware and embodied AI algorithms have expanded the range of tasks robots can perform. However, current systems still struggle with handling thin, flat objects like paper and fabric due to limitations in motion planning and perception. This paper introduces PP-Tac, a robotic system designed specifically for handling paper-like objects. We developed a multi-fingered robotic hand equipped with high-resolution tactile sensors that provide omnidirectional feedback, enabling slippage detection and precise friction control with the material. Additionally, we created a grasp trajectory synthesis pipeline to generate a dataset of flat-object grasping motions and trained a diffusion policy for real-time control. This policy was then transferred to a real-world hand-arm platform for extensive evaluation. Our experiments, involving both everyday objects (e.g., plastic bags, paper, cloth) and more challenging materials (e.g., kraft paper handbags), achieved a success rate of 87.5%. By leveraging tactile feedback, our system also adapts to varying surfaces beneath the objects. These results demonstrate the robustness of our approach. We believe PP-Tac has significant potential for applications in household and industrial settings, such as organizing documents, packaging, and cleaning, where precise handling of flat objects is essential.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p056.pdf",
        "supp": "",
        "pdf_size": 16835623,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0ZjFH67pvkUJ:scholar.google.com/&scioq=PP-Tac:+Paper+Picking+Using+Omnidirectional+Tactile+Feedback+in+Dexterous+Robotic+Hands&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Beijing Institute for General Artificial Intelligence+ShanghaiTech University; Beijing Institute for General Artificial Intelligence+Beihang University; Beijing Institute for General Artificial Intelligence; Beijing Institute for General Artificial Intelligence; ShanghaiTech University; Beijing Institute for General Artificial Intelligence",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://peilin-666.github.io/projects/PP-Tac",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/56/",
        "aff_unique_index": "0+1;0+2;0;0;1;0",
        "aff_unique_norm": "Beijing Institute for General Artificial Intelligence;ShanghaiTech University;Beihang University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.bigaiai.org/;https://www.shanghaitech.edu.cn;http://www.buaa.edu.cn/",
        "aff_unique_abbr": "BIGAI;ShanghaiTech;BUAA",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation",
        "session": "Manipulation III",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "148",
        "author": "Yifan Yin; Zhengtao Han; Shivam Aarya; Shuhang Xu; Jianxin Wang; Jiawei Peng; Angtian Wang; Alan Yuille; Tianmin Shu",
        "abstract": "Fine-grained robot manipulation, such as lifting and rotating a bottle to display the label on the cap, requires robust reasoning about object parts and their relationships with intended tasks. Despite recent advances in training general-purpose robot manipulation policies guided by language instructions, there is a notable lack of large-scale datasets for fine-grained manipulation tasks with part-level instructions and diverse 3D object instances annotated with part-level labels. In this work, we introduce PartInstruct, the first large-scale benchmark for both training and evaluating fine-grained robot manipulation models using part-level instructions. PartInstruct comprises 513 object instances across 14 categories, each annotated with part-level information, and 1302 fine-grained manipulation tasks organized into 16 task classes. We generated a training set that includes over 10,000 expert demonstrations synthesized in a 3D simulator, each annotated with an overall task instruction, a chain of basic part-based skill instructions, and ground-truth 3D information about the object and its parts. Additionally, we designed a comprehensive test suite to evaluate the generalizability of learned policies across new states, objects, and tasks. We evaluated several state-of-the-art vision-language policy learning methods for robot manipulation on our benchmark. The experimental results reveal that current models struggle to robustly ground part concepts in 3D vision and motion planning, and face challenges when manipulating object parts in long-horizon tasks.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p148.pdf",
        "supp": "",
        "pdf_size": 6737639,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:qe83kUr3JZwJ:scholar.google.com/&scioq=PartInstruct:+Part-level+Instruction+Following+for+Fine-grained+Robot+Manipulation&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Johns Hopkins University; ShanghaiTech University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University",
        "aff_domain": "jhu.edu;shanghaitech.edu.cn;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "email": "jhu.edu;shanghaitech.edu.cn;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "github": "",
        "project": "https://partinstruct.github.io",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/148/",
        "aff_unique_index": "0;1;0;0;0;0;0;0;0",
        "aff_unique_norm": "Johns Hopkins University;ShanghaiTech University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.jhu.edu;https://www.shanghaitech.edu.cn",
        "aff_unique_abbr": "JHU;ShanghaiTech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "36",
        "author": "Kaifeng Zhang; Baoyu Li; Kris Hauser; Yunzhu Li",
        "abstract": "Modeling the dynamics of deformable objects is challenging due to their diverse physical properties and the difficulty of estimating states from limited visual information. We address these challenges with a neural dynamics framework that combines object particles and spatial grids in a hybrid representation. Our particle-grid model captures global shape and motion information while predicting dense particle movements, enabling the modeling of objects with varied shapes and materials. Particles represent object shapes, while the spatial grid discretizes the 3D space to ensure spatial continuity and enhance learning efficiency. Coupled with Gaussian Splattings for visual rendering, our framework achieves a fully learning-based digital twin of deformable objects and generates 3D action-conditioned videos. Through experiments, we demonstrate that our model learns the dynamics of diverse objects\u2014such as ropes, cloths, stuffed animals, and paper bags\u2014from sparse-view RGB-D recordings of robot-object interactions, while also generalizing at the category level to unseen instances. Our approach outperforms state-of-the-art learning-based and physics-based simulators, particularly in scenarios with limited camera views. Furthermore, we showcase the utility of our learned models in model-based planning, enabling goal-conditioned object manipulation across a range of tasks.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p036.pdf",
        "supp": "",
        "pdf_size": 27501180,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/36/"
    },
    {
        "title": "Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization",
        "session": "Manipulation I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "53",
        "author": "Lujie Yang; H.j. Terry Suh; Tong Zhao; Bernhard Paus Graesdal; Tarik Kelestemur; Jiuguang Wang; Tao Pang; Russ Tedrake",
        "abstract": "We present a low-cost data generation pipeline that integrates physics-based simulation, human demonstrations, and model-based planning to efficiently generate large-scale, high-quality datasets for contact-rich robotic manipulation tasks. Starting with a small number of embodiment-flexible human demonstrations collected in a virtual reality simulation environment, the pipeline refines these demonstrations using optimization-based kinematic retargeting and trajectory optimization to adapt them across various robot embodiments and physical parameters. This process yields a diverse, physically consistent dataset that enables cross-embodiment data transfer, and offers the potential to reuse legacy datasets collected under different hardware configurations or physical parameters. We validate the pipeline\u2019s effectiveness by training diffusion policies from the generated datasets for challenging contact-rich manipulation tasks across multiple robot embodiments, including a floating Allegro hand and bimanual robot arms. The trained policies are deployed zero-shot on hardware for bimanual iiwa arms, achieving high success rates with minimal human input.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p053.pdf",
        "supp": "",
        "pdf_size": 16397104,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10954290494131975358&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Massachusetts Institute of Technology+Robotics and AI Institute; Massachusetts Institute of Technology; Robotics and AI Institute; Massachusetts Institute of Technology; Robotics and AI Institute; Robotics and AI Institute; Robotics and AI Institute; Massachusetts Institute of Technology",
        "aff_domain": "mit.edu; ; ; ; ; ; ; ",
        "email": "mit.edu; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://lujieyang.github.io/physicsgen/",
        "author_num": 8,
        "oa": "https://roboticsconference.org/program/papers/53/",
        "aff_unique_index": "0+1;0;1;0;1;1;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Robotics and AI Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;",
        "aff_unique_abbr": "MIT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "title": "Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins",
        "session": "Manipulation III",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "145",
        "author": "Chuanruo Ning; Kuan Fang; Wei-Chiu Ma",
        "abstract": "Recent advancements in open-world robot manipulation have been largely driven by vision-language models (VLMs). While these models exhibit strong generalization ability in high-level planning, they struggle to predict low-level robot controls due to limited physical-world understanding. To address this issue, we propose a model predictive control framework for open-world manipulation that combines the semantic reasoning capabilities of VLMs with physically-grounded, interactive digital twins of the real-world environments. By constructing and simulating the digital twins, our approach generates feasible motion trajectories, simulates corresponding outcomes, and prompts the VLM with future observations to evaluate and select the most suitable outcome based on language instructions of the task. To further enhance the capability of pre-trained VLMs in understanding complex scenes for robotic control, we leverage the flexible rendering capabilities of the digital twin to synthesize the scene at various novel, unoccluded viewpoints. We validate our approach on a diverse set of complex manipulation tasks, demonstrating superior performance compared to baseline methods for language-conditioned robotic control using VLMs.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p145.pdf",
        "supp": "",
        "pdf_size": 76715705,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/145/"
    },
    {
        "title": "Provably-Safe, Online System Identification",
        "session": "Control and Dynamics",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "121",
        "author": "Bohao Zhang; Zichang Zhou; Ram Vasudevan",
        "abstract": "Precise manipulation tasks require accurate knowledge of payload inertial parameters. Unfortunately, identifying these parameters for unknown payloads while ensuring that the robotic system satisfies its input and state constraints while avoiding collisions with the environment remains a significant challenge. This paper presents an integrated framework that enables robotic manipulators to safely and automatically identify payload parameters while maintaining operational safety guarantees.  The framework consists of two synergistic components:  an online, receding-horizon trajectory planning and control framework that generates provably-safe exciting trajectories for system identification that can be tracked while respecting robot constraints and avoiding obstacles and a robust system identification method that computes rigorous overapproximative bounds on end-effector inertial parameters assuming bounded sensor noise.  Experimental validation on a robotic manipulator performing challenging tasks with various unknown payloads demonstrates the framework\u2019s effectiveness in establishing accurate parameter bounds while maintaining safety throughout the identification process.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p121.pdf",
        "supp": "",
        "pdf_size": 19249587,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3JxFRVrHG38J:scholar.google.com/&scioq=Provably-Safe,+Online+System+Identification&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Robotics Institute, University of Michigan, Ann Arbor, MI; Robotics Institute, University of Michigan, Ann Arbor, MI; Robotics Institute, University of Michigan, Ann Arbor, MI",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "https://roahmlab.github.io/OnlineSafeSysID/",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/121/",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "RAMEN: Real-time Asynchronous Multi-agent Neural Implicit Mapping",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "41",
        "author": "Hongrui Zhao; Boris Ivanovic; Negar Mehr",
        "abstract": "Multi-agent neural implicit mapping allows robots to collaboratively capture and reconstruct complex environments with high fidelity. However, existing approaches often rely on synchronous communication, which is impractical in real-world scenarios with limited bandwidth and potential communication interruptions.  This paper introduces RAMEN: Real-time Asynchronous Multi-agEnt Neural implicit mapping, a novel approach designed to address this challenge.  RAMEN employs a weighted multi-agent consensus optimization algorithm that explicitly accounts for communication disruptions.  Using update information (i.e., update frequency), we quantify the uncertainty associated with each parameter of the neural network. Two neural networks are brought to consensus on the basis of their levels of uncertainty, with consensus biased towards network parameters with lower uncertainty.  To achieve this, we derive a weighted variant of the decentralized consensus alternating direction method of multipliers (C-ADMM) algorithm, facilitating robust and efficient collaboration among agents.  Through extensive evaluations on real-world datasets and real-robot hardware experiments, we demonstrate RAMEN\u2019s superior mapping performance under challenging communication conditions.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p041.pdf",
        "supp": "",
        "pdf_size": 2644542,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:hgMs3kWl1PUJ:scholar.google.com/&scioq=RAMEN:+Real-time+Asynchronous+Multi-agent+Neural+Implicit+Mapping&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "ICON Lab, Department of Aerospace Engineering, University of Illinois Urbana-Champaign; NVIDIA Research; ICON Lab, Department of Mechanical Engineering, University of California Berkeley",
        "aff_domain": "illinois.edu;nvidia.com;berkeley.edu",
        "email": "illinois.edu;nvidia.com;berkeley.edu",
        "github": "",
        "project": "https://iconlab.negarmehr.com/RAMEN/",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/41/",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;NVIDIA;University of California, Berkeley",
        "aff_unique_dep": "Department of Aerospace Engineering;NVIDIA Research;Department of Mechanical Engineering",
        "aff_unique_url": "https://illinois.edu;https://www.nvidia.com/research;https://www.berkeley.edu",
        "aff_unique_abbr": "UIUC;NVIDIA;UC Berkeley",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Urbana-Champaign;;Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for Vision-Based Drone Navigation",
        "session": "Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "142",
        "author": "Minwoo Kim; Geunsik Bae; Jinwoo Lee; Woojae Shin; Changseung Kim; Myongyol Choi; Heejung Shin; Hyongdong Oh",
        "abstract": "This paper introduces a learning-based visual planner for agile drone flight in cluttered environments. The proposed planner generates collision-free waypoints in milliseconds, enabling drones to perform agile maneuvers in complex environments without building separate perception, mapping, and planning modules. Learning-based methods, such as behavior cloning (BC) and reinforcement learning (RL), demonstrate promising performance in visual navigation but still face inherent limitations. BC is susceptible to compounding errors due to limited expert imitation, while RL struggles with reward function design and sample inefficiency. To address these limitations, this paper proposes an inverse reinforcement learning (IRL)-based framework for high-speed visual navigation. By leveraging IRL, it is possible to reduce the number of interactions with simulation environments and improve scalability in high-dimensional spaces while maintaining the robustness characteristic of RL policies. An expert dataset is collected using a motion primitive-based path planning algorithm in various environments such as narrow gaps, cubes, spheres, and trees, ensuring comprehensive coverage of diverse scenarios. While the proposed method is trained in a simulation environment only, it can be directly applied to real-world scenarios without additional training or tuning. The performance of the proposed method is validated in both simulation and real-world environments, including forests and various structures. The trained policy achieves an average speed of 7 m/s and a maximum speed of 8.8 m/s in real flight experiments. To the best of our knowledge, this is the first work to successfully apply an IRL framework for high-speed visual navigation of drones.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p142.pdf",
        "supp": "",
        "pdf_size": 85026535,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3886454527393784521&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea; Department of Mechanical Engineering, Ulsan National Institute of Science and Technology, Republic of Korea",
        "aff_domain": "unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr",
        "email": "unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr;unist.ac.kr",
        "github": "",
        "project": "https://youtu.be/ZfV6ij0qZMI",
        "author_num": 8,
        "oa": "https://roboticsconference.org/program/papers/142/",
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Ulsan National Institute of Science and Technology",
        "aff_unique_dep": "Department of Mechanical Engineering",
        "aff_unique_url": "https://www.unist.ac.kr",
        "aff_unique_abbr": "UNIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning",
        "session": "Scaling Robot Learning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "28",
        "author": "Charles Xu; Qiyang Li; Jianlan Luo; Sergey Levine",
        "abstract": "Recent advances in robotic foundation models have enabled the development of generalist policies that can adapt to diverse tasks. While these models show impressive flexibility, their performance heavily depends on the quality of their training data. In this work, we propose Reinforcement Learning Distilled Generalists (RLDG), a method that leverages reinforcement learning to generate high-quality training data for fine-tuning generalist policies. Through extensive real-world experiments on precise manipulation tasks like connector insertion and assembly, we demonstrate that generalist policies trained with RL-generated data consistently outperform those trained with human demonstrations, achieving up to 40% higher success rates while generalizing better to new tasks.  We also provide a detailed analysis that reveals this performance gain stems from both optimized action distributions and improved state coverage. Our results suggest that combining task-specific RL with generalist policy distillation offers a promising approach for developing more capable and efficient robotic manipulation systems that maintain the flexibility of foundation models while achieving the performance of specialized controllers.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p028.pdf",
        "supp": "",
        "pdf_size": 5097066,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9063647111283551195&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of California, Berkeley\u2020; University of California, Berkeley\u2020; University of California, Berkeley\u2020; University of California, Berkeley\u2020",
        "aff_domain": "berkeley.edu;berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "github": "",
        "project": "https://generalist-distillation.github.io/",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/28/",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ROMAN: Open-Set Object Map Alignment for Robust View-Invariant Global Localization",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "29",
        "author": "Mason Peterson; Yixuan Jia; Yulun Tian; Annika Thomas; Jonathan P. How",
        "abstract": "Global localization is a fundamental capability required for long-term and drift-free robot navigation. However, current methods fail to relocalize when faced with significantly different viewpoints. We present ROMAN (Robust Object Map Alignment Anywhere), a robust global localization method capable of localizing in challenging and diverse environments based on creating and aligning maps of open-set and view-invariant objects. To address localization difficulties caused by feature-sparse or perceptually aliased environments, ROMAN formulates and solves a registration problem between object submaps using a unified graph-theoretic global data association approach that simultaneously accounts for object shape and semantic similarities and a prior on gravity direction. Through a set of challenging large-scale multi-robot or multi-session SLAM experiments in indoor, urban, and unstructured/forested environments, we demonstrate that ROMAN achieves higher accuracy in terms of object map alignment than other registration methods and lower trajectory estimation errors than SLAM systems that use visual features for loop closures. Our code will be made open-source.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p029.pdf",
        "supp": "",
        "pdf_size": 1705475,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5889297014744756653&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology; University of California San Diego; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;ucsd.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;ucsd.edu;mit.edu;mit.edu",
        "github": "",
        "project": "https://acl.mit.edu/roman",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/29/",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of California, San Diego",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://ucsd.edu",
        "aff_unique_abbr": "MIT;UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "RUKA: Rethinking the Design of Humanoid Hands with Learning",
        "session": "Robot Design",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "131",
        "author": "Anya Zorin; Irmak Guzey; Nikhil X. Bhattasali; Lerrel Pinto",
        "abstract": "Dexterous manipulation is a fundamental capability for robotic systems to interact with the physical world, yet progress in this area has been limited by hardware. An ideal robotic hand must balance precision, compactness, strength, and affordability\u2014requirements that remain challenging to achieve simultaneously. Existing hand designs impose trade-offs based on available control methods and target applications. However, learning-based approaches present an opportunity to rethink some of these trade-offs, particularly to address challenges associated with tendon-driven actuation and low-cost materials. In this work, we present RUKA, a tendon-driven humanoid hand that is simple, affordable, and capable. Made from 3D-printed parts and off-the-shelf components, RUKA has 5 fingers (including an opposable thumb) and underactuated control of 18 degrees of freedom, enabling diverse human-like grasps. Its tendon-driven actuation allows powerful grasping in a compact, human-sized form. To tackle tendon-driven control challenges, we learn joint-to-actuator and fingertip-to-actuator models using motion-capture data, leveraging the hand\u2019s morphological accuracy. We extensively evaluate RUKA against commonly used robotic hands and demonstrate its superior reachability, durability, and strength. We further apply RUKA in bimanual teleoperation tasks and showcase that RUKA can be used to perform various dexterous tasks. By addressing important trade-offs in robotic hand design, we believe RUKA opens new possibilities for advancing manipulation research and expanding its accessibility. All code, data, design models, and assembly instructions are open-source and available at our website.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p131.pdf",
        "supp": "",
        "pdf_size": 27266070,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2790954317193594211&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "ruka-hand.github.io",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/131/"
    },
    {
        "title": "Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation",
        "session": "Manipulation I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "52",
        "author": "Han Xue; Jieji Ren; Wendi Chen; Gu Zhang; Fang Yuan; Guoying Gu; Huazhe Xu; Cewu Lu",
        "abstract": "Humans can accomplish complex contact-rich tasks using vision and touch, with highly reactive capabilities such as quick adjustments to environmental changes and adaptive control of contact forces; however, this remains challenging for robots. Existing visual imitation learning (IL) approaches rely on action chunking to model complex behaviors, which lacks the ability to respond instantly to real-time tactile feedback during the chunk execution. Furthermore, most teleoperation systems struggle to provide fine-grained tactile/force feedback, which limits the range of tasks that can be performed. To address these challenges, we introduce TactAR, a low-cost teleoperation system that provides real-time tactile feedback through Augmented Reality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast visual-tactile imitation learning algorithm for learning contact-rich manipulation skills. RDP employs a two-level hierarchy: (1) a slow latent diffusion policy for predicting high-level action chunks in latent space at low frequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback control at high frequency. This design enables both complex trajectory modeling and quick reactive behavior within a unified framework. Through extensive evaluation across three challenging contact-rich tasks, our approach demonstrates superior performance compared to state-of-the-art visual IL baselines while maintaining fast reactivity to tactile feedback. Furthermore, experiments show that RDP is applicable across different tactile / force sensors. More videos and results can be found in the supplementary files.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p052.pdf",
        "supp": "",
        "pdf_size": 6215634,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10035935921120269413&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Tsinghua University, IIIS + Shanghai Qi Zhi Institute + Shanghai AI Laboratory; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Tsinghua University, IIIS + Shanghai Qi Zhi Institute + Shanghai AI Laboratory; Shanghai Jiao Tong University + Shanghai Innovation Institute",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;tsinghua.edu.cn;sjtu.edu.cn;sjtu.edu.cn;tsinghua.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;tsinghua.edu.cn;sjtu.edu.cn;sjtu.edu.cn;tsinghua.edu.cn;sjtu.edu.cn",
        "github": "reactive-diffusion-policy.github.io",
        "project": "",
        "author_num": 8,
        "oa": "https://roboticsconference.org/program/papers/52/",
        "aff_unique_index": "0;0;0;1+2+3;0;0;1+2+3;0+4",
        "aff_unique_norm": "Shanghai Jiao Tong University;Tsinghua University;Shanghai Qi Zhi Institute;Shanghai AI Laboratory;Shanghai Innovation Institute",
        "aff_unique_dep": ";Institute for Interdisciplinary Information Sciences (IIIS);;;",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.tsinghua.edu.cn;https://www.qz.io;https://www.shanghai-ai-lab.com;",
        "aff_unique_abbr": "SJTU;THU;;SAIL;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0+0;0;0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety",
        "session": "Multi-Robot Systems",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "94",
        "author": "Jasmine Jerry Aloor; Jason Jangho Choi; Jingqi Li; Maria G. Mendoza; Hamsa Balakrishnan; Claire Tomlin",
        "abstract": "Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p094.pdf",
        "supp": "",
        "pdf_size": 6912227,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ioF-TWUotoEJ:scholar.google.com/&scioq=Resolving+Conflicting+Constraints+in+Multi-Agent+Reinforcement+Learning+with+Layered+Safety&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "University of California, Berkeley; Massachusetts Institute of Technology; University of California, Berkeley; University of California, Berkeley; Massachusetts Institute of Technology; University of California, Berkeley",
        "aff_domain": "berkeley.edu;mit.edu;berkeley.edu;berkeley.edu;mit.edu;berkeley.edu",
        "email": "berkeley.edu;mit.edu;berkeley.edu;berkeley.edu;mit.edu;berkeley.edu",
        "github": "",
        "project": "https://dinamo-mit.github.io/Layered-Safe-MARL/",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/94/",
        "aff_unique_index": "0;1;0;0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://web.mit.edu",
        "aff_unique_abbr": "UC Berkeley;MIT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Riemannian Direct Trajectory Optimization of Rigid Bodies on Matrix Lie Groups",
        "session": "Control and Dynamics",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "120",
        "author": "Sangli Teng; Tzu-Yuan Lin; William A. Clark; Ram Vasudevan; Maani Ghaffari",
        "abstract": "Designing dynamically feasible trajectories for rigid bodies is a fundamental problem in robotics.  Although direct trajectory optimization is widely applied to solve this problem, state-of-the-art methods overlook the manifold structures of rigid bodies, resulting in slow convergence. This paper introduces a Riemannian optimization framework for direct trajectory optimization of rigid bodies on matrix Lie groups.  The proposed approach first leverages the Lie Group Variational Integrator to formulate the discrete rigid body dynamics.  The paper then derives the exact first- and second-order Riemannian derivatives of the dynamics using the matrix Lie group structure.  Finally, this work applies a line-search Riemannian Interior Point Method (RIPM) to perform trajectory optimization. The paper demonstrates that both the derivative evaluations and Newton steps required to solve the RIPM exhibit linear complexity with respect to the planning horizon and system degrees of freedom. Simulation results illustrate that the proposed method is an order of magnitude faster than conventional methods.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p120.pdf",
        "supp": "",
        "pdf_size": 2320984,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=546521224958593979&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Michigan; University of Michigan; Ohio University; University of Michigan; University of Michigan",
        "aff_domain": "umich.edu; ; ; ; ",
        "email": "umich.edu; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/120/",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Michigan;Ohio University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umich.edu;https://www.ohio.edu",
        "aff_unique_abbr": "UM;OU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation",
        "session": "Manipulation III",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "152",
        "author_site": "Kun Wu; Chengkai Hou; Jiaming Liu; Zhengping Che; Xiaozhu Ju; Zhuqin Yang; Meng Li; Yinuo Zhao; Zhiyuan Xu; Guang Yang; Shichao Fan; Xinhua Wang; Fei Liao; Zhen Zhao; Guangyu Li; Zhao Jin; Lecheng Wang; Jilei Mao; Ning Liu; Pei Ren;",
        "author": "Kun Wu; Chengkai Hou; Jiaming Liu; Zhengping Che; Xiaozhu Ju; Zhuqin Yang; Meng Li; Yinuo Zhao; Zhiyuan Xu; Guang Yang; Shichao Fan; Xinhua Wang; Fei Liao; Zhen Zhao; Guangyu Li; Zhao Jin; Lecheng Wang; Jilei Mao; Ning Liu; Pei Ren; Qiang Zhang; Yaoxu Lyu; Mengzhen Liu; He Jingyang; Yulin Luo; Zeyu Gao; Chenxuan Li; Chenyang Gu; Yankai Fu; Di Wu; Xingyu Wang; Sixiang Chen; Zhenyu Wang; Pengju An; Siyuan Qian; Shanghang Zhang; Jian Tang",
        "abstract": "Developing robust and general-purpose manipulation policies is a key goal in robotics. To achieve effective generalization, it is essential to construct comprehensive datasets that encompass a large number of demonstration trajectories and diverse tasks. Unlike vision or language data, which can be sourced from the internet, robotic datasets require detailed observations and manipulation actions, necessitating significant investments in both hardware-software infrastructure and human labor. While existing works have focused on assembling various individual robot datasets, there is still a lack of a unified data collection standard and insufficient high-quality data across diverse tasks, scenarios, and robot types. In this paper, we introduce RoboMIND (Multi-embodiment Intelligence Normative Data for Robot Manipulation), a dataset containing 107k demonstration trajectories across 479 diverse tasks involving 96 object classes. RoboMIND is collected through human teleoperation and encompasses comprehensive robotic-related information, including multi-view observations, proprioceptive robot state information, and linguistic task descriptions. To ensure dataset consistency and reliability for imitation learning, RoboMIND is built on a unified data collection platform and standardized protocol, covering four distinct robotic embodiments: the Franka Emika Panda, the UR-5e, the AgileX dual-arm robot, and a humanoid robot with dual dexterous hands. Our dataset also includes 5k real-world failure demonstrations, each accompanied by detailed causes, enabling failure reflection and correction during policy learning. Additionally, we create a digital twin environment in the Isaac Sim simulator, replicating the real-world tasks and assets, which facilitates the low-cost collection of additional training data and enables efficient evaluation.  To demonstrate the quality and diversity of our dataset, we conducted extensive experiments using various Imitation Learning methods for single task setting and state-of-the-art Vision-Language-Action (VLA) models for multi-task scenarios. By leveraging  RoboMIND, the VLA models achieved high manipulation success rates and demonstrated strong generalization capabilities.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p152.pdf",
        "supp": "",
        "pdf_size": 3505366,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7205142392623051116&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 37,
        "oa": "https://roboticsconference.org/program/papers/152/"
    },
    {
        "title": "RoboPanoptes: The All-Seeing Robot with Whole-body Dexterity",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "42",
        "author_site": "Xiaomeng Xu; Dominik Bauer; Shuran Song",
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4102023404032044102&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "author": "",
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://roboticsconference.org/program/papers/42/"
    },
    {
        "title": "RoboVerse: A Unified Platform, Benchmark and Dataset for Scalable and Generalizable Robot Learning",
        "session": "Scaling Robot Learning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "22",
        "author_site": "Haoran Geng; Feishi Wang; Songlin Wei; Yuyang Li; Bangjun Wang; Boshi An; Haozhe Lou; Charlie Tianyue Cheng; Peihao Li; Haozhe Chen; Yutong Liang; Yuxi Qian; Jiageng Mao; Weikang Wan; Yiran Geng; Mingtong Zhang; Jiangran Lyu; Siheng Zhao; Jiazhao Zhang; Chaoyi",
        "author": "Haoran Geng; Feishi Wang; Songlin Wei; Yuyang Li; Bangjun Wang; Boshi An; Haozhe Lou; Charlie Tianyue Cheng; Peihao Li; Haozhe Chen; Yutong Liang; Yuxi Qian; Jiageng Mao; Weikang Wan; Yiran Geng; Mingtong Zhang; Jiangran Lyu; Siheng Zhao; Jiazhao Zhang; Chaoyi Xu; Jialiang Zhang; Chengyang Zhao; Haoran Lu; Yufei Ding; Ran Gong; Yuran Wang; Yuxuan Kuang; Ruihai Wu; Baoxiong Jia; Hao Dong; Siyuan Huang; Yue Wang; Jitendra Malik; Pieter Abbeel",
        "abstract": "Data scaling and standardized evaluation benchmarks have driven remarkable advances in natural language processing and computer vision. However, in robotics, scaling up data and establishing evaluation protocols pose significant challenges. Directly collecting real-world data is inefficient and resource-intensive, while benchmarking in real-world scenarios also remains highly challenging. Synthetic data and simulation environments present a promising alternative, yet existing efforts often fail to fully leverage the potential of simulation, resulting in limited data quality, diversity, and fragmented benchmarks. To address these challenges, we introduce RoboVerse, a simulation platform built on a unified data format. RoboVerse supports multiple simulators and robots, enabling seamless switching between different simulators and embodiments. In addition, by leveraging our unified data format, it enables multiple workflows to efficiently collect tasks and trajectories from various sources with high fidelity and diversity. Using RoboVerse, we generate the largest high-quality synthetic dataset to date in a unified format, along with a standardized benchmark system that reliably evaluates policies and supports assessment across different levels of generalization. We employ the RoboVerse workflows and conduct extensive experiments, demonstrating that our platform and dataset significantly enhance the performance of imitation learning, reinforcement learning, and world model learning, facilitating the transfer from simulation to real-world applications. These results demonstrate the reliability of our dataset and benchmark, highlighting RoboVerse as an effective solution for advancing simulation-assisted robot learning.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p022.pdf",
        "supp": "",
        "pdf_size": 29042227,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 34,
        "oa": "https://roboticsconference.org/program/papers/22/"
    },
    {
        "title": "Robot Data Curation with Mutual Information Estimators",
        "session": "Scaling Robot Learning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "23",
        "author": "Joey Hejna; Suvir Mirchandani; Ashwin Balakrishna; Annie Xie; Ayzaan Wahid; Jonathan Tompson; Pannag R. Sanketi; Dhruv Shah; Coline Manon Devin; Dorsa Sadigh",
        "abstract": "The performance of imitation learning policies often hinges on the datasets with which they are trained. Consequently, investment in data collection for robotics has grown across both industrial and academic labs. However, despite the marked increase in the quantity of demonstrations collected, little work has sought to assess the quality of said data despite mounting evidence of its importance in other areas such as vision and language. In this work, we take a critical step towards addressing the data quality in robotics. Given a dataset of demonstrations, we aim to estimate the relative quality of individual demonstrations in terms of both state diversity and action predictability. To do so, we estimate the average contribution of a trajectory towards the mutual information between states and actions in the entire dataset, which precisely captures both the entropy of the state distribution and the state-conditioned entropy of actions. Though commonly used mutual information estimators require vast amounts of data often beyond the scale available in robotics, we introduce a novel technique based on k-nearest neighbor estimates of mutual information on top of simple VAE embeddings of states and actions. Empirically, we demonstrate that our approach is able to partition demonstration datasets by quality according to human expert scores across a diverse set of benchmarks spanning simulation and real world environments.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p023.pdf",
        "supp": "",
        "pdf_size": 3913194,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12044668464738560371&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Google DeepMind Robotics+Stanford University; Stanford University; Google DeepMind Robotics; Google DeepMind Robotics; Google DeepMind Robotics; Google DeepMind Robotics; Google DeepMind Robotics; Google DeepMind Robotics; Google DeepMind Robotics+Stanford University; Google DeepMind Robotics+Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;google.com;google.com;google.com;google.com;google.com;google.com;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;google.com;google.com;google.com;google.com;google.com;google.com;stanford.edu;stanford.edu",
        "github": "https://github.com/jhejna/demonstration-info",
        "project": "https://jhejna.github.io/demonstration-info",
        "author_num": 10,
        "oa": "https://roboticsconference.org/program/papers/23/",
        "aff_unique_index": "0+1;1;0;0;0;0;0;0;0+1;0+1",
        "aff_unique_norm": "Google;Stanford University",
        "aff_unique_dep": "DeepMind Robotics;",
        "aff_unique_url": "https://deepmind.com;https://www.stanford.edu",
        "aff_unique_abbr": "DeepMind;Stanford",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0+1;1;0;0;0;0;0;0;0+1;0+1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Robot Learning with Super-Linear Scaling",
        "session": "Scaling Robot Learning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "25",
        "author": "Marcel Torne Villasevil; Arhan Jain; Jiayi Yuan; Vidyaaranya Macha; Lars Lien Ankile; Anthony Simeonov; Pulkit Agrawal; Abhishek Gupta",
        "abstract": "Scaling robot learning requires data collection pipelines that scale favorably with human effort. In this work, we propose",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p025.pdf",
        "supp": "",
        "pdf_size": 16036857,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2579753702357049321&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8,
        "oa": "https://roboticsconference.org/program/papers/25/"
    },
    {
        "title": "Robust Peg-in-Hole Assembly under Uncertainties via Compliant and Interactive Contact-Rich Manipulation",
        "session": "Manipulation I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "60",
        "author": "Yiting Chen; Kenneth Kimble; Howard H. Qian; Podshara Chanrungmaneekul; Robert Seney; Kaiyu Hang",
        "abstract": "Robust and adaptive robotic peg-in-hole assembly under tight tolerance is critical to various industrial applications. Still, it remains an open challenge due to perception and physical uncertainties from contact-rich interactions that easily exceed the allowed clearance. In this paper, we study how to leverage the contact between the peg and its matching hole to eliminate uncertainties in the assembly process under unstructured settings. By exploring the role of compliance under contact constraints, we present a manipulation system that plans collision-inclusive interactions for the peg to 1) iteratively identify its task environment to localize the target hole and 2) exploit environmental contact constraints to refine insertion motions into the target hole without precise perception, facilitating a robust solution to peg-in-hole assembly. By conceptualizing the above process as the composition of funneling in different state spaces, we present a formal approach to construct manipulation funnels as an uncertainty-absorbing paradigm for peg-in-hole assembly. The proposed system effectively generalizes diverse peg-in-hole scenarios across varying scales, shapes, and materials in a learning-free manner. Extensive real-world experiments on a NIST Assembly Task Board validate its robustness in real-world applications.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p060.pdf",
        "supp": "",
        "pdf_size": 2770659,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Q43l36QolZIJ:scholar.google.com/&scioq=Robust+Peg-in-Hole+Assembly+under+Uncertainties+via+Compliant+and+Interactive+Contact-Rich+Manipulation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/60/"
    },
    {
        "title": "SATA: Safe and Adaptive Torque-Based Locomotion Policies Inspired by Animal Learning",
        "session": "Mobile Manipulation and Locomotion",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "124",
        "author": "Li Peizhuo; Hongyi Li; Ge Sun; Jin Cheng; Xinrong Yang; Guillaume Bellegarda; Milad Shafiee Ashtiani; Yuhong Cao; Auke Ijspeert; Guillaume Adrien Sartoretti",
        "abstract": "Despite recent advances in learning-based controllers for legged robots, deployments in human-centric environments remain limited by safety concerns. Most of these approaches use position-based control, where policies output target joint angles that must be processed by a low-level controller (e.g., PD or impedance controllers) to compute joint torques. Although impressive results have been achieved in controlled real-world scenarios, these methods often struggle with compliance and adaptability when encountering environments or disturbances unseen during training, potentially resulting in extreme or unsafe behaviors. Inspired by how animals achieve smooth and adaptive movements by controlling muscle extension and contraction, torque-based policies offer a promising alternative by enabling precise and direct control of the actuators in torque space. This approach facilitates more effective interactions with the environment, resulting in safer and more adaptable behaviors. However, challenges such as a highly nonlinear state space and inefficient exploration during training have hindered their broader adoption. To address these limitations, we propose SATA, a bio-inspired framework that mimics key biomechanical principles and adaptive learning mechanisms observed in animal locomotion. Our approach effectively addresses the inherent challenges of learning torque-based policies by significantly improving early-stage exploration, leading to high-performance final policies. Remarkably, our method achieves zero-shot sim-to-real transfer, eliminating the need for additional fine-tuning on hardware. Our experimental results indicate that SATA demonstrates remarkable compliance and safety, even in challenging environments such as soft/slippery terrain or narrow passages, and under significant external disturbances (e.g., pushing/pulling/pressing on the robot, or manually moving individual legs). These results highlight its potential for practical deployments in human-centric and safety-critical scenarios.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p124.pdf",
        "supp": "",
        "pdf_size": 13915402,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4743375551914825644&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "MARMot Lab, National University of Singapore; MARMot Lab, National University of Singapore; MARMot Lab, National University of Singapore; Computational Robotics Lab, ETH Zurich; MARMot Lab, National University of Singapore; BioRob Lab, EPFL; BioRob Lab, EPFL; MARMot Lab, National University of Singapore; BioRob Lab, EPFL; MARMot Lab, National University of Singapore",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "https://github.com/marmotlab/SATA",
        "project": "",
        "author_num": 10,
        "oa": "https://roboticsconference.org/program/papers/124/",
        "aff_unique_index": "0;0;0;1;0;2;2;0;2;0",
        "aff_unique_norm": "National University of Singapore;ETH Zurich;EPFL",
        "aff_unique_dep": "MARMot Lab;Computational Robotics Lab;BioRob Lab",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.ethz.ch;https://www.epfl.ch",
        "aff_unique_abbr": "NUS;ETHZ;EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;1;1;0;1;0",
        "aff_country_unique": "Singapore;Switzerland"
    },
    {
        "title": "SKIL: Semantic Keypoint Imitation Learning for Generalizable Data-efficient Manipulation",
        "session": "Imitation Learning II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "161",
        "author": "Shengjie Wang; Jiacheng You; Yihang Hu; Jiongye Li; Yang Gao",
        "abstract": "Real-world tasks such as garment manipulation and table rearrangement demand robots to perform generalizable, highly precise, and long-horizon actions. Although imitation learning has proven to be an effective approach for teaching robots new skills, large amounts of expert demonstration data are still indispensible for these complex tasks, resulting in high sample complexity and costly data collection.  To address this, we propose Semantic Keypoint Imitation Learning (SKIL), a framework which automatically obtain semantic keypoints with help of vision foundation models, and forms the descriptor of semantic keypoints that enables effecient imitation learning of complex robotic tasks with significantly lower sample complexity. In real world experiments, SKIL doubles the performance of baseline methods in tasks such as picking a cup or mouse, while demonstrating exceptional robustness to variations in objects, environmental changes, and distractors. For long-horizon tasks like hanging a towel on a rack where previous methods fail completely, SKIL achieves a mean success rate of 70% with as few as 30 demonstrations. Furthermore, SKIL naturally supports cross-embodiment learning due to its semantic keypoints abstraction, our experiments demonstrate that even human videos bring considerable improvement to the learning performance. All these results demonstrate the great success of SKIL in achieving data-efficint generalizable robotic learning.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p161.pdf",
        "supp": "",
        "pdf_size": 6962186,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2979095170670469680&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "IIIS, Tsinghua University + Shanghai AI Laboratory + Shanghai Qi Zhi Institute; IIIS, Tsinghua University + Shanghai AI Laboratory + Shanghai Qi Zhi Institute; IIIS, Tsinghua University; Department of Automation, Tsinghua University; IIIS, Tsinghua University + Shanghai AI Laboratory + Shanghai Qi Zhi Institute",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "https://skil-robotics.github.io/SKIL-robotics/",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/161/",
        "aff_unique_index": "0+1+2;0+1+2;0;0;0+1+2",
        "aff_unique_norm": "Tsinghua University;Shanghai AI Laboratory;Shanghai Qi Zhi Institute",
        "aff_unique_dep": "Institute for Interdisciplinary Information Sciences;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.shanghai-ai-lab.com;https://www.qz.io",
        "aff_unique_abbr": "THU;SAIL;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "STDArm: Transfer Visuomotor Policy From Static Data Training to Dynamic Robot Manipulation",
        "session": "Manipulation III",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "147",
        "author": "Yifan Duan; Heng Li; Yilong Wu; Wenhao Yu; Xinran Zhang; Yedong Shen; Jianmin Ji; Yanyong Zhang",
        "abstract": "Learning visuomotor policy from human demonstrations serves as an effective method for robots to acquire complex tasks. However, data collection on mobile platforms such as drones is extremely challenging, resulting in most research being conducted with robots in stationary conditions for data collection and policy training. Nonetheless, these policies struggle to maintain operational stability when the robot is in motion, primarily due to the low frequency of decision-making, inconsistencies in the input data distribution and differences in the motion characteristics of different robots. To address these issues, we propose a system named STDArm, which effectively transfers stationary-trained action strategies to dynamic states, including the robot\u2019s ego-motion or wobble caused by structural factors. Specifically, we first design an action manager to efficiently store and manage the sequence of actions asynchronously output by the policy network, and to stably output actions at a higher control frequency as needed. Next, we predict the future state of the robotic arm\u2019s base by observing its historical states, thereby developing an end-effector stabilizer to counteract the robot\u2019s movements. Additionally, we incorporate a warm-up step during the robot\u2019s initialization phase to estimate the parameters used in the system online, ensuring tight integration between the stabilizer and policy modules. We conduct comprehensive evaluations of the proposed system on two types of robotic arms, two types of mobile platforms, and three tasks. Experimental results indicate that the STDArm system significantly improves task execution success rates during the robot\u2019s movement.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p147.pdf",
        "supp": "",
        "pdf_size": 10933781,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:5mmIIVZwe2cJ:scholar.google.com/&scioq=STDArm:+Transfer+Visuomotor+Policy+From+Static+Data+Training+to+Dynamic+Robot+Manipulation&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Institute of Advanced Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Artificial Intelligence and Data Science, University of Science and Technology of China, Hefei, China",
        "aff_domain": "; ; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://yjsx.top/stdarm",
        "author_num": 8,
        "oa": "https://roboticsconference.org/program/papers/147/",
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Hefei",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions",
        "session": "Control and Dynamics",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "112",
        "author": "Ji Yin; Oswin So; Eric Yang Yu; Chuchu Fan; Panagiotis Tsiotras",
        "abstract": "A common problem when using model predictive control (MPC) in practice is the satisfaction of safety beyond the prediction horizon. While theoretical works have shown that safety can be guaranteed by enforcing a suitable terminal set constraint or a sufficiently long prediction horizon, these techniques are difficult to apply and thus rarely used by practitioners, especially in the case of general nonlinear dynamics. To solve this, we make a tradeoff between exact recursive feasibility, computational tractability, and applicability to black-box dynamics by learning an approximate discrete-time control barrier function and incorporating it into variational inference MPC (VIMPC), a sampling-based MPC paradigm. To handle the resulting state constraints, we further propose a new sampling strategy that greatly reduces the variance of the estimated optimal control, improving the sample efficiency and enabling real-time planning on CPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantial safety improvements compared to existing sampling-based MPC controllers, even under badly designed cost functions. We validate our approach in both simulation and real-world hardware experiments.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p112.pdf",
        "supp": "",
        "pdf_size": 9000777,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_3cWcnGEfiIJ:scholar.google.com/&scioq=Safe+Beyond+the+Horizon:+Efficient+Sampling-based+MPC+with+Neural+Control+Barrier+Functions&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "D. Guggenheim School of Aerospace Engineering, Georgia Institute of Technology; Department of Aeronautics and Astronautics, Massachusetts Institute of Technology; Department of Aeronautics and Astronautics, Massachusetts Institute of Technology; Department of Aeronautics and Astronautics, Massachusetts Institute of Technology; D. Guggenheim School of Aerospace Engineering, Georgia Institute of Technology",
        "aff_domain": "gatech.edu;mit.edu;mit.edu;mit.edu;gatech.edu",
        "email": "gatech.edu;mit.edu;mit.edu;mit.edu;gatech.edu",
        "github": "",
        "project": "https://mit-realm.github.io/ns-vimpc/",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/112/",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Massachusetts Institute of Technology",
        "aff_unique_dep": "School of Aerospace Engineering;Department of Aeronautics and Astronautics",
        "aff_unique_url": "https://www.gatech.edu;https://web.mit.edu",
        "aff_unique_abbr": "Georgia Tech;MIT",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation",
        "session": "Mobile Manipulation and Locomotion",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "128",
        "author": "Arpit Bahety; Arnav Balaji; Ben Abbatematteo; Roberto Mart\u00edn-Mart\u00edn",
        "abstract": "For robots to become efficient helpers in the home, they must learn to perform new mobile manipulation tasks simply by watching humans perform them. Learning from a single video demonstration from a human is challenging as the robot needs to first extract from the demo what needs to be done and how, translate the strategy from a third to a first-person perspective, and then adapt it to be successful with its own morphology. Furthermore, to mitigate the dependency on costly human monitoring, this learning process should be performed in a safe and autonomous manner. We present SafeMimic, a framework to learn new mobile manipulation skills safely and autonomously from a single third-person human video. Given an initial human video demonstration of a multi-step mobile manipulation task, SafeMimic first parses the video into segments, inferring both the semantic changes caused and the motions the human executed to achieve them and translating them to an egocentric reference. Then, it adapts the behavior to the robot\u2019s own morphology by sampling candidate actions around the human ones, and verifying them for safety before execution in a receding horizon fashion using an ensemble of safety value functions trained in simulation. When safe forward progression is not possible, SafeMimic backtracks to previous states and attempts a different sequence of actions, adapting both the trajectory and the grasping modes when required for its morphology. As a result, SafeMimic yields a strategy that succeeds in the demonstrated behavior and learns task-specific grasps that reduce exploration in future attempts. Our experiments show that our method allows robots to safely and efficiently learn multi-step mobile manipulation behaviors from a single human demonstration, from different users, and in different environments, with improvements over state-of-the-art baselines across seven tasks.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p128.pdf",
        "supp": "",
        "pdf_size": 30498215,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:bf5IVMqVC84J:scholar.google.com/&scioq=SafeMimic:+Towards+Safe+and+Autonomous+Human-to-Robot+Imitation+for+Mobile+Manipulation&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/128/"
    },
    {
        "title": "Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted Motorsports",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "93",
        "author": "Donggeon David Oh; Justin Lidard; Haimin Hu; Himani Sinhmar; Elle Lazarski; Deepak Edakkattil Gopinath; Emily Sumner; Jonathan Decastro; Guy Rosman; Naomi Leonard; Jaime Fern\u00e1ndez Fisac",
        "abstract": "Recent advances in safe autonomy open new opportunities in assisting humans in safety-critical and time-sensitive tasks such as motorsports. However, existing safe control algorithms predominantly focus on fully automated settings and often undermine key requirements in human\u2013AI shared control domains\u2014maintaining human agency and respecting their decisions. Preserving human agency is crucial in human\u2013AI collaboration to reduce the risks of \u201cautomation surprise\u201d, where human operators may feel confused, uncomfortable, or even irritated by the AI\u2019s assistance. In this work, we propose Human-Centered Safety Filter (HCSF), a principled shared autonomy framework that improves robustness in human decision-making while minimally affecting human agency. HCSF is based on a novel model-free safety formulation that allows the AI \u201cassistant\u201d to step in gradually, rather than at the last possible moment, to ensure that AI interventions feel smooth and are only as large as necessary. In contrast to conventional safety filters that prioritize safety without taking into account human decisions, HCSF minimally modifies human actions and actively promotes the transparency of AI assistance. We evaluate HCSF with Assetto Corsa, a high-fidelity car racing simulator, where human drivers are assisted by an AI assistant to race against an in-game opponent. Extensive human trials demonstrate that HCSF enhances safety without compromising the driver\u2019s agency, leading to improved user confidence, comfort, and overall satisfaction.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p093.pdf",
        "supp": "",
        "pdf_size": 4026054,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ldTgZUYlatUJ:scholar.google.com/&scioq=Safety+with+Agency:+Human-Centered+Safety+Filter+with+Application+to+AI-Assisted+Motorsports&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Department of Electrical and Computer Engineering, Princeton University; Department of Mechanical and Aerospace Engineering, Princeton University; Department of Electrical and Computer Engineering, Princeton University; Department of Mechanical and Aerospace Engineering, Princeton University; Department of Electrical and Computer Engineering, Princeton University; Toyota Research Institute; Toyota Research Institute; Toyota Research Institute; Toyota Research Institute; Department of Mechanical and Aerospace Engineering, Princeton University; Department of Electrical and Computer Engineering, Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu;princeton.edu;princeton.edu;tri.global;tri.global;tri.global;tri.global;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu;princeton.edu;princeton.edu;tri.global;tri.global;tri.global;tri.global;princeton.edu;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 11,
        "oa": "https://roboticsconference.org/program/papers/93/",
        "aff_unique_index": "0;0;0;0;0;1;1;1;1;0;0",
        "aff_unique_norm": "Princeton University;Toyota Research Institute",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;",
        "aff_unique_url": "https://www.princeton.edu;https://www.tri.global",
        "aff_unique_abbr": "Princeton;TRI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Self-supervised Multi-future Occupancy Forecasting for Autonomous Driving",
        "session": "Perception and Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "3",
        "author": "Bernard Lange; Masha Itkina; Jiachen Li; Mykel Kochenderfer",
        "abstract": "Environment prediction frameworks are critical for the safe navigation of autonomous vehicles (AVs) in dynamic settings. LiDAR-generated occupancy grid maps (L-OGMs) offer a robust bird\u2019s-eye view scene representation, enabling self-supervised joint scene predictions while exhibiting resilience to partial observability and perception detection failures. Prior approaches have focused on deterministic L-OGM prediction architectures within the grid cell space. While these methods have seen some success, they frequently produce unrealistic predictions and fail to capture the stochastic nature of the environment. Additionally, they do not effectively integrate additional sensor modalities present in AVs. Our proposed framework, Latent Occupancy Prediction (LOPR), performs stochastic L-OGM prediction within the latent space of a generative architecture and allows for conditioning on RGB cameras, maps, and planned trajectories. We decode predictions using either a single-step decoder, which provides high-quality predictions in real-time, or a diffusion-based batch decoder, which can further refine the decoded frames to address temporal consistency issues and reduce compression losses. Our experiments on the nuScenes and Waymo Open datasets show that all variants of our approach qualitatively and quantitatively outperform prior approaches.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p003.pdf",
        "supp": "",
        "pdf_size": 14542831,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2672505376364520405&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/3/"
    },
    {
        "title": "Sense and Sensibility: What makes an social robot convincing to high-school students?",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "82",
        "author": "Pablo Gonzalez-Oliveras; Olov Engwall; Ali Reza Majlesi",
        "abstract": "This study with 40 high-school students demonstrates the high influence of a social educational robot on students\u2019 decision-making for a set of eight true-false questions on electric circuits, for which the theory had been covered in the students\u2019 courses. The robot argued for the correct answer on six questions and the wrong on two, and 75% of the students were persuaded by the robot to perform beyond their expected capacity, positively when the robot was correct and negatively when it was wrong. Students with more experience of using large language models were even more likely to be influenced by the robot\u2019s stance \u2013 in particular for the two easiest questions on which the robot was wrong \u2013 suggesting that familiarity with AI can increase susceptibility to misinformation by AI. We further examined how three different levels of portrayed robot certainty, displayed using semantics, prosody and facial signals, affected how the students aligned with the robot\u2019s answer on specific questions and how convincing they perceived the robot to be on these questions.The students aligned with the robot\u2019s answers in 94.4% of the cases when the robot was portrayed as Certain, 82.6% when it was Neutral and 71.4% when it was Uncertain. The alignment was thus high for all conditions, highlighting students\u2019 general susceptibility to accept the robot\u2019s stance, but alignment in the Uncertain condition was significantly lower than in the Certain. Post-test questionnaire answers further show that students found the robot most convincing when it was portrayed as Certain. These findings highlight the need for educational robots to adjust their display of certainty based on the reliability of the information they convey, to promote students\u2019 critical thinking and reduce undue influence.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p082.pdf",
        "supp": "",
        "pdf_size": 3652387,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Dept. of Intelligent Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Dept. of Intelligent Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Dept. of Education, Stockholm University, Stockholm, Sweden",
        "aff_domain": "kth.se;kth.se;edu.su.se",
        "email": "kth.se;kth.se;edu.su.se",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/82/",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "KTH Royal Institute of Technology;Stockholm University",
        "aff_unique_dep": "Dept. of Intelligent Systems;Dept. of Education",
        "aff_unique_url": "https://www.kth.se;https://www.su.se",
        "aff_unique_abbr": "KTH;SU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stockholm",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "title": "Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation",
        "session": "Manipulation II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "109",
        "author": "Abhiram Maddukuri; Zhenyu Jiang; Lawrence Yunliang Chen; Soroush Nasiriany; Yuqi Xie; Yu Fang; Wenqi Huang; Zu Wang; Zhenjia Xu; Nikita Chernyadev; Scott Reed; Ken Goldberg; Ajay Mandlekar; Linxi Fan; Yuke Zhu",
        "abstract": "Large real-world robot datasets hold great potential for developing generalist robot policies, but scaling real-world data collection is time-consuming, costly, and resource-intensive. Simulation offers a promising solution, with recent advances in generative AI and synthetic data generation tools enabling the creation of large-scale robot demonstration datasets while reducing human effort. However, when training policies solely on data from simulation we must address the sim-to-real gap, often requiring extensive human effort to carefully align simulation with the real world. Recent work has suggested that training on a mixture of simulation and real-world datasets has great promise for improving policy performance, yet a systematic understanding of how to effectively leverage simulation data for real-world vision-based manipulation remains lacking. In this work, we present a simple recipe for effectively utilizing simulation data in real-world manipulation tasks. We derive these insights from comprehensive experiments comparing co-training on various simulation and real-world datasets. Using two domains\u2014a robot arm and a humanoid\u2014across diverse tasks, we demonstrate that simulation data can significantly enhance real-world task performance, even with notable differences between the simulation and real-world data. Through controlled experiments, we provide guidelines on how to optimize across different factors in simulation data to enable successful real-world transfer.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p109.pdf",
        "supp": "",
        "pdf_size": 3197676,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6095637587361981581&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "UT Austin; NVIDIA+UT Austin; NVIDIA+UC Berkeley; NVIDIA+UT Austin; NVIDIA; NVIDIA; NVIDIA; NVIDIA+New York University; NVIDIA; NVIDIA; NVIDIA; UC Berkeley; NVIDIA; NVIDIA; NVIDIA+UT Austin",
        "aff_domain": "; ; ; ; ; ; ; ; ; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "co-training.github.io",
        "author_num": 15,
        "oa": "https://roboticsconference.org/program/papers/109/",
        "aff_unique_index": "0;1+0;1+2;1+0;1;1;1;1+3;1;1;1;2;1;1;1+0",
        "aff_unique_norm": "University of Texas at Austin;NVIDIA;University of California, Berkeley;New York University",
        "aff_unique_dep": ";NVIDIA Corporation;;",
        "aff_unique_url": "https://www.utexas.edu;https://www.nvidia.com;https://www.berkeley.edu;https://www.nyu.edu",
        "aff_unique_abbr": "UT Austin;NVIDIA;UC Berkeley;NYU",
        "aff_campus_unique_index": "0;0;2;0;;2;0",
        "aff_campus_unique": "Austin;;Berkeley",
        "aff_country_unique_index": "0;0+0;0+0;0+0;0;0;0;0+0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Sketch-to-Skill: Bootstrapping Robot Learning with Human Drawn Trajectory Sketches",
        "session": "Manipulation III",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "151",
        "author": "Peihong Yu; Amisha Bhaskar; Anukriti Singh; Zahiruddin Mahammad; Pratap Tokekar",
        "abstract": "Training robotic manipulation policies traditionally requires numerous demonstrations and/or environmental rollouts. While recent Imitation Learning (IL) and Reinforcement Learning (RL) methods have reduced the number of required demonstrations, they still rely on expert knowledge to collect high-quality data, limiting scalability and accessibility. We propose Sketch-to-Skill, a novel framework that leverages human-drawn 2D sketch trajectories to bootstrap and guide RL for robotic manipulation. Our approach extends beyond previous sketch-based methods, which were primarily focused on imitation learning or policy conditioning, limited to specific trained tasks. Sketch-to-Skill employs a Sketch-to-3D Trajectory Generator that translates 2D sketches into 3D trajectories, which are then used to autonomously collect initial demonstrations. We utilize these sketch-generated demonstrations in two ways: to pre-train an initial policy through behavior cloning and to refine this policy through RL with guided exploration. Experimental results demonstrate that Sketch-to-Skill achieves ~96% of the performance of the baseline model that leverages teleoperated demonstration data, while exceeding the performance of a pure reinforcement learning policy by ~170%, only from sketch inputs. This makes robotic manipulation learning more accessible and potentially broadens its applications across various domains.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p151.pdf",
        "supp": "",
        "pdf_size": 5114478,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4447634427992382569&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Maryland, College Park, Maryland, 20742; Department of Computer Science, University of Maryland, College Park, Maryland, 20742; Department of Computer Science, University of Maryland, College Park, Maryland, 20742; Department of Computer Science, University of Maryland, College Park, Maryland, 20742; Department of Computer Science, University of Maryland, College Park, Maryland, 20742",
        "aff_domain": "umd.edu;umd.edu;umd.edu;umd.edu;umd.edu",
        "email": "umd.edu;umd.edu;umd.edu;umd.edu;umd.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/151/",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Maryland, College Park",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL",
        "session": "Scaling Robot Learning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "27",
        "author": "Songyuan Zhang; Oswin So; Mitchell Black; Zachary Serlin; Chuchu Fan",
        "abstract": "Tasks for multi-robot systems often require the robots to collaborate and complete a team goal while maintaining safety. This problem is usually formalized as a Constrained Markov decision process (CMDP), which targets minimizing a global cost and bringing the mean of constraint violation below a user-defined threshold. Inspired by real-world robotic applications, we define safety as zero constraint violation. While many safe multi-agent reinforcement learning (MARL) algorithms have been proposed to solve CMDPs, these algorithms suffer from unstable training in this setting. To tackle this, we use the epigraph form for constrained optimization to improve training stability and prove that the centralized epigraph form problem can be solved in a distributed fashion by each agent. This results in a novel centralized training distributed execution MARL algorithm which we name",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p027.pdf",
        "supp": "",
        "pdf_size": 53199213,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:yPjaEdtfiykJ:scholar.google.com/&scioq=Solving+Multi-Agent+Safe+Optimal+Control+with+Distributed+Epigraph+Form+MARL&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Department of Aeronautics and Astronautics, MIT; Department of Aeronautics and Astronautics, MIT; MIT Lincoln Laboratory; MIT Lincoln Laboratory; Department of Aeronautics and Astronautics, MIT",
        "aff_domain": "mit.edu;mit.edu;ll.mit.edu;ll.mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;ll.mit.edu;ll.mit.edu;mit.edu",
        "github": "",
        "project": "https://mit-realm.github.io/def-marl/",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/27/",
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Massachusetts Institute of Technology Lincoln Laboratory",
        "aff_unique_dep": "Department of Aeronautics and Astronautics;Lincoln Laboratory",
        "aff_unique_url": "https://web.mit.edu;https://www.ll.mit.edu",
        "aff_unique_abbr": "MIT;MIT LL",
        "aff_campus_unique_index": "0;0;1;1;0",
        "aff_campus_unique": "Cambridge;Lexington",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Models",
        "session": "VLA Models",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "11",
        "author": "Delin Qu; Haoming Song; Qizhi Chen; Yuanqi Yao; Xinyi Ye; Jiayuan Gu; Zhigang Wang; Yan Ding; Bin Zhao; Dong Wang; Xuelong Li",
        "abstract": "In this paper, we claim that spatial understanding is the keypoint in robot manipulation, and propose SpatialVLA to explore effective spatial representations for the robot foundation model. Specifically, we propose Ego3D Position Encoding to inject 3D information into VLA\u2019s input observations, and introduce",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p011.pdf",
        "supp": "",
        "pdf_size": 6845187,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6194700719962721717&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; ShanghaiTech University; Shanghai AI Laboratory; Shanghai AI Laboratory",
        "aff_domain": "m.fudan.edu.cn; ; ;gmail.com; ; ; ; ; ; ; ",
        "email": "m.fudan.edu.cn; ; ;gmail.com; ; ; ; ; ; ; ",
        "github": "https://spatialvla.github.io",
        "project": "",
        "author_num": 11,
        "oa": "https://roboticsconference.org/program/papers/11/",
        "aff_unique_index": "0;0;0;0;0;0;0;0;1;0;0",
        "aff_unique_norm": "Shanghai AI Laboratory;ShanghaiTech University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.shanghai-ai-lab.com;https://www.shanghaitech.edu.cn",
        "aff_unique_abbr": "SAIL;ShanghaiTech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Superfast Configuration-Space Convex Set Computation on GPUs for Online Motion Planning",
        "session": "Planning",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "45",
        "author": "Peter Werner; Richard Cheng; Tom Stewart; Russ Tedrake; Daniela Rus",
        "abstract": "In this work, we leverage GPUs to construct probabilistically collision-free convex sets in robot configuration space on the fly. This extends the use of modern motion planning algorithms that leverage such representations to changing environments.  These planners rapidly and reliably optimize high-quality trajectories, without the burden of challenging nonconvex collision-avoidance constraints. We present an algorithm that inflates collision-free piecewise linear paths into sequences of convex sets (SCS) that are probabilistically collision-free using massive parallelism. We then integrate this algorithm into a motion planning pipeline, which leverages dynamic roadmaps to rapidly find one or multiple collision-free paths, and inflates them. We then optimize the trajectory through the probabilistically collision-free sets, simultaneously using the candidate trajectory to detect and remove collisions from the sets. We demonstrate the efficacy of our approach on a simulation benchmark and a KUKA iiwa 7 robot manipulator with perception in the loop. On our benchmark, our approach runs 17.1 times faster and yields a 27.9% increase in reliability over the nonlinear trajectory optimization baseline, while still producing high-quality motion plans.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p045.pdf",
        "supp": "",
        "pdf_size": 9023442,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6715404322213393740&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "MIT CSAIL+Toyota Research Institute; Toyota Research Institute; Woven by Toyota; MIT CSAIL+Toyota Research Institute; MIT CSAIL",
        "aff_domain": "mit.edu;tri.global;woven.toyota;mit.edu;mit.edu",
        "email": "mit.edu;tri.global;woven.toyota;mit.edu;mit.edu",
        "github": "",
        "project": "https://sites.google.com/view/GPUPolytopes",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/45/",
        "aff_unique_index": "0+1;1;2;0+1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Toyota Research Institute;Toyota",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;;Woven",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.tri.global;https://www.toyota-global.com",
        "aff_unique_abbr": "MIT CSAIL;TRI;Toyota",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0+0;0;1;0+0;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "title": "Tactile sensing enables vertical obstacle negotiation for elongate many-legged robots",
        "session": "Robot Design",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "133",
        "author": "Juntao He; Baxi Chong; Massimiliano Iaschi; Vincent R. Nienhusser; Sehoon Ha; Daniel Goldman",
        "abstract": "Many-legged elongated robots show promise for reliable mobility on rugged landscapes. However, most studies on these systems focus on motion planning in the 2D horizontal plane (e.g., translation and rotation) without addressing rapid vertical motion. Despite their success on mild rugged terrains, recent field tests reveal a critical need for 3D behaviors (e.g., climbing or traversing tall obstacles) in real-world application. The challenges of 3D motion planning partially lie in designing sensing and control for a complex high-degree-of-freedom system, typically with over 25 degrees of freedom. To address the first challenge, we propose a tactile antenna system that enables the robot to probe obstacles and gather information about the structure of the environment. Building on this sensory input, we develop a control framework that integrates data from the antenna and foot contact sensors to dynamically adjust the robot\u2019s vertical body undulation for effective climbing. With the addition of simple, low-bandwidth tactile sensors, a robot with high static stability and redundancy exhibits predictable climbing performance in complex environments using a simple feedback controller. Laboratory and outdoor experiments demonstrate the robot\u2019s ability to climb obstacles up to five times its height. Moreover, the robot exhibits robust climbing capabilities on obstacles covered with flowable, robot-sized random items and those characterized by rapidly changing curvatures. These findings demonstrate an alternative solution to perceive the environment and facilitate effective response for legged robots, paving ways towards future highly capable, low-profile many-legged robots.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p133.pdf",
        "supp": "",
        "pdf_size": 27563918,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VoG9DbU-m0kJ:scholar.google.com/&scioq=Tactile+sensing+enables+vertical+obstacle+negotiation+for+elongate+many-legged+robots&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Institute for Robotics and Intelligent Machines, Georgia Institute of Technology; School of Physics, Georgia Institute of Technology; School of Mechanical Engineering, Georgia Institute of Technology; School of Mechanical Engineering, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology; School of Physics, Georgia Institute of Technology",
        "aff_domain": "gatech.edu; ; ; ; ; ",
        "email": "gatech.edu; ; ; ; ; ",
        "github": "https://juntaohe.github.io/Climbing RSS2025/",
        "project": "",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/133/",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "Institute for Robotics and Intelligent Machines",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;1;0;0;0;1",
        "aff_campus_unique": "Atlanta;Georgia Tech",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Towards Uncertainty Unification: A Case Study for Preference Learning",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "91",
        "author": "Shaoting Peng; Haonan Chen; Katherine Rose Driggs-Campbell",
        "abstract": "Learning human preferences is essential for human-robot interaction, as it enables robots to adapt their behaviors to align with human expectations and goals. However, the inherent uncertainties in both human behavior and robotic systems make preference learning a challenging task. While probabilistic robotics algorithms offer uncertainty quantification, the integration of human preference uncertainty remains underexplored. To bridge this gap, we introduce uncertainty unification and propose a novel framework, uncertainty-unified preference learning (UUPL), which enhances Gaussian Process (GP)-based preference learning by unifying human and robot uncertainty. Specifically, UUPL includes a human preference uncertainty model that improves GP posterior mean estimation, and an uncertainty-unified Gaussian Mixture Model that enhances GP predictive covariance accuracy. Additionally, we design a user-specific calibration process to personalize the uncertainty parameters and further improve user experience. Comprehensive experiments and user studies demonstrate that UUPL achieves state-of-the-art performance in both prediction accuracy and human ratings. An ablation study further validates the effectiveness of uncertainty-unified covariance and the human preference uncertainty model of UUPL.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p091.pdf",
        "supp": "",
        "pdf_size": 7616628,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14915795106282006318&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/91/"
    },
    {
        "title": "Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification",
        "session": "Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "144",
        "author": "Kumar Manas; Christian Schlauch; Adrian Paschke; Christian Wirth; Nadja Klein",
        "abstract": "Deep learning-based trajectory prediction models have demonstrated promising capabilities in capturing complex interactions. However, their generalization beyond the available training data, limited in both size and diversity, remains a significant challenge. To improve generalization, we propose SHIFT (Spectral Heteroscedastic Informed Forecasting for Trajectories), a novel framework uniquely combining well-calibrated uncertainty modeling with informative priors derived through automated rule extraction. SHIFT reformulates trajectory prediction as a classification task and effectively employs Heteroscedastic Spectral Normalized Gaussian Processes to disentangle epistemic and aleatoric uncertainties. By leveraging the epistemic uncertainty, we learn informative priors from training targets, which are automatically generated from natural language driving rules, such as stop rules and drivability constraints, using a retrieval-augmented generation framework powered by a large language model. Extensive evaluations on the nuScenes dataset, including challenging low-data and cross-location scenarios, demonstrate that SHIFT outperforms state-of-the-art methods, achieving substantial gains in uncertainty calibration and displacement metrics. In particular, our model excels in complex scenarios, such as intersections, where uncertainty is inherently higher. Code and models are publicly available on Anonymous Repo (currently as a supplementary file for the review stage).",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p144.pdf",
        "supp": "",
        "pdf_size": 1396832,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ktoDbWhcObAJ:scholar.google.com/&scioq=Uncertainty-Aware+Trajectory+Prediction+via+Rule-Regularized+Heteroscedastic+Deep+Classification&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Department of Mathematics and Computer Science, Freie Universit\u00e4t Berlin; Continental Automotive Technologies GmbH, AI Lab Berlin + Karlsruhe Institute of Technology, Scientific Computing Center, Methods for Big Data; Department of Mathematics and Computer Science, Freie Universit\u00e4t Berlin + Fraunhofer Institute for Open Communication Systems, Berlin, Germany; Continental Automotive Technologies GmbH, AI Lab Berlin; Karlsruhe Institute of Technology, Scientific Computing Center, Methods for Big Data",
        "aff_domain": "fu-berlin.de; ;fu-berlin.de; ; ",
        "email": "fu-berlin.de; ;fu-berlin.de; ; ",
        "github": "",
        "project": "https://kumarmanas.github.io/SHIFT/",
        "author_num": 5,
        "oa": "https://roboticsconference.org/program/papers/144/",
        "aff_unique_index": "0;1+2;0+3;1;2",
        "aff_unique_norm": "Freie Universit\u00e4t Berlin;Continental Automotive Technologies GmbH;Karlsruhe Institute of Technology;Fraunhofer Institute for Open Communication Systems",
        "aff_unique_dep": "Department of Mathematics and Computer Science;AI Lab;Scientific Computing Center;",
        "aff_unique_url": "https://www.fu-berlin.de;https://www.continental-automotive.com;https://www.kit.edu;https://www.fokus.fraunhofer.de",
        "aff_unique_abbr": "FU Berlin;CAT;KIT;FOKUS",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Berlin",
        "aff_country_unique_index": "0;0+0;0+0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks",
        "session": "VLA Models",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "13",
        "author": "Jiazhao Zhang; Kunyu Wang; Shaoan Wang; Minghan Li; Haoran Liu; Songlin Wei; Zhongyuan Wang; Zhizheng Zhang; He Wang",
        "abstract": "Embodied Navigation is a fundamental capability for intelligent robots, requiring robots to follow human commands and move autonomously within physical environments. Despite significant advancements, most existing navigation approaches are tailored to specific navigation tasks, such as instruction following, searching objects, answering questions, tracking people, and more. However, the incremental demands of advanced embodied navigation raise the challenge of designing a practical navigation agent that can handle multi-navigation tasks and benefits from the synergy between these tasks. To this end, we present Uni-NaVid, a video-based vision-language-action (VLA) model to unify different paradigms of navigation tasks with textual instruction and RGB video streams as inputs and directly output discrete low-level actions. To efficiently process extensive RGB video streams, we propose an online token merge strategy that spatially and temporally consolidates similar visual information which leads to 5Hz inference speed. For training Uni-NaVid, we collected 3.6 million navigation data samples across four diverse navigation tasks. Extensive experiments on diverse navigation benchmarks demonstrate that Uni-NaVidachieves state-of-the-art performance within a unified framework. Additionally, real-world experiments confirm the model\u2019s effectiveness and efficiency, shedding light on its strong generalizability.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p013.pdf",
        "supp": "",
        "pdf_size": 14133864,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11538861597503714440&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "CFCS, School of Computer Science, Peking University+Galbot+Beijing Academy of Artificial Intelligence; Galbot; CFCS, School of Computer Science, Peking University+Galbot+Beijing Academy of Artificial Intelligence; Galbot; CFCS, School of Computer Science, Peking University+Galbot+Beijing Academy of Artificial Intelligence; CFCS, School of Computer Science, Peking University+Galbot+Beijing Academy of Artificial Intelligence; Galbot; CFCS, School of Computer Science, Peking University+Galbot+Beijing Academy of Artificial Intelligence; CFCS, School of Computer Science, Peking University+Galbot+Beijing Academy of Artificial Intelligence",
        "aff_domain": "gmail.com; ; ; ; ; ; ;galbot.com;pku.edu.cn",
        "email": "gmail.com; ; ; ; ; ; ;galbot.com;pku.edu.cn",
        "github": "",
        "project": "https://pku-epic.github.io/Uni-NaVid",
        "author_num": 9,
        "oa": "https://roboticsconference.org/program/papers/13/",
        "aff_unique_index": "0+1+2;1;0+1+2;1;0+1+2;0+1+2;1;0+1+2;0+1+2",
        "aff_unique_norm": "Peking University;Galbot;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "School of Computer Science;;",
        "aff_unique_url": "http://www.pku.edu.cn;;https://www.baaic.cn",
        "aff_unique_abbr": "PKU;;BAAI",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China;"
    },
    {
        "title": "Unified Video Action Model",
        "session": "Imitation Learning I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "74",
        "author": "Shuang Li; Yihuai Gao; Dorsa Sadigh; Shuran Song",
        "abstract": "A unified video and action model holds significant promise for robotics, where videos provide rich scene information for action prediction, and actions demonstrate how interactions influence visual observations. However, effectively combining video generation and action prediction remains challenging, and current video generation-based methods struggle to match the performance of direct policy learning in action accuracy and inference speed. To bridge this gap, we introduce the Unified Video Action model (UVA), which jointly optimizes video and action predictions to achieve both high accuracy and efficient action inference. The key lies in learning a joint video-action latent representation and decoupling video-action decoding. The joint latent representation bridges the visual and action domains, effectively modeling the relationship between video and action sequences. Meanwhile, the decoupled decoding, powered by two lightweight diffusion heads, enables high-speed action inference by bypassing video generation during inference. Such a unified framework further enables versatile functionality through masked input training. By selectively masking actions or videos, a single model can tackle diverse tasks beyond policy learning, such as forward and inverse dynamics modeling and video generation. Via an extensive set of experiments, we demonstrate that UVA can serve as a general-purpose solution for a wide range of robotics tasks without compromising performance compared to methods tailored for specific applications. Code and training details will be made available online to facilitate result reproduction.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p074.pdf",
        "supp": "",
        "pdf_size": 3468445,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7113718417536724845&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "oa": "https://roboticsconference.org/program/papers/74/"
    },
    {
        "title": "Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets",
        "session": "VLA Models",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "15",
        "author": "Chuning Zhu; Raymond Yu; Siyuan Feng; Benjamin Burchfiel; Paarth Shah; Abhishek Gupta",
        "abstract": "Imitation learning has emerged as a promising approach towards building generalist robots. However, the reliance on high-quality expert demonstrations poses a challenge in scaling imitation learning for large-scale robot foundation models. On the other hand, large amounts of video data depicting a wide range of environments and diverse behaviors are readily available. This data provides a rich source of information about real-world dynamics and agent-environment interactions. Leveraging this data efficiently for robotics, however, is difficult due to the lack of action annotation necessary for current imitation learning methods. In this work, we present Unified World Models, a framework that allows for leveraging video data for policy learning. Specifically, a UWM integrates an action diffusion process and a video diffusion process within a unified transformer architecture, where independent diffusion timesteps govern each modality. By controlling each diffusion timestep, UWMs can flexibly generate samples from the forward dynamics, the inverse dynamics, as well as marginal and joint distributions. Through simulated and real-world experiments, we show that: (1) UWMs can effectively be used as a policy class for behavior cloning, achieving comparable performance to state-of-the-art behavior cloning methods, (2) UWMs enable efficient pretraining on large-scale multitask robot datasets, where finetuned policies outperform baselines in terms of generalization and robustness and (3) UWMs naturally enable learning from action-free video data through independent control of modality-specific diffusion timesteps, further improving the performance of finetuned policies. Our results suggest that UWMs offer a promising step toward harnessing large, heterogeneous datasets for scalable robot learning.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p015.pdf",
        "supp": "",
        "pdf_size": 16301172,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11929040922155818249&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Paul G. Allen School of Computer Science and Engineering, University of Washington; Paul G. Allen School of Computer Science and Engineering, University of Washington; Toyota Research Institute; Toyota Research Institute; Toyota Research Institute; Paul G. Allen School of Computer Science and Engineering, University of Washington",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "https://weirdlabuw.github.io/uwm/",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/15/",
        "aff_unique_index": "0;0;1;1;1;0",
        "aff_unique_norm": "University of Washington;Toyota Research Institute",
        "aff_unique_dep": "Paul G. Allen School of Computer Science and Engineering;",
        "aff_unique_url": "https://www.washington.edu;https://www.tri.global",
        "aff_unique_abbr": "UW;TRI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Users and Wizards in Conversations: How WoZ Interface Choices Define Human-Robot Interactions",
        "session": "HRI",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "85",
        "author": "Ekaterina Torubarova; Jura Miniotaite; Andre Pereira",
        "abstract": "In this paper, we investigated how the choice of a Wizard-of-Oz (WoZ) interface affects communication with a robot from both the user\u2019s and the wizard\u2019s perspective. In a conversational setting, we used three WoZ interfaces with varying levels of dialogue input and output restrictions: a) prototypical Restricted GUI with TTS dialogue feedback and fixed-view video as input and pre-scripted robot\u2019s utterances and gestures as output; b) Unrestricted GUI with an additional real-time audio input c) VR telepresence interface with real-time audio and immersive video input and spontaneous user-generated verbal and non-verbal output.  We found that the interaction mediated by the VR interface was by far most preferred by the users in terms of robot\u2019s features and perceived social presence. For the wizards, the VR condition turned out to be the most demanding but elicited higher social connection with the users. VR interface also induced the most connected interaction in terms of inter-speaker gaps and overlaps, while Restricted GUI induced the least connected flow and the largest silence. Given these results, we argue that WoZ studies using telepresence interfaces offer a promising path to automation based on naturalistic contextualized verbal and non-verbal behavioral data.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p085.pdf",
        "supp": "",
        "pdf_size": 2689489,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Division of Speech, Music and Hearing, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Speech, Music and Hearing, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Speech, Music and Hearing, KTH Royal Institute of Technology, Stockholm, Sweden",
        "aff_domain": "kth.se;kth.se;kth.se",
        "email": "kth.se;kth.se;kth.se",
        "github": "",
        "project": "",
        "author_num": 3,
        "oa": "https://roboticsconference.org/program/papers/85/",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "KTH Royal Institute of Technology",
        "aff_unique_dep": "Division of Speech, Music and Hearing",
        "aff_unique_url": "https://www.kth.se",
        "aff_unique_abbr": "KTH",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stockholm",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "title": "V-HOP: Visuo-Haptic 6D Object Pose Tracking",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "37",
        "author": "Hongyu Li; Mingxi Jia; Mete Tuluhan Akbulut; Yu Xiang; George Konidaris; Srinath Sridhar",
        "abstract": "Humans naturally integrate vision and haptics for robust object perception during manipulation; losing either modality significantly degrades performance. Inspired by this multisensory integration, prior pose estimation research has attempted to combine visual and haptic/tactile feedback. While these works demonstrate improvements in controlled environments or synthetic datasets, they often underperform vision-only approaches in real-world settings due to poor generalization across diverse grippers, sensor layouts, or sim-to-real environments. Furthermore, they typically estimate the pose for each frame, resulting in less coherent tracking over sequences in real-world deployments. To address these limitations, we introduce a novel unified haptic representation that effectively handles multiple gripper embodiments. Building on this representation, we introduce a visuo-haptic Transformer-based pose tracker that seamlessly integrates visual and haptic input. We validate our framework on our dataset and the Feelsight dataset, demonstrating significant performance improvement on challenging sequences. Notably, our method achieves superior generalization and robustness across novel embodiments, objects, and sensor types (both taxel-based and vision-based tactile sensors). In real-world experiments, we demonstrate that our approach outperforms state-of-the-art visual trackers by a large margin. We further show that we can achieve precise manipulation tasks by incorporating our real-time tracking result into motion plans, underscoring the advantages of visuo-haptic perception. Our model and dataset will be made open-source upon paper acceptance.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p037.pdf",
        "supp": "",
        "pdf_size": 15152994,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7947811040421537352&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Brown University; Brown University; Brown University; The University of Texas at Dallas; Brown University; Brown University",
        "aff_domain": "cs.brown.edu;brown.edu;brown.edu;utdallas.edu;brown.edu;brown.edu",
        "email": "cs.brown.edu;brown.edu;brown.edu;utdallas.edu;brown.edu;brown.edu",
        "github": "",
        "project": "https://ivl.cs.brown.edu/research/v-hop",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/37/",
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "Brown University;University of Texas at Dallas",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.brown.edu;https://www.utdallas.edu",
        "aff_unique_abbr": "Brown;UT Dallas",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Dallas",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Verti-Bench: A General and Scalable Off-Road Mobility Benchmark for Vertically Challenging Terrain",
        "session": "Navigation",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "138",
        "author": "Tong Xu; Chenhui Pan; Madhan B. Rao; Aniket Datar; Anuj Pokhrel; Yuanjie Lu; Xuesu Xiao",
        "abstract": "Recent advancement in off-road autonomy has shown promises in deploying autonomous mobile robots in outdoor off-road environments. Encouraging results have been reported from both simulated and real-world experiments. However, unlike evaluating off-road perception tasks on static datasets, benchmarking off-road mobility still faces significant challenges due to a variety of factors, including variations in vehicle platforms and terrain properties. Furthermore, different vehicle-terrain interactions need to be unfolded during mobility evaluation, which requires the mobility systems to interact with the environments instead of comparing against a pre-collected dataset. In this paper, we present Verti-Bench, a mobility benchmark that focuses on extremely rugged, vertically challenging off-road environments. 100 unique off-road environments and 1000 distinct navigation tasks with millions of off-road terrain properties, including a variety of geometry and semantics, rigid and deformable surfaces, and large natural obstacles, provide standardized and objective evaluation in high-fidelity multi-physics simulation. Verti-Bench is also scalable to various vehicle platforms with different scales and actuation mechanisms. We also provide datasets from expert demonstration, random exploration, failure cases (rolling over and getting stuck), as well as a gym-like interface for reinforcement learning. We use Verti-Bench to benchmark 10 off-road mobility systems, present our findings, and identify future off-road mobility research directions.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p138.pdf",
        "supp": "",
        "pdf_size": 21509141,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14894264036602539717&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "George Mason University; George Mason University; George Mason University; George Mason University; George Mason University; George Mason University; George Mason University",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "https://cs.gmu.edu/~tildelowxiao/Research/Verti-Bench",
        "author_num": 7,
        "oa": "https://roboticsconference.org/program/papers/138/",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "George Mason University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gmu.edu",
        "aff_unique_abbr": "GMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ViTaSCOPE: Visuo-tactile Implicit Representation for In-hand Pose and Extrinsic Contact Estimation",
        "session": "Manipulation I",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "54",
        "author": "Jayjun Lee; Nima Fazeli",
        "abstract": "Mastering dexterous, contact-rich object manipulation demands precise estimation of both in-hand object poses and external contact locations\u2014tasks particularly challenging due to partial and noisy observations. We present ViTaSCOPE: Visuo-Tactile Simultaneous Contact and Object Pose Estimation, a neural implicit representation that fuses vision and high-resolution tactile feedback for contact-aware 3D object reconstruction. By representing objects as signed distance fields and conditioning on shear field data from tactile sensors alongside visual feedback, ViTaSCOPE accurately localizes objects and registers extrinsic contacts onto their 3D geometry. Our method enables seamless reasoning over complementary visuo-tactile cues, and bridges the sim-to-real gap by leveraging simulation for scalable training. We evaluate our method through comprehensive simulated and real-world experiments, demonstrating its capabilities in dexterous manipulation scenarios.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p054.pdf",
        "supp": "",
        "pdf_size": 2744352,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:sbOa15PLlEUJ:scholar.google.com/&scioq=ViTaSCOPE:+Visuo-tactile+Implicit+Representation+for+In-hand+Pose+and+Extrinsic+Contact+Estimation&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Robotics Department, University of Michigan; Robotics Department, University of Michigan",
        "aff_domain": ";",
        "email": ";",
        "github": "https://jayjunlee.github.io/vitascope",
        "project": "",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/54/",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Robotics Department",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Vib2Move: In-hand Object Reconfiguration via Fingertip Micro-vibrations",
        "session": "Manipulation II",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "108",
        "author": "Xili Yi; Nima Fazeli",
        "abstract": "We introduce Vib2Move, a novel approach for in-hand object reconfiguration that harnesses fingertip micro-vibrations and gravity to precisely reposition planar objects. Our framework comprises three key innovations. First, we design a vibration-based actuator that dynamically modulates the effective finger\u2013object friction coefficient, effectively emulating changes in gripping force. Second, we derive a sliding motion model for objects clamped in a parallel gripper with two symmetric, variable-friction contact patches. Third, we propose a motion planner that coordinates end-effector trajectories and fingertip vibrations to achieve the desired object pose. In real-world trials, Vib2Move consistently yields final positioning errors below 5 mm, demonstrating reliable, high-precision manipulation across a variety of planar objects.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p108.pdf",
        "supp": "",
        "pdf_size": 15792446,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Department of Robotics, University of Michigan, Michigan, Ann Arbor 48109; Department of Robotics, University of Michigan, Michigan, Ann Arbor 48109",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "",
        "project": "https://vib2move.github.io",
        "author_num": 2,
        "oa": "https://roboticsconference.org/program/papers/108/",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Robotics",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Vysics: Object Reconstruction Under Occlusion by Fusing Vision and Contact-Rich Physics",
        "session": "Perception",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "34",
        "author": "Bibit Bianchini; Minghan Zhu; Mengti Sun; Bowen Jiang; Camillo Jose Taylor; Michael Posa",
        "abstract": "We introduce Vysics, a vision-and-physics framework for a robot to build an expressive geometry and dynamics model of a single rigid body, using a seconds-long RGBD video and the robot\u2019s proprioception. While the computer vision community has built powerful visual 3D perception algorithms, cluttered environments with heavy occlusions can limit the visibility of objects of interest. However, observed motion of partially occluded objects can imply physical interactions took place, such as contact with a robot or the environment. These inferred contacts can supplement the visible geometry with \u201cphysible geometry,\u201d which best explains the observed object motion through physics. Vysics uses a vision-based tracking and reconstruction method, BundleSDF, to estimate the trajectory and the visible geometry from an RGBD video, and an odometry-based model learning method, Physics Learning Library (PLL), to infer the \u201cphysible\u201d geometry from the trajectory through implicit contact dynamics optimization. The visible and \u201cphysible\u201d geometries jointly factor into optimizing a signed distance function (SDF) to represent the object shape. Vysics does not require pre-training, nor tactile or force sensors. Compared with vision-only methods, Vysics yields object models with higher geometric accuracy and better dynamics prediction in experiments where the object interacts with the robot and the environment under heavy occlusion.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p034.pdf",
        "supp": "",
        "pdf_size": 18540698,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18183305260932840325&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "General Robotics, Automation, Sensing, and Perception (GRASP) Laboratory, University of Pennsylvania, Philadelphia, PA 19104; General Robotics, Automation, Sensing, and Perception (GRASP) Laboratory, University of Pennsylvania, Philadelphia, PA 19104; Amazon, Seattle, WA 98004; General Robotics, Automation, Sensing, and Perception (GRASP) Laboratory, University of Pennsylvania, Philadelphia, PA 19104; General Robotics, Automation, Sensing, and Perception (GRASP) Laboratory, University of Pennsylvania, Philadelphia, PA 19104; General Robotics, Automation, Sensing, and Perception (GRASP) Laboratory, University of Pennsylvania, Philadelphia, PA 19104",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;amazon.com;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;amazon.com;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "github": "",
        "project": "https://vysics-vision-and-physics.github.io/",
        "author_num": 6,
        "oa": "https://roboticsconference.org/program/papers/34/",
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "University of Pennsylvania;Amazon",
        "aff_unique_dep": "General Robotics, Automation, Sensing, and Perception (GRASP) Laboratory;Amazon.com, Inc.",
        "aff_unique_url": "https://www.upenn.edu;https://www.amazon.com",
        "aff_unique_abbr": "UPenn;Amazon",
        "aff_campus_unique_index": "0;0;1;0;0;0",
        "aff_campus_unique": "Philadelphia;Seattle",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations",
        "session": "Manipulation III",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "149",
        "author_site": "Huayi Zhou; Ruixiang Wang; Yunxin Tai; Yueci Deng; Guiliang Liu; Kui Jia",
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12741315124952763733&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "author": "",
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "oa": "https://roboticsconference.org/program/papers/149/"
    },
    {
        "title": "emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands",
        "session": "Robot Design",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "135",
        "author": "Sagar Verma",
        "abstract": "Tendon-driven robotic hands offer unparalleled dexterity for manipulation tasks, but learning control policies for such systems presents unique challenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a direct one-to-one mapping between motion capture (mocap) data and tendon controls, making the learning process complex and expensive. Additionally, visual tracking methods for real-world applications are prone to occlusions and inaccuracies, further complicating joint tracking. Wrist-wearable surface electromyography (sEMG) sensors present an inexpensive, robust alternative to capture hand motion. However, mapping sEMG signals to tendon control remains a significant challenge despite the availability of EMG-to-pose datasets and regression-based models in existing literature.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p135.pdf",
        "supp": "",
        "pdf_size": 3218244,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "https://emg2tendon.github.io/",
        "project": "",
        "author_num": 1,
        "oa": "https://roboticsconference.org/program/papers/135/"
    },
    {
        "title": "\u03c0\u2080: A Vision-Language-Action Flow Model for General Robot Control",
        "session": "VLA Models",
        "affs": "",
        "status": "Poster",
        "track": "main",
        "pid": "10",
        "author_site": "Kevin Black; Noah Brown; Danny Driess; Adnan Esmail; Michael Robert Equi; Chelsea Finn; Niccolo Fusai; Lachy Groom; Karol Hausman; Brian Ichter; Szymon Jakubczak; Tim Jones; Liyiming Ke; Sergey Levine; Adrian Li-Bell; Mohith Mothukuri; Suraj Nair; Karl Pertsch; Lucy Xiaoyang Shi;",
        "author": "Kevin Black; Noah Brown; Danny Driess; Adnan Esmail; Michael Robert Equi; Chelsea Finn; Niccolo Fusai; Lachy Groom; Karol Hausman; Brian Ichter; Szymon Jakubczak; Tim Jones; Liyiming Ke; Sergey Levine; Adrian Li-Bell; Mohith Mothukuri; Suraj Nair; Karl Pertsch; Lucy Xiaoyang Shi; Laura Smith; James Tanner; Quan Vuong; Anna Walling; Haohuan Wang; Ury Zhilinsky",
        "abstract": "Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks via direct prompting, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning.  Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.",
        "bibtex": "",
        "pdf": "https://www.roboticsproceedings.org/rss21/p010.pdf",
        "supp": "",
        "pdf_size": 7873003,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": ";;;;;;;;;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "https://physicalintelligence.company/blog/pi0",
        "author_num": 25,
        "oa": "https://roboticsconference.org/program/papers/10/"
    }
]