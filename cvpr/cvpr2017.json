[
    {
        "title": "3D Bounding Box Estimation Using Deep Learning and Geometry",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3303",
        "author_site": "Arsalan Mousavian, Dragomir Anguelov, John Flynn, Jana Ko\u00c5\u00a1eck\u00c3\u00a1",
        "author": "Arsalan Mousavian; Dragomir Anguelov; John Flynn; Jana Kosecka",
        "abstract": "We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method first regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The first network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which significantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark [??] both on the official metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and flat ground priors [??] and  sub-category detection [??][??]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[??].",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Mousavian_3D_Bounding_Box_CVPR_2017_paper.pdf",
        "aff": "George Mason University + Zoox, Inc.; Zoox, Inc.; Zoox, Inc.; George Mason University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.00496v2",
        "pdf_size": 1226248,
        "gs_citation": 1362,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17791401340833451444&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "gmu.edu;zoox.com;zoox.com;gmu.edu",
        "email": "gmu.edu;zoox.com;zoox.com;gmu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Mousavian_3D_Bounding_Box_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;1;0",
        "aff_unique_norm": "George Mason University;Zoox, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gmu.edu;https://www.zoox.com",
        "aff_unique_abbr": "GMU;Zoox",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "3D Convolutional Neural Networks for Efficient and Robust Hand Pose Estimation From Single Depth Images",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "737",
        "author_site": "Liuhao Ge, Hui Liang, Junsong Yuan, Daniel Thalmann",
        "author": "Liuhao Ge; Hui Liang; Junsong Yuan; Daniel Thalmann",
        "abstract": "We propose a simple, yet effective approach for real-time hand pose estimation from single depth images using three-dimensional Convolutional Neural Networks (3D CNNs). Image based features extracted by 2D CNNs are not directly suitable for 3D hand pose estimation due to the lack of 3D spatial information. Our proposed 3D CNN taking a 3D volumetric representation of the hand depth image as input can capture the 3D spatial structure of the input and accurately regress full 3D hand pose in a single pass. In order to make the 3D CNN robust to variations in hand sizes and global orientations, we perform 3D data augmentation on the training data. Experiments show that our proposed 3D CNN based approach outperforms state-of-the-art methods on two challenging hand pose datasets, and is very efficient as our implementation runs at over 215 fps on a standard computer with a single GPU.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ge_3D_Convolutional_Neural_CVPR_2017_paper.pdf",
        "aff": "Institute for Media Innovation, Interdisciplinary Graduate School, Nanyang Technological University + Institute of High Performance Computing, A*STAR, Singapore; Institute for Media Innovation, Interdisciplinary Graduate School, Nanyang Technological University; Institute for Media Innovation, Interdisciplinary Graduate School, Nanyang Technological University; Institute for Media Innovation, Interdisciplinary Graduate School, Nanyang Technological University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ge_3D_Convolutional_Neural_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2248364,
        "gs_citation": 356,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14738510317517123141&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 8,
        "aff_domain": "ntu.edu.sg;ihpc.a-star.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;ihpc.a-star.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ge_3D_Convolutional_Neural_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Nanyang Technological University;A*STAR Institute of High Performance Computing",
        "aff_unique_dep": "Institute for Media Innovation, Interdisciplinary Graduate School;Institute of High Performance Computing",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.ihpc.a-star.edu.sg",
        "aff_unique_abbr": "NTU;IHPC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "3D Face Morphable Models \"In-The-Wild\"",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "25",
        "author_site": "James Booth, Epameinondas Antonakos, Stylianos Ploumpis, George Trigeorgis, Yannis Panagakis, Stefanos Zafeiriou",
        "author": "James Booth; Epameinondas Antonakos; Stylianos Ploumpis; George Trigeorgis; Yannis Panagakis; Stefanos Zafeiriou",
        "abstract": "3D Morphable Models (3DMMs) are powerful statistical models of 3D facial shape and texture, and among the state-of-the-art methods for reconstructing facial shape from single images. With the advent of new 3D sensors, many 3D facial datasets have been collected containing both neutral as well as expressive faces. However, all datasets are captured under controlled conditions. Thus, even though powerful 3D facial shape models can be learnt from such data, it is difficult to build statistical texture models that are sufficient to reconstruct faces captured in unconstrained conditions (\"in-the-wild\"). In this paper, we propose the first, to the best of our knowledge, \"in-the-wild\" 3DMM by combining a powerful statistical model of facial shape, which describes both identity and expression, with an \"in-the-wild\" texture model. We show that the employment of such an \"in-the-wild\" texture model greatly simplifies the fitting procedure, because there is no need to optimise with regards to the illumination parameters. Furthermore, we propose a new fast algorithm for fitting the 3DMM in arbitrary images. Finally, we have captured the first 3D facial database with relatively unconstrained conditions and report quantitative evaluations with state-of-the-art performance. Complementary qualitative reconstruction results are demonstrated on standard \"in-the-wild\" facial databases.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Booth_3D_Face_Morphable_CVPR_2017_paper.pdf",
        "aff": "Imperial College London, UK+University of Oulu, Finland; Amazon, Berlin, Germany+Imperial College London, UK; Imperial College London, UK; Imperial College London, UK; Imperial College London, UK; Imperial College London, UK+University of Oulu, Finland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Booth_3D_Face_Morphable_2017_CVPR_supplemental.pdf",
        "arxiv": "1701.05360v1",
        "pdf_size": 1148537,
        "gs_citation": 213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12529327510370842694&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "imperial.ac.uk;amazon.com;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;amazon.com;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Booth_3D_Face_Morphable_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2+0;0;0;0;0+1",
        "aff_unique_norm": "Imperial College London;University of Oulu;Amazon",
        "aff_unique_dep": ";;Amazon",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.oulu.fi;https://www.amazon.de",
        "aff_unique_abbr": "ICL;UOulu;Amazon",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Berlin",
        "aff_country_unique_index": "0+1;2+0;0;0;0;0+1",
        "aff_country_unique": "United Kingdom;Finland;Germany"
    },
    {
        "title": "3D Human Pose Estimation = 2D Pose Estimation + Matching",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "3273",
        "author_site": "Ching-Hang Chen, Deva Ramanan",
        "author": "Ching-Hang Chen; Deva Ramanan",
        "abstract": "We explore 3D human pose estimation from a single RGB image. While many approaches try to directly predict 3D pose from image measurements, we explore a simple architecture that reasons through intermediate 2D pose predictions. Our approach is based on two key observations (1) Deep neural nets have revolutionized 2D pose estimation, producing accurate 2D predictions even for poses with self-occlusions (2) \"Big-data\"sets of 3D mocap data are now readily available, making it tempting to \"lift\" predicted 2D poses to 3D through simple memorization (e.g., nearest neighbors). The resulting architecture is straightforward to implement with off-the-shelf 2D pose estimation systems and 3D mocap libraries. Importantly, we demonstratethatsuchmethodsoutperformalmostallstate-of-theart 3D pose estimation systems, most of which directly try to regress 3D pose from 2D measurements.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_3D_Human_Pose_CVPR_2017_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 752292,
        "gs_citation": 738,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14175936363522308900&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "andrew.cmu.edu;cs.cmu.edu",
        "email": "andrew.cmu.edu;cs.cmu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_3D_Human_Pose_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "3D Human Pose Estimation From a Single Image via Distance Matrix Regression",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1034",
        "author": "Francesc Moreno-Noguer",
        "abstract": "This paper addresses the problem of 3D human pose estimation from a single image. We follow a standard two-step pipeline by first detecting the 2D  position of the N body joints, and then using these observations to infer 3D pose. For the first step, we use a recent CNN-based detector. For the second step, most existing approaches perform 2N-to-3N regression of the Cartesian joint coordinates. We show that more precise pose estimates can be obtained by  representing both the 2D and 3D human poses using NxN distance matrices, and formulating the problem as a 2D-to-3D distance matrix regression. For learning such a regressor we leverage on simple Neural Network architectures, which by construction, enforce positivity and symmetry of the predicted  matrices. The approach has also the advantage to naturally handle  missing observations and allowing to hypothesize the position of  non-observed  joints. Quantitative results on Humaneva and Human3.6M datasets demonstrate consistent performance gains over state-of-the-art. Qualitative evaluation on the images in-the-wild of the LSP dataset, using the regressor learned on Human3.6M, reveals very promising generalization results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Moreno-Noguer_3D_Human_Pose_CVPR_2017_paper.pdf",
        "aff": "Institut de Rob `otica i Inform `atica Industrial (CSIC-UPC), 08028, Barcelona, Spain",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.09010",
        "pdf_size": 3127634,
        "gs_citation": 499,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16259704490248444043&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "",
        "email": "",
        "author_num": 1,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Moreno-Noguer_3D_Human_Pose_CVPR_2017_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Institut de Rob\u00f3tica i Inform\u00e0tica Industrial",
        "aff_unique_dep": "CSIC-UPC",
        "aff_unique_url": "https://www.iri.upc.edu/",
        "aff_unique_abbr": "IRI",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Barcelona",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Spain"
    },
    {
        "title": "3D Menagerie: Modeling the 3D Shape and Pose of Animals",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "2825",
        "author_site": "Silvia Zuffi, Angjoo Kanazawa, David W. Jacobs, Michael J. Black",
        "author": "Silvia Zuffi; Angjoo Kanazawa; David W. Jacobs; Michael J. Black",
        "abstract": "There has been significant work on learning realistic, articulated, 3D models of the human body. In contrast, there are few such models of animals, despite many applications. The main challenge is that animals are much less cooperative than humans. The best human body models are learned from thousands of 3D scans of people in specific poses, which is infeasible with live animals. Consequently, we learn our model from a small set of 3D scans of toy figurines in arbitrary poses. We employ a novel part-based shape model to compute an initial registration to the scans. We then normalize their pose, learn a statistical shape model, and refine the registrations and the model together. In this way, we accurately align animal scans from different quadruped families with very different shapes and poses. With the registration to a common template we learn a shape space representing animals including lions, cats, dogs, horses, cows and hippos. Animal shapes can be sampled from the model, posed, animated, and fit to data. We demonstrate generalization by fitting it to images of real animals including species not seen in training.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zuffi_3D_Menagerie_Modeling_CVPR_2017_paper.pdf",
        "aff": "IMATI-CNR, Milan, Italy; University of Maryland, College Park, MD; University of Maryland, College Park, MD; Max Planck Institute for Intelligent Systems, T \u00a8ubingen, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.07700",
        "pdf_size": 3274578,
        "gs_citation": 491,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11055077264697512314&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "mi.imati.cnr.it;umiacs.umd.edu;umiacs.umd.edu;tuebingen.mpg.de",
        "email": "mi.imati.cnr.it;umiacs.umd.edu;umiacs.umd.edu;tuebingen.mpg.de",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zuffi_3D_Menagerie_Modeling_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "IMATI-CNR;University of Maryland;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.imati.cnr.it;https://www/umd.edu;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": ";UMD;MPI-IS",
        "aff_campus_unique_index": "0;1;1;2",
        "aff_campus_unique": "Milan;College Park;T\u00fcbingen",
        "aff_country_unique_index": "0;1;1;2",
        "aff_country_unique": "Italy;United States;Germany"
    },
    {
        "title": "3D Point Cloud Registration for Localization Using a Deep Neural Network Auto-Encoder",
        "session": "3D Vision 2",
        "status": "Oral",
        "track": "main",
        "pid": "1931",
        "author_site": "Gil Elbaz, Tamar Avraham, Anath Fischer",
        "author": "Gil Elbaz; Tamar Avraham; Anath Fischer",
        "abstract": "We present an algorithm for registration between a large-scale point cloud and a close-proximity scanned point cloud, providing a localization solution that is fully independent of prior information about the initial positions of the two point cloud coordinate systems. The algorithm, denoted LORAX, selects super-points--local subsets of points--and describes the geometric structure of each with a low-dimensional descriptor. These descriptors are then used to infer potential matching regions for an efficient coarse registration process, followed by a fine-tuning stage. The set of super-points is selected by covering the point clouds with overlapping spheres, and then filtering out those of low-quality or nonsalient regions. The descriptors are computed using state-of-the-art unsupervised machine learning, utilizing the technology of deep neural network based auto-encoders.   Abstract This novel framework provides a strong alternative to the common practice of using manually designed key-point descriptors for coarse point cloud registration. Utilizing super-points instead of key-points allows the available geometrical data to be better exploited to find the correct transformation. Encoding local 3D geometric structures using a deep neural network auto-encoder instead of traditional descriptors continues the trend seen in other computer vision applications and indeed leads to superior results. The algorithm is tested on challenging point cloud registration datasets, and its advantages over previous approaches as well as its robustness to density changes, noise, and missing data are shown.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Elbaz_3D_Point_Cloud_CVPR_2017_paper.pdf",
        "aff": "Technion - Israel Institute of Technology; Technion - Israel Institute of Technology; Technion - Israel Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2161587,
        "gs_citation": 297,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=122137933463291233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "alumni.technion.ac.il;cs.technion.ac.il;technion.ac.il",
        "email": "alumni.technion.ac.il;cs.technion.ac.il;technion.ac.il",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Elbaz_3D_Point_Cloud_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "3D Shape Segmentation With Projective Convolutional Networks",
        "session": "Machine Learning for 3D Vision",
        "status": "Oral",
        "track": "main",
        "pid": "1431",
        "author_site": "Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji, Siddhartha Chaudhuri",
        "author": "Evangelos Kalogerakis; Melinos Averkiou; Subhransu Maji; Siddhartha Chaudhuri",
        "abstract": "This paper introduces a deep architecture for segmenting 3D objects into their labeled semantic parts. Our architecture  combines image-based Fully Convolutional Networks (FCNs) and surface-based Conditional Random Fields (CRFs) to yield coherent segmentations of 3D shapes. The image-based FCNs are used for efficient view-based reasoning about 3D object parts. Through a special projection layer, FCN outputs are effectively aggregated across multiple views and scales, then are projected onto the 3D object surfaces. Finally, a surface-based CRF combines the projected outputs with geometric consistency cues to yield coherent segmentations. The whole architecture (multi-view FCNs and CRF) is trained end-to-end. Our approach significantly outperforms the existing state-of-the-art methods in the currently largest segmentation benchmark (ShapeNet). Finally, we demonstrate promising segmentation results on noisy 3D shapes acquired from consumer-grade depth cameras.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kalogerakis_3D_Shape_Segmentation_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kalogerakis_3D_Shape_Segmentation_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.02808v3",
        "pdf_size": 3002129,
        "gs_citation": 469,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15573335117386848542&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kalogerakis_3D_Shape_Segmentation_CVPR_2017_paper.html"
    },
    {
        "title": "3DMatch: Learning Local Geometric Descriptors From RGB-D Reconstructions",
        "session": "3D Vision 1",
        "status": "Oral",
        "track": "main",
        "pid": "668",
        "author_site": "Andy Zeng, Shuran Song, Matthias Nie\u00c3\u009fner, Matthew Fisher, Jianxiong Xiao, Thomas Funkhouser",
        "author": "Andy Zeng; Shuran Song; Matthias Niessner; Matthew Fisher; Jianxiong Xiao; Thomas Funkhouser",
        "abstract": "Matching local geometric features on real-world depth images is a challenging task due to the noisy, low-resolution, and incomplete nature of 3D scan data. These difficulties limit the performance of current state-of-art methods, which are typically based on histograms over geometric properties. In this paper, we present 3DMatch, a data-driven model that learns a local volumetric patch descriptor for establishing correspondences between partial 3D data. To amass training data for our model, we propose a self-supervised feature learning method that leverages the millions of correspondence labels found in existing RGB-D reconstructions. Experiments show that our descriptor is not only able to match local geometry in new scenes for reconstruction, but also generalize to different tasks and spatial scales (e.g. instance-level object model alignment for the Amazon Picking Challenge, and mesh surface correspondence). Results show that 3DMatch consistently outperforms other state-of-the-art approaches by a significant margin. Code, data, benchmarks, and pre-trained models are available online at http://3dmatch.cs.princeton.edu",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zeng_3DMatch_Learning_Local_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zeng_3DMatch_Learning_Local_2017_CVPR_supplemental.pdf",
        "arxiv": "1603.08182",
        "pdf_size": 1796812,
        "gs_citation": 1287,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10909682161747562088&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zeng_3DMatch_Learning_Local_CVPR_2017_paper.html"
    },
    {
        "title": "4D Light Field Superpixel and Segmentation",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2829",
        "author_site": "Hao Zhu, Qi Zhang, Qing Wang",
        "author": "Hao Zhu; Qi Zhang; Qing Wang",
        "abstract": "Superpixel segmentation of 2D image has been widely used in many computer vision tasks. However, limited to the Gaussian imaging principle, there is not a thorough segmentation solution to the ambiguity in defocus and occlusion boundary areas. In this paper, we consider the essential element of image pixel, i.e., rays in the light space and propose light field superpixel (LFSP) segmentation to eliminate the ambiguity. The LFSP is first defined mathematically and then a refocus-invariant metric named LFSP self-similarity is proposed to evaluate the segmentation performance. By building a clique system containing 80 neighbors in light field, a robust refocus-invariant LFSP segmentation algorithm is developed. Experimental results on both synthetic and real light field datasets demonstrate the advantages over the state-of-the-arts in terms of traditional evaluation metrics. Additionally the LFSP self-similarity evaluation under different light field refocus levels shows the refocus-invariance of the proposed algorithm.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_4D_Light_Field_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science, Northwestern Polytechnical University, Xi\u2019an 710072, P.R. China; School of Computer Science, Northwestern Polytechnical University, Xi\u2019an 710072, P.R. China; School of Computer Science, Northwestern Polytechnical University, Xi\u2019an 710072, P.R. China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2319859,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17923042605059545559&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "nwpu.edu.cn;nwpu.edu.cn;nwpu.edu.cn",
        "email": "nwpu.edu.cn;nwpu.edu.cn;nwpu.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhu_4D_Light_Field_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Northwestern Polytechnical University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "http://www.nwpu.edu.cn",
        "aff_unique_abbr": "NPU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Xi'an",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Clever Elimination Strategy for Efficient Minimal Solvers",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2027",
        "author_site": "Zuzana Kukelova, Joe Kileel, Bernd Sturmfels, Tomas Pajdla",
        "author": "Zuzana Kukelova; Joe Kileel; Bernd Sturmfels; Tomas Pajdla",
        "abstract": "We present a new insight into the systematic generation of minimal solvers in computer vision, which leads to smaller and faster solvers. Many minimal problem formulations are coupled sets of linear and polynomial equations where image measurements enter the linear equations only. We show that it is useful to solve such systems by first eliminating all the unknowns that do not appear in the linear equations and then extending solutions to the rest of unknowns. This can be generalized to fully non-linear systems by linearization via lifting. We demonstrate that this approach leads to more efficient solvers in three problems of partially calibrated relative camera pose computation with unknown focal length and/or radial distortion. Our approach also generates new interesting constraints on the fundamental matrices of partially calibrated cameras, which were not known before.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kukelova_A_Clever_Elimination_CVPR_2017_paper.pdf",
        "aff": "Czech Technical University in Prague, Faculty of Electrical Engineering, Prague, Czech Republic; University of California, Berkeley, USA; University of California, Berkeley, USA; Czech Technical University in Prague, Faculty of Electrical Engineering, Prague, Czech Republic",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kukelova_A_Clever_Elimination_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.05289v1",
        "pdf_size": 639758,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2710912302698315623&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cmp.felk.cvut.cz;math.berkeley.edu;math.berkeley.edu;cvut.cz",
        "email": "cmp.felk.cvut.cz;math.berkeley.edu;math.berkeley.edu;cvut.cz",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kukelova_A_Clever_Elimination_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Czech Technical University in Prague;University of California, Berkeley",
        "aff_unique_dep": "Faculty of Electrical Engineering;",
        "aff_unique_url": "https://www.cvut.cz;https://www.berkeley.edu",
        "aff_unique_abbr": "CTU;UC Berkeley",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Prague;Berkeley",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Czech Republic;United States"
    },
    {
        "title": "A Combinatorial Solution to Non-Rigid 3D Shape-To-Image Matching",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "346",
        "author_site": "Florian Bernard, Frank R. Schmidt, Johan Thunberg, Daniel Cremers",
        "author": "Florian Bernard; Frank R. Schmidt; Johan Thunberg; Daniel Cremers",
        "abstract": "We propose a combinatorial solution for the problem of non-rigidly matching a 3D shape to 3D image data. To this end, we model the shape as a triangular mesh and allow each triangle of this mesh to be rigidly transformed to achieve a suitable matching to the image. By penalising the distance and the relative rotation between neighbouring triangles our matching compromises between the image and the shape information. In this paper, we resolve two major challenges: Firstly, we address the resulting large and NP-hard combinatorial problem with a suitable graph-theoretic approach. Secondly, we propose an efficient discretisation of the unbounded 6-dimensional Lie group SE(3). To our knowledge this is the first combinatorial formulation for non-rigid 3D shape-to-image matching. In contrast to existing local (gradient descent) optimisation methods, we obtain solutions that do not require a good initialisation and that are within a bound of the optimal solution. We evaluate the proposed combinatorial method on the two problems of non-rigid 3D shape-to-shape and non-rigid 3D shape-to-image registration and demonstrate that it provides promising results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bernard_A_Combinatorial_Solution_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2933161,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2608565098041254651&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bernard_A_Combinatorial_Solution_CVPR_2017_paper.html"
    },
    {
        "title": "A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification and Domain Adaptation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2412",
        "author_site": "Chunpeng Wu, Wei Wen, Tariq Afzal, Yongmei Zhang, Yiran Chen, Hai (Helen) Li",
        "author": "Chunpeng Wu; Wei Wen; Tariq Afzal; Yongmei Zhang; Yiran Chen; Hai (Helen) Li",
        "abstract": "Recently, DNN model compression based on network architecture design, e.g., SqueezeNet, attracted a lot attention. No accuracy drop on image classification is observed on these extremely compact networks, compared to well-known models. An emerging question, however, is whether these model compression techniques hurt DNN's learning ability other than classifying images on a single dataset. Our preliminary experiment shows that these compression methods could degrade domain adaptation (DA) ability, though the classification performance is preserved. Therefore, we propose a new compact network architecture and unsupervised DA method in this paper. The DNN is built on a new basic module Conv-M which provides more diverse feature extractors without significantly increasing parameters. The unified framework of our DA method will simultaneously learn invariance across domains, reduce divergence of feature representations, and adapt label prediction. Our DNN has 4.1M parameters, which is only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN obtains GoogLeNet-level accuracy both on classification and DA, and our DA method slightly outperforms previous competitive ones. Put all together, our DA strategy based on our DNN achieves state-of-the-art on sixteen of total eighteen DA tasks on popular Office-31 and Office-Caltech datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wu_A_Compact_DNN_CVPR_2017_paper.pdf",
        "aff": "Electrical and Computer Engineering Department, University of Pittsburgh; Electrical and Computer Engineering Department, University of Pittsburgh; LG San Jose Lab; LG San Jose Lab; Electrical and Computer Engineering Department, Duke University; Electrical and Computer Engineering Department, Duke University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.04071v4",
        "pdf_size": 761871,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6083364185829236469&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "pitt.edu;pitt.edu;lge.com;lge.com;duke.edu;duke.edu",
        "email": "pitt.edu;pitt.edu;lge.com;lge.com;duke.edu;duke.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wu_A_Compact_DNN_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;1;2;2",
        "aff_unique_norm": "University of Pittsburgh;LG;Duke University",
        "aff_unique_dep": "Electrical and Computer Engineering Department;LG Electronics;Electrical and Computer Engineering Department",
        "aff_unique_url": "https://www.pitt.edu;https://www.lg.com;https://www.duke.edu",
        "aff_unique_abbr": "Pitt;LG;Duke",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";San Jose",
        "aff_country_unique_index": "0;0;1;1;0;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "title": "A Dataset and Exploration of Models for Understanding Video Data Through Fill-In-The-Blank Question-Answering",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3167",
        "author_site": "Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, Christopher Pal",
        "author": "Tegan Maharaj; Nicolas Ballas; Anna Rohrbach; Aaron Courville; Christopher Pal",
        "abstract": "While deep convolutional neural networks frequently approach or exceed human-level performance in benchmark tasks involving static images, extending this success to moving images is not straightforward. Video understanding is of interest for many applications, including content recommendation, prediction, summarization, event/object detection, and understanding human visual perception. However, many domains lack sufficient data to explore and perfect video models. In order to address the need for a simple, quantitative benchmark for developing and understanding video, we present MovieFIB, a fill-in-the-blank question-answering dataset with over 300,000 examples, based on descriptive video annotations for the visually impaired.  In addition to presenting statistics and a description of the dataset, we perform a detailed analysis of 5 different models' predictions, and compare these with human performance. We investigate the relative importance of language, static (2D) visual features, and moving (3D) visual features; the effects of increasing dataset size, the number of frames sampled; and of vocabulary size. We illustrate that: this task is not solvable by a language model alone; our model combining 2D and 3D visual information indeed provides the best result; all models perform significantly worse than human-level. We provide human evaluation for responses given by different models and find that accuracy on the MovieFIB evaluation corresponds well with human judgment. We suggest avenues for improving video models, and hope that the MovieFIB challenge can be useful for measuring and encouraging progress in this very interesting field.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Maharaj_A_Dataset_and_CVPR_2017_paper.pdf",
        "aff": "Polytechnique Montr \u00b4eal; Universit \u00b4e de Montr \u00b4eal; Max-Planck-Institut f \u00a8ur Informatik, Saarland Informatics Campus; Universit \u00b4e de Montr \u00b4eal; Polytechnique Montr \u00b4eal",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Maharaj_A_Dataset_and_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.07810v2",
        "pdf_size": 1100250,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4498853687753347434&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "polymtl.ca;umontreal.ca;mpi-inf.mpg.de;umontreal.ca;polymtl.ca",
        "email": "polymtl.ca;umontreal.ca;mpi-inf.mpg.de;umontreal.ca;polymtl.ca",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Maharaj_A_Dataset_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;1;0",
        "aff_unique_norm": "Polytechnique Montr\u00e9al;Universit\u00e9 de Montr\u00e9al;Max-Planck-Institut f\u00fcr Informatik",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.polymtl.ca;https://www.umontreal.ca;https://mpi-sws.org",
        "aff_unique_abbr": "PolyMTL;UdeM;MPII",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Montr\u00e9al;;Saarland",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "Canada;Germany"
    },
    {
        "title": "A Dataset for Benchmarking Image-Based Localization",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3852",
        "author_site": "Xun Sun, Yuanfan Xie, Pei Luo, Liang Wang",
        "author": "Xun Sun; Yuanfan Xie; Pei Luo; Liang Wang",
        "abstract": "A novel dataset for benchmarking image-based localization is presented. With increasing research interests in visual place recognition and localization, several datasets have been published in the past few years. One of the evident limitations of existing datasets is that precise ground truth camera poses of query images are not available in a meaningful 3D metric system. This is in part due to the underlying 3D models of these datasets are reconstructed from Structure from Motion methods. So far little attention has been paid to metric evaluations of localization accuracy. In this paper we address the problem of whether state-of-the-art visual localization techniques can be applied to tasks with demanding accuracy requirements. We acquired training data for a large indoor environment with cameras and a LiDAR scanner. In addition, we collected over 2000 query images with cell phone cameras. Using LiDAR point clouds as a reference, we employed a semi-automatic approach to estimate the 6 degrees of freedom camera poses precisely in the world coordinate system. The proposed dataset enables us to quantitatively assess the performance of various algorithms using a fair and intuitive metric.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sun_A_Dataset_for_CVPR_2017_paper.pdf",
        "aff": "Baidu Autonomous Driving Business Unit\u2020; Baidu Autonomous Driving Business Unit\u2020; Baidu Autonomous Driving Business Unit\u2020; Baidu Autonomous Driving Business Unit\u2020",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4027527,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16928941677026860267&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sun_A_Dataset_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Baidu",
        "aff_unique_dep": "Autonomous Driving Business Unit",
        "aff_unique_url": "https://www.baidu.com",
        "aff_unique_abbr": "Baidu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Deep Regression Architecture With Two-Stage Re-Initialization for High Performance Facial Landmark Detection",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1236",
        "author_site": "Jiangjing Lv, Xiaohu Shao, Junliang Xing, Cheng Cheng, Xi Zhou",
        "author": "Jiangjing Lv; Xiaohu Shao; Junliang Xing; Cheng Cheng; Xi Zhou",
        "abstract": "Regression based facial landmark detection methods usually learns a series of regression functions to update the landmark positions from an initial estimation. Most of existing approaches focus on learning effective mapping functions with robust image features to improve performance. The approach to dealing with the initialization issue, however, receives relatively fewer attentions. In this paper, we present a deep regression architecture with two-stage re-initialization to explicitly deal with the initialization problem. At the global stage, given an image with a rough face detection result, the full face region is firstly re-initialized by a supervised spatial transformer network to a canonical shape state and then trained to regress a coarse landmark estimation. At the local stage, different face parts are further separately re-initialized to their own canonical shape states, followed by another regression subnetwork to get the final estimation. Our proposed deep architecture is trained from end to end and obtains promising results using different kinds of unstable initialization. It also achieves superior performances over many competing algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lv_A_Deep_Regression_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 308,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3081905076399039911&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lv_A_Deep_Regression_CVPR_2017_paper.html"
    },
    {
        "title": "A Domain Based Approach to Social Relation Recognition",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1289",
        "author_site": "Qianru Sun, Bernt Schiele, Mario Fritz",
        "author": "Qianru Sun; Bernt Schiele; Mario Fritz",
        "abstract": "Social relations are the foundation of human daily life. Developing techniques to analyze such relations from visual data bears great potential to build machines that better understand us and are capable of interacting with us at a social level. Previous investigations have remained partial due to the overwhelming diversity and complexity of the topic and consequently have only focused on a handful of social relations. In this paper, we argue that the domain-based theory from social psychology is a great starting point to systematically approach this problem. The theory provides coverage of all aspects of social relations and equally is concrete and predictive about the visual attributes and behaviors defining the relations included in each domain. We provide the first dataset built on this holistic conceptualization of social life that is composed of a hierarchical label space of social domains and social relations. We also contribute the first models to recognize such domains and relations and find superior performance for attribute based features. Beyond the encouraging performance of the attribute based approach, we also find interpretable features that are in accordance with the predictions from social psychology literature. Beyond our findings, we believe that our contributions more tightly interleave visual recognition and social psychology theory that has the potential to complement the theoretical work in the area with empirical and data-driven models of social life.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sun_A_Domain_Based_CVPR_2017_paper.pdf",
        "aff": "Max Planck Institute for Informatics, Saarland Informatics Campus; Max Planck Institute for Informatics, Saarland Informatics Campus; Max Planck Institute for Informatics, Saarland Informatics Campus",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Sun_A_Domain_Based_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.06456v1",
        "pdf_size": 1851707,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3595316439451366560&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "email": "mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sun_A_Domain_Based_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Max Planck Institute for Informatics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://mpi-inf.mpg.de",
        "aff_unique_abbr": "MPII",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Saarland",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "A Dual Ascent Framework for Lagrangean Decomposition of Combinatorial Problems",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "586",
        "author_site": "Paul Swoboda, Jan Kuske, Bogdan Savchynskyy",
        "author": "Paul Swoboda; Jan Kuske; Bogdan Savchynskyy",
        "abstract": "We propose a general dual ascent (message passing) framework for Lagrangean (dual) decomposition of combinatorial problems. Although methods of this type have shown their efficiency for a number of problems, so far there was no general algorithm applicable to multiple problem types. In this work, we propose such a general algorithm. It depends on several parameters, which can be used to optimize its performance in each particular setting. We demonstrate efficiency of our method on the graph matching and the multicut problems, where it outperforms state-of-the-art solvers including those based on the subgradient optimization and off-the-shelf linear programming solvers.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Swoboda_A_Dual_Ascent_CVPR_2017_paper.pdf",
        "aff": "IST Austria; Universit \u00a8at Heidelberg; TU Dresden",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Swoboda_A_Dual_Ascent_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.05460v2",
        "pdf_size": 898652,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4934757673770573945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ist.ac.at; ; ",
        "email": "ist.ac.at; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Swoboda_A_Dual_Ascent_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Institute of Science and Technology Austria;University of Heidelberg;Technische Universit\u00e4t Dresden",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ist.ac.at;https://www.uni-heidelberg.de;https://www.tu-dresden.de",
        "aff_unique_abbr": "IST Austria;Uni Heidelberg;TUD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Austria;Germany"
    },
    {
        "title": "A General Framework for Curve and Surface Comparison and Registration With Oriented Varifolds",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1249",
        "author_site": "Ir\u00c3\u00a8ne Kaltenmark, Benjamin Charlier, Nicolas Charon",
        "author": "Irene Kaltenmark; Benjamin Charlier; Nicolas Charon",
        "abstract": "This paper introduces a general setting for the construction of data fidelity metrics between oriented or non-oriented geometric shapes like curves, curve sets or surfaces. These metrics are based on the representation of shapes as distributions of their local tangent or normal vectors and the definition of reproducing kernels on these spaces. The construction, that combines in one common setting and extends the previous frameworks of currents and varifolds, provides a very large class of kernel metrics which can be easily computed without requiring any kind of parametrization of shapes and which are smooth enough to give robustness to certain imperfections that could result e.g. from bad segmentation. We then give a sense, with synthetic examples, of the versatility and potentialities of such metrics when used in various problems like shape comparison, clustering and diffeomorphic registration.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kaltenmark_A_General_Framework_CVPR_2017_paper.pdf",
        "aff": "CMLA, ENS Cachan, CNRS, Universit \u00b4e Paris-Saclay, 94235, Cachan, France; IMAG, CNRS, Universit \u00b4e de Montpellier; Center of Imaging Sciences, Johns Hopkins University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8094497,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13388504066093674431&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cmla.ens-cachan.fr;umontpellier.fr;cis.jhu.edu",
        "email": "cmla.ens-cachan.fr;umontpellier.fr;cis.jhu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kaltenmark_A_General_Framework_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "\u00c9cole Normale Sup\u00e9rieure de Cachan;Universit\u00e9 de Montpellier;Johns Hopkins University",
        "aff_unique_dep": "CMLA;IMAG;Center of Imaging Sciences",
        "aff_unique_url": "https://www.ens-cachan.fr;https://www.univ-montp2.fr;https://www.jhu.edu",
        "aff_unique_abbr": "ENS Cachan;UM;JHU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cachan;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "France;United States"
    },
    {
        "title": "A Generative Model for Depth-Based Robust 3D Facial Pose Tracking",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1824",
        "author_site": "Lu Sheng, Jianfei Cai, Tat-Jen Cham, Vladimir Pavlovic, King Ngi Ngan",
        "author": "Lu Sheng; Jianfei Cai; Tat-Jen Cham; Vladimir Pavlovic; King Ngi Ngan",
        "abstract": "We consider the problem of depth-based robust 3D facial pose tracking under unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Unlike the previous depth-based discriminative or data-driven methods that require sophisticated training or manual intervention, we propose a generative framework that unifies pose tracking and face model adaptation on-the-fly. Particularly, we propose a statistical 3D face model that owns the flexibility to generate and predict the distribution and uncertainty underlying the face model. Moreover, unlike prior arts employing the ICP-based facial pose estimation, we propose a ray visibility constraint that regularizes the pose based on the face model's visibility against the input point cloud, which augments the robustness against the occlusions. The experimental results on Biwi and ICT-3DHP datasets reveal that the proposed framework is effective and outperforms the state-of-the-art depth-based methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sheng_A_Generative_Model_CVPR_2017_paper.pdf",
        "aff": "The Chinese University of Hong Kong; Nanyang Technological University; Nanyang Technological University; Rutgers University; The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Sheng_A_Generative_Model_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1763432,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6064171629386270611&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ntu.edu.sg;ntu.edu.sg;cs.rutgers.edu",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ntu.edu.sg;ntu.edu.sg;cs.rutgers.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sheng_A_Generative_Model_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;2;0",
        "aff_unique_norm": "Chinese University of Hong Kong;Nanyang Technological University;Rutgers University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.ntu.edu.sg;https://www.rutgers.edu",
        "aff_unique_abbr": "CUHK;NTU;Rutgers",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;1;1;2;0",
        "aff_country_unique": "China;Singapore;United States"
    },
    {
        "title": "A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1684",
        "author_site": "Junho Yim, Donggyu Joo, Jihoon Bae, Junmo Kim",
        "author": "Junho Yim; Donggyu Joo; Jihoon Bae; Junmo Kim",
        "abstract": "We introduce a novel technique for knowledge transfer, where knowledge from a pretrained deep neural network (DNN) is distilled and transferred to another DNN. As the DNN performs a mapping from the input space to the output space through many layers sequentially, we define the distilled knowledge to be transferred in terms of flow between layers, which is calculated by computing the inner product between features from two layers. When we compare the student DNN and the original network with the same size as the student DNN but trained without a teacher network, the proposed method of transferring the distilled knowledge as the flow between two layers exhibits three important phenomena  : (1) the student DNN that learns the distilled knowledge is optimized much faster than the original model; (2) the student DNN outperforms the original DNN; and (3) the student DNN can learn the distilled knowledge from a teacher DNN that is trained at a different task, and the student DNN outperforms the original DNN that is trained from scratch.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf",
        "aff": "School of Electrical Engineering, KAIST, South Korea; School of Electrical Engineering, KAIST, South Korea; Electronics and Telecommunications Research Institute; School of Electrical Engineering, KAIST, South Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 564739,
        "gs_citation": 1972,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8984267799657592702&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;etri.re.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;etri.re.kr;kaist.ac.kr",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yim_A_Gift_From_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "KAIST;Electronics and Telecommunications Research Institute",
        "aff_unique_dep": "School of Electrical Engineering;",
        "aff_unique_url": "https://www.kaist.ac.kr;http://www.etri.re.kr",
        "aff_unique_abbr": "KAIST;ETRI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "A Graph Regularized Deep Neural Network for Unsupervised Image Representation Learning",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "422",
        "author_site": "Shijie Yang, Liang Li, Shuhui Wang, Weigang Zhang, Qingming Huang",
        "author": "Shijie Yang; Liang Li; Shuhui Wang; Weigang Zhang; Qingming Huang",
        "abstract": "Deep Auto-Encoder (DAE) has shown its promising power in high-level representation learning. From the perspective of manifold learning, we propose a graph regularized deep neural network (GR-DNN) to endue traditional DAEs with the ability of retaining local geometric structure. A deep-structured regularizer is formulated upon multi-layer perceptions to capture this structure. The robust and discriminative embedding space is learned to simultaneously preserve the high-level semantics and the geometric structure within local manifold tangent space. Theoretical analysis presents the close relationship between the proposed graph regularizer and the graph Laplacian regularizer in terms of the optimization objective. We also alleviate the growth of the network complexity by introducing the anchor-based bipartite graph, which guarantees the good scalability for large scale data. The experiments on four datasets show the comparable results of the proposed GR-DNN with the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_A_Graph_Regularized_CVPR_2017_paper.pdf",
        "aff": "University of Chinese Academy of Sciences; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS; University of Chinese Academy of Sciences, School of Computer Science and Technology, Harbin Institute of Technology, Weihai; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, University of Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3737811,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7036908309198343976&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "vipl.ict.ac.cn;vipl.ict.ac.cn;ict.ac.cn;hit.edu.cn;ucas.ac.cn",
        "email": "vipl.ict.ac.cn;vipl.ict.ac.cn;ict.ac.cn;hit.edu.cn;ucas.ac.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_A_Graph_Regularized_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "University of Chinese Academy of Sciences;Chinese Academy of Sciences",
        "aff_unique_dep": ";Institute of Computing Technology",
        "aff_unique_url": "http://www.ucas.ac.cn;http://www.cas.cn",
        "aff_unique_abbr": "UCAS;CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs",
        "session": "Analyzing Humans 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "120",
        "author_site": "Jonathan Krause, Justin Johnson, Ranjay Krishna, Li Fei-Fei",
        "author": "Jonathan Krause; Justin Johnson; Ranjay Krishna; Li Fei-Fei",
        "abstract": "Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Krause_A_Hierarchical_Approach_CVPR_2017_paper.pdf",
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.06607v2",
        "pdf_size": 779781,
        "gs_citation": 487,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8210833166938957629&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Krause_A_Hierarchical_Approach_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Joint Speaker-Listener-Reinforcer Model for Referring Expressions",
        "session": "Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "3493",
        "author_site": "Licheng Yu, Hao Tan, Mohit Bansal, Tamara L. Berg",
        "author": "Licheng Yu; Hao Tan; Mohit Bansal; Tamara L. Berg",
        "abstract": "Referring expressions are natural language constructions used to identify particular objects within a scene. In this paper, we propose a unified framework for the tasks of referring expression comprehension and generation. Our model is composed of three modules: speaker, listener, and reinforcer. The speaker generates referring expressions, the listener comprehends referring expressions, and the reinforcer introduces a reward function to guide sampling of more discriminative expressions. The listener-speaker modules are trained jointly in an end-to-end learning framework, allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer's feedback. We demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets. Project and demo page: https://vision.cs.unc.edu/refer",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_A_Joint_Speaker-Listener-Reinforcer_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, University of North Carolina at Chapel Hill; Department of Computer Science, University of North Carolina at Chapel Hill; Department of Computer Science, University of North Carolina at Chapel Hill; Department of Computer Science, University of North Carolina at Chapel Hill",
        "project": "https://vision.cs.unc.edu/refer",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yu_A_Joint_Speaker-Listener-Reinforcer_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.09542v2",
        "pdf_size": 1603141,
        "gs_citation": 317,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13038961313540658511&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_A_Joint_Speaker-Listener-Reinforcer_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC Chapel Hill",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Linear Extrinsic Calibration of Kaleidoscopic Imaging System From Single 3D Point",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "179",
        "author_site": "Kosuke Takahashi, Akihiro Miyata, Shohei Nobuhara, Takashi Matsuyama",
        "author": "Kosuke Takahashi; Akihiro Miyata; Shohei Nobuhara; Takashi Matsuyama",
        "abstract": "This paper proposes a new extrinsic calibration of kaleidoscopic imaging system by estimating normals and distances of the mirrors. The problem to be solved in this paper is a simultaneous estimation of all mirror parameters consistent throughout multiple reflections. Unlike conventional methods utilizing a pair of direct and mirrored images of a reference 3D object to estimate the parameters on a per-mirror basis, our method renders the simultaneous estimation problem into solving a linear set of equations. The key contribution of this paper is to introduce a linear estimation of multiple mirror parameters from kaleidoscopic 2D projections of a single 3D point of unknown geometry. Evaluations with synthesized and real images demonstrate the performance of the proposed algorithm in comparison with conventional methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Takahashi_A_Linear_Extrinsic_CVPR_2017_paper.pdf",
        "aff": "Kyoto University; Kyoto University + Nara Institute of Science and Technology; Kyoto University; Kyoto University",
        "project": "http://vision.kuee.kyoto-u.ac.jp/~nob/proj/kaleidoscope/",
        "github": "",
        "supp": "",
        "arxiv": "1703.02826v3",
        "pdf_size": 1484318,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1374430744005696785&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "vision.kuee.kyoto-u.ac.jp;vision.kuee.kyoto-u.ac.jp;vision.kuee.kyoto-u.ac.jp;vision.kuee.kyoto-u.ac.jp",
        "email": "vision.kuee.kyoto-u.ac.jp;vision.kuee.kyoto-u.ac.jp;vision.kuee.kyoto-u.ac.jp;vision.kuee.kyoto-u.ac.jp",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Takahashi_A_Linear_Extrinsic_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Kyoto University;Nara Institute of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kyoto-u.ac.jp;https://www.nist.go.jp",
        "aff_unique_abbr": "Kyoto U;NIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "A Low Power, Fully Event-Based Gesture Recognition System",
        "session": "Theory",
        "status": "Poster",
        "track": "main",
        "pid": "3466",
        "author_site": "Arnon Amir, Brian Taba, David Berg, Timothy Melano, Jeffrey McKinstry, Carmelo Di Nolfo, Tapan Nayak, Alexander Andreopoulos, Guillaume Garreau, Marcela Mendoza, Jeff Kusnitz, Michael Debole, Steve Esser, Tobi Delbruck, Myron Flickner, Dharmendra Modha",
        "author": "Arnon Amir; Brian Taba; David Berg; Timothy Melano; Jeffrey McKinstry; Carmelo Di Nolfo; Tapan Nayak; Alexander Andreopoulos; Guillaume Garreau; Marcela Mendoza; Jeff Kusnitz; Michael Debole; Steve Esser; Tobi Delbruck; Myron Flickner; Dharmendra Modha",
        "abstract": "We present the first gesture recognition system implemented end-to-end on event-based hardware, using a TrueNorth neurosynaptic processor to recognize hand gestures in real-time at low power from events streamed live by a Dynamic Vision Sensor (DVS). The biologically inspired DVS transmits data only when a pixel detects a change, unlike traditional frame-based cameras which sample every pixel at a fixed frame rate. This sparse, asynchronous data representation lets event-based cameras operate at much lower power than frame-based cameras. However, much of the energy efficiency is lost if, as in previous work, the event stream is interpreted by conventional synchronous processors. Here, for the first time, we process a live DVS event stream using TrueNorth, a natively event-based processor with 1 million spiking neurons. Configured here as a convolutional neural network (CNN), the TrueNorth chip identifies the onset of a gesture with a latency of 105 ms while consuming less than 200 mW. The CNN achieves 96.5% out-of-sample accuracy on a newly collected DVS dataset (DvsGesture) comprising 11 hand gesture categories from 29 subjects under 3 illumination conditions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Amir_A_Low_Power_CVPR_2017_paper.pdf",
        "aff": "IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research\u2020UC San Diego; IBM Research; IBM Research; IBM Research; IBM Research\u2021UZH-ETH Zurich&iniLabs GmbH; IBM Research; IBM Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1909528,
        "gs_citation": 1129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8685452647836541303&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "us.ibm.com; ; ; ; ; ; ; ; ;ucsd.edu; ; ; ;ini.uzh.ch; ;us.ibm.com",
        "email": "us.ibm.com; ; ; ; ; ; ; ; ;ucsd.edu; ; ; ;ini.uzh.ch; ;us.ibm.com",
        "author_num": 16,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Amir_A_Low_Power_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Matrix Splitting Method for Composite Function Minimization",
        "session": "Theory",
        "status": "Poster",
        "track": "main",
        "pid": "2016",
        "author_site": "Ganzhao Yuan, Wei-Shi Zheng, Bernard Ghanem",
        "author": "Ganzhao Yuan; Wei-Shi Zheng; Bernard Ghanem",
        "abstract": "Composite function minimization captures a wide spectrum of applications in both computer vision and machine learning. It includes bound constrained optimization and cardinality regularized optimization as special cases. This paper proposes and analyzes a new Matrix Splitting Method (MSM) for minimizing composite functions. It can be viewed as a generalization of the classical Gauss-Seidel method and the Successive Over-Relaxation method for solving linear systems in the literature. Incorporating a new Gaussian elimination procedure, the matrix splitting method achieves state-of-the-art performance. For convex problems, we establish the global convergence, convergence rate, and iteration complexity of MSM, while for non-convex problems, we prove its global convergence. Finally, we validate the performance of our matrix splitting method on two particular applications: nonnegative matrix factorization and cardinality regularized sparse coding. Extensive experiments show that our method outperforms existing composite function minimization techniques in term of both efficiency and efficacy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yuan_A_Matrix_Splitting_CVPR_2017_paper.pdf",
        "aff": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia + School of Data and Computer Science, Sun Yat-sen University (SYSU), China; School of Data and Computer Science, Sun Yat-sen University (SYSU), China + Key Laboratory of Machine Intelligence and Advanced Computing (Sun Yat-sen University), Ministry of Education, China; King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.02317v1",
        "pdf_size": 1926326,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4567242343167790195&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "gmail.com;ieee.org;kaust.edu.sa",
        "email": "gmail.com;ieee.org;kaust.edu.sa",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yuan_A_Matrix_Splitting_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1+1;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology;Sun Yat-sen University",
        "aff_unique_dep": ";School of Data and Computer Science",
        "aff_unique_url": "https://www.kaust.edu.sa;http://www.sysu.edu.cn/",
        "aff_unique_abbr": "KAUST;SYSU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1+1;0",
        "aff_country_unique": "Saudi Arabia;China"
    },
    {
        "title": "A Message Passing Algorithm for the Minimum Cost Multicut Problem",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "588",
        "author_site": "Paul Swoboda, Bjoern Andres",
        "author": "Paul Swoboda; Bjoern Andres",
        "abstract": "We propose a dual decomposition and linear program relaxation of the NP-hard minimum cost multicut problem. Unlike other polyhedral relaxations of the multicut polytope, it is amenable to efficient optimization by message passing. Like other polyhedral relaxations, it can be tightened efficiently by cutting planes. We define an algorithm that alternates between message passing and efficient separation of cycle- and odd-wheel inequalities. This algorithm is more efficient than state-of-the-art algorithms based on linear programming, including algorithms written in the framework of leading commercial software, as we show in experiments with large instances of the problem from applications in computer vision, biomedical image analysis and data mining.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Swoboda_A_Message_Passing_CVPR_2017_paper.pdf",
        "aff": "IST Austria; MPI for Informatics",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Swoboda_A_Message_Passing_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.05441v2",
        "pdf_size": 883264,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12834913943050138014&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ist.ac.at;mpi-inf.mpg.de",
        "email": "ist.ac.at;mpi-inf.mpg.de",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Swoboda_A_Message_Passing_CVPR_2017_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Institute of Science and Technology Austria;Max Planck Institute for Informatics",
        "aff_unique_dep": ";Informatics",
        "aff_unique_url": "https://www.ist.ac.at;https://www.mpi-inf.mpg.de",
        "aff_unique_abbr": "IST Austria;MPII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Austria;Germany"
    },
    {
        "title": "A Minimal Solution for Two-View Focal-Length Estimation Using Two Affine Correspondences",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2619",
        "author_site": "Daniel Barath, Tekla Toth, Levente Hajder",
        "author": "Daniel Barath; Tekla Toth; Levente Hajder",
        "abstract": "A minimal solution using two affine correspondences is presented to estimate the common focal length and the fundamental matrix between two semi-calibrated cameras - known intrinsic parameters except a common focal length. To the best of our knowledge, this problem is unsolved. The proposed approach extends point correspondence-based techniques with linear constraints derived from local affine transformations. The obtained multivariate polynomial system is efficiently solved by the hidden-variable technique. Observing the geometry of local affinities, we introduce novel conditions eliminating invalid roots. To select the best one out of the remaining candidates, a root selection technique is proposed outperforming the recent ones especially in case of high-level noise. The proposed 2-point algorithm is validated on both synthetic data and 104 publicly available real image pairs. A Matlab implementation of the proposed solution is included in the paper.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Barath_A_Minimal_Solution_CVPR_2017_paper.pdf",
        "aff": "Machine Perception Research Laboratory, MTA SZTAKI, Budapest, Hungary; Machine Perception Research Laboratory, MTA SZTAKI, Budapest, Hungary; Machine Perception Research Laboratory, MTA SZTAKI, Budapest, Hungary",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1706.01649v1",
        "pdf_size": 951395,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12629841459861883093&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "sztaki.mta.hu; ;sztaki.mta.hu",
        "email": "sztaki.mta.hu; ;sztaki.mta.hu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Barath_A_Minimal_Solution_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "MTA SZTAKI",
        "aff_unique_dep": "Machine Perception Research Laboratory",
        "aff_unique_url": "https://www.sztaki.hu",
        "aff_unique_abbr": "SZTAKI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Budapest",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Hungary"
    },
    {
        "title": "A Multi-View Stereo Benchmark With High-Resolution Images and Multi-Camera Videos",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1213",
        "author_site": "Thomas Sch\u00c3\u00b6ps, Johannes L. Sch\u00c3\u00b6nberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, Andreas Geiger",
        "author": "Thomas Schops; Johannes L. Schonberger; Silvano Galliani; Torsten Sattler; Konrad Schindler; Marc Pollefeys; Andreas Geiger",
        "abstract": "Motivated by the limitations of existing multi-view stereo benchmarks, we present a novel dataset for this task. Towards this goal, we recorded a variety of indoor and outdoor scenes using a high-precision laser scanner and captured both high-resolution DSLR imagery as well as synchronized low-resolution stereo videos with varying fields-of-view. To align the images with the laser scans, we propose a robust technique which minimizes photometric errors conditioned on the geometry. In contrast to previous datasets, our benchmark provides novel challenges and covers a diverse set of viewpoints and scene types, ranging from natural scenes to man-made indoor and outdoor environments. Furthermore, we provide data at significantly higher temporal and spatial resolution. Our benchmark is the first to cover the important use case of hand-held mobile devices while also providing high-resolution DSLR camera images. We make our datasets and an online evaluation server available at http://www.eth3d.net.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Schops_A_Multi-View_Stereo_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Schops_A_Multi-View_Stereo_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2158359,
        "gs_citation": 1009,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6691735026003180807&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Schops_A_Multi-View_Stereo_CVPR_2017_paper.html"
    },
    {
        "title": "A New Rank Constraint on Multi-View Fundamental Matrices, and Its Application to Camera Location Recovery",
        "session": "3D Vision 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "1992",
        "author_site": "Soumyadip Sengupta, Tal Amir, Meirav Galun, Tom Goldstein, David W. Jacobs, Amit Singer, Ronen Basri",
        "author": "Soumyadip Sengupta; Tal Amir; Meirav Galun; Tom Goldstein; David W. Jacobs; Amit Singer; Ronen Basri",
        "abstract": "Accurate estimation of camera matrices is an important step in structure from motion algorithms. In this paper we introduce a novel rank constraint on collections of fundamental matrices in multi-view settings. We show that in general, with the selection of proper scale factors, a matrix formed by stacking fundamental matrices between pairs of images has rank 6. Moreover, this matrix forms the symmetric part of a rank 3 matrix whose factors relate directly to the corresponding camera matrices. We use this new characterization to produce better estimations of fundamental matrices by optimizing an L1-cost function using Iterative Re-weighted Least Squares and Alternate Direction Method of Multiplier. We further show that this procedure can improve the recovery of camera locations, particularly in multi-view settings in which fewer images are available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sengupta_A_New_Rank_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Sengupta_A_New_Rank_2017_CVPR_supplemental.pdf",
        "arxiv": "1702.03023v1",
        "pdf_size": 696703,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17514365754478872808&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sengupta_A_New_Rank_CVPR_2017_paper.html"
    },
    {
        "title": "A New Representation of Skeleton Sequences for 3D Action Recognition",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1226",
        "author_site": "Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, Farid Boussaid",
        "author": "Qiuhong Ke; Mohammed Bennamoun; Senjian An; Ferdous Sohel; Farid Boussaid",
        "abstract": "This paper presents a new method for 3D action recognition with skeleton sequences (i.e., 3D trajectories of human skeleton joints). The proposed method first transforms each skeleton sequence into three clips each consisting of several frames for spatial temporal feature learning using deep neural networks. Each clip is generated from one channel of the cylindrical coordinates of the skeleton sequence. Each frame of the generated clips represents the temporal information of the entire skeleton sequence, and incorporates one particular spatial relationship between the joints. The entire clips include multiple frames with different spatial relationships, which provide useful spatial structural information of the human skeleton. We propose to use deep convolutional neural networks to learn long-term temporal information of the skeleton sequence from the frames of the generated clips, and then use a Multi-Task Learning Network (MTLN) to jointly process all frames of the clips in parallel to incorporate spatial structural information for action recognition. Experimental results clearly show the effectiveness of the proposed new representation and feature learning method for 3D action recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ke_A_New_Representation_CVPR_2017_paper.pdf",
        "aff": "The University of Western Australia; The University of Western Australia; The University of Western Australia; Murdoch University; The University of Western Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.03492v3",
        "pdf_size": 724513,
        "gs_citation": 1080,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5642923214882762901&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "research.uwa.edu.au;uwa.edu.au;uwa.edu.au;murdoch.edu.au;uwa.edu.au",
        "email": "research.uwa.edu.au;uwa.edu.au;uwa.edu.au;murdoch.edu.au;uwa.edu.au",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ke_A_New_Representation_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Western Australia;Murdoch University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uwa.edu.au;https://www.murdoch.edu.au",
        "aff_unique_abbr": "UWA;MU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "A Non-Convex Variational Approach to Photometric Stereo Under Inaccurate Lighting",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "39",
        "author_site": "Yvain Qu\u00c3\u00a9au, Tao Wu, Fran\u00c3\u00a7ois Lauze, Jean-Denis Durou, Daniel Cremers",
        "author": "Yvain Queau; Tao Wu; Francois Lauze; Jean-Denis Durou; Daniel Cremers",
        "abstract": "This paper tackles the photometric stereo problem in the presence of inaccurate lighting, obtained either by calibration or by an uncalibrated photometric stereo method. Based on a precise modeling of noise and outliers, a robust variational approach is introduced. It explicitly accounts for self-shadows, and enforces robustness to cast-shadows and specularities by resorting to redescending M-estimators. The resulting non-convex model is solved by means of a computationally efficient alternating reweighted least-squares algorithm. Since it implicitly enforces integrability, the new variational approach can refine both the intensities and the directions of the lighting.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Queau_A_Non-Convex_Variational_CVPR_2017_paper.pdf",
        "aff": "Technical University of Munich, Germany; Technical University of Munich, Germany; University of Copenhagen, Denmark; University of Toulouse, France; Technical University of Munich, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1665187,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13287985737963823076&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "tum.de;tum.de;di.ku.dk;irit.fr;tum.de",
        "email": "tum.de;tum.de;di.ku.dk;irit.fr;tum.de",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Queau_A_Non-Convex_Variational_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Technical University of Munich;University of Copenhagen;University of Toulouse",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tum.de;https://www.ku.dk;https://www.univ-toulouse.fr",
        "aff_unique_abbr": "TUM;UCPH;UT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2;0",
        "aff_country_unique": "Germany;Denmark;France"
    },
    {
        "title": "A Non-Local Low-Rank Framework for Ultrasound Speckle Reduction",
        "session": "Biomedical Image/Video Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "2395",
        "author_site": "Lei Zhu, Chi-Wing Fu, Michael S. Brown, Pheng-Ann Heng",
        "author": "Lei Zhu; Chi-Wing Fu; Michael S. Brown; Pheng-Ann Heng",
        "abstract": "`Speckle' refers to the granular patterns that occur in ultrasound images due to wave interference. Speckle removal can greatly improve the visibility of the underlying structures in an ultrasound image and enhance subsequent post processing.  We present a novel framework for speckle removal based on low-rank non-local filtering. Our approach works by first computing a guidance image that assists in the selection of candidate patches for non-local filtering in the face of significant speckles.  The candidate patches are further refined using a low-rank minimization estimated using a truncated weighted nuclear norm (TWNN) and structured sparsity.  We show that the proposed filtering framework produces results that outperform state-of-the-art methods both qualitatively and quantitatively.  This framework also provides better segmentation results when used for pre-processing ultrasound images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_A_Non-Local_Low-Rank_CVPR_2017_paper.pdf",
        "aff": "The Chinese University of Hong Kong; The Chinese University of Hong Kong + York University + Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China; York University; The Chinese University of Hong Kong + Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhu_A_Non-Local_Low-Rank_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2310483,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4635936643835433110&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;eecs.yorku.ca;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;eecs.yorku.ca;cse.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhu_A_Non-Local_Low-Rank_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1+2;1;0+2",
        "aff_unique_norm": "Chinese University of Hong Kong;York University;Chinese Academy of Sciences",
        "aff_unique_dep": ";;Shenzhen Key Laboratory of Virtual Reality and Human Interaction Technology",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.yorku.ca;http://www.cas.cn",
        "aff_unique_abbr": "CUHK;York U;CAS",
        "aff_campus_unique_index": "0;0+2;0+2",
        "aff_campus_unique": "Hong Kong SAR;;Shenzhen",
        "aff_country_unique_index": "0;0+1+0;1;0+0",
        "aff_country_unique": "China;Canada"
    },
    {
        "title": "A Novel Tensor-Based Video Rain Streaks Removal Approach via Utilizing Discriminatively Intrinsic Priors",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1636",
        "author_site": "Tai-Xiang Jiang, Ting-Zhu Huang, Xi-Le Zhao, Liang-Jian Deng, Yao Wang",
        "author": "Tai-Xiang Jiang; Ting-Zhu Huang; Xi-Le Zhao; Liang-Jian Deng; Yao Wang",
        "abstract": "Rain streaks removal is an important issue of the outdoor vision system and has been recently investigated extensively. In this paper, we propose a novel tensor based video rain streaks removal approach by fully considering the discriminatively intrinsic characteristics of rain streaks and clean videos, which needs neither rain detection nor time-consuming dictionary learning stage. In specific, on the one hand, rain streaks are sparse and smooth along the raindrops' direction, and on the other hand, the clean videos possess smoothness along the rain-perpendicular direction and global and local correlation along time direction. We use the l_1 norm to enhance the sparsity of the underlying rain, two unidirectional Total Variation (TV) regularizers to guarantee the different discriminative smoothness, and a tensor nuclear norm and a time directional difference operator to characterize the exclusive correlation of the clean video along time. Alternation direction method of multipliers (ADMM) is employed to solve the proposed concise tensor based convex model. Experiments implemented on synthetic and real data substantiate the effectiveness and efficiency of the proposed method. Under comprehensive quantitative performance measures, our approach outperforms other state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jiang_A_Novel_Tensor-Based_CVPR_2017_paper.pdf",
        "aff": "School of Mathematical Sciences, University of Electronic Science and Technology of China; School of Mathematical Sciences, University of Electronic Science and Technology of China; School of Mathematical Sciences, University of Electronic Science and Technology of China; School of Mathematical Sciences, University of Electronic Science and Technology of China; School of Mathematics and Statistics, Xian Jiaotong University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Jiang_A_Novel_Tensor-Based_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1785847,
        "gs_citation": 194,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9512497369803463016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gmail.com;126.com;163.com;126.com;gmail.com",
        "email": "gmail.com;126.com;163.com;126.com;gmail.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jiang_A_Novel_Tensor-Based_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "University of Electronic Science and Technology of China;Xian Jiao Tong University",
        "aff_unique_dep": "School of Mathematical Sciences;School of Mathematics and Statistics",
        "aff_unique_url": "https://www.uestc.edu.cn;http://en.xjtu.edu.cn/",
        "aff_unique_abbr": "UESTC;XJTU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Xian",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Point Set Generation Network for 3D Object Reconstruction From a Single Image",
        "session": "3D Vision 2",
        "status": "Oral",
        "track": "main",
        "pid": "190",
        "author_site": "Haoqiang Fan, Hao Su, Leonidas J. Guibas",
        "author": "Haoqiang Fan; Hao Su; Leonidas J. Guibas",
        "abstract": "Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output -- point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthordox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3D reconstruction benchmarks; but it also shows strong performance for 3D shape completion and promising ability in making multiple plausible predictions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Fan_A_Point_Set_CVPR_2017_paper.pdf",
        "aff": "Institute for Interdisciplinary Information Sciences, Tsinghua University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Fan_A_Point_Set_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.00603",
        "pdf_size": 1684895,
        "gs_citation": 2836,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2714970910185586914&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff_domain": "gmail.com;cs.stanford.edu;cs.stanford.edu",
        "email": "gmail.com;cs.stanford.edu;cs.stanford.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Fan_A_Point_Set_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Tsinghua University;Stanford University",
        "aff_unique_dep": "Institute for Interdisciplinary Information Sciences;Computer Science Department",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.stanford.edu",
        "aff_unique_abbr": "Tsinghua;Stanford",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "A Practical Method for Fully Automatic Intrinsic Camera Calibration Using Directionally Encoded Light",
        "session": "3D Vision 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "380",
        "author_site": "Mahdi Abbaspour Tehrani, Thabo Beeler, Anselm Grundh\u00c3\u00b6fer",
        "author": "Mahdi Abbaspour Tehrani; Thabo Beeler; Anselm Grundhofer",
        "abstract": "Calibrating the intrinsic properties of a camera is one of the fundamental tasks required for a variety of computer vision and image processing tasks. The precise measurement of focal length, location of the principal point as well as distortion parameters of the lens is crucial, for example, for 3D reconstruction. Although a variety of methods exist to achieve this goal, they are often cumbersome to carry out, require substantial manual interaction, expert knowledge, and a significant operating volume. We propose a novel calibration method based on the usage of directionally encoded light rays for estimating the intrinsic parameters. It enables a fully automatic calibration with a small device mounted close to the front lens element and still enables an accuracy comparable to standard methods even when the lens is focused up to infinity.  Our method overcomes the mentioned limitations since it guarantees an accurate calibration without any human intervention while requiring only a limited amount of space. Besides that, the approach also allows to estimate the distance of the focal plane as well as the size of the aperture. We demonstrate the advantages of the proposed method by evaluating several camera/lens configurations using prototypical devices.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tehrani_A_Practical_Method_CVPR_2017_paper.pdf",
        "aff": "University of California Irvine; Disney Research; Disney Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Tehrani_A_Practical_Method_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1097023,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6958145679585533938&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "ics.uci.edu;disneyresearch.com;disneyresearch.com",
        "email": "ics.uci.edu;disneyresearch.com;disneyresearch.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tehrani_A_Practical_Method_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of California, Irvine;Disney Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uci.edu;https://research.disney.com",
        "aff_unique_abbr": "UCI;Disney Research",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Reinforcement Learning Approach to the View Planning Problem",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3201",
        "author_site": "Mustafa Devrim Kaba, Mustafa Gokhan Uzunbas, Ser Nam Lim",
        "author": "Mustafa Devrim Kaba; Mustafa Gokhan Uzunbas; Ser Nam Lim",
        "abstract": "We present a Reinforcement Learning (RL) solution to the view planning problem (VPP), which generates a sequence of view points that are capable of sensing all accessible area of a given object represented as a 3D model. In doing so, the goal is to minimize the number of view points, making the VPP a class of set covering optimization problem (SCOP). The SCOP is NP-hard, and the inapproximability results tell us that the greedy algorithm provides the best approximation that runs in polynomial time. In order to find a solution that is better than the greedy algorithm, (i) we introduce a novel score function by exploiting the geometry of the 3D model, (ii) we device an intuitive approach to VPP using this score function, and (iii) we cast VPP as a Markovian Decision Process (MDP), and solve the MDP in RL framework using well-known RL algorithms. In particular, we use SARSA, Watkins-Q and TD with function approximation to solve the MDP. We compare the results of our method with the baseline greedy algorithm in an extensive set of test objects, and show that we can outperform the baseline in almost all cases.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kaba_A_Reinforcement_Learning_CVPR_2017_paper.pdf",
        "aff": "General Electric Global Research Center, 1 Research Circle, Niskayuna, NY 12309; General Electric Global Research Center, 1 Research Circle, Niskayuna, NY 12309; General Electric Global Research Center, 1 Research Circle, Niskayuna, NY 12309",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kaba_A_Reinforcement_Learning_2017_CVPR_supplemental.pdf",
        "arxiv": "1610.06204",
        "pdf_size": 1854076,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17069305410914593358&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ge.com;ge.com;ge.com",
        "email": "ge.com;ge.com;ge.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kaba_A_Reinforcement_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "General Electric",
        "aff_unique_dep": "Global Research Center",
        "aff_unique_url": "https://www.ge.com/research",
        "aff_unique_abbr": "GE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Study of Lagrangean Decompositions and Dual Ascent Solvers for Graph Matching",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "587",
        "author_site": "Paul Swoboda, Carsten Rother, Hassan Abu Alhaija, Dagmar Kainm\u00c3\u00bcller, Bogdan Savchynskyy",
        "author": "Paul Swoboda; Carsten Rother; Hassan Abu Alhaija; Dagmar Kainmuller; Bogdan Savchynskyy",
        "abstract": "We study the quadratic assignment problem, in computer vision also known as graph matching. Two leading solvers for this problem optimize the Lagrange decomposition duals with sub-gradient and dual ascent (also known as message passing) updates. We explore this direction further and propose several additional Lagrangean relaxations of the graph matching problem along with corresponding algorithms, which are all based on a common dual ascent framework. Our extensive empirical evaluation gives several theoretical insights and suggests a new state-of-the-art anytime solver for the considered problem. Our improvement over state-of-the-art is particularly visible on a new dataset with large-scale sparse problem instances containing more than 500 graph nodes each.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Swoboda_A_Study_of_CVPR_2017_paper.pdf",
        "aff": "IST Austria; TU Dresden; TU Dresden; MPI-CBG; TU Dresden",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Swoboda_A_Study_of_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.05476",
        "pdf_size": 944332,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1552901428112439911&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "ist.ac.at; ; ; ; ",
        "email": "ist.ac.at; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Swoboda_A_Study_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "Institute of Science and Technology Austria;Technische Universit\u00e4t Dresden;Max Planck Institute of Molecular Cell Biology and Genetics",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ist.ac.at;https://www.tu-dresden.de;https://www.mpi-cbg.de",
        "aff_unique_abbr": "IST Austria;TUD;MPI-CBG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "Austria;Germany"
    },
    {
        "title": "A Unified Approach of Multi-Scale Deep and Hand-Crafted Features for Defocus Estimation",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "645",
        "author_site": "Jinsun Park, Yu-Wing Tai, Donghyeon Cho, In So Kweon",
        "author": "Jinsun Park; Yu-Wing Tai; Donghyeon Cho; In So Kweon",
        "abstract": "In this paper, we introduce robust and synergetic hand-crafted features and a simple but efficient deep feature from a convolutional neural network (CNN) architecture for defocus estimation. This paper systematically analyzes the effectiveness of different features, and shows how each feature can compensate for the weaknesses of other features when they are concatenated. For a full defocus map estimation, we extract image patches on strong edges sparsely, after which we use them for deep and hand-crafted feature extraction. In order to reduce the degree of patch-scale dependency, we also propose a multi-scale patch extraction strategy. A sparse defocus map is generated using a neural network classifier followed by a probability-joint bilateral filter. The final defocus map is obtained from the sparse defocus map with guidance from an edge-preserving filtered input image. Experimental results show that our algorithm is superior to state-of-the-art algorithms in terms of defocus estimation. Our work can be used for applications such as segmentation, blur magnification, all-in-focus image generation, and 3-D estimation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Park_A_Unified_Approach_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.08992v1",
        "gs_citation": 152,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1969076149043964263&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Park_A_Unified_Approach_CVPR_2017_paper.html"
    },
    {
        "title": "A Wide-Field-Of-View Monocentric Light Field Camera",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "2082",
        "author_site": "Donald G. Dansereau, Glenn Schuster, Joseph Ford, Gordon Wetzstein",
        "author": "Donald G. Dansereau; Glenn Schuster; Joseph Ford; Gordon Wetzstein",
        "abstract": "Light field (LF) capture and processing are important in an expanding range of computer vision applications, offering rich textural and depth information and simplification of conventionally complex tasks. Although LF cameras are commercially available, no existing device offers wide field-of-view (FOV) imaging. This is due in part to the limitations of fisheye lenses, for which a fundamentally constrained entrance pupil diameter severely limits depth sensitivity. In this work we describe a novel, compact optical design that couples a monocentric lens with multiple sensors using microlens arrays, allowing LF capture with an unprecedented FOV. Leveraging capabilities of the LF representation, we propose a novel method for efficiently coupling the spherical lens and planar sensors, replacing expensive and bulky fiber bundles. We construct a single-sensor LF camera prototype, rotating the sensor relative to a fixed main lens to emulate a wide-FOV multi-sensor scenario. Finally, we describe a processing toolchain, including a convenient spherical LF parameterization, and demonstrate depth estimation and post-capture refocus for indoor and outdoor panoramas with 15 x 15 x 1600 x 200 pixels (72 MPix) and a 138-degree FOV.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dansereau_A_Wide-Field-Of-View_Monocentric_CVPR_2017_paper.pdf",
        "aff": "Stanford University, Department of Electrical Engineering; University of California San Diego, Department of Electrical & Computer Engineering; University of California San Diego, Department of Electrical & Computer Engineering; Stanford University, Department of Electrical Engineering",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Dansereau_A_Wide-Field-Of-View_Monocentric_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1514865,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11392736357693496430&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;ucsd.edu;ucsd.edu;stanford.edu",
        "email": "gmail.com;ucsd.edu;ucsd.edu;stanford.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dansereau_A_Wide-Field-Of-View_Monocentric_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Stanford University;University of California, San Diego",
        "aff_unique_dep": "Department of Electrical Engineering;Department of Electrical & Computer Engineering",
        "aff_unique_url": "https://www.stanford.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "Stanford;UCSD",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Stanford;San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "946",
        "author_site": "Xiaolong Wang, Abhinav Shrivastava, Abhinav Gupta",
        "author": "Xiaolong Wang; Abhinav Shrivastava; Abhinav Gupta",
        "abstract": "How do we learn an object detector that is invariant to occlusions and deformations? Our current solution is to use a data-driven strategy -- collect large-scale datasets which have object instances under different conditions. The hope is that the final classifier can use these examples to learn invariances. But is it really possible to see all the occlusions in a dataset? We argue that like categories, occlusions and object deformations also follow a long-tail. Some occlusions and deformations are so rare that they  hardly happen; yet we want to learn a model invariant to such occurrences. In this paper, we propose an alternative solution. We propose to learn an adversarial network that generates examples with occlusions and deformations. The goal of the adversary is to generate examples that are difficult for the object detector to classify. In our framework both the original detector and adversary are learned in a  joint manner. Our experimental results indicate a 2.3% mAP boost on VOC07 and a 2.6% mAP boost on VOC2012 object detection challenge compared to the Fast-RCNN pipeline.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_A-Fast-RCNN_Hard_Positive_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 802,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16482036712739720535&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_A-Fast-RCNN_Hard_Positive_CVPR_2017_paper.html"
    },
    {
        "title": "A-Lamp: Adaptive Layout-Aware Multi-Patch Deep Convolutional Neural Network for Photo Aesthetic Assessment",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1857",
        "author_site": "Shuang Ma, Jing Liu, Chang Wen Chen",
        "author": "Shuang Ma; Jing Liu; Chang Wen Chen",
        "abstract": "Deep convolutional neural networks (CNN) have recently been shown to generate promising results for aesthetics assessment. However, the performance of these deep CNN methods is often compromised by the constraint that the neural network only takes the fixed-size input. To accommodate this requirement, input images need to be transformed via cropping, warping, or padding, which often alter image composition, reduce image resolution, or cause image distortion. Thus the aesthetics of the original images is impaired because of potential loss of fine grained details and holistic image layout. However, such fine grained details and holistic image layout is critical for evaluating an image's aesthetics. In this paper, we present an Adaptive Layout-Aware Multi-Patch Convolutional Neural Network (A-Lamp CNN) architecture for photo aesthetic assessment. This novel scheme is able to accept arbitrary sized images, and learn from both fined grained details and holistic image layout simultaneously. To enable training on these hybrid inputs, we extend the method by developing a dedicated double-subnet neural network structure, i.e. a Multi-Patch subnet and a Layout-Aware subnet. We further construct an aggregation layer to effectively combine the hybrid features from these two subnets. Extensive experiments on the large-scale aesthetics assessment benchmark (AVA) demonstrate significant performance improvement over the state-of-the-art in photo aesthetic assessment.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ma_A-Lamp_Adaptive_Layout-Aware_CVPR_2017_paper.pdf",
        "aff": "Computer Science and Engineering, SUNY at Buffalo; Electrical and Information Engineering, Tianjin University; Computer Science and Engineering, SUNY at Buffalo",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.00248",
        "pdf_size": 1973113,
        "gs_citation": 260,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1586987804498069800&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "buffalo.edu;tju.edu.cn;buffalo.edu",
        "email": "buffalo.edu;tju.edu.cn;buffalo.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ma_A-Lamp_Adaptive_Layout-Aware_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "State University of New York at Buffalo;Tianjin University",
        "aff_unique_dep": "Computer Science and Engineering;Electrical and Information Engineering",
        "aff_unique_url": "https://www.buffalo.edu;http://www.tju.edu.cn",
        "aff_unique_abbr": "SUNY Buffalo;Tianjin U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Buffalo;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "AGA: Attribute-Guided Augmentation",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Oral",
        "track": "main",
        "pid": "3856",
        "author_site": "Mandar Dixit, Roland Kwitt, Marc Niethammer, Nuno Vasconcelos",
        "author": "Mandar Dixit; Roland Kwitt; Marc Niethammer; Nuno Vasconcelos",
        "abstract": "We consider the problem of data augmentation, i.e., generating artificial samples to extend a given corpus of training data. Specifically, we propose attributed-guided augmentation (AGA) which learns a mapping that allows to synthesize data such that an attribute of a synthesized sample is at a desired value or strength. This is particularly interesting in situations where little data with no attribute annotation is available for learning, but we have access to a large external corpus of heavily annotated samples. While prior works primarily augment in the space of images, we propose to perform augmentation in feature space instead. We implement our approach as a deep encoder-decoder architecture that  learns the synthesis function in an end-to-end manner. We demonstrate the utility of our approach on the problems of (1) one-shot object recognition in a transfer-learning setting where we have no prior knowledge of the new classes, as well as (2) object-based one-shot scene recognition. As external data, we leverage 3D depth and pose information from the SUN RGB-D dataset. Our experiments show that attribute-guided augmentation of high-level CNN features considerably improves one-shot recognition performance on both problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dixit_AGA_Attribute-Guided_Augmentation_CVPR_2017_paper.pdf",
        "aff": "UC San Diego; University of Salzburg; UNC Chapel Hill; UC San Diego",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.02559v2",
        "pdf_size": 795366,
        "gs_citation": 154,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14229821186230487374&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "ucsd.edu;gmx.at;cs.unc.edu;ucsd.edu",
        "email": "ucsd.edu;gmx.at;cs.unc.edu;ucsd.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dixit_AGA_Attribute-Guided_Augmentation_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of California, San Diego;University of Salzburg;University of North Carolina at Chapel Hill",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.uni-salzburg.at;https://www.unc.edu",
        "aff_unique_abbr": "UCSD;USAL;UNC",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "San Diego;;Chapel Hill",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Austria"
    },
    {
        "title": "AMC: Attention guided Multi-modal Correlation Learning for Image Search",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "956",
        "author_site": "Kan Chen, Trung Bui, Chen Fang, Zhaowen Wang, Ram Nevatia",
        "author": "Kan Chen; Trung Bui; Chen Fang; Zhaowen Wang; Ram Nevatia",
        "abstract": "Given a user's query, traditional image search systems rank images according to its relevance to a single modality (e.g., image content or surrounding text). Nowadays, an increasing number of images on the Internet are available with associated meta data in rich modalities (e.g., titles, keywords, tags, etc.), which can be exploited for better similarity measure with queries. In this paper, we leverage visual and textual modalities for image search by learning their correlation with input query. According to the intent of query, attention mechanism can be introduced to adaptively balance the importance of different modalities. We propose a novel Attention guided Multi-modal Correlation (AMC) learning method which consists of a jointly learned hierarchy of intra and inter-attention networks. Conditioned on query's intent, intra-attention networks (i.e., visual intra-attention network and language intra-attention network) attend on informative parts within each modality; a multi-modal inter-attention network promotes the importance of the most query-relevant modalities. In experiments, we evaluate AMC models on the search logs from two real world image search engines and show a significant boost on the ranking of user-clicked images in search results. Additionally, we extend AMC models to caption ranking task on COCO dataset and achieve competitive results compared with recent state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_AMC_Attention_guided_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.00763v1",
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2136571101739605810&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_AMC_Attention_guided_CVPR_2017_paper.html"
    },
    {
        "title": "AMVH: Asymmetric Multi-Valued Hashing",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "226",
        "author_site": "Cheng Da, Shibiao Xu, Kun Ding, Gaofeng Meng, Shiming Xiang, Chunhong Pan",
        "author": "Cheng Da; Shibiao Xu; Kun Ding; Gaofeng Meng; Shiming Xiang; Chunhong Pan",
        "abstract": "Most existing hashing methods resort to binary codes for similarity search, owing to the high efficiency of computation and storage. However, binary codes lack enough capability in similarity preservation, resulting in less desirable performance. To address this issue, we propose an asymmetric multi-valued hashing method supported by two different non-binary embeddings. (1) A real-valued embedding is used for representing the newly-coming query. (2) A multi-integer-embedding is employed for compressing the whole database, which is modeled by binary sparse representation with fixed sparsity. With these two non-binary embeddings, the similarities between data points can be preserved precisely. To perform meaningful asymmetric similarity computation for efficient semantic search, these embeddings are jointly learnt by preserving the label-based similarity. Technically, this results in a mixed integer programming problem, which is efficiently solved by alternative optimization. Extensive experiments on three multilabel datasets demonstrate that our approach not only outperforms the existing binary hashing methods in search accuracy, but also retains their query and storage efficiency.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Da_AMVH_Asymmetric_Multi-Valued_CVPR_2017_paper.pdf",
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences+School of Computer and Control Engineering, University of Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences+School of Computer and Control Engineering, University of Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences+School of Computer and Control Engineering, University of Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 694902,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16158447420713583226&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Da_AMVH_Asymmetric_Multi-Valued_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0+1;0;0+1;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation;School of Computer and Control Engineering",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Accurate Depth and Normal Maps From Occlusion-Aware Focal Stack Symmetry",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1030",
        "author_site": "Michael Strecke, Anna Alperovich, Bastian Goldluecke",
        "author": "Michael Strecke; Anna Alperovich; Bastian Goldluecke",
        "abstract": "We introduce a novel approach to jointly estimate consistent depth and normal maps from 4D light fields, with two main contributions. First, we build a cost volume from focal stack symmetry. However, in contrast to previous approaches, we introduce partial focal stacks in order to be able to robustly deal with occlusions. This idea already yields significanly better disparity maps. Second, even recent sublabel-accurate methods for multi-label optimization recover only a piecewise flat disparity map from the cost volume, with normals pointing mostly towards the image plane. This renders normal maps recovered from these approaches unsuitable for potential subsequent applications. We therefore propose regularization with a novel prior linking depth to normals, and imposing smoothness of the resulting normal field. We then jointly optimize over depth and normals to achieve estimates for both which surpass previous work in accuracy on a recent benchmark.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Strecke_Accurate_Depth_and_CVPR_2017_paper.pdf",
        "aff": "University of Konstanz; University of Konstanz; University of Konstanz",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6713178,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6896692216323987140&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "uni-konstanz.de;uni-konstanz.de;uni-konstanz.de",
        "email": "uni-konstanz.de;uni-konstanz.de;uni-konstanz.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Strecke_Accurate_Depth_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Konstanz",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-konstanz.de",
        "aff_unique_abbr": "Uni Konstanz",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Accurate Optical Flow via Direct Cost Volume Processing",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "464",
        "author_site": "Jia Xu, Ren\u00c3\u00a9 Ranftl, Vladlen Koltun",
        "author": "Jia Xu; Rene Ranftl; Vladlen Koltun",
        "abstract": "We present an optical flow estimation approach that operates on the full four-dimensional cost volume. This direct approach shares the structural benefits of leading stereo matching pipelines, which are known to yield high accuracy. To this day, such approaches have been considered impractical due to the size of the cost volume. We show that the full four-dimensional cost volume can be constructed in a fraction of a second due to its regularity. We then exploit this regularity further by adapting semi-global matching to the four-dimensional setting. This yields a pipeline that achieves significantly higher accuracy than state-of-the-art optical flow methods while being faster than most. Our approach outperforms all published general-purpose optical flow methods on both Sintel and KITTI 2015 benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Accurate_Optical_Flow_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.07325v1",
        "pdf_size": 3141260,
        "gs_citation": 280,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14694539072842554840&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xu_Accurate_Optical_Flow_CVPR_2017_paper.html"
    },
    {
        "title": "Accurate Single Stage Detector Using Recurrent Rolling Convolution",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2293",
        "author_site": "Jimmy Ren, Xiaohao Chen, Jianbo Liu, Wenxiu Sun, Jiahao Pang, Qiong Yan, Yu-Wing Tai, Li Xu",
        "author": "Jimmy Ren; Xiaohao Chen; Jianbo Liu; Wenxiu Sun; Jiahao Pang; Qiong Yan; Yu-Wing Tai; Li Xu",
        "abstract": "Most of the recent successful methods in accurate object detection and localization used some variants of R-CNN style two stage Convolutional Neural Networks (CNN) where plausible regions were proposed in the first stage then followed by a second stage for decision refinement. Despite the simplicity of training and the efficiency in deployment, the single stage detection methods have not been as competitive when evaluated in benchmarks consider mAP for high IoU thresholds. In this paper, we proposed a novel single stage end-to-end trainable object detection network to overcome this limitation. We achieved this by introducing Recurrent Rolling Convolution (RRC) architecture over multi-scale feature maps to construct object classifiers and bounding box regressors which are \"deep in context\". We evaluated our method in the challenging KITTI dataset which measures methods under IoU threshold of 0.7. We showed that with RRC, a single reduced VGG-16 based model already significantly outperformed all the previously published results. At the time this paper was written our models ranked the first in KITTI car detection (the hard level), the first in cyclist detection and the second in pedestrian detection. These results were not reached by the previous single stage methods. The code is publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ren_Accurate_Single_Stage_CVPR_2017_paper.pdf",
        "aff": "SenseTime Group Limited; SenseTime Group Limited; SenseTime Group Limited; SenseTime Group Limited; SenseTime Group Limited; SenseTime Group Limited; SenseTime Group Limited; SenseTime Group Limited",
        "project": "",
        "github": "https://github.com/xiaohaoChen/rrc_detection",
        "supp": "",
        "arxiv": "1704.05776v1",
        "pdf_size": 1100213,
        "gs_citation": 374,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5780311761574416093&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com",
        "email": "sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ren_Accurate_Single_Stage_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "SenseTime Group Limited",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sensetime.com",
        "aff_unique_abbr": "SenseTime",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Acquiring Axially-Symmetric Transparent Objects Using Single-View Transmission Imaging",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1314",
        "author_site": "Jaewon Kim, Ilya Reshetouski, Abhijeet Ghosh",
        "author": "Jaewon Kim; Ilya Reshetouski; Abhijeet Ghosh",
        "abstract": "We propose a novel, practical solution for high quality reconstruction of axially-symmetric transparent objects. While a special case, such transparent objects are ubiquitous in the real world. Common examples of these are glasses, goblets, tumblers, carafes, etc., that can have very unique and visually appealing forms making their reconstruction interesting for vision and graphics applications. Our acquisition setup involves imaging such objects from a single viewpoint while illuminating them from directly behind with a few patterns emitted by an LCD panel. Our reconstruction step is then based on optimization of the object's geometry and its refractive index to minimize the difference between observed and simulated transmission/refraction of rays passing through the object. We exploit the object's axial symmetry as a strong shape prior which allows us to achieve robust reconstruction from a single viewpoint using a simple, commodity acquisition setup. We demonstrate high quality reconstruction of several common rotationally symmetric as well as more complex n-fold symmetric transparent objects with our approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kim_Acquiring_Axially-Symmetric_Transparent_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kim_Acquiring_Axially-Symmetric_Transparent_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14433931089063581623&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kim_Acquiring_Axially-Symmetric_Transparent_CVPR_2017_paper.html"
    },
    {
        "title": "Action Unit Detection With Region Adaptation, Multi-Labeling Learning and Optimal Temporal Fusing",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "685",
        "author_site": "Wei Li, Farnaz Abtahi, Zhigang Zhu",
        "author": "Wei Li; Farnaz Abtahi; Zhigang Zhu",
        "abstract": "Action Unit (AU) detection becomes essential for facial analysis. Many proposed approaches face challenging problems in dealing with the alignments of different face regions, in the effective fusion of temporal information, and in training a model  for multiple  AU labels. To better address these problems, we propose a deep learning framework for AU detection with region of interest (ROI) adaptation, integrated multi-label learning, and optimal LSTM-based temporal fusing.  First, an ROI cropping net is designed to make sure specific interested regions of faces are learned independently; each sub-region has a local convolutional neural network (CNN) whose convolutional filters will only be trained for the corresponding region. Second, multi-label learning is employed to integrate the outputs of those individual ROI cropping nets, which learns the inter-relationships of various AUs and acquires global features  across sub-regions for AU detection. Finally, the optimal selection of multiple LSTM layers are carried out to best fuse temporal features, in order to make the AU prediction the most accurate. The proposed approach is evaluated on two popular AU detection datasets, BP4D and DISFA, outperforming the state of the art significantly, with an average improvement of around 13% in BP4D and 25% in DISFA, respectively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Action_Unit_Detection_CVPR_2017_paper.pdf",
        "aff": "Dept of Electrical Engineering, CUNY City College, New York, USA; Dept of Computer Science, CUNY Graduate Center, New York, USA; Dept of Computer Science, CUNY Graduate Center and City College, New York, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03067",
        "pdf_size": 827040,
        "gs_citation": 207,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16512630752392239545&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "ccny.cuny.edu;gradcenter.cuny.edu;cs.ccny.cuny.edu",
        "email": "ccny.cuny.edu;gradcenter.cuny.edu;cs.ccny.cuny.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Action_Unit_Detection_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "City College of New York;CUNY Graduate Center;City University of New York",
        "aff_unique_dep": "Department of Electrical Engineering;Dept of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.ccny.cuny.edu;https://www.gc.cuny.edu;https://www.cuny.edu",
        "aff_unique_abbr": "CCNY;CUNY GC;CUNY",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "New York;Graduate Center and City College",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Action-Decision Networks for Visual Tracking With Deep Reinforcement Learning",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Spotlight",
        "track": "main",
        "pid": "985",
        "author_site": "Sangdoo Yun, Jongwon Choi, Youngjoon Yoo, Kimin Yun, Jin Young Choi",
        "author": "Sangdoo Yun; Jongwon Choi; Youngjoon Yoo; Kimin Yun; Jin Young Choi",
        "abstract": "This paper proposes a novel tracker which is controlled by sequentially pursuing actions learned by deep reinforcement learning. In contrast to the existing trackers using deep networks, the proposed tracker is designed to achieve a light computation as well as satisfactory tracking accuracy in both location and scale. The deep network to control actions is pre-trained using various training sequences and fine-tuned during tracking for online adaptation to target and background changes. The pre-training is done by utilizing deep reinforcement learning as well as supervised learning. The use of reinforcement learning enables even partially labeled data to be successfully utilized for semi-supervised learning. Through evaluation of the OTB dataset, the proposed tracker is validated to achieve a competitive performance that is three times faster than state-of-the-art, deep network-based trackers. The fast version of the proposed method, which operates in real-time on GPU, outperforms the state-of-the-art real-time trackers.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yun_Action-Decision_Networks_for_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yun_Action-Decision_Networks_for_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 635,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2540846610689605576&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yun_Action-Decision_Networks_for_CVPR_2017_paper.html"
    },
    {
        "title": "ActionVLAD: Learning Spatio-Temporal Aggregation for Action Classification",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "327",
        "author_site": "Rohit Girdhar, Deva Ramanan, Abhinav Gupta, Josef Sivic, Bryan Russell",
        "author": "Rohit Girdhar; Deva Ramanan; Abhinav Gupta; Josef Sivic; Bryan Russell",
        "abstract": "In this work, we introduce a new video representation for action classification that aggregates local convolutional features across the entire spatio-temporal extent of the video. We do so by integrating state-of-the-art two-stream networks with learnable spatio-temporal feature aggregation. The resulting architecture is end-to-end trainable for whole-video classification. We investigate different strategies for pooling across space and time and combining signals from the different streams. We find that: (i) it is important to pool jointly across space and time, but (ii) appearance and motion streams are best aggregated into their own separate representations. Finally, we show that our representation outperforms the two-stream base architecture by a large margin (13% relative) as well as outperforms other baselines with comparable base architectures on HMDB51, UCF101, and Charades video classification benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Girdhar_ActionVLAD_Learning_Spatio-Temporal_CVPR_2017_paper.pdf",
        "aff": "Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Adobe Research+INRIA; Adobe Research",
        "project": "",
        "github": "http://rohitgirdhar.github.io/ActionVLAD",
        "supp": "",
        "arxiv": "1704.02895v1",
        "pdf_size": 3056249,
        "gs_citation": 607,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14985577374027311324&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cmu.edu;cmu.edu;cmu.edu;inria.fr;adobe.com",
        "email": "cmu.edu;cmu.edu;cmu.edu;inria.fr;adobe.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Girdhar_ActionVLAD_Learning_Spatio-Temporal_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1+2;1",
        "aff_unique_norm": "Carnegie Mellon University;Adobe;INRIA",
        "aff_unique_dep": "Robotics Institute;Adobe Research;",
        "aff_unique_url": "https://www.cmu.edu;https://research.adobe.com;https://www.inria.fr",
        "aff_unique_abbr": "CMU;Adobe;INRIA",
        "aff_campus_unique_index": "0;0;0;",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;0;0;0+1;0",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Active Convolution: Learning the Shape of Convolution for Image Classification",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1709",
        "author_site": "Yunho Jeon, Junmo Kim",
        "author": "Yunho Jeon; Junmo Kim",
        "abstract": "In recent years, deep learning has achieved great success in many computer vision applications. Convolutional neural networks (CNNs) have lately emerged as a major approach to image classification. Most research on CNNs thus far has focused on developing architectures such as the Inception and residual networks. The convolution layer is the core of the CNN, but few studies have addressed the convolution unit itself. In this paper, we introduce a convolution unit called the active convolution unit (ACU). A new convolution has no fixed shape, because of which we can define any form of convolution. Its shape can be learned through backpropagation during training. Our proposed unit has a few advantages. First, the ACU is a generalization of convolution; it can define not only all conventional convolutions, but also convolutions with fractional pixel coordinates. We can freely change the shape of the convolution, which provides greater freedom to form CNN structures. Second, the shape of the convolution is learned while training and there is no need to tune it by hand. Third, the ACU can learn better than a conventional unit, where we obtained the improvement simply by changing the conventional convolution to an ACU. We tested our proposed method on plain and residual networks, and the results showed significant improvement using our method on various datasets and architectures in comparison with the baseline.  Code is available at https://github.com/jyh2986/Active-Convolution.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jeon_Active_Convolution_Learning_CVPR_2017_paper.pdf",
        "aff": "EE, KAIST; EE, KAIST",
        "project": "",
        "github": "https://github.com/jyh2986/Active-Convolution",
        "supp": "",
        "arxiv": "1703.09076v1",
        "pdf_size": 1344933,
        "gs_citation": 224,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10968769167272986016&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jeon_Active_Convolution_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "KAIST",
        "aff_unique_dep": "Electrical Engineering",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1254",
        "author_site": "Amlan Kar, Nishant Rai, Karan Sikka, Gaurav Sharma",
        "author": "Amlan Kar; Nishant Rai; Karan Sikka; Gaurav Sharma",
        "abstract": "We propose a novel method for temporally pooling frames in a video for the task of human action recognition. The method is motivated by the observation that there are only a small number of frames which, together, contain sufficient information to discriminate an action class present in a video, from the rest. The proposed method learns to pool such discriminative and informative frames, while discarding a majority of the non-informative frames in a single temporal scan of the video. Our algorithm does so by continuously predicting the discriminative importance of each video frame and subsequently pooling them in a deep learning framework. We show the effectiveness of our proposed pooling method on standard benchmarks where it consistently improves on baseline pooling methods, with both RGB and optical flow based Convolutional networks. Further, in combination with complementary video representations, we show results that are competitive with respect to the state-of-the-art results on two challenging and publicly available benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kar_AdaScan_Adaptive_Scan_CVPR_2017_paper.pdf",
        "aff": "IIT Kanpur; IIT Kanpur; SRI International + UCSD; IIT Kanpur",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.08240v4",
        "pdf_size": 1085938,
        "gs_citation": 201,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15638508089495936081&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cse.iitk.ac.in;cse.iitk.ac.in;sri.com;cse.iitk.ac.in",
        "email": "cse.iitk.ac.in;cse.iitk.ac.in;sri.com;cse.iitk.ac.in",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kar_AdaScan_Adaptive_Scan_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1+2;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur;SRI International;University of California, San Diego",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.iitk.ac.in;https://www.sri.com;https://ucsd.edu",
        "aff_unique_abbr": "IITK;SRI;UCSD",
        "aff_campus_unique_index": "0;0;2;0",
        "aff_campus_unique": "Kanpur;;La Jolla",
        "aff_country_unique_index": "0;0;1+1;0",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "Adaptive Class Preserving Representation for Image Classification",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "3839",
        "author_site": "Jian-Xun Mi, Qiankun Fu, Weisheng Li",
        "author": "Jian-Xun Mi; Qiankun Fu; Weisheng Li",
        "abstract": "In linear representation-based image classification, an unlabeled sample is represented by the entire training set. To obtain a stable and discriminative solution, regularization on the vector of representation coefficients is necessary. For example, the representation in sparse representation-based classification (SRC) uses L1 norm penalty as regularization, which is equal to lasso. However, lasso overemphasizes the role of sparseness while ignoring the inherent structure among samples belonging to a same class. Many recent developed representation classifications have adopted lasso-type regressions to improve the performance. In this paper, we propose the adaptive class preserving representation for classification (ACPRC). Our method is related to group lasso based classification but different in two key points: When training samples in a class are uncorrelated, ACPRC turns into SRC; when samples in a class are highly correlated, it obtains similar result as group lasso. The superiority of ACPRC over other state-of-the-art regularization techniques including lasso, group lasso, sparse group lasso, etc. are evaluated by extensive experiments.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Mi_Adaptive_Class_Preserving_CVPR_2017_paper.pdf",
        "aff": "Chongqing Key Laboratory of Computational Intelligence, Chongqing University of Posts and Telecommunications, Chongqing, China + College of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Computational Intelligence, Chongqing University of Posts and Telecommunications, Chongqing, China + College of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Computational Intelligence, Chongqing University of Posts and Telecommunications, Chongqing, China + College of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Chongqing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1467755,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9498299517329703953&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;stu.cqupt.edu.cn;cqupt.edu.cn",
        "email": "gmail.com;stu.cqupt.edu.cn;cqupt.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Mi_Adaptive_Class_Preserving_CVPR_2017_paper.html",
        "aff_unique_index": "0+0;0+0;0+0",
        "aff_unique_norm": "Chongqing University of Posts and Telecommunications",
        "aff_unique_dep": "Chongqing Key Laboratory of Computational Intelligence",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Chongqing",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3685",
        "author_site": "Zheng Xu, M\u00c3\u00a1rio A. T. Figueiredo, Xiaoming Yuan, Christoph Studer, Tom Goldstein",
        "author": "Zheng Xu; Mario A. T. Figueiredo; Xiaoming Yuan; Christoph Studer; Tom Goldstein",
        "abstract": "Many modern computer vision and machine learning applications rely on solving difficult optimization problems that involve non-differentiable objective functions and constraints. The alternating direction method of multipliers (ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a generalization of ADMM that often achieves better performance, but its efficiency depends strongly on algorithm parameters that must be chosen by an expert user. We propose an adaptive method that automatically tunes the key algorithm parameters to achieve optimal performance without user oversight. Inspired by recent work on adaptivity, the proposed adaptive relaxed ADMM (ARADMM) is derived by assuming a Barzilai-Borwein style linear gradient. A detailed convergence analysis of ARADMM is provided, and numerical results on several applications demonstrate fast practical convergence.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Adaptive_Relaxed_ADMM_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, University of Maryland, College Park, MD; Instituto de Telecomunica\u00e7\u00f5es, Instituto Superior T\u00e9cnico, Universidade de Lisboa, Portugal; Department of Mathematics, Hong Kong Baptist University, Kowloon Tong, Hong Kong; School of Electrical and Computer Engineering, Cornell University, Ithaca, NY; Department of Computer Science, University of Maryland, College Park, MD",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Xu_Adaptive_Relaxed_ADMM_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.02712",
        "pdf_size": 777776,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11169031158943488545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.umd.edu; ; ; ; ",
        "email": "cs.umd.edu; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xu_Adaptive_Relaxed_ADMM_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "University of Maryland, College Park;Universidade de Lisboa;Hong Kong Baptist University;Cornell University",
        "aff_unique_dep": "Department of Computer Science;Instituto de Telecomunica\u00e7\u00f5es, Instituto Superior T\u00e9cnico;Department of Mathematics;School of Electrical and Computer Engineering",
        "aff_unique_url": "https://www/umd.edu;https://www IST.pt;https://www.hkbu.edu.hk;https://www.cornell.edu",
        "aff_unique_abbr": "UMD;ULisboa;HKBU;Cornell",
        "aff_campus_unique_index": "0;2;3;0",
        "aff_campus_unique": "College Park;;Hong Kong SAR;Ithaca",
        "aff_country_unique_index": "0;1;2;0;0",
        "aff_country_unique": "United States;Portugal;China"
    },
    {
        "title": "Adaptive and Move Making Auxiliary Cuts for Binary Pairwise Energies",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2032",
        "author_site": "Lena Gorelick, Yuri Boykov, Olga Veksler",
        "author": "Lena Gorelick; Yuri Boykov; Olga Veksler",
        "abstract": "Many computer vision problems require optimization of binary non-submodular energies. In this context, iterative submodularization techniques based on trust region (LSA-TR) and auxiliary functions (LSA-AUX) have been recently proposed [??]. They achieve state-of-the-art-results on a number of computer vision applications.  In this paper we extend the LSA-AUX framework in two directions. First, unlike LSA-AUX which selects auxiliary functions based solely on the current solution, we propose to incorporate several additional criteria. This results in tighter bounds for configurations that are more likely or closer to the current solution. Second, we propose move-making extensions of LSA-AUX which achieve tighter bounds by restricting the search space.  Finally, we evaluate our methods on several applications. We show that for each application at least one of our extensions significantly outperforms the original LSA-AUX. Moreover, the best extension of LSA-AUX is comparable to or better than LSA-TR on five out of six applications, achieving state-of-the-arts results on four out of six applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gorelick_Adaptive_and_Move_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Gorelick_Adaptive_and_Move_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1130908962442319959&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gorelick_Adaptive_and_Move_CVPR_2017_paper.html"
    },
    {
        "title": "Additive Component Analysis",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "908",
        "author_site": "Calvin Murdock, Fernando De la Torre",
        "author": "Calvin Murdock; Fernando De la Torre",
        "abstract": "Principal component analysis (PCA) is one of the most versatile tools for unsupervised learning with applications ranging from dimensionality reduction to exploratory data analysis and visualization. While much effort has been devoted to encouraging meaningful representations through regularization (e.g. non-negativity or sparsity), underlying linearity assumptions can limit their effectiveness. To address this issue, we propose Additive Component Analysis (ACA), a novel nonlinear extension of PCA. Inspired by multivariate nonparametric regression with additive models, ACA fits a smooth manifold to data by learning an explicit mapping from a low-dimensional latent space to the input space, which trivially enables applications like denoising. Furthermore, ACA can be used as a drop-in replacement in many algorithms that use linear component analysis methods as a subroutine via the local tangent space of the learned manifold. Unlike many other nonlinear dimensionality reduction techniques, ACA can be efficiently applied to large datasets since it does not require computing pairwise similarities or storing training data during testing. Multiple ACA layers can also be composed and learned jointly with essentially the same procedure for improved representational power, demonstrating the encouraging potential of nonparametric deep learning. We evaluate ACA on a variety of datasets, showing improved robustness, reconstruction performance, and interpretability.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Murdock_Additive_Component_Analysis_CVPR_2017_paper.pdf",
        "aff": "Machine Learning Department, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 31052829,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17897476823896007441&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Murdock_Additive_Component_Analysis_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adversarial Discriminative Domain Adaptation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3384",
        "author_site": "Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell",
        "author": "Eric Tzeng; Judy Hoffman; Kate Saenko; Trevor Darrell",
        "abstract": "Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper.pdf",
        "aff": "University of California, Berkeley; Stanford University; Boston University; University of California, Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.05464v1",
        "pdf_size": 1009485,
        "gs_citation": 6156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14531515109021297962&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "eecs.berkeley.edu;cs.stanford.edu;bu.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;cs.stanford.edu;bu.edu;eecs.berkeley.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of California, Berkeley;Stanford University;Boston University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.stanford.edu;https://www.bu.edu",
        "aff_unique_abbr": "UC Berkeley;Stanford;BU",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Berkeley;Stanford;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adversarially Tuned Scene Generation",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "941",
        "author_site": "VSR Veeravasarapu, Constantin Rothkopf, Ramesh Visvanathan",
        "author": "VSR Veeravasarapu; Constantin Rothkopf; Ramesh Visvanathan",
        "abstract": "Generalization performance of trained computer vision (CV) systems that use computer graphics (CG) generated data is not yet effective due to the concept of 'domain-shift' between virtual and real data. Although simulated data augmented with a few real-world samples has been shown to mitigate domain shift and improve transferability of trained models, guiding or bootstrapping the virtual data generation with the distributions learnt from target real world domain is desired, especially in the fields where annotating even few real images is laborious (such as semantic labeling, optical flow, and intrinsic images etc.).     In order to address this problem in an unsupervised manner, our work combines recent advances in CG, which aims at generating stochastic scene layouts using large collections of 3D object models, and generative adversarial training, which aims at training generative models by measuring discrepancy between generated and real data in terms of their separability in the space of a deep discriminatively-trained classifier. Our method uses iterative estimation of the posterior density of prior distributions for a generative graphical model. This is done within a rejection sampling framework. Initially, we assume uniform distributions as priors over parameters of a scene described by a generative graphical model. As iterations proceed the uniform prior distributions are updated sequentially to distributions that are closer to the unknown distributions of target data.       We demonstrate the utility of adversarially tuned scene generation on two real world benchmark datasets (CityScapes and CamVid) for traffic scene semantic labeling with a deep convolutional net (DeepLab). We obtained performance improvements by 2.28 and 3.14 points on the IoU metric between the DeepLab models trained on simulated sets prepared from the scene generation models before and after tuning to CityScapes and CamVid respectively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Veeravasarapu_Adversarially_Tuned_Scene_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.00405v2",
        "pdf_size": 2259297,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2088870011552032311&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Veeravasarapu_Adversarially_Tuned_Scene_CVPR_2017_paper.html"
    },
    {
        "title": "Age Progression/Regression by Conditional Adversarial Autoencoder",
        "session": "Machine Learning 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "2503",
        "author_site": "Zhifei Zhang, Yang Song, Hairong Qi",
        "author": "Zhifei Zhang; Yang Song; Hairong Qi",
        "abstract": "\"If I provide you a face image of mine (without telling you the actual age when I took the picture) and a large amount of face images that I crawled (containing labeled faces of different ages but not necessarily paired), can you show me what I would look like when I am 80 or what I was like when I was 5?\" The answer is probably a \"No.\" Most existing face aging works attempt to learn the transformation between age groups and thus would require the paired samples as well as the labeled query image. In this paper, we look at the problem from a generative modeling perspective such that no paired samples is required. In addition, given an unlabeled image, the generative model can directly produce the image with desired age attribute. We propose a conditional adversarial autoencoder (CAAE) that learns a face manifold, traversing on which smooth age progression and regression can be realized simultaneously. In CAAE, the face is first mapped to a latent vector through a convolutional encoder, and then the vector is projected to the face manifold conditional on age through a deconvolutional generator. The latent vector preserves personalized face features (i.e., personality) and the age condition controls progression vs. regression. Two adversarial networks are imposed on the encoder and generator, respectively, forcing to generate more photo-realistic faces. Experimental results demonstrate the appealing performance and flexibility of the proposed framework by comparing with the state-of-the-art and ground truth.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Age_ProgressionRegression_by_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.08423v2",
        "gs_citation": 1463,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=69113299933699289&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Age_ProgressionRegression_by_CVPR_2017_paper.html"
    },
    {
        "title": "Agent-Centric Risk Assessment: Accident Anticipation and Risky Region Localization",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Spotlight",
        "track": "main",
        "pid": "799",
        "author_site": "Kuo-Hao Zeng, Shih-Han Chou, Fu-Hsiang Chan, Juan Carlos Niebles, Min Sun",
        "author": "Kuo-Hao Zeng; Shih-Han Chou; Fu-Hsiang Chan; Juan Carlos Niebles; Min Sun",
        "abstract": "For survival, a living agent (e.g., human in Fig. 1(a)) must have the ability to assess risk (1) by temporally anticipating accidents before they occur (Fig. 1(b)), and (2) by spatially localizing risky regions (Fig. 1(c)) in the environment to move away from threats. In this paper, we take an agent-centric approach to study the accident anticipation and risky region localization tasks. We propose a novel soft-attention Recurrent Neural Network (RNN) which explicitly models both spatial and appearance-wise non-linear interaction between the agent triggering the event and another agent or static-region involved. In order to test our proposed method, we introduce the Epic Fail (EF) dataset consisting of 3000 viral videos capturing various accidents. In the experiments, we evaluate the risk assessment accuracy both in the temporal domain (accident anticipation) and spatial domain (risky region localization) on our EF dataset and the Street Accident (SA) dataset. Our method consistently outperforms other baselines on both datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zeng_Agent-Centric_Risk_Assessment_CVPR_2017_paper.pdf",
        "aff": "Stanford University\u2020; National Tsing Hua University\u2217; National Tsing Hua University\u2217; Stanford University\u2020; National Tsing Hua University\u2217",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zeng_Agent-Centric_Risk_Assessment_2017_CVPR_supplemental.pdf",
        "arxiv": "1705.06560v1",
        "pdf_size": 4395033,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13588304206449344785&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;gmail.com;gmail.com;ee.nthu.edu.tw",
        "email": "cs.stanford.edu;cs.stanford.edu;gmail.com;gmail.com;ee.nthu.edu.tw",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zeng_Agent-Centric_Risk_Assessment_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "Stanford University;National Tsing Hua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.nthu.edu.tw",
        "aff_unique_abbr": "Stanford;NTHU",
        "aff_campus_unique_index": "0;1;1;0;1",
        "aff_campus_unique": "Stanford;Taiwan",
        "aff_country_unique_index": "0;1;1;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Aggregated Residual Transformations for Deep Neural Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "552",
        "author_site": "Saining Xie, Ross Girshick, Piotr Doll\u00c3\u00a1r, Zhuowen Tu, Kaiming He",
        "author": "Saining Xie; Ross Girshick; Piotr Dollar; Zhuowen Tu; Kaiming He",
        "abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call \"cardinality\" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.pdf",
        "aff": "UC San Diego; Facebook AI Research; Facebook AI Research; UC San Diego; Facebook AI Research",
        "project": "",
        "github": "https://github.com/facebookresearch/ResNeXt",
        "supp": "",
        "arxiv": "1611.05431v2",
        "pdf_size": 646491,
        "gs_citation": 14783,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11890020786646058356&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ucsd.edu;fb.com;fb.com;ucsd.edu;fb.com",
        "email": "ucsd.edu;fb.com;fb.com;ucsd.edu;fb.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "University of California, San Diego;Meta",
        "aff_unique_dep": ";Facebook AI Research",
        "aff_unique_url": "https://www.ucsd.edu;https://research.facebook.com",
        "aff_unique_abbr": "UCSD;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "All You Need Is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks With Orthonormality and Modulation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2717",
        "author_site": "Di Xie, Jiang Xiong, Shiliang Pu",
        "author": "Di Xie; Jiang Xiong; Shiliang Pu",
        "abstract": "Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. Moreover, we design a backward error modulation mechanism based on the quasi-isometry assumption between two consecutive parametric layers. Equipped with these two ingredients, we propose several novel optimization solutions that can be utilized for training a specific-structured (repetitively triple modules of Conv-BNReLU) extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/ identity mappings from scratch. Experiments show that our proposed solutions can achieve distinct improvements for a 44-layer and a 110-layer plain networks on both the CIFAR-10 and ImageNet datasets. Moreover, we can successfully train plain CNNs to match the performance of the residual counterparts.   Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_All_You_Need_CVPR_2017_paper.pdf",
        "aff": "Hikvision Research Institute; Hikvision Research Institute; Hikvision Research Institute",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Xie_All_You_Need_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.01827v3",
        "pdf_size": 1045828,
        "gs_citation": 233,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15375936368437877117&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "hikvision.com;hikvision.com;hikvision.com",
        "email": "hikvision.com;hikvision.com;hikvision.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xie_All_You_Need_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hikvision Research Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hikvision.com/cn/",
        "aff_unique_abbr": "Hikvision",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Alternating Direction Graph Matching",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2763",
        "author_site": "D. Khu\u00c3\u00aa L\u00c3\u00aa-Huu, Nikos Paragios",
        "author": "D. Khue Le-Huu; Nikos Paragios",
        "abstract": "In this paper, we introduce a graph matching method that can account for constraints of arbitrary order, with arbitrary potential functions. Unlike previous decomposition approaches that rely on the graph structures, we introduce a decomposition of the matching constraints. Graph matching is then reformulated as a non-convex non-separable optimization problem that can be split into smaller and much-easier-to-solve subproblems, by means of the alternating direction method of multipliers. The proposed framework is modular, scalable, and can be instantiated into different variants. Two instantiations are studied exploring pairwise and higher-order constraints. Experimental results on widely adopted benchmarks involving synthetic and real examples demonstrate that the proposed solutions outperform existing pairwise graph matching methods, and competitive with the state of the art in higher-order settings.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Le-Huu_Alternating_Direction_Graph_CVPR_2017_paper.pdf",
        "aff": "CentraleSup \u00b4elec, Universit \u00b4e Paris-Saclay, France; CentraleSup \u00b4elec, Universit \u00b4e Paris-Saclay, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.07583",
        "pdf_size": 1184908,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9502532271818239526&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "centralesupelec.fr;centralesupelec.fr",
        "email": "centralesupelec.fr;centralesupelec.fr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Le-Huu_Alternating_Direction_Graph_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "CentraleSup\u00e9lec",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.centralesupelec.fr",
        "aff_unique_abbr": "CentraleSup\u00e9lec",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paris-Saclay",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Amodal Detection of 3D Objects: Inferring 3D Bounding Boxes From 2D Ones in RGB-Depth Images",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2455",
        "author_site": "Zhuo Deng, Longin Jan Latecki",
        "author": "Zhuo Deng; Longin Jan Latecki",
        "abstract": "This paper addresses the problem of amodal perception of 3D object detection. The task is to not only find object localizations in the 3D world, but also estimate their physical sizes and poses, even if only parts of them are visible in the RGB-D image. Recent approaches have attempted to harness point cloud from depth channel to exploit 3D features directly in the 3D space and demonstrated the superiority over traditional 2.5D representation approaches. We revisit the amodal 3D detection problem by sticking to the 2.5D representation framework, and directly relate 2.5D visual appearance to 3D objects. We propose a novel 3D object detection system that simultaneously predicts objects' 3D locations, physical sizes, and orientations in indoor scenes. Experiments on the NYUV2 dataset show our algorithm significantly outperforms the state-of-the-art and indicates 2.5D representation is capable of encoding features for 3D amodal object detection. All source code and data is on https://github.com/phoenixnn/Amodal3Det.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Deng_Amodal_Detection_of_CVPR_2017_paper.pdf",
        "aff": "Temple University, Philadelphia, USA; Temple University, Philadelphia, USA",
        "project": "",
        "github": "https://github.com/phoenixnn/Amodal3Det",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2740365,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=306868455895396526&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;temple.edu",
        "email": "gmail.com;temple.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Deng_Amodal_Detection_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Temple University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.temple.edu",
        "aff_unique_abbr": "Temple",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Philadelphia",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "An Efficient Algebraic Solution to the Perspective-Three-Point Problem",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3442",
        "author_site": "Tong Ke, Stergios I. Roumeliotis",
        "author": "Tong Ke; Stergios I. Roumeliotis",
        "abstract": "In this work, we present an algebraic solution to the classical perspective-3-point (P3P) problem for determining the position and attitude of a camera from observations of three known reference points. In contrast to previous approaches, we first directly determine the camera's attitude by employing the corresponding geometric constraints to formulate a system of trigonometric equations. This is then efficiently solved, following an algebraic approach, to determine the unknown rotation matrix and subsequently the camera's position. As compared to recent alternatives, our method avoids computing unnecessary (and potentially numerically unstable) intermediate results, and thus achieves higher numerical accuracy and robustness at a lower computational cost. These benefits are validated through extensive Monte-Carlo simulations for both nominal and close-to-singular geometric configurations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ke_An_Efficient_Algebraic_CVPR_2017_paper.pdf",
        "aff": "University of Minnesota; University of Minnesota",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ke_An_Efficient_Algebraic_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1440663,
        "gs_citation": 187,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5125890339409899969&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.umn.edu;cs.umn.edu",
        "email": "cs.umn.edu;cs.umn.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ke_An_Efficient_Algebraic_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Minnesota",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.minnesota.edu",
        "aff_unique_abbr": "UMN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "An Efficient Background Term for 3D Reconstruction and Tracking With Smooth Surface Models",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3398",
        "author_site": "Mariano Jaimez, Thomas J. Cashman, Andrew Fitzgibbon, Javier Gonzalez-Jimenez, Daniel Cremers",
        "author": "Mariano Jaimez; Thomas J. Cashman; Andrew Fitzgibbon; Javier Gonzalez-Jimenez; Daniel Cremers",
        "abstract": "We present a novel strategy to shrink and constrain a 3D model, represented as a smooth spline-like surface, within the visual hull of an object observed from one or multiple views. This new 'background' or 'silhouette' term combines the efficiency of previous approaches based on an image-plane distance transform with the accuracy of formulations based on raycasting or ray potentials. The overall formulation is solved by alternating an inner nonlinear minization (raycasting) with a joint optimization of the surface geometry, the camera poses and the data correspondences. Experiments on 3D reconstruction and object tracking show that the new formulation corrects several deficiencies of existing approaches, for instance when modelling non-convex shapes. Moreover, our proposal is more robust against defects in the object segmentation and inherently handles the presence of uncertainty in the measurements (e.g. null depth values in images provided by RGB-D cameras).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jaimez_An_Efficient_Background_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Jaimez_An_Efficient_Background_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5009388863802214591&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jaimez_An_Efficient_Background_CVPR_2017_paper.html"
    },
    {
        "title": "An Empirical Evaluation of Visual Question Answering for Novel Objects",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1780",
        "author_site": "Santhosh K. Ramakrishnan, Ambar Pal, Gaurav Sharma, Anurag Mittal",
        "author": "Santhosh K. Ramakrishnan; Ambar Pal; Gaurav Sharma; Anurag Mittal",
        "abstract": "We study the problem of answering questions about images in the harder setting, where the test questions and corresponding images contain novel objects, which were not queried about in the training data. Such setting is inevitable in real world--owing to the heavy tailed distribution of the visual categories, there would be some objects which would not be annotated in the train set. We show that the performance of two popular existing methods drop significantly (21-28%) when evaluated on novel objects cf. known objects. We propose methods which use large existing external corpora of (i) unlabeled text, i.e. books, and (ii) images tagged with classes, to achieve novel object based visual question answering. We systematically study both, an oracle case where the novel objects are known textually, as well as a fully automatic case without any explicit knowledge of the novel objects, but with the minimal assumption that the novel objects are semantically related to the existing objects in training. The proposed methods for novel object based visual question answering are modular and can potentially be used with many visual question answering architectures. We show consistent improvements with the two popular architectures and give qualitative analysis of the cases where the model does well and of those where it fails to bring improvements.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ramakrishnan_An_Empirical_Evaluation_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ramakrishnan_An_Empirical_Evaluation_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.02516",
        "pdf_size": 985775,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9379527282684715996&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ramakrishnan_An_Empirical_Evaluation_CVPR_2017_paper.html"
    },
    {
        "title": "An Exact Penalty Method for Locally Convergent Maximum Consensus",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "702",
        "author_site": "Huu Le, Tat-Jun Chin, David Suter",
        "author": "Huu Le; Tat-Jun Chin; David Suter",
        "abstract": "Maximum consensus estimation plays a critically important role in computer vision. Currently, the most prevalent approach draws from the class of non-deterministic hypothesize-and-verify algorithms, which are cheap but do not guarantee solution quality. On the other extreme, there are global algorithms which are exhaustive search in nature and can be costly for practical-sized inputs. This paper aims to fill the gap between the two extremes by proposing a locally convergent maximum consensus algorithm. Our method is based on a formulating the problem with linear complementarity constraints, then defining a penalized version which is provably equivalent to the original problem. Based on the penalty problem, we develop a Frank-Wolfe algorithm that can deterministically solve the maximum consensus problem. Compared to the randomized techniques, our method is deterministic and locally convergent; relative to the global algorithms, our method is much more practical on realistic input sizes. Further, our approach is naturally applicable to problems with geometric residuals.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Le_An_Exact_Penalty_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science, The University of Adelaide; School of Computer Science, The University of Adelaide; School of Computer Science, The University of Adelaide",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1911689,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12042551979506551721&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Le_An_Exact_Penalty_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Adelaide",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Analyzing Computer Vision Data - The Good, the Bad and the Ugly",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "731",
        "author_site": "Oliver Zendel, Katrin Honauer, Markus Murschitz, Martin Humenberger, Gustavo Fern\u00c3\u00a1ndez Dom\u00c3\u00adnguez",
        "author": "Oliver Zendel; Katrin Honauer; Markus Murschitz; Martin Humenberger; Gustavo Fernandez Dominguez",
        "abstract": "In recent years, a great number of datasets were published to train and evaluate computer vision (CV) algorithms. These valuable contributions helped to push CV solutions to a level where they can be used for safety-relevant applications, such as autonomous driving. However, major questions concerning quality and usefulness of test data for CV evaluation are still unanswered. Researchers and engineers try to cover all test cases by using as much test data as possible. In this paper, we propose a different solution for this challenge. We introduce a method for dataset analysis which builds upon an improved version of the CV-HAZOP checklist, a list of potential hazards within the CV domain. Picking stereo vision as an example, we provide an extensive survey of 28 datasets covering the last two decades. We create a tailored checklist and apply it to the datasets Middlebury, KITTI, Sintel, Freiburg, and HCI to present a thorough characterization and quantitative comparison. We confirm the usability of our checklist for identification of challenging stereo situations by applying nine state-of-the-art stereo matching algorithms on the analyzed datasets, showing that hazard frames correlate with difficult frames. We show that challenging datasets still allow a meaningful algorithm evaluation even for small subsets. Finally, we provide a list of missing test cases that are still not covered by current datasets as inspiration for researchers who want to participate in future dataset creation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zendel_Analyzing_Computer_Vision_CVPR_2017_paper.pdf",
        "aff": "AIT, Austrian Institute of Technology, Donau-City-Strasse 1, 1220, Vienna, Austria; HCI, IWR at Heidelberg University, Berliner Strasse 43 D-69120 Heidelberg, Germany; AIT, Austrian Institute of Technology, Donau-City-Strasse 1, 1220, Vienna, Austria; AIT, Austrian Institute of Technology, Donau-City-Strasse 1, 1220, Vienna, Austria; AIT, Austrian Institute of Technology, Donau-City-Strasse 1, 1220, Vienna, Austria",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zendel_Analyzing_Computer_Vision_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2836796,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13263093416945973525&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ait.ac.at;iwr.uni-heidelberg.de;ait.ac.at;ait.ac.at;ait.ac.at",
        "email": "ait.ac.at;iwr.uni-heidelberg.de;ait.ac.at;ait.ac.at;ait.ac.at",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zendel_Analyzing_Computer_Vision_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Austrian Institute of Technology;Heidelberg University",
        "aff_unique_dep": ";IWR (Interdisciplinary Center for Scientific Computing)",
        "aff_unique_url": "https://www.ait.ac.at;https://www.uni-heidelberg.de",
        "aff_unique_abbr": "AIT;Uni HD",
        "aff_campus_unique_index": "0;1;0;0;0",
        "aff_campus_unique": "Vienna;Heidelberg",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "Austria;Germany"
    },
    {
        "title": "AnchorNet: A Weakly Supervised Network to Learn Geometry-Sensitive Features for Semantic Matching",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2210",
        "author_site": "David Novotny, Diane Larlus, Andrea Vedaldi",
        "author": "David Novotny; Diane Larlus; Andrea Vedaldi",
        "abstract": "Despite significant progress of deep learning in recent years, state-of-the-art semantic matching methods still rely on legacy features such as SIFT or HoG. We argue that the strong invariance properties that are key to the success of recent deep architectures on the classification task make them unfit for dense correspondence tasks, unless a large amount of supervision is used. In this work, we propose a deep network, termed AnchorNet, that produces image representations that are well-suited for semantic matching. It relies on a set of filters whose response is geometrically consistent across different object instances, even in the presence of strong intra-class, scale, or viewpoint variations. Trained only with weak image-level labels, the final representation successfully captures information about the object structure and improves results of state-of-the-art semantic matching methods such as the Deformable Spatial Pyramid or the Proposal Flow methods. We show positive results on the cross-instance matching task where different instances of the same object category are matched as well as on a new cross-category semantic matching task aligning pairs of instances each from a different object class.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Novotny_AnchorNet_A_Weakly_CVPR_2017_paper.pdf",
        "aff": "Visual Geometry Group, Dept. of Engineering Science, University of Oxford + Computer Vision Group, Xerox Research Centre Europe; Computer Vision Group, Xerox Research Centre Europe; Visual Geometry Group, Dept. of Engineering Science, University of Oxford",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Novotny_AnchorNet_A_Weakly_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.04749v1",
        "pdf_size": 5121396,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6300585515461937405&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "robots.ox.ac.uk;xrce.xerox.com;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;xrce.xerox.com;robots.ox.ac.uk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Novotny_AnchorNet_A_Weakly_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "University of Oxford;Xerox Research Centre Europe",
        "aff_unique_dep": "Dept. of Engineering Science;Computer Vision Group",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.xerox.com/research-centre-europe.html",
        "aff_unique_abbr": "Oxford;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oxford;",
        "aff_country_unique_index": "0+1;1;0",
        "aff_country_unique": "United Kingdom;Unknown"
    },
    {
        "title": "Annotating Object Instances With a Polygon-RNN",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Oral",
        "track": "main",
        "pid": "2187",
        "author_site": "Llu\u00c3\u00ads Castrej\u00c3\u00b3n, Kaustav Kundu, Raquel Urtasun, Sanja Fidler",
        "author": "Lluis Castrejon; Kaustav Kundu; Raquel Urtasun; Sanja Fidler",
        "abstract": "In this paper, we propose an approach for semi-automatic annotation of object instances. While most current methods treat object segmentation as a pixel-labeling problem, we here cast it as a polygon prediction task, mimicking how most current datasets have been annotated. In particular, our approach takes as input an image crop and produces a vertex of the polygon, one at a time, allowing the human annotator to interfere at any time and correct the point. Our model easily integrates any correction, producing as accurate segmentations as desired by the annotator. We show that our annotation method speeds up the annotation process by factor of 4.7 across all classes, while achieving 78.4% agreement in IoU with original ground-truth, matching the typical agreement between human annotators. For cars, our speed-up factor is even higher, at 7.3 for agreement of 82.2%. We further show generalization capabilities of our approach on unseen datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Castrejon_Annotating_Object_Instances_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.05548v1",
        "pdf_size": 1685465,
        "gs_citation": 450,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6253213508714647808&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Castrejon_Annotating_Object_Instances_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Anti-Glare: Tightly Constrained Optimization for Eyeglass Reflection Removal",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "450",
        "author_site": "Tushar Sandhan, Jin Young Choi",
        "author": "Tushar Sandhan; Jin Young Choi",
        "abstract": "Absence of a clear eye visibility not only degrades the aesthetic value of an entire face image but also creates difficulties in many computer vision tasks. Even mild reflections produce the undesired superpositions of visual information, whose decomposition into the background and reflection layers using a single image is a highly ill-posed problem. In this work, we enforce the tight constraints derived by thoroughly analysing the properties of an eyeglass reflection. In addition, our strategy regularizes gradients of the reflection layer to be highly sparse and proposes the facial symmetry prior via formulating a non-convex optimization scheme, which removes the reflections within a few iterations. Experiments on frontal face image inputs demonstrate the high quality reflection removal results and improvement of the iris detection rate.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sandhan_Anti-Glare_Tightly_Constrained_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=987247066910994504&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sandhan_Anti-Glare_Tightly_Constrained_CVPR_2017_paper.html"
    },
    {
        "title": "Are Large-Scale 3D Models Really Necessary for Accurate Visual Localization?",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "598",
        "author_site": "Torsten Sattler, Akihiko Torii, Josef Sivic, Marc Pollefeys, Hajime Taira, Masatoshi Okutomi, Tomas Pajdla",
        "author": "Torsten Sattler; Akihiko Torii; Josef Sivic; Marc Pollefeys; Hajime Taira; Masatoshi Okutomi; Tomas Pajdla",
        "abstract": "Accurate visual localization is a key technology for autonomous navigation. 3D structure-based methods employ 3D models of the scene to estimate the full 6DOF pose of a camera very accurately. However, constructing (and extending) large-scale 3D models is still a significant challenge. In contrast, 2D image retrieval-based methods only require a database of geo-tagged images, which is trivial to construct and to maintain. They are often considered inaccurate since they only approximate the positions of the cameras. Yet, the exact camera pose can theoretically be recovered when enough relevant database images are retrieved. In this paper, we demonstrate experimentally that large-scale 3D models are not strictly necessary for accurate visual localization. We create reference poses for a large and challenging urban dataset. Using these poses, we show that combining image-based methods with local reconstructions results in a pose accuracy similar to the state-of-the-art structure-based methods. Our results suggest that we might want to reconsider the current approach for accurate large-scale localization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sattler_Are_Large-Scale_3D_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, ETH Z\u00fcrich; Tokyo Institute of Technology; Inria; Microsoft, Redmond; Czech Technical University in Prague; Tokyo Institute of Technology; Czech Technical University in Prague",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5177386,
        "gs_citation": 257,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7898673693288404319&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sattler_Are_Large-Scale_3D_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;3;4;1;4",
        "aff_unique_norm": "ETH Zurich;Tokyo Institute of Technology;INRIA;Microsoft;Czech Technical University",
        "aff_unique_dep": "Department of Computer Science;;;Microsoft Corporation;",
        "aff_unique_url": "https://www.ethz.ch;https://www.titech.ac.jp;https://www.inria.fr;https://www.microsoft.com;https://www.ctu.cz",
        "aff_unique_abbr": "ETHZ;Titech;Inria;Microsoft;CTU",
        "aff_campus_unique_index": "1;2;2",
        "aff_campus_unique": ";Redmond;Prague",
        "aff_country_unique_index": "0;1;2;3;4;1;4",
        "aff_country_unique": "Switzerland;Japan;France;United States;Czech Republic"
    },
    {
        "title": "Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension",
        "session": "Machine Learning 4",
        "status": "Spotlight",
        "track": "main",
        "pid": "2059",
        "author_site": "Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, Hannaneh Hajishirzi",
        "author": "Aniruddha Kembhavi; Minjoon Seo; Dustin Schwenk; Jonghyun Choi; Ali Farhadi; Hannaneh Hajishirzi",
        "abstract": "We introduce the task of Multi-Modal Machine Comprehension (M3C), which aims at answering multimodal questions given a context of text, diagrams and images. We present the Textbook Question Answering (TQA) dataset that includes 1,076 lessons and 26,260 multi-modal questions, taken from middle school science curricula. Our analysis shows that a significant portion of questions require complex parsing of the text and the diagrams and reasoning, indicating that our dataset is more complex compared to previous machine comprehension and visual question answering datasets. We extend state-of-the-art methods for textual machine comprehension and visual question answering to the TQA dataset. Our experiments show that these models do not perform well on TQA. The presented dataset opens new challenges for research in question answering and reasoning across multiple modalities.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kembhavi_Are_You_Smarter_CVPR_2017_paper.pdf",
        "aff": "Allen Institute for Arti\ufb01cial Intelligence; University of Washington + Allen Institute for Arti\ufb01cial Intelligence; Allen Institute for Arti\ufb01cial Intelligence; Allen Institute for Arti\ufb01cial Intelligence; Allen Institute for Arti\ufb01cial Intelligence + University of Washington; University of Washington",
        "project": "http://textbookqa.org",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2186181,
        "gs_citation": 360,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8084164285801573966&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "allenai.org;washington.edu;allenai.org;allenai.org;allenai.org;washington.edu",
        "email": "allenai.org;washington.edu;allenai.org;allenai.org;allenai.org;washington.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kembhavi_Are_You_Smarter_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+0;0;0;0+1;1",
        "aff_unique_norm": "Allen Institute for Artificial Intelligence;University of Washington",
        "aff_unique_dep": "Artificial Intelligence;",
        "aff_unique_url": "https://allenai.org;https://www.washington.edu",
        "aff_unique_abbr": "AI2;UW",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ArtTrack: Articulated Multi-Person Tracking in the Wild",
        "session": "Analyzing Humans 1",
        "status": "Oral",
        "track": "main",
        "pid": "2880",
        "author_site": "Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin, Siyu Tang, Evgeny Levinkov, Bjoern Andres, Bernt Schiele",
        "author": "Eldar Insafutdinov; Mykhaylo Andriluka; Leonid Pishchulin; Siyu Tang; Evgeny Levinkov; Bjoern Andres; Bernt Schiele",
        "abstract": "In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos. Our starting point is a model that resembles existing architectures for single-frame pose estimation but is substantially faster. We achieve this in two ways: (1) by simplifying and sparsifying the body-part relationship graph and leveraging recent methods for faster inference, and (2) by offloading a substantial share of computation onto a feed-forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter. We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio-temporal grouping of such proposals. This allows to jointly solve the association problem for all people in the scene by propagating evidence from strong detections through time and enforcing constraints that each proposal can be assigned to one person only. We report results on a public \"MPII Human Pose\" benchmark and on a new \"MPII Video Pose\" dataset of image sequences with multiple people. We demonstrate that our model achieves state-of-the-art results while using only a fraction of time and is able to leverage temporal information to improve state-of-the-art for crowded scenes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Insafutdinov_ArtTrack_Articulated_Multi-Person_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Insafutdinov_ArtTrack_Articulated_Multi-Person_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.01465v3",
        "pdf_size": 1347133,
        "gs_citation": 379,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12982390445291092651&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Insafutdinov_ArtTrack_Articulated_Multi-Person_CVPR_2017_paper.html"
    },
    {
        "title": "Asymmetric Feature Maps With Application to Sketch Based Retrieval",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "876",
        "author_site": "Giorgos Tolias, Ond\u00c5\u0099ej Chum",
        "author": "Giorgos Tolias; Ondrej Chum",
        "abstract": "We propose a novel concept of asymmetric feature maps (AFM), which allows to evaluate multiple kernels between a query and database entries without increasing the memory requirements. To demonstrate the advantages of the AFM method, we derive a short vector image representation that, due to asymmetric feature maps, supports efficient scale and translation invariant sketch-based image retrieval. Unlike most of the short-code based retrieval systems, the proposed method provides the query localization in the retrieved image. The efficiency of the search is boosted by approximating a 2D translation search via trigonometric polynomial of scores by 1D projections. The projections are a special case of AFM. An order of magnitude speed-up is achieved compared to traditional trigonometric polynomials. The results are boosted by an image-based average query expansion, exceeding significantly the state of the art on standard benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tolias_Asymmetric_Feature_Maps_CVPR_2017_paper.pdf",
        "aff": "Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague; Visual Recognition Group, Faculty of Electrical Engineering, Czech Technical University in Prague",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03946v1",
        "pdf_size": 2925445,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8663633209328451111&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cmp.felk.cvut.cz;cmp.felk.cvut.cz",
        "email": "cmp.felk.cvut.cz;cmp.felk.cvut.cz",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tolias_Asymmetric_Feature_Maps_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Czech Technical University in Prague",
        "aff_unique_dep": "Faculty of Electrical Engineering",
        "aff_unique_url": "https://www.cvut.cz",
        "aff_unique_abbr": "CTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Prague",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Czech Republic"
    },
    {
        "title": "Asynchronous Temporal Fields for Action Recognition",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "186",
        "author_site": "Gunnar A. Sigurdsson, Santosh Divvala, Ali Farhadi, Abhinav Gupta",
        "author": "Gunnar A. Sigurdsson; Santosh Divvala; Ali Farhadi; Abhinav Gupta",
        "abstract": "Actions are more than just movements and trajectories: we cook to eat and we hold a cup to drink from it. A thorough understanding of videos requires going beyond appearance modeling and necessitates reasoning about the sequence of activities, as well as the higher-level constructs such as intentions. But how do we model and reason about these? We propose a fully-connected temporal CRF model for reasoning over various aspects of activities that includes objects, actions, and intentions, where the potentials are predicted by a deep network. End-to-end training of such structured models is a challenging endeavor: For inference and learning we need to construct mini-batches consisting of whole videos, leading to mini-batches with only a few videos. This causes high-correlation between data points leading to breakdown of the backprop algorithm. To address this challenge, we present an asynchronous variational inference method that allows efficient end-to-end training. Our method achieves a classification mAP of 22.4% on the Charades benchmark, outperforming the state-of-the-art (17.2% mAP), and offers equal gains on the task of temporal localization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sigurdsson_Asynchronous_Temporal_Fields_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Sigurdsson_Asynchronous_Temporal_Fields_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.06371",
        "pdf_size": 1171356,
        "gs_citation": 212,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12711228213462622985&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sigurdsson_Asynchronous_Temporal_Fields_CVPR_2017_paper.html"
    },
    {
        "title": "Attend in Groups: A Weakly-Supervised Deep Learning Framework for Learning From Web Data",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "700",
        "author_site": "Bohan Zhuang, Lingqiao Liu, Yao Li, Chunhua Shen, Ian Reid",
        "author": "Bohan Zhuang; Lingqiao Liu; Yao Li; Chunhua Shen; Ian Reid",
        "abstract": "Large-scale datasets have driven the rapid development of deep neural networks for visual recognition. However, annotating a massive dataset is expensive and time-consuming. Web images and their labels are, in comparison, much easier to obtain, but direct training on such automatially harvested images can lead to unsatisfactory performance, because the noisy labels of Web images adversely affect the learned recognition models. To address this drawback we propose an end-to-end weakly-supervised deep learning framework which is robust to the label noise in Web images. The proposed framework relies on two unified strategies -- random grouping and attention -- to effectively reduce the negative impact of noisy web image annotations.  Specifically, random grouping stacks multiple images into a single training instance and thus increases the labeling accuracy at the instance level. Attention, on the other hand, suppresses the noisy signals from both incorrectly labeled images and less discriminative image regions.  By conducting intensive experiments on two challenging datasets, including a newly collected fine-grained dataset with Web images of different car models, the superior performance of the proposed methods over competitive baselines is clearly demonstrated.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhuang_Attend_in_Groups_CVPR_2017_paper.pdf",
        "aff": "The University of Adelaide, Australia; Australian Centre for Robotic Vision; The University of Adelaide, Australia; Australian Centre for Robotic Vision; The University of Adelaide, Australia",
        "project": "https://bitbucket.org/jingruixiaozhuang/cvpr2017_code_dataset/",
        "github": "",
        "supp": "",
        "arxiv": "1611.09960v1",
        "pdf_size": 981728,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10611962062565773345&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhuang_Attend_in_Groups_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "University of Adelaide;Australian Centre for Robotic Vision",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.adelaide.edu.au;https://roboticvision.org/",
        "aff_unique_abbr": "Adelaide;ACRV",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Attend to You: Personalized Image Captioning With Context Sequence Memory Networks",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "293",
        "author_site": "Cesc Chunseong Park, Byeongchang Kim, Gunhee Kim",
        "author": "Cesc Chunseong Park; Byeongchang Kim; Gunhee Kim",
        "abstract": "We address personalization issues of image captioning, which have not been discussed yet in previous research. For a query image, we aim to generate a descriptive sentence, accounting for prior knowledge such as the user's active vocabularies in previous documents. As applications of personalized image captioning, we tackle two post automation tasks: hashtag prediction and post generation, on our newly collected Instagram dataset, consisting of 1.1M posts from 6.3K users. We propose a novel captioning model named Context Sequence Memory Network (CSMN). Its unique updates over previous memory network models include (i) exploiting memory as a repository for multiple types of context information, (ii) appending previously generated words into memory to capture long-term information without suffering from the vanishing gradient problem, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots for better context understanding. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the effectiveness of the three novel features of CSMN and its performance enhancement for personalized image captioning over state-of-the-art captioning models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Park_Attend_to_You_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.06485",
        "gs_citation": 228,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11114926943127674156&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Park_Attend_to_You_CVPR_2017_paper.html"
    },
    {
        "title": "Attention-Aware Face Hallucination via Deep Reinforcement Learning",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "213",
        "author_site": "Qingxing Cao, Liang Lin, Yukai Shi, Xiaodan Liang, Guanbin Li",
        "author": "Qingxing Cao; Liang Lin; Yukai Shi; Xiaodan Liang; Guanbin Li",
        "abstract": "Face hallucination is a domain-specific super-resolution problem with the goal to generate high-resolution (HR) faces from low-resolution (LR) input images. In contrast to existing methods that often learn a single patch-to-patch mapping from LR to HR images and are regardless of the contextual interdependency between patches, we propose a novel Attention-aware Face Hallucination (Attention-FH) framework which resorts to deep reinforcement learning for sequentially discovering attended patches and then performing the facial part enhancement by fully exploiting the global interdependency of the image. Specifically, in each time step, the recurrent policy network is proposed to dynamically specify a new attended region by incorporating what happened in the past. The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our approach significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations.The state (i.e., face hallucination result for the whole image) can thus be exploited and updated by the local enhancement network on the selected region. The Attention-FH approach jointly learns the recurrent policy network and local enhancement network through maximizing the long-term reward that reflects the hallucination performance over the whole image. Therefore, our proposed Attention-FH is capable of adaptively personalizing an optimal searching path for each face image according to its own characteristic. Extensive experiments show our approach significantly surpasses the state-of-the-arts on in-the-wild faces with large pose and illumination variations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cao_Attention-Aware_Face_Hallucination_CVPR_2017_paper.pdf",
        "aff": "School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1708.03132v1",
        "pdf_size": 1067715,
        "gs_citation": 249,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2553223462943257945&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "mail2.sysu.edu.cn;ieee.org;mail2.sysu.edu.cn;gmail.com;mail.sysu.edu.cn",
        "email": "mail2.sysu.edu.cn;ieee.org;mail2.sysu.edu.cn;gmail.com;mail.sysu.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cao_Attention-Aware_Face_Hallucination_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Sun Yat-sen University",
        "aff_unique_dep": "School of Data and Computer Science",
        "aff_unique_url": "http://www.sysu.edu.cn",
        "aff_unique_abbr": "SYSU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Guangzhou",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Attentional Correlation Filter Network for Adaptive Visual Tracking",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "1996",
        "author_site": "Jongwon Choi, Hyung Jin Chang, Sangdoo Yun, Tobias Fischer, Yiannis Demiris, Jin Young Choi",
        "author": "Jongwon Choi; Hyung Jin Chang; Sangdoo Yun; Tobias Fischer; Yiannis Demiris; Jin Young Choi",
        "abstract": "We propose a new tracking framework with an attentional mechanism that chooses a subset of the associated correlation filters for increased robustness and computational efficiency. The subset of filters is adaptively selected by a deep attentional network according to the dynamic properties of the tracking target. Our contributions are manifold, and are summarised as follows: (i) Introducing the Attentional Correlation Filter Network which allows adaptive tracking of dynamic targets. (ii) Utilising an attentional network which shifts the attention to the best candidate modules, as well as predicting the estimated accuracy of currently inactive modules. (iii) Enlarging the variety of correlation filters which cover target drift, blurriness, occlusion, scale changes, and flexible aspect ratio. (iv) Validating the robustness and efficiency of the attentional mechanism for visual tracking through a number of experiments. Our method achieves similar performance to non real-time trackers, and state-of-the-art performance amongst real-time trackers.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Choi_Attentional_Correlation_Filter_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Choi_Attentional_Correlation_Filter_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 388,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7048582756083604217&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Choi_Attentional_Correlation_Filter_CVPR_2017_paper.html"
    },
    {
        "title": "Attentional Push: A Deep Convolutional Network for Augmenting Image Salience With Shared Attention Modeling in Social Scenes",
        "session": "Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "911",
        "author_site": "Siavash Gorji, James J. Clark",
        "author": "Siavash Gorji; James J. Clark",
        "abstract": "We present a novel visual attention tracking technique based on Shared Attention modeling. By considering the viewer as a participant in the activity occurring in the scene, our model learns the loci of attention of the scene actors and use it to augment image salience. We go beyond image salience and instead of only computing the power of image regions to pull attention, we also consider the strength with which the scene actors push attention to the region in question, thus the term Attentional Push. We present a convolutional neural network (CNN) which augments standard saliency models with Attentional Push. Our model contains two pathways: an Attentional Push pathway which learns the gaze location of the scene actors and a saliency pathway. These are followed by a shallow augmented saliency CNN which combines them and generates the augmented saliency. For training, we use transfer learning to initialize and train the Attentional Push CNN by minimizing the classification error of following the actors' gaze location on a 2-D grid using a large-scale gaze-following dataset. The Attentional Push CNN is then fine-tuned along with the augmented saliency CNN to minimize the Euclidean distance between the augmented saliency and ground truth fixations using an eye-tracking dataset, annotated with the head and the gaze location of the scene actors. We evaluate our model on three challenging eye fixation datasets, SALICON, iSUN and CAT2000, and illustrate significant improvements in predicting viewers' fixations in social scenes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gorji_Attentional_Push_A_CVPR_2017_paper.pdf",
        "aff": "Centre for Intelligent Machines, Department of Electrical and Computer Engineering, McGill University; Centre for Intelligent Machines, Department of Electrical and Computer Engineering, McGill University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1607054,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=697369841555558230&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cim.mcgill.ca;cim.mcgill.ca",
        "email": "cim.mcgill.ca;cim.mcgill.ca",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gorji_Attentional_Push_A_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "McGill University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.mcgill.ca",
        "aff_unique_abbr": "McGill",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Automatic Discovery, Association Estimation and Learning of Semantic Attributes for a Thousand Categories",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "191",
        "author_site": "Ziad Al-Halah, Rainer Stiefelhagen",
        "author": "Ziad Al-Halah; Rainer Stiefelhagen",
        "abstract": "Attribute-based recognition models, due to their impressive performance and their ability to generalize well on novel categories, have been widely adopted for many computer vision applications. However, usually both the attribute vocabulary and the class-attribute associations have to be provided manually by domain experts or large number of annotators. This is very costly and not necessarily optimal regarding recognition performance, and most importantly, it limits the applicability of attribute-based models to large scale data sets. To tackle this problem, we propose an end-to-end unsupervised attribute learning approach. We utilize online text corpora to automatically discover a salient and discriminative vocabulary that correlates well with the human concept of semantic attributes. Moreover, we propose a deep convolutional model to optimize class-attribute associations with a linguistic prior that accounts for noise and missing data in text. In a thorough evaluation on ImageNet, we demonstrate that our model is able to efficiently discover and learn semantic attributes at a large scale. Furthermore, we demonstrate that our model outperforms the state-of-the-art in zero-shot learning on three data sets: ImageNet, Animals with Attributes and aPascal/aYahoo. Finally, we enable attribute-based learning on ImageNet and will share the attributes and associations for future research.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Al-Halah_Automatic_Discovery_Association_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Al-Halah_Automatic_Discovery_Association_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15495580389026584653&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Al-Halah_Automatic_Discovery_Association_CVPR_2017_paper.html"
    },
    {
        "title": "Automatic Understanding of Image and Video Advertisements",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Spotlight",
        "track": "main",
        "pid": "617",
        "author_site": "Zaeem Hussain, Mingda Zhang, Xiaozhong Zhang, Keren Ye, Christopher Thomas, Zuha Agha, Nathan Ong, Adriana Kovashka",
        "author": "Zaeem Hussain; Mingda Zhang; Xiaozhong Zhang; Keren Ye; Christopher Thomas; Zuha Agha; Nathan Ong; Adriana Kovashka",
        "abstract": "There is more to images than their objective physical content: for example, advertisements are created to persuade a viewer to take a certain action. We propose the novel problem of automatic advertisement understanding. To enable research on this problem, we create two datasets: an image dataset of 64,832 image ads, and a video dataset of 3,477 ads. Our data contains rich annotations encompassing the topic and sentiment of the ads, questions and answers describing what actions the viewer is prompted to take and the reasoning that the ad presents to persuade the viewer (\"What should I do according to this ad, and why should I do it?\"), and symbolic references ads make (e.g. a dove symbolizes peace). We also analyze the most common persuasive strategies ads use, and the capabilities that computer vision systems should have to understand these strategies. We present baseline classification results for several prediction tasks, including automatically answering questions about the messages of the ads.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hussain_Automatic_Understanding_of_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science; Department of Computer Science; Department of Computer Science; Department of Computer Science; Department of Computer Science; Department of Computer Science; Department of Computer Science; Department of Computer Science",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Hussain_Automatic_Understanding_of_2017_CVPR_supplemental.pdf",
        "arxiv": "1707.03067v1",
        "pdf_size": 1196471,
        "gs_citation": 219,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3797826603619786452&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.pitt.edu;cs.pitt.edu;cs.pitt.edu;cs.pitt.edu;cs.pitt.edu;cs.pitt.edu;cs.pitt.edu;cs.pitt.edu",
        "email": "cs.pitt.edu;cs.pitt.edu;cs.pitt.edu;cs.pitt.edu;cs.pitt.edu;cs.pitt.edu;cs.pitt.edu;cs.pitt.edu",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hussain_Automatic_Understanding_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Unknown Institution",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Awesome Typography: Statistics-Based Text Effects Transfer",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3867",
        "author_site": "Shuai Yang, Jiaying Liu, Zhouhui Lian, Zongming Guo",
        "author": "Shuai Yang; Jiaying Liu; Zhouhui Lian; Zongming Guo",
        "abstract": "In this work, we explore the problem of generating fantastic special-effects for the typography. It is quite challenging due to the model diversities to illustrate varied text effects for different characters. To address this issue, our key idea is to exploit the analytics on the high regularity of the spatial distribution for text effects to guide the synthesis process. Specifically, we characterize the stylized patches by their normalized positions and the optimal scales to depict their style elements. Our method first estimates these two features and derives their correlation statistically. They are then converted into soft constraints for texture transfer to accomplish adaptive multi-scale texture synthesis and to make style element distribution uniform. It allows our algorithm to produce artistic typography that fits for both local texture patterns and the global spatial distribution in the example. Experimental results demonstrate the superiority of our method for various text effects over conventional style transfer methods. In addition, we validate the effectiveness of our algorithm with extensive artistic typography library generation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_Awesome_Typography_Statistics-Based_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yang_Awesome_Typography_Statistics-Based_2017_CVPR_supplemental.zip",
        "arxiv": "1611.09026v2",
        "pdf_size": 2619970,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14411710086111458752&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Awesome_Typography_Statistics-Based_CVPR_2017_paper.html"
    },
    {
        "title": "BIND: Binary Integrated Net Descriptors for Texture-Less Object Recognition",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "757",
        "author_site": "Jacob Chan, Jimmy Addison Lee, Qian Kemao",
        "author": "Jacob Chan; Jimmy Addison Lee; Qian Kemao",
        "abstract": "This paper presents BIND (Binary Integrated Net Descriptor), a texture-less object detector that encodes multi-layered binary-represented nets for high precision edge-based description. Our proposed concept aligns layers of object-sized patches (nets) onto highly fragmented occlusion resistant line-segment midpoints (linelets) to encode regional information into efficient binary strings. These lightweight nets encourage discriminative object description through their high-spatial resolution, enabling highly precise encoding of the object's edges and internal texture-less information. BIND achieved various invariant properties such as rotation, scale and edge-polarity through its unique binary logical-operated encoding and matching techniques, while performing remarkably well in occlusion and clutter. Apart from yielding efficient computational performance, BIND also attained remarkable recognition rates surpassing recent state-of-the-art texture-less object detectors such as BORDER, BOLD and LINE2D.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chan_BIND_Binary_Integrated_CVPR_2017_paper.pdf",
        "aff": "School of Computer Engineering (SCE), Nanyang Technological University; Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR); School of Computer Engineering (SCE), Nanyang Technological University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3041760,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3621585064999228464&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "ntu.edu.sg;i2r.a-star.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;i2r.a-star.edu.sg;ntu.edu.sg",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chan_BIND_Binary_Integrated_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nanyang Technological University;",
        "aff_unique_dep": "School of Computer Engineering;",
        "aff_unique_url": "https://www.ntu.edu.sg;",
        "aff_unique_abbr": "NTU;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "NTU;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore;"
    },
    {
        "title": "BRISKS: Binary Features for Spherical Images on a Geodesic Grid",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1843",
        "author_site": "Hao Guan, William A. P. Smith",
        "author": "Hao Guan; William A. P. Smith",
        "abstract": "In this paper, we develop an interest point detector and binary feature descriptor for spherical images. We take as inspiration a recent framework developed for planar images, BRISK (Binary Robust Invariant Scalable Keypoints), and adapt the method to operate on spherical images. All of our processing is intrinsic to the sphere and avoids the distortion inherent in storing and indexing spherical images in a 2D representation. We discretise images on a spherical geodesic grid formed by recursive subdivision of a triangular mesh. This leads to a multiscale pixel grid comprising mainly hexagonal pixels that lends itself naturally to a spherical image pyramid representation. For interest point detection, we use a variant of the Accelerated Segment Test (AST) corner detector which operates on our geodesic grid. We estimate a continuous scale and location for features and descriptors are built by sampling onto a regular pattern in the tangent space. We evaluate repeatability, precision and recall on both synthetic spherical images with known ground truth correspondences and real images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Guan_BRISKS_Binary_Features_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, The University of York, UK; Department of Computer Science, The University of York, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7213677,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8672323836839744632&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "york.ac.uk;york.ac.uk",
        "email": "york.ac.uk;york.ac.uk",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Guan_BRISKS_Binary_Features_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of York",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.york.ac.uk",
        "aff_unique_abbr": "York",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Bayesian Supervised Hashing",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "2812",
        "author_site": "Zihao Hu, Junxuan Chen, Hongtao Lu, Tongzhen Zhang",
        "author": "Zihao Hu; Junxuan Chen; Hongtao Lu; Tongzhen Zhang",
        "abstract": "Among learning based hashing methods, supervised hashing seeks compact binary representation of the training data to preserve semantic similarities. Recent years have witnessed various problem formulations and optimization methods for supervised hashing. Most of them optimize a form of loss function with a regulization term, which can be viewed as a maximum a posterior (MAP) estimation of the hashing codes. However, these approaches are prone to overfitting unless hyperparameters are tuned carefully. To address this problem, we present a novel fully Bayesian treatment for supervised hashing problem, named Bayesian Supervised Hashing (BSH), in which hyperparameters are automatically tuned during optimization. Additionally, by utilizing automatic relevance determination (ARD), we can figure out relative discriminating ability of different hashing bits and select most informative bits among them. Experimental results on three real-world image datasets with semantic information show that BSH can achieve superior performance over state-of-the-art methods with comparable training time.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hu_Bayesian_Supervised_Hashing_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 798762,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16769536642529761325&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hu_Bayesian_Supervised_Hashing_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Benchmarking Denoising Algorithms With Real Photographs",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "585",
        "author_site": "Tobias Pl\u00c3\u00b6tz, Stefan Roth",
        "author": "Tobias Plotz; Stefan Roth",
        "abstract": "Lacking realistic ground truth data, image denoising techniques are traditionally evaluated on images corrupted by synthesized i.i.d. Gaussian noise. We aim to obviate this unrealistic setting by developing a methodology for benchmarking denoising techniques on real photographs. We capture pairs of images with different ISO values and appropriately adjusted exposure times, where the nearly noise-free low-ISO image serves as reference. To derive the ground truth, careful post-processing is needed. We correct spatial misalignment, cope with inaccuracies in the exposure parameters through a linear intensity transform based on a novel heteroscedastic Tobit regression model, and remove residual low-frequency bias that stems, e.g., from minor illumination changes. We then capture a novel benchmark dataset, the Darmstadt Noise Dataset (DND), with consumer cameras of differing sensor sizes. One interesting finding is that various recent techniques that perform well on synthetic noise are clearly outperformed by BM3D on photographs with real noise. Our benchmark delineates realistic evaluation scenarios that deviate strongly from those commonly used in the scientific literature.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Plotz_Benchmarking_Denoising_Algorithms_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, TU Darmstadt; Department of Computer Science, TU Darmstadt",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Plotz_Benchmarking_Denoising_Algorithms_2017_CVPR_supplemental.pdf",
        "arxiv": "1707.01313v1",
        "pdf_size": 2824996,
        "gs_citation": 714,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16041111009935081030&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Plotz_Benchmarking_Denoising_Algorithms_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technische Universit\u00e4t Darmstadt",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.tu-darmstadt.de",
        "aff_unique_abbr": "TU Darmstadt",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Darmstadt",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Beyond Instance-Level Image Retrieval: Leveraging Captions to Learn a Global Visual Representation for Semantic Retrieval",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2972",
        "author_site": "Albert Gordo, Diane Larlus",
        "author": "Albert Gordo; Diane Larlus",
        "abstract": "Querying with an example image is a simple and intuitive interface to retrieve information from a visual database. Most of the research in image retrieval has focused on the task of instance-level image retrieval, where the goal is to retrieve images that contain the same object instance as the query image. In this work we move beyond instance-level retrieval and consider the task of semantic image retrieval in complex scenes, where the goal is to retrieve images that share the same semantics as the query image. We show that, despite its subjective nature, the task of semantically ranking visual scenes is consistently implemented across a pool of human annotators. We also show that a similarity based on human-annotated region-level captions is highly correlated with the human ranking and constitutes a good computable surrogate. Following this observation, we learn a visual embedding of the images where the similarity in the visual space is correlated with their semantic similarity surrogate. We further extend our model to learn a joint embedding of visual and textual cues that allows one to query the database using a text modifier in addition to the query image, adapting the results to the modifier. Finally, our model can ground the ranking decisions by showing regions that contributed the most to the similarity between pairs of images, providing a visual explanation of the similarity.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gordo_Beyond_Instance-Level_Image_CVPR_2017_paper.pdf",
        "aff": "Computer Vision group, Xerox Research Center Europe; Computer Vision group, Xerox Research Center Europe",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4474630,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13228381399280346132&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "xrce.xerox.com;xrce.xerox.com",
        "email": "xrce.xerox.com;xrce.xerox.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gordo_Beyond_Instance-Level_Image_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Xerox Research Center Europe",
        "aff_unique_dep": "Computer Vision group",
        "aff_unique_url": "https://www.xerox.com/research-centers/europe.html",
        "aff_unique_abbr": "XRC Europe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Unknown"
    },
    {
        "title": "Beyond Triplet Loss: A Deep Quadruplet Network for Person Re-Identification",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Spotlight",
        "track": "main",
        "pid": "140",
        "author_site": "Weihua Chen, Xiaotang Chen, Jianguo Zhang, Kaiqi Huang",
        "author": "Weihua Chen; Xiaotang Chen; Jianguo Zhang; Kaiqi Huang",
        "abstract": "Person re-identification (ReID) is an important task in wide area video surveillance which focuses on identifying people across different cameras. Recently, deep learning networks with a triplet loss become a common framework for person ReID. However, the triplet loss pays main attentions on obtaining correct orders on the training set. It still suffers from a weaker generalization capability from the training set to the testing set, thus resulting in inferior performance. In this paper, we design a quadruplet loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve a higher performance on the testing set. In particular, a quadruplet deep network using a margin-based online hard negative mining is proposed based on the quadruplet loss for the person ReID. In extensive experiments, the proposed network outperforms most of the state-of-the-art algorithms on representative datasets which clearly demonstrates the effectiveness of our proposed method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Beyond_Triplet_Loss_CVPR_2017_paper.pdf",
        "aff": "CRIPAC&NLPR, CASIA+University of Chinese Academy of Sciences; CRIPAC&NLPR, CASIA+University of Chinese Academy of Sciences; Computing, School of Science and Engineering, University of Dundee, United Kingdom; CRIPAC&NLPR, CASIA+University of Chinese Academy of Sciences+CAS Center for Excellence in Brain Science and Intelligence Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.01719v1",
        "pdf_size": 1149805,
        "gs_citation": 1545,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15352923940543802650&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;dundee.ac.uk;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;dundee.ac.uk;nlpr.ia.ac.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Beyond_Triplet_Loss_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;2;0+1+3",
        "aff_unique_norm": "Chinese Academy of Sciences Institute of Automation;University of Chinese Academy of Sciences;University of Dundee;Chinese Academy of Sciences",
        "aff_unique_dep": "CRIPAC (Computational Intelligence & Pattern Analysis Group) & NLPR (National Laboratory of Pattern Recognition);;School of Science and Engineering;Center for Excellence in Brain Science and Intelligence Technology",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;https://www.dundee.ac.uk;http://www.cas.cn/",
        "aff_unique_abbr": "CASIA;UCAS;Dundee;CAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;1;0+0+0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "title": "Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence Models for Fill-In-The-Blank Image Captioning",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3219",
        "author_site": "Qing Sun, Stefan Lee, Dhruv Batra",
        "author": "Qing Sun; Stefan Lee; Dhruv Batra",
        "abstract": "We develop the first approximate inference algorithm for 1-Best (and M-Best) decoding in bidirectional neural sequence models by extending Beam Search (BS) to reason about both forward and backward time dependencies.   Beam Search (BS) is a widely used approximate inference algorithm for decoding sequences from unidirectional neural sequence models. Interestingly, approximate inference in bidirectional models remains an open problem, despite their significant advantage in modeling information from both the past and future. To enable the use of bidirectional models, we present Bidirectional Beam Search (BiBS), an efficient algorithm for approximate bidirectional inference.  To evaluate our method and as an interesting problem in its own right, we introduce a novel Fill-in-the-Blank Image Captioning task which requires reasoning about both past and future sentence structure to reconstruct sensible image descriptions.  We use this task as well as the Visual Madlibs dataset to demonstrate the effectiveness of our approach, consistently outperforming all baseline methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sun_Bidirectional_Beam_Search_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Sun_Bidirectional_Beam_Search_2017_CVPR_supplemental.pdf",
        "arxiv": "1705.08759v1",
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12998925937962222768&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sun_Bidirectional_Beam_Search_CVPR_2017_paper.html"
    },
    {
        "title": "Bidirectional Multirate Reconstruction for Temporal Modeling in Videos",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Spotlight",
        "track": "main",
        "pid": "959",
        "author_site": "Linchao Zhu, Zhongwen Xu, Yi Yang",
        "author": "Linchao Zhu; Zhongwen Xu; Yi Yang",
        "abstract": "Despite the recent success of neural networks in image feature learning, a major problem in the video domain is the lack of sufficient labeled data for learning to model temporal information. In this paper, we propose an unsupervised temporal modeling method that learns from untrimmed videos. The speed of motion varies constantly, e.g., a man may run quickly or slowly. We therefore train a Multirate Visual Recurrent Model (MVRM) by encoding frames of a clip with different intervals. This learning process makes the learned model more capable of dealing with motion speed variance. Given a clip sampled from a video, we use its past and future neighboring clips as the temporal context, and reconstruct the two temporal transitions, i.e., present->past transition and present->future transition, reflecting the temporal information in different views. The proposed method exploits the two transitions simultaneously by incorporating a bidirectional reconstruction which consists of a backward reconstruction and a forward reconstruction. We apply the proposed method to two challenging video tasks, i.e., complex event detection and video captioning, in which it achieves state-of-the-art performance. Notably, our method generates the best single feature for event detection with a relative improvement of 10.4% on the MEDTest-13 dataset and achieves the best performance in video captioning across all evaluation metrics on the YouTube2Text dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Bidirectional_Multirate_Reconstruction_CVPR_2017_paper.pdf",
        "aff": "CAI, University of Technology Sydney; CAI, University of Technology Sydney; CAI, University of Technology Sydney",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.09053v1",
        "pdf_size": 2118317,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5569714723986563504&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhu_Bidirectional_Multirate_Reconstruction_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Technology Sydney",
        "aff_unique_dep": "CAI",
        "aff_unique_url": "https://www.uts.edu.au",
        "aff_unique_abbr": "UTS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "BigHand2.2M Benchmark: Hand Pose Dataset and State of the Art Analysis",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2015",
        "author_site": "Shanxin Yuan, Qi Ye, Bj\u00c3\u00b6rn Stenger, Siddhant Jain, Tae-Kyun Kim",
        "author": "Shanxin Yuan; Qi Ye; Bjorn Stenger; Siddhant Jain; Tae-Kyun Kim",
        "abstract": "In this paper we introduce a large-scale hand pose dataset, collected using a novel capture method. Existing datasets are either generated synthetically or captured using depth sensors: synthetic datasets exhibit a certain level of appearance difference from real depth images, and real datasets are limited in quantity and coverage, mainly due to the difficulty to annotate them. We propose a tracking system with six 6D magnetic sensors and inverse kinematics to automatically obtain 21-joints hand pose annotations of depth maps captured with minimal restriction on the range of motion. The capture protocol aims to fully cover the natural hand pose space. As shown in embedding plots, the new dataset exhibits a significantly wider and denser range of hand poses compared to existing benchmarks. Current state-of-the-art methods are evaluated on the dataset, and we demonstrate significant improvements in cross-benchmark performance. We also show significant improvements in egocentric hand pose estimation with a CNN trained on the new dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yuan_BigHand2.2M_Benchmark_Hand_CVPR_2017_paper.pdf",
        "aff": "Imperial College London; Imperial College London; Rakuten Institute of Technology; IIT Jodhpur; Imperial College London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3148368,
        "gs_citation": 329,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11679021347733866270&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yuan_BigHand2.2M_Benchmark_Hand_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Imperial College London;Rakuten Institute of Technology;Indian Institute of Technology Jodhpur",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.imperial.ac.uk;https://rit.rakuten.com;https://www.iitj.ac.in",
        "aff_unique_abbr": "ICL;RIT;IITJ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2;0",
        "aff_country_unique": "United Kingdom;Japan;India"
    },
    {
        "title": "Binarized Mode Seeking for Scalable Visual Pattern Discovery",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "1500",
        "author_site": "Wei Zhang, Xiaochun Cao, Rui Wang, Yuanfang Guo, Zhineng Chen",
        "author": "Wei Zhang; Xiaochun Cao; Rui Wang; Yuanfang Guo; Zhineng Chen",
        "abstract": "This paper studies visual pattern discovery in large-scale image collections via binarized mode seeking, where images can only be represented as binary codes for efficient storage and computation. We address this problem from the perspective of binary space mode seeking. First, a binary mean shift (bMS) is proposed to discover frequent patterns via mode seeking directly in binary space. The binomial-based kernel and binary constraint are introduced for binarized analysis. Second, we further extend bMS to a more general form, namely contrastive binary mean shift (cbMS), which maximizes the contrastive density in binary space, for finding informative patterns that are both frequent and discriminative for the dataset. With the binarized algorithm and optimization, our methods demonstrate significant computation (50X) and storage (32X) improvement compared to standard techniques operating in Euclidean space, while the performance does not largely degenerate. Furthermore, cbMS discovers more informative patterns by suppressing low discriminative modes. We evaluate our methods on both annotated ILSVRC (1M images) and un-annotated blind Flickr (10M images) datasets with million scale images, which demonstrates both the scalability and effectiveness of our algorithms for discovering frequent and informative patterns in large scale collection.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Binarized_Mode_Seeking_CVPR_2017_paper.pdf",
        "aff": "SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences; SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences; SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences; SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences; Institute of Automation, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1212664,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3037266990141535236&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;ia.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;ia.ac.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Binarized_Mode_Seeking_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.iiis.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Binary Coding for Partial Action Analysis With Limited Observation Ratios",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "56",
        "author_site": "Jie Qin, Li Liu, Ling Shao, Bingbing Ni, Chen Chen, Fumin Shen, Yunhong Wang",
        "author": "Jie Qin; Li Liu; Ling Shao; Bingbing Ni; Chen Chen; Fumin Shen; Yunhong Wang",
        "abstract": "Traditional action recognition methods aim to recognize actions with complete observations/executions. However, it is often difficult to capture fully executed actions due to occlusions, interruptions, etc. Meanwhile, action prediction/recognition in advance based on partial observations is essential for preventing the situation from deteriorating. Besides, fast spotting human activities using partially observed data is a critical ingredient for retrieval systems. Inspired by the recent success of data binarization in efficient retrieval/recognition, we propose a novel approach, named Partial Reconstructive Binary Coding (PRBC), for action analysis based on limited frame glimpses during any period of the complete execution. Specifically, we learn discriminative compact binary codes for partial actions via a joint learning framework, which collaboratively tackles feature reconstruction as well as binary coding. We obtain the solution to PRBC based on a discrete alternating iteration algorithm. Extensive experiments on four realistic action datasets in terms of three tasks (i.e., partial action retrieval, recognition and prediction) clearly show the superiority of PRBC over the state-of-the-art methods, along with significantly reduced memory load and computational costs during the online test.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Qin_Binary_Coding_for_CVPR_2017_paper.pdf",
        "aff": "Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University+State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; Malong Technologies Co., Ltd.+University of East Anglia; University of East Anglia; Shanghai Jiao Tong University; Center for Research in Computer Vision, University of Central Florida; University of Electronic Science and Technology of China; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University+State Key Laboratory of Virtual Reality Technology and Systems, Beihang University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2350809,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9864081085631842928&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;malongtech.cn;ieee.org;sjtu.edu.cn;gmail.com;gmail.com;buaa.edu.cn",
        "email": "gmail.com;malongtech.cn;ieee.org;sjtu.edu.cn;gmail.com;gmail.com;buaa.edu.cn",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Qin_Binary_Coding_for_CVPR_2017_paper.html",
        "aff_unique_index": "0+0;1+2;2;3;4;5;0+0",
        "aff_unique_norm": "Beihang University;Malong Technologies;University of East Anglia;Shanghai Jiao Tong University;University of Central Florida;University of Electronic Science and Technology of China",
        "aff_unique_dep": "Beijing Advanced Innovation Center for Big Data and Brain Computing;;;;Center for Research in Computer Vision;",
        "aff_unique_url": "http://www.buaa.edu.cn/;https://www.malong.com/;https://www.uea.ac.uk;https://www.sjtu.edu.cn;https://www.ucf.edu;https://www.uestc.edu.cn",
        "aff_unique_abbr": "Beihang;;UEA;SJTU;UCF;UESTC",
        "aff_campus_unique_index": "0;;2;0",
        "aff_campus_unique": "Beijing;;Orlando",
        "aff_country_unique_index": "0+0;0+1;1;0;2;0;0+0",
        "aff_country_unique": "China;United Kingdom;United States"
    },
    {
        "title": "Binary Constraint Preserving Graph Matching",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1789",
        "author_site": "Bo Jiang, Jin Tang, Chris Ding, Bin Luo",
        "author": "Bo Jiang; Jin Tang; Chris Ding; Bin Luo",
        "abstract": "Graph matching is a fundamental problem in computer vision and pattern recognition area. In general, it can be formulated as an Integer Quadratic Programming (IQP) problem. Since it is NP-hard, approximate relaxations are required. In this paper, a new graph matching method has been proposed. There are three main contributions of the proposed method: (1) we propose a new graph matching relaxation model, called Binary Constraint Preserving Graph Matching (BPGM), which aims to incorporate the discrete binary mapping constraints more in graph matching relaxation. Our BPGM is motivated by a new observation that the discrete binary constraints in IQP matching problem can be represented (or encoded) exactly by a l2-norm constraint. (2) An effective projection algorithm has been derived to solve BPGM model. (3) Using BPGM, we propose a path-following strategy to optimize IQP matching problem and thus obtain a desired discrete solution at convergence. Promising experimental results show the effectiveness of the proposed method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jiang_Binary_Constraint_Preserving_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science and Technology, Anhui University, Hefei, 230601, China+ CSE Department, University of Texas at Arlington, Arlington, TX 76019, USA; School of Computer Science and Technology, Anhui University, Hefei, 230601, China+ CSE Department, University of Texas at Arlington, Arlington, TX 76019, USA; CSE Department, University of Texas at Arlington, Arlington, TX 76019, USA; School of Computer Science and Technology, Anhui University, Hefei, 230601, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 911291,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3852138528037998971&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "ahu.edu.cn;gmail.com;uta.edu;ahu.edu.cn",
        "email": "ahu.edu.cn;gmail.com;uta.edu;ahu.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jiang_Binary_Constraint_Preserving_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;1;0",
        "aff_unique_norm": "Anhui University;University of Texas at Arlington",
        "aff_unique_dep": "School of Computer Science and Technology;CSE Department",
        "aff_unique_url": ";https://www.uta.edu",
        "aff_unique_abbr": ";UTA",
        "aff_campus_unique_index": "0+1;0+1;1;0",
        "aff_campus_unique": "Hefei;Arlington",
        "aff_country_unique_index": "0+1;0+1;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Binge Watching: Scaling Affordance Learning From Sitcoms",
        "session": "Analyzing Humans 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "945",
        "author_site": "Xiaolong Wang, Rohit Girdhar, Abhinav Gupta",
        "author": "Xiaolong Wang; Rohit Girdhar; Abhinav Gupta",
        "abstract": "In recent years, there has been a renewed interest in jointly modeling perception and action. At the core of this investigation is the idea of modeling affordances. However, when it comes to predicting affordances, even the state of the art approaches still do not use any ConvNets. Why is that? Unlike semantic or 3D tasks, there still does not exist any large-scale dataset for affordances. In this paper, we tackle the challenge of creating one of the biggest dataset for learning affordances. We use seven sitcoms to extract a diverse set of scenes and how actors interact with different objects in the scenes. Our dataset consists of more than 10K scenes and 28K ways humans can interact with these 10K images. We also propose a two-step approach to predict affordances in a new scene. In the first step, given a location in the scene we classify which of the 30 pose classes is the likely affordance pose. Given the pose class and the scene, we then use a Variational Autoencoder (VAE) to extract the scale and deformation of the pose. The VAE allows us to sample the distribution of possible poses at test time. Finally, we show the importance of large-scale data in learning a generalizable and robust model of affordances.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Binge_Watching_Scaling_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1804.03080v1",
        "pdf_size": 2877067,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12581245184887687678&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Binge_Watching_Scaling_CVPR_2017_paper.html"
    },
    {
        "title": "Borrowing Treasures From the Wealthy: Deep Transfer Learning Through Selective Joint Fine-Tuning",
        "session": "Machine Learning 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "373",
        "author_site": "Weifeng Ge, Yizhou Yu",
        "author": "Weifeng Ge; Yizhou Yu",
        "abstract": "Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a deep transfer learning scheme, called selective joint fine-tuning, for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task. Experiments demonstrate that our deep transfer learning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67, and fine-grained classification problems (Oxford Flowers 102 and Stanford Dogs 120). In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% - 10% using a single model. Codes and models are available at https://github.com/ZYYSzj/Selective-Joint-Fine-tuning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ge_Borrowing_Treasures_From_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, The University of Hong Kong; Department of Computer Science, The University of Hong Kong",
        "project": "",
        "github": "https://github.com/ZYYSzj/Selective-Joint-Fine-tuning",
        "supp": "",
        "arxiv": "1702.08690v2",
        "pdf_size": 1147014,
        "gs_citation": 297,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17353663690050698115&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ge_Borrowing_Treasures_From_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Hong Kong",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.hku.hk",
        "aff_unique_abbr": "HKU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Boundary-Aware Instance Segmentation",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2429",
        "author_site": "Zeeshan Hayder, Xuming He, Mathieu Salzmann",
        "author": "Zeeshan Hayder; Xuming He; Mathieu Salzmann",
        "abstract": "We address the problem of instance-level semantic seg- mentation, which aims at jointly detecting, segmenting and classifying every individual object in an image. In this con- text, existing methods typically propose candidate objects, usually as bounding boxes, and directly predict a binary mask within each such proposal. As a consequence, they cannot recover from errors in the object candidate genera- tion process, such as too small or shifted boxes. In this paper, we introduce a novel object segment rep- resentation based on the distance transform of the object masks. We then design an object mask network (OMN) with a new residual-deconvolution architecture that infers such a representation and decodes it into the final binary object mask. This allows us to predict masks that go beyond the scope of the bounding boxes and are thus robust to inaccu- rate object candidates. We integrate our OMN into a Mul- titask Network Cascade framework, and learn the result- ing boundary-aware instance segmentation (BAIS) network in an end-to-end manner. Our experiments on the PAS- CAL VOC 2012 and the Cityscapes datasets demonstrate the benefits of our approach, which outperforms the state- of-the-art in both object proposal generation and instance segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hayder_Boundary-Aware_Instance_Segmentation_CVPR_2017_paper.pdf",
        "aff": "Australian National University + Data61/CSIRO; Australian National University + Data61/CSIRO; CVLab, EPFL, Switzerland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Hayder_Boundary-Aware_Instance_Segmentation_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.03129v2",
        "pdf_size": 1296752,
        "gs_citation": 186,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12682045460362589556&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "anu.edu.au;anu.edu.au;epfl.ch",
        "email": "anu.edu.au;anu.edu.au;epfl.ch",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hayder_Boundary-Aware_Instance_Segmentation_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;2",
        "aff_unique_norm": "Australian National University;Commonwealth Scientific and Industrial Research Organisation;EPFL",
        "aff_unique_dep": ";Data61;CVLab",
        "aff_unique_url": "https://www.anu.edu.au;https://www.csiro.au;https://cvlab.epfl.ch",
        "aff_unique_abbr": "ANU;CSIRO;EPFL",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;1",
        "aff_country_unique": "Australia;Switzerland"
    },
    {
        "title": "BranchOut: Regularization for Online Ensemble Tracking With Convolutional Neural Networks",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "1251",
        "author_site": "Bohyung Han, Jack Sim, Hartwig Adam",
        "author": "Bohyung Han; Jack Sim; Hartwig Adam",
        "abstract": "We propose an extremely simple but effective regularization technique of convolutional neural networks (CNNs), referred to as BranchOut, for online ensemble tracking.  Our algorithm employs a CNN for target representation, which has a common convolutional layers but has multiple branches of fully connected layers.  For better regularization, a subset of branches in the CNN are selected randomly for online learning whenever target appearance models need to be updated.  Each branch may have a different number of layers to maintain variable abstraction levels of target appearances. BranchOut with multi-level target representation allows us to learn robust target appearance models with diversity and handle various challenges in visual tracking problem effectively.  The proposed algorithm is evaluated in standard tracking benchmarks and shows the state-of-the-art performance even without additional pretraining on external tracking sequences.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Han_BranchOut_Regularization_for_CVPR_2017_paper.pdf",
        "aff": "POSTECH, Korea; Google Inc.; Google Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3376140,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6570220534700384582&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "postech.ac.kr;google.com;google.com",
        "email": "postech.ac.kr;google.com;google.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Han_BranchOut_Regularization_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Pohang University of Science and Technology;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.postech.ac.kr;https://www.google.com",
        "aff_unique_abbr": "POSTECH;Google",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Pohang;Mountain View",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "title": "Budget-Aware Deep Semantic Video Segmentation",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "356",
        "author_site": "Behrooz Mahasseni, Sinisa Todorovic, Alan Fern",
        "author": "Behrooz Mahasseni; Sinisa Todorovic; Alan Fern",
        "abstract": "In this work, we study a poorly understood trade-off between accuracy and runtime costs for deep semantic video segmentation. While recent work has demonstrated advantages of learning to speed-up deep activity detection, it is not clear if similar advantages will hold for our very different segmentation loss function, which is defined over individual pixels across the frames. In deep video segmentation, the most time consuming step represents the application of a CNN to every frame for assigning class labels to every pixel, typically taking 6-9 times of the video footage. This motivates our new budget-aware framework that learns to optimally select a small subset of frames for pixelwise labeling by a CNN, and then efficiently interpolates the obtained segmentations to yet unprocessed frames. This interpolation may use either a simple optical-flow guided mapping of pixel labels, or another significantly less complex and thus faster CNN. We formalize the frame selection as a Markov Decision Process, and specify a Long Short-Term Memory (LSTM) network to model a policy for selecting the frames. For training the LSTM, we develop a policy-gradient reinforcement-learning approach for approximating the gradient of our non-decomposable and non-differentiable objective. Evaluation on two benchmark video datasets show that our new framework is able to significantly reduce computation time, and maintain competitive video segmentation accuracy under varying budgets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Mahasseni_Budget-Aware_Deep_Semantic_CVPR_2017_paper.pdf",
        "aff": "Oregon State University; Oregon State University; Oregon State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1450128,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14827805431105188702&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;oregonstate.edu;oregonstate.edu",
        "email": "gmail.com;oregonstate.edu;oregonstate.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Mahasseni_Budget-Aware_Deep_Semantic_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Building a Regular Decision Boundary With Deep Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2121",
        "author": "Edouard Oyallon",
        "abstract": "In this work, we build a generic architecture of Convolutional Neural Networks to discover empirical properties of neural networks. Our first contribution is to introduce a state-of-the-art framework that depends upon few hyper parameters and to study the network when we vary them. It has no max pooling, no biases, only 13 layers,  is purely convolutional and yields up to 95.4% and 79.6% accuracy respectively on CIFAR10 and CIFAR100. We show that the nonlinearity of a deep network does not need to be continuous, non expansive or point-wise, to achieve good performance. We show that increasing the width of our network permits being competitive with very deep networks. Our second contribution is an analysis of the contraction and separation properties of this network. Indeed, a 1-nearest neighbor classifier applied on deep features progressively improves with depth, which indicates that the representation is progressively more regular. Besides, we defined and analyzed local support vectors that separate classes locally.  All our experiments are reproducible and code will be available online, based on TensorFlow.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Oyallon_Building_a_Regular_CVPR_2017_paper.pdf",
        "aff": "",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.01775v1",
        "pdf_size": 571337,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12805210598339361067&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "",
        "email": "",
        "author_num": 1,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Oyallon_Building_a_Regular_CVPR_2017_paper.html"
    },
    {
        "title": "CASENet: Deep Category-Aware Semantic Edge Detection",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2581",
        "author_site": "Zhiding Yu, Chen Feng, Ming-Yu Liu, Srikumar Ramalingam",
        "author": "Zhiding Yu; Chen Feng; Ming-Yu Liu; Srikumar Ramalingam",
        "abstract": "Boundary and edge cues are highly beneficial in improving a wide variety of vision tasks such as semantic segmentation, object recognition, stereo, and object proposal generation. Recently, the problem of edge detection has been revisited and significant progress has been made with deep learning. While classical edge detection is a challenging binary problem in itself, the category-aware semantic edge detection by nature is an even more challenging multi-label problem. We model the problem such that each edge pixel can be associated with more than one class as they appear in contours or junctions belonging to two or more semantic classes. To this end, we propose a novel end-to-end deep semantic edge learning architecture based on ResNet and a new skip-layer architecture where category-wise edge activations at the top convolution layer share and are fused with the same set of bottom layer features. We then propose a multi-label loss function to supervise the fused activations. We show that our proposed architecture benefits this problem with better performance, and we outperform the current state-of-the-art semantic edge detection methods by a large margin on standard data sets such as SBD and Cityscapes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_CASENet_Deep_Category-Aware_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yu_CASENet_Deep_Category-Aware_2017_CVPR_supplemental.pdf",
        "arxiv": "1705.09759v1",
        "pdf_size": 7298829,
        "gs_citation": 400,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11457971255382321799&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_CASENet_Deep_Category-Aware_CVPR_2017_paper.html"
    },
    {
        "title": "CATS: A Color and Thermal Stereo Benchmark",
        "session": "3D Vision 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "1075",
        "author_site": "Wayne Treible, Philip Saponaro, Scott Sorensen, Abhishek Kolagunda, Michael O'Neal, Brian Phelan, Kelly Sherbondy, Chandra Kambhamettu",
        "author": "Wayne Treible; Philip Saponaro; Scott Sorensen; Abhishek Kolagunda; Michael O'Neal; Brian Phelan; Kelly Sherbondy; Chandra Kambhamettu",
        "abstract": "Stereo matching is a well researched area using visible-band color cameras. Thermal images are typically lower resolution, have less texture, and are noisier compared to their visible-band counterparts and are more challenging for stereo matching algorithms. Previous benchmarks for stereo matching either focus entirely on visible-band cameras or contain only a single thermal camera. We present the Color And Thermal Stereo (CATS) benchmark, a dataset consisting of stereo thermal, stereo color, and cross-modality image pairs with high accuracy ground truth (< 2mm) generated from a LiDAR. We scanned 100 cluttered indoor and 80 outdoor scenes featuring challenging environments and conditions. CATS contains approximately 1400 images of pedestrians, vehicles, electronics, and other thermally interesting objects in different environmental conditions, including nighttime, daytime, and foggy scenes. Ground truth was projected to each of the four cameras to generate color-color, thermal-thermal, and cross-modality disparity maps. We develop a semi-automatic LiDAR to camera alignment procedure that does not require a calibration target. We compare state-of-the-art algorithms to baseline the dataset and show that in the thermal and cross modalities there is still much room for improvement. We expect our dataset to provide researchers with a more diverse set of imaged locations, objects, and modalities than previous benchmarks for stereo matching.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Treible_CATS_A_Color_CVPR_2017_paper.pdf",
        "aff": "University of Delaware; University of Delaware; University of Delaware; University of Delaware; University of Delaware; U.S. Army Research Laboratory; U.S. Army Research Laboratory; University of Delaware+Vision Systems Incorporated",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1864119,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5444626889185215619&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "udel.edu;udel.edu;udel.edu;udel.edu;udel.edu;mail.mil;mail.mil;udel.edu",
        "email": "udel.edu;udel.edu;udel.edu;udel.edu;udel.edu;mail.mil;mail.mil;udel.edu",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Treible_CATS_A_Color_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;1;1;0+2",
        "aff_unique_norm": "University of Delaware;U.S. Army Research Laboratory;Vision Systems Incorporated",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.udel.edu;https://www.arl.army.mil;",
        "aff_unique_abbr": "UD;ARL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Oral",
        "track": "main",
        "pid": "2448",
        "author_site": "Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, Shih-Fu Chang",
        "author": "Zheng Shou; Jonathan Chan; Alireza Zareian; Kazuyuki Miyazawa; Shih-Fu Chang",
        "abstract": "Temporal action localization is an important yet challenging problem. Given a long, untrimmed video consisting of multiple action instances and complex background contents, we need not only to recognize their action categories, but also to localize the start time and end time of each instance. Many state-of-the-art systems use segment-level classifiers to select and rank proposal segments of pre-determined boundaries. However, a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. To this end, we design a novel Convolutional-De-Convolutional (CDC) network that places CDC filters on top of 3D ConvNets, which have been shown to be effective for abstracting action semantics but reduce the temporal length of the input data. The proposed CDC filter performs the required temporal upsampling and spatial downsampling operations simultaneously to predict actions at the frame-level granularity. It is unique in jointly modeling action semantics in space-time and fine-grained temporal dynamics. We train the CDC network in an end-to-end manner efficiently. Our model not only achieves superior performance in detecting actions in every frame, but also significantly boosts the precision of localizing temporal boundaries. Finally, the CDC network demonstrates a very high efficiency with the ability to process 500 frames per second on a single GPU server. Source code and trained models are available online at https://bitbucket.org/columbiadvmm/cdc.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shou_CDC_Convolutional-De-Convolutional_Networks_CVPR_2017_paper.pdf",
        "aff": "Columbia University; Columbia University; Columbia University; Mitsubishi Electric; Columbia University",
        "project": "https://bitbucket.org/columbiadvmm/cdc",
        "github": "",
        "supp": "",
        "arxiv": "1703.01515v2",
        "pdf_size": 1104184,
        "gs_citation": 706,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6590899420708274254&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "columbia.edu;columbia.edu;columbia.edu;cw.mitsubishielectric.co.jp;columbia.edu",
        "email": "columbia.edu;columbia.edu;columbia.edu;cw.mitsubishielectric.co.jp;columbia.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shou_CDC_Convolutional-De-Convolutional_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Columbia University;Mitsubishi Electric Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.columbia.edu;https://www.mitsubishielectric.com",
        "aff_unique_abbr": "Columbia;MELCO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "title": "CERN: Confidence-Energy Recurrent Network for Group Activity Recognition",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "2332",
        "author_site": "Tianmin Shu, Sinisa Todorovic, Song-Chun Zhu",
        "author": "Tianmin Shu; Sinisa Todorovic; Song-Chun Zhu",
        "abstract": "This work is about recognizing human activities occurring in videos at distinct semantic levels, including individual actions, interactions, and group activities. The recognition is realized using a two-level hierarchy of Long Short-Term Memory (LSTM) networks, forming a feed-forward deep architecture, which can be trained end-to-end. In comparison with existing architectures of LSTMs, we make two key contributions giving the name to our approach as Confidence-Energy Recurrent Network -- CERN. First, instead of using the common softmax layer for prediction, we specify a novel energy layer (EL) for estimating the energy of our predictions. Second, rather than finding the common minimum-energy class assignment, which may be numerically unstable under uncertainty, we specify that the EL additionally computes the p-values of the solutions, and in this way estimates the most confident energy minimum. The evaluation on the Collective Activity and Volleyball datasets demonstrates: (i) advantages of our two contributions relative to the common softmax and energy-minimization formulations and (ii) a superior performance relative to the state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shu_CERN_Confidence-Energy_Recurrent_CVPR_2017_paper.pdf",
        "aff": "University of California, Los Angeles; Oregon State University; University of California, Los Angeles",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Shu_CERN_Confidence-Energy_Recurrent_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.03058v1",
        "pdf_size": 1236944,
        "gs_citation": 228,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15703842979661204122&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ucla.edu;onid.orst.edu;stat.ucla.edu",
        "email": "ucla.edu;onid.orst.edu;stat.ucla.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shu_CERN_Confidence-Energy_Recurrent_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Los Angeles;Oregon State University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucla.edu;https://oregonstate.edu",
        "aff_unique_abbr": "UCLA;OSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1062",
        "author_site": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick",
        "author": "Justin Johnson; Bharath Hariharan; Laurens van der Maaten; Li Fei-Fei; C. Lawrence Zitnick; Ross Girshick",
        "abstract": "When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover short- comings. Existing benchmarks for visual question answer- ing can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conflate multiple sources of error, making it hard to pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Johnson_CLEVR_A_Diagnostic_CVPR_2017_paper.pdf",
        "aff": "Stanford University; Stanford University; Facebook AI Research; Facebook AI Research; Facebook AI Research; Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.06890v1",
        "pdf_size": 2025323,
        "gs_citation": 2819,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1033880884200484288&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "stanford.edu;stanford.edu;fb.com;fb.com;fb.com;fb.com",
        "email": "stanford.edu;stanford.edu;fb.com;fb.com;fb.com;fb.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Johnson_CLEVR_A_Diagnostic_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;1;1;1",
        "aff_unique_norm": "Stanford University;Meta",
        "aff_unique_dep": ";Facebook AI Research",
        "aff_unique_url": "https://www.stanford.edu;https://research.facebook.com",
        "aff_unique_abbr": "Stanford;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "CLKN: Cascaded Lucas-Kanade Networks for Image Alignment",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "798",
        "author_site": "Che-Han Chang, Chun-Nan Chou, Edward Y. Chang",
        "author": "Che-Han Chang; Chun-Nan Chou; Edward Y. Chang",
        "abstract": "This paper proposes a data-driven approach for image alignment. Our main contribution is a novel network architecture that combines the strengths of convolutional neural networks (CNNs) and the Lucas-Kanade algorithm. The main component of this architecture is a Lucas-Kanade layer that performs the inverse compositional algorithm on convolutional feature maps. To train our network, we develop a cascaded feature learning method that incorporates the coarse-to-fine strategy into the training process. This method learns a pyramid representation of convolutional features in a cascaded manner and yields a cascaded network that performs coarse-to-fine alignment on the feature pyramids. We apply our model to the task of homography estimation, and perform training and evaluation on a large labeled dataset generated from the MS-COCO dataset. Experimental results show that the proposed approach significantly outperforms the other methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chang_CLKN_Cascaded_Lucas-Kanade_CVPR_2017_paper.pdf",
        "aff": "HTC Research; HTC Research; HTC Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1699689,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4502838945742782315&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "htc.com;htc.com;htc.com",
        "email": "htc.com;htc.com;htc.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chang_CLKN_Cascaded_Lucas-Kanade_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "HTC Corporation",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.htc.com/",
        "aff_unique_abbr": "HTC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "CNN-Based Patch Matching for Optical Flow With Thresholded Hinge Embedding Loss",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "1208",
        "author_site": "Christian Bailer, Kiran Varanasi, Didier Stricker",
        "author": "Christian Bailer; Kiran Varanasi; Didier Stricker",
        "abstract": "Learning based approaches have not yet achieved their full potential in optical flow estimation, where their performance still trails heuristic approaches. In this paper, we present a CNN based patch matching approach for optical flow estimation. An important contribution of our approach is a novel thresholded loss for Siamese networks. We demonstrate that our loss performs clearly better than existing losses. It also allows to speed up training by a factor of 2 in our tests. Furthermore, we present a novel way for calculating CNN based features for different image scales, which performs better than existing methods. We also discuss new ways of evaluating the robustness of trained features for the application of patch matching for optical flow. An interesting discovery in our paper is that low-pass filtering of feature maps can increase the robustness of features created by CNNs. We proved the competitive performance of our approach by submitting it to the KITTI 2012, KITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art results on all three datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bailer_CNN-Based_Patch_Matching_CVPR_2017_paper.pdf",
        "aff": "German Research Center for Artificial Intelligence (DFKI) + University of Kaiserslautern; German Research Center for Artificial Intelligence (DFKI) + University of Kaiserslautern; German Research Center for Artificial Intelligence (DFKI) + University of Kaiserslautern",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Bailer_CNN-Based_Patch_Matching_2017_CVPR_supplemental.pdf",
        "arxiv": "1607.08064v4",
        "pdf_size": 881663,
        "gs_citation": 149,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18066363394917147875&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "dfki.de;dfki.de;dfki.de",
        "email": "dfki.de;dfki.de;dfki.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bailer_CNN-Based_Patch_Matching_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "German Research Center for Artificial Intelligence;University of Kaiserslautern",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.dFKI.de;https://www.uni-kl.de",
        "aff_unique_abbr": "DFKI;Uni KL",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "CNN-SLAM: Real-Time Dense Monocular SLAM With Learned Depth Prediction",
        "session": "Machine Learning for 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "2762",
        "author_site": "Keisuke Tateno, Federico Tombari, Iro Laina, Nassir Navab",
        "author": "Keisuke Tateno; Federico Tombari; Iro Laina; Nassir Navab",
        "abstract": "Given the recent advances in depth prediction from Convolutional Neural Networks (CNNs), this paper investigates how predicted depth maps from a deep neural network can be deployed for the goal of accurate and dense monocular reconstruction. We propose a method where CNN-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular SLAM, based on a scheme that privileges depth prediction in image locations where monocular SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa. We demonstrate the use of depth prediction to estimate the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular SLAM. Finally, we propose a framework to efficiently fuse semantic labels, obtained from a single frame, with dense SLAM, so to yield semantically coherent scene reconstruction from a single view. Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tateno_CNN-SLAM_Real-Time_Dense_CVPR_2017_paper.pdf",
        "aff": "CAMP - TU Munich+Canon Inc.; CAMP - TU Munich; CAMP - TU Munich; CAMP - TU Munich+Johns Hopkins University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Tateno_CNN-SLAM_Real-Time_Dense_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2170935,
        "gs_citation": 1024,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13958434312553724438&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff_domain": "in.tum.de;in.tum.de;in.tum.de;in.tum.de",
        "email": "in.tum.de;in.tum.de;in.tum.de;in.tum.de",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tateno_CNN-SLAM_Real-Time_Dense_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;0+2",
        "aff_unique_norm": "Technical University of Munich;Canon Inc.;Johns Hopkins University",
        "aff_unique_dep": "Computer Architecture and Mobile Programming;;",
        "aff_unique_url": "https://www.tum.de;https://www.canon.com;https://www.jhu.edu",
        "aff_unique_abbr": "TUM;Canon;JHU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Munich;",
        "aff_country_unique_index": "0+1;0;0;0+2",
        "aff_country_unique": "Germany;Japan;United States"
    },
    {
        "title": "Can Walking and Measuring Along Chord Bunches Better Describe Leaf Shapes?",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2689",
        "author_site": "Bin Wang, Yongsheng Gao, Changming Sun, Michael Blumenstein, John La Salle",
        "author": "Bin Wang; Yongsheng Gao; Changming Sun; Michael Blumenstein; John La Salle",
        "abstract": "Effectively describing and recognizing leaf shapes under arbitrary deformations, particularly from a large database, remains an unsolved problem. In this research, we attempted a new strategy of describing shape by walking along a bunch of chords that pass through the shape to measure the regions trespassed. A novel chord bunch walks (CBW) descriptor is developed through the chord walking that effectively integrates the shape image function over the walked chord to reflect the contour features and the inner properties of the shape. For each contour point, the chord bunch groups multiple pairs of chord walks to build a hierarchical framework for a coarse-to-fine description. The proposed CBW descriptor is invariant to rotation, scaling, translation, and mirror transforms. Instead of using the expensive optimal correspondence based matching, an improved Hausdorff distance encoded correspondence information is proposed for efficient yet effective shape matching. In experimental studies, the proposed method obtained substantially higher accuracies with low computational cost over the benchmarks, which indicates the research potential along this direction.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Can_Walking_and_CVPR_2017_paper.pdf",
        "aff": "School of Engineering, Griffith University, Australia; School of Engineering, Griffith University, Australia; CSIRO Data61, PO Box 76, Epping, NSW 1710, Australia; FEIT, University of Technology Sydney, Australia; Atlas of Living Australia, CSIRO National Research Collections Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1204255,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16138719171380219028&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "griffith.edu.au;griffith.edu.au;csiro.au;uts.edu.au;csiro.au",
        "email": "griffith.edu.au;griffith.edu.au;csiro.au;uts.edu.au;csiro.au",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Can_Walking_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;2;3",
        "aff_unique_norm": "Griffith University;CSIRO Data61;University of Technology Sydney;CSIRO National Research Collections Australia",
        "aff_unique_dep": "School of Engineering;;Faculty of Engineering and Information Technology;Atlas of Living Australia",
        "aff_unique_url": "https://www.griffith.edu.au;https://www.csiro.au/en/Research/Data61;https://www.uts.edu.au;https://www.csiro.au",
        "aff_unique_abbr": "Griffith;CSIRO Data61;UTS;CSIRO",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Sydney",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Captioning Images With Diverse Objects",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Oral",
        "track": "main",
        "pid": "2454",
        "author_site": "Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach, Raymond Mooney, Trevor Darrell, Kate Saenko",
        "author": "Subhashini Venugopalan; Lisa Anne Hendricks; Marcus Rohrbach; Raymond Mooney; Trevor Darrell; Kate Saenko",
        "abstract": "Recent captioning models are limited in their ability to scale and describe concepts unseen in  paired image-text corpora. We propose the Novel Object Captioner (NOC), a deep visual semantic captioning model that can describe a large number of object categories not present in existing image-caption datasets. Our model takes advantage of external sources -- labeled images from object recognition datasets, and semantic knowledge extracted from unannotated text.  We propose minimizing a joint objective which can learn from these diverse data sources and leverage distributional semantic embeddings,  enabling the model to generalize and describe novel objects outside of image-caption datasets.  We demonstrate that our model exploits semantic information to generate captions for hundreds of object categories in the ImageNet object recognition dataset that are not observed in MSCOCO image-caption training data, as well as many categories that are observed very rarely. Both automatic evaluations and human judgements show that our model considerably outperforms prior work in being able to describe many more categories of objects.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Venugopalan_Captioning_Images_With_CVPR_2017_paper.pdf",
        "aff": "UT Austin\u2020; UC Berkeley\u2217; UC Berkeley\u2217; UT Austin\u2020; UC Berkeley\u2217; Boston Univ.\u2021",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Venugopalan_Captioning_Images_With_2017_CVPR_supplemental.pdf",
        "arxiv": "1606.07770",
        "pdf_size": 2326645,
        "gs_citation": 226,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2776714697465656838&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.utexas.edu;eecs.berkeley.edu;eecs.berkeley.edu;cs.utexas.edu;eecs.berkeley.edu;bu.edu",
        "email": "cs.utexas.edu;eecs.berkeley.edu;eecs.berkeley.edu;cs.utexas.edu;eecs.berkeley.edu;bu.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Venugopalan_Captioning_Images_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0;1;2",
        "aff_unique_norm": "University of Texas at Austin;University of California, Berkeley;Boston University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utexas.edu;https://www.berkeley.edu;https://www.bu.edu",
        "aff_unique_abbr": "UT Austin;UC Berkeley;BU",
        "aff_campus_unique_index": "0;1;1;0;1",
        "aff_campus_unique": "Austin;Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ChestX-ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases",
        "session": "Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "766",
        "author_site": "Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, Ronald M. Summers",
        "author": "Xiaosong Wang; Yifan Peng; Le Lu; Zhiyong Lu; Mohammadhadi Bagheri; Ronald M. Summers",
        "abstract": "The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems.   In this paper, we present a new chest X-ray database, namely \"ChestX-ray8\", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based \"reading chest X-rays\" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf",
        "aff": "Department of Radiology and Imaging Sciences, Clinical Center; National Center for Biotechnology Information, National Library of Medicine, National Institutes of Health, Bethesda, MD 20892; Department of Radiology and Imaging Sciences, Clinical Center; National Center for Biotechnology Information, National Library of Medicine, National Institutes of Health, Bethesda, MD 20892; Department of Radiology and Imaging Sciences, Clinical Center; Department of Radiology and Imaging Sciences, Clinical Center",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1642634,
        "gs_citation": 5353,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5116144877511284006&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "nih.gov;nih.gov;nih.gov;nih.gov;nih.gov;nih.gov",
        "email": "nih.gov;nih.gov;nih.gov;nih.gov;nih.gov;nih.gov",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1;0;0",
        "aff_unique_norm": "Clinical Center;National Institutes of Health",
        "aff_unique_dep": "Department of Radiology and Imaging Sciences;National Center for Biotechnology Information, National Library of Medicine",
        "aff_unique_url": "https://www.cc.nih.gov;https://www.nih.gov",
        "aff_unique_abbr": ";NIH",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Bethesda",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "CityPersons: A Diverse Dataset for Pedestrian Detection",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "1191",
        "author_site": "Shanshan Zhang, Rodrigo Benenson, Bernt Schiele",
        "author": "Shanshan Zhang; Rodrigo Benenson; Bernt Schiele",
        "abstract": "Convnets have enabled significant progress in pedestrian detection recently, but there are still open questions regard- ing suitable architectures and training data. We revisit CNN design and point out key adaptations, enabling plain Fas- terRCNN to obtain state-of-the-art results on the Caltech dataset. To achieve further improvement from more and better data, we introduce CityPersons, a new set of person annotations on top of the Cityscapes dataset. The di- versity of CityPersons allows us for the first time to train one single CNN model that generalizes well over mul- tiple benchmarks. Moreover, with additional training with CityPersons, we obtain top results using FasterRCNN on Caltech, improving especially for more difficult cases (heavy occlusion and small scale) and providing higher loc- alization quality.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_CityPersons_A_Diverse_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science and Engineering, Nanjing University of Science and Technology, China + Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Germany",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_CityPersons_A_Diverse_2017_CVPR_supplemental.pdf",
        "arxiv": "1702.05693v1",
        "pdf_size": 1171579,
        "gs_citation": 1138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15173863929848013439&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "njust.edu.cn;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "email": "njust.edu.cn;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_CityPersons_A_Diverse_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "Nanjing University of Science and Technology;Max Planck Institute for Informatics",
        "aff_unique_dep": "School of Computer Science and Engineering;",
        "aff_unique_url": ";https://mpi-inf.mpg.de",
        "aff_unique_abbr": ";MPII",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Saarland",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "China;Germany"
    },
    {
        "title": "Co-Occurrence Filter",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1175",
        "author_site": "Roy J. Jevnisek, Shai Avidan",
        "author": "Roy J. Jevnisek; Shai Avidan",
        "abstract": "Co-occurrence Filter (CoF) is a boundary preserving filter. It is based on the Bilateral Filter (BF) but instead of using a Gaussian on the range values to preserve edges it relies on a co-occurrence matrix. Pixel values that co-occur frequently in the image (i.e., inside textured regions) will have a high weight in the co-occurrence matrix. This, in turn, means  that such pixel pairs will be averaged and hence smoothed, regardless of their intensity differences. On the other hand, pixel values that rarely co-occur (i.e., across texture boundaries) will have a low weight in the co-occurrence matrix. As a result, they will not be averaged and the boundary between them will be preserved. The CoF therefore extends the  BF to deal with boundaries, not just edges. It learns co-occurrences directly from the image. We can achieve various filtering results by directing it to learn the co-occurrence matrix from a part of the image, or a different image. We give the definition of the filter, discuss how to use it with color images and show several use cases.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jevnisek_Co-Occurrence_Filter_CVPR_2017_paper.pdf",
        "aff": "Tel-Aviv University; Tel-Aviv University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.04111",
        "pdf_size": 2413556,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3010297359030095920&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "post.tau.ac.il;eng.tau.ac.il",
        "email": "post.tau.ac.il;eng.tau.ac.il",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jevnisek_Co-Occurrence_Filter_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tel Aviv University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Coarse-To-Fine Segmentation With Shape-Tailored Continuum Scale Spaces",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1722",
        "author_site": "Naeemullah Khan, Byung-Woo Hong, Anthony Yezzi, Ganesh Sundaramoorthi",
        "author": "Naeemullah Khan; Byung-Woo Hong; Anthony Yezzi; Ganesh Sundaramoorthi",
        "abstract": "We formulate an energy for segmentation that is designed to have preference for segmenting the coarse over fine structure of the image, without smoothing across boundaries of regions. The energy is formulated by integrating a continuum of scales from a scale space computed from the heat equation within regions. We show that the energy can be optimized without computing a continuum of scales, but instead from a single scale. This makes the method computationally efficient in comparison to energies using a discrete set of scales. We apply our method to texture and motion segmentation. Experiments on benchmark datasets show that a continuum of scales leads to better segmentation accuracy over discrete scales and other competing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Khan_Coarse-To-Fine_Segmentation_With_CVPR_2017_paper.pdf",
        "aff": "KAUST, Saudi Arabia; Chung-Ang University, Korea; Georgia Tech, USA; KAUST, Saudi Arabia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Khan_Coarse-To-Fine_Segmentation_With_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 7602017,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3108723462276407166&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "kaust.edu.sa;cau.ac.kr;ece.gatech.edu;kaust.edu.sa",
        "email": "kaust.edu.sa;cau.ac.kr;ece.gatech.edu;kaust.edu.sa",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Khan_Coarse-To-Fine_Segmentation_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology;Chung-Ang University;Georgia Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.kaust.edu.sa;http://www.cau.ac.kr;https://www.gatech.edu",
        "aff_unique_abbr": "KAUST;CAU;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "Saudi Arabia;South Korea;United States"
    },
    {
        "title": "Coarse-To-Fine Volumetric Prediction for Single-Image 3D Human Pose",
        "session": "Analyzing Humans 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "3269",
        "author_site": "Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas Daniilidis",
        "author": "Georgios Pavlakos; Xiaowei Zhou; Konstantinos G. Derpanis; Kostas Daniilidis",
        "abstract": "This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization and a subsequent optimization step to recover 3D pose. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a fine discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-fine prediction scheme. This step addresses the large dimensionality increase and enables iterative refinement and repeated processing of the image features. The proposed approach outperforms all state-of-the-art methods on standard benchmarks achieving a relative error reduction greater than 30% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our end-to-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Pavlakos_Coarse-To-Fine_Volumetric_Prediction_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Pavlakos_Coarse-To-Fine_Volumetric_Prediction_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1802152,
        "gs_citation": 1177,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5441238285159300924&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Pavlakos_Coarse-To-Fine_Volumetric_Prediction_CVPR_2017_paper.html"
    },
    {
        "title": "Cognitive Mapping and Planning for Visual Navigation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "948",
        "author_site": "Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik",
        "author": "Saurabh Gupta; James Davidson; Sergey Levine; Rahul Sukthankar; Jitendra Malik",
        "abstract": "We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as \"go to a chair\".",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gupta_Cognitive_Mapping_and_CVPR_2017_paper.pdf",
        "aff": "UC Berkeley+Google; Google; UC Berkeley+Google; Google; UC Berkeley+Google",
        "project": "https://sites.google.com/view/cognitive-mapping-and-planning/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Gupta_Cognitive_Mapping_and_2017_CVPR_supplemental.pdf",
        "arxiv": "1702.03920v3",
        "pdf_size": 1269504,
        "gs_citation": 876,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3064722014073988464&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff_domain": "eecs.berkeley.edu;google.com;eecs.berkeley.edu;google.com;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;google.com;eecs.berkeley.edu;google.com;eecs.berkeley.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gupta_Cognitive_Mapping_and_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;0+1;1;0+1",
        "aff_unique_norm": "University of California, Berkeley;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.berkeley.edu;https://www.google.com",
        "aff_unique_abbr": "UC Berkeley;Google",
        "aff_campus_unique_index": "0+1;1;0+1;1;0+1",
        "aff_campus_unique": "Berkeley;Mountain View",
        "aff_country_unique_index": "0+0;0;0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Collaborative Deep Reinforcement Learning for Joint Object Search",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "615",
        "author_site": "Xiangyu Kong, Bo Xin, Yizhou Wang, Gang Hua",
        "author": "Xiangyu Kong; Bo Xin; Yizhou Wang; Gang Hua",
        "abstract": "We examine the problem of joint top-down active search of multiple objects under interaction, e.g., person riding a bicycle, cups held by the table, etc.. Such objects under interaction often can provide contextual cues to each other to facilitate more efficient search. By treating each detector as an agent, we present the first collaborative multi-agent deep reinforcement learning algorithm to learn the optimal policy for joint active object localization, which effectively exploits such beneficial contextual information. We learn inter-agent communication through cross connections with gates between the Q-networks, which is facilitated by a novel multi-agent deep Q-learning algorithm with joint exploitation sampling. We verify our proposed method on multiple object detection benchmarks. Not only does our model help to improve the performance of state-of-the-art active localization models, it also reveals interesting co-detection patterns that are intuitively interpretable.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kong_Collaborative_Deep_Reinforcement_CVPR_2017_paper.pdf",
        "aff": "Nat\u2019l Eng. Lab. for Video Technology, Cooperative Medianet Innovation Center, Peking University; Microsoft Research; Nat\u2019l Eng. Lab. for Video Technology, Cooperative Medianet Innovation Center, Peking University; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.05573v1",
        "pdf_size": 2216041,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=224271291040654329&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "pku.edu.cn;microsoft.com;pku.edu.cn;microsoft.com",
        "email": "pku.edu.cn;microsoft.com;pku.edu.cn;microsoft.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kong_Collaborative_Deep_Reinforcement_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Peking University;Microsoft",
        "aff_unique_dep": "Nat\u2019l Eng. Lab. for Video Technology, Cooperative Medianet Innovation Center;Microsoft Research",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Peking University;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Collaborative Summarization of Topic-Related Videos",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "3324",
        "author_site": "Rameswar Panda, Amit K. Roy-Chowdhury",
        "author": "Rameswar Panda; Amit K. Roy-Chowdhury",
        "abstract": "Large collections of videos are grouped into clusters by a topic keyword, such as \"Eiffel Tower\" or \"Surfing\", with many important visual concepts repeating across them. Such a topically close set of videos have mutual influence on each other, which could be used to summarize one of them by exploiting information from others in the set. We build on this intuition to develop a novel approach to extract a summary that simultaneously captures both important particularities arising in the given video, as well as, generalities identified from the set of videos. The topic-related videos provide visual context to identify the important parts of the video being summarized. We achieve this by developing a collaborative sparse optimization method which can be efficiently solved by a half-quadratic minimization algorithm. Our work builds upon the idea of collaborative techniques from information retrieval and natural language processing, which typically use the attributes of other similar objects to predict the attribute of a given object. Experiments on two challenging and diverse datasets well demonstrate the efficacy of our approach over state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Panda_Collaborative_Summarization_of_CVPR_2017_paper.pdf",
        "aff": "Department of ECE, UC Riverside; Department of ECE, UC Riverside",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1706.03114",
        "pdf_size": 1548876,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8100229135106978030&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ucr.edu;ece.ucr.edu",
        "email": "ucr.edu;ece.ucr.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Panda_Collaborative_Summarization_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Riverside",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucr.edu",
        "aff_unique_abbr": "UCR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Riverside",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Colorization as a Proxy Task for Visual Understanding",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3160",
        "author_site": "Gustav Larsson, Michael Maire, Gregory Shakhnarovich",
        "author": "Gustav Larsson; Michael Maire; Gregory Shakhnarovich",
        "abstract": "We investigate and improve self-supervision as a drop-in replacement for ImageNet pretraining, focusing on automatic colorization as the proxy task.  Self-supervised training has been shown to be more promising for utilizing unlabeled data than other, traditional unsupervised learning methods. We build on this success and evaluate the ability of our self-supervised network in several contexts. On VOC segmentation and classification tasks, we present results that are state-of-the-art among methods not using ImageNet labels for pretraining representations.  Moreover, we present the first in-depth analysis of self-supervision via colorization, concluding that formulation of the loss, training details and network architecture play important roles in its effectiveness. This investigation is further expanded by revisiting the ImageNet pretraining paradigm, asking questions such as: How much training data is needed? How many labels are needed? How much do features change when fine-tuned? We relate these questions back to self-supervision by showing that colorization provides a similarly powerful supervisory signal as various flavors of ImageNet pretraining.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Larsson_Colorization_as_a_CVPR_2017_paper.pdf",
        "aff": "University of Chicago; TTI Chicago; TTI Chicago",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.04044v3",
        "pdf_size": 1464978,
        "gs_citation": 622,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12625061753515399628&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.uchicago.edu;ttic.edu;ttic.edu",
        "email": "cs.uchicago.edu;ttic.edu;ttic.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Larsson_Colorization_as_a_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Chicago;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uchicago.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UChicago;TTI",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Combining Bottom-Up, Top-Down, and Smoothness Cues for Weakly Supervised Image Segmentation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1306",
        "author_site": "Anirban Roy, Sinisa Todorovic",
        "author": "Anirban Roy; Sinisa Todorovic",
        "abstract": "This paper addresses the problem of weakly supervised semantic image segmentation. Our goal is to label every pixel in a new image, given only image-level object labels associated with training images. Our problem statement differs from common semantic segmentation, where pixel-wise annotations are typically assumed available in training. We specify a novel deep architecture which fuses three distinct computation processes toward semantic segmentation -- namely, (i) the bottom-up computation of neural activations in a CNN for the image-level prediction of object classes; (ii) the top-down estimation of conditional likelihoods of the CNN's activations given the predicted objects, resulting in probabilistic attention maps per object class; and (iii) the lateral attention-message passing from neighboring neurons at the same CNN layer. The fusion of (i)-(iii) is realized via a conditional random field as recurrent network aimed at generating a smooth and boundary-preserving segmentation. Unlike existing work, we formulate a unified end-to-end learning of all components of our deep architecture. Evaluation on the benchmark PASCAL VOC 2012 dataset demonstrates that we outperform reasonable weakly supervised baselines and state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Roy_Combining_Bottom-Up_Top-Down_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1233556,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13497788365288003074&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Roy_Combining_Bottom-Up_Top-Down_CVPR_2017_paper.html"
    },
    {
        "title": "Commonly Uncommon: Semantic Sparsity in Situation Recognition",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3417",
        "author_site": "Mark Yatskar, Vicente Ordonez, Luke Zettlemoyer, Ali Farhadi",
        "author": "Mark Yatskar; Vicente Ordonez; Luke Zettlemoyer; Ali Farhadi",
        "abstract": "Semantic sparsity is a common challenge in structured visual classification problems; when the output space is complex, the vast majority of the possible predictions are rarely, if ever, seen in the training set. This paper studies semantic sparsity in situation recognition, the task of producing structured summaries of what is happening in images, including activities, objects and the roles objects play within the activity. For this problem, we find empirically that most substructures required for prediction are rare, and current state-of-the-art model performance dramatically decreases if even one such rare substructure exists in the target output.We avoid many such errors by (1) introducing a novel tensor composition function that learns to share examples across substructures more effectively and (2) se- mantically augmenting our training data with automatically gathered examples of rarely observed outputs using web data. When integrated within a complete CRF-based structured prediction model, the tensor-based approach outperforms existing state of the art by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role accuracy, respectively. Adding 5 million images with our semantic aug- mentation techniques gives further relative improvements of 6.23% and 9.57% on top-5 verb and noun-role accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yatskar_Commonly_Uncommon_Semantic_CVPR_2017_paper.pdf",
        "aff": "Computer Science & Engineering, University of Washington, Seattle, WA; Allen Institute for Arti\ufb01cial Intelligence (AI2), Seattle, WA + Department of Computer Science, University of Virginia, Charlottesville, VA; Computer Science & Engineering, University of Washington, Seattle, WA; Computer Science & Engineering, University of Washington, Seattle, WA + Allen Institute for Arti\ufb01cial Intelligence (AI2), Seattle, WA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.00901v1",
        "pdf_size": 1240514,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14927335219637955031&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.washington.edu;cs.virginia.edu;cs.washington.edu;cs.washington.edu",
        "email": "cs.washington.edu;cs.virginia.edu;cs.washington.edu;cs.washington.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yatskar_Commonly_Uncommon_Semantic_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+2;0;0+1",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence;University of Virginia",
        "aff_unique_dep": "Computer Science & Engineering;Artificial Intelligence;Department of Computer Science",
        "aff_unique_url": "https://www.washington.edu;https://allenai.org;https://www.virginia.edu",
        "aff_unique_abbr": "UW;AI2;UVA",
        "aff_campus_unique_index": "0;0+1;0;0+0",
        "aff_campus_unique": "Seattle;Charlottesville",
        "aff_country_unique_index": "0;0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Compact Matrix Factorization With Dependent Subspaces",
        "session": "Machine Learning 3",
        "status": "Oral",
        "track": "main",
        "pid": "106",
        "author_site": "Viktor Larsson, Carl Olsson",
        "author": "Viktor Larsson; Carl Olsson",
        "abstract": "Traditional matrix factorization methods approximate high dimensional data with a low dimensional subspace. This imposes constraints on the matrix elements which allow for estimation of missing entries. A lower rank provides stronger constraints and makes estimation of the missing entries less ambiguous at the cost of measurement fit. In this paper we propose a new factorization model that further constrains the matrix entries. Our approach can be seen as a unification of traditional low-rank matrix factorization and the more recent union-of-subspace approach. It adaptively finds clusters that can be modeled with low dimensional local subspaces and simultaneously uses a global rank constraint to capture the overall scene interactions. For inference we use an energy that penalizes a trade-off between data fit and degrees-of-freedom of the resulting factorization. We show qualitatively and quantitatively that regularizing both local and global dynamics yields significantly improved missing data estimation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Larsson_Compact_Matrix_Factorization_CVPR_2017_paper.pdf",
        "aff": "Centre for Mathematical Sciences, Lund University+Department of Signals and Systems, Chalmers University of Technology; Centre for Mathematical Sciences, Lund University+Department of Signals and Systems, Chalmers University of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Larsson_Compact_Matrix_Factorization_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1985877,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15187073039944457179&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "maths.lth.se;maths.lth.se",
        "email": "maths.lth.se;maths.lth.se",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Larsson_Compact_Matrix_Factorization_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Lund University;Chalmers University of Technology",
        "aff_unique_dep": "Centre for Mathematical Sciences;Department of Signals and Systems",
        "aff_unique_url": "https://www.lunduniversity.lu.se;https://www.chalmers.se",
        "aff_unique_abbr": "LU;Chalmers",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lund;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Sweden"
    },
    {
        "title": "Comparative Evaluation of Hand-Crafted and Learned Local Features",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "551",
        "author_site": "Johannes L. Sch\u00c3\u00b6nberger, Hans Hardmeier, Torsten Sattler, Marc Pollefeys",
        "author": "Johannes L. Schonberger; Hans Hardmeier; Torsten Sattler; Marc Pollefeys",
        "abstract": "Matching local image descriptors is a key step in many computer vision applications. For more than a decade, hand-crafted descriptors such as SIFT have been used for this task. Recently, multiple new descriptors learned from data have been proposed and shown to improve on SIFT in terms of discriminative power. This paper is dedicated to an extensive experimental evaluation of learned local features to establish a single evaluation protocol that ensures comparable results. In terms of matching performance, we evaluate the different descriptors regarding standard criteria. However, considering matching performance in isolation only provides an incomplete measure of a descriptor's quality. For example, finding additional correct matches between similar images does not necessarily lead to a better performance when trying to match images under extreme viewpoint or illumination changes. Besides pure descriptor matching, we thus also evaluate the different descriptors in the context of image-based reconstruction. This enables us to study the descriptor performance on a set of more practical criteria including image retrieval, the ability to register images under strong viewpoint and illumination changes, and the accuracy and completeness of the reconstructed cameras and scenes. To facilitate future research, the full evaluation pipeline is made publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Schonberger_Comparative_Evaluation_of_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, ETH Z\u00fcrich; Department of Computer Science, ETH Z\u00fcrich; Department of Computer Science, ETH Z\u00fcrich; Department of Computer Science, ETH Z\u00fcrich+Microsoft Corp.",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Schonberger_Comparative_Evaluation_of_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 738560,
        "gs_citation": 381,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8539385725673915418&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Schonberger_Comparative_Evaluation_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "ETH Zurich;Microsoft",
        "aff_unique_dep": "Department of Computer Science;Microsoft Corporation",
        "aff_unique_url": "https://www.ethz.ch;https://www.microsoft.com",
        "aff_unique_abbr": "ETHZ;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "Comprehension-Guided Referring Expressions",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3329",
        "author_site": "Ruotian Luo, Gregory Shakhnarovich",
        "author": "Ruotian Luo; Gregory Shakhnarovich",
        "abstract": "We consider generation and comprehension of natural language referring expression for objects in an image. Unlike generic \"image captioning\" which lacks natural standard evaluation criteria, quality of a referring expression may be measured by the receiver's ability to correctly infer which object is being described. Following this intuition, we propose two approaches to utilize models trained for comprehension task to generate better expressions. First, we use a comprehension module trained on human-generated expressions, as a \"critic\" of referring expression generator. The comprehension module serves as a differentiable proxy of human evaluation, providing training signal to the generation module. Second, we use the comprehension model in a generate-and-rerank pipeline, which chooses from candidate expressions generated by a model according to their performance on the comprehension task. We show that both approaches lead to improved referring expression generation on multiple benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Luo_Comprehension-Guided_Referring_Expressions_CVPR_2017_paper.pdf",
        "aff": "TTI-Chicago; TTI-Chicago",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.03439v1",
        "pdf_size": 666062,
        "gs_citation": 187,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1086541084313163309&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ttic.edu;ttic.edu",
        "email": "ttic.edu;ttic.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Luo_Comprehension-Guided_Referring_Expressions_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tti-chicago.org",
        "aff_unique_abbr": "TTI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Computational Imaging on the Electric Grid",
        "session": "Computational Photography",
        "status": "Oral",
        "track": "main",
        "pid": "2872",
        "author_site": "Mark Sheinin, Yoav Y. Schechner, Kiriakos N. Kutulakos",
        "author": "Mark Sheinin; Yoav Y. Schechner; Kiriakos N. Kutulakos",
        "abstract": "Night beats with alternating current (AC) illumination. By passively sensing this beat, we reveal new scene information which includes: the type of bulbs in the scene, the phases of the electric grid up to city scale, and the light transport matrix. This information yields unmixing of reflections and semi-reflections, nocturnal high dynamic range, and scene rendering with bulbs not observed during acquisition. The latter is facilitated by a database of bulb response functions for a range of sources, which we collected and provide. To do all this, we built a novel coded-exposure high-dynamic-range imaging technique, specifically designed to operate on the grid's AC lighting.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sheinin_Computational_Imaging_on_CVPR_2017_paper.pdf",
        "aff": "Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of Technology; Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of Technology; Dept. of Computer Science, University of Toronto",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Sheinin_Computational_Imaging_on_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3026674,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4789568516983968637&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "gmail.com;ee.technion.ac.il;cs.toronto.edu",
        "email": "gmail.com;ee.technion.ac.il;cs.toronto.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sheinin_Computational_Imaging_on_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Technion - Israel Institute of Technology;University of Toronto",
        "aff_unique_dep": "Viterbi Faculty of Electrical Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.technion.ac.il;https://www.utoronto.ca",
        "aff_unique_abbr": "Technion;U of T",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Toronto",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Israel;Canada"
    },
    {
        "title": "Conditional Similarity Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "273",
        "author_site": "Andreas Veit, Serge Belongie, Theofanis Karaletsos",
        "author": "Andreas Veit; Serge Belongie; Theofanis Karaletsos",
        "abstract": "What makes images similar? To measure the similarity between images, they are typically embedded in a feature-vector space, in which their distance preserve the relative dissimilarity. However, when learning such similarity embeddings the simplifying assumption is commonly made that images are only compared to one unique measure of similarity. A main reason for this is that contradicting notions of similarities cannot be captured in a single space. To address this shortcoming, we propose Conditional Similarity Networks (CSNs) that learn embeddings differentiated into semantically distinct subspaces that capture the different notions of similarities. CSNs jointly learn a disentangled embedding where features for different similarities are encoded in separate dimensions as well as masks that select and reweight relevant dimensions to induce a subspace that encodes a specific similarity notion. We show that our approach learns interpretable image representations with visually relevant semantic subspaces. Further, when evaluating on triplet questions from multiple similarity notions our model even outperforms the accuracy obtained by training individual specialized networks for each notion separately.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Veit_Conditional_Similarity_Networks_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science & Cornell Tech, Cornell University; Department of Computer Science & Cornell Tech, Cornell University; Uber AI Labs + Computational Biology, Sloan Kettering Institute",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1603.07810v3",
        "pdf_size": 4777634,
        "gs_citation": 199,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18184987372440847109&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cornell.edu;cornell.edu;gmail.com",
        "email": "cornell.edu;cornell.edu;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Veit_Conditional_Similarity_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1+2",
        "aff_unique_norm": "Cornell University;Uber;Sloan Kettering Institute",
        "aff_unique_dep": "Department of Computer Science;Uber AI Labs;Computational Biology",
        "aff_unique_url": "https://www.cornell.edu;https://www.uber.com;https://www.ski.org",
        "aff_unique_abbr": "Cornell;Uber AI Labs;SKI",
        "aff_campus_unique_index": "0;0;",
        "aff_campus_unique": "Cornell Tech;",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Connecting Look and Feel: Associating the Visual and Tactile Properties of Physical Materials",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Oral",
        "track": "main",
        "pid": "2370",
        "author_site": "Wenzhen Yuan, Shaoxiong Wang, Siyuan Dong, Edward Adelson",
        "author": "Wenzhen Yuan; Shaoxiong Wang; Siyuan Dong; Edward Adelson",
        "abstract": "For machines to interact with the physical world, they must understand the physical properties of objects and materials they encounter. We use fabrics as an example of a deformable material with a rich set of mechanical properties. A thin flexible fabric, when draped, tends to look different from a heavy stiff fabric. It also feels different when touched. Using a collection of 118 fabric samples, we captured color and depth images of draped fabrics along with tactile data from a high-resolution touch sensor. We then sought to associate the information from vision and touch by jointly training CNN's across the three modalities. Through the CNN, each input, regardless of the modality, generates an embedding vector that records the fabric's physical property. By comparing the embedding vectors, our system is able to look at a fabric image and predict how it will feel, and vice versa. We also show that a system jointly trained on vision and touch data can outperform a similar system trained only on visual data when tested purely with visual inputs.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yuan_Connecting_Look_and_CVPR_2017_paper.pdf",
        "aff": "MIT; MIT+Tsinghua University; MIT; MIT",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03822v1",
        "pdf_size": 2423544,
        "gs_citation": 149,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6976714623274700756&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu;mit.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yuan_Connecting_Look_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Tsinghua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "MIT;THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Consensus Maximization With Linear Matrix Inequality Constraints",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2034",
        "author_site": "Pablo Speciale, Danda Pani Paudel, Martin R. Oswald, Till Kroeger, Luc Van Gool, Marc Pollefeys",
        "author": "Pablo Speciale; Danda Pani Paudel; Martin R. Oswald; Till Kroeger; Luc Van Gool; Marc Pollefeys",
        "abstract": "Consensus maximization has proven to be a useful tool for robust estimation. While randomized methods like RANSAC are fast, they do not guarantee global optimality and fail to manage large amounts of outliers. On the other hand, global methods are commonly slow because they do not exploit the structure of the problem at hand. In this paper, we show that the solution space can be reduced by introducing  Linear Matrix Inequality (LMI) constraints. This leads to significant speed ups of the optimization time even for large amounts of outliers, while maintaining global optimality. We study several cases in which the objective variables have a special structure, such as rotation, scaled-rotation, and essential matrices, which are posed as LMI constraints. This is very useful in several standard computer vision problems, such as estimating Similarity Transformations, Absolute Poses, and Relative Poses, for which we obtain compelling results on both synthetic and real datasets. With up to 90 percent outlier rate, where RANSAC often fails, our constrained approach is consistently faster than the non-constrained one - while finding the same global solution.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Speciale_Consensus_Maximization_With_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, ETH Z\u00fcrich + Microsoft, Redmond, USA; Computer Vision Laboratory, D-ITET, ETH Z\u00fcrich; Department of Computer Science, ETH Z\u00fcrich + Microsoft, Redmond, USA; Computer Vision Laboratory, D-ITET, ETH Z\u00fcrich; Computer Vision Laboratory, D-ITET, ETH Z\u00fcrich + VISICS, ESAT/PSI, KU Leuven, Belgium; Department of Computer Science, ETH Z\u00fcrich + Microsoft, Redmond, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 944869,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11776460256054014121&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "inf.ethz.ch;vision.ee.ethz.ch;inf.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;vision.ee.ethz.ch;inf.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;inf.ethz.ch",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Speciale_Consensus_Maximization_With_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0+1;0;0+2;0+1",
        "aff_unique_norm": "ETH Zurich;Microsoft;KU Leuven",
        "aff_unique_dep": "Department of Computer Science;Microsoft Corporation;VISICS, ESAT/PSI",
        "aff_unique_url": "https://www.ethz.ch;https://www.microsoft.com;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;Microsoft;KU Leuven",
        "aff_campus_unique_index": "1;1;;1",
        "aff_campus_unique": ";Redmond",
        "aff_country_unique_index": "0+1;0;0+1;0;0+2;0+1",
        "aff_country_unique": "Switzerland;United States;Belgium"
    },
    {
        "title": "Consistent-Aware Deep Learning for Person Re-Identification in a Camera Network",
        "session": "Analyzing Humans 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "2458",
        "author_site": "Ji Lin, Liangliang Ren, Jiwen Lu, Jianjiang Feng, Jie Zhou",
        "author": "Ji Lin; Liangliang Ren; Jiwen Lu; Jianjiang Feng; Jie Zhou",
        "abstract": "In this paper, we propose a consistent-aware deep learning (CADL) framework for person re-identification in a camera network. Unlike most existing person re-identification methods which identify whether two body images are from the same person, our approach aims to obtain the maximal correct matches for the whole camera network. Different from recently proposed camera network based re-identification methods which only consider the consistent information in the matching stage to obtain a global optimal association, we exploit such consistent-aware information under a deep learning framework where both feature representation and image matching are automatically learned with certain consistent constraints. Specifically, we reach the global optimal solution and balance the performance between different cameras by optimizing the similarity and association iteratively. Experimental results show that our method obtains significant performance improvement and outperforms the state-of-the-art methods by large margins.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Consistent-Aware_Deep_Learning_CVPR_2017_paper.pdf",
        "aff": "Department of Automation, Tsinghua University; Department of Automation, Tsinghua University + State Key Lab of Intelligent Technologies and Systems + Tsinghua National Laboratory for Information Science and Technology (TNList); Department of Automation, Tsinghua University + State Key Lab of Intelligent Technologies and Systems + Tsinghua National Laboratory for Information Science and Technology (TNList); Department of Automation, Tsinghua University + State Key Lab of Intelligent Technologies and Systems + Tsinghua National Laboratory for Information Science and Technology (TNList); Department of Automation, Tsinghua University + State Key Lab of Intelligent Technologies and Systems + Tsinghua National Laboratory for Information Science and Technology (TNList)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 694467,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12765645679343410705&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Consistent-Aware_Deep_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1+0;0+1+0;0+1+0;0+1+0",
        "aff_unique_norm": "Tsinghua University;State Key Lab of Intelligent Technologies and Systems",
        "aff_unique_dep": "Department of Automation;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;",
        "aff_unique_abbr": "THU;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0+0;0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Context-Aware Captions From Context-Agnostic Supervision",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Spotlight",
        "track": "main",
        "pid": "99",
        "author_site": "Ramakrishna Vedantam, Samy Bengio, Kevin Murphy, Devi Parikh, Gal Chechik",
        "author": "Ramakrishna Vedantam; Samy Bengio; Kevin Murphy; Devi Parikh; Gal Chechik",
        "abstract": "We introduce an inference technique to produce discriminative context-aware image captions (captions that describe differences between images or visual concepts) using only generic context-agnostic training data (captions that describe a concept or an image in isolation). For example, given images and captions of \"siamese cat\" and \"tiger cat\", we generate language that describes the \"siamese cat\" in a way that distinguishes it from \"tiger cat\". Our key novelty is that we show how to do joint inference over a language model that is context-agnostic and a listener which distinguishes closely-related concepts. We first apply our technique to a justification task, namely to describe why an image contains a particular fine-grained category as opposed to another closely-related category of the CUB- 200-2011 dataset. We then study discriminative image captioning to generate language that uniquely refers to one of two semantically-similar images in the COCO dataset. Evaluations with discriminative ground truth for justification and human studies for discriminative image captioning reveal that our approach outperforms baseline generative and speaker-listener approaches for discrimination.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Vedantam_Context-Aware_Captions_From_CVPR_2017_paper.pdf",
        "aff": "Virginia Tech; Google; Google; Georgia Institute of Technology; Google",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.02870v3",
        "pdf_size": 1257462,
        "gs_citation": 174,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=89318584606928440&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "vt.edu;google.com;google.com;gatech.edu;google.com",
        "email": "vt.edu;google.com;google.com;gatech.edu;google.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Vedantam_Context-Aware_Captions_From_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "Virginia Tech;Google;Georgia Institute of Technology",
        "aff_unique_dep": ";Google;",
        "aff_unique_url": "https://www.vt.edu;https://www.google.com;https://www.gatech.edu",
        "aff_unique_abbr": "VT;Google;Georgia Tech",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Context-Aware Correlation Filter Tracking",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Oral",
        "track": "main",
        "pid": "526",
        "author_site": "Matthias Mueller, Neil Smith, Bernard Ghanem",
        "author": "Matthias Mueller; Neil Smith; Bernard Ghanem",
        "abstract": "Correlation filter (CF) based trackers have recently gained a lot of popularity due to their impressive performance on benchmark datasets, while maintaining high frame rates. A significant amount of recent research focuses on the incorporation of stronger features for a richer representation of the tracking target. However, this only helps to discriminate the target from background within a small neighborhood. In this paper, we present a framework that allows the explicit incorporation of global context within CF trackers. We reformulate the original optimization problem and provide a closed form solution for single and multi-dimensional features in the primal and dual domain. Extensive experiments demonstrate that this framework significantly improves the performance of many CF trackers with only a modest impact on frame rate.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Mueller_Context-Aware_Correlation_Filter_CVPR_2017_paper.pdf",
        "aff": "King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Mueller_Context-Aware_Correlation_Filter_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1240598,
        "gs_citation": 781,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1394621027173719087&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "kaust.edu.sa;kaust.edu.sa;kaust.edu.sa",
        "email": "kaust.edu.sa;kaust.edu.sa;kaust.edu.sa",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Mueller_Context-Aware_Correlation_Filter_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaust.edu.sa",
        "aff_unique_abbr": "KAUST",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Thuwal",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Saudi Arabia"
    },
    {
        "title": "Contour-Constrained Superpixels for Image and Video Processing",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "889",
        "author_site": "Se-Ho Lee, Won-Dong Jang, Chang-Su Kim",
        "author": "Se-Ho Lee; Won-Dong Jang; Chang-Su Kim",
        "abstract": "A novel contour-constrained superpixel (CCS) algorithm is proposed in this work. We initialize superpixels and regions in a regular grid and then refine the superpixel label of each region hierarchically from block to pixel levels. To make superpixel boundaries compatible with object contours, we propose the notion of contour pattern matching and formulate an objective function including the contour constraint. Furthermore, we extend the CCS algorithm to generate temporal superpixels for video processing. We initialize superpixel labels in each frame by transferring those in the previous frame and refine the labels to make superpixels temporally consistent as well as compatible with object contours. Experimental results demonstrate that the proposed algorithm provides better performance than the state-of-the-art superpixel methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lee_Contour-Constrained_Superpixels_for_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7651615593434384093&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lee_Contour-Constrained_Superpixels_for_CVPR_2017_paper.html"
    },
    {
        "title": "Controlling Perceptual Factors in Neural Style Transfer",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "1592",
        "author_site": "Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, Aaron Hertzmann, Eli Shechtman",
        "author": "Leon A. Gatys; Alexander S. Ecker; Matthias Bethge; Aaron Hertzmann; Eli Shechtman",
        "abstract": "Neural Style Transfer has shown very exciting results enabling new forms of image manipulation. Here we extend the existing method to introduce control over spatial location, colour information and across spatial scale. We demonstrate how this enhances the method by allowing high-resolution controlled stylisation and helps to alleviate common failure cases such as applying ground textures to sky regions. Furthermore, by decomposing style into these perceptual factors we enable the combination of style information from multiple sources to generate new, perceptually appealing styles from existing ones. We also describe how these methods can be used to more efficiently produce large size, high-quality stylisation. Finally we show how the introduced control measures can be applied in recent methods for Fast Neural Style Transfer.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gatys_Controlling_Perceptual_Factors_CVPR_2017_paper.pdf",
        "aff": "University of T\u00fcbingen; University of T\u00fcbingen; University of T\u00fcbingen; Adobe Research; Adobe Research",
        "project": "bethgelab.org/media/uploads/stylecontrol/supplement",
        "github": "github.com/leongatys/NeuralImageSynthesis",
        "supp": "",
        "arxiv": "1611.07865",
        "pdf_size": 5338520,
        "gs_citation": 594,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13263550021606276798&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gatys_Controlling_Perceptual_Factors_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "University of T\u00fcbingen;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.uni-tuebingen.de/;https://research.adobe.com",
        "aff_unique_abbr": "Uni T\u00fcbingen;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;1",
        "aff_country_unique": "Germany;United States"
    },
    {
        "title": "Convex Global 3D Registration With Lagrangian Duality",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2042",
        "author_site": "Jesus Briales, Javier Gonzalez-Jimenez",
        "author": "Jesus Briales; Javier Gonzalez-Jimenez",
        "abstract": "The registration of 3D models by a Euclidean transformation is a fundamental task at the core of many application in computer vision. This problem is non-convex due to the presence of rotational constraints, making traditional local optimization methods prone to getting stuck in local minima. This paper addresses finding the globally optimal transformation in various 3D registration problems by a unified formulation that integrates common geometric registration modalities (namely point-to-point, point-to-line and point-to-plane). This formulation renders the optimization problem independent of both the number and nature of the correspondences.  The main novelty of our proposal is the introduction of a strengthened Lagrangian dual relaxation for this problem, which surpasses previous similar approaches [32] in effectiveness. In fact, even though with no theoretical guarantees, exhaustive empirical evaluation in both synthetic and real experiments always resulted on a tight relaxation that allowed to recover a guaranteed globally optimal solution by exploiting duality theory.  Thus, our approach allows for effectively solving the 3D registration with global optimality guarantees while running at a fraction of the time for the state-of-the-art alternative [34], based on a more computationally intensive Branch and Bound method.  [32] C. Olsson and A. Eriksson, \"Solving Quadratically Constrained Geometrical Problems using Lagrangian Duality,\" in Proc. 19th Int. Conf. Pattern Recognition (ICPR2008), pp. 1-5. [34] C. Olsson, F. Kahl, and M. Oskarsson. \"Branch-and-Bound Methods for Euclidean Registration Problems,\" in IEEE Trans. Pattern Anal. Mach. Intell., 31(5):783-794, 2009.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Briales_Convex_Global_3D_CVPR_2017_paper.pdf",
        "aff": "MAPIR-UMA Group, University of Malaga, Spain; MAPIR-UMA Group, University of Malaga, Spain",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Briales_Convex_Global_3D_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 770018,
        "gs_citation": 116,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10250904772542187760&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "uma.es;uma.es",
        "email": "uma.es;uma.es",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Briales_Convex_Global_3D_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Malaga",
        "aff_unique_dep": "MAPIR-UMA Group",
        "aff_unique_url": "https://www.uma.es",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Spain"
    },
    {
        "title": "Convolutional Neural Network Architecture for Geometric Matching",
        "session": "Machine Learning 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "2704",
        "author_site": "Ignacio Rocco, Relja Arandjelovi\u00c4\u0087, Josef Sivic",
        "author": "Ignacio Rocco; Relja Arandjelovic; Josef Sivic",
        "abstract": "We address the problem of determining correspondences between two images in agreement with a geometric model such as an affine or thin-plate spline transformation, and estimating its parameters. The contributions of this work are three-fold. First, we propose a convolutional neural network architecture for geometric matching. The architecture is based on three main components that mimic the standard steps of feature extraction, matching and simultaneous inlier detection and model parameter estimation, while being trainable end-to-end. Second, we demonstrate that the network parameters can be trained from synthetically generated imagery without the need for manual annotation and that our matching layer significantly increases generalization capabilities to never seen before images. Finally, we show that the same model can perform both instance-level and category-level matching giving state-of-the-art results on the challenging Proposal Flow dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rocco_Convolutional_Neural_Network_CVPR_2017_paper.pdf",
        "aff": "DI ENS+INRIA; DI ENS+INRIA; DI ENS+INRIA+CIIRC",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.05593v2",
        "pdf_size": 1204251,
        "gs_citation": 703,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8324362856621568563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": "ens.fr;ens.fr;ens.fr",
        "email": "ens.fr;ens.fr;ens.fr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Rocco_Convolutional_Neural_Network_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0+1+2",
        "aff_unique_norm": "\u00c9cole Normale Sup\u00e9rieure;INRIA;CIIRC",
        "aff_unique_dep": "DI;;",
        "aff_unique_url": "https://www.ens.fr;https://www.inria.fr;https://www.ciirc.cvut.cz/",
        "aff_unique_abbr": "ENS;INRIA;CIIRC",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0+1",
        "aff_country_unique": "France;Czech Republic"
    },
    {
        "title": "Convolutional Random Walk Networks for Semantic Image Segmentation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "282",
        "author_site": "Gedas Bertasius, Lorenzo Torresani, Stella X. Yu, Jianbo Shi",
        "author": "Gedas Bertasius; Lorenzo Torresani; Stella X. Yu; Jianbo Shi",
        "abstract": "Most current semantic segmentation methods rely on fully convolutional networks (FCNs). However, their use of large receptive fields and many pooling layers cause low spatial resolution inside the deep layers. This leads to predictions with poor localization around the boundaries. Prior work has attempted to address this issue by post-processing predictions with CRFs or MRFs. But such models often fail to capture semantic relationships between objects, which causes spatially disjoint predictions. To overcome these problems, recent methods integrated CRFs or MRFs into an FCN framework. The downside of these new models is that they have much higher complexity than traditional FCNs, which renders training and testing more challenging. In this work we introduce a simple, yet effective Convolutional Random Walk Network (RWN) that addresses the issues of poor boundary localization and spatially fragmented predictions with very little increase in model complexity. Our proposed RWN jointly optimizes the objectives of pixelwise affinity and semantic segmentation. It combines these two objectives via a novel random walk layer that enforces consistent spatial grouping in the deep layers of the network. Our RWN is implemented using standard convolution and matrix multiplication. This allows an easy integration into existing FCN frameworks and it enables end-to-end training of the whole network via standard back-propagation. Our implementation of RWN requires just 131 additional parameters compared to the traditional FCNs, and yet it consistently produces an improvement over the FCNs on semantic segmentation and scene labeling.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bertasius_Convolutional_Random_Walk_CVPR_2017_paper.pdf",
        "aff": "University of Pennsylvania; Dartmouth College; UC Berkeley ICSI; University of Pennsylvania",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1322956,
        "gs_citation": 175,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7131103734928123278&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "seas.upenn.edu;dartmouth.edu;berkeley.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;dartmouth.edu;berkeley.edu;seas.upenn.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bertasius_Convolutional_Random_Walk_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Pennsylvania;Dartmouth College;University of California, Berkeley",
        "aff_unique_dep": ";;International Computer Science Institute",
        "aff_unique_url": "https://www.upenn.edu;https://www.dartmouth.edu;https://www.icsi.berkeley.edu",
        "aff_unique_abbr": "UPenn;Dartmouth;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Correlational Gaussian Processes for Cross-Domain Visual Recognition",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "47",
        "author_site": "Chengjiang Long, Gang Hua",
        "author": "Chengjiang Long; Gang Hua",
        "abstract": "We present a probabilistic model that captures higher order co-occurrence statistics for joint visual recognition in a collection of images and across multiple domains. More importantly, we predict the structured output across multiple domains by correlating outputs from the multi-classes Gaussian process classifiers in each individual domain. A set of correlational tensors is adopted to model the relationship within a single domain as well as across multiple domains. This renders it possible to explore a high-order relational model instead of using just a set of pairwise relational models. Such tensor relations are based on both the positive and negative co-occurrences of different categories of visual instances across multi-domains. This is in contrast to most previous models where only pair-wise relationships are explored. We conduct experiments on four challenging image collections. The experimental results clearly demonstrate the efficacy of our proposed model.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Long_Correlational_Gaussian_Processes_CVPR_2017_paper.pdf",
        "aff": "Kitware Inc.; Microsoft Research Asia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 952708,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2613928230429714854&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "kitware.com;gmail.com",
        "email": "kitware.com;gmail.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Long_Correlational_Gaussian_Processes_CVPR_2017_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Kitware Inc.;Microsoft",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://www.kitware.com;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "Kitware;MSR Asia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Counting Everyday Objects in Everyday Scenes",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "386",
        "author_site": "Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath R. Selvaraju, Dhruv Batra, Devi Parikh",
        "author": "Prithvijit Chattopadhyay; Ramakrishna Vedantam; Ramprasaath R. Selvaraju; Dhruv Batra; Devi Parikh",
        "abstract": "We are interested in counting the number of instances of object classes in natural, everyday images. Previous counting approaches tackle the problem in restricted domains such as counting pedestrians in surveillance videos. Counts can also be estimated from outputs of other vision tasks like object detection. In this work, we build dedicated models for counting designed to tackle the large variance in counts, appearances, and scales of objects found in natural scenes. Our approach is inspired by the phenomenon of subitizing - the ability of humans to make quick assessments of counts given a perceptual signal, for small count values. Given a natural scene, we employ a divide and conquer strategy while incorporating context across the scene to adapt the subitizing idea to counting. Our approach offers consistent improvements over numerous baseline approaches for counting on the PASCAL VOC 2007 and COCO datasets. Subsequently, we study how counting can be used to improve object detection. We then show a proof of concept application of our counting methods to the task of Visual Question Answering, by studying the 'how many?' questions in the VQA and COCO-QA datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chattopadhyay_Counting_Everyday_Objects_CVPR_2017_paper.pdf",
        "aff": "Virginia Tech; Virginia Tech; Virginia Tech; Georgia Institute of Technology; Georgia Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1604.03505",
        "pdf_size": 1237963,
        "gs_citation": 205,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10721680110605566702&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "vt.edu;vt.edu;vt.edu;gatech.edu;gatech.edu",
        "email": "vt.edu;vt.edu;vt.edu;gatech.edu;gatech.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chattopadhyay_Counting_Everyday_Objects_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Virginia Tech;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.vt.edu;https://www.gatech.edu",
        "aff_unique_abbr": "VT;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Creativity: Generating Diverse Questions Using Variational Autoencoders",
        "session": "Machine Learning 4",
        "status": "Spotlight",
        "track": "main",
        "pid": "2922",
        "author_site": "Unnat Jain, Ziyu Zhang, Alexander G. Schwing",
        "author": "Unnat Jain; Ziyu Zhang; Alexander G. Schwing",
        "abstract": "Generating diverse questions for given images is an important task for computational education, entertainment and AI assistants. Different from many conventional prediction techniques is the need for algorithms to generate a diverse set of plausible questions, which we refer to as \"creativity\". In this paper we propose a  creative algorithm for visual question generation which combines the advantages of variational autoencoders with long short-term memory networks. We demonstrate that our framework is able to generate a large set of varying questions given a single input image.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jain_Creativity_Generating_Diverse_CVPR_2017_paper.pdf",
        "aff": "UIUC; Northwestern University; UIUC",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Jain_Creativity_Generating_Diverse_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.03493",
        "pdf_size": 4768757,
        "gs_citation": 189,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10305482717892593406&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "illinois.edu;u.northwestern.edu;illinois.edu",
        "email": "illinois.edu;u.northwestern.edu;illinois.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jain_Creativity_Generating_Diverse_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Northwestern University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www illinois.edu;https://www.northwestern.edu",
        "aff_unique_abbr": "UIUC;NU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Cross-Modality Binary Code Learning via Fusion Similarity Hashing",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3667",
        "author_site": "Hong Liu, Rongrong Ji, Yongjian Wu, Feiyue Huang, Baochang Zhang",
        "author": "Hong Liu; Rongrong Ji; Yongjian Wu; Feiyue Huang; Baochang Zhang",
        "abstract": "Binary code learning has been emerging topic in large-scale cross-modality retrieval recently. It aims to map features from multiple modalities into a common Hamming space, where the cross-modality similarity can be approximated efficiently via Hamming distance. To this end, most existing works learn binary codes directly from data instances in multiple modalities, which preserve both intra- and inter-modal similarities respectively. Few methods consider to preserve the \"fusion similarity\" among multi-modal instances instead, which can explicitly capture their heterogeneous correlation in cross-modality retrieval. In this paper, we propose a hashing scheme, termed Fusion Similarity Hashing (FSH), which explicitly embeds the graph-based fusion similarity across modalities into a common Hamming space. Inspired by the \"fusion by diffusion\", our core idea is to construct an undirected asymmetric graph to model the fusion similarity among different modalities, upon which a graph hashing scheme with alternating optimization is introduced to learn binary codes that embeds such fusion similarity. Quantitative evaluations on three widely used benchmarks, i.e., UCI Handwritten Digit, MIR-Flickr25K and NUS-WIDE, demonstrate that the proposed FSH approach can achieve superior performance over the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Cross-Modality_Binary_Code_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 255,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17391349891903593138&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Cross-Modality_Binary_Code_CVPR_2017_paper.html"
    },
    {
        "title": "Cross-View Image Matching for Geo-Localization in Urban Environments",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1325",
        "author_site": "Yicong Tian, Chen Chen, Mubarak Shah",
        "author": "Yicong Tian; Chen Chen; Mubarak Shah",
        "abstract": "In this paper, we address the problem of cross-view image geo-localization. Specifically, we aim to estimate the GPS location of a query street view image by finding the matching images in a reference database of geo-tagged bird's eye view images, or vice versa. To this end, we present a new framework for cross-view image geo-localization by taking advantage of the tremendous success of deep convolutional neural networks (CNNs) in image  classification and object detection. First, we employ the Faster R-CNN to detect buildings in the query and reference images. Next, for each building in the query image, we retrieve the k nearest neighbors from the reference buildings using a Siamese network trained on both positive matching image pairs and negative pairs. To find the correct NN for each query building, we develop an efficient multiple nearest neighbors matching method based on dominant sets. We evaluate the proposed framework on a new dataset that consists of pairs of street view and bird's eye view images. Experimental results show that the proposed method achieves better geo-localization accuracy than other approaches and is able to generalize to images at unseen locations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tian_Cross-View_Image_Matching_CVPR_2017_paper.pdf",
        "aff": "Center for Research in Computer Vision (CRCV), University of Central Florida (UCF); Center for Research in Computer Vision (CRCV), University of Central Florida (UCF); Center for Research in Computer Vision (CRCV), University of Central Florida (UCF)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.07815v1",
        "pdf_size": 2804739,
        "gs_citation": 233,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5415464698488790487&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;gmail.com;crcv.ucf.edu",
        "email": "gmail.com;gmail.com;crcv.ucf.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tian_Cross-View_Image_Matching_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Central Florida",
        "aff_unique_dep": "Center for Research in Computer Vision (CRCV)",
        "aff_unique_url": "https://www.ucf.edu",
        "aff_unique_abbr": "UCF",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "UCF",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Crossing Nets: Combining GANs and VAEs With a Shared Latent Space for Hand Pose Estimation",
        "session": "Analyzing Humans 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "210",
        "author_site": "Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao",
        "author": "Chengde Wan; Thomas Probst; Luc Van Gool; Angela Yao",
        "abstract": "State-of-the-art methods for 3D hand pose estimation from depth images require large amounts of annotated training data. We propose modelling the statistical relationship of 3D hand poses and corresponding depth images using two deep generative models with a shared latent space. By design, our architecture allows for learning from unlabeled image data in a semi-supervised manner. Assuming a one-to-one mapping between a pose and a depth map, any given point in the shared latent space can be projected into both a hand pose or into a corresponding depth map. Regressing the hand pose can then be done by learning a discriminator to estimate the posterior of the latent pose given some depth map. To prevent over-fitting and to better exploit unlabeled depth maps, the generator and discriminator are trained jointly. At each iteration, the generator is updated with the back-propagated gradient from the discriminator to synthesize realistic depth maps of the articulated hand, while the discriminator benefits from an augmented training set of synthesized samples and unlabeled depth maps. The proposed discriminator network architecture is highly efficient and runs at 90fps on the CPU with accuracies comparable or better than state-of-art on 3 publicly available benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wan_Crossing_Nets_Combining_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.03431v2",
        "pdf_size": 1850812,
        "gs_citation": 184,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11907053087113877039&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wan_Crossing_Nets_Combining_CVPR_2017_paper.html"
    },
    {
        "title": "DESIRE: Distant Future Prediction in Dynamic Scenes With Interacting Agents",
        "session": "Machine Learning 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "122",
        "author_site": "Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B. Choy, Philip H. S. Torr, Manmohan Chandraker",
        "author": "Namhoon Lee; Wongun Choi; Paul Vernaza; Christopher B. Choy; Philip H. S. Torr; Manmohan Chandraker",
        "abstract": "We introduce a Deep Stochastic IOC RNN Encoder-decoder framework, DESIRE, for the task of future predictions of multiple interacting agents in dynamic scenes. DESIRE effectively predicts future locations of objects in multiple scenes by 1) accounting for the multi-modal nature of the future prediction (i.e., given the same context, future may vary), 2) foreseeing the potential future outcomes and make a strategic prediction based on that, and 3) reasoning not only from the past motion history, but also from the scene context as well as the interactions among the agents. DESIRE achieves these in a single end-to-end trainable neural network model, while being computationally efficient. The model first obtains a diverse set of hypothetical future prediction samples employing a conditional variational auto-encoder, which are ranked and refined by the following RNN scoring-regression module. Samples are scored by accounting for accumulated future rewards, which enables better long-term strategic decisions similar to IOC frameworks. An RNN scene context fusion module jointly captures past motion histories, the semantic scene context and interactions among multiple agents. A feedback mechanism iterates over the ranking and refinement to further boost the prediction accuracy. We evaluate our model on two publicly available datasets: KITTI and Stanford Drone Dataset. Our experiments show that the proposed model significantly improves the prediction accuracy compared to other baseline methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lee_DESIRE_Distant_Future_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Lee_DESIRE_Distant_Future_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.04394v1",
        "gs_citation": 1300,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11959830312445464047&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lee_DESIRE_Distant_Future_CVPR_2017_paper.html"
    },
    {
        "title": "DOPE: Distributed Optimization for Pairwise Energies",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3112",
        "author_site": "Jose Dolz, Ismail Ben Ayed, Christian Desrosiers",
        "author": "Jose Dolz; Ismail Ben Ayed; Christian Desrosiers",
        "abstract": "We formulate an Alternating Direction Method of Multipliers (ADMM) that systematically distributes the computations of any technique for optimizing pairwise functions, including  non-submodular potentials. Such discrete functions are very useful in segmentation and a breadth of other vision problems. Our method decomposes the problem into a large set of small sub-problems, each involving a sub-region of the image domain, which can be solved in parallel. We achieve consistency between the sub-problems through a novel constraint that can be used for a large class of pairwise functions. We give an iterative numerical solution that alternates between solving the sub-problems and updating consistency variables, until convergence. We report comprehensive experiments, which demonstrate the benefit of our general distributed solution in the case of the popular serial algorithm of Boykov and Kolmogorov (BK algorithm) and, also, in the context of non-submodular functions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dolz_DOPE_Distributed_Optimization_CVPR_2017_paper.pdf",
        "aff": "Laboratory for Imagery, Vision and Artificial Intelligence, Ecole de Technologie Superieure, Montreal, Canada; Laboratory for Imagery, Vision and Artificial Intelligence, Ecole de Technologie Superieure, Montreal, Canada; Laboratory for Imagery, Vision and Artificial Intelligence, Ecole de Technologie Superieure, Montreal, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03116",
        "pdf_size": 10256612,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5697202206500696866&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 11,
        "aff_domain": "livia.etsmtl.ca;etsmtl.ca;etsmtl.ca",
        "email": "livia.etsmtl.ca;etsmtl.ca;etsmtl.ca",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dolz_DOPE_Distributed_Optimization_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Ecole de technologie superieure",
        "aff_unique_dep": "Laboratory for Imagery, Vision and Artificial Intelligence",
        "aff_unique_url": "https://www.etsmtl.ca",
        "aff_unique_abbr": "ETS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Montreal",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "DSAC - Differentiable RANSAC for Camera Localization",
        "session": "3D Vision 2",
        "status": "Oral",
        "track": "main",
        "pid": "3060",
        "author_site": "Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, Carsten Rother",
        "author": "Eric Brachmann; Alexander Krull; Sebastian Nowozin; Jamie Shotton; Frank Michel; Stefan Gumhold; Carsten Rother",
        "abstract": "RANSAC is an important algorithm in robust optimization and a central building block for many computer vision applications. In recent years, traditionally hand-crafted pipelines have been replaced by deep learning pipelines, which can be trained in an end-to-end fashion. However, RANSAC has so far not been used as part of such deep learning pipelines, because its hypothesis selection procedure is non-differentiable. In this work, we present two different ways to overcome this limitation. The most promising approach is inspired by reinforcement learning, namely to replace the deterministic hypothesis selection by a probabilistic selection for which we can derive the expected loss w.r.t. to all learnable parameters. We call this approach DSAC, the differentiable counterpart of RANSAC. We apply DSAC to the problem of camera localization, where deep learning has so far failed to improve on traditional approaches. We demonstrate that by directly minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we achieve an increase in accuracy. In the future, any deep learning pipeline can use DSAC as a robust optimization component.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Brachmann_DSAC_-_Differentiable_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Brachmann_DSAC_-_Differentiable_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 737,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14275705625585988293&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Brachmann_DSAC_-_Differentiable_CVPR_2017_paper.html"
    },
    {
        "title": "DUST: Dual Union of Spatio-Temporal Subspaces for Monocular Multiple Object 3D Reconstruction",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2775",
        "author_site": "Antonio Agudo, Francesc Moreno-Noguer",
        "author": "Antonio Agudo; Francesc Moreno-Noguer",
        "abstract": "We present an approach to reconstruct the 3D shape of multiple deforming objects from incomplete 2D trajectories acquired by a single camera. Additionally, we simultaneously provide spatial segmentation (i.e., we identify each of the objects in every frame) and temporal clustering (i.e., we split the sequence into primitive actions). This advances existing work, which only tackled the  problem for  one single object and non-occluded tracks. In order to handle several objects at a time from partial observations, we model point  trajectories as a union of spatial and temporal subspaces, and optimize the parameters of both modalities,  the non-observed point tracks and the 3D shape via augmented Lagrange multipliers. The algorithm is fully unsupervised and results in a formulation which does not need initialization. We thoroughly validate the method on challenging scenarios with several human subjects performing different activities which involve complex motions and close interaction. We show our approach achieves state-of-the-art 3D reconstruction results, while it also provides spatial and temporal segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Agudo_DUST_Dual_Union_CVPR_2017_paper.pdf",
        "aff": "Institut de Rob `otica i Inform `atica Industrial (CSIC-UPC), 08028, Barcelona, Spain; Institut de Rob `otica i Inform `atica Industrial (CSIC-UPC), 08028, Barcelona, Spain",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Agudo_DUST_Dual_Union_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 8914709,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17467089910288469223&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Agudo_DUST_Dual_Union_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Institut de Rob\u00f3tica i Inform\u00e0tica Industrial",
        "aff_unique_dep": "CSIC-UPC",
        "aff_unique_url": "https://www.iri.upc.edu/",
        "aff_unique_abbr": "IRI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Barcelona",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Spain"
    },
    {
        "title": "DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "64",
        "author_site": "Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu",
        "author": "Swaminathan Gurumurthy; Ravi Kiran Sarvadevabhatla; R. Venkatesh Babu",
        "abstract": "A class of recent approaches for generating images, called Generative Adversarial Networks (GAN), have been used to generate impressively realistic images of objects, bedrooms, handwritten digits and a variety of other image modalities. However, typical GAN-based approaches require large amounts of training data to capture the diversity across the image modality. In this paper, we propose DeLiGAN -- a novel GAN-based architecture for diverse and limited training data scenarios. In our approach, we reparameterize the latent generative space as a mixture model and learn the mixture model's parameters along with those of GAN. This seemingly simple modification to the GAN framework is surprisingly effective and results in models which enable diversity in generated samples although trained with limited data. In our work, we show that DeLiGAN can generate images of handwritten digits, objects and hand-drawn sketches, all using limited amounts of data. To quantitatively characterize intra-class diversity of generated samples, we also introduce a modified version of \"inception-score\", a measure which has been found to correlate well with human assessment of generated samples.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper.pdf",
        "aff": "Video Analytics Lab, CDS, Indian Institute of Science; Video Analytics Lab, CDS, Indian Institute of Science; Video Analytics Lab, CDS, Indian Institute of Science",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1321892,
        "gs_citation": 377,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16366073296318004852&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "gmail.com;gmail.com;cds.iisc.ac.in",
        "email": "gmail.com;gmail.com;cds.iisc.ac.in",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Computer Data Systems",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "DeMoN: Depth and Motion Network for Learning Monocular Stereo",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2080",
        "author_site": "Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, Thomas Brox",
        "author": "Benjamin Ummenhofer; Huizhong Zhou; Jonas Uhrig; Nikolaus Mayer; Eddy Ilg; Alexey Dosovitskiy; Thomas Brox",
        "abstract": "In this paper we formulate structure from motion as a learning problem. We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs. The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions. The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching. A crucial component of the approach is a training loss based on spatial relative differences. Compared to traditional two-frame structure from motion methods, results are more accurate and more robust. In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ummenhofer_DeMoN_Depth_and_CVPR_2017_paper.pdf",
        "aff": "University of Freiburg+Daimler AG R&D; University of Freiburg+Daimler AG R&D; University of Freiburg; University of Freiburg; University of Freiburg; University of Freiburg; University of Freiburg",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ummenhofer_DeMoN_Depth_and_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.02401v2",
        "pdf_size": 2926897,
        "gs_citation": 880,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17697059828222209959&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de",
        "email": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ummenhofer_DeMoN_Depth_and_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0;0;0;0;0",
        "aff_unique_norm": "University of Freiburg;Daimler AG",
        "aff_unique_dep": ";Research and Development",
        "aff_unique_url": "https://www.uni-freiburg.de;https://www.daimler.com",
        "aff_unique_abbr": "UoF;Daimler",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Deep 360 Pilot: Learning a Deep Agent for Piloting Through 360deg Sports Videos",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Oral",
        "track": "main",
        "pid": "1277",
        "author_site": "Hou-Ning Hu, Yen-Chen Lin, Ming-Yu Liu, Hsien-Tzu Cheng, Yung-Ju Chang, Min Sun",
        "author": "Hou-Ning Hu; Yen-Chen Lin; Ming-Yu Liu; Hsien-Tzu Cheng; Yung-Ju Chang; Min Sun",
        "abstract": "Watching a 360* sports video requires a viewer to continuously select a viewing angle, either through a sequence of mouse clicks or head movements. To relieve the viewer from this \"360 piloting\" task, we propose \"deep 360 pilot\" - a deep learning-based agent for piloting through 360* sports videos automatically. At each frame, the agent observes a panoramic image and has the knowledge of previously selected viewing angles. The task of the agent is to shift the current viewing angle (i.e. action) to the next preferred one (i.e., goal). We propose to directly learn an online policy of the agent from data. Specifically, we leverage a state-of-the-art object detector to propose a few candidate objects of interest (yellow boxes in Fig. 1). Then, a recurrent neural network is used to select the main object (green dash boxes in Fig. 1). Given the main object and previously selected viewing angles, our method regresses a shift in viewing angle to move to the next one. We use the policy gradient technique to jointly train our pipeline, by minimizing: (1) a regression loss measuring the distance between the selected and ground truth viewing angles, (2) a smoothness loss encouraging smooth transition in viewing angle, and (3) maximizing an expected reward of focusing on a foreground object. To evaluate our method, we built a new 360-Sports video dataset consisting of five sports domains. We trained domain-specific agents and achieved the best performance on viewing angle selection accuracy and users' preference compared to [54] and other baselines.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hu_Deep_360_Pilot_CVPR_2017_paper.pdf",
        "aff": "National Tsing Hua University; National Tsing Hua University; NVIDIA research; National Tsing Hua University; National Chiao Tung University; National Tsing Hua University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Hu_Deep_360_Pilot_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1835791,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10536067817637085362&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gapp.nthu.edu.tw;gapp.nthu.edu.tw;gmail.com;gmail.com;cs.nctu.edu.tw;ee.nthu.edu.tw",
        "email": "gapp.nthu.edu.tw;gapp.nthu.edu.tw;gmail.com;gmail.com;cs.nctu.edu.tw;ee.nthu.edu.tw",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hu_Deep_360_Pilot_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0;2;0",
        "aff_unique_norm": "National Tsing Hua University;NVIDIA;National Chiao Tung University",
        "aff_unique_dep": ";NVIDIA Research;",
        "aff_unique_url": "https://www.nthu.edu.tw;https://www.nvidia.com/research;https://www.nctu.edu.tw",
        "aff_unique_abbr": "NTHU;NVIDIA;NCTU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Taiwan;",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Deep Affordance-Grounded Sensorimotor Object Recognition",
        "session": "Machine Learning 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "2715",
        "author_site": "Spyridon Thermos, Georgios Th. Papadopoulos, Petros Daras, Gerasimos Potamianos",
        "author": "Spyridon Thermos; Georgios Th. Papadopoulos; Petros Daras; Gerasimos Potamianos",
        "abstract": "It is well-established by cognitive neuroscience that human perception of objects constitutes a complex process, where object appearance information is combined with evidence about the so-called object \"affordances\", namely the types of actions that humans typically perform when interacting with them. This fact has recently motivated the \"sensorimotor\" approach to the challenging task of automatic object recognition, where both information sources are fused to improve robustness. In this work, the aforementioned paradigm is adopted, surpassing current limitations of sensorimotor object recognition research. Specifically, the deep learning paradigm is introduced to the problem for the first time, developing a number of novel neuro-biologically and neuro-physiologically inspired architectures that utilize state-of-the-art neural networks for fusing the available information sources in multiple ways. The proposed methods are evaluated using a large RGB-D corpus, which is specifically collected for the task of sensorimotor object recognition and is made publicly available. Experimental results demonstrate the utility of affordance information to object recognition, achieving an up to 29% relative error reduction by its inclusion.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Thermos_Deep_Affordance-Grounded_Sensorimotor_CVPR_2017_paper.pdf",
        "aff": "Information Technologies Institute, Centre for Research and Technology Hellas, Greece+Department of Electrical and Computer Engineering, University of Thessaly, Greece; Information Technologies Institute, Centre for Research and Technology Hellas, Greece; Information Technologies Institute, Centre for Research and Technology Hellas, Greece; Department of Electrical and Computer Engineering, University of Thessaly, Greece",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.02787",
        "pdf_size": 2483841,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17109228179352852115&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "iti.gr;iti.gr;iti.gr;ieee.org",
        "email": "iti.gr;iti.gr;iti.gr;ieee.org",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Thermos_Deep_Affordance-Grounded_Sensorimotor_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;1",
        "aff_unique_norm": "Centre for Research and Technology Hellas;University of Thessaly",
        "aff_unique_dep": "Information Technologies Institute;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.cert.hellas.gr;https://www.uth.gr",
        "aff_unique_abbr": "CERTH;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Greece"
    },
    {
        "title": "Deep Co-Occurrence Feature Learning for Visual Object Recognition",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1676",
        "author_site": "Ya-Fang Shih, Yang-Ming Yeh, Yen-Yu Lin, Ming-Fang Weng, Yi-Chang Lu, Yung-Yu Chuang",
        "author": "Ya-Fang Shih; Yang-Ming Yeh; Yen-Yu Lin; Ming-Fang Weng; Yi-Chang Lu; Yung-Yu Chuang",
        "abstract": "This paper addresses three issues in integrating part-based representations into convolutional neural networks (CNNs) for object recognition. First, most part-based models rely on a few pre-specified object parts. However, the optimal object parts for recognition often vary from category to category. Second, acquiring training data with part-level annotation is labor-intensive. Third, modeling spatial relationships between parts in CNNs often involves an exhaustive search of part templates over multiple network streams. We tackle the three issues by introducing a new network layer, called co-occurrence layer. It can extend a convolutional layer to encode the co-occurrence between the visual parts detected by the numerous neurons, instead of a few pre-specified parts. To this end, the feature maps serve as both filters and images, and mutual correlation filtering is conducted between them. The co-occurrence layer is end-to-end trainable. The resultant co-occurrence features are rotation- and translation-invariant, and are robust to object deformation. By applying this new layer to the VGG-16 and ResNet-152, we achieve the recognition rates of 83.6% and 85.8% on the Caltech-UCSD bird benchmark, respectively. The source code is available at https://github.com/yafangshih/Deep-COOC.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shih_Deep_Co-Occurrence_Feature_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10588934164328331894&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shih_Deep_Co-Occurrence_Feature_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Crisp Boundaries",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1512",
        "author_site": "Yupei Wang, Xin Zhao, Kaiqi Huang",
        "author": "Yupei Wang; Xin Zhao; Kaiqi Huang",
        "abstract": "Edge detection had made significant progress with the help of deep Convolutional Networks (ConvNet). ConvNet based edge detectors approached human level performance on standard benchmarks. We provide a systematical study of these detector outputs, and show that they failed to accurately localize edges, which can be adversarial for tasks that require crisp edge inputs. In addition, we propose a novel refinement architecture to address the challenging problem of learning a crisp edge detector using ConvNet. Our method leverages a top-down backward refinement pathway, and progressively increases the resolution of feature maps to generate crisp edges. Our results achieve promising performance on BSDS500, surpassing human accuracy when using standard criteria, and largely outperforming state-of-the-art methods when using more strict criteria. We further demonstrate the benefit of crisp edge maps for estimating optical flow and generating object proposals.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Deep_Crisp_Boundaries_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1801.02439v3",
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=766082632764083124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Deep_Crisp_Boundaries_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Cross-Modal Hashing",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "1194",
        "author_site": "Qing-Yuan Jiang, Wu-Jun Li",
        "author": "Qing-Yuan Jiang; Wu-Jun Li",
        "abstract": "Due to its low storage cost and fast query speed, cross-modal hashing (CMH) has been widely used for similarity search in multimedia retrieval applications. However, most existing CMH methods are based on hand-crafted features which might not be optimally compatible with the hash-code learning procedure. As a result, existing CMH methods with hand-crafted features may not achieve satisfactory performance. In this paper, we propose a novel CMH method, called deep cross-modal hashing (DCMH), by integrating feature learning and hash-code learning intothe same framework. DCMH is an end-to-end learning framework with deep neural networks, one for each modality, to perform feature learning from scratch. Experiments on three real datasets with image-text modalities show that DCMH can outperform other baselines to achieve the state-of-the-art performance in cross-modal retrieval applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jiang_Deep_Cross-Modal_Hashing_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1602.02255v2",
        "gs_citation": 929,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17946468176227632778&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jiang_Deep_Cross-Modal_Hashing_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Feature Flow for Video Recognition",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "863",
        "author_site": "Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, Yichen Wei",
        "author": "Xizhou Zhu; Yuwen Xiong; Jifeng Dai; Lu Yuan; Yichen Wei",
        "abstract": "Deep convolutional neutral networks have achieved great success on image recognition tasks. Yet, it is non-trivial to transfer the state-of-the-art image recognition networks to videos as per-frame evaluation is too slow and unaffordable. We present deep feature flow, a fast and accurate framework for video recognition. It runs the expensive convolutional sub-network only on sparse key frames and propagates their deep feature maps to other frames via a flow field. It achieves significant speedup as flow computation is relatively fast. The end-to-end training of the whole architecture significantly boosts the recognition accuracy. Deep feature flow is flexible and general. It is validated on two recent large scale video datasets. It makes a large step towards practical video recognition. Code would be released.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Deep_Feature_Flow_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.07715v2",
        "gs_citation": 889,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18443351914908959065&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhu_Deep_Feature_Flow_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Feature Interpolation for Image Content Changes",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3299",
        "author_site": "Paul Upchurch, Jacob Gardner, Geoff Pleiss, Robert Pless, Noah Snavely, Kavita Bala, Kilian Weinberger",
        "author": "Paul Upchurch; Jacob Gardner; Geoff Pleiss; Robert Pless; Noah Snavely; Kavita Bala; Kilian Weinberger",
        "abstract": "We propose Deep Feature Interpolation (DFI), a new data- driven baseline for automatic high-resolution image transformation. As the name suggests, DFI relies only on simple linear interpolation of deep convolutional features from pre-trained convnets. We show that despite its simplicity, DFI can perform high-level semantic transformations like \"make older/younger\", \"make bespectacled\", \"add smile\", among others, surprisingly well--sometimes even matching or outperforming the state-of-the-art. This is particularly unexpected as DFI requires no specialized network architecture or even any deep network to be trained for these tasks. DFI therefore can be used as a new baseline to evaluate more complex algorithms and provides a practical answer to the question of which image transformation tasks are still challenging after the advent of deep learning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Upchurch_Deep_Feature_Interpolation_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Upchurch_Deep_Feature_Interpolation_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.05507v2",
        "pdf_size": 2700799,
        "gs_citation": 386,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11990962697703675714&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Upchurch_Deep_Feature_Interpolation_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Future Gaze: Gaze Anticipation on Egocentric Videos Using Adversarial Networks",
        "session": "Applications",
        "status": "Oral",
        "track": "main",
        "pid": "1775",
        "author_site": "Mengmi Zhang, Keng Teck Ma, Joo Hwee Lim, Qi Zhao, Jiashi Feng",
        "author": "Mengmi Zhang; Keng Teck Ma; Joo Hwee Lim; Qi Zhao; Jiashi Feng",
        "abstract": "We introduce a new problem of gaze anticipation on egocentric videos. This substantially extends the conventional gaze prediction problem to future frames by no longer confining it on the current frame. To solve this problem, we propose a new generative adversarial neural network based model, Deep Future Gaze (DFG). DFG generates multiple future frames conditioned on the single current frame and anticipates corresponding future gazes in next few seconds. It consists of two networks: generator and discriminator. The generator uses a two-stream spatial temporal convolution architecture (3D-CNN) explicitly untangling the foreground and the background to generate future frames. It then attaches another 3D-CNN for gaze anticipation based on these synthetic frames. The discriminator plays against the generator by differentiating the synthetic frames of the generator from the real frames. Through competition with discriminator, the generator progressively improves quality of the future frames and thus anticipates future gaze better. Experimental results on the publicly available egocentric datasets show that DFG significantly outperforms all well-established baselines. Moreover, we demonstrate that DFG achieves better performance of gaze prediction on current frames than state-of-the-art methods. This is due to benefiting from learning motion discriminative representations in frame generation. We further contribute a new egocentric dataset (OST) in the object search task. DFG also achieves the best performance for this challenging dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Deep_Future_Gaze_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_Deep_Future_Gaze_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13797256095695355621&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Deep_Future_Gaze_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Hashing Network for Unsupervised Domain Adaptation",
        "session": "Machine Learning 4",
        "status": "Spotlight",
        "track": "main",
        "pid": "2074",
        "author_site": "Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, Sethuraman Panchanathan",
        "author": "Hemanth Venkateswara; Jose Eusebio; Shayok Chakraborty; Sethuraman Panchanathan",
        "abstract": "In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efficiency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we first introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the first research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efficient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Venkateswara_Deep_Hashing_Network_CVPR_2017_paper.pdf",
        "aff": "Center for Cognitive Ubiquitous Computing, Arizona State University, Tempe, AZ, USA; Center for Cognitive Ubiquitous Computing, Arizona State University, Tempe, AZ, USA; Center for Cognitive Ubiquitous Computing, Arizona State University, Tempe, AZ, USA; Center for Cognitive Ubiquitous Computing, Arizona State University, Tempe, AZ, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Venkateswara_Deep_Hashing_Network_2017_CVPR_supplemental.pdf",
        "arxiv": "1706.07522v1",
        "pdf_size": 1982188,
        "gs_citation": 2742,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2740507953562184613&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "asu.edu;asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu;asu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Venkateswara_Deep_Hashing_Network_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "Center for Cognitive Ubiquitous Computing",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Tempe",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Image Harmonization",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1434",
        "author_site": "Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, Ming-Hsuan Yang",
        "author": "Yi-Hsuan Tsai; Xiaohui Shen; Zhe Lin; Kalyan Sunkavalli; Xin Lu; Ming-Hsuan Yang",
        "abstract": "Compositing is one of the most common operations in photo editing. To generate realistic composites, the appearances of foreground and background need to be adjusted to make them compatible. Previous approaches to harmonize composites have focused on learning statistical relationships between hand-crafted appearance features of the foreground and background, which is unreliable especially when the contents in the two layers are vastly different. In this work, we propose an end-to-end deep convolutional neural network for image harmonization, which can capture both the context and semantic information of the composite images during harmonization. We also introduce an efficient way to collect large-scale and high-quality training data that can facilitate the training process. Experiments on the synthesized dataset and real composite images show that the proposed network outperforms previous state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tsai_Deep_Image_Harmonization_CVPR_2017_paper.pdf",
        "aff": "University of California, Merced; Adobe Research; Adobe Research; Adobe Research; Adobe Research; University of California, Merced",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Tsai_Deep_Image_Harmonization_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.00069v1",
        "pdf_size": 1423981,
        "gs_citation": 340,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2107303661736723197&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ucmerced.edu;ucmerced.edu;adobe.com;adobe.com;adobe.com;adobe.com",
        "email": "ucmerced.edu;ucmerced.edu;adobe.com;adobe.com;adobe.com;adobe.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tsai_Deep_Image_Harmonization_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "University of California, Merced;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.ucmerced.edu;https://research.adobe.com",
        "aff_unique_abbr": "UC Merced;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Merced;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Image Matting",
        "session": "Low- & Mid-Level Vision",
        "status": "Oral",
        "track": "main",
        "pid": "1081",
        "author_site": "Ning Xu, Brian Price, Scott Cohen, Thomas Huang",
        "author": "Ning Xu; Brian Price; Scott Cohen; Thomas Huang",
        "abstract": "Image matting is a fundamental computer vision problem and has many applications. Previous algorithms have poor performance when an image has similar foreground and background colors or complicated textures. The main reasons are prior methods 1) only use low-level features and 2) lack high-level context. In this paper, we propose a novel deep learning based algorithm that can tackle both these problems. Our deep model has two parts. The first part is a deep convolutional encoder-decoder network that takes an image and the corresponding trimap as inputs and predict the alpha matte of the image. The second part is a small convolutional network that refines the alpha matte predictions of the first network to have more accurate alpha values and sharper edges. In addition, we also create a large-scale image matting dataset including 49300 training images and 1000 testing images. We evaluate our algorithm on the image matting benchmark, our testing set, and a wide variety of real images. Experimental results clearly demonstrate the superiority of our algorithm over previous methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Deep_Image_Matting_CVPR_2017_paper.pdf",
        "aff": "Beckman Institute for Advanced Science and Technology + University of Illinois at Urbana-Champaign; Adobe Research; Adobe Research; Beckman Institute for Advanced Science and Technology + University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.03872v3",
        "pdf_size": 4204358,
        "gs_citation": 685,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8173380115462127179&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "illinois.edu;adobe.com;adobe.com;illinois.edu",
        "email": "illinois.edu;adobe.com;adobe.com;illinois.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xu_Deep_Image_Matting_CVPR_2017_paper.html",
        "aff_unique_index": "0+0;1;1;0+0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Adobe",
        "aff_unique_dep": "Beckman Institute for Advanced Science and Technology;Adobe Research",
        "aff_unique_url": "https://beckman.illinois.edu;https://research.adobe.com",
        "aff_unique_abbr": "Beckman Institute;Adobe",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Joint Rain Detection and Removal From a Single Image",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "492",
        "author_site": "Wenhan Yang, Robby T. Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, Shuicheng Yan",
        "author": "Wenhan Yang; Robby T. Tan; Jiashi Feng; Jiaying Liu; Zongming Guo; Shuicheng Yan",
        "abstract": "In this paper, we address a rain removal problem from a single image, even in the presence of heavy rain and rain streak accumulation. Our core ideas lie in our new rain image model and new deep learning architecture. We add a binary map that provides rain streak locations to an existing model, which comprises a rain streak layer and a background layer. We create a model consisting of a component representing rain streak accumulation (where individual streaks cannot be seen, and thus visually similar to mist or fog), and another component representing various shapes and directions of overlapping rain streaks, which usually happen in heavy rain. Based on the model, we develop a multi-task deep learning architecture that learns the binary rain streak map, the appearance of rain streaks, and the clean background, which is our ultimate output. The additional binary map is critically beneficial, since its loss function can provide additional strong information to the network. To handle rain streak accumulation (again, a phenomenon visually similar to mist or fog) and various shapes and directions of overlapping rain streaks, we propose a recurrent rain detection and removal network that removes rain streaks and clears up the rain accumulation iteratively and progressively. In each recurrence of our method, a new contextualized dilated network is developed to exploit regional contextual information and to produce better representations for rain detection. The evaluation on real images, particularly on heavy rain, shows the effectiveness of our models and architecture.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_Deep_Joint_Rain_CVPR_2017_paper.pdf",
        "aff": "Institute of Computer Science and Technology, Peking University; National University of Singapore+Yale-NUS College; National University of Singapore; Institute of Computer Science and Technology, Peking University; Institute of Computer Science and Technology, Peking University; 360 AI Institute+National University of Singapore",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yang_Deep_Joint_Rain_2017_CVPR_supplemental.pdf",
        "arxiv": "1609.07769",
        "pdf_size": 3515448,
        "gs_citation": 1366,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4940345009505302354&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "; ; ; ;pku.edu.cn; ",
        "email": "; ; ; ;pku.edu.cn; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Deep_Joint_Rain_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+2;1;0;0;3+1",
        "aff_unique_norm": "Peking University;National University of Singapore;Yale-NUS College;360 AI Institute",
        "aff_unique_dep": "Institute of Computer Science and Technology;;;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.nus.edu.sg;https://www.yale-nus.edu.sg;",
        "aff_unique_abbr": "PKU;NUS;Yale-NUS;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+1;1;0;0;0+1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "title": "Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "192",
        "author_site": "Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang",
        "author": "Wei-Sheng Lai; Jia-Bin Huang; Narendra Ahuja; Ming-Hsuan Yang",
        "abstract": "Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper.pdf",
        "aff": "University of California, Merced; Virginia Tech; University of Illinois, Urbana-Champaign; University of California, Merced",
        "project": "http://vllab1.ucmerced.edu/~wlai24/LapSRN",
        "github": "",
        "supp": "",
        "arxiv": "1704.03915v2",
        "pdf_size": 2541947,
        "gs_citation": 3356,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10927478870884296040&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of California, Merced;Virginia Tech;University of Illinois",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucmerced.edu;https://www.vt.edu;https://illinois.edu",
        "aff_unique_abbr": "UC Merced;VT;UIUC",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Merced;;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Learning Human Mind for Automated Visual Classification",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Oral",
        "track": "main",
        "pid": "3126",
        "author_site": "Concetto Spampinato, Simone Palazzo, Isaak Kavasidis, Daniela Giordano, Nasim Souly, Mubarak Shah",
        "author": "Concetto Spampinato; Simone Palazzo; Isaak Kavasidis; Daniela Giordano; Nasim Souly; Mubarak Shah",
        "abstract": "What if we could effectively read the mind and transfer human visual capabilities to computer vision methods? In this paper, we aim at addressing this question by developing the first visual object classifier driven by human brain signals. In particular, we employ EEG data evoked by visual object stimuli combined with Recurrent Neural Networks (RNN) to learn a discriminative brain activity manifold of visual categories in a reading the mind effort. Afterward, we transfer the learned capabilities to machines by training a Convolutional Neural Network (CNN)-based regressor to project images onto the learned manifold, thus allowing machines to employ human brain-based features for automated visual classification. We use a 128-channel EEG with active electrodes to record brain activity of several subjects while looking at images of 40 ImageNet object classes. The proposed RNN-based approach for discriminating object classes using brain signals reaches an average accuracy of about 83%, which greatly outperforms existing methods attempting to learn EEG visual object representations. As for automated object categorization, our human brain-driven approach obtains competitive performance, comparable to those achieved by powerful CNN models and it is also able to generalize over different visual datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Spampinato_Deep_Learning_Human_CVPR_2017_paper.pdf",
        "aff": "Department of Electrical, Electronics and Computer Engineering - PeRCeiVe Lab; Department of Electrical, Electronics and Computer Engineering - PeRCeiVe Lab; Department of Electrical, Electronics and Computer Engineering - PeRCeiVe Lab; Department of Electrical, Electronics and Computer Engineering - PeRCeiVe Lab; Center for Research in Computer Vision \u2013 University of Central Florida; Center for Research in Computer Vision \u2013 University of Central Florida",
        "project": "http://perceive.dieei.unict.it; http://crcv.ucf.edu/",
        "github": "",
        "supp": "",
        "arxiv": "1609.00344v2",
        "pdf_size": 2738282,
        "gs_citation": 319,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11732910083342518458&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Spampinato_Deep_Learning_Human_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "PeRCeiVe Lab;University of Central Florida",
        "aff_unique_dep": "Department of Electrical, Electronics and Computer Engineering;Center for Research in Computer Vision",
        "aff_unique_url": ";https://www.ucf.edu",
        "aff_unique_abbr": ";UCF",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Orlando",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Deep Learning With Low Precision by Half-Wave Gaussian Quantization",
        "session": "Machine Learning 4",
        "status": "Spotlight",
        "track": "main",
        "pid": "2557",
        "author_site": "Zhaowei Cai, Xiaodong He, Jian Sun, Nuno Vasconcelos",
        "author": "Zhaowei Cai; Xiaodong He; Jian Sun; Nuno Vasconcelos",
        "abstract": "The problem of quantizing the activations of a deep neural network is considered. An examination of the popular binary quantization approach shows that this consists of approximating a classical non-linearity, the hyperbolic tangent, by two functions: a piecewise constant sign function, which is used in feedforward network computations, and a piecewise linear hard tanh function, used in the backpropagation step during network learning. The problem of approximating the widely used ReLU non-linearity is then considered. An half-wave Gaussian quantizer (HWGQ) is proposed for forward approximation and shown to have efficient implementation, by exploiting the statistics of of network activations and batch normalization operations. To overcome the problem of gradient mismatch, due to the use of different forward and backward approximations, several piece-wise backward approximators are then investigated. The implementation of the resulting quantized network, denoted as HWGQ-Net, is shown to achieve much closer performance to full precision networks, such as AlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision networks, with 1-bit binary weights and 2-bit quantized activations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cai_Deep_Learning_With_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.00953v1",
        "gs_citation": 632,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4389194298658911796&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cai_Deep_Learning_With_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Learning of Human Visual Sensitivity in Image Quality Assessment Framework",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "610",
        "author_site": "Jongyoo Kim, Sanghoon Lee",
        "author": "Jongyoo Kim; Sanghoon Lee",
        "abstract": "Since human observers are the ultimate receivers of digital images, image quality metrics should be designed from a human-oriented perspective. Conventionally, a number of full-reference image quality assessment (FR-IQA) methods adopted various computational models of the human visual system (HVS) from psychological vision science research. In this paper, we propose a novel convolutional neural networks (CNN) based FR-IQA model, named Deep Image Quality Assessment (DeepQA), where the behavior of the HVS is learned from the underlying data distribution of IQA databases. Different from previous studies, our model seeks the optimal visual weight based on understanding of database information itself without any prior knowledge of the HVS. Through the experiments, we show that the predicted visual sensitivity maps agree with the human subjective opinions. In addition, DeepQA achieves the state-of-the-art prediction accuracy among FR-IQA models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kim_Deep_Learning_of_CVPR_2017_paper.pdf",
        "aff": "Department of Electrical and Electronic Engineering, Yonsei Universiy, Seoul, Korea; Department of Electrical and Electronic Engineering, Yonsei Universiy, Seoul, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1128300,
        "gs_citation": 315,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2740183793311606230&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr",
        "email": "yonsei.ac.kr;yonsei.ac.kr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kim_Deep_Learning_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Yonsei University",
        "aff_unique_dep": "Department of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.yonsei.ac.kr",
        "aff_unique_abbr": "Yonsei",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Deep Learning on Lie Groups for Skeleton-Based Action Recognition",
        "session": "Analyzing Humans 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "2684",
        "author_site": "Zhiwu Huang, Chengde Wan, Thomas Probst, Luc Van Gool",
        "author": "Zhiwu Huang; Chengde Wan; Thomas Probst; Luc Van Gool",
        "abstract": "In recent years, skeleton-based action recognition has become a popular 3D classification problem. State-of-the-art methods typically first represent each motion sequence as a high-dimensional trajectory on a Lie group with an additional dynamic time warping, and then shallowly learn favorable Lie group features. In this paper we incorporate the Lie group structure into a deep network architecture to learn more appropriate Lie group features for 3D action recognition. Within the network structure, we design rotation mapping layers to transform the input Lie group features into desirable ones, which are aligned better in the temporal domain. To reduce the high feature  dimensionality, the architecture is equipped with rotation pooling layers for the elements on the Lie group. Furthermore, we propose a logarithm mapping layer to map the resulting manifold data into a tangent space that facilitates the application of regular output layers for the final classification. Evaluations of the proposed network for standard 3D human action recognition datasets clearly demonstrate its superiority over existing shallow Lie group feature learning methods as well as most conventional deep learning methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Deep_Learning_on_CVPR_2017_paper.pdf",
        "aff": "Computer Vision Lab, ETH Zurich, Switzerland+VISICS, KU Leuven, Belgium; Computer Vision Lab, ETH Zurich, Switzerland+VISICS, KU Leuven, Belgium; Computer Vision Lab, ETH Zurich, Switzerland+VISICS, KU Leuven, Belgium; Computer Vision Lab, ETH Zurich, Switzerland+VISICS, KU Leuven, Belgium",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.05877v2",
        "pdf_size": 2780574,
        "gs_citation": 350,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11365672510993771645&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Deep_Learning_on_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "ETH Zurich;KU Leuven",
        "aff_unique_dep": "Computer Vision Lab;VISICS",
        "aff_unique_url": "https://www.ethz.ch;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0+1;0+1",
        "aff_country_unique": "Switzerland;Belgium"
    },
    {
        "title": "Deep Level Sets for Salient Object Detection",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "839",
        "author_site": "Ping Hu, Bing Shuai, Jun Liu, Gang Wang",
        "author": "Ping Hu; Bing Shuai; Jun Liu; Gang Wang",
        "abstract": "Deep learning has been applied to saliency detection in recent years. The superior performance has proved that deep networks can model the semantic properties of salient objects. Yet it is difficult for a deep network to discriminate pixels belonging to similar receptive fields around the object boundaries, thus deep networks may output maps with blurred saliency and inaccurate boundaries. To tackle such an issue, in this work, we propose a deep Level Set network to produce compact and uniform saliency maps. Our method drives the network to learn a Level Set function for salient objects so it can output more accurate boundaries and compact saliency. Besides, to propagate saliency information among pixels and recover full resolution saliency map, we extend a superpixel-based guided filter to be a layer in the network. The proposed network has a simple structure and is trained end-to-end. During testing, the network can produce saliency maps by efficiently feedforwarding testing images at a speed over 12FPS on GPUs. Evaluations on benchmark datasets show that the proposed method achieves state-of-the-art performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hu_Deep_Level_Sets_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 238,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9640649606607488875&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hu_Deep_Level_Sets_CVPR_2017_paper.html"
    },
    {
        "title": "Deep MANTA: A Coarse-To-Fine Many-Task Network for Joint 2D and 3D Vehicle Analysis From Monocular Image",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "753",
        "author_site": "Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa, C\u00c3\u00a9line Teuli\u00c3\u00a8re, Thierry Chateau",
        "author": "Florian Chabot; Mohamed Chaouch; Jaonary Rabarisoa; Celine Teuliere; Thierry Chateau",
        "abstract": "In this paper, we present a novel approach, called Deep MANTA (Deep Many-Tasks), for many-task vehicle analysis from a given image. A robust convolutional network is introduced for simultaneous vehicle detection, part localization, visibility characterization and 3D dimension estimation. Its architecture is based on a new coarse-to-fine object proposal that boosts the vehicle detection. Moreover, the Deep MANTA network is able to localize vehicle parts even if these parts are not visible. In the inference, the network's outputs are used by a real time robust pose estimation algorithm for fine orientation estimation and 3D vehicle localization. We show in experiments that our method outperforms monocular state-of-the-art approaches on vehicle detection, orientation and 3D location tasks on the very challenging KITTI benchmark.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chabot_Deep_MANTA_A_CVPR_2017_paper.pdf",
        "aff": "CEA-LIST Vision and Content Engineering Laboratory; CEA-LIST Vision and Content Engineering Laboratory; CEA-LIST Vision and Content Engineering Laboratory; Pascal Institute, Blaise Pascal University; Pascal Institute, Blaise Pascal University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Chabot_Deep_MANTA_A_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.07570v1",
        "pdf_size": 2306274,
        "gs_citation": 564,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6800278553154417320&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cea.fr;cea.fr;cea.fr;univ-bpclermont.fr;univ-bpclermont.fr",
        "email": "cea.fr;cea.fr;cea.fr;univ-bpclermont.fr;univ-bpclermont.fr",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chabot_Deep_MANTA_A_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "CEA-LIST;Blaise Pascal University",
        "aff_unique_dep": "Vision and Content Engineering Laboratory;Pascal Institute",
        "aff_unique_url": "https://www-list.cea.fr;https://www.univ-bleu.fr",
        "aff_unique_abbr": "CEA-LIST;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Deep Matching Prior Network: Toward Tighter Multi-Oriented Text Detection",
        "session": "Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "728",
        "author_site": "Yuliang Liu, Lianwen Jin",
        "author": "Yuliang Liu; Lianwen Jin",
        "abstract": "Detecting incidental scene text is a challenging task because of multi-orientation, perspective distortion, and variation of text size, color and scale. Retrospective research has only focused on using rectangular bounding box or horizontal sliding window to localize text, which may result in redundant background noise, unnecessary overlap or even information loss. To address these issues, we propose a new Convolutional Neural Networks (CNNs) based method, named Deep Matching Prior Network (DMPNet), to detect text with tighter quadrangle. First, we use quadrilateral sliding windows in several specific intermediate convolutional layers to roughly recall the text with higher overlapping area and then a shared Monte-Carlo method is proposed for fast and accurate computing of the polygonal areas. After that, we designed a sequential protocol for relative regression which can exactly predict text with compact quadrangle. Moreover, a auxiliary smooth Ln loss is also proposed for further regressing the position of text, which has better overall performance than L2 loss and smooth L1 loss in terms of robustness and stability. The effectiveness of our approach is evaluated on a public word-level, multi-oriented scene text database, ICDAR 2015 Robust Reading Competition Challenge 4 \"Incidental scene text localization\". The performance of our method is evaluated by using F-measure and found to be 70.64%, outperforming the existing state-of-the-art method with F-measure 63.76%.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Deep_Matching_Prior_CVPR_2017_paper.pdf",
        "aff": "College of Electronic Information Engineering, South China University of Technology; College of Electronic Information Engineering, South China University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.01425v1",
        "pdf_size": 1577182,
        "gs_citation": 421,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17621265355024312105&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "scut.edu.cn;gmail.com",
        "email": "scut.edu.cn;gmail.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Deep_Matching_Prior_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "South China University of Technology",
        "aff_unique_dep": "College of Electronic Information Engineering",
        "aff_unique_url": "http://www.scut.edu.cn",
        "aff_unique_abbr": "SCUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Metric Learning via Facility Location",
        "session": "Machine Learning 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "2271",
        "author_site": "Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, Kevin Murphy",
        "author": "Hyun Oh Song; Stefanie Jegelka; Vivek Rathod; Kevin Murphy",
        "abstract": "Learning image similarity metrics in an end-to-end fashion with deep networks has demonstrated excellent results on tasks such as clustering and retrieval. However, current methods, all focus on a very local view of the data. In this paper, we propose a new metric learning scheme, based on structured prediction, that is aware of the global structure of the embedding space, and which is designed to optimize a clustering quality metric (NMI). We show state of the art performance on standard datasets, such as CUB200-2011, Cars196, and Stanford online products on NMI and R@K evaluation metrics.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Song_Deep_Metric_Learning_CVPR_2017_paper.pdf",
        "aff": "Google Research; MIT; Google Research; Google Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.01213",
        "pdf_size": 1162225,
        "gs_citation": 365,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3291009074279939649&as_sdt=5,24&sciodt=0,24&hl=en",
        "gs_version_total": 10,
        "aff_domain": "google.com;csail.mit.edu;google.com;google.com",
        "email": "google.com;csail.mit.edu;google.com;google.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Song_Deep_Metric_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Google;Massachusetts Institute of Technology",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://web.mit.edu",
        "aff_unique_abbr": "Google Research;MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Mixture of Linear Inverse Regressions Applied to Head-Pose Estimation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1998",
        "author_site": "St\u00c3\u00a9phane Lathuili\u00c3\u00a8re, R\u00c3\u00a9mi Juge, Pablo Mesejo, Rafael Mu\u00c3\u00b1oz-Salinas, Radu Horaud",
        "author": "Stephane Lathuiliere; Remi Juge; Pablo Mesejo; Rafael Munoz-Salinas; Radu Horaud",
        "abstract": "Convolutional Neural Networks (ConvNets) have become the state-of-the-art for many classification and regression problems in computer vision. When it comes to regression, approaches such as measuring the Euclidean distance of target and predictions are often employed as output layer. In this paper, we propose the coupling of a Gaussian mixture of linear inverse regressions with a ConvNet, and we describe the methodological foundations and the associated algorithm to jointly train the deep network and the regression function. We test our model on the head-pose estimation problem. In this particular problem, we show that inverse regression outperforms regression models currently used by state-of-the-art computer vision methods. Our method does not require the incorporation of additional data, as it is often proposed in the literature, thus it is able to work well on relatively small training datasets. Finally, it outperforms state-of-the-art methods in head-pose estimation using a widely used head-pose dataset. To the best of our knowledge, we are the first to incorporate inverse regression into deep learning for computer vision applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lathuiliere_Deep_Mixture_of_CVPR_2017_paper.pdf",
        "aff": "Inria Grenoble Rh \u02c6one-Alpes, France; Inria Grenoble Rh \u02c6one-Alpes, France; Inria Grenoble Rh \u02c6one-Alpes, France; Computing and Numerical Analysis Department, Cordoba University, Spain; Inria Grenoble Rh \u02c6one-Alpes, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 890156,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10156182474862555514&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "inria.fr; ; ; ; ",
        "email": "inria.fr; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lathuiliere_Deep_Mixture_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "INRIA;Cordoba University",
        "aff_unique_dep": ";Computing and Numerical Analysis Department",
        "aff_unique_url": "https://www.inria.fr;https://www.uco.es",
        "aff_unique_abbr": "Inria;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Grenoble;",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "France;Spain"
    },
    {
        "title": "Deep Multi-Scale Convolutional Neural Network for Dynamic Scene Deblurring",
        "session": "Low- & Mid-Level Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "1509",
        "author_site": "Seungjun Nah, Tae Hyun Kim, Kyoung Mu Lee",
        "author": "Seungjun Nah; Tae Hyun Kim; Kyoung Mu Lee",
        "abstract": "Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Nah_Deep_Multi-Scale_Convolutional_CVPR_2017_paper.pdf",
        "aff": "Department of ECE, ASRI, Seoul National University, 151-742, Seoul, Korea; Department of ECE, ASRI, Seoul National University, 151-742, Seoul, Korea; Department of ECE, ASRI, Seoul National University, 151-742, Seoul, Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Nah_Deep_Multi-Scale_Convolutional_2017_CVPR_supplemental.zip",
        "arxiv": "1612.02177",
        "pdf_size": 870062,
        "gs_citation": 2652,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5306020987846368461&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "gmail.com;gmail.com;snu.ac.kr",
        "email": "gmail.com;gmail.com;snu.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Nah_Deep_Multi-Scale_Convolutional_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Deep Multimodal Representation Learning From Temporal Data",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2305",
        "author_site": "Xitong Yang, Palghat Ramesh, Radha Chitta, Sriganesh Madhvanath, Edgar A. Bernal, Jiebo Luo",
        "author": "Xitong Yang; Palghat Ramesh; Radha Chitta; Sriganesh Madhvanath; Edgar A. Bernal; Jiebo Luo",
        "abstract": "In recent years, Deep Learning has been successfully applied to multimodal learning problems, with the aim of learning useful joint representations in data fusion applications. When the available modalities consist of time series data such as video, audio and sensor signals, it becomes imperative to consider their temporal structure during the fusion process. In this paper, we propose the Correlational Recurrent Neural Network (CorrRNN), a novel temporal fusion model for fusing multiple input modalities that are inherently temporal in nature. Key features of our proposed model include: (i) simultaneous learning of the joint representation and temporal dependencies between modalities, (ii) use of multiple loss terms in the objective function, including a maximum correlation loss term to enhance learning of cross-modal information, and (iii) the use of an attention model to dynamically adjust the contribution of different input modalities to the joint representation. We validate our model via experimentation on two different tasks: video- and sensor-based activity classification, and audio-visual speech recognition. We empirically analyze the contributions of different components of the proposed CorrRNN model, and demonstrate its robustness, effectiveness and state-of-the-art performance on multiple datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_Deep_Multimodal_Representation_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03152v1",
        "pdf_size": 1368944,
        "gs_citation": 138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3113248835047921771&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Deep_Multimodal_Representation_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Multitask Architecture for Integrated 2D and 3D Human Sensing",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2789",
        "author_site": "Alin-Ionut Popa, Mihai Zanfir, Cristian Sminchisescu",
        "author": "Alin-Ionut Popa; Mihai Zanfir; Cristian Sminchisescu",
        "abstract": "We propose a deep multitask architecture for fully automatic 2d and 3d human sensing (DMHS), including recognition and reconstruction, in monocular images. The system computes the figure-ground segmentation, semantically identifies the human body parts at pixel level, and estimates the 2d and 3d pose of the person. The model supports the joint training of all components by means of multi-task losses where early processing stages recursively feed into advanced ones for increasingly complex calculations, accuracy and robustness. The design allows us to tie a complete training protocol, by taking advantage of multiple datasets that would otherwise restrictively cover only some of the model components: complex 2d image data with no body part labeling and without associated 3d ground truth, or complex 3d data with limited 2d background variability. In detailed experiments based on several challenging 2d and 3d datasets (LSP, HumanEva, Human3.6M), we evaluate the sub-structures of the model, the effect of various types of training data in the multitask loss, and demonstrate that state-of-the-art results can be achieved at all processing levels. We also show that in the wild our monocular RGB architecture is perceptually competitive to a state-of-the art (commercial) Kinect system based on RGB-D data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Popa_Deep_Multitask_Architecture_CVPR_2017_paper.pdf",
        "aff": "Institute of Mathematics of the Romanian Academy; Institute of Mathematics of the Romanian Academy; Department of Mathematics, Faculty of Engineering, Lund University + Institute of Mathematics of the Romanian Academy",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.08985v1",
        "pdf_size": 1696079,
        "gs_citation": 183,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3050802195406218329&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "imar.ro;imar.ro;math.lth.se",
        "email": "imar.ro;imar.ro;math.lth.se",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Popa_Deep_Multitask_Architecture_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "Romanian Academy;Lund University",
        "aff_unique_dep": "Institute of Mathematics;Department of Mathematics",
        "aff_unique_url": "https://www.math.ro/;https://www.lunduniversity.lu.se",
        "aff_unique_abbr": "IMAR;LU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "Romania;Sweden"
    },
    {
        "title": "Deep Network Flow for Multi-Object Tracking",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "3208",
        "author_site": "Samuel Schulter, Paul Vernaza, Wongun Choi, Manmohan Chandraker",
        "author": "Samuel Schulter; Paul Vernaza; Wongun Choi; Manmohan Chandraker",
        "abstract": "Data association problems are an important component of many computer vision applications, with multi-object tracking being one of the most prominent examples. A typical approach to data association involves finding a graph matching or network flow that minimizes a sum of pairwise association costs, which are often either hand-crafted or learned as linear functions of fixed features. In this work, we demonstrate that it is possible to learn features for network-flow-based data association via backpropagation, by expressing the optimum of a smoothed network flow problem as a differentiable function of the pairwise association costs. We apply this approach to multi-object tracking with a network flow formulation. Our experiments demonstrate that we are able to successfully learn all cost functions for the association problem in an end-to-end fashion, which outperform hand-crafted costs in all settings. The integration and combination of various sources of inputs becomes easy and the cost functions can be learned entirely from data, alleviating tedious hand-designing of costs.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Schulter_Deep_Network_Flow_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Schulter_Deep_Network_Flow_2017_CVPR_supplemental.pdf",
        "arxiv": "1706.08482v1",
        "pdf_size": 780163,
        "gs_citation": 273,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7259696666433404843&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Schulter_Deep_Network_Flow_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Outdoor Illumination Estimation",
        "session": "Computational Photography",
        "status": "Oral",
        "track": "main",
        "pid": "3565",
        "author_site": "Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano Gambaretto, Jean-Fran\u00c3\u00a7ois Lalonde",
        "author": "Yannick Hold-Geoffroy; Kalyan Sunkavalli; Sunil Hadap; Emiliano Gambaretto; Jean-Francois Lalonde",
        "abstract": "We present a CNN-based technique to estimate high-dynamic range outdoor illumination from a single low dynamic range image. To train the CNN, we leverage a large dataset of outdoor panoramas. We fit a low-dimensional physically-based outdoor illumination model to the skies in these panoramas giving us a compact set of parameters (including sun position, atmospheric conditions, and camera parameters). We extract limited field-of-view images from the panoramas, and train a CNN with this large set of input image--output lighting parameter pairs. Given a test image, this network can be used to infer illumination parameters that can, in turn, be used to reconstruct an outdoor illumination environment map. We demonstrate that our approach allows the recovery of plausible illumination conditions and enables photorealistic virtual object insertion from a single image. An extensive evaluation on both the panorama dataset and captured HDR environment maps shows that our technique significantly outperforms previous solutions to this problem.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hold-Geoffroy_Deep_Outdoor_Illumination_CVPR_2017_paper.pdf",
        "aff": "Universit\u00e9 Laval*; Adobe Research\u2020; Adobe Research\u2020; Adobe Research\u2020; Universit\u00e9 Laval*",
        "project": "http://www.jflalonde.ca/projects/deepOutdoorLight",
        "github": "",
        "supp": "",
        "arxiv": "1611.06403",
        "pdf_size": 2114595,
        "gs_citation": 272,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6317462437448197007&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ulaval.ca;adobe.com;adobe.com;adobe.com;gel.ulaval.ca",
        "email": "ulaval.ca;adobe.com;adobe.com;adobe.com;gel.ulaval.ca",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hold-Geoffroy_Deep_Outdoor_Illumination_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Universit\u00e9 Laval;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.ulaval.ca;https://research.adobe.com",
        "aff_unique_abbr": "ULaval;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Deep Photo Style Transfer",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2058",
        "author_site": "Fujun Luan, Sylvain Paris, Eli Shechtman, Kavita Bala",
        "author": "Fujun Luan; Sylvain Paris; Eli Shechtman; Kavita Bala",
        "abstract": "This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon the recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom fully differentiable energy term. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Luan_Deep_Photo_Style_CVPR_2017_paper.pdf",
        "aff": "Cornell University; Adobe; Adobe; Cornell University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Luan_Deep_Photo_Style_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.07511v3",
        "pdf_size": 2208917,
        "gs_citation": 892,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3345032592903383977&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.cornell.edu;adobe.com;adobe.com;cs.cornell.edu",
        "email": "cs.cornell.edu;adobe.com;adobe.com;cs.cornell.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Luan_Deep_Photo_Style_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Cornell University;Adobe",
        "aff_unique_dep": ";Adobe Inc.",
        "aff_unique_url": "https://www.cornell.edu;https://www.adobe.com",
        "aff_unique_abbr": "Cornell;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Pyramidal Residual Networks",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2559",
        "author_site": "Dongyoon Han, Jiwhan Kim, Junmo Kim",
        "author": "Dongyoon Han; Jiwhan Kim; Junmo Kim",
        "abstract": "Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes. This also applies to residual networks and is very closely related to their performance. In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible. This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Han_Deep_Pyramidal_Residual_CVPR_2017_paper.pdf",
        "aff": "EE, KAIST; EE, KAIST; EE, KAIST",
        "project": "",
        "github": "https://github.com/jhkim89/PyramidNet",
        "supp": "",
        "arxiv": "1610.02915v4",
        "pdf_size": 3488673,
        "gs_citation": 916,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4834983461917588713&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Han_Deep_Pyramidal_Residual_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "KAIST",
        "aff_unique_dep": "Electrical Engineering",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Deep Quantization: Encoding Convolutional Activations With Deep Generative Model",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3107",
        "author_site": "Zhaofan Qiu, Ting Yao, Tao Mei",
        "author": "Zhaofan Qiu; Ting Yao; Tao Mei",
        "abstract": "Deep convolutional neural networks (CNNs) have proven highly effective for visual recognition, where learning a universal representation from activations of convolutional layer plays a fundamental problem. In this paper, we present Fisher Vector encoding with Variational Auto-Encoder (FV-VAE), a novel deep architecture that quantizes the local activations of convolutional layer in a deep generative model, by training them in an end-to-end manner. To incorporate FV encoding strategy into deep generative models, we introduce Variational Auto-Encoder model, which steers a variational inference and learning in a neural network which can be straightforwardly optimized using standard stochastic gradient method. Different from the FV characterized by conventional generative models (e.g., Gaussian Mixture Model) which parsimoniously fit a discrete mixture model to data distribution, the proposed FV-VAE is more flexible to represent the natural property of data for better generalization. Extensive experiments are conducted on three public datasets, i.e., UCF101, ActivityNet, and CUB-200-2011 in the context of video action recognition and fine-grained image classification, respectively. Superior results are reported when compared to state-of-the-art representations. Most remarkably, our proposed FV-VAE achieves to-date the best published accuracy of 94.2% on UCF101.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Qiu_Deep_Quantization_Encoding_CVPR_2017_paper.pdf",
        "aff": "University of Science and Technology of China, Hefei, China + Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.09502v1",
        "pdf_size": 688177,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17124210247561853698&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;microsoft.com;microsoft.com",
        "email": "gmail.com;microsoft.com;microsoft.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Qiu_Deep_Quantization_Encoding_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "University of Science and Technology of China;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.microsoft.com/en-us/research/group/microsoft-research-asia",
        "aff_unique_abbr": "USTC;MSR",
        "aff_campus_unique_index": "0+1;1;1",
        "aff_campus_unique": "Hefei;Beijing",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Reinforcement Learning-Based Image Captioning With Embedding Reward",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Oral",
        "track": "main",
        "pid": "108",
        "author_site": "Zhou Ren, Xiaoyu Wang, Ning Zhang, Xutao Lv, Li-Jia Li",
        "author": "Zhou Ren; Xiaoyu Wang; Ning Zhang; Xutao Lv; Li-Jia Li",
        "abstract": "Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a \"policy network\" and a \"value network\" to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ren_Deep_Reinforcement_Learning-Based_CVPR_2017_paper.pdf",
        "aff": "Snap Inc.+Google Inc.; Snap Inc.+Google Inc.; Snap Inc.+Google Inc.; Snap Inc.+Google Inc.; Google Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03899v1",
        "pdf_size": 1214430,
        "gs_citation": 426,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10964380961315872265&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "snap.com;snap.com;snap.com;snap.com;cs.stanford.edu",
        "email": "snap.com;snap.com;snap.com;snap.com;cs.stanford.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ren_Deep_Reinforcement_Learning-Based_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;0+1;1",
        "aff_unique_norm": "Snap Inc.;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.snapinc.com;https://www.google.com",
        "aff_unique_abbr": "Snap;Google",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Representation Learning for Human Motion Prediction and Classification",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2714",
        "author_site": "Judith B\u00c3\u00bctepage, Michael J. Black, Danica Kragic, Hedvig Kjellstr\u00c3\u00b6m",
        "author": "Judith Butepage; Michael J. Black; Danica Kragic; Hedvig Kjellstrom",
        "abstract": "Generative models of 3D human motion are often restricted to a small number of activities and can therefore not generalize well to novel movements or applications.    In this work we propose a deep learning framework for  human motion capture data that learns a generic representation from a large corpus of motion capture data and generalizes well to new, unseen, motions. Using an encoding-decoding network that learns to predict future 3D poses from the most recent past, we extract a feature representation of human motion. Most work on deep learning for sequence prediction focuses on video and speech. Since skeletal data has a different structure, we present and evaluate different network architectures that make different assumptions about time dependencies and limb correlations. To quantify the learned features, we use the output of different layers for action classification and visualize the receptive fields of the network units. Our method outperforms the recent state of the art in skeletal motion prediction even though these use action specific training data. Our results show that deep feedforward networks, trained from a generic mocap database, can successfully be used for feature extraction from human motion data and that this representation can be used as a foundation for classification and prediction.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Butepage_Deep_Representation_Learning_CVPR_2017_paper.pdf",
        "aff": "Department of Robotics, Perception, and Learning, CSC, KTH, Stockholm, Sweden; Perceiving Systems Department, Max Planck Institute for Intelligent Systems, T \u00a8ubingen, Germany; Department of Robotics, Perception, and Learning, CSC, KTH, Stockholm, Sweden; Department of Robotics, Perception, and Learning, CSC, KTH, Stockholm, Sweden",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Butepage_Deep_Representation_Learning_2017_CVPR_supplemental.pdf",
        "arxiv": "1702.07486",
        "pdf_size": 777222,
        "gs_citation": 519,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7856660868149106043&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff_domain": "kth.se;tuebingen.mpg.de;kth.se;kth.se",
        "email": "kth.se;tuebingen.mpg.de;kth.se;kth.se",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Butepage_Deep_Representation_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "KTH - Royal Institute of Technology;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Department of Robotics, Perception, and Learning;Perceiving Systems Department",
        "aff_unique_url": "https://www.kth.se;https://www.mpituebingen.mpg.de",
        "aff_unique_abbr": "KTH;MPI-IS",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Stockholm;T\u00fcbingen",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Sweden;Germany"
    },
    {
        "title": "Deep Roots: Improving CNN Efficiency With Hierarchical Filter Groups",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "449",
        "author_site": "Yani Ioannou, Duncan Robertson, Roberto Cipolla, Antonio Criminisi",
        "author": "Yani Ioannou; Duncan Robertson; Roberto Cipolla; Antonio Criminisi",
        "abstract": "We propose a new method for creating computationally efficient and compact convolutional neural networks (CNNs) using a novel sparse connection structure that resembles a tree root. This allows a significant reduction in computational cost and number of parameters compared to state-of-the-art deep CNNs, without compromising accuracy, by exploiting the sparsity of inter-layer filter dependencies. We validate our approach by using it to train more efficient variants of state-of-the-art CNN architectures, evaluated on the CIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than the baseline architectures with much less computation, as measured by CPU and GPU timings. For example, for ResNet 50, our model has 40% fewer parameters, 45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU). For the deeper ResNet 200 our model has 48% fewer parameters and 27% fewer floating point operations, while maintaining state-of-the-art accuracy. For GoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU (GPU).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ioannou_Deep_Roots_Improving_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ioannou_Deep_Roots_Improving_2017_CVPR_supplemental.pdf",
        "arxiv": "1605.06489v3",
        "pdf_size": 1138454,
        "gs_citation": 383,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8641087947045658611&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ioannou_Deep_Roots_Improving_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Self-Taught Learning for Weakly Supervised Object Localization",
        "session": "Machine Learning 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "508",
        "author_site": "Zequn Jie, Yunchao Wei, Xiaojie Jin, Jiashi Feng, Wei Liu",
        "author": "Zequn Jie; Yunchao Wei; Xiaojie Jin; Jiashi Feng; Wei Liu",
        "abstract": "Most existing weakly supervised localization (WSL) approaches learn detectors by finding positive bounding boxes based on features learned with image-level supervision. However, those features do not contain spatial location related information and usually provide poor-quality positive samples for training a detector. To overcome this issue, we propose a deep self-taught learning approach, which makes the detector learn the object-level features reliable for acquiring tight positive samples and afterwards re-train itself based on them. Consequently, the detector progressively improves its detection ability and localizes more informative positive samples. To implement such self-taught learning, we propose a seed sample acquisition method via image-to-object transferring and dense subgraph discovery to find reliable positive samples for initializing the detector. An online supportive sample harvesting scheme is further proposed to dynamically select the most confident tight positive samples and train the detector in a mutual boosting way. To prevent the detector from being trapped in poor optima due to overfitting, we propose a new relative improvement of predicted CNN scores for guiding the self-taught learning process. Extensive experiments on PASCAL 2007 and 2012 show that our approach outperforms the state-of-the-arts, strongly validating its effectiveness.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jie_Deep_Self-Taught_Learning_CVPR_2017_paper.pdf",
        "aff": "National University of Singapore; National University of Singapore; National University of Singapore; National University of Singapore; Tencent AI Lab",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.05188v2",
        "pdf_size": 974401,
        "gs_citation": 249,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15624113740351874609&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 9,
        "aff_domain": "nus.edu.sg;nus.edu.sg;u.nus.edu;nus.edu.sg;ee.columbia.edu",
        "email": "nus.edu.sg;nus.edu.sg;u.nus.edu;nus.edu.sg;ee.columbia.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jie_Deep_Self-Taught_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "National University of Singapore;Tencent",
        "aff_unique_dep": ";Tencent AI Lab",
        "aff_unique_url": "https://www.nus.edu.sg;https://ai.tencent.com",
        "aff_unique_abbr": "NUS;Tencent AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "Singapore;China"
    },
    {
        "title": "Deep Semantic Feature Matching",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3196",
        "author_site": "Nikolai Ufer, Bj\u00c3\u00b6rn Ommer",
        "author": "Nikolai Ufer; Bjorn Ommer",
        "abstract": "Estimating dense visual correspondences between objects with intra-class variation, deformations and background clutter remains a challenging problem. Thanks to the breakthrough of CNNs there are new powerful features available. Despite their easy accessibility and great success, existing semantic flow methods could not significantly benefit from these without extensive additional training. We introduce a novel method for semantic matching with pre-trained CNN features which is based on convolutional feature pyramids and activation guided feature selection. For the final matching we propose a sparse graph matching framework where each salient feature selects among a small subset of nearest neighbors in the target image. To improve our method in the unconstrained setting without bounding box annotations we introduce novel object proposal based matching constraints. Furthermore, we show that the sparse matching can be transformed into a dense correspondence field. Extensive experimental evaluations on benchmark datasets show that our method significantly outperforms existing semantic matching methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ufer_Deep_Semantic_Feature_CVPR_2017_paper.pdf",
        "aff": "Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Germany; Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3526386,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11897437612887273655&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iwr.uni-heidelberg.de;uni-heidelberg.de",
        "email": "iwr.uni-heidelberg.de;uni-heidelberg.de",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ufer_Deep_Semantic_Feature_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Heidelberg University",
        "aff_unique_dep": "Institute for Computer Science (IWR)",
        "aff_unique_url": "https://www.uni-heidelberg.de",
        "aff_unique_abbr": "Uni HD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Heidelberg",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Deep Sequential Context Networks for Action Prediction",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "550",
        "author_site": "Yu Kong, Zhiqiang Tao, Yun Fu",
        "author": "Yu Kong; Zhiqiang Tao; Yun Fu",
        "abstract": "This paper proposes efficient and powerful deep networks for action prediction from partially observed videos containing temporally incomplete action executions. Different from after-the-fact action recognition, action prediction task requires action labels to be predicted from these partially observed videos. Our approach exploits abundant sequential context information to enrich the feature representations of partial videos. We reconstruct missing information in the features extracted from partial videos by learning from fully observed action videos. The amount of the information is temporally ordered for the purpose of modeling temporal orderings of action segments. Label information is also used to better separate the learned features of different categories. We develop a new learning formulation that enables efficient model training. Extensive experimental results on UCF101, Sports-1M and BIT datasets demonstrate that our approach remarkably outperforms state-of-the-art methods, and is up to 300x faster than these methods. Results also show that actions differ in their prediction characteristics; some actions can be correctly predicted even though only the beginning 10% portion of videos is observed.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kong_Deep_Sequential_Context_CVPR_2017_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering; Department of Electrical and Computer Engineering + College of Computer and Information Science",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1147755,
        "gs_citation": 192,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11260034599791314264&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ece.neu.edu;ece.neu.edu;ece.neu.edu",
        "email": "ece.neu.edu;ece.neu.edu;ece.neu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kong_Deep_Sequential_Context_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Unknown Institution;Northeastern University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;College of Computer and Information Science",
        "aff_unique_url": ";https://ccis.northeastern.edu/",
        "aff_unique_abbr": ";CCIS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "title": "Deep Sketch Hashing: Fast Free-Hand Sketch-Based Image Retrieval",
        "session": "Computational Photography",
        "status": "Spotlight",
        "track": "main",
        "pid": "1043",
        "author_site": "Li Liu, Fumin Shen, Yuming Shen, Xianglong Liu, Ling Shao",
        "author": "Li Liu; Fumin Shen; Yuming Shen; Xianglong Liu; Ling Shao",
        "abstract": "Free-hand sketch-based image retrieval (SBIR) is a specific cross-view retrieval task, in which queries are abstract and ambiguous sketches while the retrieval database is formed with natural images. Work in this area mainly focuses on extracting representative and shared features for sketches and natural images. However, these can neither cope well with the geometric distortion between sketches and images nor be feasible for large-scale SBIR due to the heavy continuous-valued distance computation. In this paper, we speed up SBIR by introducing a novel binary coding method, named Deep Sketch Hashing (DSH), where a semi-heterogeneous deep architecture is proposed and incorporated into an end-to-end binary coding framework. Specifically, three convolutional neural networks are utilized to encode free-hand sketches, natural images and, especially, the auxiliary sketch-tokens which are adopted as bridges to mitigate the sketch-image geometric distortion. The learned DSH codes can effectively capture the cross-view similarities as well as the intrinsic semantic correlations between different categories. To the best of our knowledge, DSH is the first hashing work specifically designed for category-level SBIR with an end-to-end deep architecture. The proposed DSH is comprehensively evaluated on two large-scale datasets of TU-Berlin Extension and Sketchy, and the experiments consistently show DSH's superior SBIR accuracies over several state-of-the-art methods, while achieving significantly reduced retrieval time and memory footprint.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Deep_Sketch_Hashing_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.05605v1",
        "gs_citation": 319,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9586164504807571344&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Deep_Sketch_Hashing_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Structured Learning for Facial Action Unit Intensity Estimation",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1259",
        "author_site": "Robert Walecki, Ognjen (Oggi) Rudovic, Vladimir Pavlovic, Bj\u00c3\u00b6ern Schuller, Maja Pantic",
        "author": "Robert Walecki; Ognjen (Oggi) Rudovic; Vladimir Pavlovic; Bjoern Schuller; Maja Pantic",
        "abstract": "We consider the task of automated estimation of facial expression intensity. This involves estimation of multiple output variables (facial action units --- AUs) that are structurally dependent. Their structure arises from statistically induced co-occurrence patterns of AU intensity levels. Modeling this structure is critical for improving the estimation performance; however, this performance is bounded by the quality of the input features extracted from face images. The goal of this paper is to model these structures and estimate complex feature representations simultaneously by combining conditional random field (CRF) encoded AU dependencies with deep learning. To this end, we propose a novel Copula CNN deep learning approach for modeling multivariate ordinal variables. Our model accounts for ordinal structure in output variables and their non-linear dependencies via copula functions modeled as cliques of a CRF. These are jointly optimized with deep CNN feature encoding layers using a newly introduced balanced batch iterative training algorithm. We demonstrate the effectiveness of our approach on the task of AU intensity estimation on two benchmark datasets. We show that joint learning of the deep features and the target output structure results in significant performance gains compared to existing structured deep models and deep models for analysis of facial expressions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Walecki_Deep_Structured_Learning_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.04481",
        "pdf_size": 964369,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7226708848359983034&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Walecki_Deep_Structured_Learning_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Supervision With Shape Concepts for Occlusion-Aware 3D Object Parsing",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2317",
        "author_site": "Chi Li, M. Zeeshan Zia, Quoc-Huy Tran, Xiang Yu, Gregory D. Hager, Manmohan Chandraker",
        "author": "Chi Li; M. Zeeshan Zia; Quoc-Huy Tran; Xiang Yu; Gregory D. Hager; Manmohan Chandraker",
        "abstract": "Monocular 3D object parsing is highly desirable in various scenarios including occlusion reasoning and holistic scene interpretation. We present a deep convolutional neural network (CNN) architecture to localize semantic parts in 2D image and 3D space while inferring their visibility states, given a single RGB image. Our key insight is to exploit domain knowledge to regularize the network by deeply supervising its hidden layers, in order to sequentially infer intermediate concepts associated with the final task. To acquire training data in desired quantities with ground truth 3D shape and relevant concepts, we render 3D object CAD models to generate large-scale synthetic data and simulate challenging occlusion configurations between objects. We train the network only on synthetic data and demonstrate state-of-the-art performances on real image benchmarks including an extended version of KITTI, PASCAL VOC, PASCAL3D+ and IKEA for 2D and 3D keypoint localization and instance segmentation. The empirical results substantiate the utility of our deep supervision scheme by demonstrating effective transfer of knowledge from synthetic data to real images, resulting in less overfitting compared to standard end-to-end training.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Deep_Supervision_With_CVPR_2017_paper.pdf",
        "aff": "Johns Hopkins University; NEC Labs America; NEC Labs America; NEC Labs America; Johns Hopkins University; UC San Diego",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Deep_Supervision_With_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.02699",
        "pdf_size": 1044002,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18083124466669148372&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Deep_Supervision_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;0;2",
        "aff_unique_norm": "Johns Hopkins University;NEC Labs America;University of California, San Diego",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.jhu.edu;https://www.nec-labs.com;https://www.ucsd.edu",
        "aff_unique_abbr": "JHU;NEC LA;UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep TEN: Texture Encoding Network",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "221",
        "author_site": "Hang Zhang, Jia Xue, Kristin Dana",
        "author": "Hang Zhang; Jia Xue; Kristin Dana",
        "abstract": "We propose a Deep Texture Encoding Network (TEN) with a novel Encoding Layer integrated on top of convolutional layers, which ports the entire dictionary learning and encoding pipeline into a single model. Current methods build from distinct components, using standard encoders with separate off-the-shelf features such as such as SIFT descriptors or pre-trained CNN features for material recognition. Our new approach provides an end-to-end learning framework, where the inherent visual vocabularies are learned directly from the loss function. That is, the features, dictionaries and the encoding representation for the classifier are all learned simultaneously. The representation is orderless and therefore is particularly useful for material and texture recognition. This Encoding Layer generalizes robust residual encoders such as VLAD and Fisher Vectors, and has the property of discarding domain specific information which makes the learned convolutional features easier to transfer. Additionally, joint training using multiple datasets of varied sizes and class labels is supported resulting in increased recognition performance. The experimental results show superior performance as compared to state-of-the-art methods using gold-standard databases such as MINC-2500, Flicker Material Database, KTH-TIPS-2b, and a new ground terrain multiview database. The source code for the complete system will be publicly available upon publication.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Deep_TEN_Texture_CVPR_2017_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, USA; Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, USA; Department of Electrical and Computer Engineering, Rutgers University, New Brunswick, USA",
        "project": "http://ece.rutgers.edu/vision",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_Deep_TEN_Texture_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.02844v1",
        "pdf_size": 544885,
        "gs_citation": 331,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10447517577005372801&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "rutgers.edu;rutgers.edu;ece.rutgers.edu",
        "email": "rutgers.edu;rutgers.edu;ece.rutgers.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Deep_TEN_Texture_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Rutgers University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.rutgers.edu",
        "aff_unique_abbr": "Rutgers",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "New Brunswick",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Temporal Linear Encoding Networks",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "851",
        "author_site": "Ali Diba, Vivek Sharma, Luc Van Gool",
        "author": "Ali Diba; Vivek Sharma; Luc Van Gool",
        "abstract": "The CNN-encoding of features from entire videos for the representation of human actions has rarely been addressed. Instead, CNN work has focused on approaches to fuse spatial and temporal networks, but these were typically limited to processing shorter sequences. We present a new video representation, called temporal linear encoding (TLE) and embedded inside of CNNs as a new layer, which captures the appearance and motion throughout entire videos. It encodes this aggregated information into a robust video feature representation, via end-to-end learning. Advantages of TLEs are: (a) they encode the entire video into a compact feature representation, learning the semantics and a discriminative feature space; (b) they are applicable to all kinds of networks like 2D and 3D CNNs for video classification; and (c) they model feature interactions in a more expressive way and without loss of information. We conduct experiments on two challenging human action datasets: HMDB51 and UCF101. The experiments show that TLE outperforms current state-of-the-art methods on both datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Diba_Deep_Temporal_Linear_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.06678v1",
        "gs_citation": 317,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15693955794916408013&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Diba_Deep_Temporal_Linear_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Unsupervised Similarity Learning Using Partially Ordered Sets",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3350",
        "author_site": "Miguel A. Bautista, Artsiom Sanakoyeu, Bj\u00c3\u00b6rn Ommer",
        "author": "Miguel A. Bautista; Artsiom Sanakoyeu; Bjorn Ommer",
        "abstract": "Unsupervised learning of visual similarities is of paramount importance to computer vision, particularly due to lacking training data for fine-grained similarities. Deep learning of similarities is often based on relationships between pairs or triplets of samples. Many of these relations are unreliable and mutually contradicting, implying inconsistencies when trained without supervision information that relates different tuples or triplets to each other. To overcome this problem, we use local estimates of reliable (dis-)similarities to initially group samples into compact surrogate classes and use local partial orders of samples to classes to link classes to each other. Similarity learning is then formulated as a partial ordering task with soft correspondences of all samples to classes. Adopting a strategy of self-supervision, a CNN is trained to optimally represent samples in a mutually consistent manner while updating the classes. The similarity learning and grouping procedure are integrated in a single model and optimized jointly. The proposed unsupervised approach shows competitive performance on detailed pose estimation and object classification.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bautista_Deep_Unsupervised_Similarity_CVPR_2017_paper.pdf",
        "aff": "Heidelberg Collaboratory for Image Processing; Heidelberg Collaboratory for Image Processing; Heidelberg Collaboratory for Image Processing",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Bautista_Deep_Unsupervised_Similarity_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.02268",
        "pdf_size": 927557,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11566995570398904282&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de;iwr.uni-heidelberg.de",
        "email": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de;iwr.uni-heidelberg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bautista_Deep_Unsupervised_Similarity_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Heidelberg University",
        "aff_unique_dep": "Heidelberg Collaboratory for Image Processing",
        "aff_unique_url": "https://www.kip.uni-heidelberg.de",
        "aff_unique_abbr": "KIP",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Heidelberg",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Deep Variation-Structured Reinforcement Learning for Visual Relationship and Attribute Detection",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "279",
        "author_site": "Xiaodan Liang, Lisa Lee, Eric P. Xing",
        "author": "Xiaodan Liang; Lisa Lee; Eric P. Xing",
        "abstract": "Despite progress in visual perception tasks such as image classification and detection, computers still struggle to understand the interdependency of objects in the scene as a whole, e.g., relations between objects or their attributes. Existing methods often ignore global context cues capturing the interactions among different object instances, and can only recognize a handful of types by exhaustively training individual detectors for all possible relationships. To capture such global interdependency, we propose a deep Variation-structured Reinforcement Learning (VRL) framework to sequentially discover object relationships and attributes in the whole image. First, a directed semantic action graph is built using language priors to provide a rich and compact representation of semantic correlations between object categories, predicates, and attributes. Next, we use a variation-structured traversal over the action graph to construct a small, adaptive action set for each step based on the current state and historical actions. In particular, an ambiguity-aware object mining scheme is used to resolve semantic ambiguity among object categories that the object detector fails to distinguish. We then make sequential predictions using a deep RL framework, incorporating global context cues and semantic embeddings of previously extracted phrases in the state vector. Our experiments on the Visual Relationship Detection (VRD) dataset and the large-scale Visual Genome dataset validate the superiority of VRL, which can achieve significantly better detection results on datasets involving thousands of relationship and attribute types. We also demonstrate that VRL is able to predict unseen types embedded in our action graph by learning correlations on shared graph nodes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liang_Deep_Variation-Structured_Reinforcement_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.03054",
        "gs_citation": 315,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13161672413784485518&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liang_Deep_Variation-Structured_Reinforcement_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Video Deblurring for Hand-Held Cameras",
        "session": "Low- & Mid-Level Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "460",
        "author_site": "Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo Sapiro, Wolfgang Heidrich, Oliver Wang",
        "author": "Shuochen Su; Mauricio Delbracio; Jue Wang; Guillermo Sapiro; Wolfgang Heidrich; Oliver Wang",
        "abstract": "Motion blur from camera shake is a major problem in videos captured by hand-held devices. Unlike single-image deblurring, video-based approaches can take advantage of the abundant information that exists across neighboring frames. As a result the best performing methods rely on the alignment of nearby frames. However, aligning images is a computationally expensive and fragile procedure, and methods that aggregate information must therefore be able to identify which regions have been accurately aligned and which have not, a task that requires high level scene understanding. In this work, we introduce a deep learning solution to video deblurring, where a CNN is trained end-to-end to learn how to accumulate information across frames. To train this network, we collected a dataset of real videos recorded with a high frame rate camera, which we use to generate synthetic motion blur for supervision. We show that the features learned from this dataset extend to deblurring motion blur that arises due to camera shake in a wide range of videos, and compare the quality of results to a number of other baselines.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Su_Deep_Video_Deblurring_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Su_Deep_Video_Deblurring_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 8309260,
        "gs_citation": 711,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4032392505584693684&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Su_Deep_Video_Deblurring_CVPR_2017_paper.html"
    },
    {
        "title": "Deep View Morphing",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "784",
        "author_site": "Dinghuang Ji, Junghyun Kwon, Max McFarland, Silvio Savarese",
        "author": "Dinghuang Ji; Junghyun Kwon; Max McFarland; Silvio Savarese",
        "abstract": "Recently, convolutional neural networks (CNN) have been successfully applied to view synthesis problems. However, such CNN-based methods can suffer from lack of texture details, shape distortions, or high computational complexity. In this paper, we propose a novel CNN architecture for view synthesis called \"Deep View Morphing\" that does not suffer from these issues. To synthesize a middle view of two input images, a rectification network first rectifies the two input images. An encoder-decoder network then generates dense correspondences between the rectified images and blending masks to predict the visibility of pixels of the rectified images in the middle view. A view morphing network finally synthesizes the middle view using the dense correspondences and blending masks. We experimentally show the proposed method significantly outperforms the state-of-the-art CNN-based view synthesis method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ji_Deep_View_Morphing_CVPR_2017_paper.pdf",
        "aff": "UNC at Chapel Hill + Ricoh Innovations; Ricoh Innovations; Ricoh Innovations; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.02168v1",
        "pdf_size": 1273451,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12338983798837520670&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.unc.edu;gmail.com;ric.ricoh.com;stanford.edu",
        "email": "cs.unc.edu;gmail.com;ric.ricoh.com;stanford.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ji_Deep_View_Morphing_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;1;2",
        "aff_unique_norm": "University of North Carolina at Chapel Hill;Ricoh Innovations;Stanford University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unc.edu;https://www.ricoh-innovations.com;https://www.stanford.edu",
        "aff_unique_abbr": "UNC;;Stanford",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Chapel Hill;;Stanford",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Visual-Semantic Quantization for Efficient Image Retrieval",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "483",
        "author_site": "Yue Cao, Mingsheng Long, Jianmin Wang, Shichen Liu",
        "author": "Yue Cao; Mingsheng Long; Jianmin Wang; Shichen Liu",
        "abstract": "Compact coding has been widely applied to approximate nearest neighbor search for large-scale image retrieval, due to its computation efficiency and retrieval quality. This paper presents a compact coding solution with a focus on the deep learning to quantization approach, which improves retrieval quality by end-to-end representation learning and compact encoding and has already shown the superior performance over the hashing solutions for similarity retrieval. We propose Deep Visual-Semantic Quantization (DVSQ), which is the first approach to learning deep quantization models from labeled image data as well as the semantic information underlying general text domains. The main contribution lies in jointly learning deep visual-semantic embeddings and visual-semantic quantizers using carefully-designed hybrid networks and well-specified loss functions. DVSQ enables efficient and effective image retrieval by supporting maximum inner-product search, which is computed based on learned codebooks with fast distance table lookup. Comprehensive empirical evidence shows that DVSQ can generate compact binary codes and yield state-of-the-art similarity retrieval performance on standard benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cao_Deep_Visual-Semantic_Quantization_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6443828,
        "gs_citation": 372,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11433754202637009780&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cao_Deep_Visual-Semantic_Quantization_CVPR_2017_paper.html"
    },
    {
        "title": "Deep Watershed Transform for Instance Segmentation",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2179",
        "author_site": "Min Bai, Raquel Urtasun",
        "author": "Min Bai; Raquel Urtasun",
        "abstract": "Most contemporary approaches to instance segmentation use complex pipelines involving conditional random fields, recurrent neural networks, object proposals, or template matching schemes. In this paper, we present a simple yet powerful end-to-end convolutional neural network to tackle this task. Our approach combines intuitions from the classical watershed transform and modern deep learning to produce an energy map of the image where object instances are unambiguously represented as energy basins. We then perform a cut at a single energy level to directly yield connected components corresponding to object instances. Our model achieves more than double the performance over the state-of-the-art on the challenging Cityscapes Instance Level Segmentation task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bai_Deep_Watershed_Transform_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.08303v2",
        "pdf_size": 2024565,
        "gs_citation": 730,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1196246158692376911&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bai_Deep_Watershed_Transform_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "DeepNav: Learning to Navigate Large Cities",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2150",
        "author_site": "Samarth Brahmbhatt, James Hays",
        "author": "Samarth Brahmbhatt; James Hays",
        "abstract": "We present DeepNav, a Convolutional Neural Network (CNN) based algorithm for navigating large cities using locally visible street-view images. The DeepNav agent learns to reach its destination quickly by making the correct navigation decisions at intersections. We collect a large-scale dataset of street-view images organized in a graph where nodes are connected by roads. This dataset contains 10 city graphs and a total of more than 1 million street-view images. We propose 3 supervised learning approaches for the navigation task, and show how A* search in the city graph can be used to generate labels for the images. Our annotation process is fully automated using publicly available mapping services, and requires no human input. We evaluate the proposed DeepNav models on 4 held-out cities for navigating to 5 different types of destinations and show that our algorithms outperform previous work that uses hand-crafted features and Support Vector Regression (SVR).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Brahmbhatt_DeepNav_Learning_to_CVPR_2017_paper.pdf",
        "aff": "Georgia Institute of Technology; Georgia Institute of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Brahmbhatt_DeepNav_Learning_to_2017_CVPR_supplemental.pdf",
        "arxiv": "1701.09135v2",
        "pdf_size": 3047467,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14263881362952761856&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Brahmbhatt_DeepNav_Learning_to_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "DeepPermNet: Visual Permutation Learning",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1565",
        "author_site": "Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, Stephen Gould",
        "author": "Rodrigo Santa Cruz; Basura Fernando; Anoop Cherian; Stephen Gould",
        "abstract": "We present a principled approach to uncover the structure of visual data by solving a novel deep learning task coined visual permutation learning. The goal of this task is to find the permutation that recovers the structure of data from shuffled versions of it. In the case of natural images, this task boils down to recovering the original image from patches shuffled by an unknown permutation matrix. Unfortunately, permutation matrices are discrete, thereby posing difficulties for gradient-based methods. To this end, we resort to a continuous approximation of these matrices using doubly-stochastic matrices which we generate from standard CNN predictions using Sinkhorn iterations. Unrolling these iterations in a Sinkhorn network layer, we propose DeepPermNet, an end-to-end CNN model for this task. The utility of DeepPermNet is demonstrated on two challenging computer vision problems, namely, (i) relative attributes learning and (ii) self-supervised representation learning. Our results show state-of-the-art performance on the Public Figures and OSR benchmarks for (i) and on the classification and segmentation tasks on the PASCAL VOC dataset for (ii).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Santa_Cruz_DeepPermNet_Visual_Permutation_CVPR_2017_paper.pdf",
        "aff": "Australian Centre for Robotic Vision, Australian National University, Canberra, Australia; Australian Centre for Robotic Vision, Australian National University, Canberra, Australia; Australian Centre for Robotic Vision, Australian National University, Canberra, Australia; Australian Centre for Robotic Vision, Australian National University, Canberra, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.02729",
        "pdf_size": 852033,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5853250317288068196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au;anu.edu.au",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Santa_Cruz_DeepPermNet_Visual_Permutation_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "Australian Centre for Robotic Vision",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Canberra",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Deeply Aggregated Alternating Minimization for Image Restoration",
        "session": "Low- & Mid-Level Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "2866",
        "author_site": "Youngjung Kim, Hyungjoo Jung, Dongbo Min, Kwanghoon Sohn",
        "author": "Youngjung Kim; Hyungjoo Jung; Dongbo Min; Kwanghoon Sohn",
        "abstract": "Regularization-based image restoration has remained an active research topic in image processing and computer vision. It often leverages a guidance signal captured in different fields as an additional cue. In this work, we present a general framework for image restoration, called deeply aggregated alternating minimization (DeepAM). We propose to train deep neural network to advance two of the steps in the conventional AM algorithm: proximal mapping and b-continuation. Both steps are learned from a large dataset in an end-to-end manner. The proposed framework enables the convolutional neural networks (CNNs) to operate as a regularizer in the AM algorithm. We show that our learned regularizer via deep aggregation outperforms the recent data-driven approaches as well as the nonlocal-based methods. The flexibility and effectiveness of our framework are demonstrated in several restoration tasks, including single image denoising, RGB-NIR restoration, and depth super-resolution.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kim_Deeply_Aggregated_Alternating_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.06508v1",
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=528739807388540874&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kim_Deeply_Aggregated_Alternating_CVPR_2017_paper.html"
    },
    {
        "title": "Deeply Supervised Salient Object Detection With Short Connections",
        "session": "Theory",
        "status": "Poster",
        "track": "main",
        "pid": "1183",
        "author_site": "Qibin Hou, Ming-Ming Cheng, Xiaowei Hu, Ali Borji, Zhuowen Tu, Philip H. S. Torr",
        "author": "Qibin Hou; Ming-Ming Cheng; Xiaowei Hu; Ali Borji; Zhuowen Tu; Philip H. S. Torr",
        "abstract": "Recent progress on saliency detection is substantial, benefiting mostly from the explosive development of Convolutional Neural Networks (CNNs). Semantic segmentation and saliency detection algorithms developed lately have been mostly based on Fully Convolutional Neural Networks (FCNs). There is still a large room for improvement over the generic FCN models that do not explicitly deal with the scale-space problem. Holisitcally-Nested Edge Detector (HED) provides a skip-layer structure with deep supervision for edge and boundary detection, but the performance gain of HED on saliency detection is not obvious. In this paper, we propose a new saliency method by introducing short connections to the skip-layer structures within the HED architecture. Our framework provides rich multi-scale feature maps at each layer, a property that is critically needed to perform segment detection. Our method produces state-of-the-art results on 5 widely tested salient object detection benchmarks, with advantages in terms of efficiency (0.08 seconds per image), effectiveness, and simplicity over the existing algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hou_Deeply_Supervised_Salient_CVPR_2017_paper.pdf",
        "aff": "CCCE, Nankai University; CCCE, Nankai University; CCCE, Nankai University; CRCV, UCF; UCSD; University of Oxford",
        "project": "http://mmcheng.net/dss/",
        "github": "",
        "supp": "",
        "arxiv": "1611.04849v4",
        "pdf_size": 1666105,
        "gs_citation": 1892,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16939402952199717847&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff_domain": "nankai.edu.cn;nankai.edu.cn;nankai.edu.cn;crcv.ucf.edu;ucsd.edu;eng.ox.ac.uk",
        "email": "nankai.edu.cn;nankai.edu.cn;nankai.edu.cn;crcv.ucf.edu;ucsd.edu;eng.ox.ac.uk",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hou_Deeply_Supervised_Salient_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;2;3",
        "aff_unique_norm": "Nankai University;University of Central Florida;University of California, San Diego;University of Oxford",
        "aff_unique_dep": "CCCE;Center for Research in Computer Vision;;",
        "aff_unique_url": "http://www.nankai.edu.cn;https://www.crcv.ucf.edu;https://ucsd.edu;https://www.ox.ac.uk",
        "aff_unique_abbr": ";UCF;UCSD;Oxford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";La Jolla",
        "aff_country_unique_index": "0;0;0;1;1;2",
        "aff_country_unique": "China;United States;United Kingdom"
    },
    {
        "title": "Dense Captioning With Joint Inference and Visual Context",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "792",
        "author_site": "Linjie Yang, Kevin Tang, Jianchao Yang, Li-Jia Li",
        "author": "Linjie Yang; Kevin Tang; Jianchao Yang; Li-Jia Li",
        "abstract": "Dense captioning is a newly emerging computer vision topic for understanding images with dense language descriptions. The goal is to densely detect visual concepts (e.g., objects, object parts, and interactions between them) from images, labeling each with a short descriptive phrase. We identify two key challenges of dense captioning that need to be properly addressed when tackling the problem. First, dense visual concept annotations in each image are associated with highly overlapping target regions, making accurate localization of each visual concept challenging. Second, the large amount of visual concepts makes it hard to recognize each of them by appearance alone. We propose a new model pipeline based on two novel ideas, joint inference and context fusion, to alleviate these two challenges. We design our model architecture in a methodical manner and thoroughly evaluate the variations in architecture. Our final model,  compact and efficient, achieves state-of-the-art accuracy on Visual Genome for dense captioning with a relative gain of 73% compared to the previous best algorithm. Qualitative experiments also reveal the semantic capabilities of our model in dense captioning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_Dense_Captioning_With_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.06949v2",
        "pdf_size": 2219013,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12507980608734645942&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Dense_Captioning_With_CVPR_2017_paper.html"
    },
    {
        "title": "DenseReg: Fully Convolutional Dense Shape Regression In-The-Wild",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "3115",
        "author_site": "R\u00c4\u00b1za Alp G\u00c3\u00bcler, George Trigeorgis, Epameinondas Antonakos, Patrick Snape, Stefanos Zafeiriou, Iasonas Kokkinos",
        "author": "Riza Alp Guler; George Trigeorgis; Epameinondas Antonakos; Patrick Snape; Stefanos Zafeiriou; Iasonas Kokkinos",
        "abstract": "In this paper we propose to learn a mapping from image pixels into a dense template  grid through a fully convolutional network.  We formulate this task as a regression problem and train our network by leveraging upon manually annotated facial landmarks 'in-the-wild'. We use such landmarks to establish a dense correspondence field between a three-dimensional object template and the input image, which then serves as the ground-truth for training our regression system. We show that we can combine ideas from semantic segmentation with regression networks, yielding a highly-accurate `quantized regression' architecture.  Our system, called DenseReg, allows us to estimate dense image-to-template correspondences in a fully convolutional manner. As such our network can provide useful correspondence information  as a stand-alone system, while when used as an initialization for Statistical Deformable Models we obtain landmark localization results that largely outperform the current state-of-the-art on the challenging 300W benchmark.  We thoroughly evaluate our method on a host of facial analysis tasks, and demonstrate its use for other correspondence estimation tasks, such as the human body and the human ear. DenseReg code is made available at http://alpguler.com/DenseReg.html  along with supplementary materials.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Guler_DenseReg_Fully_Convolutional_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "http://alpguler.com/DenseReg.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4507845,
        "gs_citation": 239,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=849781052322304511&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Guler_DenseReg_Fully_Convolutional_CVPR_2017_paper.html"
    },
    {
        "title": "Densely Connected Convolutional Networks",
        "session": "Machine Learning 2",
        "status": "Oral",
        "track": "main",
        "pid": "1954",
        "author_site": "Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger",
        "author": "Gao Huang; Zhuang Liu; Laurens van der Maaten; Kilian Q. Weinberger",
        "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections--one between each layer and its subsequent layer--our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR- 10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf",
        "aff": "Cornell University; Tsinghua University; Facebook AI Research; Cornell University",
        "project": "",
        "github": "https://github.com/liuzhuang13/DenseNet",
        "supp": "",
        "arxiv": "1608.06993v5",
        "pdf_size": 844083,
        "gs_citation": 54232,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4205512852566836101&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 34,
        "aff_domain": "cornell.edu;mails.tsinghua.edu.cn;fb.com;cornell.edu",
        "email": "cornell.edu;mails.tsinghua.edu.cn;fb.com;cornell.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Cornell University;Tsinghua University;Meta",
        "aff_unique_dep": ";;Facebook AI Research",
        "aff_unique_url": "https://www.cornell.edu;https://www.tsinghua.edu.cn;https://research.facebook.com",
        "aff_unique_abbr": "Cornell;THU;FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Depth From Defocus in the Wild",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "999",
        "author_site": "Huixuan Tang, Scott Cohen, Brian Price, Stephen Schiller, Kiriakos N. Kutulakos",
        "author": "Huixuan Tang; Scott Cohen; Brian Price; Stephen Schiller; Kiriakos N. Kutulakos",
        "abstract": "We consider the problem of two-frame depth from defocus  in conditions unsuitable for existing methods yet typical of everyday photography: a handheld cellphone camera, a small aperture, a non-stationary scene and sparse surface texture. Our approach combines a global analysis of image content---3D surfaces, deformations, figure-ground relations, textures---with local estimation of joint depth-flow likelihoods in tiny patches. To enable local estimation we (1) derive novel defocus-equalization filters that induce brightness constancy across frames and (2) impose a tight upper bound on defocus blur---just three pixels in radius---through an appropriate choice of the second frame. For global analysis we use a novel piecewise-spline scene representation that can propagate depth and flow across large irregularly-shaped regions. Our experiments show that this combination preserves sharp boundaries and yields good depth and flow maps in the face of significant noise, uncertainty, non-rigidity, and data sparsity.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tang_Depth_From_Defocus_CVPR_2017_paper.pdf",
        "aff": "University of Toronto; Adobe Research; Adobe Research; Adobe Research; University of Toronto + Adobe Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3187008,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16157197127280887938&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tang_Depth_From_Defocus_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;0+1",
        "aff_unique_norm": "University of Toronto;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.utoronto.ca;https://research.adobe.com",
        "aff_unique_abbr": "U of T;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;0+1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "DeshadowNet: A Multi-Context Embedding Deep Network for Shadow Removal",
        "session": "Computational Photography",
        "status": "Spotlight",
        "track": "main",
        "pid": "1637",
        "author_site": "Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, Rynson W. H. Lau",
        "author": "Liangqiong Qu; Jiandong Tian; Shengfeng He; Yandong Tang; Rynson W. H. Lau",
        "abstract": "Shadow removal is a challenging task as it requires the detection/annotation of shadows as well as semantic understanding of the scene. In this paper, we propose an automatic and end-to-end deep neural network (DeshadowNet) to tackle these problems in a unified manner. DeshadowNet is designed with a multi-context architecture, where the output shadow matte is predicted by embedding information from three different perspectives. The first global network extracts shadow features from a global view. Two levels of features are derived from the global network and transferred to two parallel networks. While one extracts the appearance of the input image, the other one involves semantic understanding for final prediction. These two complementary networks generate multi-context features to obtain the shadow matte with fine local details. To evaluate the performance of the proposed method, we construct the first large scale benchmark with 3088 image pairs. Extensive experiments on two publicly available benchmarks and our large-scale benchmark show that the proposed method performs favorably against several state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Qu_DeshadowNet_A_Multi-Context_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6501092,
        "gs_citation": 367,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9956408789100146609&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Qu_DeshadowNet_A_Multi-Context_CVPR_2017_paper.html"
    },
    {
        "title": "Designing Effective Inter-Pixel Information Flow for Natural Image Matting",
        "session": "Low- & Mid-Level Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "23",
        "author_site": "Ya\u00c4\u009fiz Aksoy, Tun\u00c3\u00a7 Ozan Aydin, Marc Pollefeys",
        "author": "Yagiz Aksoy; Tunc Ozan Aydin; Marc Pollefeys",
        "abstract": "We present a novel, purely affinity-based natural image matting algorithm. Our method relies on carefully defined pixel-to-pixel connections that enable effective use of information available in the image and the trimap. We control the information flow from the known-opacity regions into the unknown region, as well as within the unknown region itself, by utilizing multiple definitions of pixel affinities. This way we achieve significant improvements on matte quality near challenging regions of the foreground object. Among other forms of information flow, we introduce color-mixture flow, which builds upon local linear embedding and effectively encapsulates the relation between different pixel opacities. Our resulting novel linear system formulation can be solved in closed-form and is robust against several fundamental challenges in natural matting such as holes and remote intricate structures. While our method is primarily designed as a standalone natural matting tool, we show that it can also be used for regularizing mattes obtained by various sampling-based methods. Our evaluation using the public alpha matting benchmark suggests a significant performance improvement over the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Aksoy_Designing_Effective_Inter-Pixel_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1707.05055",
        "pdf_size": 1735802,
        "gs_citation": 220,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9163759624365086722&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Aksoy_Designing_Effective_Inter-Pixel_CVPR_2017_paper.html"
    },
    {
        "title": "Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware Pruning",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2417",
        "author_site": "Tien-Ju Yang, Yu-Hsin Chen, Vivienne Sze",
        "author": "Tien-Ju Yang; Yu-Hsin Chen; Vivienne Sze",
        "abstract": "Deep convolutional neural networks (CNNs) are indispensable to state-of-the-art computer vision algorithms. However, they are still rarely deployed on battery-powered mobile devices, such as smartphones and wearable gadgets, where vision algorithms can enable many revolutionary real-world applications. The key limiting factor is the high energy consumption of CNN processing due to its high computational complexity. While there are many previous efforts that try to reduce the CNN model size or the amount of computation, we find that they do not necessarily result in lower energy consumption. Therefore, these targets do not serve as a good metric for energy cost estimation.  To close the gap between CNN design and energy consumption optimization, we propose an energy-aware pruning algorithm for CNNs that directly uses the energy consumption of a CNN to guide the pruning process. The energy estimation methodology uses parameters extrapolated from actual hardware measurements. The proposed layer-by-layer pruning algorithm also prunes more aggressively than previously proposed pruning methods by minimizing the error in the output feature maps instead of the filter weights. For each layer, the weights are first pruned and then locally fine-tuned with a closed-form least-square solution to quickly restore the accuracy. After all layers are pruned, the entire network is globally fine-tuned using back-propagation. With the proposed pruning method, the energy consumption of AlexNet and GoogLeNet is reduced by 3.7x and 1.6x, respectively, with less than 1% top-5 accuracy loss. We also show that reducing the number of target classes in AlexNet greatly decreases the number of weights, but has a limited impact on energy consumption.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_Designing_Energy-Efficient_Convolutional_CVPR_2017_paper.pdf",
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05128v4",
        "pdf_size": 567976,
        "gs_citation": 1106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12390538587338513101&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Designing_Energy-Efficient_Convolutional_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Designing Illuminant Spectral Power Distributions for Surface Classification",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "787",
        "author_site": "Henryk Blasinski, Joyce Farrell, Brian Wandell",
        "author": "Henryk Blasinski; Joyce Farrell; Brian Wandell",
        "abstract": "There are many scientific, medical and industrial imaging applications where users have full control of the scene illumination and color reproduction is not the primary objective For example, it is possible to co-design sensors and spectral illumination in order to classify and detect changes in biological tissues, organic and inorganic materials, and object surface properties.  In this paper, we propose two different approaches to illuminant spectrum selection for surface classification. In the supervised framework we formulate a biconvex optimization problem where we alternate between optimizing support vector classifier weights and optimal illuminants. We also describe a sparse Principal Component Analysis (PCA) dimensionality reduction approach that can be used with unlabeled data. We efficiently solve the non-convex PCA problem using a convex relaxation and Alternating Direction Method of Multipliers (ADMM). We compare the classification accuracy of a monochrome imaging sensor with optimized illuminants to the classification accuracy of conventional RGB cameras with natural broadband illumination.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Blasinski_Designing_Illuminant_Spectral_CVPR_2017_paper.pdf",
        "aff": "Department of Electrical Engineering, Stanford University; Department of Electrical Engineering, Stanford University; Department of Electrical Engineering, Stanford University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Blasinski_Designing_Illuminant_Spectral_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2167185,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10196039351225372299&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Blasinski_Designing_Illuminant_Spectral_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Detailed, Accurate, Human Shape Estimation From Clothed 3D Scan Sequences",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "1708",
        "author_site": "Chao Zhang, Sergi Pujades, Michael J. Black, Gerard Pons-Moll",
        "author": "Chao Zhang; Sergi Pujades; Michael J. Black; Gerard Pons-Moll",
        "abstract": "We address the problem of estimating human pose and body shape from 3D scans over time. Reliable estimation of 3D body shape is necessary for many applications including virtual try-on, health monitoring, and avatar creation for virtual reality. Scanning bodies in minimal clothing, however, presents a practical barrier to these applications. We address this problem by estimating body shape under clothing from a sequence of 3D scans. Previous methods that have exploited body models produce smooth shapes lacking personalized details. We contribute a new approach to recover a personalized shape of the person. The estimated shape deviates from a parametric model to fit the 3D scans. We demonstrate the method using high quality 4D data as well as sequences of visual hulls extracted from multi-view images.  We also make available BUFF, a new 4D dataset that enables quantitative evaluation (http://buff.is.tue.mpg.de). Our method outperforms the state of the art  in both pose estimation and shape estimation, qualitatively and quantitatively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Detailed_Accurate_Human_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_Detailed_Accurate_Human_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.04454",
        "pdf_size": 1495126,
        "gs_citation": 344,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15213240969419514022&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Detailed_Accurate_Human_CVPR_2017_paper.html"
    },
    {
        "title": "Detangling People: Individuating Multiple Close People and Their Body Parts via Region Assembly",
        "session": "Analyzing Humans 2",
        "status": "Oral",
        "track": "main",
        "pid": "2626",
        "author_site": "Hao Jiang, Kristen Grauman",
        "author": "Hao Jiang; Kristen Grauman",
        "abstract": "Today's person detection methods work best when people are in common upright poses and appear reasonably well spaced out in the image. However, in many real images, that's not what people do. People often appear quite close to each other, e.g., with limbs linked or heads touching, and their poses are often not pedestrian-like. We propose an approach to detangle people in multi-person images. We formulate the task as a region assembly problem. Starting from a large set of overlapping regions from body part semantic segmentation and generic object proposals, our optimization approach reassembles those pieces together into multiple person instances. Since optimal region assembly is a challenging combinatorial problem, we present a Lagrangian relaxation method to accelerate the lower bound estimation, thereby enabling a fast branch and bound solution for the global optimum. As output, our method produces a pixel-level map indicating both 1) the body part labels (arm, leg, torso, and head), and 2) which parts belong to which individual person. Our results on challenging datasets show our method is robust to clutter, occlusion, and complex poses. It outperforms a variety of competing methods, including existing detector CRF methods and region CNN approaches. In addition, we demonstrate its impact on a proxemics recognition task, which demands a precise representation of \"whose body part is where\" in crowded images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jiang_Detangling_People_Individuating_CVPR_2017_paper.pdf",
        "aff": "Boston College, USA; University of Texas at Austin, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1604.03880v1",
        "pdf_size": 4379264,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6155978187918204570&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.bc.edu;cs.utexas.edu",
        "email": "cs.bc.edu;cs.utexas.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jiang_Detangling_People_Individuating_CVPR_2017_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Boston College;University of Texas at Austin",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bostoncollege.edu;https://www.utexas.edu",
        "aff_unique_abbr": "BC;UT Austin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Detect, Replace, Refine: Deep Structured Prediction for Pixel Wise Labeling",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2202",
        "author_site": "Spyros Gidaris, Nikos Komodakis",
        "author": "Spyros Gidaris; Nikos Komodakis",
        "abstract": "Pixel wise image labeling is an interesting and challenging problem with great significance in the computer vision community. In order for a dense labeling algorithm to be able to achieve accurate and precise results, it has to consider the dependencies that exist in the joint space of both the input and the output variables. An implicit approach for modeling those dependencies is by training a deep neural network that, given as input an initial estimate of the output labels and the input image, it will be able to predict a new refined estimate for the labels. In this context, our work is concerned with what is the optimal architecture for performing the label improvement task. We argue that the prior approaches of either directly predicting new label estimates or predicting residual corrections w.r.t. the initial labels with feed-forward deep network architectures are sub-optimal. Instead, we propose a generic architecture that decomposes the label improvement task to three steps: 1) detecting the initial label estimates that are incorrect, 2) replacing the incorrect labels with new ones, and finally 3) refining the renewed labels by predicting residual corrections w.r.t. them. Furthermore, we explore and compare various other alternative architectures that consist of the aforementioned Detection, Replace, and Refine components. We extensively evaluate the examined architectures in the challenging task of dense disparity estimation (stereo matching) and we report both quantitative and qualitative results on three different datasets. Finally, our dense disparity estimation network that implements the proposed generic architecture, achieves state-of-the-art results in the KITTI 2015 test surpassing prior approaches by a significant margin. We plan to release the Torch code that implements the paper in: https://github.com/gidariss/DRR_struct_pred/ .",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gidaris_Detect_Replace_Refine_CVPR_2017_paper.pdf",
        "aff": "University Paris-Est, LIGM + Ecole des Ponts ParisTech; University Paris-Est, LIGM + Ecole des Ponts ParisTech",
        "project": "",
        "github": "https://github.com/gidariss/DRR_struct_pred/",
        "supp": "",
        "arxiv": "1612.04770v1",
        "pdf_size": 757453,
        "gs_citation": 195,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12918350164540875181&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "imagine.enpc.fr;enpc.fr",
        "email": "imagine.enpc.fr;enpc.fr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gidaris_Detect_Replace_Refine_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University Paris-Est;Ecole des Ponts ParisTech",
        "aff_unique_dep": "LIGM;",
        "aff_unique_url": "https://www.univ-Paris12.fr;https://www.ponts.org",
        "aff_unique_abbr": ";ENPC",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "France"
    },
    {
        "title": "Detecting Masked Faces in the Wild With LLE-CNNs",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "970",
        "author_site": "Shiming Ge, Jia Li, Qiting Ye, Zhao Luo",
        "author": "Shiming Ge; Jia Li; Qiting Ye; Zhao Luo",
        "abstract": "Detecting masked faces (i.e., faces with occlusions) is a challenging task due to two main reasons: 1)the absence of large datasets of masked faces, and 2)the absence of facial cues from the masked regions. To address these issues, this paper first introduces a dataset with 30,811 Internet images and 35,806 annotated MAsked FAces, which is denoted as MAFA. Different from many previous datasets, each annotated face in MAFA is partially occluded by mask. By analyzing the characteristics of masked faces, we propose LLE-CNNs that detect masked face via three major modules. The proposal module first combines two pre-trained CNNs to extract candidate facial regions from the input image and represent them with high dimensional descriptors. After that, the embedding module turns such descriptors into vectors of weights with respect to the components in pre-trained dictionaries of representative normal faces and non-faces by using locally linear embedding. In this manner, missing facial cues in the masked regions can be largely recovered, and the influences of noisy cues introduced by diversified masks can be greatly alleviated. Finally, the verification module takes the weight vectors as input and identifies real facial regions as well as their accurate positions by jointly performing the classification and regression tasks within unified CNNs. Experimental results on MAFA show that the proposed approach significantly outperforms 6 state-of-the-arts by at least 15.6% in detecting masked faces.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ge_Detecting_Masked_Faces_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1202862,
        "gs_citation": 557,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6433214971462802833&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ge_Detecting_Masked_Faces_CVPR_2017_paper.html"
    },
    {
        "title": "Detecting Oriented Text in Natural Images by Linking Segments",
        "session": "Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "919",
        "author_site": "Baoguang Shi, Xiang Bai, Serge Belongie",
        "author": "Baoguang Shi; Xiang Bai; Serge Belongie",
        "abstract": "Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method. The main idea is to decompose text into two locally detectable elements, namely segments and links. A segment is an oriented box covering a part of a word or text line; A link connects two adjacent segments, indicating that they belong to the same word or text line. Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network. Final detections are produced by combining segments connected by links. Compared with previous methods, SegLink improves along the dimensions of accuracy, speed, and ease of training. It achieves an f-measure of 75.0% on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin. It runs at over 20 FPS on 512x512 images. Moreover, without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shi_Detecting_Oriented_Text_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.06520v3",
        "gs_citation": 864,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13171496269524451923&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shi_Detecting_Oriented_Text_CVPR_2017_paper.html"
    },
    {
        "title": "Detecting Visual Relationships With Deep Relational Networks",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Oral",
        "track": "main",
        "pid": "1142",
        "author_site": "Bo Dai, Yuqi Zhang, Dahua Lin",
        "author": "Bo Dai; Yuqi Zhang; Dahua Lin",
        "abstract": "Relationships among objects play a crucial role in image understanding. Despite the great success of deep learning techniques in recognizing individual objects, reasoning about the relationships among objects remains a challenging task. Previous methods often treat this as a classification problem, considering each type of relationship (e.g. \"ride\") or each distinct visual phrase (e.g. \"person- ride-horse\") as a category. Such approaches are faced with significant difficulties caused by the high diversity of visual appearance for each kind of relationships or the large number of distinct visual phrases. We propose an integrated framework to tackle this problem. At the heart of this framework is the Deep Relational Network, a novel formulation designed specifically for exploiting the statistical dependencies between objects and their relationships. On two large data sets, the proposed method achieves substantial improvement over state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dai_Detecting_Visual_Relationships_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03114v2",
        "gs_citation": 625,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16252262600375830477&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dai_Detecting_Visual_Relationships_CVPR_2017_paper.html"
    },
    {
        "title": "Differential Angular Imaging for Material Recognition",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "248",
        "author_site": "Jia Xue, Hang Zhang, Kristin Dana, Ko Nishino",
        "author": "Jia Xue; Hang Zhang; Kristin Dana; Ko Nishino",
        "abstract": "Material recognition for real-world outdoor surfaces has become increasingly important for computer vision to support its operation \"in the wild.\" Computational surface modeling that underlies material recognition has transitioned from reflectance modeling using in-lab controlled radiometric measurements to image-based representations based on internet-mined images of materials captured in the scene. We propose to take a middle-ground approach for material recognition that takes advantage of both rich radiometric cues and flexible image capture. We realize this by developing a framework for differential angular imaging, where small angular variations in image capture provide an enhanced appearance representation and significant recognition improvement. We build a large-scale material database, Ground Terrain in Outdoor Scenes (GTOS) database, geared towards real use for autonomous agents. The database consists of over 30,000 images covering 40 classes of outdoor ground terrain under varying weather and lighting conditions. We develop a novel approach for material recognition called a Differential An- gular Imaging Network (DAIN) to fully leverage this large dataset. With this novel network architecture, we extract characteristics of materials encoded in the angular and spatial gradients of their appearance. Our results show that DAIN achieves recognition performance that surpasses single view or coarsely quantized multiview images. These results demonstrate the effectiveness of differential angular imaging as a means for flexible, in-place material recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xue_Differential_Angular_Imaging_CVPR_2017_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ 08854; Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ 08854; Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ 08854; Department of Computer Science, Drexel University, Philadelphia, PA 19104",
        "project": "http://ece.rutgers.edu/vision/",
        "github": "",
        "supp": "",
        "arxiv": "1612.02372v2",
        "pdf_size": 1134786,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7679813408104364227&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "rutgers.edu;rutgers.edu;ece.rutgers.edu;drexel.edu",
        "email": "rutgers.edu;rutgers.edu;ece.rutgers.edu;drexel.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xue_Differential_Angular_Imaging_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Rutgers University;Drexel University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.rutgers.edu;https://drexel.edu",
        "aff_unique_abbr": "Rutgers;Drexel",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Piscataway;Philadelphia",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Dilated Residual Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "153",
        "author_site": "Fisher Yu, Vladlen Koltun, Thomas Funkhouser",
        "author": "Fisher Yu; Vladlen Koltun; Thomas Funkhouser",
        "abstract": "Convolutional networks for image classification progressively reduce resolution until the image is represented by tiny feature maps in which the spatial structure of the scene is no longer discernible. Such loss of spatial acuity can limit image classification accuracy and complicate the transfer of the model to downstream applications that require detailed scene understanding. These problems can be alleviated by dilation, which increases the resolution of output feature maps without reducing the receptive field of individual neurons. We show that dilated residual networks (DRNs) outperform their non-dilated counterparts in image classification without increasing the model's depth or complexity. We then study gridding artifacts introduced by dilation, develop an approach to removing these artifacts (`degridding'), and show that this further increases the performance of DRNs. In addition, we show that the accuracy advantage of DRNs is further magnified in downstream applications such as object localization and semantic segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_Dilated_Residual_Networks_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.09914v1",
        "pdf_size": 1671017,
        "gs_citation": 2214,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2962359587881515950&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_Dilated_Residual_Networks_CVPR_2017_paper.html"
    },
    {
        "title": "Direct Photometric Alignment by Mesh Deformation",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "884",
        "author_site": "Kaimo Lin, Nianjuan Jiang, Shuaicheng Liu, Loong-Fah Cheong, Minh Do, Jiangbo Lu",
        "author": "Kaimo Lin; Nianjuan Jiang; Shuaicheng Liu; Loong-Fah Cheong; Minh Do; Jiangbo Lu",
        "abstract": "The choice of motion models is vital in applications like image/video stitching and video stabilization. Conventional methods explored different approaches ranging from simple global parametric models to complex per-pixel optical flow. Mesh-based warping methods achieve a good balance between computational complexity and model flexibility. However, they typically require high quality feature correspondences and suffer from mismatches and low-textured image content. In this paper, we propose a mesh-based photometric alignment method that minimizes pixel intensity difference instead of Euclidean distance of known feature correspondences. The proposed method combines the superior performance of dense photometric alignment with the efficiency of mesh-based image warping. It achieves better global alignment quality than the feature-based counterpart in textured images, and more importantly, it is also robust to low-textured image content. Abundant experiments show that our method can handle a variety of images and videos, and outperforms representative state-of-the-art methods in both image stitching and video stabilization tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Direct_Photometric_Alignment_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1852861,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10588705292104744593&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Direct_Photometric_Alignment_CVPR_2017_paper.html"
    },
    {
        "title": "Discover and Learn New Objects From Documentaries",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Spotlight",
        "track": "main",
        "pid": "1145",
        "author_site": "Kai Chen, Hang Song, Chen Change Loy, Dahua Lin",
        "author": "Kai Chen; Hang Song; Chen Change Loy; Dahua Lin",
        "abstract": "Despite the remarkable progress in recent years, detecting objects in a new context remains a challenging task. Detectors learned from a public dataset can only work with a fixed list of categories, while training from scratch usually requires a large amount of training data with detailed annotations. This work aims to explore a novel approach -- learning object detectors from documentary films in a weakly supervised manner. This is inspired by the observation that documentaries often provide dedicated exposition of certain object categories, where visual presentations are aligned with subtitles. We believe that object detectors can be learned from such a rich source of information. Towards this goal, we develop a joint probabilistic framework, where individual pieces of information, including video frames and subtitles, are brought together via both visual and linguistic links. On top of this formulation, we further derive a weakly supervised learning algorithm, where object model learning and training set mining are unified in an optimization procedure. Experimental results on a real world dataset demonstrate that this is an effective approach to learning new object detectors.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Discover_and_Learn_CVPR_2017_paper.pdf",
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1707.09593",
        "pdf_size": 1580992,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2554420897064003099&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Discover_and_Learn_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Information Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Discovering Causal Signals in Images",
        "session": "Machine Learning 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "3235",
        "author_site": "David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard Sch\u00c3\u00b6lkopf, L\u00c3\u00a9on Bottou",
        "author": "David Lopez-Paz; Robert Nishihara; Soumith Chintala; Bernhard Scholkopf; Leon Bottou",
        "abstract": "This paper establishes the existence of observable footprints that reveal the \"causal dispositions\" of the object categories appearing in collections of images.  We achieve this goal in two steps.  First, we take a learning approach to observational causal discovery, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, given samples from their joint distribution.  Second, we use our causal direction classifier to effectively distinguish between features of objects and features of their contexts in collections of static images.  Our experiments demonstrate the existence of a relation between the direction of causality and the difference between objects and their contexts, and by the same token, the existence of observable signals that reveal the causal dispositions of objects.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lopez-Paz_Discovering_Causal_Signals_CVPR_2017_paper.pdf",
        "aff": "Facebook AI Research; UC Berkeley; Facebook AI Research; MPI for Intelligent Systems; Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1605.08179",
        "pdf_size": 1208467,
        "gs_citation": 302,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4684930995355004969&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "fb.com;eecs.berkeley.edu;fb.com;tue.mpg.de;bottou.org",
        "email": "fb.com;eecs.berkeley.edu;fb.com;tue.mpg.de;bottou.org",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lopez-Paz_Discovering_Causal_Signals_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "Meta;University of California, Berkeley;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Facebook AI Research;;",
        "aff_unique_url": "https://research.facebook.com;https://www.berkeley.edu;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "FAIR;UC Berkeley;MPI-IS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "Discretely Coding Semantic Rank Orders for Supervised Image Hashing",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "530",
        "author_site": "Li Liu, Ling Shao, Fumin Shen, Mengyang Yu",
        "author": "Li Liu; Ling Shao; Fumin Shen; Mengyang Yu",
        "abstract": "Learning to hash has been recognized to accomplish highly efficient storage and retrieval for large-scale visual data. Particularly, ranking-based hashing techniques have recently attracted broad research attention because ranking accuracy among the retrieved data is well explored and their objective is more applicable to realistic search tasks. However, directly optimizing discrete hash codes without continuous-relaxations on a nonlinear ranking objective is infeasible by either traditional optimization methods or even recent discrete hashing algorithms. To address this challenging issue, in this paper, we introduce a novel supervised hashing method, dubbed Discrete Semantic Ranking Hashing (DSeRH), which aims to directly embed semantic rank orders into binary codes. In DSeRH, a generalized Adaptive Discrete Minimization (ADM) approach is proposed to discretely optimize binary codes with the quadratic nonlinear ranking objective in an iterative manner and is guaranteed to converge quickly. Additionally, instead of using 0/1 independent labels to form rank orders as in previous works, we generate the listwise rank orders from the high-level semantic word embeddings which can quantitatively capture the intrinsic correlation between different categories. We evaluate our DSeRH, coupled with both linear and deep convolutional neural network (CNN) hash functions, on three image datasets, i.e., CIFAR-10, SUN397 and ImageNet100, and the results manifest that DSeRH can outperform the state-of-the-art ranking-based hashing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Discretely_Coding_Semantic_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Liu_Discretely_Coding_Semantic_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3555749483953248934&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Discretely_Coding_Semantic_CVPR_2017_paper.html"
    },
    {
        "title": "Discriminative Bimodal Networks for Visual Localization and Detection With Natural Language Queries",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Spotlight",
        "track": "main",
        "pid": "178",
        "author_site": "Yuting Zhang, Luyao Yuan, Yijie Guo, Zhiyuan He, I-An Huang, Honglak Lee",
        "author": "Yuting Zhang; Luyao Yuan; Yijie Guo; Zhiyuan He; I-An Huang; Honglak Lee",
        "abstract": "Associating image regions with text queries has been recently explored as a new way to bridge visual and linguistic representations. A few pioneering approaches have been proposed based on recurrent neural language models trained generatively (e.g., generating captions), but achieving somewhat limited localization accuracy. To better address natural-language-based visual entity localization, we propose a discriminative approach. We formulate a discriminative bimodal neural network (DBNet), which can be trained by a classifier with extensive use of negative samples. Our training objective encourages better localization on single images, incorporates text phrases in a broad range, and properly pairs image regions with text phrases into positive and negative examples. Experiments on the Visual Genome dataset demonstrate the proposed DBNet significantly outperforms previous state-of-the-art methods both for localization on single images and for detection on multiple images. We we also establish an evaluation protocol for natural-language visual detection.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Discriminative_Bimodal_Networks_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_Discriminative_Bimodal_Networks_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.03944v2",
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1251174635592626721&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Discriminative_Bimodal_Networks_CVPR_2017_paper.html"
    },
    {
        "title": "Discriminative Correlation Filter With Channel and Spatial Reliability",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "2792",
        "author_site": "Alan Luke\u00c5\u00bei\u00c4\u008d, Tom\u00c3\u00a1\u00c5\u00a1 Voj\u00c3\u00ad\u00c5\u0099, Luka \u00c4\u008cehovin Zajc, Ji\u00c5\u0099\u00c3\u00ad Matas, Matej Kristan",
        "author": "Alan Lukezic; Tomas Vojir; Luka Cehovin Zajc; Jiri Matas; Matej Kristan",
        "abstract": "Short-term tracking is an open and challenging  problem for which discriminative correlation filters (DCF) have shown  excellent performance. We introduce the channel and spatial reliability concepts to DCF tracking and provide a novel learning algorithm for its efficient and seamless integration in the filter update and the tracking process. The spatial reliability map adjusts the  filter support to the part of the object suitable for tracking. This allows tracking of non-rectangular objects as well as  extending  the search region. Channel reliability reflects the quality of the learned filter and it is used as a feature weighting coefficient in localization. Experimentally,  with only two simple standard  features, HOGs and Colornames, the novel CSR-DCF method -- DCF with Channel and Spatial Reliability -- achieves state-of-the-art results on VOT 2016, VOT 2015  and OTB. The CSR-DCF runs in real-time on a CPU.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lukezic_Discriminative_Correlation_Filter_CVPR_2017_paper.pdf",
        "aff": "Faculty of Computer and Information Science, University of Ljubljana, Slovenia; Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic; Faculty of Computer and Information Science, University of Ljubljana, Slovenia; Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic; Faculty of Computer and Information Science, University of Ljubljana, Slovenia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Lukezic_Discriminative_Correlation_Filter_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.08461",
        "pdf_size": 2169376,
        "gs_citation": 1672,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10607552131352769008&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": "fri.uni-lj.si;fri.uni-lj.si;fri.uni-lj.si;cmp.felk.cvut.cz;cmp.felk.cvut.cz",
        "email": "fri.uni-lj.si;fri.uni-lj.si;fri.uni-lj.si;cmp.felk.cvut.cz;cmp.felk.cvut.cz",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lukezic_Discriminative_Correlation_Filter_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "University of Ljubljana;Czech Technical University in Prague",
        "aff_unique_dep": "Faculty of Computer and Information Science;Faculty of Electrical Engineering",
        "aff_unique_url": "https://www.fcis.unilj.si;https://www.cvut.cz",
        "aff_unique_abbr": ";CTU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Prague",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "Slovenia;Czech Republic"
    },
    {
        "title": "Discriminative Covariance Oriented Representation Learning for Face Recognition With Image Sets",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2377",
        "author_site": "Wen Wang, Ruiping Wang, Shiguang Shan, Xilin Chen",
        "author": "Wen Wang; Ruiping Wang; Shiguang Shan; Xilin Chen",
        "abstract": "For face recognition with image sets, while most existing works mainly focus on building robust set models with hand-crafted feature, it remains a research gap to learn better image representations which can closely match the subsequent image set modeling and classification. Taking sample covariance matrix as set model in the light of its recent promising success, we present a Discriminative Covariance oriented Representation Learning (DCRL) framework to bridge the above gap. The framework constructs a feature learning network (e.g. a CNN) to project the face images into a target representation space, and the network is trained towards the goal that the set covariance matrix calculated in the target space has maximum discriminative ability. To encode the discriminative ability of set covariance matrices, we elaborately design two different loss functions, which respectively lead to two different representation learning schemes, i.e., the Graph Embedding scheme and the Softmax Regression scheme. Both schemes optimize the whole network containing both image representation mapping and set model classification in a joint learning manner. The proposed method is extensively validated on three challenging and large scale databases for the task of face recognition with image sets, i.e., YouTube Celebrities, YouTube Face DB and Point-and-Shoot Challenge.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Discriminative_Covariance_Oriented_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Wang_Discriminative_Covariance_Oriented_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5022178215250032368&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Discriminative_Covariance_Oriented_CVPR_2017_paper.html"
    },
    {
        "title": "Discriminative Optimization: Theory and Applications to Point Cloud Registration",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1663",
        "author_site": "Jayakorn Vongkulbhisal, Fernando De la Torre, Jo\u00c3\u00a3o P. Costeira",
        "author": "Jayakorn Vongkulbhisal; Fernando De la Torre; Joao P. Costeira",
        "abstract": "Many computer vision problems are formulated as the optimization of a cost function. This approach faces two main challenges: (1) designing a cost function with a local optimum at an acceptable solution, and (2) developing an efficient numerical method to search for one (or multiple) of these local optima. While designing such functions is feasible in the noiseless case, the stability and location of local optima are mostly unknown under noise, occlusion, or missing data. In practice, this can result in undesirable local optima or not having a local optimum in the expected place. On the other hand, numerical optimization algorithms in high-dimensional spaces are typically local and often rely on expensive first or second order information to guide the search. To overcome these limitations, this paper proposes Discriminative Optimization (DO), a method that learns search directions from data without the need of a cost function. Specifically, DO explicitly learns a sequence of updates in the search space that leads to stationary points that correspond to desired solutions. We provide a formal analysis of DO and illustrate its benefits in the problem of 2D and 3D point cloud registration both in synthetic and range-scan data. We show that DO outperforms state-of-the-art algorithms by a large margin in terms of accuracy, robustness to perturbations, and computational efficiency.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Vongkulbhisal_Discriminative_Optimization_Theory_CVPR_2017_paper.pdf",
        "aff": "ISR - IST, Universidade de Lisboa, Lisboa, Portugal\u2021Carnegie Mellon University, Pittsburgh, PA, USA; ISR - IST, Universidade de Lisboa, Lisboa, Portugal\u2021Carnegie Mellon University, Pittsburgh, PA, USA; ISR - IST, Universidade de Lisboa, Lisboa, Portugal",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Vongkulbhisal_Discriminative_Optimization_Theory_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 3804545,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16754373166407185907&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "andrew.cmu.edu;cs.cmu.edu;isr.ist.utl.pt",
        "email": "andrew.cmu.edu;cs.cmu.edu;isr.ist.utl.pt",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Vongkulbhisal_Discriminative_Optimization_Theory_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universidade de Lisboa",
        "aff_unique_dep": "ISR - IST",
        "aff_unique_url": "https://www.ulusiada.pt",
        "aff_unique_abbr": "UL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lisboa",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Portugal"
    },
    {
        "title": "Disentangled Representation Learning GAN for Pose-Invariant Face Recognition",
        "session": "Analyzing Humans 1",
        "status": "Oral",
        "track": "main",
        "pid": "528",
        "author_site": "Luan Tran, Xi Yin, Xiaoming Liu",
        "author": "Luan Tran; Xi Yin; Xiaoming Liu",
        "abstract": "The large pose discrepancy between two face images is one of the key challenges in face recognition. Conventional approaches for pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator allows DR-GAN to learn a generative and discriminative representation, in addition to image synthesis. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified representation along with an arbitrary number of synthetic images. Quantitative and qualitative evaluation on both controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tran_Disentangled_Representation_Learning_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 1217,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6254408511674575433&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tran_Disentangled_Representation_Learning_CVPR_2017_paper.html"
    },
    {
        "title": "Distinguishing the Indistinguishable: Exploring Structural Ambiguities via Geodesic Context",
        "session": "3D Vision 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "1473",
        "author_site": "Qingan Yan, Long Yang, Ling Zhang, Chunxia Xiao",
        "author": "Qingan Yan; Long Yang; Ling Zhang; Chunxia Xiao",
        "abstract": "A perennial problem in structure from motion (SfM) is visual ambiguity posed by repetitive structures. Recent disambiguating algorithms infer ambiguities mainly via explicit background context, thus face limitations in highly ambiguous scenes which are visually indistinguishable. Instead of analyzing local visual information, we propose a novel algorithm for SfM disambiguation that explores the global topology as encoded in photo collections. An important adaptation of this work is to approximate the available imagery using a manifold of viewpoints. We note that, while ambiguous images appear deceptively similar in appearance, they are actually located far apart on geodesics. We establish the manifold by adaptively identifying cameras with adjacent viewpoint, and detect ambiguities via a new measure, geodesic consistency. We demonstrate the accuracy and efficiency of the proposed approach on a range of complex ambiguity datasets, even including the challenging scenes without background conflicts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yan_Distinguishing_the_Indistinguishable_CVPR_2017_paper.pdf",
        "aff": "School of Computer, Wuhan University, China; School of Computer, Wuhan University, China; School of Computer, Wuhan University, China; State Key Lab of Software Engineering, Wuhan University, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3195562,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=673272287826064651&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn",
        "email": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yan_Distinguishing_the_Indistinguishable_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Wuhan University",
        "aff_unique_dep": "School of Computer",
        "aff_unique_url": "http://www.whu.edu.cn",
        "aff_unique_abbr": "WHU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Wuhan",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Diverse Image Annotation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "926",
        "author_site": "Baoyuan Wu, Fan Jia, Wei Liu, Bernard Ghanem",
        "author": "Baoyuan Wu; Fan Jia; Wei Liu; Bernard Ghanem",
        "abstract": "In this work we study the task of image annotation, of which the goal is to describe an image using a few tags. Instead of predicting the full list of tags, here we target for providing a short list of tags under a limited number (e.g., 3), to cover as much information as possible of the image. The tags in such a short list should be representative and diverse. It means they are required to be not only corresponding to the contents of the image, but also be different to each other.  To this end, we treat the image annotation as a subset selection problem based on the conditional determinantal point process (DPP) model, which formulates the representation and diversity jointly.   We further explore the semantic hierarchy and synonyms among the candidate tags, and require that two tags in a semantic hierarchy or in a pair of synonyms should not be selected simultaneously. This requirement is then embedded into the sampling algorithm according to the learned conditional DPP model.   Besides, we find that traditional metrics for image annotation (e.g., precision, recall and F1 score) only consider the representation, but ignore the diversity. Thus we propose new metrics to evaluate the quality of the selected subset (i.e., the tag list), based on the semantic hierarchy and synonyms. Human study through Amazon Mechanical Turk verifies that the proposed metrics are more close to the human's judgment than traditional metrics. Experiments on two benchmark datasets show that the proposed method can produce more representative and diverse tags, compared with existing image annotation methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wu_Diverse_Image_Annotation_CVPR_2017_paper.pdf",
        "aff": "King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia+Tencent AI Lab, Shenzhen, China; King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Tencent AI Lab, Shenzhen, China; King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Wu_Diverse_Image_Annotation_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1436267,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6984498864375444631&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;kaust.edu.sa;ee.columbia.edu;kaust.edu.sa",
        "email": "gmail.com;kaust.edu.sa;ee.columbia.edu;kaust.edu.sa",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wu_Diverse_Image_Annotation_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;1;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology;Tencent",
        "aff_unique_dep": ";AI Lab",
        "aff_unique_url": "https://www.kaust.edu.sa;https://ai.tencent.com",
        "aff_unique_abbr": "KAUST;Tencent AI Lab",
        "aff_campus_unique_index": "0+1;0;1;0",
        "aff_campus_unique": "Thuwal;Shenzhen",
        "aff_country_unique_index": "0+1;0;1;0",
        "aff_country_unique": "Saudi Arabia;China"
    },
    {
        "title": "Diversified Texture Synthesis With Feed-Forward Networks",
        "session": "Low- & Mid-Level Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "1532",
        "author_site": "Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang",
        "author": "Yijun Li; Chen Fang; Jimei Yang; Zhaowen Wang; Xin Lu; Ming-Hsuan Yang",
        "abstract": "Recent progresses on deep discriminative and generative modeling have shown promising results on texture synthesis. However, existing feed-forward based methods trade off generality for efficiency, which suffer from many issues, such as shortage of generality (i.e., build one network per texture), lack of diversity (i.e., always produce visually identical output) and suboptimality (i.e., generate less satisfying visual effects). In this work, we focus on solving these issues for improved texture synthesis. We propose a deep generative feed-forward network which enables efficient synthesis of multiple textures within one single network and meaningful interpolation between them. Meanwhile, a suite of important techniques are introduced to achieve better convergence and diversity. With extensive experiments, we demonstrate the effectiveness of the proposed model and techniques for synthesizing a large number of textures and show its applications with the stylization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Diversified_Texture_Synthesis_CVPR_2017_paper.pdf",
        "aff": "University of California, Merced; Adobe Research; Adobe Research; Adobe Research; Adobe Research; University of California, Merced",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.01664v1",
        "pdf_size": 2077800,
        "gs_citation": 341,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16394956919586545760&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ucmerced.edu;adobe.com;adobe.com;adobe.com;adobe.com;ucmerced.edu",
        "email": "ucmerced.edu;adobe.com;adobe.com;adobe.com;adobe.com;ucmerced.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Diversified_Texture_Synthesis_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "University of California, Merced;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.ucmerced.edu;https://research.adobe.com",
        "aff_unique_abbr": "UC Merced;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Merced;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Domain Adaptation by Mixture of Alignments of Second- or Higher-Order Scatter Tensors",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1823",
        "author_site": "Piotr Koniusz, Yusuf Tas, Fatih Porikli",
        "author": "Piotr Koniusz; Yusuf Tas; Fatih Porikli",
        "abstract": "In this paper, we propose an approach to the domain adaptation, dubbed Second- or Higher-order Transfer of Knowledge (So-HoT), based on the mixture of alignments of second- or higher-order scatter statistics between the source and target domains. The human ability to learn from few labeled samples is a recurring motivation in the literature for domain adaptation. Towards this end, we investigate the supervised target scenario for which few labeled target training samples per category exist. Specifically, we utilize two CNN streams: the source and target networks fused at the classifier level. Features from the fully connected layers fc7 of each network are used to compute second- or even higher-order scatter tensors; one per network stream per class. As the source and target distributions are somewhat different despite being related, we align the scatters of the two network streams of the same class (within-class scatters) to a desired degree with our bespoke loss while maintaining good separation of the between-class scatters. We train the entire network in end-to-end fashion. We provide evaluations on the standard Office benchmark (visual domains) and RGB-D combined with Caltech256 (depth-to-rgb transfer). We attain state-of-the-art results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Koniusz_Domain_Adaptation_by_CVPR_2017_paper.pdf",
        "aff": "Data61/CSIRO+Australian National University; Data61/CSIRO+Australian National University; Australian National University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 943332,
        "gs_citation": 163,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1690840825009680659&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "data61.csiro.au;data61.csiro.au;anu.edu.au",
        "email": "data61.csiro.au;data61.csiro.au;anu.edu.au",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Koniusz_Domain_Adaptation_by_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;1",
        "aff_unique_norm": "Commonwealth Scientific and Industrial Research Organisation;Australian National University",
        "aff_unique_dep": "Data61;",
        "aff_unique_url": "https://www.csiro.au;https://www.anu.edu.au",
        "aff_unique_abbr": "CSIRO;ANU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Dual Attention Networks for Multimodal Reasoning and Matching",
        "session": "Machine Learning 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "110",
        "author_site": "Hyeonseob Nam, Jung-Woo Ha, Jeonghee Kim",
        "author": "Hyeonseob Nam; Jung-Woo Ha; Jeonghee Kim",
        "abstract": "We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Nam_Dual_Attention_Networks_CVPR_2017_paper.pdf",
        "aff": "Search Solutions Inc.; NA VER Corp.; NA VER LABS Corp.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.00471v2",
        "pdf_size": 1514510,
        "gs_citation": 882,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10727729956342153729&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "navercorp.com;navercorp.com;naverlabs.com",
        "email": "navercorp.com;navercorp.com;naverlabs.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Nam_Dual_Attention_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Search Solutions Inc.;NAVER Corporation;NAVER LABS",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.naver.com;https://www.naverlabs.com",
        "aff_unique_abbr": ";NAVER;NAVER LABS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "title": "Dynamic Attention-Controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-Set Sample Weighting",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "903",
        "author_site": "Zhen-Hua Feng, Josef Kittler, William Christmas, Patrik Huber, Xiao-Jun Wu",
        "author": "Zhen-Hua Feng; Josef Kittler; William Christmas; Patrik Huber; Xiao-Jun Wu",
        "abstract": "We present a new Cascaded Shape Regression (CSR) architecture, namely Dynamic Attention-Controlled CSR (DAC-CSR), for robust facial landmark detection on unconstrained faces. Our DAC-CSR divides facial landmark detection into three cascaded sub-tasks: face bounding box refinement, general CSR and attention-controlled CSR. The first two stages refine initial face bounding boxes and output intermediate facial landmarks. Then, an online dynamic model selection method is used to choose appropriate domain-specific CSRs for further landmark refinement. The key innovation of our DAC-CSR is the fault-tolerant mechanism, using fuzzy set sample weighting, for attention-controlled domain-specific model training. Moreover, we advocate data augmentation with a simple but effective 2D profile face generator, and context-aware feature extraction for better facial feature representation. Experimental results obtained on challenging datasets demonstrate the merits of our DAC-CSR over the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Feng_Dynamic_Attention-Controlled_Cascaded_CVPR_2017_paper.pdf",
        "aff": "Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford GU2 7XH, UK; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford GU2 7XH, UK; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford GU2 7XH, UK; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford GU2 7XH, UK; School of IoT Engineering, Jiangnan University, Wuxi 214122, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05396v2",
        "pdf_size": 2014044,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4368156886784251274&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "surrey.ac.uk;surrey.ac.uk;surrey.ac.uk;surrey.ac.uk;jiangnan.edu.cn",
        "email": "surrey.ac.uk;surrey.ac.uk;surrey.ac.uk;surrey.ac.uk;jiangnan.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Feng_Dynamic_Attention-Controlled_Cascaded_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "University of Surrey;Jiangnan University",
        "aff_unique_dep": "Centre for Vision, Speech and Signal Processing;School of IoT Engineering",
        "aff_unique_url": "https://www.surrey.ac.uk;https://www.jiangnan.edu.cn",
        "aff_unique_abbr": "Surrey;",
        "aff_campus_unique_index": "0;0;0;0;1",
        "aff_campus_unique": "Guildford;Wuxi",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "title": "Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs",
        "session": "Machine Learning 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "1358",
        "author_site": "Martin Simonovsky, Nikos Komodakis",
        "author": "Martin Simonovsky; Nikos Komodakis",
        "abstract": "A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Simonovsky_Dynamic_Edge-Conditioned_Filters_CVPR_2017_paper.pdf",
        "aff": "Universit \u00b4e Paris Est, \u00b4Ecole des Ponts ParisTech; Universit \u00b4e Paris Est, \u00b4Ecole des Ponts ParisTech",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Simonovsky_Dynamic_Edge-Conditioned_Filters_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.02901v3",
        "pdf_size": 934654,
        "gs_citation": 1648,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11073703403545981773&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "enpc.fr;enpc.fr",
        "email": "enpc.fr;enpc.fr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Simonovsky_Dynamic_Edge-Conditioned_Filters_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 Paris Est",
        "aff_unique_dep": "\u00c9cole des Ponts ParisTech",
        "aff_unique_url": "https://www.univ-Paris-est.fr",
        "aff_unique_abbr": "UPE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Dynamic FAUST: Registering Human Bodies in Motion",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Oral",
        "track": "main",
        "pid": "2752",
        "author_site": "Federica Bogo, Javier Romero, Gerard Pons-Moll, Michael J. Black",
        "author": "Federica Bogo; Javier Romero; Gerard Pons-Moll; Michael J. Black",
        "abstract": "While the ready availability of 3D scan data has influenced research throughout computer vision, less attention has focused on 4D data; that is 3D scans of moving non-rigid objects, captured over time. To be useful for vision research, such 4D scans need to be registered, or aligned, to a common topology. Consequently, extending mesh registration methods to 4D is important. Unfortunately, no ground-truth datasets are available for quantitative evaluation and comparison of 4D registration methods. To address this we create a novel dataset of high-resolution 4D scans of human subjects in motion, captured at 60 fps. We propose a new mesh registration method that uses both 3D geometry and texture information to register all scans in a sequence to a common reference topology. The approach exploits consistency in texture over both short and long time intervals and deals with temporal offsets between shape and texture capture. We show how using geometry alone results in significant errors in alignment when the motions are fast and non-rigid. We evaluate the accuracy of our registration and provide a dataset of 40,000 raw and aligned meshes. Dynamic FAUST extends the popular FAUST dataset to dynamic 4D data, and is available for research purposes at http://dfaust.is.tue.mpg.de.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bogo_Dynamic_FAUST_Registering_CVPR_2017_paper.pdf",
        "aff": "Microsoft, Cambridge, UK; Body Labs Inc., New York, NY; MPI for Intelligent Systems, T\u00fcbingen, Germany; MPI for Intelligent Systems, T\u00fcbingen, Germany",
        "project": "http://dfaust.is.tue.mpg.de",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1929118,
        "gs_citation": 445,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10005402680584463684&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "microsoft.com;bodylabs.com;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "microsoft.com;bodylabs.com;tuebingen.mpg.de;tuebingen.mpg.de",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bogo_Dynamic_FAUST_Registering_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Microsoft;Body Labs Inc.;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Microsoft;;",
        "aff_unique_url": "https://www.microsoft.com;https://www.bodylabs.com;https://www.mpituebingen.mpg.de",
        "aff_unique_abbr": "MSFT;;MPI-IS",
        "aff_campus_unique_index": "0;2;2",
        "aff_campus_unique": "Cambridge;;T\u00fcbingen",
        "aff_country_unique_index": "0;1;2;2",
        "aff_country_unique": "United Kingdom;United States;Germany"
    },
    {
        "title": "Dynamic Facial Analysis: From Bayesian Filtering to Recurrent Neural Network",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "574",
        "author_site": "Jinwei Gu, Xiaodong Yang, Shalini De Mello, Jan Kautz",
        "author": "Jinwei Gu; Xiaodong Yang; Shalini De Mello; Jan Kautz",
        "abstract": "Facial analysis in videos, including head pose estimation and facial landmark localization, is key for many applications such as facial animation capture, human activity recognition, and human-computer interaction. In this paper, we propose to use a recurrent neural network (RNN) for joint estimation and tracking of facial features in videos. We are inspired by the fact that the computation performed in an RNN bears resemblance to Bayesian filters, which have been used for tracking in many previous methods for facial analysis from videos. Bayesian filters used in these methods, however, require complicated, problem-specific design and tuning. In contrast, our proposed RNN-based method avoids such tracker-engineering by learning from training data, similar to how a convolutional neural network (CNN) avoids feature-engineering for image classification. As an end-to-end network, the proposed RNN-based method provides a generic and holistic solution for joint estimation and tracking of various types of facial features from consecutive video frames. Extensive experimental results on head pose estimation and facial landmark localization from videos demonstrate that the proposed RNN-based method outperforms frame-wise models and Bayesian filtering. In addition, we create a large-scale synthetic dataset for head pose estimation, with which we achieve state-of-the-art performance on a benchmark dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gu_Dynamic_Facial_Analysis_CVPR_2017_paper.pdf",
        "aff": "NVIDIA; NVIDIA; NVIDIA; NVIDIA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Gu_Dynamic_Facial_Analysis_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2385237,
        "gs_citation": 163,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15921574232288782764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "nvidia.com;nvidia.com;nvidia.com;nvidia.com",
        "email": "nvidia.com;nvidia.com;nvidia.com;nvidia.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gu_Dynamic_Facial_Analysis_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "NVIDIA",
        "aff_unique_dep": "NVIDIA Corporation",
        "aff_unique_url": "https://www.nvidia.com",
        "aff_unique_abbr": "NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Dynamic Time-Of-Flight",
        "session": "3D Vision 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "2685",
        "author_site": "Michael Schober, Amit Adam, Omer Yair, Shai Mazor, Sebastian Nowozin",
        "author": "Michael Schober; Amit Adam; Omer Yair; Shai Mazor; Sebastian Nowozin",
        "abstract": "Time-of-flight (TOF) depth cameras provide robust depth inference at low power requirements in a wide variety of consumer and industrial applications.  These cameras reconstruct a single depth frame from a given set of infrared (IR) frames captured over a very short exposure period.  Operating in this mode the camera essentially forgets all information previously captured - and performs depth inference from scratch for every frame.  We challenge this practice and propose using previously captured information when inferring depth. An inherent problem we have to address is camera motion over this longer period of collecting observations. We derive a probabilistic framework combining a simple but robust model of camera and object motion, together with an observation model. This combination allows us to  integrate information over multiple frames while remaining robust to rapid changes.  Operating the camera in this manner has implications in terms of both computational efficiency and how information should be captured. We address these two issues and demonstrate a realtime TOF system with robust temporal integration that improves depth accuracy over strong baseline methods including adaptive spatio-temporal filters.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Schober_Dynamic_Time-Of-Flight_CVPR_2017_paper.pdf",
        "aff": "Max Planck Institute for Intelligent Systems; Microsoft AIT, Haifa, Israel; Microsoft AIT, Haifa, Israel; Microsoft AIT, Haifa, Israel; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Schober_Dynamic_Time-Of-Flight_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 3279239,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9517362724421405675&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "tuebingen.mpg.de;gmail.com;gmail.com;gmail.com;microsoft.com",
        "email": "tuebingen.mpg.de;gmail.com;gmail.com;gmail.com;microsoft.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Schober_Dynamic_Time-Of-Flight_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Microsoft",
        "aff_unique_dep": "Intelligent Systems;Microsoft AIT",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.microsoft.com",
        "aff_unique_abbr": "MPI-IS;Microsoft",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Haifa",
        "aff_country_unique_index": "0;1;1;1;2",
        "aff_country_unique": "Germany;Israel;United States"
    },
    {
        "title": "EAST: An Efficient and Accurate Scene Text Detector",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "2343",
        "author_site": "Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang Zhou, Weiran He, Jiajun Liang",
        "author": "Xinyu Zhou; Cong Yao; He Wen; Yuzhi Wang; Shuchang Zhou; Weiran He; Jiajun Liang",
        "abstract": "Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even when equipped with deep neural network models, because the overall performance is determined by the interplay of multiple stages and components in the pipelines. In this work, we propose a simple yet powerful pipeline that yields fast and accurate text detection in natural scenes. The pipeline directly predicts words or text lines of arbitrary orientations and quadrilateral shapes in full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation and word partitioning), with a single neural network. The simplicity of our pipeline allows concentrating efforts on designing loss functions and neural network architecture. Experiments on standard datasets including ICDAR 2015, COCO-Text and MSRA-TD500 demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods in terms of both accuracy and efficiency. On the ICDAR 2015 dataset, the proposed algorithm achieves an F-score of 0.7820 at 13.2fps at 720p resolution.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf",
        "aff": "Megvii Technology Inc., Beijing, China; Megvii Technology Inc., Beijing, China; Megvii Technology Inc., Beijing, China; Megvii Technology Inc., Beijing, China; Megvii Technology Inc., Beijing, China; Megvii Technology Inc., Beijing, China; Megvii Technology Inc., Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03155v2",
        "pdf_size": 1403368,
        "gs_citation": 2207,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14855999552089039794&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "megvii.com;megvii.com;megvii.com;megvii.com;megvii.com;megvii.com;megvii.com",
        "email": "megvii.com;megvii.com;megvii.com;megvii.com;megvii.com;megvii.com;megvii.com",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_EAST_An_Efficient_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Megvii Technology Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.megvii.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "ECO: Efficient Convolution Operators for Tracking",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "3044",
        "author_site": "Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg",
        "author": "Martin Danelljan; Goutam Bhat; Fahad Shahbaz Khan; Michael Felsberg",
        "abstract": "In recent years, Discriminative Correlation Filter (DCF) based methods have significantly advanced the state-of-the-art in tracking. However, in the pursuit of ever increasing tracking performance, their characteristic speed and real-time capability have gradually faded. Further, the increasingly complex models, with massive number of trainable parameters, have introduced the risk of severe over-fitting. In this work, we tackle the key causes behind the problems of computational complexity and over-fitting, with the aim of simultaneously improving both speed and performance.  We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model; (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples; (iii) a conservative model update strategy with improved robustness and reduced complexity. We perform comprehensive experiments on four benchmarks: VOT2016, UAV123, OTB-2015, and TempleColor. When using expensive deep features, our tracker provides a 20-fold speedup and achieves a 13.0% relative gain in Expected Average Overlap compared to the top ranked method in the VOT2016 challenge. Moreover, our fast variant, using hand-crafted features, operates at 60 Hz on a single CPU, while obtaining 65.0% AUC on OTB-2015.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Danelljan_ECO_Efficient_Convolution_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Danelljan_ECO_Efficient_Convolution_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.09224",
        "pdf_size": 1654877,
        "gs_citation": 3142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16858432041905061958&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Danelljan_ECO_Efficient_Convolution_CVPR_2017_paper.html"
    },
    {
        "title": "ER3: A Unified Framework for Event Retrieval, Recognition and Recounting",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "810",
        "author_site": "Zhanning Gao, Gang Hua, Dongqing Zhang, Nebojsa Jojic, Le Wang, Jianru Xue, Nanning Zheng",
        "author": "Zhanning Gao; Gang Hua; Dongqing Zhang; Nebojsa Jojic; Le Wang; Jianru Xue; Nanning Zheng",
        "abstract": "We develop a unified framework for complex event retrieval, recognition and recounting. The framework is based on a compact video representation that exploits the temporal correlations in image features. Our feature alignment procedure identifies and removes the feature redundancies across frames and outputs an intermediate tensor representation we call video imprint. The video imprint is then fed into a reasoning network, whose attention mechanism parallels that of memory networks used in language modeling. The reasoning network simultaneously recognizes the event category and locates the key pieces of evidence for event recounting. In event retrieval tasks, we show that the compact video representation aggregated from the video imprint achieves significantly better retrieval accuracy compared with existing methods. We also set new state of the art results in event recognition tasks with an additional benefit: The latent structure in our reasoning network highlights the areas of the video imprint and can be directly used for event recounting. As video imprint maps back to locations in the video frames, the network allows not only the identification of key frames but also specific areas inside each frame which are most influential to the decision process.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gao_ER3_A_Unified_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9048153074391723187&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gao_ER3_A_Unified_CVPR_2017_paper.html"
    },
    {
        "title": "Efficient Diffusion on Region Manifolds: Recovering Small Objects With Compact CNN Representations",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "758",
        "author_site": "Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon, Ond\u00c5\u0099ej Chum",
        "author": "Ahmet Iscen; Giorgos Tolias; Yannis Avrithis; Teddy Furon; Ondrej Chum",
        "abstract": "Query expansion is a popular method to improve the quality of image retrieval with both conventional and CNN representations. It has been so far limited to global image similarity. This work focuses on diffusion, a mechanism that captures the image manifold in the feature space. An efficient off-line stage allows optional reduction in the number of stored regions. In the on-line stage, the proposed handling of unseen queries in the indexing stage removes additional computation to adjust the precomputed data. We perform diffusion through a sparse linear system solver, yielding practical query times well below one second.  Experimentally, we observe a significant boost in performance of image retrieval with compact CNN descriptors on standard benchmarks, especially when the query object covers only a small part of the image. Small objects have been a common failure case of CNN-based retrieval.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Iscen_Efficient_Diffusion_on_CVPR_2017_paper.pdf",
        "aff": "Inria Rennes; VRG, FEE, CTU in Prague; Inria Rennes; Inria Rennes; VRG, FEE, CTU in Prague",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05113v3",
        "pdf_size": 2371328,
        "gs_citation": 233,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11128317942489453247&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 27,
        "aff_domain": "inria.fr;cmp.felk.cvut.cz;inria.fr;inria.fr;cmp.felk.cvut.cz",
        "email": "inria.fr;cmp.felk.cvut.cz;inria.fr;inria.fr;cmp.felk.cvut.cz",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Iscen_Efficient_Diffusion_on_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0;1",
        "aff_unique_norm": "INRIA;Czech Technical University in Prague",
        "aff_unique_dep": ";Faculty of Electrical Engineering",
        "aff_unique_url": "https://www.inria.fr;https://www.fel.cvut.cz",
        "aff_unique_abbr": "Inria;CTU",
        "aff_campus_unique_index": "0;1;0;0;1",
        "aff_campus_unique": "Rennes;Prague",
        "aff_country_unique_index": "0;1;0;0;1",
        "aff_country_unique": "France;Czech Republic"
    },
    {
        "title": "Efficient Global Point Cloud Alignment Using Bayesian Nonparametric Mixtures",
        "session": "3D Vision 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "1071",
        "author_site": "Julian Straub, Trevor Campbell, Jonathan P. How, John W. Fisher III",
        "author": "Julian Straub; Trevor Campbell; Jonathan P. How; John W. Fisher III",
        "abstract": "Point cloud alignment is a common problem in computer vision and robotics, with applications ranging from 3D object recognition to reconstruction. We propose a novel approach to the alignment problem that utilizes Bayesian nonparametrics to describe the point cloud and surface normal densities, and branch and bound (BB) optimization to recover the relative transformation. BB uses a novel, refinable, near-uniform tessellation of rotation space using 4D tetrahedra, leading to more efficient optimization compared to the common axis-angle tessellation. We provide objective function bounds for pruning given the proposed tessellation, and prove that BB converges to the optimum of the cost function along with providing its computational complexity. Finally, we empirically demonstrate the efficiency of the proposed approach as well as its robustness to real-world conditions such as missing data and partial overlap.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Straub_Efficient_Global_Point_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Straub_Efficient_Global_Point_2017_CVPR_supplemental.pdf",
        "arxiv": "1603.04868",
        "pdf_size": 2591835,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18149043246727682906&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Straub_Efficient_Global_Point_CVPR_2017_paper.html"
    },
    {
        "title": "Efficient Linear Programming for Dense CRFs",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1227",
        "author_site": "Thalaiyasingam Ajanthan, Alban Desmaison, Rudy Bunel, Mathieu Salzmann, Philip H. S. Torr, M. Pawan Kumar",
        "author": "Thalaiyasingam Ajanthan; Alban Desmaison; Rudy Bunel; Mathieu Salzmann; Philip H. S. Torr; M. Pawan Kumar",
        "abstract": "The fully connected conditional random field (CRF) with Gaussian pairwise potentials has proven popular and effective for multi-class semantic segmentation. While the energy of a dense CRF can be minimized accurately using a linear programming (LP) relaxation, the state-of-the-art algorithm is too slow to be useful in practice. To alleviate this deficiency, we introduce an efficient LP minimization algorithm for dense CRFs. To this end, we develop a proximal minimization framework, where the dual of each proximal problem is optimized via block coordinate descent. We show that each block of variables can be efficiently optimized. Specifically, for one block, the problem decomposes into significantly smaller subproblems, each of which is defined over a single pixel. For the other block, the problem is optimized via conditional gradient descent. This has two advantages: 1) the conditional gradient can be computed in a time linear in the number of pixels and labels; and 2) the optimal step size can be computed analytically. Our experiments on standard datasets provide compelling evidence that our approach outperforms all existing baselines including the previous LP based approach for dense CRFs.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ajanthan_Efficient_Linear_Programming_CVPR_2017_paper.pdf",
        "aff": "Australian National University & Data61, CSIRO; Department of Engineering Science, University of Oxford; Department of Engineering Science, University of Oxford; Computer Vision Laboratory, EPFL; Department of Engineering Science, University of Oxford; Department of Engineering Science, University of Oxford + Alan Turing Institute",
        "project": "https://arxiv.org/abs/1611.09718",
        "github": "https://github.com/oval-group/DenseCRF",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ajanthan_Efficient_Linear_Programming_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.09718v2",
        "pdf_size": 1078928,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6923503215750754117&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ajanthan_Efficient_Linear_Programming_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;2;1;1+3",
        "aff_unique_norm": "Australian National University;University of Oxford;EPFL;Alan Turing Institute",
        "aff_unique_dep": ";Department of Engineering Science;Computer Vision Laboratory;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.ox.ac.uk;https://cvl.epfl.ch;https://www.turing.ac.uk",
        "aff_unique_abbr": "ANU;Oxford;EPFL;ATI",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Oxford",
        "aff_country_unique_index": "0;1;1;2;1;1+1",
        "aff_country_unique": "Australia;United Kingdom;Switzerland"
    },
    {
        "title": "Efficient Multiple Instance Metric Learning Using Weakly Supervised Data",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "183",
        "author_site": "Marc T. Law, Yaoliang Yu, Raquel Urtasun, Richard S. Zemel, Eric P. Xing",
        "author": "Marc T. Law; Yaoliang Yu; Raquel Urtasun; Richard S. Zemel; Eric P. Xing",
        "abstract": "We consider learning a distance metric in a weakly supervised setting where \"bags\" (or sets) of instances are labeled with \"bags\" of labels. A general approach is to formulate the problem as a Multiple Instance Learning (MIL) problem where the metric is learned so that the distances between instances inferred to be similar are smaller than the distances between instances inferred to be dissimilar. Classic approaches alternate the optimization over the learned metric and the assignment of similar instances. In this paper, we propose an efficient method that jointly learns the metric and the assignment of instances. In particular, our model is learned by solving an extension of k-means for MIL problems where instances are assigned to categories depending on annotations provided at bag-level. Our learning algorithm is much faster than existing metric learning methods for MIL problems and obtains state-of-the-art recognition performance in automated image annotation and instance classification for face identification.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Law_Efficient_Multiple_Instance_CVPR_2017_paper.pdf",
        "aff": "University of Toronto; University of Waterloo; University of Toronto; University of Toronto; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Law_Efficient_Multiple_Instance_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 725445,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7976621686278900196&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Law_Efficient_Multiple_Instance_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0;2",
        "aff_unique_norm": "University of Toronto;University of Waterloo;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utoronto.ca;https://uwaterloo.ca;https://www.cmu.edu",
        "aff_unique_abbr": "U of T;UW;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Efficient Optimization for Hierarchically-structured Interacting Segments (HINTS)",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "539",
        "author_site": "Hossam Isack, Olga Veksler, Ipek Oguz, Milan Sonka, Yuri Boykov",
        "author": "Hossam Isack; Olga Veksler; Ipek Oguz; Milan Sonka; Yuri Boykov",
        "abstract": "We propose an effective optimization algorithm for a general hierarchical segmentation model with geometric interactions between segments. Any given tree can specify a partial order over object labels defining a hierarchy. It is well-established that segment interactions, such as inclusion/exclusion and margin constraints, make the model significantly more discriminant. However, existing optimization methods do not allow full use of such models. Generic a-expansion results in weak local minima, while common binary multi-layered formulations lead to non-submodularity, complex high-order potentials, or polar domain unwrapping and shape biases. In practice, applying these methods to arbitrary trees does not work except for simple cases. Our main contribution is an optimization method for the Hierarchically-structured Interacting Segments (HINTS) model with arbitrary trees. Our Path-Moves algorithm is based on multi-label MRF formulation and can be seen as a combination of well-known a-expansion and Ishikawa techniques. We show state-of-the-art biomedical segmentation for many diverse examples of complex trees.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Isack_Efficient_Optimization_for_CVPR_2017_paper.pdf",
        "aff": "University of Western Ontario; University of Western Ontario; University of Pennsylvania; University of Iowa; University of Western Ontario",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.10530v1",
        "pdf_size": 2027575,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1296496816872712529&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Isack_Efficient_Optimization_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University of Western Ontario;University of Pennsylvania;University of Iowa",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uwo.ca;https://www.upenn.edu;https://www.uiowa.edu",
        "aff_unique_abbr": "UWO;UPenn;UIowa",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Efficient Solvers for Minimal Problems by Syzygy-Based Reduction",
        "session": "3D Vision 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "270",
        "author_site": "Viktor Larsson, Kalle \u00c3\u0085str\u00c3\u00b6m, Magnus Oskarsson",
        "author": "Viktor Larsson; Kalle Astrom; Magnus Oskarsson",
        "abstract": "In this paper we study the problem of automatically generating polynomial solvers for minimal problems. The main contribution is a new method for finding small elimination templates by making use of the syzygies (i.e. the polynomial relations) that exist between the original equations. Using these syzygies we can essentially parameterize the set of possible elimination templates. We evaluate our method on a wide variety of problems from geometric computer vision and show improvement compared to both handcrafted and automatically generated solvers. Furthermore we apply our method on two previously unsolved relative orientation problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Larsson_Efficient_Solvers_for_CVPR_2017_paper.pdf",
        "aff": "Centre for Mathematical Sciences, Lund University; Centre for Mathematical Sciences, Lund University; Centre for Mathematical Sciences, Lund University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 684840,
        "gs_citation": 163,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4341655908107815811&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "maths.lth.se;maths.lth.se;maths.lth.se",
        "email": "maths.lth.se;maths.lth.se;maths.lth.se",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Larsson_Efficient_Solvers_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Lund University",
        "aff_unique_dep": "Centre for Mathematical Sciences",
        "aff_unique_url": "https://www.lunduniversity.lu.se",
        "aff_unique_abbr": "LU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lund",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "title": "Elastic Shape-From-Template With Spatially Sparse Deforming Forces",
        "session": "3D Vision 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "1248",
        "author_site": "Abed Malti, C\u00c3\u00a9dric Herzet",
        "author": "Abed Malti; Cedric Herzet",
        "abstract": "Current Elastic SfT (Shape from Template) methods are based on l2-norm minimization. None can accurately recover the spatial location of the acting forces since l2-norm based  minimization tends to find the best tradeoff among noisy data to fit an elastic model. In this work, we study shapes that are deformed with spatially sparse set of forces. We propose two formulations for a new class of SfT problems dubbed here SLE-SfT (Sparse Linear Elastic-SfT). The First ideal formulation uses an l0-norm to minimize the cardinal of non-zero components of the deforming forces. The second relaxed formulation uses an l1-norm to minimize the sum of absolute values of force components. These new formulations do not use Solid Boundary Constraints (SBC) which are usually needed to rigidly position the shape in the frame of the deformed image. We introduce the Projective Elastic Space Property (PESP) that jointly encodes the reprojection constraint and the elastic model. We prove that filling this property is necessary and sufficient for the relaxed formulation to: (i) retrieve the ground-truth 3D deformed shape, (ii) recover the right spatial domain of non-zero deforming forces. (iii) It also proves that we can rigidly place the deformed shape in the image frame without using SBC. Finally, we prove that when filling PESP, resolving the relaxed formulation provides the same ground-truth solution as the ideal formulation. Results with simulated and real data show substantial improvements in recovering the deformed shapes as well as the spatial location of the deforming forces.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Malti_Elastic_Shape-From-Template_With_CVPR_2017_paper.pdf",
        "aff": "AutoMed/LAT, Universit\u00e9 de Tlemcen, Tlemcen, Algeria; INRIA-Rennes, Campus de Beaulieu, Rennes, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5286403,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13760935090732148632&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Malti_Elastic_Shape-From-Template_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universit\u00e9 de Tlemcen;INRIA",
        "aff_unique_dep": "AutoMed/LAT;",
        "aff_unique_url": ";https://www.inria.fr",
        "aff_unique_abbr": ";INRIA",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Tlemcen;Rennes",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Algeria;France"
    },
    {
        "title": "Emotion Recognition in Context",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "605",
        "author_site": "Ronak Kosti, Jose M. Alvarez, Adria Recasens, Agata Lapedriza",
        "author": "Ronak Kosti; Jose M. Alvarez; Adria Recasens; Agata Lapedriza",
        "abstract": "Understanding what a person is experiencing from her frame of reference is essential in our everyday life. For this reason, one can think that machines with this type of ability would interact better with people. However, there are no current systems capable of understanding in detail people's emotional states. Previous research on computer vision to recognize emotions has mainly focused on analyzing the facial expression, usually classifying it into the 6 basic emotions [11]. However, the context plays an important role in emotion perception, and when the context is incorporated, we can infer more emotional states. In this paper we present the Emotions in Context Database (EMCO), a dataset of images containing people in context in non-controlled environments. In these images, people are annotated with 26 emotional categories and also with the continuous dimensions valence, arousal, and dominance [21]. With the EMCO dataset, we trained a Convolutional Neural Network model that jointly analyzes the person and the whole scene to recognize rich information about emotional states. With this, we show the importance of considering the context for recognizing people's emotions in images, and provide a benchmark in the task of emotion recognition in visual context.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kosti_Emotion_Recognition_in_CVPR_2017_paper.pdf",
        "aff": "Universitat Oberta de Catalunya\u2217; Data61 / CSIRO\u2020; Massachusetts Institute of Technology\u2021; Universitat Oberta de Catalunya\u2217",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kosti_Emotion_Recognition_in_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3534940,
        "gs_citation": 273,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13406388896961534149&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "uoc.edu;uoc.edu;cvc.uab.es;mit.edu",
        "email": "uoc.edu;uoc.edu;cvc.uab.es;mit.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kosti_Emotion_Recognition_in_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Universitat Oberta de Catalunya;CSIRO;Massachusetts Institute of Technology",
        "aff_unique_dep": ";Data61;",
        "aff_unique_url": "https://www.uoc.edu;https://www.csiro.au;https://web.mit.edu",
        "aff_unique_abbr": "UOC;CSIRO;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "Spain;Australia;United States"
    },
    {
        "title": "End-To-End 3D Face Reconstruction With Deep Neural Networks",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2554",
        "author_site": "Pengfei Dou, Shishir K. Shah, Ioannis A. Kakadiaris",
        "author": "Pengfei Dou; Shishir K. Shah; Ioannis A. Kakadiaris",
        "abstract": "Monocular 3D facial shape reconstruction from a single 2D facial image has been an active research area due to its wide applications. Inspired by the success of deep neural networks (DNN), we propose a DNN-based approach for End-to-End 3D FAce Reconstruction (UH-E2FAR) from a single 2D image. Different from recent works that reconstruct and refine the 3D face in an iterative manner using both an RGB image and an initial 3D facial shape rendering, our DNN model is end-to-end, and thus the complicated 3D rendering process can be avoided. Moreover, we integrate in the DNN architecture two components, namely a multi-task loss function and a fusion convolutional neural network (CNN) to improve facial expression reconstruction. With the multi-task loss function, 3D face reconstruction is divided into neutral 3D facial shape reconstruction and expressive 3D facial shape reconstruction. The neutral 3D facial shape is class-specific. Therefore, higher layer features are useful. In comparison, the expressive 3D facial shape favors lower or intermediate layer features. With the fusion-CNN, features from different intermediate layers are fused and transformed for predicting the 3D expressive facial shape. Through extensive experiments, we demonstrate the superiority of our end-to-end framework in improving the accuracy of 3D face reconstruction.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dou_End-To-End_3D_Face_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 311,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16000442432889324762&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dou_End-To-End_3D_Face_CVPR_2017_paper.html"
    },
    {
        "title": "End-To-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "1165",
        "author_site": "Youngjae Yu, Hyungjin Ko, Jongwook Choi, Gunhee Kim",
        "author": "Youngjae Yu; Hyungjin Ko; Jongwook Choi; Gunhee Kim",
        "abstract": "We propose a high-level concept word detector that can be integrated with any video-to-language models. It takes a video as input and generates a list of concept words as useful semantic priors for language generation models. The proposed word detector has two important properties. First, it does not require any external knowledge sources for training. Second, the proposed word detector is trainable in an end-to-end manner jointly with any video-to-language models. To effectively exploit the detected words, we also develop a semantic attention mechanism that selectively focuses on the detected concept words and fuse them with the word encoding and decoding in the language model. In order to demonstrate that the proposed approach indeed improves the performance of multiple video-to-language tasks, we participate in all the four tasks of LSMDC 2016. Our approach has won three of them, including fill-in-the-blank, multiple-choice test, and movie retrieval.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_End-To-End_Concept_Word_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1610.02947v3",
        "gs_citation": 273,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17062229652188144239&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_End-To-End_Concept_Word_CVPR_2017_paper.html"
    },
    {
        "title": "End-To-End Instance Segmentation With Recurrent Attention",
        "session": "Low- & Mid-Level Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "3051",
        "author_site": "Mengye Ren, Richard S. Zemel",
        "author": "Mengye Ren; Richard S. Zemel",
        "abstract": "While convolutional neural networks have gained impressive success recently in solving structured prediction problems such as semantic segmentation, it remains a challenge to differentiate individual object instances in the scene. Instance segmentation is very important in a variety of applications, such as autonomous driving, image captioning, and visual question answering. Techniques that combine large graphical models with low-level vision have been proposed to address this problem; however,  we propose an end-to-end recurrent neural network (RNN) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations. The network is jointly trained to sequentially produce regions of interest as well as a dominant object segmentation within each region. The proposed model achieves competitive results on the CVPPP, KITTI, and Cityscapes datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper.pdf",
        "aff": "University of Toronto1 + Canadian Institute for Advanced Research2; University of Toronto1 + Canadian Institute for Advanced Research2",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2291464,
        "gs_citation": 434,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1266212002293381120&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ren_End-To-End_Instance_Segmentation_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Toronto;Canadian Institute for Advanced Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utoronto.ca;https://www.cifar.ca",
        "aff_unique_abbr": "U of T;CIFAR",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "End-To-End Learning of Driving Models From Large-Scale Video Datasets",
        "session": "Applications",
        "status": "Oral",
        "track": "main",
        "pid": "788",
        "author_site": "Huazhe Xu, Yang Gao, Fisher Yu, Trevor Darrell",
        "author": "Huazhe Xu; Yang Gao; Fisher Yu; Trevor Darrell",
        "abstract": "Robust perception-action models should be learned from training data with diverse visual appearances and realistic behaviors, yet current approaches to deep visuomotor policy learning have been generally limited to in-situ models learned from a single vehicle or simulation environment. We advocate learning a generic vehicle motion model from large scale crowd-sourced video data, and develop an end-to-end trainable architecture for learning to predict a distribution over future vehicle egomotion from instantaneous monocular camera observations and previous vehicle state. Our model incorporates a novel FCN-LSTM architecture, which can be learned from large-scale crowd-sourced vehicle action data, and leverages available scene segmentation side tasks to improve performance under a privileged learning paradigm. We provide a novel large-scale dataset of crowd-sourced driving behavior suitable for training our model, and report results predicting the driver action on held out sequences across diverse conditions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_End-To-End_Learning_of_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.01079v2",
        "pdf_size": 1807807,
        "gs_citation": 1126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7247109571287718894&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xu_End-To-End_Learning_of_CVPR_2017_paper.html"
    },
    {
        "title": "End-To-End Representation Learning for Correlation Filter Based Tracking",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1027",
        "author_site": "Jack Valmadre, Luca Bertinetto, Jo\u00c3\u00a3o Henriques, Andrea Vedaldi, Philip H. S. Torr",
        "author": "Jack Valmadre; Luca Bertinetto; Joao Henriques; Andrea Vedaldi; Philip H. S. Torr",
        "abstract": "The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Valmadre_End-To-End_Representation_Learning_CVPR_2017_paper.pdf",
        "aff": "University of Oxford; University of Oxford; University of Oxford; University of Oxford; University of Oxford",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Valmadre_End-To-End_Representation_Learning_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.06036v1",
        "pdf_size": 543364,
        "gs_citation": 1892,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13208202265319570145&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": "eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk",
        "email": "eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Valmadre_End-To-End_Representation_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "End-To-End Training of Hybrid CNN-CRF Models for Stereo",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "853",
        "author_site": "Patrick Kn\u00c3\u00b6belreiter, Christian Reinbacher, Alexander Shekhovtsov, Thomas Pock",
        "author": "Patrick Knobelreiter; Christian Reinbacher; Alexander Shekhovtsov; Thomas Pock",
        "abstract": "We propose a novel and principled hybrid CNN+CRF model for stereo estimation. Our model allows to exploit the advantages of both, convolutional neural networks (CNNs) and conditional random fields (CRFs) in an unified approach. The CNNs compute expressive features for matching and distinctive color edges, which in turn are used to compute the unary and binary costs of the CRF. For inference, we apply a recently proposed highly parallel dual block descent algorithm which only needs a small fixed number of iterations to compute a high-quality approximate minimizer. As the main contribution of the paper, we propose a theoretically sound method based on the structured output support vector machine (SSVM) to train the hybrid CNN+CRF model on large-scale data end-to-end. Our trained models perform very well despite the fact that we are using shallow CNNs and do not apply any kind of post-processing to the final output of the CRF. We evaluate our combined models on challenging stereo benchmarks such as Middlebury 2014 and Kitti 2015 and also investigate the performance of each individual component.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Knobelreiter_End-To-End_Training_of_CVPR_2017_paper.pdf",
        "aff": "Institute for Computer Graphics and Vision, Graz University of Technology + Center for Vision, Automation & Control, AIT Austrian Institute of Technology; Institute for Computer Graphics and Vision, Graz University of Technology; Institute for Computer Graphics and Vision, Graz University of Technology; Institute for Computer Graphics and Vision, Graz University of Technology + Center for Vision, Automation & Control, AIT Austrian Institute of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Knobelreiter_End-To-End_Training_of_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.10229v2",
        "pdf_size": 1503304,
        "gs_citation": 170,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5448244363648080051&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "email": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Knobelreiter_End-To-End_Training_of_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;0+1",
        "aff_unique_norm": "Graz University of Technology;AIT Austrian Institute of Technology",
        "aff_unique_dep": "Institute for Computer Graphics and Vision;Center for Vision, Automation & Control",
        "aff_unique_url": "https://www.tugraz.at;https://www.ait.ac.at",
        "aff_unique_abbr": "TU Graz;AIT",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Graz;",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "Austria"
    },
    {
        "title": "Enhancing Video Summarization via Vision-Language Embedding",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "2488",
        "author_site": "Bryan A. Plummer, Matthew Brown, Svetlana Lazebnik",
        "author": "Bryan A. Plummer; Matthew Brown; Svetlana Lazebnik",
        "abstract": "This paper addresses video summarization, or the problem of distilling a raw video into a shorter form while still capturing the original story. We show that visual representations supervised by freeform language make a good fit for this application by extending a recent submodular summarization approach with representativeness and interestingness objectives computed on features from a joint vision-language embedding space. We perform an evaluation on two diverse datasets, UT Egocentric and TV Episodes, and show that our new objectives give improved summarization ability compared to standard visual features alone. Our experiments also show that the vision-language embedding need not be trained on domain specific data, but can be learned from standard still image vision-language datasets and transferred to video. A further benefit of our model is the ability to guide a summary using freeform text input at test time, allowing user customization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Plummer_Enhancing_Video_Summarization_CVPR_2017_paper.pdf",
        "aff": "University of Illinois at Urbana Champaign\u2021; Google Research\u2020; University of Illinois at Urbana Champaign\u2021",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1202822,
        "gs_citation": 135,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11245308116761949749&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "illinois.edu;google.com;illinois.edu",
        "email": "illinois.edu;google.com;illinois.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Plummer_Enhancing_Video_Summarization_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://illinois.edu;https://research.google",
        "aff_unique_abbr": "UIUC;Google Research",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Urbana-Champaign;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Episodic CAMN: Contextual Attention-Based Memory Networks With Iterative Feedback for Scene Labeling",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2351",
        "author_site": "Abrar H. Abdulnabi, Bing Shuai, Stefan Winkler, Gang Wang",
        "author": "Abrar H. Abdulnabi; Bing Shuai; Stefan Winkler; Gang Wang",
        "abstract": "Scene labeling can be seen as a sequence-sequence prediction task (pixels-labels), and it is quite important to leverage relevant context to enhance the performance of pixel classification. In this paper, we introduce an episodic attention-based memory network to achieve the goal. We present a unified framework that mainly consists of a Convolutional Neural Network (CNN), specifically, Fully Convolutional Network (FCN) and an attention-based memory module with feedback connections to perform context selection and refinement. The full model produces context-aware representation for each target patch by aggregating the activated context and its original local representation produced by the convolution layers. We evaluate our model on PASCAL Context, SIFT Flow and PASCAL VOC 2011 datasets and achieve competitive results to other state-of-the-art methods in scene labeling.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Abdulnabi_Episodic_CAMN_Contextual_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Abdulnabi_Episodic_CAMN_Contextual_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 852099,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2746550560619564807&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Abdulnabi_Episodic_CAMN_Contextual_CVPR_2017_paper.html"
    },
    {
        "title": "Event-Based Visual Inertial Odometry",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "2274",
        "author_site": "Alex Zihao Zhu, Nikolay Atanasov, Kostas Daniilidis",
        "author": "Alex Zihao Zhu; Nikolay Atanasov; Kostas Daniilidis",
        "abstract": "Event-based cameras provide a new visual sensing model by detecting changes in image intensity asynchronously across all pixels on the camera. By providing these events at extremely high rates (up to 1MHz), they allow for sensing in both high speed and high dynamic range situations where traditional cameras may fail. In this paper, we present the first algorithm to fuse a purely event-based tracking algorithm with an inertial measurement unit, to provide accurate metric tracking of a camera's full 6dof pose. Our algorithm is asynchronous, and provides measurement updates at a rate proportional to the camera velocity. The algorithm selects features in the image plane, and tracks spatiotemporal windows around these features within the event stream. An Extended Kalman Filter with a structureless measurement model then fuses the feature tracks with the output of the IMU. The camera poses from the filter are then used to initialize the next step of the tracker and reject failed tracks. We show that our method successfully tracks camera motion on the Event-Camera Dataset in a number of challenging situations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Event-Based_Visual_Inertial_CVPR_2017_paper.pdf",
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhu_Event-Based_Visual_Inertial_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 4502012,
        "gs_citation": 294,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9065111085018944774&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhu_Event-Based_Visual_Inertial_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Exclusivity-Consistency Regularized Multi-View Subspace Clustering",
        "session": "Machine Learning 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "305",
        "author_site": "Xiaobo Wang, Xiaojie Guo, Zhen Lei, Changqing Zhang, Stan Z. Li",
        "author": "Xiaobo Wang; Xiaojie Guo; Zhen Lei; Changqing Zhang; Stan Z. Li",
        "abstract": "Multi-view subspace clustering aims to partition a set of multi-source data into their underlying groups. To boost the performance of multi-view clustering, numerous subspace learning algorithms have been developed in recent years, but with rare exploitation of the representation complementarity between different views as well as the indicator consistency among the representations, let alone considering them simultaneously. In this paper, we propose a novel multi-view subspace clustering model that attempts to harness the complementary information between different representations by introducing a novel position-aware exclusivity term. Meanwhile, a consistency term is employed to make these complementary representations to further have a common indicator. We formulate the above concerns into a unified optimization framework. Experimental results on several benchmark datasets are conducted to reveal the effectiveness of our algorithm over other state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Exclusivity-Consistency_Regularized_Multi-View_CVPR_2017_paper.pdf",
        "aff": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences+University of Chinese Academy of Sciences; University of Chinese Academy of Sciences+State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences; Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences+University of Chinese Academy of Sciences; School of Computer Science and Technology, Tianjin University; Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences+University of Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1236776,
        "gs_citation": 324,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16914625764101412811&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "nlpr.ia.ac.cn;gmail.com;nlpr.ia.ac.cn;tju.edu.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;gmail.com;nlpr.ia.ac.cn;tju.edu.cn;nlpr.ia.ac.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Exclusivity-Consistency_Regularized_Multi-View_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1+0;0+1;2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Tianjin University",
        "aff_unique_dep": "Institute of Automation;;School of Computer Science and Technology",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;http://www.tju.edu.cn",
        "aff_unique_abbr": "CAS;UCAS;Tianjin University",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Expecting the Unexpected: Training Detectors for Unusual Pedestrians With Adversarial Imposters",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "809",
        "author_site": "Shiyu Huang, Deva Ramanan",
        "author": "Shiyu Huang; Deva Ramanan",
        "abstract": "As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance.  Pedestrian detection is a highly researched topic with mature methods,  but most datasets (for both training and evaluation) focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios that are rarely observed, such as children playing in the street and people using bicycles/skateboards in unexpected ways. Such \"in-the-tail\" data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a  dedicated collection effort, it is relatively small by contemporary standards (~ 1000 images). To explore large-scale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right \"priors\" or parameters for synthesis: we would like realistic data with realistic poses and object configurations. Inspired by Generative Adversarial Networks, we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset (that fools the classifier), which we deem Synthetic Imposters. We demonstrate that this pipeline allows one to generate realistic (or adverserial) training data by making use of rendering/animation engines. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Synthetic Imposters can also be used for \"in-the-tail\" validation at test-time, a notoriously difficult challenge for real-world deployment.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Expecting_the_Unexpected_CVPR_2017_paper.pdf",
        "aff": "Tsinghua University, Beijing, China; Carnegie Mellon University, Pittsburgh, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.06283v2",
        "pdf_size": 2773165,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4382595650272497707&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "mails.tsinghua.edu.cn;cs.cmu.edu",
        "email": "mails.tsinghua.edu.cn;cs.cmu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Expecting_the_Unexpected_CVPR_2017_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Tsinghua University;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.cmu.edu",
        "aff_unique_abbr": "THU;CMU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Beijing;Pittsburgh",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Expert Gate: Lifelong Learning With a Network of Experts",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1252",
        "author_site": "Rahaf Aljundi, Punarjay Chakravarty, Tinne Tuytelaars",
        "author": "Rahaf Aljundi; Punarjay Chakravarty; Tinne Tuytelaars",
        "abstract": "In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process, data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision which expert to deploy at test time.  We introduce a set of gating autoencoders that learn a representation for the task at hand, and, at test time, automatically forward the test sample to the relevant expert. This also brings memory efficiency as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert, with fine-tuning or learning-without-forgetting, can be selected. We evaluate our method on image classification and video prediction problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Aljundi_Expert_Gate_Lifelong_CVPR_2017_paper.pdf",
        "aff": "KU Leuven, ESAT-PSI, IMEC, Belgium; KU Leuven, ESAT-PSI, IMEC, Belgium; KU Leuven, ESAT-PSI, IMEC, Belgium",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Aljundi_Expert_Gate_Lifelong_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.06194v2",
        "pdf_size": 1194049,
        "gs_citation": 862,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12490134934821269979&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "esat.kuleuven.be;esat.kuleuven.be;esat.kuleuven.be",
        "email": "esat.kuleuven.be;esat.kuleuven.be;esat.kuleuven.be",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Aljundi_Expert_Gate_Lifelong_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "KU Leuven",
        "aff_unique_dep": "ESAT-PSI",
        "aff_unique_url": "https://www.kuleuven.be",
        "aff_unique_abbr": "KU Leuven",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "title": "Exploiting 2D Floorplan for Building-Scale Panorama RGBD Alignment",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "111",
        "author_site": "Erik Wijmans, Yasutaka Furukawa",
        "author": "Erik Wijmans; Yasutaka Furukawa",
        "abstract": "This paper presents a novel algorithm that utilizes a 2D floorplan to align panorama RGBD scans. While effective panorama RGBD alignment techniques exist, such a system requires extremely dense RGBD image sampling. Our approach can significantly reduce the number of necessary scans with the aid of a floorplan image. We formulate a novel Markov Random Field inference problem as a scan placement over the floorplan, as opposed to the conventional scan-to-scan alignment. The technical contributions lie in multi-modal image correspondence cues (between scans and schematic floorplan) as well as a novel coverage potential avoiding an inherent stacking bias. The proposed approach has been evaluated on five challenging large indoor spaces. To the best of our knowledge, we present the first effective system that utilizes a 2D floorplan image for building-scale 3D pointcloud alignment. The source code and the data are shared with the community to further enhance indoor mapping research.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wijmans_Exploiting_2D_Floorplan_CVPR_2017_paper.pdf",
        "aff": "Washington University in St. Louis; Washington University in St. Louis",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Wijmans_Exploiting_2D_Floorplan_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.02859v1",
        "pdf_size": 6005449,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8106057616813752293&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "wustl.edu;wustl.edu",
        "email": "wustl.edu;wustl.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wijmans_Exploiting_2D_Floorplan_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Washington University in St. Louis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wustl.edu",
        "aff_unique_abbr": "WashU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "St. Louis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Exploiting Saliency for Object Segmentation From Image Level Labels",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1802",
        "author_site": "Seong Joon Oh, Rodrigo Benenson, Anna Khoreva, Zeynep Akata, Mario Fritz, Bernt Schiele",
        "author": "Seong Joon Oh; Rodrigo Benenson; Anna Khoreva; Zeynep Akata; Mario Fritz; Bernt Schiele",
        "abstract": "There have been remarkable improvements in the semantic labelling task in the recent years. However, the state of the art methods rely on large-scale pixel-level annotations. This paper studies the problem of training a pixel-wise semantic labeller network from image-level annotations of the present object classes. Recently, it has been shown that high quality seeds indicating discriminative object regions can be obtained from image-level labels. Without additional information, obtaining the full extent of the object is an inherently ill-posed problem due to co-occurrences. We propose using a saliency model as additional information and hereby exploit prior knowledge on the object extent and image statistics. We show how to combine both information sources in order to recover 80% of the fully supervised performance - which is the new state of the art in weakly supervised training for pixel-wise semantic labelling.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Oh_Exploiting_Saliency_for_CVPR_2017_paper.pdf",
        "aff": "Max Planck Institute for Informatics; Max Planck Institute for Informatics; Max Planck Institute for Informatics; Max Planck Institute for Informatics+Amsterdam Machine Learning Lab; Max Planck Institute for Informatics; Max Planck Institute for Informatics",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Oh_Exploiting_Saliency_for_2017_CVPR_supplemental.pdf",
        "arxiv": "1701.08261",
        "pdf_size": 927960,
        "gs_citation": 236,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8670208145726951355&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;uva.nl;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "email": "mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;uva.nl;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Oh_Exploiting_Saliency_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0+1;0;0",
        "aff_unique_norm": "Max Planck Institute for Informatics;Amsterdam Machine Learning Lab",
        "aff_unique_dep": ";Machine Learning",
        "aff_unique_url": "https://mpi-inf.mpg.de;https://amlab.nl",
        "aff_unique_abbr": "MPII;AMLab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1;0;0",
        "aff_country_unique": "Germany;Netherlands"
    },
    {
        "title": "Exploiting Symmetry and/or Manhattan Properties for 3D Object Structure Estimation From Single and Multiple Images",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3708",
        "author_site": "Yuan Gao, Alan L. Yuille",
        "author": "Yuan Gao; Alan L. Yuille",
        "abstract": "Many man-made objects have intrinsic symmetries and Manhattan structure. By assuming an orthographic projection model, this paper addresses the estimation of 3D structures and camera projection using symmetry and/or Manhattan structure cues, which occur when the input is single- or multiple-image from the same category, e.g., multiple different cars. Specifically, analysis on the single image case implies that Manhattan alone is sufficient to recover the camera projection, and then the 3D structure can be reconstructed uniquely exploiting symmetry. However, Manhattan structure can be difficult to observe from a single image due to occlusion. To this end, we extend to the multiple-image case which can also exploit symmetry but does not require Manhattan axes. We propose a novel rigid structure from motion method, exploiting symmetry and using multiple images from the same category as input. Experimental results on the Pascal3D+ dataset show that our method significantly outperforms baseline methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gao_Exploiting_Symmetry_andor_CVPR_2017_paper.pdf",
        "aff": "Tencent AI Lab, Shenzhen, China; Johns Hopkins University, Baltimore, MD + UCLA, Los Angeles, CA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Gao_Exploiting_Symmetry_andor_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 829678,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17183790748820586695&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tencent.com;jhu.edu",
        "email": "tencent.com;jhu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gao_Exploiting_Symmetry_andor_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Tencent;Johns Hopkins University;University of California, Los Angeles",
        "aff_unique_dep": "AI Lab;;",
        "aff_unique_url": "https://ai.tencent.com;https://www.jhu.edu;https://www.ucla.edu",
        "aff_unique_abbr": "Tencent AI Lab;JHU;UCLA",
        "aff_campus_unique_index": "0;1+2",
        "aff_campus_unique": "Shenzhen;Baltimore;Los Angeles",
        "aff_country_unique_index": "0;1+1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "FASON: First and Second Order Information Fusion Network for Texture Recognition",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3609",
        "author_site": "Xiyang Dai, Joe Yue-Hei Ng, Larry S. Davis",
        "author": "Xiyang Dai; Joe Yue-Hei Ng; Larry S. Davis",
        "abstract": "Deep networks have shown impressive performance on many computer vision tasks. Recently, deep convolutional neural networks (CNNs) have been used to learn discriminative texture representations. One of the most successful approaches is Bilinear CNN model that explicitly captures the second order statistics within deep features. However, these networks cut off the first order information flow in the deep network and make gradient back-propagation difficult. We propose an effective fusion architecture - FASON that combines second order information flow and first order information flow. Our method allows gradients to back-propagate through both flows freely and can be trained effectively. We then build a multi-level deep architecture to exploit the first and second order information within different convolutional layers. Experiments show that our method achieves improvements over state-of-the-art methods on several benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dai_FASON_First_and_CVPR_2017_paper.pdf",
        "aff": "Institution for Advanced Computer Studies, University of Maryland, College Park; Institution for Advanced Computer Studies, University of Maryland, College Park; Institution for Advanced Computer Studies, University of Maryland, College Park",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1206217,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12147770175124216332&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "email": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dai_FASON_First_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Institution for Advanced Computer Studies",
        "aff_unique_url": "https://www.umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "FC4: Fully Convolutional Color Constancy With Confidence-Weighted Pooling",
        "session": "Low- & Mid-Level Vision",
        "status": "Oral",
        "track": "main",
        "pid": "1641",
        "author_site": "Yuanming Hu, Baoyuan Wang, Stephen Lin",
        "author": "Yuanming Hu; Baoyuan Wang; Stephen Lin",
        "abstract": "Improvements in color constancy have arisen from the use of convolutional neural networks (CNNs). However, the patch-based CNNs that exist for this problem are faced with the issue of estimation ambiguity, where a patch may contain insufficient information to establish a unique or even a limited possible range of illumination colors. Image patches with estimation ambiguity not only appear with great frequency in photographs, but also significantly degrade the quality of network training and inference. To overcome this problem, we present a fully convolutional network architecture in which patches throughout an image can carry different confidence weights according to the value they provide for color constancy estimation. These confidence weights are learned and applied within a novel pooling layer where the local estimates are merged into a global solution. With this formulation, the network is able to determine \"what to learn\" and \"how to pool\" automatically from color constancy datasets without additional supervision. The proposed network also allows for end-to-end training, and achieves higher efficiency and accuracy. On standard benchmarks, our network outperforms the previous state-of-the-art while achieving 120x greater efficiency.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hu_FC4_Fully_Convolutional_CVPR_2017_paper.pdf",
        "aff": "Tsinghua University; Microsoft Research; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Hu_FC4_Fully_Convolutional_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1245292,
        "gs_citation": 335,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7092535673756570553&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;microsoft.com;microsoft.com",
        "email": "gmail.com;microsoft.com;microsoft.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hu_FC4_Fully_Convolutional_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Tsinghua University;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "THU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "FCSS: Fully Convolutional Self-Similarity for Dense Semantic Correspondence",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2965",
        "author_site": "Seungryong Kim, Dongbo Min, Bumsub Ham, Sangryul Jeon, Stephen Lin, Kwanghoon Sohn",
        "author": "Seungryong Kim; Dongbo Min; Bumsub Ham; Sangryul Jeon; Stephen Lin; Kwanghoon Sohn",
        "abstract": "We present a descriptor, called fully convolutional self-similarity (FCSS), for dense semantic correspondence. To robustly match points among different instances within the same object class, we formulate FCSS using local self-similarity (LSS) within a fully convolutional network. In contrast to existing CNN-based descriptors, FCSS is inherently insensitive to intra-class appearance variations because of its LSS-based structure, while maintaining the precise localization ability of deep neural networks. The sampling patterns of local structure and the self-similarity measure are jointly learned within the proposed network in an end-to-end and multi-scale manner. As training data for semantic correspondence is rather limited, we propose to leverage object candidate priors provided in existing image datasets and also correspondence consistency between object pairs to enable weakly-supervised learning. Experiments demonstrate that FCSS outperforms conventional handcrafted descriptors and CNN-based descriptors on various benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kim_FCSS_Fully_Convolutional_CVPR_2017_paper.pdf",
        "aff": "Yonsei University; Chungnam National University; Yonsei University; Yonsei University; Microsoft Research; Yonsei University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.00926v1",
        "pdf_size": 2031414,
        "gs_citation": 176,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16126479292751184923&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": "yonsei.ac.kr;cnu.ac.kr;yonsei.ac.kr;yonsei.ac.kr;microsoft.com;yonsei.ac.kr",
        "email": "yonsei.ac.kr;cnu.ac.kr;yonsei.ac.kr;yonsei.ac.kr;microsoft.com;yonsei.ac.kr",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kim_FCSS_Fully_Convolutional_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0;2;0",
        "aff_unique_norm": "Yonsei University;Chungnam National University;Microsoft",
        "aff_unique_dep": ";;Microsoft Research",
        "aff_unique_url": "https://www.yonsei.ac.kr;http://www.cnu.ac.kr;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Yonsei;CNU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "title": "FFTLasso: Large-Scale LASSO in the Fourier Domain",
        "session": "Machine Learning 3",
        "status": "Oral",
        "track": "main",
        "pid": "630",
        "author_site": "Adel Bibi, Hani Itani, Bernard Ghanem",
        "author": "Adel Bibi; Hani Itani; Bernard Ghanem",
        "abstract": "In this paper, we revisit the LASSO sparse representation problem, which has been studied and used in a variety of different areas, ranging from signal processing and information theory to computer vision and machine learning. In the vision community, it found its way into many important applications, including face recognition, tracking, super resolution, image denoising, to name a few. Despite advances in efficient sparse algorithms, solving large-scale LASSO problems remains a challenge. To circumvent this difficulty, people tend to downsample and subsample the problem (e.g. via dimensionality reduction) to maintain a manageable sized LASSO, which usually comes at the cost of losing solution accuracy. This paper proposes a novel circulant reformulation of the LASSO that lifts the problem to a higher dimension, where ADMM can be efficiently applied to its dual form. Because of this lifting, all optimization variables are updated using only basic element-wise operations, the most computationally expensive of which is a 1D FFT. In this way, there is no need for a linear system solver nor matrix-vector multiplication. Since all operations in our FFTLasso method are element-wise, the subproblems are completely independent and can be trivially parallelized (e.g. on a GPU). The attractive computational properties of FFTLasso are verified by extensive experiments on synthetic and real data and on the face recognition task. They demonstrate that FFTLasso scales much more effectively than a state-of-the-art solver.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bibi_FFTLasso_Large-Scale_LASSO_CVPR_2017_paper.pdf",
        "aff": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Bibi_FFTLasso_Large-Scale_LASSO_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1373469,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3512363283899825232&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "kaust.edu.sa;mail.aub.edu;kaust.edu.sa",
        "email": "kaust.edu.sa;mail.aub.edu;kaust.edu.sa",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bibi_FFTLasso_Large-Scale_LASSO_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaust.edu.sa",
        "aff_unique_abbr": "KAUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Saudi Arabia"
    },
    {
        "title": "Face Normals \"In-The-Wild\" Using Fully Convolutional Networks",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "24",
        "author_site": "George Trigeorgis, Patrick Snape, Iasonas Kokkinos, Stefanos Zafeiriou",
        "author": "George Trigeorgis; Patrick Snape; Iasonas Kokkinos; Stefanos Zafeiriou",
        "abstract": "In this work we pursue a data-driven approach to the problem of estimating surface normals from a single intensity image, focusing in particular on human faces. We introduce new methods to exploit the currently available facial databases for dataset construction and tailor a deep convolutional neural network to the task of estimating facial surface normals `in-the-wild'. We train a fully convolutional network that can accurately recover facial normals from images including a challenging variety of expressions and facial poses. We compare against state-of-the-art face Shape-from-Shading and 3D reconstruction techniques and show that the proposed network can recover substantially more accurate and realistic normals. Furthermore, in contrast to other existing face-specific surface recovery methods, we do not require the solving of an explicit alignment step due to the fully convolutional nature of our network.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Trigeorgis_Face_Normals_In-The-Wild_CVPR_2017_paper.pdf",
        "aff": "Imperial College London; Imperial College London; University College London; Imperial College London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1527538,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15646793553365866147&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;cs.ucl.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;cs.ucl.ac.uk;imperial.ac.uk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Trigeorgis_Face_Normals_In-The-Wild_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Imperial College London;University College London",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.ucl.ac.uk",
        "aff_unique_abbr": "ICL;UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Factorized Variational Autoencoders for Modeling Audience Reactions to Movies",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "940",
        "author_site": "Zhiwei Deng, Rajitha Navarathna, Peter Carr, Stephan Mandt, Yisong Yue, Iain Matthews, Greg Mori",
        "author": "Zhiwei Deng; Rajitha Navarathna; Peter Carr; Stephan Mandt; Yisong Yue; Iain Matthews; Greg Mori",
        "abstract": "Matrix and tensor factorization methods are often used for finding underlying low-dimensional patterns from noisy data. In this paper, we study non-linear tensor factoriza- tion methods based on deep variational autoencoders. Our approach is well-suited for settings where the relationship between the latent representation to be learned and the raw data representation is highly complex. We apply our ap- proach to a large dataset of facial expressions of movie- watching audiences (over 16 million faces). Our experi- ments show that compared to conventional linear factoriza- tion methods, our method achieves better reconstruction of the data, and further discovers interpretable latent factors.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Deng_Factorized_Variational_Autoencoders_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3279683,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9175893300900075552&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Deng_Factorized_Variational_Autoencoders_CVPR_2017_paper.html"
    },
    {
        "title": "Fast 3D Reconstruction of Faces With Glasses",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2989",
        "author_site": "Fabio Maninchedda, Martin R. Oswald, Marc Pollefeys",
        "author": "Fabio Maninchedda; Martin R. Oswald; Marc Pollefeys",
        "abstract": "We present a method for the fast 3D face reconstruction of people wearing glasses. Our method explicitly and robustly models the case in which a face to be reconstructed is partially occluded by glasses. We propose a simple and generic model for glasses that copes with a wide variety of different shapes, colors and styles, without the need for any database or learning. Our algorithm is simple, fast and requires only small amounts of both memory and runtime resources, allowing for a fast interactive 3D reconstruction on commodity mobile phones. The thorough evaluation of our approach on synthetic and real data demonstrates superior reconstruction results due to the explicit modeling of glasses.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Maninchedda_Fast_3D_Reconstruction_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2387784,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=948192402784223970&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Maninchedda_Fast_3D_Reconstruction_CVPR_2017_paper.html"
    },
    {
        "title": "Fast Boosting Based Detection Using Scale Invariant Multimodal Multiresolution Filtered Features",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3054",
        "author_site": "Arthur Daniel Costea, Robert Varga, Sergiu Nedevschi",
        "author": "Arthur Daniel Costea; Robert Varga; Sergiu Nedevschi",
        "abstract": "In this paper we propose a novel boosting-based sliding window solution for object detection which can keep up with the precision of the state-of-the art deep learning approaches, while being 10 to 100 times faster. The solution takes advantage of multisensorial perception and exploits information from color, motion and depth. We introduce multimodal multiresolution filtering of signal intensity, gradient magnitude and orientation channels, in order to capture structure at multiple scales and orientations. To achieve scale invariant classification features, we analyze the effect of scale change on features for different filter types and propose a correction scheme. To improve recognition we incorporate 2D and 3D context by generating spatial, geometric and symmetrical channels. Finally, we evaluate the proposed solution on multiple benchmarks for the detection of pedestrians, cars and bicyclists. We achieve competitive results at over 25 frames per second.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Costea_Fast_Boosting_Based_CVPR_2017_paper.pdf",
        "aff": "Image Processing and Pattern Recognition Research Center, Technical University of Cluj-Napoca, Romania; Image Processing and Pattern Recognition Research Center, Technical University of Cluj-Napoca, Romania; Image Processing and Pattern Recognition Research Center, Technical University of Cluj-Napoca, Romania",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1010015,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9984026891990306046&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "cs.utcluj.ro;cs.utcluj.ro;cs.utcluj.ro",
        "email": "cs.utcluj.ro;cs.utcluj.ro;cs.utcluj.ro",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Costea_Fast_Boosting_Based_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technical University of Cluj-Napoca",
        "aff_unique_dep": "Image Processing and Pattern Recognition Research Center",
        "aff_unique_url": "https://www.utcluj.ro",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Romania"
    },
    {
        "title": "Fast Fourier Color Constancy",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "287",
        "author_site": "Jonathan T. Barron, Yun-Ta Tsai",
        "author": "Jonathan T. Barron; Yun-Ta Tsai",
        "abstract": "We present Fast Fourier Color Constancy (FFCC), a color constancy algorithm which solves illuminant estimation by reducing it to a spatial localization task on a torus. By operating in the frequency domain, FFCC produces lower error rates than the previous state-of-the-art by 13-20% while being 250-3000 times faster. This unconventional approach introduces challenges regarding aliasing, directional statistics, and preconditioning, which we address. By producing a complete posterior distribution over illuminants instead of a single illuminant estimate, FFCC enables better training techniques, an effective temporal smoothing technique, and richer methods for error analysis. Our implementation of FFCC runs at  700 frames per second on a mobile device, allowing it to be used as an accurate, real-time, temporally-coherent automatic white balance algorithm.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Barron_Fast_Fourier_Color_CVPR_2017_paper.pdf",
        "aff": "Google; Google",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Barron_Fast_Fourier_Color_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.07596",
        "pdf_size": 747219,
        "gs_citation": 244,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5448854653320631324&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "google.com;google.com",
        "email": "google.com;google.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Barron_Fast_Fourier_Color_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fast Haze Removal for Nighttime Image Using Maximum Reflectance Prior",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3729",
        "author_site": "Jing Zhang, Yang Cao, Shuai Fang, Yu Kang, Chang Wen Chen",
        "author": "Jing Zhang; Yang Cao; Shuai Fang; Yu Kang; Chang Wen Chen",
        "abstract": "In this paper, we address a haze removal problem from a single nighttime image, even in the presence of varicolored and non-uniform illumination. The core idea lies in a novel maximum reflectance prior. We first introduce the nighttime hazy imaging model, which includes a local ambient illumination item in both direct attenuation term and scattering term. Then, we propose a simple but effective image prior, maximum reflectance prior, to estimate the varying ambient illumination. The maximum reflectance prior is based on a key observation: for most daytime haze-free image patches, each color channel has very high intensity at some pixels. For the nighttime haze image, the local maximum intensities at each color channel are mainly contributed by the ambient illumination. Therefore, we can directly estimate the ambient illumination and transmission map, and consequently restore a high quality haze-free image. Experimental results on various nighttime hazy images demonstrate the effectiveness of the proposed approach. In particular, our approach has the advantage of computational efficiency, which is 10-100 times faster than state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Fast_Haze_Removal_CVPR_2017_paper.pdf",
        "aff": "University of Science and Technology of China; University of Science and Technology of China; Hefei University of Technology; University of Science and Technology of China; State University of New York at Buffalo",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4321977,
        "gs_citation": 234,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15805281496302467538&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mail.ustc.edu.cn;ustc.edu.cn;hfut.edu.cn;ustc.edu.cn;buffalo.edu",
        "email": "mail.ustc.edu.cn;ustc.edu.cn;hfut.edu.cn;ustc.edu.cn;buffalo.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Fast_Haze_Removal_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0;2",
        "aff_unique_norm": "University of Science and Technology of China;Hefei University of Technology;State University of New York at Buffalo",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.ustc.edu.cn;http://www.hfut.edu.cn/;https://www.buffalo.edu",
        "aff_unique_abbr": "USTC;HUT;SUNY Buffalo",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Buffalo",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Fast Multi-Frame Stereo Scene Flow With Motion Segmentation",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "1556",
        "author_site": "Tatsunori Taniai, Sudipta N. Sinha, Yoichi Sato",
        "author": "Tatsunori Taniai; Sudipta N. Sinha; Yoichi Sato",
        "abstract": "We propose a new multi-frame method for efficiently computing scene flow (dense depth and optical flow) and camera ego-motion for a dynamic scene observed from a moving stereo camera rig. Our technique also segments out moving objects from the rigid scene. In our method, we first estimate the disparity map and the 6-DOF camera motion using stereo matching and visual odometry.  We then identify regions inconsistent with the estimated camera motion and compute per-pixel optical flow only at these regions. This flow proposal is fused with the camera motion-based flow proposal using fusion moves to obtain the final optical flow and motion segmentation. This unified framework benefits all four tasks -- stereo, optical flow, visual odometry and motion segmentation leading to overall higher accuracy and efficiency. Our method is currently ranked third on the KITTI 2015 scene flow benchmark. Furthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3 orders of magnitude faster than the top six methods. We also report a thorough evaluation on challenging Sintel sequences with fast camera and object motion, where our method consistently outperforms OSF [Menze2015], which is currently ranked second on the KITTI benchmark.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Taniai_Fast_Multi-Frame_Stereo_CVPR_2017_paper.pdf",
        "aff": "RIKEN AIPS; Microsoft Research; The University of Tokyo",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Taniai_Fast_Multi-Frame_Stereo_2017_CVPR_supplemental.zip",
        "arxiv": "1707.01307",
        "pdf_size": 3839232,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1755905885932628098&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "riken.jp;microsoft.com;cs.t.u-tokyo.ac.jp",
        "email": "riken.jp;microsoft.com;cs.t.u-tokyo.ac.jp",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Taniai_Fast_Multi-Frame_Stereo_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "RIKEN;Microsoft;University of Tokyo",
        "aff_unique_dep": "Advanced Institute for Computational Science;Microsoft Research;",
        "aff_unique_url": "https://aips.riken.jp;https://www.microsoft.com/en-us/research;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "RIKEN AIPS;MSR;UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "title": "Fast Person Re-Identification via Cross-Camera Semantic Binary Transformation",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1506",
        "author_site": "Jiaxin Chen, Yunhong Wang, Jie Qin, Li Liu, Ling Shao",
        "author": "Jiaxin Chen; Yunhong Wang; Jie Qin; Li Liu; Ling Shao",
        "abstract": "Numerous methods have been proposed for person re-identification, most of which however neglect the matching efficiency. Recently, several hashing based approaches have been developed to make re-identification more scalable for large-scale gallery sets. Despite their efficiency, these works ignore cross-camera variations, which severely deteriorate the final matching accuracy. To address the above issues, we propose a novel hashing based method for fast person re-identification, namely Cross-camera Semantic Binary Transformation (CSBT). CSBT aims to transform original high-dimensional feature vectors into compact identity-preserving binary codes. To this end, CSBT first employs a subspace projection to mitigate cross-camera variations, by maximizing intra-person similarities and inter-person discrepancies. Subsequently, a binary coding scheme is proposed via seamlessly incorporating both the semantic pairwise relationships and local affinity information. Finally, a joint learning framework is proposed for simultaneous subspace projection learning and binary coding based on discrete alternating optimization. Experimental results on four benchmarks clearly demonstrate the superiority of CSBT over the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Fast_Person_Re-Identification_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1023213,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11628878260074932685&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Fast_Person_Re-Identification_CVPR_2017_paper.html"
    },
    {
        "title": "Fast Video Classification via Adaptive Cascading of Deep Models",
        "session": "Machine Learning 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "1339",
        "author_site": "Haichen Shen, Seungyeop Han, Matthai Philipose, Arvind Krishnamurthy",
        "author": "Haichen Shen; Seungyeop Han; Matthai Philipose; Arvind Krishnamurthy",
        "abstract": "Recent advances have enabled \"oracle\" classifiers that can classify across many classes and input distributions with high accuracy without retraining. However, these classifiers are relatively heavyweight, so that applying them to classify video is costly. We show that day-to-day video exhibits highly skewed class distributions over the short term, and that these distributions can be classified by much simpler models. We formulate the problem of detecting the short-term skews online and exploiting models based on it as a new sequential decision making problem dubbed the Online Bandit Problem, and present a new algorithm to solve it. When applied to recognizing faces in TV shows and movies, we realize end-to-end classification speedups of 2.4-7.8x/2.6-11.2x (on GPU/CPU) relative to a state-of-the-art convolutional neural network, at competitive accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shen_Fast_Video_Classification_CVPR_2017_paper.pdf",
        "aff": "University of Washington; Rubrik, Inc.; Microsoft Research; University of Washington",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Shen_Fast_Video_Classification_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.06453v2",
        "pdf_size": 716601,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13643878686338335873&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "cs.washington.edu;rubrik.com;microsoft.com;cs.washington.edu",
        "email": "cs.washington.edu;rubrik.com;microsoft.com;cs.washington.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shen_Fast_Video_Classification_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Washington;Rubrik, Inc.;Microsoft",
        "aff_unique_dep": ";;Microsoft Research",
        "aff_unique_url": "https://www.washington.edu;https://www.rubrik.com;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UW;Rubrik;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fast-At: Fast Automatic Thumbnail Generation Using Deep Neural Networks",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1925",
        "author_site": "Seyed A. Esmaeili, Bharat Singh, Larry S. Davis",
        "author": "Seyed A. Esmaeili; Bharat Singh; Larry S. Davis",
        "abstract": "Fast-AT is an automatic thumbnail generation system based on deep neural networks. It is a  fully-convolutional deep neural network, which learns specific filters for thumbnails of different sizes and aspect ratios. During inference, the appropriate filter is selected depending on the dimensions of the target thumbnail. Unlike most previous work, Fast-AT does not utilize saliency but addresses the problem directly. In addition, it eliminates the need to conduct region search over the saliency map. The model generalizes to thumbnails of different sizes including those with extreme aspect ratios and can generate thumbnails in real time. A data set of more than 70,000 thumbnail annotations was collected to train Fast-AT. We show competitive results in comparison to existing techniques.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Esmaeili_Fast-At_Fast_Automatic_CVPR_2017_paper.pdf",
        "aff": "University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Esmaeili_Fast-At_Fast_Automatic_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1176233,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10418545735716796739&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "umd.edu;cs.umd.edu;umiacs.umd.edu",
        "email": "umd.edu;cs.umd.edu;umiacs.umd.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Esmaeili_Fast-At_Fast_Automatic_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "FastMask: Segment Multi-Scale Object Candidates in One Shot",
        "session": "Computational Photography",
        "status": "Spotlight",
        "track": "main",
        "pid": "340",
        "author_site": "Hexiang Hu, Shiyi Lan, Yuning Jiang, Zhimin Cao, Fei Sha",
        "author": "Hexiang Hu; Shiyi Lan; Yuning Jiang; Zhimin Cao; Fei Sha",
        "abstract": "Objects appear to scale differently in natural images. This fact requires methods dealing with object-centric tasks (e.g. object proposal) to have robust performance over variances in object scales. In the paper, we present a novel segment proposal framework, namely FastMask, which takes advantage of hierarchical features in deep convolutional neural networks to segment multi-scale objects in one shot. Innovatively, we adapt segment proposal network into three different functional components (body, neck and head). We further propose a weight-shared residual neck module as well as a scale-tolerant attentional head module for efficient one-shot inference. On MS COCO benchmark, the proposed FastMask outperforms all state-of-the-art segment proposal methods in average recall being 2 5 times faster. Moreover, with a slight trade-off in accuracy, FastMask can segment objects in near real time ( 13 fps) with 800*600 resolution images, demonstrating its potential in practical applications. Our implementation is available on https://github.com/voidrank/FastMask.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hu_FastMask_Segment_Multi-Scale_CVPR_2017_paper.pdf",
        "aff": "USC\u2020; Fudan University\u2020; Megvii Inc.; Megvii Inc.; USC",
        "project": "",
        "github": "https://github.com/voidrank/FastMask",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Hu_FastMask_Segment_Multi-Scale_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.08843v4",
        "pdf_size": 1884672,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11913965569425348008&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "usc.edu;fudan.edu.cn;megvii.com;megvii.com;usc.edu",
        "email": "usc.edu;fudan.edu.cn;megvii.com;megvii.com;usc.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hu_FastMask_Segment_Multi-Scale_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;2;0",
        "aff_unique_norm": "University of Southern California;Fudan University;Megvii Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.usc.edu;https://www.fudan.edu.cn;https://www.megvii.com/",
        "aff_unique_abbr": "USC;Fudan;Megvii",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;1;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Feature Pyramid Networks for Object Detection",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "777",
        "author_site": "Tsung-Yi Lin, Piotr Doll\u00c3\u00a1r, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie",
        "author": "Tsung-Yi Lin; Piotr Dollar; Ross Girshick; Kaiming He; Bharath Hariharan; Serge Belongie",
        "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf",
        "aff": "Facebook AI Research (FAIR) + Cornell University and Cornell Tech; Facebook AI Research (FAIR); Facebook AI Research (FAIR); Facebook AI Research (FAIR); Facebook AI Research (FAIR); Cornell University and Cornell Tech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.03144v2",
        "pdf_size": 528997,
        "gs_citation": 33044,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13353537855176955572&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;0;0;1",
        "aff_unique_norm": "Meta;Cornell University",
        "aff_unique_dep": "Facebook AI Research;",
        "aff_unique_url": "https://research.facebook.com;https://www.cornell.edu",
        "aff_unique_abbr": "FAIR;Cornell",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Ithaca",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Feedback Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "474",
        "author_site": "Amir R. Zamir, Te-Lin Wu, Lin Sun, William B. Shen, Bertram E. Shi, Jitendra Malik, Silvio Savarese",
        "author": "Amir R. Zamir; Te-Lin Wu; Lin Sun; William B. Shen; Bertram E. Shi; Jitendra Malik; Silvio Savarese",
        "abstract": "urrently, the most successful learning models in computer vision are based on learning successive representations followed by a decision layer. This is usually actualized through feedforward multilayer neural networks, e.g. ConvNets, where each layer forms one of such successive representations. However, an alternative that can achieve the same goal is a feedback based approach in which the representation is formed in an iterative manner based on a feedback received from previous iteration's output.  We establish that a feedback based approach has several core advantages over feedforward: it enables making early predictions at the query time, its output naturally conforms to a hierarchical structure in the label space (e.g. a taxonomy), and it provides a new basis for Curriculum Learning. We observe that feedback develops a considerably different representation compared to feedforward counterparts, in line with the aforementioned advantages. We provide a general feedback based learning architecture, instantiated using existing RNNs, with the endpoint results on par or better than existing feedforward networks and the addition of the above advantages.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zamir_Feedback_Networks_CVPR_2017_paper.pdf",
        "aff": "Stanford University; Stanford University; Stanford University + HKUST; Stanford University; HKUST; University of California, Berkeley; Stanford University",
        "project": "http://feedbacknet.stanford.edu/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zamir_Feedback_Networks_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.09508",
        "pdf_size": 2157086,
        "gs_citation": 263,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1158905987759360744&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zamir_Feedback_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0+1;0;1;2;0",
        "aff_unique_norm": "Stanford University;Hong Kong University of Science and Technology;University of California, Berkeley",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.ust.hk;https://www.berkeley.edu",
        "aff_unique_abbr": "Stanford;HKUST;UC Berkeley",
        "aff_campus_unique_index": "0;0;0+1;0;1;2;0",
        "aff_campus_unique": "Stanford;Hong Kong SAR;Berkeley",
        "aff_country_unique_index": "0;0;0+1;0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Few-Shot Object Recognition From Machine-Labeled Web Images",
        "session": "Machine Learning 4",
        "status": "Spotlight",
        "track": "main",
        "pid": "398",
        "author_site": "Zhongwen Xu, Linchao Zhu, Yi Yang",
        "author": "Zhongwen Xu; Linchao Zhu; Yi Yang",
        "abstract": "With the tremendous advances made by Convolutional Neural Networks (ConvNets) on object recognition, we can now easily obtain adequately reliable machine-labeled annotations easily from predictions by off-the-shelf ConvNets. In this work, we present an \"abstraction memory\" based framework for few-shot learning, building upon machine-labeled image annotations. Our method takes large-scale machine-annotated dataset (e.g., OpenImages) as an external memory bank. In the external memory bank, the information is stored in the memory slots in the form of key-value, in which image feature is regarded as the key and the label embedding serves as the value. When queried by the few-shot examples, our model selects visually similar data from the external memory bank and writes the useful information obtained from related external data into another memory bank, i.e., abstraction memory. Long Short-Term Memory (LSTM) controllers and attention mechanisms are utilized to guarantee the data written to the abstraction memory correlates with the query example. The abstraction memory concentrates information from the external memory bank to make the few-shot recognition effective. In the experiments, we first confirm that our model can learn to conduct few-shot object recognition on clean human-labeled data from the ImageNet dataset. Then, we demonstrate that with our model, machine-labeled image annotations are very effective and abundant resources for performing object recognition on novel categories. Experimental results show that our proposed model with machine-labeled annotations achieves great results, with only a 1% difference in accuracy between the machine-labeled annotations and the human-labeled annotations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Few-Shot_Object_Recognition_CVPR_2017_paper.pdf",
        "aff": "CAI, University of Technology Sydney; CAI, University of Technology Sydney; CAI, University of Technology Sydney",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.06152v1",
        "pdf_size": 1029181,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10470437360223650968&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xu_Few-Shot_Object_Recognition_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Technology Sydney",
        "aff_unique_dep": "CAI",
        "aff_unique_url": "https://www.uts.edu.au",
        "aff_unique_abbr": "UTS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Filter Flow Made Practical: Massively Parallel and Lock-Free",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1311",
        "author_site": "Sathya N. Ravi, Yunyang Xiong, Lopamudra Mukherjee, Vikas Singh",
        "author": "Sathya N. Ravi; Yunyang Xiong; Lopamudra Mukherjee; Vikas Singh",
        "abstract": "This paper is inspired by a relatively recent work of Seitz and Baker which introduced the so-called Filter Flow model. Filter flow finds the transformation relating a pair of (or multiple) images by identifying a large set of local linear filters; imposing additional constraints on certain structural properties of these filters enables Filter Flow to serve as a general \"one stop\" construction for a spectrum of problems in vision: from optical flow to defocus to stereo to affine alignment. The idea is beautiful yet the benefits are not borne out in practice because of significant computational challenges. This issue makes most (if not all) deployments for practical vision problems out of reach. The key thrust of our work is to identify mathematically (near) equivalent reformulations of this model that can eliminate this serious limitation. We demonstrate via a detailed optimization-focused development that Filter Flow can indeed be solved fairly efficiently for a wide range of instantiations. We derive efficient algorithms, perform extensive theoretical analysis focused on convergence and parallelization and show how results competitive with the state of the art for many applications can be achieved with negligible application specific adjustments or post-processing. The actual numerical scheme is easy to understand and, implement (30 lines in Matlab) -- this development will enable Filter Flow to be a viable general solver and testbed for numerous applications in the community, going forward.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ravi_Filter_Flow_Made_CVPR_2017_paper.pdf",
        "aff": "University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Whitewater; University of Wisconsin-Madison",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1441983,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4101689659696595462&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "wisc.edu;wisc.edu;uww.edu;biostat.wisc.edu",
        "email": "wisc.edu;wisc.edu;uww.edu;biostat.wisc.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ravi_Filter_Flow_Made_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Wisconsin-Madison;University of Wisconsin-Whitewater",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.wisc.edu;https://www.uww.edu",
        "aff_unique_abbr": "UW-Madison;UW-Whitewater",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Madison;Whitewater",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Finding Tiny Faces",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "314",
        "author_site": "Peiyun Hu, Deva Ramanan",
        "author": "Peiyun Hu; Deva Ramanan",
        "abstract": "Though tremendous strides have been made in object recognition, one of the remaining open challenges is detecting small objects. We explore three aspects of the problem in the context of finding small faces: the role of scale invariance, image resolution, and contextual reasoning. While most recognition approaches aim to be scale-invariant, the cues for recognizing a 3px tall face are fundamentally different than those for recognizing a 300px tall face. We take a different approach and train separate detectors for different scales. To maintain efficiency, detectors are trained in a multi-task fashion: they make use of features extracted from multiple layers of single (deep) feature hierarchy. While training detectors for large objects is straightforward, the crucial challenge remains training detectors for small objects. We show that context is crucial, and define templates that make use of massively-large receptive fields (where 99% of the template extends beyond the object of interest). Finally, we explore the role of scale in pre-trained deep networks, providing ways to extrapolate networks tuned for limited scales to rather extreme ranges. We demonstrate state-of-the-art results on massively-benchmarked face datasets (FDDB and WIDER FACE). In particular, when compared to prior art on WIDER FACE, our results reduce error by a factor of 2 (our models produce an AP of 82% while prior art ranges from 29-64%).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hu_Finding_Tiny_Faces_CVPR_2017_paper.pdf",
        "aff": "Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Hu_Finding_Tiny_Faces_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.04402v2",
        "pdf_size": 2327315,
        "gs_citation": 1025,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5935623751055355825&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hu_Finding_Tiny_Faces_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fine-Grained Image Classification via Combining Vision and Language",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2611",
        "author_site": "Xiangteng He, Yuxin Peng",
        "author": "Xiangteng He; Yuxin Peng",
        "abstract": "Fine-grained image classification is a challenging task due to the large intra-class variance and small inter-class variance, aiming at recognizing hundreds of sub-categories belonging to the same basic-level category. Most existing fine-grained image classification methods generally learn part detection models to obtain the semantic parts for better classification accuracy. Despite achieving promising results, these methods mainly have two limitations: (1) not all the parts which obtained through the part detection models are beneficial and indispensable for classification, and (2) fine-grained image classification requires more detailed visual descriptions which could not be provided by the part locations or attribute annotations. For addressing the above two limitations, this paper proposes the two-stream model combing vision and language (CVL) for learning latent semantic representations. The vision stream learns deep representations from the original visual information via deep convolutional neural network. The language stream utilizes the natural language descriptions which could point out the discriminative parts or characteristics for each image, and provides a flexible and compact way of encoding the salient visual aspects for distinguishing sub-categories. Since the two streams are complementary, combing the two streams can further achieves better classification accuracy. Comparing with 12 state-of-the-art methods on the widely used CUB-200-2011 dataset for fine-grained image classification, the experimental results demonstrate our CVL approach achieves the best performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/He_Fine-Grained_Image_Classification_CVPR_2017_paper.pdf",
        "aff": "Institute of Computer Science and Technology, Peking University; Institute of Computer Science and Technology, Peking University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2165188,
        "gs_citation": 217,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11886078604027373664&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/He_Fine-Grained_Image_Classification_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "Institute of Computer Science and Technology",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Fine-Grained Recognition as HSnet Search for Informative Image Parts",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Oral",
        "track": "main",
        "pid": "912",
        "author_site": "Michael Lam, Behrooz Mahasseni, Sinisa Todorovic",
        "author": "Michael Lam; Behrooz Mahasseni; Sinisa Todorovic",
        "abstract": "This work addresses fine-grained image classification. Our work is based on the hypothesis that when dealing with subtle differences among object classes it is critical to identify and only account for a few informative image parts, as the remaining image context may not only be uninformative but may also hurt recognition. This motivates us to formulate our problem as a sequential search for informative parts over a deep feature map produced by a deep Convolutional Neural Network (CNN). A state of this search is a set of proposal bounding boxes in the image, whose \"informativeness\" is evaluated by the heuristic function (H), and used for generating new candidate states by the successor function (S). The two functions are unified via a Long Short-Term Memory network (LSTM) into a new deep recurrent architecture, called HSnet. Thus, HSnet (i) generates proposals of informative image parts and (ii) fuses all proposals toward final fine-grained recognition. We specify both supervised and weakly supervised training of HSnet depending on the availability of object part annotations. Evaluation on the benchmark Caltech-UCSD Birds 200-2011 and Cars-196 datasets demonstrate our competitive performance relative to the state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lam_Fine-Grained_Recognition_as_CVPR_2017_paper.pdf",
        "aff": "Oregon State University; Oregon State University; Oregon State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 767521,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17113057675622670030&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "oregonstate.edu;gmail.com;oregonstate.edu",
        "email": "oregonstate.edu;gmail.com;oregonstate.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lam_Fine-Grained_Recognition_as_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fine-Grained Recognition of Thousands of Object Categories With Single-Example Training",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1665",
        "author_site": "Leonid Karlinsky, Joseph Shtok, Yochay Tzur, Asaf Tzadok",
        "author": "Leonid Karlinsky; Joseph Shtok; Yochay Tzur; Asaf Tzadok",
        "abstract": "We approach the problem of fast detection and recognition of a large number (thousands) of object categories while training on a very limited amount of examples, usually one per category. Examples of this task include: (i) detection of retail products, where we have only one studio image of each product available for training; (ii) detection of brand logos; and (iii) detection of 3D objects and their respective poses within a static 2D image, where only a sparse subset of (partial) object views is available for training, with a single example for each view. Building a detector based on so few examples presents a significant challenge for the current top-performing (deep) learning based techniques, which require large amounts of data to train. Our approach for this task is based on a non-parametric probabilistic model for initial detection, CNN-based refinement and temporal integration where applicable. We successfully demonstrate its usefulness in a variety of experiments on both existing and our own benchmarks achieving state-of-the-art performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Karlinsky_Fine-Grained_Recognition_of_CVPR_2017_paper.pdf",
        "aff": "IBM Research; IBM Research; IBM Research; IBM Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Karlinsky_Fine-Grained_Recognition_of_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1888171,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=115481976626839420&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "il.ibm.com;il.ibm.com;il.ibm.com;il.ibm.com",
        "email": "il.ibm.com;il.ibm.com;il.ibm.com;il.ibm.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Karlinsky_Fine-Grained_Recognition_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fine-To-Coarse Global Registration of RGB-D Scans",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "647",
        "author_site": "Maciej Halber, Thomas Funkhouser",
        "author": "Maciej Halber; Thomas Funkhouser",
        "abstract": "RGB-D scanning of indoor environments is important for many applications, including real estate, interior design, and virtual reality. However, it is still challenging to register RGB-D images from a hand-held camera over a long video sequence into a globally consistent 3D model. Current methods often can lose tracking or drift and thus fail to reconstruct salient structures in large environments (e.g., parallel walls in different rooms). To address this problem, we propose a \"fine-to-coarse\" global registration algorithm that leverages robust registrations at finer scales to seed detection and enforcement of new correspondence and structural constraints at coarser scales. To test global registration algorithms, we provide a benchmark with 10,401 manually-clicked point correspondences in 25 scenes from the SUN3D dataset. During experiments with this benchmark, we find that our fine-to-coarse algorithm registers long RGB-D sequences better than previous methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Halber_Fine-To-Coarse_Global_Registration_CVPR_2017_paper.pdf",
        "aff": "Princeton University; Princeton University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2721408,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5835964606746046977&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.princeton.edu;cs.princeton.edu",
        "email": "cs.princeton.edu;cs.princeton.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Halber_Fine-To-Coarse_Global_Registration_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fine-Tuning Convolutional Neural Networks for Biomedical Image Analysis: Actively and Incrementally",
        "session": "Biomedical Image/Video Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "3589",
        "author_site": "Zongwei Zhou, Jae Shin, Lei Zhang, Suryakanth Gurudu, Michael Gotway, Jianming Liang",
        "author": "Zongwei Zhou; Jae Shin; Lei Zhang; Suryakanth Gurudu; Michael Gotway; Jianming Liang",
        "abstract": "Intense interest in applying convolutional neural networks (CNNs) in biomedical image analysis is wide spread, but its success is impeded by the lack of large annotated datasets in biomedical imaging. Annotating biomedical images is not only tedious and time consuming, but also demanding of costly, specialty - oriented knowledge and skills, which are not easily accessible. To dramatically reduce annotation cost, this paper presents a novel method called AIFT (active, incremental fine-tuning) to naturally integrate active learning and transfer learning into a single framework. AIFT starts directly with a pre-trained CNN to seek \"worthy\" samples from the unannotated for annotation, and the (fine-tuned) CNN is further fine-tuned continuously by incorporating newly annotated samples in each iteration to enhance the CNN's performance incrementally. We have evaluated our method in three different biomedical imaging applications, demonstrating that the cost of annotation can be cut by at least half. This performance is attributed to the several advantages derived from the advanced active and incremental capability of our AIFT method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Fine-Tuning_Convolutional_Neural_CVPR_2017_paper.pdf",
        "aff": "Arizona State University; Arizona State University; Arizona State University; Mayo Clinic; Mayo Clinic; Arizona State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4061186,
        "gs_citation": 516,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16815684028886503198&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "asu.edu;asu.edu;asu.edu;mayo.edu;mayo.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu;mayo.edu;mayo.edu;asu.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Fine-Tuning_Convolutional_Neural_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;1;0",
        "aff_unique_norm": "Arizona State University;Mayo Clinic",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.asu.edu;https://www.mayoclinic.org",
        "aff_unique_abbr": "ASU;Mayo Clinic",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fixed-Point Factorized Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1607",
        "author_site": "Peisong Wang, Jian Cheng",
        "author": "Peisong Wang; Jian Cheng",
        "abstract": "In recent years, Deep Neural Networks (DNN) based methods have achieved remarkable performance in a wide range of tasks and have been among the most powerful and widely used techniques in computer vision. However, DNN-based methods are both computational-intensive and resource-consuming, which hinders the application of these methods on embedded systems like smart phones. To alleviate this problem, we introduce a novel Fixed-point Factorized Networks (FFN) for pretrained models to reduce the computational complexity as well as the storage requirement of networks. The resulting networks have only weights of -1, 0 and 1, which significantly eliminates the most resource-consuming multiply-accumulate operations (MACs). Extensive experiments on large-scale ImageNet classification task show the proposed FFN only requires one-thousandth of multiply operations with comparable accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Fixed-Point_Factorized_Networks_CVPR_2017_paper.pdf",
        "aff": "Institute of Automation, Chinese Academy of Sciences+University of Chinese Academy of Sciences; Institute of Automation, Chinese Academy of Sciences+University of Chinese Academy of Sciences+Center for Excellence in Brain Science and Intelligence Technology, CAS",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.01972v2",
        "pdf_size": 531479,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11834480434059950711&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Fixed-Point_Factorized_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1+0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Flexible Spatio-Temporal Networks for Video Prediction",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "2945",
        "author_site": "Chaochao Lu, Michael Hirsch, Bernhard Sch\u00c3\u00b6lkopf",
        "author": "Chaochao Lu; Michael Hirsch; Bernhard Scholkopf",
        "abstract": "We describe a modular framework for video frame prediction. We refer to it as a Flexible Spatio-Temporal Network (FSTN) as it allows the extrapolation of a video sequence as well as the estimation of synthetic frames lying in between observed frames and thus the generation of slow-motion videos. By devising a customized objective function comprising decoding, encoding, and adversarial losses, we are able to mitigate the common problem of blurry predictions, managing to retain high frequency information even for relatively distant future predictions. We propose and analyse different training strategies to optimize our model. Extensive experiments on several challenging public datasets demonstrate both the versatility and validity of our model.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Flexible_Spatio-Temporal_Networks_CVPR_2017_paper.pdf",
        "aff": "University of Cambridge; Max Planck Institute for Intelligent Systems; Max Planck Institute for Intelligent Systems",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4413627,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8260444226677990814&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "tue.mpg.de;tue.mpg.de;tue.mpg.de",
        "email": "tue.mpg.de;tue.mpg.de;tue.mpg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lu_Flexible_Spatio-Temporal_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Cambridge;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";Intelligent Systems",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "Cambridge;MPI-IS",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "title": "Flight Dynamics-Based Recovery of a UAV Trajectory Using Ground Cameras",
        "session": "3D Vision 2",
        "status": "Oral",
        "track": "main",
        "pid": "2630",
        "author_site": "Artem Rozantsev, Sudipta N. Sinha, Debadeepta Dey, Pascal Fua",
        "author": "Artem Rozantsev; Sudipta N. Sinha; Debadeepta Dey; Pascal Fua",
        "abstract": "We propose a new method to estimate the 6-dof trajectory of a flying object such as a quadrotor UAV within a 3D airspace monitored using multiple fixed ground cameras. It is based on a new structure from motion formulation for the 3D reconstruction of a single moving point with known motion dynamics. Our main contribution is a new bundle adjustment procedure, which in addition to optimizing the camera poses, regularizes the point trajectory using a prior based on motion dynamics (or specifically flight dynamics). Furthermore, we can infer the underlying control input sent to the UAV's autopilot that determined its flight trajectory.  Our method requires neither perfect single-view tracking nor appearance matching across views. For robustness, we allow the tracker to generate multiple detections per frame in each video. The true detections and the data association across videos is estimated using robust multi-view triangulation and subsequently refined in our bundle adjustment formulation. Quantitative evaluation on simulated data and experiments on real videos from indoor and outdoor scenes shows that our technique is superior to existing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rozantsev_Flight_Dynamics-Based_Recovery_CVPR_2017_paper.pdf",
        "aff": "Computer Vision Laboratory, EPFL; Microsoft Research; Microsoft Research; Computer Vision Laboratory, EPFL",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Rozantsev_Flight_Dynamics-Based_Recovery_2017_CVPR_supplemental.zip",
        "arxiv": "1612.00192",
        "pdf_size": 2140510,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9286865033369404831&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "epfl.ch;microsoft.com;microsoft.com;epfl.ch",
        "email": "epfl.ch;microsoft.com;microsoft.com;epfl.ch",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Rozantsev_Flight_Dynamics-Based_Recovery_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "EPFL;Microsoft",
        "aff_unique_dep": "Computer Vision Laboratory;Microsoft Research",
        "aff_unique_url": "https://cvl.epfl.ch;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "EPFL;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "FlowNet 2.0: Evolution of Optical Flow Estimation With Deep Networks",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "900",
        "author_site": "Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox",
        "author": "Eddy Ilg; Nikolaus Mayer; Tonmoy Saikia; Margret Keuper; Alexey Dosovitskiy; Thomas Brox",
        "abstract": "The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ilg_FlowNet_2.0_Evolution_CVPR_2017_paper.pdf",
        "aff": "University of Freiburg, Germany; University of Freiburg, Germany; University of Freiburg, Germany; University of Freiburg, Germany; University of Freiburg, Germany; University of Freiburg, Germany",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ilg_FlowNet_2.0_Evolution_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4599839,
        "gs_citation": 4066,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10327949235117821487&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de",
        "email": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de;cs.uni-freiburg.de",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ilg_FlowNet_2.0_Evolution_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Freiburg",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-freiburg.de",
        "aff_unique_abbr": "UoF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Forecasting Human Dynamics From Static Images",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "174",
        "author_site": "Yu-Wei Chao, Jimei Yang, Brian Price, Scott Cohen, Jia Deng",
        "author": "Yu-Wei Chao; Jimei Yang; Brian Price; Scott Cohen; Jia Deng",
        "abstract": "This paper presents the first study on forecasting human dynamics from static images. The problem is to input a single RGB image and generate a sequence of upcoming human body poses in 3D. To address the problem, we propose the 3D Pose Forecasting Network (3D-PFNet). Our 3D-PFNet integrates recent advances on single-image human pose estimation and sequence prediction, and converts the 2D predictions into 3D space. We train our 3D-PFNet using a three-step training strategy to leverage a diverse source of training data, including image and video based human pose datasets and 3D motion capture (MoCap) data. We demonstrate competitive performance of our 3D-PFNet on 2D pose forecasting and 3D structure recovery through quantitative and qualitative results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chao_Forecasting_Human_Dynamics_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Chao_Forecasting_Human_Dynamics_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.03432v1",
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9377917773183337759&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chao_Forecasting_Human_Dynamics_CVPR_2017_paper.html"
    },
    {
        "title": "Forecasting Interactive Dynamics of Pedestrians With Fictitious Play",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "249",
        "author_site": "Wei-Chiu Ma, De-An Huang, Namhoon Lee, Kris M. Kitani",
        "author": "Wei-Chiu Ma; De-An Huang; Namhoon Lee; Kris M. Kitani",
        "abstract": "We develop predictive models of pedestrian dynamics by encoding the coupled nature of multi-pedestrian interaction using game theory and deep learning-based visual analysis to estimate person-specific behavior parameters. We focus on predictive models since they are important for developing interactive autonomous systems (e.g., autonomous cars, home robots, smart homes) that can understand different human behavior and pre-emptively respond to future human actions. Building predictive models for multi-pedestrian interactions however, is very challenging due to two reasons: (1) the dynamics of interaction are complex interdependent processes, where the decision of one person can affect others; and (2) dynamics are variable, where each person may behave differently (e.g., an older person may walk slowly while the younger person may walk faster). To address these challenges, we utilize concepts from game theory to model the intertwined decision making process of multiple pedestrians and use visual classifiers to learn a mapping from pedestrian appearance to behavior parameters. We evaluate our proposed model on several public multiple pedestrian interaction video datasets. Results show that our strategic planning model predicts and explains human interactions 25% better when compared to a state-of-the-art activity forecasting method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ma_Forecasting_Interactive_Dynamics_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1604.01431v3",
        "pdf_size": 1282226,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6047927347357851783&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ma_Forecasting_Interactive_Dynamics_CVPR_2017_paper.html"
    },
    {
        "title": "Fractal Dimension Invariant Filtering and Its CNN-Based Implementation",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1298",
        "author_site": "Hongteng Xu, Junchi Yan, Nils Persson, Weiyao Lin, Hongyuan Zha",
        "author": "Hongteng Xu; Junchi Yan; Nils Persson; Weiyao Lin; Hongyuan Zha",
        "abstract": "Fractal analysis has been widely used in computer vision, especially in texture image processing and texture analysis.  The key concept of fractal-based image model is the fractal dimension, which is invariant to bi-Lipschitz transformation of image, and thus capable of representing intrinsic structural information of image robustly.  However, the invariance of fractal dimension generally does not hold after filtering, which limits the application of fractal-based image model.  In this paper, we propose a novel fractal dimension invariant filtering (FDIF) method, extending the invariance of fractal dimension to filtering operations.  Utilizing the notion of local self-similarity, we first develop a local fractal model for images. By adding a nonlinear post-processing step behind anisotropic filter banks, we demonstrate that the proposed filtering method is capable of preserving the local invariance of the fractal dimension of image. Meanwhile, we show that the FDIF method can be re-instantiated approximately via a CNN-based architecture, where the convolution layer extracts anisotropic structure of image and the nonlinear layer enhances the structure via preserving local fractal dimension of image.  The proposed filtering method provides us with a novel geometric interpretation of CNN-based image model.  Focusing on a challenging image processing task --- detecting complicated curves from the texture-like images, the proposed method obtains superior results to the state-of-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Fractal_Dimension_Invariant_CVPR_2017_paper.pdf",
        "aff": "School of ECE, CSE, Chemical & Biomolecular Engineering, Georgia Tech; IBM Research \u2013 China; School of ECE, CSE, Chemical & Biomolecular Engineering, Georgia Tech; Department of EE, Shanghai Jiao Tong University; School of ECE, CSE, Chemical & Biomolecular Engineering, Georgia Tech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1603.06036v3",
        "pdf_size": 1872198,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16854772091587137134&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gatech.edu;cn.ibm.com;gatech.edu;sjtu.edu.cn;cc.gatech.edu",
        "email": "gatech.edu;cn.ibm.com;gatech.edu;sjtu.edu.cn;cc.gatech.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xu_Fractal_Dimension_Invariant_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "Georgia Institute of Technology;IBM;Shanghai Jiao Tong University",
        "aff_unique_dep": "School of Electrical and Computer Engineering, School of Computer Science, School of Chemical & Biomolecular Engineering;China;Department of EE",
        "aff_unique_url": "https://www.gatech.edu;https://www.ibm.com/research/global/china;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "Georgia Tech;IBM;SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Fried Binary Embedding for High-Dimensional Visual Features",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1001",
        "author_site": "Weixiang Hong, Junsong Yuan, Sreyasee Das Bhattacharjee",
        "author": "Weixiang Hong; Junsong Yuan; Sreyasee Das Bhattacharjee",
        "abstract": "Most existing binary embedding methods prefer compact binary codes (b-dimensional) to avoid high computational and memory cost of projecting high-dimensional visual features (d-dimensional, b < d). We argue that long binary codes (b   O(d)) are critical to fully utilize the discriminative power of high-dimensional visual features, and can achieve better results in various tasks such as approximate nearest neighbour search. Generating long binary codes involves large projection matrix and high-dimensional matrix-vector multiplication, thus is memory and compute intensive. To tackle these problems, we propose Fried Binary Embedding (FBE) to decompose the projection matrix using adaptive Fastfood transform, which is the multiplication of several structured matrices. As a result, FBE can reduce the computational complexity from O(d2) to O(dlogd), and memory cost from O(d2) to O(d), respectively. More importantly, by using the structured matrices, FBE can regulate the projection matrix against over-fitting and lead to even better accuracy than using unconstrained projection matrix (like ITQ [4]) with the same long code length. Experimental comparisons with state-of-the-art methods over various visual applications demonstrate both the efficiency and performance advantages of the FBE.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hong_Fried_Binary_Embedding_CVPR_2017_paper.pdf",
        "aff": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 571217,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6947842263774732442&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;gmail.com",
        "email": "ntu.edu.sg;ntu.edu.sg;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hong_Fried_Binary_Embedding_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "From Local to Global: Edge Profiles to Camera Motion in Blurred Images",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1818",
        "author_site": "Subeesh Vasu, A. N. Rajagopalan",
        "author": "Subeesh Vasu; A. N. Rajagopalan",
        "abstract": "In this work, we investigate the relation between the edge profiles present in a motion blurred image and the underlying camera motion responsible for causing the motion blur. While related works on camera motion estimation (CME) rely on the strong assumption of space-invariant blur, we handle the challenging case of general camera motion. We first show how edge profiles `alone' can be harnessed to perform direct CME from a single observation. While it is routine for conventional methods to jointly estimate the latent image too through alternating minimization, our above scheme is best-suited when such a pursuit is either impractical or inefficacious. For applications that actually favor an alternating minimization strategy, the edge profiles can serve as a valuable cue. We incorporate a suitably derived constraint from edge profiles into an existing blind deblurring framework and demonstrate improved restoration performance. Experiments reveal that this approach yields state-of-the-art results for the blind deblurring problem.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Vasu_From_Local_to_CVPR_2017_paper.pdf",
        "aff": "Indian Institute of Technology Madras; Indian Institute of Technology Madras",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Vasu_From_Local_to_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1617264,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2138571253685095034&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;ee.iitm.ac.in",
        "email": "gmail.com;ee.iitm.ac.in",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Vasu_From_Local_to_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Madras",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitm.ac.in",
        "aff_unique_abbr": "IIT Madras",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madras",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "From Motion Blur to Motion Flow: A Deep Learning Solution for Removing Heterogeneous Motion Blur",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "846",
        "author_site": "Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian Reid, Chunhua Shen, Anton van den Hengel, Qinfeng Shi",
        "author": "Dong Gong; Jie Yang; Lingqiao Liu; Yanning Zhang; Ian Reid; Chunhua Shen; Anton van den Hengel; Qinfeng Shi",
        "abstract": "Removing pixel-wise heterogeneous motion blur is challenging due to the ill-posed nature of the problem. The predominant solution is to estimate the blur kernel by adding a prior, but extensive literature on the subject indicates the difficulty in identifying a prior which is suitably informative, and general. Rather than imposing a prior based on theory, we propose instead to learn one from the data. Learning a prior over the latent image would require modeling all possible image content. The critical observation underpinning our approach, however, is that learning the motion flow instead allows the model to focus on the cause of the blur, irrespective of the image content. This is a much easier learning task, but it also avoids the iterative process through which latent image priors are typically applied. Our approach directly estimates the motion flow from the blurred image through a fully-convolutional deep neural network (FCN) and recovers the unblurred image from the estimated motion flow. Our FCN is the first universal end-to-end mapping from the blurred image to the dense motion flow. To train the FCN, we simulate motion flows to generate synthetic blurred-image-motion-flow pairs thus avoiding the need for human labeling. Extensive experiments on challenging realistic blurred images demonstrate that the proposed method outperforms the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gong_From_Motion_Blur_CVPR_2017_paper.pdf",
        "aff": ";;;;;;;",
        "project": "",
        "github": "https://donggong1.github.io/blur2mflow",
        "supp": "",
        "arxiv": "1612.02583v1",
        "pdf_size": 1948543,
        "gs_citation": 504,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4620661169044128023&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gong_From_Motion_Blur_CVPR_2017_paper.html"
    },
    {
        "title": "From Red Wine to Red Tomato: Composition With Context",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Oral",
        "track": "main",
        "pid": "665",
        "author_site": "Ishan Misra, Abhinav Gupta, Martial Hebert",
        "author": "Ishan Misra; Abhinav Gupta; Martial Hebert",
        "abstract": "Compositionality and contextuality are key building blocks of intelligence. They allow us to compose known concepts to generate new and complex ones. However, traditional learning methods do not model both these properties and require copious amounts of labeled data to learn new concepts. A large fraction of existing techniques, e.g., using late fusion, compose concepts but fail to model contextuality. For example, red in red wine is different from red in red tomatoes. In this paper, we present a simple method that respects contextuality in order to compose classifiers of known visual concepts. Our method builds upon the intuition that classifiers lie in a smooth space where compositional transforms can be modeled. We show how it can generalize to unseen combinations of concepts. Our results on composing attributes, objects as well as composing subject, predicate, and objects demonstrate its strong generalization performance compared to baselines. Finally, we present detailed analysis of our method and highlight its properties.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Misra_From_Red_Wine_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 297,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6959320578989247472&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Misra_From_Red_Wine_CVPR_2017_paper.html"
    },
    {
        "title": "From Zero-Shot Learning to Conventional Supervised Classification: Unseen Visual Data Synthesis",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "594",
        "author_site": "Yang Long, Li Liu, Ling Shao, Fumin Shen, Guiguang Ding, Jungong Han",
        "author": "Yang Long; Li Liu; Ling Shao; Fumin Shen; Guiguang Ding; Jungong Han",
        "abstract": "Robust object recognition systems usually rely on powerful feature extraction mechanisms from a large number of real images. However, in many realistic applications, collecting sufficient images for ever-growing new classes is unattainable. In this paper, we propose a new Zero-shot learning (ZSL) framework that can synthesise visual features for unseen classes without acquiring real images. Using the proposed Unseen Visual Data Synthesis (UVDS) algorithm, semantic attributes are effectively utilised as an intermediate clue to synthesise unseen visual features at the training stage. Hereafter, ZSL recognition is converted into the conventional supervised problem, i.e. the synthesised visual features can be straightforwardly fed to typical classifiers such as SVM. On four benchmark datasets, we demonstrate the benefit of using synthesised unseen data. Extensive experimental results manifest that our proposed approach significantly improve the state-of-the-art results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Long_From_Zero-Shot_Learning_CVPR_2017_paper.pdf",
        "aff": "Department of Electronic and Electrical Engineering, University of Sheffield, UK; School of Computing Science, University of East Anglia, UK; School of Computing Science, University of East Anglia, UK; Center for Future Media, University of Electronic Science and Technology of China, China; School of Software, Tsinghua University, China; Department of Computer Science and Digital Technologies, Northumbria University, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.01782v1",
        "pdf_size": 4755907,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14865160090709677242&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "sheffield.ac.uk;uea.ac.uk;uea.ac.uk;gmail.com;tsinghua.edu.cn;northumbria.ac.uk",
        "email": "sheffield.ac.uk;uea.ac.uk;uea.ac.uk;gmail.com;tsinghua.edu.cn;northumbria.ac.uk",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Long_From_Zero-Shot_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;2;3;4",
        "aff_unique_norm": "University of Sheffield;University of East Anglia;University of Electronic Science and Technology of China;Tsinghua University;Northumbria University",
        "aff_unique_dep": "Department of Electronic and Electrical Engineering;School of Computing Science;Center for Future Media;School of Software;Department of Computer Science and Digital Technologies",
        "aff_unique_url": "https://www.sheffield.ac.uk;https://www.uea.ac.uk;http://www.uestc.edu.cn;https://www.tsinghua.edu.cn;https://www.northumbria.ac.uk",
        "aff_unique_abbr": "Sheffield;UEA;UESTC;THU;Northumbria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;1;0",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "title": "Full Resolution Image Compression With Recurrent Neural Networks",
        "session": "Machine Learning 4",
        "status": "Oral",
        "track": "main",
        "pid": "2230",
        "author_site": "George Toderici, Damien Vincent, Nick Johnston, Sung Jin Hwang, David Minnen, Joel Shor, Michele Covell",
        "author": "George Toderici; Damien Vincent; Nick Johnston; Sung Jin Hwang; David Minnen; Joel Shor; Michele Covell",
        "abstract": "This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study \"one-shot\" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3%-8.8% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Toderici_Full_Resolution_Image_CVPR_2017_paper.pdf",
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Toderici_Full_Resolution_Image_2017_CVPR_supplemental.pdf",
        "arxiv": "1608.05148v2",
        "pdf_size": 644362,
        "gs_citation": 1111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12135576749020741024&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Toderici_Full_Resolution_Image_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Oral",
        "track": "main",
        "pid": "1691",
        "author_site": "Tobias Pohlen, Alexander Hermans, Markus Mathias, Bastian Leibe",
        "author": "Tobias Pohlen; Alexander Hermans; Markus Mathias; Bastian Leibe",
        "abstract": "Semantic image segmentation is an essential component of modern autonomous driving systems, as an accurate understanding of the surrounding scene is crucial to navigation and action planning. Current state-of-the-art approaches in semantic image segmentation rely on pre-trained networks that were initially developed for classifying images as a whole. While these networks exhibit outstanding recognition performance (i.e., what is visible?), they lack localization accuracy (i.e., where precisely is something located?). Therefore, additional processing steps have to be performed in order to obtain pixel-accurate segmentation masks at the full image resolution. To alleviate this problem we propose a novel ResNet-like architecture that exhibits strong localization and recognition performance. We combine multi-scale context with pixel-level accuracy by using two processing streams within our network: One stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The other stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals. Without additional processing steps and without pre-training, our approach achieves an intersection-over-union score of 71.8% on the Cityscapes dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Pohlen_Full-Resolution_Residual_Networks_CVPR_2017_paper.pdf",
        "aff": "Visual Computing Institute; Visual Computing Institute; Visual Computing Institute; Visual Computing Institute",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.08323v2",
        "pdf_size": 1002233,
        "gs_citation": 741,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3052363802725327044&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de",
        "email": "rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Pohlen_Full-Resolution_Residual_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Visual Computing Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Fully Convolutional Instance-Aware Semantic Segmentation",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "864",
        "author_site": "Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei",
        "author": "Yi Li; Haozhi Qi; Jifeng Dai; Xiangyang Ji; Yichen Wei",
        "abstract": "We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The network architecture is highly integrated and efficient. It achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. Code would be released.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Fully_Convolutional_Instance-Aware_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.07709v2",
        "gs_citation": 1418,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8993319314849501947&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Fully_Convolutional_Instance-Aware_CVPR_2017_paper.html"
    },
    {
        "title": "Fully-Adaptive Feature Sharing in Multi-Task Networks With Applications in Person Attribute Classification",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Spotlight",
        "track": "main",
        "pid": "2243",
        "author_site": "Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, Rogerio Feris",
        "author": "Yongxi Lu; Abhishek Kumar; Shuangfei Zhai; Yu Cheng; Tara Javidi; Rogerio Feris",
        "abstract": "Multi-task learning aims to improve generalization performance of multiple prediction tasks by appropriately sharing relevant information across them. In the context of deep neural networks, this idea is often realized by hand-designed network architectures with layers that are shared across tasks and branches that encode task-specific features. However, the space of possible multi-task deep architectures is combinatorially large and often the final architecture is arrived at by manual exploration of this space, which can be both error-prone and tedious. We propose an automatic approach for designing compact multi-task deep learning architectures. Our approach starts with a thin multi-layer network and dynamically widens it in a greedy manner during training. By doing so iteratively, it creates a tree-like deep architecture, on which similar tasks reside in the same branch until at the top layers. Evaluation on person attributes classification tasks involving facial and clothing attributes suggests that the models produced by the proposed method are fast, compact and can closely match or exceed the state-of-the-art accuracy from strong baselines by much more expensive models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Fully-Adaptive_Feature_Sharing_CVPR_2017_paper.pdf",
        "aff": "UC San Diego; IBM Research; Binghamton Univeristy, SUNY; IBM Research; UC San Diego; IBM Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05377v1",
        "pdf_size": 1736226,
        "gs_citation": 509,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=183343417795270167&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ucsd.edu;us.ibm.com;binghamton.edu;us.ibm.com;eng.ucsd.edu;us.ibm.com",
        "email": "ucsd.edu;us.ibm.com;binghamton.edu;us.ibm.com;eng.ucsd.edu;us.ibm.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lu_Fully-Adaptive_Feature_Sharing_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;1;0;1",
        "aff_unique_norm": "University of California, San Diego;IBM;Binghamton University",
        "aff_unique_dep": ";IBM Research;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.ibm.com/research;https://www.binghamton.edu",
        "aff_unique_abbr": "UCSD;IBM;Binghamton",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "FusionSeg: Learning to Combine Motion and Appearance for Fully Automatic Segmentation of Generic Objects in Videos",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1345",
        "author_site": "Suyog Dutt Jain, Bo Xiong, Kristen Grauman",
        "author": "Suyog Dutt Jain; Bo Xiong; Kristen Grauman",
        "abstract": "We propose an end-to-end learning framework for segmenting generic objects in videos. Our method learns to combine appearance and motion information to produce pixel level segmentation masks for all prominent objects in videos. We formulate this task as a structured prediction problem and design a two-stream fully convolutional neural network which fuses together motion and appearance in a unified framework. Since large-scale video datasets with pixel level segmentations are problematic, we show how to bootstrap weakly annotated videos together with existing image recognition datasets for training. Through experiments on three challenging video segmentation benchmarks, our method substantially improves the state-of-the-art for segmenting generic (unseen) objects. Code and pre-trained models are available on the project website.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jain_FusionSeg_Learning_to_CVPR_2017_paper.pdf",
        "aff": "University of Texas at Austin; University of Texas at Austin; University of Texas at Austin",
        "project": "http://vision.cs.utexas.edu/projects/fusionseg/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Jain_FusionSeg_Learning_to_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3110577,
        "gs_citation": 477,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12649389472131688044&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu;cs.utexas.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jain_FusionSeg_Learning_to_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "G2DeNet: Global Gaussian Distribution Embedding Network and Its Application to Visual Recognition",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Oral",
        "track": "main",
        "pid": "996",
        "author_site": "Qilong Wang, Peihua Li, Lei Zhang",
        "author": "Qilong Wang; Peihua Li; Lei Zhang",
        "abstract": "Recently, plugging trainable structural layers into deep convolutional neural networks (CNNs) as image representations has made promising progress. However, there has been little work on inserting parametric probability distributions, which can effectively model feature statistics, into deep CNNs in an end-to-end manner. This paper proposes a Global Gaussian Distribution embedding Network (G2DeNet) to take a step towards addressing this problem. The core of G2DeNet is a novel trainable layer of a global Gaussian as an image representation plugged into deep CNNs for end-to-end learning. The challenge is that the proposed layer involves Gaussian distributions whose space is not a linear space, which makes its forward and backward propagations be non-intuitive and non-trivial. To tackle this issue, we employ a Gaussian embedding strategy which respects the structures of both Riemannian manifold and smooth group of Gaussians. Based on this strategy, we construct the proposed global Gaussian embedding layer and decompose it into two sub-layers: the matrix partition sub-layer decoupling the mean vector and covariance matrix entangled in the embedding matrix, and the square-rooted, symmetric positive definite matrix sub-layer. In this way, we can derive the partial derivatives associated with the proposed structural layer and thus allow backpropagation of gradients. Experimental results on large scale region classification and fine-grained recognition tasks show that G2DeNet is superior to its counterparts, capable of achieving state-of-the-art performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_G2DeNet_Global_Gaussian_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7024991237077491993&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_G2DeNet_Global_Gaussian_CVPR_2017_paper.html"
    },
    {
        "title": "GMS: Grid-based Motion Statistics for Fast, Ultra-Robust Feature Correspondence",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1703",
        "author_site": "JiaWang Bian, Wen-Yan Lin, Yasuyuki Matsushita, Sai-Kit Yeung, Tan-Dat Nguyen, Ming-Ming Cheng",
        "author": "JiaWang Bian; Wen-Yan Lin; Yasuyuki Matsushita; Sai-Kit Yeung; Tan-Dat Nguyen; Ming-Ming Cheng",
        "abstract": "Incorporating smoothness constraints into feature matching is known to enable ultra-robust matching. However, such formulations are both complex and slow, making them unsuitable for video applications. This paper proposes GMS (Grid-based Motion Statistics), a simple means of encapsulating motion smoothness as the statistical likelihood of a certain number of matches in a region. GMS enables translation of high match numbers into high match quality. This provides a real-time, ultra-robust correspondence system. Evaluation on videos, with low textures, blurs and wide-baselines show GMS consistently out-performs other real-time matchers and can achieve parity with more sophisticated, much slower techniques.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bian_GMS_Grid-based_Motion_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Bian_GMS_Grid-based_Motion_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 862,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11855706309480928327&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 27,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bian_GMS_Grid-based_Motion_CVPR_2017_paper.html"
    },
    {
        "title": "Gated Feedback Refinement Network for Dense Image Labeling",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1402",
        "author_site": "Md Amirul Islam, Mrigank Rochan, Neil D. B. Bruce, Yang Wang",
        "author": "Md Amirul Islam; Mrigank Rochan; Neil D. B. Bruce; Yang Wang",
        "abstract": "Effective integration of local and global contextual information is crucial for dense labeling problems. Most existing methods based on an encoder-decoder architecture simply concatenate features from earlier layers to obtain higher-frequency details in the refinement stages. However, there are limits to the quality of refinement possible if ambiguous information is passed forward. In this paper we propose Gated Feedback Refinement Network (G-FRNet), an end-to-end deep learning framework for dense labeling tasks that addresses this limitation of existing methods. Initially, G-FRNet makes a coarse prediction and then it progressively refines the details by efficiently integrating local and global contextual information during the refinement stages. We introduce gate units that control the information passed forward in order to filter out ambiguity. Experiments on three challenging dense labeling datasets (CamVid, PASCAL VOC 2012, and Horse-Cow Parsing) show the effectiveness of our method. Our proposed approach achieves state-of-the-art results on the CamVid and Horse-Cow Parsing datasets, and produces competitive results on the PASCAL VOC 2012 dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Islam_Gated_Feedback_Refinement_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, University of Manitoba; Department of Computer Science, University of Manitoba; Department of Computer Science, University of Manitoba; Department of Computer Science, University of Manitoba",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2552898,
        "gs_citation": 286,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5395494123851821824&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.umanitoba.ca;cs.umanitoba.ca;cs.umanitoba.ca;cs.umanitoba.ca",
        "email": "cs.umanitoba.ca;cs.umanitoba.ca;cs.umanitoba.ca;cs.umanitoba.ca",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Islam_Gated_Feedback_Refinement_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Manitoba",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://umanitoba.ca",
        "aff_unique_abbr": "U of M",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Gaze Embeddings for Zero-Shot Image Classification",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "1855",
        "author_site": "Nour Karessli, Zeynep Akata, Bernt Schiele, Andreas Bulling",
        "author": "Nour Karessli; Zeynep Akata; Bernt Schiele; Andreas Bulling",
        "abstract": "Zero-shot image classification using auxiliary information, such as attributes describing discriminative object properties, requires time-consuming annotation by domain experts. We instead propose a method that relies on human gaze as auxiliary information, exploiting that even non-expert users have a natural ability to judge class membership. We present a data collection paradigm that involves a discrimination task to increase the information content obtained from gaze data. Our method extracts discriminative descriptors from the data and learns a compatibility function between image and gaze using three novel gaze embeddings: Gaze Histograms (GH), Gaze Features with Grid (GFG) and Gaze Features with Sequence (GFS). We introduce two new gaze-annotated datasets for fine-grained image classification and show that human gaze data is indeed class discriminative, provides a competitive alternative to expert-annotated attributes, and outperforms other baselines for zero-shot image classification.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Karessli_Gaze_Embeddings_for_CVPR_2017_paper.pdf",
        "aff": "Max Planck Institute for Informatics+Eyeem; Max Planck Institute for Informatics+Amsterdam Machine Learning Lab; Max Planck Institute for Informatics; Max Planck Institute for Informatics",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.09309v2",
        "pdf_size": 7434066,
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4362941549882829775&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Karessli_Gaze_Embeddings_for_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+2;0;0",
        "aff_unique_norm": "Max Planck Institute for Informatics;Eyeem;Amsterdam Machine Learning Lab",
        "aff_unique_dep": ";;Machine Learning",
        "aff_unique_url": "https://mpi-inf.mpg.de;https://www.eyeem.com;https://amlab.nl",
        "aff_unique_abbr": "MPII;;AMLab",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+2;0;0",
        "aff_country_unique": "Germany;United States;Netherlands"
    },
    {
        "title": "General Models for Rational Cameras and the Case of Two-Slit Projections",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "718",
        "author_site": "Matthew Trager, Bernd Sturmfels, John Canny, Martial Hebert, Jean Ponce",
        "author": "Matthew Trager; Bernd Sturmfels; John Canny; Martial Hebert; Jean Ponce",
        "abstract": "The rational camera model recently introduced in [18] provides a general methodology for studying abstract nonlinear imaging systems and their multi-view geometry. This paper builds on this framework to study \"physical realizations\" of rational cameras. More precisely, we give an explicit account of the mapping between between physical visual rays and image points (missing in the original description), which allows us to give simple analytical expressions for direct and inverse projections. We also consider \"primitive\" camera models, that are orbits under the action of various projective transformations, and lead to a general notion of intrinsic parameters. The methodology is general, but it is illustrated concretely by an in-depth study of two-slit cameras, that we model using pairs of linear projections. This simple analytical form allows us to describe models for the corresponding primitive cameras, to introduce intrinsic parameters with a clear geometric meaning, and to define an epipolar tensor characterizing two-view correspondences. In turn, this leads to new algorithms for structure from motion and self-calibration.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Trager_General_Models_for_CVPR_2017_paper.pdf",
        "aff": "\u00b4Ecole Normale Sup\u00e9rieure, CNRS, PSL Research University+Inria; UC Berkeley; Carnegie Mellon University; Carnegie Mellon University; \u00b4Ecole Normale Sup\u00e9rieure, CNRS, PSL Research University+Inria",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Trager_General_Models_for_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.01160v4",
        "pdf_size": 1596366,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8483352795096963332&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Trager_General_Models_for_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2;3;3;0+1",
        "aff_unique_norm": "Ecole Normale Sup\u00e9rieure;INRIA;University of California, Berkeley;Carnegie Mellon University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ens.fr;https://www.inria.fr;https://www.berkeley.edu;https://www.cmu.edu",
        "aff_unique_abbr": "ENS;Inria;UC Berkeley;CMU",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0+0;1;1;1;0+0",
        "aff_country_unique": "France;United States"
    },
    {
        "title": "Generalized Deep Image to Image Regression",
        "session": "Machine Learning 4",
        "status": "Spotlight",
        "track": "main",
        "pid": "2384",
        "author_site": "Venkataraman Santhanam, Vlad I. Morariu, Larry S. Davis",
        "author": "Venkataraman Santhanam; Vlad I. Morariu; Larry S. Davis",
        "abstract": "We present a Deep Convolutional Neural Network architecture which serves as a generic image-to-image regressor that can be trained end-to-end without any further machinery. Our proposed architecture, the Recursively Branched Deconvolutional Network (RBDN), develops a cheap multi-context image representation very early on using an efficient recursive branching scheme with extensive parameter sharing and learnable upsampling. This multi-context representation is subjected to a highly non-linear locality preserving transformation by the remainder of our network comprising of a series of convolutions/deconvolutions without any spatial downsampling. The RBDN architecture is fully convolutional and can handle variable sized images during inference. We provide qualitative/quantitative results on 3 diverse tasks: relighting, denoising and colorization and show that our proposed RBDN architecture obtains comparable results to the state-of-the-art on each of these tasks when used off-the-shelf without any post processing or task-specific architectural modifications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Santhanam_Generalized_Deep_Image_CVPR_2017_paper.pdf",
        "aff": "UMIACS; UMIACS; UMIACS",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Santhanam_Generalized_Deep_Image_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.03268",
        "pdf_size": 2541692,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9937107506282814171&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "email": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Santhanam_Generalized_Deep_Image_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Maryland Institute for Advanced Computer Studies",
        "aff_unique_dep": "Institute for Advanced Computer Studies",
        "aff_unique_url": "https://www.cs.umd.edu/umiacs",
        "aff_unique_abbr": "UMIACS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Generalized Rank Pooling for Activity Recognition",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1192",
        "author_site": "Anoop Cherian, Basura Fernando, Mehrtash Harandi, Stephen Gould",
        "author": "Anoop Cherian; Basura Fernando; Mehrtash Harandi; Stephen Gould",
        "abstract": "Most popular deep models for action recognition split video sequences into short sub-sequences consisting of a few frames; frame-based features are then pooled for recognizing the activity. Usually, this pooling step discards the temporal order of the frames, which could otherwise be used for better recognition. Towards this end, we propose a novel pooling method, generalized rank pooling (GRP), that takes as input, features from the intermediate layers of a CNN that is trained on tiny sub-sequences, and produces as output the parameters of a subspace  which (i) provides a low-rank approximation to the features and (ii) preserves their temporal order. We propose to use these parameters as a compact representation for the video sequence, which is then used in a classification setup. We formulate an objective for computing this subspace as a Riemannian optimization problem on the Grassmann manifold, and propose an efficient conjugate gradient scheme for solving it. Experiments on several activity recognition datasets show that our scheme leads to state-of-the-art performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cherian_Generalized_Rank_Pooling_CVPR_2017_paper.pdf",
        "aff": "Australian Centre for Robotic Vision + The Australian National University; Australian Centre for Robotic Vision + The Australian National University; Data61/CSIRO + The Australian National University; Australian Centre for Robotic Vision + The Australian National University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.02112v3",
        "pdf_size": 1260339,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5993609506124495706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "anu.edu.au;anu.edu.au;data61.csiro.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;data61.csiro.au;anu.edu.au",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cherian_Generalized_Rank_Pooling_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;2+1;0+1",
        "aff_unique_norm": "Australian Centre for Robotic Vision;Australian National University;Commonwealth Scientific and Industrial Research Organisation",
        "aff_unique_dep": ";;Data61",
        "aff_unique_url": "https://roboticvision.org/;https://www.anu.edu.au;https://www.csiro.au",
        "aff_unique_abbr": "ACRV;ANU;CSIRO",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Generalized Semantic Preserving Hashing for N-Label Cross-Modal Retrieval",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "1639",
        "author_site": "Devraj Mandal, Kunal N. Chaudhury, Soma Biswas",
        "author": "Devraj Mandal; Kunal N. Chaudhury; Soma Biswas",
        "abstract": "Due to availability of large amounts of multimedia data, cross-modal matching is gaining increasing importance. Hashing based techniques provide an attractive solution to this problem when the data size is large. Different scenarios of cross-modal matching are possible, for example, data from the different modalities can be associated with a single label or multiple labels, and in addition may or may not have one-to-one correspondence. Most of the existing approaches have been developed for the case where there is one-to-one correspondence between the data of the two modalities. In this paper, we propose a simple, yet effective generalized hashing framework which can work for all the different scenarios, while preserving the semantic distance between the data points. The approach first learns the optimum hash codes for the two modalities simultaneously, so as to preserve the semantic similarity between the data points, and then learns the hash functions to map from the features to the hash codes. Extensive experiments on single label dataset like Wiki and multi-label datasets like NUS-WIDE, Pascal and LabelMe under all the different scenarios and comparisons with the state-of-the-art shows the effectiveness of the proposed approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Mandal_Generalized_Semantic_Preserving_CVPR_2017_paper.pdf",
        "aff": "Indian Institute of Science, Bangalore - 560012; Indian Institute of Science, Bangalore - 560012; Indian Institute of Science, Bangalore - 560012",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1239555,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17120442094797259927&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "ee.iisc.ernet.in;ee.iisc.ernet.in;ee.iisc.ernet.in",
        "email": "ee.iisc.ernet.in;ee.iisc.ernet.in;ee.iisc.ernet.in",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Mandal_Generalized_Semantic_Preserving_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Generating Descriptions With Grounded and Co-Referenced People",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2052",
        "author_site": "Anna Rohrbach, Marcus Rohrbach, Siyu Tang, Seong Joon Oh, Bernt Schiele",
        "author": "Anna Rohrbach; Marcus Rohrbach; Siyu Tang; Seong Joon Oh; Bernt Schiele",
        "abstract": "Learning how to generate descriptions of images or videos received major interest both in the Computer Vision and Natural Language Processing communities. While a few works have proposed to learn a grounding during the generation process in an unsupervised way (via an attention mechanism), it remains unclear how good the quality of the grounding is and whether it benefits the description quality. In this work we propose a movie description model which learns to generate description and jointly ground (localize) the mentioned characters as well as do visual co-reference resolution between pairs of consecutive sentences/clips. We also propose to use weak localization supervision through character mentions provided in movie descriptions to learn the character grounding. At training time, we first learn how to localize characters by relating their visual appearance to mentions in the descriptions via a semi-supervised approach. We then provide this (noisy) supervision into our description model which greatly improves its performance. Our proposed description model improves over prior work w.r.t. generated description quality and additionally provides grounding and local co-reference resolution. We evaluate it on the MPII Movie Description dataset using automatic and human evaluation measures and using our newly collected grounding and co-reference data for characters.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rohrbach_Generating_Descriptions_With_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.01518v1",
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3618864067793461711&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Rohrbach_Generating_Descriptions_With_CVPR_2017_paper.html"
    },
    {
        "title": "Generating Holistic 3D Scene Abstractions for Text-Based Image Retrieval",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "69",
        "author_site": "Ang Li, Jin Sun, Joe Yue-Hei Ng, Ruichi Yu, Vlad I. Morariu, Larry S. Davis",
        "author": "Ang Li; Jin Sun; Joe Yue-Hei Ng; Ruichi Yu; Vlad I. Morariu; Larry S. Davis",
        "abstract": "Spatial relationships between objects provide important information for text-based image retrieval. As users are more likely to describe a scene from a real world perspective, using 3D spatial relationships rather than 2D relationships that assume a particular viewing direction, one of the main challenges is to infer the 3D structure that bridges images with users' text descriptions. However, direct inference of 3D structure from images requires learning from large scale annotated data. Since interactions between objects can be reduced to a limited set of atomic spatial relations in 3D, we study the possibility of inferring 3D structure from a text description rather than an image, applying physical relation models to synthesize holistic 3D abstract object layouts satisfying the spatial constraints present in a textual description. We present a generic framework for retrieving images from a textual description of a scene by matching images with these generated abstract object layouts. Images are ranked by matching object detection outputs (bounding boxes) to 2D layout candidates (also represented by bounding boxes) which are obtained by projecting the 3D scenes with sampled camera directions. We validate our approach using public indoor scene datasets and show that our method outperforms baselines built upon object occurrence histograms and learned 2D pairwise relations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Generating_Holistic_3D_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Generating_Holistic_3D_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.09392",
        "pdf_size": 849313,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3398229100910003312&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Generating_Holistic_3D_CVPR_2017_paper.html"
    },
    {
        "title": "Generating the Future With Adversarial Transformers",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "353",
        "author_site": "Carl Vondrick, Antonio Torralba",
        "author": "Carl Vondrick; Antonio Torralba",
        "abstract": "We learn models to generate the immediate future in video. This problem has two main challenges. Firstly, since the future is uncertain, models should be multi-modal, which can be difficult to learn. Secondly, since the future is similar to the past, models store low-level details, which complicates learning of high-level semantics. We propose a framework to tackle both of these challenges. We present a model that generates the future by transforming pixels in the past. Our approach explicitly disentangles the model's memory from the prediction, which helps the model learn desirable invariances. Experiments suggest that this model can generate short videos of plausible futures. We believe predictive models have many applications in robotics, health-care, and video understanding.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Vondrick_Generating_the_Future_CVPR_2017_paper.pdf",
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1650199,
        "gs_citation": 213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9759067785717005998&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mit.edu;mit.edu",
        "email": "mit.edu;mit.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Vondrick_Generating_the_Future_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Generative Attribute Controller With Conditional Filtered Generative Adversarial Networks",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2668",
        "author_site": "Takuhiro Kaneko, Kaoru Hiramatsu, Kunio Kashino",
        "author": "Takuhiro Kaneko; Kaoru Hiramatsu; Kunio Kashino",
        "abstract": "We present a generative attribute controller (GAC), a novel functionality for generating or editing an image while intuitively controlling large variations of an attribute. This controller is based on a novel generative model called the conditional filtered generative adversarial network (CFGAN), which is an extension of the conventional conditional GAN (CGAN) that incorporates a filtering architecture into the generator input. Unlike the conventional CGAN, which represents an attribute directly using an observable variable (e.g., the binary indicator of attribute presence) so its controllability is restricted to attribute labeling (e.g., restricted to an ON or OFF control), the CFGAN has a filtering architecture that associates an attribute with a multi-dimensional latent variable, enabling latent variations of the attribute to be represented. We also define the filtering architecture and training scheme considering controllability, enabling the variations of the attribute to be intuitively controlled using typical controllers (radio buttons and slide bars). We evaluated our CFGAN on MNIST, CUB, and CelebA datasets and show that it enables large variations of an attribute to be not only represented but also intuitively controlled while retaining identity. We also show that the learned latent space has enough expressive power to conduct attribute transfer and attribute-based image retrieval.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kaneko_Generative_Attribute_Controller_CVPR_2017_paper.pdf",
        "aff": "NTT Communication Science Laboratories, NTT Corporation; NTT Communication Science Laboratories, NTT Corporation; NTT Communication Science Laboratories, NTT Corporation",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kaneko_Generative_Attribute_Controller_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 8604935,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17654734247869950932&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "lab.ntt.co.jp;lab.ntt.co.jp;lab.ntt.co.jp",
        "email": "lab.ntt.co.jp;lab.ntt.co.jp;lab.ntt.co.jp",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kaneko_Generative_Attribute_Controller_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "NTT Corporation",
        "aff_unique_dep": "Communication Science Laboratories",
        "aff_unique_url": "https://www.ntt.co.jp",
        "aff_unique_abbr": "NTT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Generative Face Completion",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1526",
        "author_site": "Yijun Li, Sifei Liu, Jimei Yang, Ming-Hsuan Yang",
        "author": "Yijun Li; Sifei Liu; Jimei Yang; Ming-Hsuan Yang",
        "abstract": "In this paper, we propose an effective face completion algorithm using a deep generative model. Different from well-studied background completion, the face completion task is more challenging as it often requires to generate semantically new pixels for the missing key components (e.g., eyes and mouths) that contain large appearance variations. Unlike existing nonparametric algorithms that search for patches to synthesize, our algorithm directly generates contents for missing regions based on a neural network. The model is trained with a combination of a reconstruction loss, two adversarial losses and a semantic parsing loss, which ensures pixel faithfulness and local-global contents consistency. With extensive experimental results, we demonstrate qualitatively and quantitatively that our model is able to deal with a large area of missing pixels in arbitrary shapes and generate realistic face completion results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Generative_Face_Completion_CVPR_2017_paper.pdf",
        "aff": "University of California, Merced; University of California, Merced; Adobe Research; University of California, Merced",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.05838v1",
        "pdf_size": 832640,
        "gs_citation": 812,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7906503087354808363&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ucmerced.edu;ucmerced.edu;adobe.com;ucmerced.edu",
        "email": "ucmerced.edu;ucmerced.edu;adobe.com;ucmerced.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Generative_Face_Completion_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of California, Merced;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.ucmerced.edu;https://research.adobe.com",
        "aff_unique_abbr": "UC Merced;Adobe",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Merced;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Generative Hierarchical Learning of Sparse FRAME Models",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3692",
        "author_site": "Jianwen Xie, Yifei Xu, Erik Nijkamp, Ying Nian Wu, Song-Chun Zhu",
        "author": "Jianwen Xie; Yifei Xu; Erik Nijkamp; Ying Nian Wu; Song-Chun Zhu",
        "abstract": "This paper proposes a method for generative learning of hierarchical random field models. The resulting model, which we call the hierarchical sparse FRAME (Filters, Random field, And Maximum Entropy) model, is a generalization of the original sparse FRAME model by decomposing it into multiple parts that are allowed to shift their locations, scales and rotations, so that the resulting model becomes a hierarchical deformable template. The model can be trained by an EM-type algorithm that alternates the following two steps: (1) Inference: Given the current model, we match it to each training image by inferring the unknown locations, scales, and rotations of the object and its parts by recursive sum-max maps, and (2) Re-learning: Given the inferred geometric configurations of the objects and their parts, we re-learn the model parameters by maximum likelihood estimation via stochastic gradient algorithm. Experiments show that the proposed method is capable of learning meaningful and interpretable templates that can be used for object detection, classification and clustering.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Generative_Hierarchical_Learning_CVPR_2017_paper.pdf",
        "aff": "University of California, Los Angeles (UCLA), USA; Shanghai Jiao Tong University, Shanghai, China; University of California, Los Angeles (UCLA), USA; University of California, Los Angeles (UCLA), USA; University of California, Los Angeles (UCLA), USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1120987,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9728708051934765563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ucla.edu;sjtu.edu.cn;ucla.edu;stat.ucla.edu;stat.ucla.edu",
        "email": "ucla.edu;sjtu.edu.cn;ucla.edu;stat.ucla.edu;stat.ucla.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Generative_Hierarchical_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "University of California, Los Angeles;Shanghai Jiao Tong University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucla.edu;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "UCLA;SJTU",
        "aff_campus_unique_index": "0;1;0;0;0",
        "aff_campus_unique": "Los Angeles;Shanghai",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Geodesic Distance Descriptors",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2860",
        "author_site": "Gil Shamai, Ron Kimmel",
        "author": "Gil Shamai; Ron Kimmel",
        "abstract": "The Gromov-Hausdorff (GH) distance is traditionally used for measuring distances between metric spaces. It was adapted for non-rigid shape comparison and matching of isometric surfaces, and is defined as the minimal distortion of embedding one surface into the other, while the optimal correspondence can be described as the map that minimizes this distortion. Solving such a minimization is a hard combinatorial problem that requires precomputation and storing of all pairwise geodesic distances for the matched surfaces. A popular way for compact representation of functions on surfaces is by projecting them into the leading eigenfunctions of the Laplace-Beltrami Operator (LBO). When truncated, the basis of the LBO is known to be the optimal for representing functions with bounded gradient in a min-max sense. Methods such as Spectral-GMDS exploit this idea to simplify and efficiently approximate a minimization related to the GH distance by operating in the truncated spectral domain, and obtain state of the art results for matching of nearly isometric shapes. However, when considering only a specific set of functions on the surface, such as geodesic distances, an optimized basis could be considered as an even better alternative. Moreover, current simplifications of approximating the GH distance introduce errors due to low rank approximations and relaxations of the permutation matrices. Here, we define the geodesic distance basis, which is optimal for compact approximation of geodesic distances, in terms of Frobenius norm. We use the suggested basis to extract the Geodesic Distance Descriptor (GDD), which encodes the geodesic distances information as a linear combination of the basis functions. We then show how these ideas can be used to efficiently and accurately approximate the metric spaces matching problem with almost no loss of information. We incorporate recent methods for efficient approximation of the proposed basis and descriptor without actually computing and storing all geodesic distances. These observations are used to construct a very simple and efficient procedure for shape correspondence. Experimental results show that the GDD improves both accuracy and efficiency of state of the art shape matching procedures.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shamai_Geodesic_Distance_Descriptors_CVPR_2017_paper.pdf",
        "aff": "Technion - Israel Institute of Technologies; Technion - Israel Institute of Technologies",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Shamai_Geodesic_Distance_Descriptors_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.07360v1",
        "pdf_size": 758614,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7525490675596419250&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "tx.technion.ac.il;cs.technion.ac.il",
        "email": "tx.technion.ac.il;cs.technion.ac.il",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shamai_Geodesic_Distance_Descriptors_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs",
        "session": "Machine Learning 4",
        "status": "Oral",
        "track": "main",
        "pid": "2123",
        "author_site": "Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodol\u00c3\u00a0, Jan Svoboda, Michael M. Bronstein",
        "author": "Federico Monti; Davide Boscaini; Jonathan Masci; Emanuele Rodola; Jan Svoboda; Michael M. Bronstein",
        "abstract": "Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Monti_Geometric_Deep_Learning_CVPR_2017_paper.pdf",
        "aff": "USI Lugano; USI Lugano; USI Lugano+Nnaisense; USI Lugano; USI Lugano; USI Lugano+Tel Aviv University+Intel Perceptual Computing",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Monti_Geometric_Deep_Learning_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.08402v3",
        "pdf_size": 6605970,
        "gs_citation": 2443,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14362062008359202020&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Monti_Geometric_Deep_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0+1;0;0;0+2+3",
        "aff_unique_norm": "Universit\u00e0 della Svizzera italiana;NNAISENSE;Tel Aviv University;Intel",
        "aff_unique_dep": ";;;Perceptual Computing",
        "aff_unique_url": "https://www.usi.ch;;https://www.tau.ac.il;https://www.intel.com",
        "aff_unique_abbr": "USI;;TAU;Intel",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Lugano;",
        "aff_country_unique_index": "0;0;0;0;0;0+2+3",
        "aff_country_unique": "Switzerland;;Israel;United States"
    },
    {
        "title": "Geometric Loss Functions for Camera Pose Regression With Deep Learning",
        "session": "Machine Learning for 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "2602",
        "author_site": "Alex Kendall, Roberto Cipolla",
        "author": "Alex Kendall; Roberto Cipolla",
        "abstract": "Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single image. It learns to localize using high level features and is robust to difficult lighting, motion blur and unknown camera intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoretical treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to automatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique significantly improves PoseNet's performance across datasets ranging from indoor rooms to a small city.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kendall_Geometric_Loss_Functions_CVPR_2017_paper.pdf",
        "aff": "University of Cambridge; University of Cambridge",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.00390v2",
        "pdf_size": 1871235,
        "gs_citation": 970,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10977732371209027056&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kendall_Geometric_Loss_Functions_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Global Context-Aware Attention LSTM Networks for 3D Action Recognition",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "600",
        "author_site": "Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, Alex C. Kot",
        "author": "Jun Liu; Gang Wang; Ping Hu; Ling-Yu Duan; Alex C. Kot",
        "abstract": "Long Short-Term Memory (LSTM) networks have shown superior performance in 3D human action recognition due to their power in modeling the dynamics and dependencies in sequential data. Since not all joints are informative for action analysis and the irrelevant joints often bring a lot of noise, we need to pay more attention to the informative ones. However, original LSTM does not have strong attention capability. Hence we propose a new class of LSTM network, Global Context-Aware Attention LSTM (GCA-LSTM), for 3D action recognition, which is able to selectively focus on the informative joints in the action sequence with the assistance of global contextual information. In order to achieve a reliable attention representation for the action sequence, we further propose a recurrent attention mechanism for our GCA-LSTM network, in which the attention performance is improved iteratively. Experiments show that our end-to-end network can reliably focus on the most informative joints in each frame of the skeleton sequence. Moreover, our network yields state-of-the-art performance on three challenging datasets for 3D action recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.pdf",
        "aff": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Alibaba Group, Hangzhou, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; National Engineering Lab for Video Technology, Peking University, Beijing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1201730,
        "gs_citation": 822,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8402249122917065687&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg;pku.edu.cn;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg;pku.edu.cn;ntu.edu.sg",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Global_Context-Aware_Attention_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "Nanyang Technological University;Alibaba Group;Peking University",
        "aff_unique_dep": "School of Electrical and Electronic Engineering;;National Engineering Lab for Video Technology",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.alibaba.com;http://www.pku.edu.cn",
        "aff_unique_abbr": "NTU;Alibaba;PKU",
        "aff_campus_unique_index": "0;1;0;2;0",
        "aff_campus_unique": "Singapore;Hangzhou;Beijing",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "title": "Global Hypothesis Generation for 6D Object Pose Estimation",
        "session": "3D Vision 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "147",
        "author_site": "Frank Michel, Alexander Kirillov, Eric Brachmann, Alexander Krull, Stefan Gumhold, Bogdan Savchynskyy, Carsten Rother",
        "author": "Frank Michel; Alexander Kirillov; Eric Brachmann; Alexander Krull; Stefan Gumhold; Bogdan Savchynskyy; Carsten Rother",
        "abstract": "This paper addresses the task of estimating the 6D-pose of a known 3D object from a single RGB-D image. Most modern approaches solve this task in three steps: i) compute local features; ii) generate a pool of pose-hypotheses; iii) select and refine a pose from the pool. This work focuses on the second step. While all existing approaches generate the hypotheses pool via local reasoning, e.g. RANSAC or Hough-Voting, we are the first to show that global reasoning is beneficial at this stage. In particular, we formulate a novel fully-connected Conditional Random Field (CRF) that outputs a very small number of pose-hypotheses. Despite the potential functions of the CRF being non-Gaussian, we give a new, efficient two-step optimization procedure, with some guarantees for optimality. We utilize our global hypotheses generation procedure to produce results that exceed state-of-the-art for the challenging \"Occluded Object Dataset\".",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Michel_Global_Hypothesis_Generation_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.02287v3",
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16707428201246393450&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Michel_Global_Hypothesis_Generation_CVPR_2017_paper.html"
    },
    {
        "title": "Global Optimality in Neural Network Training",
        "session": "Machine Learning 3",
        "status": "Oral",
        "track": "main",
        "pid": "3586",
        "author_site": "Benjamin D. Haeffele, Ren\u00c3\u00a9 Vidal",
        "author": "Benjamin D. Haeffele; Rene Vidal",
        "abstract": "The past few years have seen a dramatic increase in the performance of recognition systems thanks to the introduction of deep networks for representation learning. However, the mathematical reasons for this success remain elusive. A key issue is that the neural network training problem is nonconvex, hence optimization algorithms may not return a global minima. This paper provides sufficient conditions to guarantee that local minima are globally optimal and that a local descent strategy can reach a global minima from any initialization. Our conditions require both the network output and the regularization to be positively homogeneous functions of the network parameters, with the regularization being designed to control the network size. Our results apply to networks with one hidden layer, where size is measured by the number of neurons in the hidden layer, and multiple deep subnetworks connected in parallel, where size is measured by the number of subnetworks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Haeffele_Global_Optimality_in_CVPR_2017_paper.pdf",
        "aff": "Johns Hopkins University, Center for Imaging Science, Baltimore, MD 21218, USA; Johns Hopkins University, Center for Imaging Science, Baltimore, MD 21218, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Haeffele_Global_Optimality_in_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 528938,
        "gs_citation": 177,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13704363024977859768&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "jhu.edu;cis.jhu.edu",
        "email": "jhu.edu;cis.jhu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Haeffele_Global_Optimality_in_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "Center for Imaging Science",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Baltimore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Graph-Structured Representations for Visual Question Answering",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "6",
        "author_site": "Damien Teney, Lingqiao Liu, Anton van den Hengel",
        "author": "Damien Teney; Lingqiao Liu; Anton van den Hengel",
        "abstract": "This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which do not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. We show that this approach achieves significant improvements over the state-of-the-art, increasing accuracy from 71.2% to 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Teney_Graph-Structured_Representations_for_CVPR_2017_paper.pdf",
        "aff": "Australian Centre for Visual Technologies; Australian Centre for Visual Technologies; Australian Centre for Visual Technologies",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Teney_Graph-Structured_Representations_for_2017_CVPR_supplemental.pdf",
        "arxiv": "1609.05600",
        "pdf_size": 969208,
        "gs_citation": 525,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10944287582850224442&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Teney_Graph-Structured_Representations_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Australian Centre for Visual Technologies",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Grassmannian Manifold Optimization Assisted Sparse Spectral Clustering",
        "session": "Theory",
        "status": "Poster",
        "track": "main",
        "pid": "2208",
        "author_site": "Qiong Wang, Junbin Gao, Hong Li",
        "author": "Qiong Wang; Junbin Gao; Hong Li",
        "abstract": "Spectral Clustering is one of pioneered clustering methods in machine learning and pattern recognition field. It relies on the spectral decomposition criterion to learn a low-dimensonal embedding of data for a basic clustering algorithm such as the k-means. The recent sparse Spectral clustering (SSC) introduces the sparsity for the similarity in low-dimensional space by enforcing a sparsity-induced penalty, resulting a non-convex optimization, and the solution is calculated through a relaxed convex problem via the standard ADMM (Alternative Direction Method of Multipliers), rather than inferring latent representation from eigen-structure. This paper provides a direct solution as solving a new Grassmann optimization problem. By this way calculating latent embedding becomes part of optmization on manifolds and the recently developed manifold optimization methods can be applied.   It turns out the learned new features are not only very informative for clustering, but also more intuitive and effective in visualization after dimensionality reduction. We conduct empirical studies on simulated datasets and several real-world benchmark datasets to validate the proposed methods. Experimental results exhibit the effectiveness of this new manifold-based clustering and dimensionality reduction method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Grassmannian_Manifold_Optimization_CVPR_2017_paper.pdf",
        "aff": "School of Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan 430074, China; Discipline of Business Analytics, The University of Sydney Business School, The University of Sydney, NSW 2006, Australia; School of Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan 430074, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2394154,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11775240485639031161&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "hust.edu.cn;sydney.edu.au;hust.edu.cn",
        "email": "hust.edu.cn;sydney.edu.au;hust.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Grassmannian_Manifold_Optimization_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Huazhong University of Science and Technology;University of Sydney",
        "aff_unique_dep": "School of Mathematics and Statistics;The University of Sydney Business School",
        "aff_unique_url": "http://www.hust.edu.cn;https://www.sydney.edu.au",
        "aff_unique_abbr": "HUST;USYD",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Wuhan;Sydney",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Group-Wise Point-Set Registration Based on Renyi's Second Order Entropy",
        "session": "3D Vision 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "3074",
        "author_site": "Luis G. Sanchez Giraldo, Erion Hasanbelliu, Murali Rao, Jose C. Principe",
        "author": "Luis G. Sanchez Giraldo; Erion Hasanbelliu; Murali Rao; Jose C. Principe",
        "abstract": "In this paper, we describe a set of robust algorithms for group-wise registration using both rigid and non-rigid transformations of multiple unlabelled point-sets with no bias toward a given set. These methods mitigate the need to establish a correspondence among the point-sets by representing them as probability density functions where the registration is treated as a multiple distribution alignment. Holder's and Jensen's inequalities provide a notion of similarity/distance among point-sets and Renyi's second order entropy yields a closed-form solution to the cost function and update equations. We also show that the methods can be improved by normalizing the entropy with a scale factor. These provide simple, fast and accurate algorithms to compute the spatial transformation function needed to register multiple point-sets. The algorithms are compared against two well-known methods for group-wise point-set registration. The results show an improvement in both accuracy and computational complexity.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Giraldo_Group-Wise_Point-Set_Registration_CVPR_2017_paper.pdf",
        "aff": "University of Miami, FL, USA; University of Florida, FL, USA; University of Florida, FL, USA; University of Florida, FL, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 17962253,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3656805698591637203&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cs.miami.edu;gmail.com;ufl.edu;cnel.ufl.edu",
        "email": "cs.miami.edu;gmail.com;ufl.edu;cnel.ufl.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Giraldo_Group-Wise_Point-Set_Registration_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Miami;University of Florida",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.miami.edu;https://www.ufl.edu",
        "aff_unique_abbr": "UM;UF",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Coral Gables;FL",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Growing a Brain: Fine-Tuning by Increasing Model Capacity",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "902",
        "author_site": "Yu-Xiong Wang, Deva Ramanan, Martial Hebert",
        "author": "Yu-Xiong Wang; Deva Ramanan; Martial Hebert",
        "abstract": "CNNs have made an undeniable impact on computer vision through the ability to learn high-capacity models with large annotated training sets. One of their remarkable properties is the ability to transfer knowledge from a large source dataset to a (typically smaller) target dataset. This is usually accomplished through fine-tuning a fixed-size network on new target data. Indeed, virtually every contemporary visual recognition system makes use of fine-tuning to transfer knowledge from ImageNet. In this work, we analyze what components and parameters change during fine-tuning, and discover that increasing model capacity allows for more natural model adaptation through fine-tuning. By making an analogy to developmental learning, we demonstrate that growing a CNN with additional units, either by widening existing layers or deepening the overall network, significantly outperforms classic fine-tuning approaches. But in order to properly grow a network, we show that newly-added units must be appropriately normalized to allow for a pace of learning that is consistent with existing units. We empirically validate our approach on several benchmark datasets, producing state-of-the-art results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Growing_a_Brain_CVPR_2017_paper.pdf",
        "aff": "Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University; Robotics Institute, Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1907.07844v1",
        "pdf_size": 9908093,
        "gs_citation": 203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12488441228801717912&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Growing_a_Brain_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "GuessWhat?! Visual Object Discovery Through Multi-Modal Dialogue",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "2327",
        "author_site": "Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, Aaron Courville",
        "author": "Harm de Vries; Florian Strub; Sarath Chandar; Olivier Pietquin; Hugo Larochelle; Aaron Courville",
        "abstract": "We introduce GuessWhat?!, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions.  Higher-level image understanding, like spatial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial baselines of the introduced tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/de_Vries_GuessWhat_Visual_Object_CVPR_2017_paper.pdf",
        "aff": "University of Montreal; Univ. Lille, CNRS, Centrale Lille, Inria, UMR 9189 CRIStAL; University of Montreal; DeepMind; Twitter; University of Montreal",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/de_Vries_GuessWhat_Visual_Object_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.08481v2",
        "pdf_size": 935911,
        "gs_citation": 482,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16362132917840718319&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "harmdevries.com;inria.fr;gmail.com;google.com;twitter.com;gmail.com",
        "email": "harmdevries.com;inria.fr;gmail.com;google.com;twitter.com;gmail.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/de_Vries_GuessWhat_Visual_Object_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;2;3;0",
        "aff_unique_norm": "University of Montreal;University of Lille;DeepMind;Twitter, Inc.",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://wwwumontreal.ca;https://www.univ-lille.fr;https://deepmind.com;https://twitter.com",
        "aff_unique_abbr": "UM;Univ. Lille;DeepMind;Twitter",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2;3;0",
        "aff_country_unique": "Canada;France;United Kingdom;United States"
    },
    {
        "title": "HOPE: Hierarchical Object Prototype Encoding for Efficient Object Instance Search in Videos",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "887",
        "author_site": "Tan Yu, Yuwei Wu, Junsong Yuan",
        "author": "Tan Yu; Yuwei Wu; Junsong Yuan",
        "abstract": "This paper tackles the problem of efficient and effective object instance search in videos. To effectively capture the relevance between a query and video frames and precisely localize the particular object, we leverage the object proposals to improve the quality of object instance search in videos. However, hundreds of object proposals obtained from each frame could result in unaffordable memory and computational cost. To this end, we present a simple yet effective hierarchical object prototype encoding (HOPE) model to accelerate the object instance search without sacrificing accuracy, which exploits both the spatial and temporal self-similarity property existing in object proposals generated from video frames. We design two types of sphere k-means methods, i.e., spatially-constrained sphere k-means and temporally-constrained sphere k-means to learn frame-level object prototypes and dataset-level object prototypes, respectively.  In this way, the object instance search problem is cast to the sparse matrix-vector multiplication problem. Thanks to the sparsity of the codes, both the memory and computational cost are significantly reduced. Experimental results on two video datasets demonstrate that our approach significantly improves the performance of video object instance search over other state-of-the-art fast search schemes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_HOPE_Hierarchical_Object_CVPR_2017_paper.pdf",
        "aff": "ROSE Lab, Interdisciplinary Graduate School, Nanyang Technological University, Singapore; ROSE Lab, Interdisciplinary Graduate School, Nanyang Technological University, Singapore + Beijing Laboratory of Intelligent Information Technology, Beijing Institute of Technology, Beijing; ROSE Lab, Interdisciplinary Graduate School, Nanyang Technological University, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1099337,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3501724129997176148&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "ntu.edu.sg;bit.edu.cn;ntu.edu.sg",
        "email": "ntu.edu.sg;bit.edu.cn;ntu.edu.sg",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_HOPE_Hierarchical_Object_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Nanyang Technological University;Beijing Institute of Technology",
        "aff_unique_dep": "Interdisciplinary Graduate School;Beijing Laboratory of Intelligent Information Technology",
        "aff_unique_url": "https://www.ntu.edu.sg;http://www.bit.edu.cn/",
        "aff_unique_abbr": "NTU;BIT",
        "aff_campus_unique_index": "0;0+1;0",
        "aff_campus_unique": "Singapore;Beijing",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "title": "HPatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2146",
        "author_site": "Vassileios Balntas, Karel Lenc, Andrea Vedaldi, Krystian Mikolajczyk",
        "author": "Vassileios Balntas; Karel Lenc; Andrea Vedaldi; Krystian Mikolajczyk",
        "abstract": "In this paper, we propose a novel benchmark for evaluating local image descriptors. We demonstrate that the existing datasets and evaluation protocols do not specify unambiguously all aspects of evaluation, leading to ambiguities and inconsistencies in results reported in the literature. Furthermore, these datasets are nearly saturated due to the recent improvements in local descriptors obtained by learning them from large annotated datasets. Therefore, we introduce a new large dataset suitable for training and testing modern descriptors, together with strictly defined evaluation protocols in several tasks such as matching, retrieval and classification. This allows for more realistic, and thus more reliable comparisons in different application scenarios. We evaluate the performance of several state-of-the-art descriptors and analyse their properties. We show that a simple normalisation of traditional hand-crafted descriptors can boost their performance to the level of deep learning based descriptors within a realistic benchmarks evaluation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Balntas_HPatches_A_Benchmark_CVPR_2017_paper.pdf",
        "aff": "Imperial College London; University of Oxford; University of Oxford; Imperial College London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.05939v1",
        "pdf_size": 804847,
        "gs_citation": 980,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7893894684615898404&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff_domain": "imperial.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk;imperial.ac.uk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Balntas_HPatches_A_Benchmark_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Imperial College London;University of Oxford",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.ox.ac.uk",
        "aff_unique_abbr": "ICL;Oxford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "HSfM: Hybrid Structure-from-Motion",
        "session": "3D Vision 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "439",
        "author_site": "Hainan Cui, Xiang Gao, Shuhan Shen, Zhanyi Hu",
        "author": "Hainan Cui; Xiang Gao; Shuhan Shen; Zhanyi Hu",
        "abstract": "Structure-from-Motion (SfM) methods can be broadly categorized as incremental or global according to their ways to estimate initial camera poses. While incremental system has advanced in robustness and accuracy, the efficiency remains its key challenge. To solve this problem, global reconstruction system simultaneously estimates all camera poses from the epipolar geometry graph, but it is usually sensitive to outliers. In this work, we propose a new hybrid SfM method to tackle the issues of efficiency, accuracy and robustness in a unified framework. More specifically, we propose an adaptive community-based rotation averaging method first to estimate camera rotations in a global manner. Then, based on these estimated camera rotations, camera centers are computed in an incremental way. Extensive experiments show that our hybrid method performs similarly or better than many of the state-of-the-art global SfM approaches, in terms of computational efficiency, while achieves similar reconstruction accuracy and robustness with two other state-of-the-art incremental SfM approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cui_HSfM_Hybrid_Structure-from-Motion_CVPR_2017_paper.pdf",
        "aff": "NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences, Beijing, China; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences, Beijing, China; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences, Beijing, China; NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China+University of Chinese Academy of Sciences, Beijing, China+CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Cui_HSfM_Hybrid_Structure-from-Motion_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3317961,
        "gs_citation": 188,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1131488642727521066&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cui_HSfM_Hybrid_Structure-from-Motion_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;0+1+0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Hallucinating Very Low-Resolution Unaligned and Noisy Face Images by Transformative Discriminative Autoencoders",
        "session": "Machine Learning 4",
        "status": "Spotlight",
        "track": "main",
        "pid": "1413",
        "author_site": "Xin Yu, Fatih Porikli",
        "author": "Xin Yu; Fatih Porikli",
        "abstract": "Most of the conventional face hallucination methods assume the input image is sufficiently large and aligned, and all require the input image to be noise-free. Their performance degrades drastically if the input image is tiny, unaligned, and contaminated by noise.   In this paper, we introduce a novel transformative discriminative autoencoder to 8X super-resolve unaligned noisy and tiny (16X16) low-resolution face images. In contrast to encoder-decoder based autoencoders, our method uses decoder-encoder-decoder networks. We first employ a transformative discriminative decoder network to upsample and denoise simultaneously. Then we use a transformative encoder network to project the intermediate HR faces to aligned and noise-free LR faces. Finally, we use the second decoder to generate hallucinated HR images. Our extensive evaluations on a very large face dataset show that our method achieves superior hallucination results and outperforms the state-of-the-art by a large margin of 1.82dB PSNR.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_Hallucinating_Very_Low-Resolution_CVPR_2017_paper.pdf",
        "aff": "Australian National University; Australian National University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 997447,
        "gs_citation": 178,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15013642805081868754&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_Hallucinating_Very_Low-Resolution_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Australian National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.anu.edu.au",
        "aff_unique_abbr": "ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Hand Keypoint Detection in Single Images Using Multiview Bootstrapping",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "394",
        "author_site": "Tomas Simon, Hanbyul Joo, Iain Matthews, Yaser Sheikh",
        "author": "Tomas Simon; Hanbyul Joo; Iain Matthews; Yaser Sheikh",
        "abstract": "We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand. We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand. The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers. Finally, the reprojected triangulations are used as new labeled training data to improve the detector. We repeat this process, generating more labeled data in each iteration. We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector. The method is used to train a hand keypoint detector for single images. The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors. The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Simon_Hand_Keypoint_Detection_CVPR_2017_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.07809v1",
        "pdf_size": 3651289,
        "gs_citation": 1579,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=841979590126160273&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Simon_Hand_Keypoint_Detection_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Hard Mixtures of Experts for Large Scale Weakly Supervised Vision",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3159",
        "author_site": "Sam Gross, Marc'Aurelio Ranzato, Arthur Szlam",
        "author": "Sam Gross; Marc'Aurelio Ranzato; Arthur Szlam",
        "abstract": "Training convolutional networks (CNN's) that fit on a single GPU with minibatch stochastic gradient descent has become effective in practice.   However, there is still no effective method for training large networks that do not fit in the memory of a few GPU cards, or for parallelizing CNN training.  In this work we show that a simple hard mixture of experts model  can be efficiently trained to good effect on large scale hashtag (multilabel) prediction tasks.  Mixture of experts models are not new [??], but in the past, researchers have had to devise sophisticated methods to deal with data fragmentation.  We show empirically that modern weakly supervised data sets are large enough to support naive partitioning schemes where each data point is assigned to a single expert.  Because the experts are independent, training them in parallel is easy, and evaluation is cheap for the size of the model.   Furthermore, we show that we can use a single decoding layer for all the experts, allowing a unified feature embedding space.   We demonstrate that it is feasible (and in fact relatively painless) to train far larger models than could be practically trained with standard CNN architectures, and that the extra capacity can be well used on current datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gross_Hard_Mixtures_of_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Gross_Hard_Mixtures_of_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.06363v1",
        "pdf_size": 1454072,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16381344617055570759&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gross_Hard_Mixtures_of_CVPR_2017_paper.html"
    },
    {
        "title": "Hardware-Efficient Guided Image Filtering for Multi-Label Problem",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2237",
        "author_site": "Longquan Dai, Mengke Yuan, Zechao Li, Xiaopeng Zhang, Jinhui Tang",
        "author": "Longquan Dai; Mengke Yuan; Zechao Li; Xiaopeng Zhang; Jinhui Tang",
        "abstract": "The Guided Filter (GF) is well-known for its linear complexity. However, when filtering an image with an n-channel guidance, GF needs to invert an n xn matrix for each pixel. To the best of our knowledge existing matrix inverse algorithms are inefficient on current hardwares. This shortcoming limits applications of multichannel guidance in computation intensive system such as multi-label system.  We need a new GF-like filter that can perform fast multichannel image guided filtering. Since the optimal linear complexity of GF cannot be minimized further, the only way thus is to bring all potentialities of current parallel computing hardwares into full play. In this paper we propose a hardware-efficient Guided Filter (HGF), which solves the efficiency problem of multichannel guided image filtering and yields competent results when applying it to multi-label problems with synthesized polynomial multichannel guidance. Specifically, in order to boost the filtering  performance,  HGF takes a new matrix inverse algorithm which only involves two hardware-efficient operations: element-wise arithmetic calculations and box filtering. In order to break the linear model restriction, HGF synthesizes a polynomial multichannel guidance to introduce nonlinearity. Benefiting from our polynomial guidance and hardware-efficient matrix inverse algorithm, HGF not only is more sensitive to the underlying structure of guidance but also achieves the fastest computing speed. Due to these merits, HGF obtains state-of-the-art results in terms of accuracy and efficiency in the computation intensive multi-label systems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dai_Hardware-Efficient_Guided_Image_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science and Engineering, Nanjing University of Science and Technology; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Science; School of Computer Science and Engineering, Nanjing University of Science and Technology; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Science; School of Computer Science and Engineering, Nanjing University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1803.00005v1",
        "pdf_size": 5177495,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6066139582718083243&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "njust.edu.cn;nlpr.ia.ac.cn;njust.edu.cn;nlpr.ia.ac.cn;njust.edu.cn",
        "email": "njust.edu.cn;nlpr.ia.ac.cn;njust.edu.cn;nlpr.ia.ac.cn;njust.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dai_Hardware-Efficient_Guided_Image_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "Nanjing University of Science and Technology;Chinese Academy of Sciences",
        "aff_unique_dep": "School of Computer Science and Engineering;Institute of Automation",
        "aff_unique_url": "http://www.nust.edu.cn;http://www.ia.cas.cn",
        "aff_unique_abbr": "NUST;CAS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Nanjing;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Harmonic Networks: Deep Translation and Rotation Equivariance",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2077",
        "author_site": "Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, Gabriel J. Brostow",
        "author": "Daniel E. Worrall; Stephan J. Garbin; Daniyar Turmukhambetov; Gabriel J. Brostow",
        "abstract": "Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch.   H-Nets use a rich, parameter-efficient and fixed computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Worrall_Harmonic_Networks_Deep_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "http://visual.cs.ucl.ac.uk/pubs/harmonicNets/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Worrall_Harmonic_Networks_Deep_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.04642",
        "pdf_size": 2401733,
        "gs_citation": 867,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12624946476992861125&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Worrall_Harmonic_Networks_Deep_CVPR_2017_paper.html"
    },
    {
        "title": "Harvesting Multiple Views for Marker-Less 3D Human Pose Annotations",
        "session": "Analyzing Humans 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "3245",
        "author_site": "Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas Daniilidis",
        "author": "Georgios Pavlakos; Xiaowei Zhou; Konstantinos G. Derpanis; Kostas Daniilidis",
        "abstract": "Recent advances with Convolutional Networks (ConvNets) have shifted the bottleneck for many computer vision tasks to annotated data collection. In this paper, we present a geometry-driven approach to automatically collect annotations for human pose prediction tasks. Starting from a generic ConvNet for 2D human pose, and assuming a multi-view setup, we describe an automatic way to collect accurate 3D human pose annotations. We capitalize on constraints offered by the 3D geometry of the camera setup and the 3D structure of the human body to probabilistically combine per view 2D ConvNet predictions into a globally optimal 3D pose. This 3D pose is used as the basis for harvesting annotations. The benefit of the annotations produced automatically with our approach is demonstrated in two challenging settings: (i) fine-tuning a generic ConvNet-based 2D pose predictor to capture the discriminative aspects of a subject's appearance (i.e.,\"personalization\"), and (ii) training a ConvNet from scratch for single view 3D human pose prediction without leveraging 3D pose groundtruth. The proposed multi-view pose estimator achieves state-of-the-art results on standard benchmarks, demonstrating the effectiveness of our method in exploiting the available multi-view information.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Pavlakos_Harvesting_Multiple_Views_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Pavlakos_Harvesting_Multiple_Views_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.04793",
        "pdf_size": 1004214,
        "gs_citation": 248,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16290432664524453915&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Pavlakos_Harvesting_Multiple_Views_CVPR_2017_paper.html"
    },
    {
        "title": "Hidden Layers in Perceptual Learning",
        "session": "Machine Learning 4",
        "status": "Spotlight",
        "track": "main",
        "pid": "1881",
        "author_site": "Gad Cohen, Daphna Weinshall",
        "author": "Gad Cohen; Daphna Weinshall",
        "abstract": "Studies in visual perceptual learning investigate the way human performance improves with practice, in the context of relatively simple (and therefore more manageable) visual tasks. Building on the powerful tools currently available for the training of Convolution Neural Networks (CNN), networks whose original architecture was inspired by the visual system, we revisited some of the open computational questions in perceptual learning. We first replicated two representative sets of perceptual learning experiments by training a shallow CNN to perform the relevant tasks. These networks qualitatively showed most of the characteristic behavior observed in perceptual learning, including the hallmark phenomena of specificity and its various manifestations in the forms of transfer or partial transfer, and learning enabling. We next analyzed the dynamics of weight modifications in the networks, identifying patterns which appeared to be instrumental for the transfer (or generalization) of learned skills from one task to another in the simulated networks. These patterns may identify ways by which the domain of search in the parameter space during network re-training can be significantly reduced, thereby accomplishing knowledge transfer.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cohen_Hidden_Layers_in_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem 91904, Israel; School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem 91904, Israel",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1126809,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5297297712680361489&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;mail.huji.ac.il",
        "email": "gmail.com;mail.huji.ac.il",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cohen_Hidden_Layers_in_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Hierarchical Boundary-Aware Neural Encoder for Video Captioning",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "601",
        "author_site": "Lorenzo Baraldi, Costantino Grana, Rita Cucchiara",
        "author": "Lorenzo Baraldi; Costantino Grana; Rita Cucchiara",
        "abstract": "The use of Recurrent Neural Networks for video captioning has recently gained a lot of attention, since they can be used both to encode the input video and to generate the corresponding description. In this paper, we present a recurrent video encoding scheme which can discover and leverage the hierarchical structure of the video. Unlike the classical encoder-decoder approach, in which a video is encoded continuously by a recurrent layer, we propose a novel LSTM cell which can identify discontinuity points between frames or segments and modify the temporal connections of the encoding layer accordingly. We evaluate our approach on three large-scale datasets: the Montreal Video Annotation dataset, the MPII Movie Description dataset and the Microsoft Video Description Corpus. Experiments show that our approach can discover appropriate hierarchical representations of input videos and improve the state of the art results on movie description datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper.pdf",
        "aff": "University of Modena and Reggio Emilia; University of Modena and Reggio Emilia; University of Modena and Reggio Emilia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Baraldi_Hierarchical_Boundary-Aware_Neural_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.09312v3",
        "pdf_size": 932750,
        "gs_citation": 248,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17607393133652506561&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "unimore.it;unimore.it;unimore.it",
        "email": "unimore.it;unimore.it;unimore.it",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Modena and Reggio Emilia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unimore.it",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "title": "Hierarchical Multimodal Metric Learning for Multimodal Classification",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1128",
        "author_site": "Heng Zhang, Vishal M. Patel, Rama Chellappa",
        "author": "Heng Zhang; Vishal M. Patel; Rama Chellappa",
        "abstract": "Multimodal classification arises in many computer vision tasks such as object classification and image retrieval. The idea is to utilize multiple sources (modalities) measuring the same instance to improve the overall performance compared to using a single source (modality). The varying characteristics exhibited by multiple modalities make it necessary to simultaneously learn the corresponding metrics. In this paper, we propose a multiple metrics learning algorithm for multimodal data. Metric of each modality is a product of two matrices: one matrix is modality specific, the other is enforced to be shared by all the modalities. The learned metrics can improve multimodal classification accuracy and experimental results on four datasets show that the proposed algorithm outperforms existing learning algorithms based on multiple metrics as well as other approaches tested on these datasets. Specifically, we report 95.0% object instance recognition accuracy, 89.2% object category recognition accuracy on the multi-view RGB-D dataset and 52.3% scene category recognition accuracy on SUN RGB-D dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Hierarchical_Multimodal_Metric_CVPR_2017_paper.pdf",
        "aff": "Center for Automation Research, University of Maryland, College Park, MD 20742; Department of Electrical and Computer Engineering, Rutgers University, NJ, 08854; Center for Automation Research, University of Maryland, College Park, MD 20742",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 828078,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15165351246029094858&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "umiacs.umd.edu;rutgers.edu;umiacs.umd.edu",
        "email": "umiacs.umd.edu;rutgers.edu;umiacs.umd.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Hierarchical_Multimodal_Metric_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Maryland;Rutgers University",
        "aff_unique_dep": "Center for Automation Research;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.umd.edu;https://www.rutgers.edu",
        "aff_unique_abbr": "UMD;Rutgers",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "College Park;New Brunswick",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "High-Resolution Image Inpainting Using Multi-Scale Neural Patch Synthesis",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3092",
        "author_site": "Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang, Hao Li",
        "author": "Chao Yang; Xin Lu; Zhe Lin; Eli Shechtman; Oliver Wang; Hao Li",
        "abstract": "Recent advances in deep learning have shown exciting promise in filling large holes in natural images with semantically plausible and context aware details, impacting fundamental image manipulation tasks such as object removal. While these learning-based methods are significantly more effective in capturing high-level features than prior techniques, they can only handle very low-resolution inputs due to memory limitations and difficulty in training. Even for slightly larger images, the inpainted regions would appear blurry and unpleasant boundaries become visible. We propose a multi-scale neural patch synthesis approach based on joint optimization of image content and texture constraints, which not only preserves contextual structures but also produces high-frequency details by matching and adapting patches with the most similar mid-layer feature correlations of a deep classification network. We evaluate our method on the ImageNet and Paris Streetview datasets and achieved state-of-the-art inpainting accuracy. We show our approach produces sharper and more coherent results than prior methods, especially for high-resolution images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_High-Resolution_Image_Inpainting_CVPR_2017_paper.pdf",
        "aff": "University of Southern California; Adobe Research; Adobe Research; Adobe Research; Adobe Research; University of Southern California+Pinscreen+USC Institute for Creative Technologies",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.09969v2",
        "pdf_size": 2280060,
        "gs_citation": 1114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=154146008869681014&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff_domain": "usc.edu;adobe.com;adobe.com;adobe.com;adobe.com;hao-li.com",
        "email": "usc.edu;adobe.com;adobe.com;adobe.com;adobe.com;hao-li.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_High-Resolution_Image_Inpainting_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;1;0+2+0",
        "aff_unique_norm": "University of Southern California;Adobe;Pinscreen",
        "aff_unique_dep": ";Adobe Research;",
        "aff_unique_url": "https://www.usc.edu;https://research.adobe.com;https://www.pinscreen.com",
        "aff_unique_abbr": "USC;Adobe;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0+1+0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "title": "Human Shape From Silhouettes Using Generative HKS Descriptors and Cross-Modal Neural Networks",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "2001",
        "author_site": "Endri Dibra, Himanshu Jain, Cengiz \u00c3\u0096ztireli, Remo Ziegler, Markus Gross",
        "author": "Endri Dibra; Himanshu Jain; Cengiz Oztireli; Remo Ziegler; Markus Gross",
        "abstract": "In this work, we present a novel method for capturing human body shape from a single scaled silhouette. We combine deep correlated features capturing different 2D views, and embedding spaces based on 3D cues in a novel convolutional neural network (CNN) based architecture. We first train a CNN to find a richer body shape representation space from pose invariant 3D human shape descriptors. Then, we learn a mapping from silhouettes to this representation space, with the help of a novel architecture that exploits correlation of multi-view data during training time, to improve prediction at test time. We extensively validate our results on synthetic and real data, demonstrating significant improvements in accuracy as compared to the state-of-the-art, and providing a practical system for detailed human body measurements from a single image.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dibra_Human_Shape_From_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Vizrt; Department of Computer Science, ETH Zurich",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Dibra_Human_Shape_From_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1152644,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10180817709859245513&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "inf.ethz.ch;student.ethz.ch;inf.ethz.ch;vizrt.com;inf.ethz.ch",
        "email": "inf.ethz.ch;student.ethz.ch;inf.ethz.ch;vizrt.com;inf.ethz.ch",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dibra_Human_Shape_From_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "ETH Zurich;Vizrt",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.ethz.ch;https://www.vizrt.com",
        "aff_unique_abbr": "ETHZ;Vizrt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Switzerland;Norway"
    },
    {
        "title": "Hyper-Laplacian Regularized Unidirectional Low-Rank Tensor Recovery for Multispectral Image Denoising",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1726",
        "author_site": "Yi Chang, Luxin Yan, Sheng Zhong",
        "author": "Yi Chang; Luxin Yan; Sheng Zhong",
        "abstract": "Recent low-rank based matrix/tensor recovery methods have been widely explored in multispectral images (MSI) denoising. These methods, however, ignore the difference of the intrinsic structure correlation along spatial sparsity, spectral correlation and non-local self-similarity mode. In this paper, we go further by giving a detailed analysis about the rank properties both in matrix and tensor cases, and figure out the non-local self-similarity is the key ingredient, while the low-rank assumption of others may not hold. This motivates us to design a simple yet effective unidirectional low-rank tensor recovery model that is capable of truthfully capturing the intrinsic structure correlation with reduced computational burden. However, the low-rank models suffer from the ringing artifacts, due to the aggregation of overlapped patches/cubics. While previous methods resort to spatial information, we offer a new perspective by utilizing the exclusively spectral information in MSIs to address the issue. The analysis-based hyper-Laplacian prior is introduced to model the global spectral structures, so as to indirectly alleviate the ringing artifacts in spatial domain. The advantages of the proposed method over the existing ones are multi-fold: more reasonably structure correlation representability, less processing time, and less artifacts in the overlapped regions. The proposed method is extensively evaluated on several benchmarks, and significantly outperforms state-of-the-art MSI denoising methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chang_Hyper-Laplacian_Regularized_Unidirectional_CVPR_2017_paper.pdf",
        "aff": "National Key Laboratory of Science and Technology on Multispectral Information Processing; School of Automation, Huazhong University of Science and Technology, China; School of Automation, Huazhong University of Science and Technology, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Chang_Hyper-Laplacian_Regularized_Unidirectional_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 20136991,
        "gs_citation": 220,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9858411494642709449&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "email": "hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chang_Hyper-Laplacian_Regularized_Unidirectional_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "National Key Laboratory of Science and Technology on Multispectral Information Processing;Huazhong University of Science and Technology",
        "aff_unique_dep": ";School of Automation",
        "aff_unique_url": ";http://www.hust.edu.cn",
        "aff_unique_abbr": ";HUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Hyperspectral Image Super-Resolution via Non-Local Sparse Tensor Factorization",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2245",
        "author_site": "Renwei Dian, Leyuan Fang, Shutao Li",
        "author": "Renwei Dian; Leyuan Fang; Shutao Li",
        "abstract": "Hyperspectral image(HSI)super-resolution, which fuses a low-resolution (LR) HSI with a high-resolution (HR) multispectral image (MSI), has recently attracted much attention. Most of the current HSI super-resolution approaches are based on matrix factorization, which unfolds the three-dimensional HSI as a matrix before processing. In general, the matrix data representation obtained after the matrix unfolding operation makes it hard to fully exploit the inherent HSI spatial-spectral structures. In this paper, a novel HSI super-resolution method based on non-local sparse tensor factorization (called as the NLSTF) is proposed. The sparse tensor factorization can directly decompose each cube of the HSI as a sparse core tensor and dictionaries of three modes, which reformulates the HSI super-resolution problem as the estimation of sparse core tensor and dictionaries for each cube. To further exploit the non-local spatial self-similarities of the HSI, similar cubes are grouped together, and they are assumed to share the same dictionaries. The dictionaries are learned from the LR-HSI and HR-MSI for each group, and corresponding sparse core tensors are estimated by spare coding on the learned dictionaries for each cube. Experimental results demonstrate the superiority of the proposed NLSTF approach over several state-of-the-art HSI super-resolution approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dian_Hyperspectral_Image_Super-Resolution_CVPR_2017_paper.pdf",
        "aff": "College of Electrical and Information Engineering, Hunan University; College of Electrical and Information Engineering, Hunan University; College of Electrical and Information Engineering, Hunan University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 869996,
        "gs_citation": 326,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12245494594409572206&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "hnu.edu.cn;gmail.com;hnu.edu.cn",
        "email": "hnu.edu.cn;gmail.com;hnu.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dian_Hyperspectral_Image_Super-Resolution_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hunan University",
        "aff_unique_dep": "College of Electrical and Information Engineering",
        "aff_unique_url": "http://www.hnu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "IM2CAD",
        "session": "3D Vision 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "2129",
        "author_site": "Hamid Izadinia, Qi Shan, Steven M. Seitz",
        "author": "Hamid Izadinia; Qi Shan; Steven M. Seitz",
        "abstract": "Given a single photo of a room and a large database of furniture CAD models, our goal is to reconstruct a scene that is as similar as possible to the scene depicted in the photograph, and composed of objects drawn from the database. We present a completely automatic system to address this IM2CAD problem that produces high quality results on challenging imagery from interior home design and remodeling websites. Our approach iteratively optimizes the placement and scale of objects in the room to best match scene renderings to the input photo, using image comparison metrics trained via deep convolutional neural nets. By operating jointly on the full scene at once, we account for inter-object occlusions. We also show the applicability of our method in standard scene understanding benchmarks where we obtain significant improvement.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Izadinia_IM2CAD_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1608.05137",
        "pdf_size": 1817748,
        "gs_citation": 263,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8279361598178131028&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Izadinia_IM2CAD_CVPR_2017_paper.html"
    },
    {
        "title": "IRINA: Iris Recognition (Even) in Inaccurately Segmented Data",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "170",
        "author_site": "Hugo Proen\u00c3\u00a7a, Jo\u00c3\u00a3o C. Neves",
        "author": "Hugo Proenca; Joao C. Neves",
        "abstract": "The effectiveness of current iris recognition systems depends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefficients of the biometric signatures. This paper describes IRINA, an algorithm for Iris Recognition that is robust against INAccurately segmented samples, which makes it a good candidate to work in poor-quality data. The process is based in the concept of \"corresponding\" patch between pairs of images, that is used to estimate the posterior probabilities that patches regard the same biological region, even in case of segmentation errors and non-linear texture deformations. Such information enables to infer a free-form deformation field (2D registration vectors) between images, whose first and second-order statistics provide effective biometric discriminating power. Extensive experiments were carried out in four datasets (CASIA-IrisV3-Lamp, CASIA-IrisV4-Lamp, CASIA-IrisV4-Thousand and WVU) and show that IRINA not only achieves state-of-the-art performance in good quality data, but also handles effectively severe segmentation errors and large differences in pupillary dilation / constriction.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Proenca_IRINA_Iris_Recognition_CVPR_2017_paper.pdf",
        "aff": "IT - Instituto de Telecomunicac\u00b8 \u02dcoes, University of Beira Interior, Portugal; IT - Instituto de Telecomunicac\u00b8 \u02dcoes, University of Beira Interior, Portugal",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2016834,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14014932111147538182&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "di.ubi.pt;di.ubi.pt",
        "email": "di.ubi.pt;di.ubi.pt",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Proenca_IRINA_Iris_Recognition_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Beira Interior",
        "aff_unique_dep": "Instituto de Telecomunica\u00e7\u00f5es",
        "aff_unique_url": "https://www.ubi.pt",
        "aff_unique_abbr": "UBI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Portugal"
    },
    {
        "title": "Identifying First-Person Camera Wearers in Third-Person Videos",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "2125",
        "author_site": "Chenyou Fan, Jangwon Lee, Mingze Xu, Krishna Kumar Singh, Yong Jae Lee, David J. Crandall, Michael S. Ryoo",
        "author": "Chenyou Fan; Jangwon Lee; Mingze Xu; Krishna Kumar Singh; Yong Jae Lee; David J. Crandall; Michael S. Ryoo",
        "abstract": "We  consider  scenarios  in  which  we  wish  to  perform joint scene understanding,  object tracking,  activity recognition, and other tasks in scenarios in which multiple people are wearing body-worn cameras while a third-person static camera also captures the scene.  To do this, we need to establish person-level correspondences across first- and third-person videos, which is challenging because the camera wearer is not visible from his/her own egocentric video, preventing the use of direct feature matching.   In this paper,  we  propose  a  new  semi-Siamese  Convolutional  Neural  Network  architecture  to  address  this  novel  challenge. We  formulate  the  problem  as  learning  a  joint  embedding space for first- and third-person videos that considers both spatial- and motion-domain cues.  A new triplet loss function is designed to minimize the distance between correct first- and third-person  matches while maximizing the  distance  between  incorrect  ones.   This  end-to-end  approach performs significantly better than several baselines, in part by learning the first- and third-person features optimized for matching jointly with the distance measure itself.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Fan_Identifying_First-Person_Camera_CVPR_2017_paper.pdf",
        "aff": "Indiana University Bloomington; Indiana University Bloomington; Indiana University Bloomington; University of California, Davis; University of California, Davis; Indiana University Bloomington; Indiana University Bloomington",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.06340v1",
        "pdf_size": 1837127,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3652164703577940417&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 20,
        "aff_domain": "indiana.edu; ; ; ; ;indiana.edu;indiana.edu",
        "email": "indiana.edu; ; ; ; ;indiana.edu;indiana.edu",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Fan_Identifying_First-Person_Camera_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;1;0;0",
        "aff_unique_norm": "Indiana University;University of California, Davis",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.indiana.edu;https://www.ucdavis.edu",
        "aff_unique_abbr": "IU;UC Davis",
        "aff_campus_unique_index": "0;0;0;1;1;0;0",
        "aff_campus_unique": "Bloomington;Davis",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Illuminant-Camera Communication to Observe Moving Objects Under Strong External Light by Spread Spectrum Modulation",
        "session": "Computational Photography",
        "status": "Spotlight",
        "track": "main",
        "pid": "2120",
        "author_site": "Ryusuke Sagawa, Yutaka Satoh",
        "author": "Ryusuke Sagawa; Yutaka Satoh",
        "abstract": "Many algorithms of computer vision use light sources to illuminate objects to actively create situation appropriate to extract their characteristics. For example, the shape and reflectance are measured by a projector-camera system, and some human-machine or VR systems use projectors and displays for interaction. As existing active lighting systems usually assume no severe external lights to observe projected lights clearly, it is one of the limitations of active illumination. In this paper, we propose a method of energy-efficient active illumination in an environment with severe external lights. The proposed method extracts the light signals of illuminants by removing external light using spread spectrum modulation. Because an image sequence is needed to observe modulated signals, the proposed method extends signal processing to realize signal detection projected onto moving objects by combining spread spectrum modulation and spatio-temporal filtering. In the experiments, we apply the proposed method to a structured-light system under sunlight, to photometric stereo with external lights, and to insensible image embedding.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sagawa_Illuminant-Camera_Communication_to_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Sagawa_Illuminant-Camera_Communication_to_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2751851,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8858615372408395133&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sagawa_Illuminant-Camera_Communication_to_CVPR_2017_paper.html"
    },
    {
        "title": "Image Deblurring via Extreme Channels Prior",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1605",
        "author_site": "Yanyang Yan, Wenqi Ren, Yuanfang Guo, Rui Wang, Xiaochun Cao",
        "author": "Yanyang Yan; Wenqi Ren; Yuanfang Guo; Rui Wang; Xiaochun Cao",
        "abstract": "Camera motion introduces motion blur, affecting many computer vision tasks. Dark Channel Prior (DCP) helps the blind deblurring on scenes including natural, face, text, and low-illumination images. However, it has limitations and is less likely to support the kernel estimation while bright pixels dominate the input image. We observe that the bright pixels in the clear images are not likely to be bright after the blur process. Based on this observation, we first illustrate this phenomenon mathematically and define it as the Bright Channel Prior (BCP). Then, we propose a technique for deblurring such images which elevates the performance of existing motion deblurring algorithms. The proposed method takes advantage of both Bright and Dark Channel Prior. This joint prior is named as extreme channels prior and is crucial for achieving efficient restorations by leveraging both the bright and dark information. Extensive experimental results demonstrate that the proposed method is more robust and performs favorably against the state-of-the-art image deblurring methods on both synthesized and natural images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yan_Image_Deblurring_via_CVPR_2017_paper.pdf",
        "aff": "State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences + University of Chinese Academy of Sciences, School of Cyber Security; State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences + Tianjin University, School of Computer Science and Technology; State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences; State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences; State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences + University of Chinese Academy of Sciences, School of Cyber Security",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yan_Image_Deblurring_via_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 915760,
        "gs_citation": 434,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18383052159020472187&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yan_Image_Deblurring_via_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+2;0;0;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Tianjin University",
        "aff_unique_dep": "State Key Laboratory of Information Security;School of Cyber Security;School of Computer Science and Technology",
        "aff_unique_url": "http://www.ucas.ac.cn;http://www.ucas.ac.cn;http://www.tju.edu.cn",
        "aff_unique_abbr": "CAS;UCAS;Tianjin U",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Image Splicing Detection via Camera Response Function Analysis",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2110",
        "author_site": "Can Chen, Scott McCloskey, Jingyi Yu",
        "author": "Can Chen; Scott McCloskey; Jingyi Yu",
        "abstract": "Recent advances on image manipulation techniques have made image forgery detection increasingly more challenging. An important component in such tools is to fake motion and/or defocus blurs through boundary splicing and copy-move operators, to emulate wide aperture and slow shutter effects. In this paper, we present a new technique based on the analysis of the camera response functions (CRF) for efficient and robust splicing and copy-move forgery detection and localization. We first analyze how non-linear CRFs affect edges in terms of the intensity-gradient bivariable histograms. We show distinguishable shape differences on real vs. forged blurs near edges after a splicing operation. Based on our analysis, we introduce a deep-learning framework to detect and localize forged edges. In particular, we show the problem can be transformed to a handwriting recognition problem an resolved by using a convolutional neural network. We generate a large dataset of forged images produced by splicing followed by retouching and comprehensive experiments show our proposed method outperforms the state-of-the-art techniques in accuracy and robustness.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Image_Splicing_Detection_CVPR_2017_paper.pdf",
        "aff": "University of Delaware; Honeywell ACS Labs; University of Delaware + ShanghaiTech University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Chen_Image_Splicing_Detection_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 794404,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16019953930728629342&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "udel.edu;honeywell.com;eecis.udel.edu",
        "email": "udel.edu;honeywell.com;eecis.udel.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Image_Splicing_Detection_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "University of Delaware;Honeywell;ShanghaiTech University",
        "aff_unique_dep": ";ACS Labs;",
        "aff_unique_url": "https://www.udel.edu;https://www.honeywell.com;https://www.shanghaitech.edu.cn",
        "aff_unique_abbr": "UD;;ShanghaiTech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Image Super-Resolution via Deep Recursive Residual Network",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1162",
        "author_site": "Ying Tai, Jian Yang, Xiaoming Liu",
        "author": "Ying Tai; Jian Yang; Xiaoming Liu",
        "abstract": "Recently, Convolutional Neural Network (CNN) based models have achieved great success in Single Image Super-Resolution (SISR). Owing to the strength of deep networks, these CNN models learn an effective nonlinear mapping from the low-resolution input image to the high-resolution target image, at the cost of requiring enormous parameters. This paper proposes a very deep CNN model (up to 52 convolutional layers) named Deep Recursive Residual Network (DRRN) that strives for deep yet concise networks. Specifically, residual learning is adopted, both in global and local manners, to mitigate the difficulty of training very deep networks; recursive learning is used to control the model parameters while increasing the depth. Extensive benchmark evaluation shows that DRRN significantly outperforms state of the art in SISR, while utilizing far fewer parameters. Code is available at https://github.com/tyshiwo/DRRN_CVPR17.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tai_Image_Super-Resolution_via_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science and Engineering, Nanjing University of Science and Technology; Department of Computer Science and Engineering, Nanjing University of Science and Technology; Department of Computer Science and Engineering, Michigan State University",
        "project": "",
        "github": "https://github.com/tyshiwo/DRRNCVPR17",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9169508,
        "gs_citation": 2749,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2094856323826769158&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "njust.edu.cn;njust.edu.cn;cse.msu.edu",
        "email": "njust.edu.cn;njust.edu.cn;cse.msu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tai_Image_Super-Resolution_via_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Nanjing University of Science and Technology;Michigan State University",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Computer Science and Engineering",
        "aff_unique_url": "http://www.nust.edu.cn;https://www.msu.edu",
        "aff_unique_abbr": "NJUST;MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Image-To-Image Translation With Conditional Adversarial Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "385",
        "author_site": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros",
        "author": "Phillip Isola; Jun-Yan Zhu; Tinghui Zhou; Alexei A. Efros",
        "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.07004v3",
        "pdf_size": 2010745,
        "gs_citation": 27229,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16757839449706651543&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.html"
    },
    {
        "title": "Improved Stereo Matching With Constant Highway Networks and Reflective Confidence Learning",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "1934",
        "author_site": "Amit Shaked, Lior Wolf",
        "author": "Amit Shaked; Lior Wolf",
        "abstract": "We present an improved three-step pipeline for the stereo matching problem and introduce multiple novelties at each stage. We propose a new highway network architecture for computing the matching cost at each possible disparity, based on multilevel weighted residual shortcuts, trained with a hybrid loss that supports multilevel comparison of image patches. A novel post-processing step is then introduced, which employs a second deep convolutional neural network for pooling global information from multiple disparities. This network outputs both the image disparity map, which replaces the conventional \"winner takes all\" strategy, and a confidence in the prediction. The confidence score is achieved by training the network with a new technique that we call the reflective loss. Lastly, the learned confidence is employed in order to better detect outliers in the refinement step. The proposed pipeline achieves state of the art accuracy on the largest and most competitive stereo benchmarks, and the learned confidence is shown to outperform all existing alternatives.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shaked_Improved_Stereo_Matching_CVPR_2017_paper.pdf",
        "aff": "The Blavatnik School of Computer Science, Tel Aviv University, Israel+Facebook AI Research; The Blavatnik School of Computer Science, Tel Aviv University, Israel+Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Shaked_Improved_Stereo_Matching_2017_CVPR_supplemental.pdf",
        "arxiv": "1701.00165v1",
        "pdf_size": 576178,
        "gs_citation": 259,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17466653390791837065&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shaked_Improved_Stereo_Matching_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Tel Aviv University;Meta",
        "aff_unique_dep": "Blavatnik School of Computer Science;Facebook AI Research",
        "aff_unique_url": "https://www.tau.ac.il;https://research.facebook.com",
        "aff_unique_abbr": "TAU;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tel Aviv;",
        "aff_country_unique_index": "0+1;0+1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "title": "Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3198",
        "author_site": "Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky",
        "author": "Dmitry Ulyanov; Andrea Vedaldi; Victor Lempitsky",
        "abstract": "The recent work of Gatys et al., who characterized the style of an image by the statistics of convolutional neural network filters, ignited a renewed interest in the texture generation and image stylization problems. While their image generation technique uses a slow optimization process, recently several authors have proposed to learn generator neural networks that can produce similar outputs in one quick forward pass. While generator networks are promising, they are still inferior in visual quality and diversity compared to generation-by-optimization. In this work, we advance them in two significant ways. First, we introduce an instance normalization module to replace batch normalization with significant improvements to the quality of image stylization. Second, we improve diversity by introducing a new learning formulation that encourages generators to sample unbiasedly from the Julesz texture ensemble, which is the equivalence class of all images characterized by certain filter responses. Together, these two improvements take feed forward texture synthesis and image stylization much closer to the quality of generation-via-optimization, while retaining the speed advantage.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ulyanov_Improved_Texture_Networks_CVPR_2017_paper.pdf",
        "aff": "Skolkovo Institute of Science and Technology & Yandex; University of Oxford; Skolkovo Institute of Science and Technology",
        "project": "",
        "github": "https://github.com/DmitryUlyanov/texture_nets",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ulyanov_Improved_Texture_Networks_2017_CVPR_supplemental.pdf",
        "arxiv": "1701.02096v2",
        "pdf_size": 2953188,
        "gs_citation": 1004,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15121556168732029899&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "skoltech.ru;robots.ox.ac.uk;skoltech.ru",
        "email": "skoltech.ru;robots.ox.ac.uk;skoltech.ru",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ulyanov_Improved_Texture_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Skolkovo Institute of Science and Technology;University of Oxford",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.skoltech.ru;https://www.ox.ac.uk",
        "aff_unique_abbr": "Skoltech;Oxford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Russian Federation;United Kingdom"
    },
    {
        "title": "Improving Facial Attribute Prediction Using Semantic Segmentation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3202",
        "author_site": "Mahdi M. Kalayeh, Boqing Gong, Mubarak Shah",
        "author": "Mahdi M. Kalayeh; Boqing Gong; Mubarak Shah",
        "abstract": "Attributes are semantically meaningful characteristics whose applicability widely crosses category boundaries. They are particularly important in describing and recognizing concepts where no explicit training example is given, e.g., zero-shot learning. Additionally, since attributes are human describable, they can be used for efficient human-computer interaction. In this paper, we propose to employ semantic segmentation to improve facial attribute prediction. The core idea lies in the fact that many facial attributes describe local properties. In other words, the probability of an attribute to appear in a face image is far from being uniform in the spatial domain. We build our facial attribute prediction model jointly with a deep semantic segmentation network. This harnesses the localization cues learned by the semantic segmentation to guide the attention of the attribute prediction to the regions where different attributes naturally show up. As a result of this approach, in addition to recognition, we are able to localize the attributes, despite merely having access to image level labels (weak supervision) during training. We evaluate our proposed method on CelebA  and LFWA datasets and achieve superior results to the prior arts. Furthermore, we show that in the reverse problem, semantic face parsing improves when facial attributes are available. That reaffirms the need to jointly model these two interconnected tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kalayeh_Improving_Facial_Attribute_CVPR_2017_paper.pdf",
        "aff": "Center for Research in Computer Vision; Center for Research in Computer Vision; Center for Research in Computer Vision",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.08740",
        "pdf_size": 1906694,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16825062565975321271&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "eecs.ucf.edu;crcv.ucf.edu;crcv.ucf.edu",
        "email": "eecs.ucf.edu;crcv.ucf.edu;crcv.ucf.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kalayeh_Improving_Facial_Attribute_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Center for Research in Computer Vision",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Improving Interpretability of Deep Neural Networks With Semantic Information",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1742",
        "author_site": "Yinpeng Dong, Hang Su, Jun Zhu, Bo Zhang",
        "author": "Yinpeng Dong; Hang Su; Jun Zhu; Bo Zhang",
        "abstract": "Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dong_Improving_Interpretability_of_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Dong_Improving_Interpretability_of_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.04096v2",
        "pdf_size": 2169098,
        "gs_citation": 178,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14637287970088216514&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dong_Improving_Interpretability_of_CVPR_2017_paper.html"
    },
    {
        "title": "Improving Pairwise Ranking for Multi-Label Image Classification",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1329",
        "author_site": "Yuncheng Li, Yale Song, Jiebo Luo",
        "author": "Yuncheng Li; Yale Song; Jiebo Luo",
        "abstract": "Learning to rank has recently emerged as an attractive technique to train deep convolutional neural networks for various computer vision tasks. Pairwise ranking, in particular, has been successful in multi-label image classification, achieving state-of-the-art results on various benchmarks. However, most existing approaches use the hinge loss to train their models, which is non-smooth and thus is difficult to optimize especially with deep networks. Furthermore, they employ simple heuristics, such as top-k or thresholding, to determine which labels to include in the output from a ranked list of labels, which limits their use in the real-world setting. In this work, we propose two techniques to improve pairwise ranking based multi-label image classification by solving the aforementioned problems: (1) we propose a novel loss function for pairwise ranking, which is smooth everywhere; and (2) we incorporate a label decision module into the model, estimating the optimal confidence thresholds for each visual concept. We provide theoretical analyses of our loss function from the point of view of the Bayes consistency and risk minimization, and show its benefit over existing pairwise ranking formulations. We also demonstrate the effectiveness of our approach on two large-scale datasets, NUS-WIDE and MS-COCO, achieving the best reported result in the literature.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Improving_Pairwise_Ranking_CVPR_2017_paper.pdf",
        "aff": "University of Rochester; Yahoo Research; University of Rochester",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03135v3",
        "pdf_size": 1387847,
        "gs_citation": 282,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8656759709591179368&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.rochester.edu;yahoo-inc.com;cs.rochester.edu",
        "email": "cs.rochester.edu;yahoo-inc.com;cs.rochester.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Improving_Pairwise_Ranking_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Rochester;Yahoo",
        "aff_unique_dep": ";Yahoo Research",
        "aff_unique_url": "https://www.rochester.edu;https://research.yahoo.com",
        "aff_unique_abbr": "U of R;Yahoo Research",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Improving RANSAC-Based Segmentation Through CNN Encapsulation",
        "session": "Biomedical Image/Video Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "2808",
        "author_site": "Dustin Morley, Hassan Foroosh",
        "author": "Dustin Morley; Hassan Foroosh",
        "abstract": "In this work, we present a method for improving a random sample consensus (RANSAC) based image segmentation algorithm by encapsulating it within a convolutional neural network (CNN). The improvements are gained by gradient descent training on the set of pre-RANSAC filtering and thresholding operations using a novel RANSAC-based loss function, which is geared toward optimizing the strength of the correct model relative to the most convincing false model. Thus, it can be said that our loss function trains the network on metrics that directly dictate the success or failure of the final segmentation rather than metrics that are merely correlated to the success or failure. We demonstrate successful application of this method to a RANSAC method for identifying the pupil boundary in images from the CASIA-IrisV3 iris recognition data set, and we expect that this method could be successfully applied to any RANSAC-based segmentation algorithm.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Morley_Improving_RANSAC-Based_Segmentation_CVPR_2017_paper.pdf",
        "aff": "University of Central Florida; University of Central Florida",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 798451,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8135221425867663250&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "knights.ucf.edu;cs.ucf.edu",
        "email": "knights.ucf.edu;cs.ucf.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Morley_Improving_RANSAC-Based_Segmentation_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Central Florida",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucf.edu",
        "aff_unique_abbr": "UCF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Improving Training of Deep Neural Networks via Singular Value Bounding",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1766",
        "author_site": "Kui Jia, Dacheng Tao, Shenghua Gao, Xiangmin Xu",
        "author": "Kui Jia; Dacheng Tao; Shenghua Gao; Xiangmin Xu",
        "abstract": "Deep learning methods achieve great success recently on many computer vision problems. In spite of these practical successes, optimization of deep networks remains an active topic in deep learning research. In this work, we focus on investigation of the network solution properties that can potentially lead to good performance. Our research is inspired by theoretical and empirical results that use orthogonal matrices to initialize networks, but we are interested in investigating how orthogonal weight matrices perform when network training converges. To this end, we propose to constrain the solutions of weight matrices in the orthogonal feasible set during the whole process of network training, and achieve this by a simple yet effective method called Singular Value Bounding (SVB). In SVB, all singular values of each weight matrix are simply bounded in a narrow band around the value of 1. Based on the same motivation, we also propose Bounded Batch Normalization (BBN), which improves Batch Normalization by removing its potential risk of ill-conditioned layer transform. We present both theoretical and empirical results to justify our proposed methods. Experiments on benchmark image classification datasets show the efficacy of our proposed SVB and BBN. In particular, we achieve the state-of-the-art results of 3.06% error rate on CIFAR10 and 16.90% on CIFAR100, using off-the-shelf network architectures (Wide ResNets). Our preliminary results on ImageNet also show the promise in large-scale learning. We release the implementation code of our methods at www.aperture-lab.net/research/svb.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jia_Improving_Training_of_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Jia_Improving_Training_of_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.06013",
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9047069023973963352&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jia_Improving_Training_of_CVPR_2017_paper.html"
    },
    {
        "title": "Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2967",
        "author_site": "Ting Yao, Yingwei Pan, Yehao Li, Tao Mei",
        "author": "Ting Yao; Yingwei Pan; Yehao Li; Tao Mei",
        "abstract": "Image captioning often requires a large set of training image-sentence pairs. In practice, however, acquiring sufficient training pairs is always expensive, making the recent captioning models limited in their ability to describe objects outside of training corpora (i.e., novel objects). In this paper, we present Long Short-Term Memory with Copying Mechanism (LSTM-C) --- a new architecture that incorporates copying into the Convolutional Neural Networks (CNN) plus Recurrent Neural Networks (RNN) image captioning framework, for describing novel objects in captions. Specifically, freely available object recognition datasets are leveraged to develop classifiers for novel objects. Our LSTM-C then nicely integrates the standard word-by-word sentence generation by a decoder RNN with copying mechanism which may instead select words from novel objects at proper places in the output sentence. Extensive experiments are conducted on both MSCOCO image captioning and ImageNet datasets, demonstrating the ability of our proposed LSTM-C architecture to describe novel objects. Furthermore, superior results are reported when compared to state-of-the-art deep models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yao_Incorporating_Copying_Mechanism_CVPR_2017_paper.pdf",
        "aff": "Microsoft Research, Beijing, China; University of Science and Technology of China, Hefei, China; Sun Yat-Sen University, Guangzhou, China; Microsoft Research, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1708.05271v1",
        "pdf_size": 942194,
        "gs_citation": 181,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4271466427993555575&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "microsoft.com;gmail.com;gmail.com;microsoft.com",
        "email": "microsoft.com;gmail.com;gmail.com;microsoft.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yao_Incorporating_Copying_Mechanism_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Microsoft;University of Science and Technology of China;Sun Yat-sen University",
        "aff_unique_dep": "Microsoft Research;;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-asia;http://www.ustc.edu.cn;http://www.sysu.edu.cn/",
        "aff_unique_abbr": "MSR;USTC;SYSU",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Beijing;Hefei;Guangzhou",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Incremental Kernel Null Space Discriminant Analysis for Novelty Detection",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "258",
        "author_site": "Juncheng Liu, Zhouhui Lian, Yi Wang, Jianguo Xiao",
        "author": "Juncheng Liu; Zhouhui Lian; Yi Wang; Jianguo Xiao",
        "abstract": "Novelty detection, which aims to determine whether a given data belongs to any category of training data or not, is considered to be an important and challenging problem in areas of Pattern Recognition, Machine Learning, etc. Recently, kernel null space method (KNDA) was reported to have state-of-the-art performance in novelty detection. However, KNDA is hard to scale up because of its high computational cost. With the ever-increasing size of data, accelerating the implementing speed of KNDA is desired and critical. Moreover, it becomes incapable when there exist successively injected data. To address these issues, we propose the Incremental Kernel Null Space based Discriminant Analysis (IKNDA) algorithm. The key idea is to extract new information brought by newly-added samples and integrate it with the existing model by an efficient updating scheme. Experiments conducted on two publicly-available datasets demonstrate that the proposed IKNDA yields comparable performance as the batch KNDA yet significantly reduces the computational complexity, and our IKNDA based novelty detection methods markedly outperform approaches using deep neural network (DNN) classifiers. This validates the superiority of our IKNDA against the state of the art in novelty detection for large-scale data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Incremental_Kernel_Null_CVPR_2017_paper.pdf",
        "aff": "Institute of Computer Science and Technology, Peking University, China; Institute of Computer Science and Technology, Peking University, China; School of Software, Dalian University of Technology, China+Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, China; Institute of Computer Science and Technology, Peking University, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Liu_Incremental_Kernel_Null_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3776984,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1380397092560035826&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "pku.edu.cn;pku.edu.cn;dlut.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;dlut.edu.cn;pku.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Incremental_Kernel_Null_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1+2;0",
        "aff_unique_norm": "Peking University;Dalian University of Technology;Key Laboratory for Ubiquitous Network and Service Software",
        "aff_unique_dep": "Institute of Computer Science and Technology;School of Software;Liaoning Province",
        "aff_unique_url": "http://www.pku.edu.cn;http://www.dlut.edu.cn;",
        "aff_unique_abbr": "Peking U;DUT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Indoor Scene Parsing With Instance Segmentation, Semantic Labeling and Support Relationship Inference",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2294",
        "author_site": "Wei Zhuo, Mathieu Salzmann, Xuming He, Miaomiao Liu",
        "author": "Wei Zhuo; Mathieu Salzmann; Xuming He; Miaomiao Liu",
        "abstract": "Over the years, indoor scene parsing has attracted a growing interest in the computer vision community. Existing methods have typically focused on diverse subtasks of this challenging problem. In particular, while some of them aim at segmenting the image into regions, such as object or surface instances, others aim at inferring the semantic labels of given regions, or their support relationships. These different tasks are typically treated as separate ones. However, they bear strong connections: good regions should respect the semantic labels; support can only be defined for meaningful regions; support relationships strongly depend on semantics. In this paper, we therefore introduce an approach to jointly segment the instances and infer their semantic labels and support relationships from a single input image. By exploiting a hierarchical segmentation, we formulate our problem as that of jointly finding  the regions in the hierarchy that correspond to instances and estimating their class labels and pairwise support relationships. We express this via a Markov Random Field, which allows us to further encode links between the different types of variables. Inference in this model can be done exactly via integer linear programming, and we learn its parameters in a structural SVM framework. Our experiments on NYUv2 demonstrate the benefits of reasoning jointly about all these subtasks of indoor scene parsing.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhuo_Indoor_Scene_Parsing_CVPR_2017_paper.pdf",
        "aff": "Australian National University, Canberra, Australia + Data61, CSIRO, Canberra, Australia; CVLab, EPFL, Switzerland; Australian National University, Canberra, Australia + Data61, CSIRO, Canberra, Australia; Australian National University, Canberra, Australia + Data61, CSIRO, Canberra, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2762894,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8883162128061276444&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "anu.edu.au;epfl.ch;data61.csiro.au;data61.csiro.au",
        "email": "anu.edu.au;epfl.ch;data61.csiro.au;data61.csiro.au",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhuo_Indoor_Scene_Parsing_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2;0+1;0+1",
        "aff_unique_norm": "Australian National University;CSIRO;EPFL",
        "aff_unique_dep": ";Data61;CVLab",
        "aff_unique_url": "https://www.anu.edu.au;https://www.csiro.au;https://cvlab.epfl.ch",
        "aff_unique_abbr": "ANU;CSIRO;EPFL",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Canberra;",
        "aff_country_unique_index": "0+0;1;0+0;0+0",
        "aff_country_unique": "Australia;Switzerland"
    },
    {
        "title": "Infinite Variational Autoencoder for Semi-Supervised Learning",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2542",
        "author_site": "M. Ehsan Abbasnejad, Anthony Dick, Anton van den Hengel",
        "author": "M. Ehsan Abbasnejad; Anthony Dick; Anton van den Hengel",
        "abstract": "This paper presents an infinite variational autoencoder (VAE) whose capacity adapts to suit the input data. This is achieved using a mixture model where the mixing coefficients are modeled by a Dirichlet process, allowing us to integrate over the coefficients when performing inference. Critically, this then allows us to automatically vary the number of autoencoders in the mixture based on the data. Experiments show the flexibility of our method, particularly for semi-supervised learning, where only a small number of training samples are available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Abbasnejad_Infinite_Variational_Autoencoder_CVPR_2017_paper.pdf",
        "aff": "The University of Adelaide; The University of Adelaide; The University of Adelaide",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Abbasnejad_Infinite_Variational_Autoencoder_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.07800",
        "pdf_size": 14387094,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7337188341514178184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Abbasnejad_Infinite_Variational_Autoencoder_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Adelaide",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Instance-Aware Image and Sentence Matching With Selective Multimodal LSTM",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "840",
        "author_site": "Yan Huang, Wei Wang, Liang Wang",
        "author": "Yan Huang; Wei Wang; Liang Wang",
        "abstract": "Effective image and sentence matching depends on how to well measure their global visual-semantic similarity. Based on the observation that such a global similarity arises from a complex aggregation of multiple local similarities between pairwise instances of image (objects) and sentence (words), we propose a selective multimodal Long Short-Term Memory network (sm-LSTM) for instance-aware image and sentence matching. The sm-LSTM includes a multimodal context-modulated attention scheme at each timestep that can selectively attend to a pair of instances of image and sentence, by predicting pairwise instance-aware saliency maps for image and sentence. For selected pairwise instances, their representations are obtained based on the predicted saliency maps, and then compared to measure their local similarity. By similarly measuring multiple local similarities within a few timesteps, the sm-LSTM sequentially aggregates them with hidden states to obtain a final matching score as the desired global similarity. Extensive experiments show that our model can well match image and sentence with complex content, and achieve the state-of-the-art results on two public benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Instance-Aware_Image_and_CVPR_2017_paper.pdf",
        "aff": "Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR) + University of Chinese Academy of Sciences (UCAS); Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR) + University of Chinese Academy of Sciences (UCAS); Center for Excellence in Brain Science and Intelligence Technology (CEBSIT), Institute of Automation, Chinese Academy of Sciences (CASIA) + University of Chinese Academy of Sciences (UCAS)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05588v1",
        "pdf_size": 1443503,
        "gs_citation": 279,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10054207428900117445&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Instance-Aware_Image_and_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;2+1",
        "aff_unique_norm": "National Laboratory of Pattern Recognition;University of Chinese Academy of Sciences;Chinese Academy of Sciences",
        "aff_unique_dep": "Center for Research on Intelligent Perception and Computing;;Institute of Automation",
        "aff_unique_url": ";http://www.ucas.ac.cn;http://www.ia.cas.cn",
        "aff_unique_abbr": "NLPR;UCAS;CASIA",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Instance-Level Salient Object Segmentation",
        "session": "Low- & Mid-Level Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "877",
        "author_site": "Guanbin Li, Yuan Xie, Liang Lin, Yizhou Yu",
        "author": "Guanbin Li; Yuan Xie; Liang Lin; Yizhou Yu",
        "abstract": "Image saliency detection has recently witnessed rapid progress due to deep convolutional neural networks. However, none of the existing methods is able to identify object instances in the detected salient regions. In this paper, we present a salient instance segmentation method that produces a saliency mask with distinct object instance labels for an input image. Our method consists of three steps, estimating saliency map, detecting salient object contours and identifying salient object instances. For the first two steps, we propose a multiscale saliency refinement network, which generates high-quality salient region masks and salient object contours. Once integrated with multiscale combinatorial grouping and a MAP-based subset optimization framework, our method can generate very promising salient object instance segmentation results. To promote further research and evaluation of salient instance segmentation, we also construct a new database of 1000 images and their pixelwise salient instance annotations. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks for salient region detection as well as on our new dataset for salient instance segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Instance-Level_Salient_Object_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Instance-Level_Salient_Object_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.03604v1",
        "gs_citation": 339,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6983359107805957213&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 21,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Instance-Level_Salient_Object_CVPR_2017_paper.html"
    },
    {
        "title": "InstanceCut: From Edges to Instances With MultiCut",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2064",
        "author_site": "Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bogdan Savchynskyy, Carsten Rother",
        "author": "Alexander Kirillov; Evgeny Levinkov; Bjoern Andres; Bogdan Savchynskyy; Carsten Rother",
        "abstract": "This work addresses the task of instance-aware semantic segmentation. Our key motivation is to design a simple method with a new modelling-paradigm, which therefore has a different trade-off between advantages and disadvantages compared to known approaches. Our approach, we term InstanceCut, represents the problem by two output modalities: (i) an instance-agnostic semantic segmentation and (ii) all instance-boundaries. The former is computed from a standard convolutional neural network for semantic segmentation, and the latter is derived from a new instance-aware edge detection model. To reason globally about the optimal partitioning of an image into instances, we combine these two modalities into a novel MultiCut formulation. We evaluate our approach on the challenging CityScapes dataset. Despite the conceptual simplicity of our approach, we achieve the best result among all published methods, and perform particularly well for rare object classes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kirillov_InstanceCut_From_Edges_CVPR_2017_paper.pdf",
        "aff": "TU Dresden, Dresden, Germany; MPI for Informatics, Saarbr\u00fccken, Germany; MPI for Informatics, Saarbr\u00fccken, Germany; TU Dresden, Dresden, Germany; TU Dresden, Dresden, Germany",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kirillov_InstanceCut_From_Edges_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.08272v1",
        "pdf_size": 2970351,
        "gs_citation": 324,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14049975847563398137&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "tu-dresden.de;mpi-inf.mpg.de;mpi-inf.mpg.de;tu-dresden.de;tu-dresden.de",
        "email": "tu-dresden.de;mpi-inf.mpg.de;mpi-inf.mpg.de;tu-dresden.de;tu-dresden.de",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kirillov_InstanceCut_From_Edges_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0;0",
        "aff_unique_norm": "Technische Universit\u00e4t Dresden;Max Planck Institute for Informatics",
        "aff_unique_dep": ";Informatics",
        "aff_unique_url": "https://www.tu-dresden.de;https://mpi-inf.mpg.de",
        "aff_unique_abbr": "TUD;MPII",
        "aff_campus_unique_index": "0;1;1;0;0",
        "aff_campus_unique": "Dresden;Saarbr\u00fccken",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "InterpoNet, a Brain Inspired Neural Network for Optical Flow Dense Interpolation",
        "session": "Theory",
        "status": "Poster",
        "track": "main",
        "pid": "1887",
        "author_site": "Shay Zweig, Lior Wolf",
        "author": "Shay Zweig; Lior Wolf",
        "abstract": "Sparse-to-dense interpolation for optical flow is a fundamental phase in the pipeline of most of the leading optical flow estimation algorithms. The current state-of-the-art method for interpolation, EpicFlow, is a local average method based on an edge aware geodesic distance. We propose a new data-driven sparse-to-dense interpolation algorithm based on a fully convolutional network. We draw inspiration from the filling-in process in the visual cortex and introduce lateral dependencies between neurons and multi-layer supervision into our learning process. We also show the importance of the image contour to the learning process. Our method is robust and outperforms EpicFlow on competitive optical flow benchmarks with several underlying matching algorithms. This leads to state-of-the-art performance on the Sintel and KITTI 2012 benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zweig_InterpoNet_a_Brain_CVPR_2017_paper.pdf",
        "aff": "The Gonda Multidisciplinary Brain Research Center, Bar Ilan University, Israel; The Blavatnik School of Computer Science, Tel Aviv University, Israel+Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zweig_InterpoNet_a_Brain_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.09803v3",
        "pdf_size": 1265338,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=794791370330659596&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;cs.tau.ac.il",
        "email": "gmail.com;cs.tau.ac.il",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zweig_InterpoNet_a_Brain_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Bar-Ilan University;Tel Aviv University;Meta",
        "aff_unique_dep": "Gonda Multidisciplinary Brain Research Center;Blavatnik School of Computer Science;Facebook AI Research",
        "aff_unique_url": "https://www.biu.ac.il;https://www.tau.ac.il;https://research.facebook.com",
        "aff_unique_abbr": "BIU;TAU;FAIR",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tel Aviv",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "title": "Interpretable Structure-Evolving LSTM",
        "session": "Machine Learning 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "347",
        "author_site": "Xiaodan Liang, Liang Lin, Xiaohui Shen, Jiashi Feng, Shuicheng Yan, Eric P. Xing",
        "author": "Xiaodan Liang; Liang Lin; Xiaohui Shen; Jiashi Feng; Shuicheng Yan; Eric P. Xing",
        "abstract": "This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liang_Interpretable_Structure-Evolving_LSTM_CVPR_2017_paper.pdf",
        "aff": "Carnegie Mellon University+Sun Yat-sen University; Sun Yat-sen University+SenseTime Group (Limited); Adobe Research; National University of Singapore; National University of Singapore; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.03055v1",
        "pdf_size": 1179680,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7396279849203119498&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.cmu.edu;ieee.org;adobe.com;nus.edu.sg;nus.edu.sg;cs.cmu.edu",
        "email": "cs.cmu.edu;ieee.org;adobe.com;nus.edu.sg;nus.edu.sg;cs.cmu.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liang_Interpretable_Structure-Evolving_LSTM_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1+2;3;4;4;0",
        "aff_unique_norm": "Carnegie Mellon University;Sun Yat-sen University;SenseTime Group;Adobe;National University of Singapore",
        "aff_unique_dep": ";;;Adobe Research;",
        "aff_unique_url": "https://www.cmu.edu;http://www.sysu.edu.cn/;https://www.sensetime.com;https://research.adobe.com;https://www.nus.edu.sg",
        "aff_unique_abbr": "CMU;SYSU;SenseTime;Adobe;NUS",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1+1;0;2;2;0",
        "aff_country_unique": "United States;China;Singapore"
    },
    {
        "title": "Interspecies Knowledge Transfer for Facial Keypoint Detection",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "3171",
        "author_site": "Maheen Rashid, Xiuye Gu, Yong Jae Lee",
        "author": "Maheen Rashid; Xiuye Gu; Yong Jae Lee",
        "abstract": "We present a method for localizing facial keypoints on animals by transferring knowledge gained from human faces. Instead of directly finetuning a network trained to detect keypoints on human faces to animal faces (which is sub-optimal since human and animal faces can look quite different), we propose to first adapt the animal images to the pre-trained human detection network by correcting for the differences in animal and human face shape. We first find the nearest human neighbors for each animal image using an unsupervised shape matching method. We use these matches to train a thin plate spline warping network to warp each animal face to look more human-like. The warping network is then jointly finetuned with a pre-trained human facial keypoint detection network using an animal dataset. We demonstrate state-of-the-art results on both horse and sheep facial keypoint detection, and significant improvement over simple finetuning, especially when training data is scarce.  Additionally, we present a new dataset with 3717 images with horse face and facial keypoint annotations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rashid_Interspecies_Knowledge_Transfer_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.04023",
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=356030283598594726&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Rashid_Interspecies_Knowledge_Transfer_CVPR_2017_paper.html"
    },
    {
        "title": "Intrinsic Grassmann Averages for Online Linear and Robust Subspace Learning",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2723",
        "author_site": "Rudrasis Chakraborty, S\u00c3\u00b8ren Hauberg, Baba C. Vemuri",
        "author": "Rudrasis Chakraborty; Soren Hauberg; Baba C. Vemuri",
        "abstract": "Principal Component Analysis (PCA) is a fundamental method for estimating a linear subspace approximation to high-dimensional data. Many algorithms exist in literature to achieve a statistically robust version of PCA called RPCA.  In this paper, we present a geometric framework for computing the principal linear subspaces in both situations that amounts to computing the intrinsic average on the space of all subspaces (the Grassmann manifold).  Points on this manifold are defined as the subspaces spanned by K-tuples of observations. We show that the intrinsic Grassmann average of these subspaces coincide with the principal components of the observations when they are drawn from a Gaussian distribution. Similar results are also shown to hold for the RPCA. Further, we propose an efficient online algorithm to do subspace averaging which is of linear complexity in terms of number of samples and has a linear convergence rate.  When the data has outliers, our proposed online robust subspace averaging algorithm shows significant performance (accuracy and computation time) gain over a recently published RPCA methods with publicly accessible code.  We have demonstrated competitive performance of our proposed online subspace algorithm method on one synthetic and two real data sets. Experimental results depicting stability of our proposed method are also presented. Furthermore, on two real outlier corrupted datasets, we present comparison experiments showing lower reconstruction error using our online RPCA algorithm.  In terms of reconstruction error and time required, both our algorithms outperform the competition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chakraborty_Intrinsic_Grassmann_Averages_CVPR_2017_paper.pdf",
        "aff": "Department of CISE, University of Florida, FL 32611, USA; Technical University of Denmark, Richard Petersens Plads, Denmark; Department of CISE, University of Florida, FL 32611, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.01005",
        "pdf_size": 2438260,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10235982717353703180&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "gmail.com;dtu.dk;gmail.com",
        "email": "gmail.com;dtu.dk;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chakraborty_Intrinsic_Grassmann_Averages_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Florida;Technical University of Denmark",
        "aff_unique_dep": "Department of CISE;",
        "aff_unique_url": "https://www.ufl.edu;https://www.teknologisk.dk",
        "aff_unique_abbr": "UF;DTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Gainesville;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Denmark"
    },
    {
        "title": "Inverse Compositional Spatial Transformer Networks",
        "session": "Machine Learning 2",
        "status": "Oral",
        "track": "main",
        "pid": "932",
        "author_site": "Chen-Hsuan Lin, Simon Lucey",
        "author": "Chen-Hsuan Lin; Simon Lucey",
        "abstract": "In this paper, we establish a theoretical connection between the classical Lucas & Kanade (LK) algorithm and the emerging topic of Spatial Transformer Networks (STNs). STNs are of interest to the vision and learning communities due to their natural ability to combine alignment and classification within the same theoretical framework. Inspired by the Inverse Compositional (IC) variant of the LK algorithm, we present Inverse Compositional Spatial Transformer Networks (IC-STNs). We demonstrate that IC-STNs can achieve better performance than conventional STNs with less model capacity; in particular, we show superior performance in pure image alignment tasks as well as joint alignment/classification problems on real-world problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Inverse_Compositional_Spatial_CVPR_2017_paper.pdf",
        "aff": "The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Lin_Inverse_Compositional_Spatial_2017_CVPR_supplemental.zip",
        "arxiv": "1612.03897v1",
        "pdf_size": 866503,
        "gs_citation": 185,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5521560331691147229&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff_domain": "andrew.cmu.edu;cs.cmu.edu",
        "email": "andrew.cmu.edu;cs.cmu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Inverse_Compositional_Spatial_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "The Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Joint Detection and Identification Feature Learning for Person Search",
        "session": "Analyzing Humans 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "1262",
        "author_site": "Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, Xiaogang Wang",
        "author": "Tong Xiao; Shuang Li; Bochao Wang; Liang Lin; Xiaogang Wang",
        "abstract": "Existing person re-identification benchmarks and methods mainly focus on matching cropped pedestrian images between queries and candidates. However, it is different from real-world scenarios where the annotations of pedestrian bounding boxes are unavailable and the target person needs to be searched from a gallery of whole scene images. To close the gap, we propose a new deep learning framework for person search. Instead of breaking it down into two separate tasks---pedestrian detection and person re-identification, we jointly handle both aspects in a single convolutional neural network. An Online Instance Matching (OIM) loss function is proposed to train the network effectively, which is scalable to datasets with numerous identities. To validate our approach, we collect and annotate a large-scale benchmark dataset for person search. It contains 18,184 images, 8,432 identities, and 96,143 pedestrian bounding boxes. Experiments show that our framework outperforms other separate approaches, and the proposed OIM loss function converges much faster and better than the conventional Softmax loss.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xiao_Joint_Detection_and_CVPR_2017_paper.pdf",
        "aff": "Shenzhen key lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China + The Chinese University of Hong Kong; The Chinese University of Hong Kong; Sun Yat-Sen University; Sun Yat-Sen University + SenseTime Group Limited; The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1604.01850v3",
        "pdf_size": 985056,
        "gs_citation": 1086,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2818455563202720828&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;mail2.sysu.edu.cn;ieee.org;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;mail2.sysu.edu.cn;ieee.org;ee.cuhk.edu.hk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xiao_Joint_Detection_and_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;2;2+3;1",
        "aff_unique_norm": "Shenzhen Institute of Advanced Technology;Chinese University of Hong Kong;Sun Yat-sen University;SenseTime Group Limited",
        "aff_unique_dep": "key lab of Computer Vision & Pattern Recognition;;;",
        "aff_unique_url": "http://www.siat.ac.cn;https://www.cuhk.edu.hk;http://www.sysu.edu.cn/;https://www.sensetime.com",
        "aff_unique_abbr": "SIAT;CUHK;SYSU;SenseTime",
        "aff_campus_unique_index": "0+1;1;;1",
        "aff_campus_unique": "Shenzhen;Hong Kong SAR;",
        "aff_country_unique_index": "0+0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Joint Discriminative Bayesian Dictionary and Classifier Learning",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "417",
        "author_site": "Naveed Akhtar, Ajmal Mian, Fatih Porikli",
        "author": "Naveed Akhtar; Ajmal Mian; Fatih Porikli",
        "abstract": "We propose to jointly learn a Discriminative Bayesian dictionary along a linear classifier using coupled Beta-Bernoulli Processes. Our representation model uses separate base measures for the dictionary and the classifier, but associates them to the class-specific training data using the same Bernoulli distributions. The Bernoulli distributions control the frequency with which the factors (e.g. dictionary atoms) are used in data representations, and they are inferred while accounting for the class labels in our approach. To further encourage discrimination in the dictionary, our model uses separate (sets of) Bernoulli distributions to represent data from different classes. Our approach adaptively learns the association between the dictionary atoms and the class labels while tailoring the classifier to this relation with a joint inference over the dictionary and the classifier. Once a test sample is represented over the dictionary, its representation is accurately labelled by the classifier due to the strong coupling between the dictionary and the classifier. We derive the Gibbs Sampling equations for our joint representation model and test our approach for face, object, scene and action recognition to establish its effectiveness.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Akhtar_Joint_Discriminative_Bayesian_CVPR_2017_paper.pdf",
        "aff": "Australian National University; University of Western Australia; Australian National University",
        "project": "http://staffhome.ecm.uwa.edu.au/~00053650/code.html",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Akhtar_Joint_Discriminative_Bayesian_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 624248,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5393069653998682720&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "anu.edu.au;uwa.edu.au;anu.edu.au",
        "email": "anu.edu.au;uwa.edu.au;anu.edu.au",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Akhtar_Joint_Discriminative_Bayesian_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Australian National University;University of Western Australia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.anu.edu.au;https://www.uwa.edu.au",
        "aff_unique_abbr": "ANU;UWA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Joint Gap Detection and Inpainting of Line Drawings",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "2442",
        "author_site": "Kazuma Sasaki, Satoshi Iizuka, Edgar Simo-Serra, Hiroshi Ishikawa",
        "author": "Kazuma Sasaki; Satoshi Iizuka; Edgar Simo-Serra; Hiroshi Ishikawa",
        "abstract": "We propose a novel data-driven approach for automatically detecting and completing gaps in line drawings with a Convolutional Neural Network. In the case of existing inpainting approaches for natural images, masks indicating the missing regions are generally required as input. Here, we show that line drawings have enough structures that can be learned by the CNN to allow automatic detection and completion of the gaps without any such input. Thus, our method can find the gaps in line drawings and complete them without user interaction. Furthermore, the completion realistically conserves thickness and curvature of the line segments. All the necessary heuristics for such realistic line completion are learned naturally from a dataset of line drawings, where various patterns of line completion are generated on the fly as training pairs to improve the model generalization. We evaluate our method qualitatively on a diverse set of challenging line drawings and also provide quantitative results with a user study, where it significantly outperforms the state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sasaki_Joint_Gap_Detection_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science and Engineering, Waseda University, Tokyo, Japan; Department of Computer Science and Engineering, Waseda University, Tokyo, Japan; Department of Computer Science and Engineering, Waseda University, Tokyo, Japan; Department of Computer Science and Engineering, Waseda University, Tokyo, Japan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1627668,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11908277585679936025&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ruri.waseda.jp;aoni.waseda.jp;aoni.waseda.jp;waseda.jp",
        "email": "ruri.waseda.jp;aoni.waseda.jp;aoni.waseda.jp;waseda.jp",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sasaki_Joint_Gap_Detection_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Waseda University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.waseda.jp/top",
        "aff_unique_abbr": "Waseda",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Joint Geometrical and Statistical Alignment for Visual Domain Adaptation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "692",
        "author_site": "Jing Zhang, Wanqing Li, Philip Ogunbona",
        "author": "Jing Zhang; Wanqing Li; Philip Ogunbona",
        "abstract": "This paper presents a novel unsupervised domain adaptation method for cross-domain visual recognition. We propose a unified framework that reduces the shift between domains both statistically and geometrically, referred to as Joint Geometrical and Statistical Alignment (JGSA). Specifically, we learn two coupled projections that project the source domain and target domain data into low-dimensional subspaces where the geometrical shift and distribution shift are reduced simultaneously. The objective function can be solved efficiently in a closed form. Extensive experiments have verified that the proposed method significantly outperforms several state-of-the-art domain adaptation methods on a synthetic dataset and three different real world cross-domain visual recognition tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Joint_Geometrical_and_CVPR_2017_paper.pdf",
        "aff": "Advanced Multimedia Research Lab, University of Wollongong, Australia; Advanced Multimedia Research Lab, University of Wollongong, Australia; Advanced Multimedia Research Lab, University of Wollongong, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.05498v1",
        "pdf_size": 1974046,
        "gs_citation": 677,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6740413594319912402&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "uowmail.edu.au;uow.edu.au;uow.edu.au",
        "email": "uowmail.edu.au;uow.edu.au;uow.edu.au",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Joint_Geometrical_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Wollongong",
        "aff_unique_dep": "Advanced Multimedia Research Lab",
        "aff_unique_url": "https://www.uow.edu.au",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Joint Graph Decomposition & Node Labeling: Problem, Algorithms, Applications",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2621",
        "author_site": "Evgeny Levinkov, Jonas Uhrig, Siyu Tang, Mohamed Omran, Eldar Insafutdinov, Alexander Kirillov, Carsten Rother, Thomas Brox, Bernt Schiele, Bjoern Andres",
        "author": "Evgeny Levinkov; Jonas Uhrig; Siyu Tang; Mohamed Omran; Eldar Insafutdinov; Alexander Kirillov; Carsten Rother; Thomas Brox; Bernt Schiele; Bjoern Andres",
        "abstract": "We state a combinatorial optimization problem whose feasible solutions define both a decomposition and a node labeling of a given graph. This problem offers a common mathematical abstraction of seemingly unrelated computer vision tasks, including instance-separating semantic segmentation, articulated human body pose estimation and multiple object tracking. Conceptually, it generalizes the unconstrained integer quadratic program and the minimum cost lifted multicut problem, both of which are NP-hard. In order to find feasible solutions efficiently, we define two local search algorithms that converge monotonously to a local optimum, offering a feasible solution at any time. To demonstrate the effectiveness of these algorithms in tackling computer vision tasks, we apply them to instances of the problem that we construct from published data, using published algorithms. We report state-of-the-art application-specific accuracy in the three above-mentioned applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Levinkov_Joint_Graph_Decomposition_CVPR_2017_paper.pdf",
        "aff": ";;;;;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Levinkov_Joint_Graph_Decomposition_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.04399",
        "pdf_size": 886708,
        "gs_citation": 131,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6420821501280711439&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "author_num": 10,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Levinkov_Joint_Graph_Decomposition_CVPR_2017_paper.html"
    },
    {
        "title": "Joint Intensity and Spatial Metric Learning for Robust Gait Recognition",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2430",
        "author_site": "Yasushi Makihara, Atsuyuki Suzuki, Daigo Muramatsu, Xiang Li, Yasushi Yagi",
        "author": "Yasushi Makihara; Atsuyuki Suzuki; Daigo Muramatsu; Xiang Li; Yasushi Yagi",
        "abstract": "This paper describes a joint intensity metric learning method to improve the robustness of gait recognition with silhouette-based descriptors such as gait energy images. Because existing methods often use the difference of image intensities between a matching pair (e.g., the absolute difference of gait energies for the l_1-norm) to measure a dissimilarity, large intrasubject differences derived from covariate conditions (e.g., large gait energies caused by carried objects vs. small gait energies caused by the background), may wash out subtle intersubject differences (e.g., the difference of middle-level gait energies derived from motion differences). We therefore introduce a metric on joint intensity to mitigate the large intrasubject differences as well as leverage the subtle intersubject differences. More specifically, we formulate the joint intensity and spatial metric learning in a unified framework and alternately optimize it by linear or ranking support vector machines. Experiments using the OU-ISIR treadmill data set B with the largest clothing variation and large population data set with bag, b version containing carrying status in the wild demonstrate the effectiveness of the proposed method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Makihara_Joint_Intensity_and_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Makihara_Joint_Intensity_and_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5021576143420017799&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Makihara_Joint_Intensity_and_CVPR_2017_paper.html"
    },
    {
        "title": "Joint Multi-Person Pose Estimation and Semantic Part Segmentation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3111",
        "author_site": "Fangting Xia, Peng Wang, Xianjie Chen, Alan L. Yuille",
        "author": "Fangting Xia; Peng Wang; Xianjie Chen; Alan L. Yuille",
        "abstract": "Human pose estimation and semantic part segmentation are two complementary tasks in computer vision. In this paper, we propose to solve the two tasks jointly for natural multi-person images, in which the estimated pose provides object-level shape prior to regularize part segments while the part-level segments constrain the variation of pose locations. Specifically, we first train two fully convolutional neural networks (FCNs), namely Pose FCN and Part FCN, to provide initial estimation of pose joint potential and semantic part potential. Then, to refine pose joint location, the two types of potentials are fused with a fully-connected conditional random field (FCRF), where a novel segment-joint smoothness term is used to encourage semantic and spatial consistency between parts and joints. To refine part segments, the refined pose and the original part potential are integrated through a Part FCN, where the skeleton feature from pose serves as additional regularization cues for part segments. Finally, to reduce the complexity of the FCRF, we induce human detection boxes and infer the graph inside each box, making the inference forty times faster.   Since there's no dataset that contains both part segments and pose labels, we extend the PASCAL VOC part dataset with human pose joints and perform extensive experiments to compare our method against several most recent strategies. We show that our algorithm surpasses competing methods by 10.6% in pose estimation with much faster speed and by 1.5% in semantic part segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xia_Joint_Multi-Person_Pose_CVPR_2017_paper.pdf",
        "aff": "University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles + Johns Hopkins University; Johns Hopkins University",
        "project": "https://sukixia.github.io/paper.html",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Xia_Joint_Multi-Person_Pose_2017_CVPR_supplemental.pdf",
        "arxiv": "1708.03383v1",
        "pdf_size": 2720729,
        "gs_citation": 282,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6201099075662039498&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 11,
        "aff_domain": "gmail.com;gmail.com;ucla.edu;jhu.edu",
        "email": "gmail.com;gmail.com;ucla.edu;jhu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xia_Joint_Multi-Person_Pose_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0+1;1",
        "aff_unique_norm": "University of California, Los Angeles;Johns Hopkins University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucla.edu;https://www.jhu.edu",
        "aff_unique_abbr": "UCLA;JHU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Joint Registration and Representation Learning for Unconstrained Face Identification",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1006",
        "author_site": "Munawar Hayat, Salman H. Khan, Naoufel Werghi, Roland Goecke",
        "author": "Munawar Hayat; Salman H. Khan; Naoufel Werghi; Roland Goecke",
        "abstract": "Recent advances in deep learning have resulted in human-level performances on popular unconstrained face datasets including Labeled Faces in the Wild and YouTube Faces. To further advance research, IJB-A benchmark was recently introduced with more challenges especially in the form of extreme head poses. Registration of such faces is quite demanding and often requires laborious procedures like facial landmark localization. In this paper, we propose a Convolutional Neural Networks based data-driven approach which learns to simultaneously register and represent faces. We validate the proposed scheme on template based unconstrained face identification. Here, a template contains multiple media in the form of images and video frames. Unlike existing methods which synthesize all template media information at feature level, we propose to keep the template media intact. Instead, we represent gallery templates by their trained one-vs-rest discriminative models and then employ a Bayesian strategy which optimally fuses decisions of all medias in a query template. We demonstrate the efficacy of the proposed scheme on IJB-A, YouTube Celebrities and COX datasets where our approach achieves significant relative performance boosts of 3.6%, 21.6% and 12.8% respectively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hayat_Joint_Registration_and_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4685686383990297891&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hayat_Joint_Registration_and_CVPR_2017_paper.html"
    },
    {
        "title": "Joint Sequence Learning and Cross-Modality Convolution for 3D Biomedical Segmentation",
        "session": "Biomedical Image/Video Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "2842",
        "author_site": "Kuan-Lun Tseng, Yen-Liang Lin, Winston Hsu, Chung-Yang Huang",
        "author": "Kuan-Lun Tseng; Yen-Liang Lin; Winston Hsu; Chung-Yang Huang",
        "abstract": "Deep learning models such as convolutional neural network have been widely used in 3D biomedical segmentation and achieve state-of-the-art performance. However, most of them often adapt a single modality or stack multiple modalities as different input channels, which ignores the correlations among them. To leverage the multi-modalities, we propose a deep convolution encoder-decoder structure with fusion layers to incorporate different modalities of MRI data. In addition, we exploit convolutional LSTM (convLSTM) to model a sequence of 2D slices, and jointly learn the multi-modalities and convLSTM in an end-to-end manner. To avoid converging to the certain labels, we adopt a re-weighting scheme and two phase training to handle the label imbalance. Experimental results on BRATS-2015 show that our method outperforms state-of-the-art biomedical segmentation approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tseng_Joint_Sequence_Learning_CVPR_2017_paper.pdf",
        "aff": "National Taiwan University; GE Global Research; National Taiwan University; National Taiwan University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.07754v1",
        "pdf_size": 787037,
        "gs_citation": 226,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11247827467808125858&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;gmail.com;ntu.edu.tw;ntu.edu.tw",
        "email": "gmail.com;gmail.com;ntu.edu.tw;ntu.edu.tw",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tseng_Joint_Sequence_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "National Taiwan University;GE Global Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.ge.com/research",
        "aff_unique_abbr": "NTU;GE Global Research",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Taiwan;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Jointly Learning Energy Expenditures and Activities Using Egocentric Multimodal Signals",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "694",
        "author_site": "Katsuyuki Nakamura, Serena Yeung, Alexandre Alahi, Li Fei-Fei",
        "author": "Katsuyuki Nakamura; Serena Yeung; Alexandre Alahi; Li Fei-Fei",
        "abstract": "Physiological signals such as heart rate can provide valuable information about an individual's state and activity. However, existing work on computer vision has not yet explored leveraging these signals to enhance egocentric video understanding. In this work, we propose a model for reasoning on multimodal data to jointly predict activities and energy expenditures. We use heart rate signals as privileged self-supervision to derive energy expenditure in a training stage. A multitask objective is used to jointly optimize the two tasks. Additionally, we introduce a dataset that contains 31 hours of egocentric video augmented with heart rate and acceleration signals. This study can lead to new applications such as a visual calorie counter.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Nakamura_Jointly_Learning_Energy_CVPR_2017_paper.pdf",
        "aff": "Hitachi, Ltd.+Stanford University; Stanford University; Stanford University; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6392141,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3095735516866339791&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "hitachi.com;cs.stanford.edu;stanford.edu;cs.stanford.edu",
        "email": "hitachi.com;cs.stanford.edu;stanford.edu;cs.stanford.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Nakamura_Jointly_Learning_Energy_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;1;1",
        "aff_unique_norm": "Hitachi, Ltd.;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.hitachi.com;https://www.stanford.edu",
        "aff_unique_abbr": "Hitachi;Stanford",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0+1;1;1;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "title": "Kernel Pooling for Convolutional Neural Networks",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1064",
        "author_site": "Yin Cui, Feng Zhou, Jiang Wang, Xiao Liu, Yuanqing Lin, Serge Belongie",
        "author": "Yin Cui; Feng Zhou; Jiang Wang; Xiao Liu; Yuanqing Lin; Serge Belongie",
        "abstract": "Convolutional Neural Networks (CNNs) with Bilinear Pooling, initially in their full form and later using compact representations, have yielded impressive performance gains on a wide range of visual tasks, including fine-grained visual categorization, visual question answering, face recognition, and description of texture and style. The key to their success lies in the spatially invariant modeling of pairwise (2nd order) feature interactions. In this work, we propose a general pooling framework that captures higher order interactions of features in the form of kernels. We demonstrate how to approximate kernels such as Gaussian RBF up to a given order using compact explicit feature maps in a parameter-free manner. Combined with CNNs, the composition of the kernel can be learned from data in an end-to-end fashion via error back-propagation.  The proposed kernel pooling scheme is evaluated in terms of both kernel approximation error and visual recognition accuracy. Experimental evaluations demonstrate state-of-the-art performance on commonly used fine-grained recognition datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cui_Kernel_Pooling_for_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, Cornell University + Cornell Tech; Baidu Research; Google Research; Baidu Research; Baidu Research; Department of Computer Science, Cornell University + Cornell Tech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1518033,
        "gs_citation": 405,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14986083936213449450&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.cornell.edu;cs.cornell.edu;gmail.com;baidu.com;baidu.com; ",
        "email": "cs.cornell.edu;cs.cornell.edu;gmail.com;baidu.com;baidu.com; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cui_Kernel_Pooling_for_CVPR_2017_paper.html",
        "aff_unique_index": "0+0;1;2;1;1;0+0",
        "aff_unique_norm": "Cornell University;Baidu;Google",
        "aff_unique_dep": "Department of Computer Science;Baidu Research;Google Research",
        "aff_unique_url": "https://www.cornell.edu;https://research.baidu.com;https://research.google",
        "aff_unique_abbr": "Cornell;Baidu;Google Research",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";New York City;Mountain View",
        "aff_country_unique_index": "0+0;1;0;1;1;0+0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Kernel Square-Loss Exemplar Machines for Image Retrieval",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "882",
        "author_site": "Rafael S. Rezende, Joaquin Zepeda, Jean Ponce, Francis Bach, Patrick P\u00c3\u00a9rez",
        "author": "Rafael S. Rezende; Joaquin Zepeda; Jean Ponce; Francis Bach; Patrick Perez",
        "abstract": "Zepeda and Perez have recently demonstrated the promise of the exemplar SVM (ESVM) as a feature encoder for image retrieval. This paper extends this approach in several directions: We first show that replacing the hinge loss by the square loss in the ESVM cost function significantly reduces encoding time with negligible effect on accuracy. We call this model square-loss exemplar machine, or SLEM.  We then introduce a kernelized SLEM which can be implemented efficiently through low-rank matrix decomposition, and displays improved performance.  Both SLEM variants exploit the fact that the negative examples are fixed, so most of the SLEM computational complexity is relegated to an offline process independent of the positive examples.  Our experiments establish the performance and computational advantages of our approach using a large array of base features and standard image retrieval datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rezende_Kernel_Square-Loss_Exemplar_CVPR_2017_paper.pdf",
        "aff": "Inria Paris; Technicolor+Amazon; Inria Paris+\u00c9cole Normale Sup\u00e9rieure/PSL Research University; Inria Paris; Technicolor",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 527052,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6331920790602609661&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Rezende_Kernel_Square-Loss_Exemplar_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+2;0+3;0;1",
        "aff_unique_norm": "INRIA;Technicolor;Amazon;\u00c9cole Normale Sup\u00e9rieure",
        "aff_unique_dep": ";;Amazon.com, Inc.;",
        "aff_unique_url": "https://www.inria.fr;https://www.technicolor.com;https://www.amazon.com;https://www.ens.fr",
        "aff_unique_abbr": "Inria;Tec;Amazon;ENS",
        "aff_campus_unique_index": "0;;0;0",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0;0+1;0+0;0;0",
        "aff_country_unique": "France;United States"
    },
    {
        "title": "KillingFusion: Non-Rigid 3D Reconstruction Without Correspondences",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "517",
        "author_site": "Miroslava Slavcheva, Maximilian Baust, Daniel Cremers, Slobodan Ilic",
        "author": "Miroslava Slavcheva; Maximilian Baust; Daniel Cremers; Slobodan Ilic",
        "abstract": "We introduce a geometry-driven approach for real-time 3D reconstruction of deforming surfaces from a single RGB-D stream without any templates or shape priors. To this end, we tackle the problem of non-rigid registration by level set evolution without explicit correspondence search. Given a pair of signed distance fields (SDFs) representing the shapes of interest, we estimate a dense deformation field that aligns them. It is defined as a displacement vector field of the same resolution as the SDFs and is determined iteratively via variational minimization. To ensure it generates plausible shapes, we propose a novel regularizer that imposes local rigidity by requiring the deformation to be a smooth and approximately Killing vector field, i.e. generating nearly isometric motions. Moreover, we enforce that the level set property of unity gradient magnitude is preserved over iterations. As a result, KillingFusion reliably reconstructs objects that are undergoing topological changes and fast inter-frame motion. In addition to incrementally building a model from scratch, our system can also deform complete surfaces. We demonstrate these capabilities on several public datasets and introduce our own sequences that permit both qualitative and quantitative comparison to related approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Slavcheva_KillingFusion_Non-Rigid_3D_CVPR_2017_paper.pdf",
        "aff": "Technische Universit \u00a8at M \u00a8unchen+Siemens Corporate Technology; Technische Universit \u00a8at M \u00a8unchen; Technische Universit \u00a8at M \u00a8unchen; Technische Universit \u00a8at M \u00a8unchen+Siemens Corporate Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Slavcheva_KillingFusion_Non-Rigid_3D_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2848298,
        "gs_citation": 210,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2376987951542776116&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Slavcheva_KillingFusion_Non-Rigid_3D_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;0+1",
        "aff_unique_norm": "Technische Universit\u00e4t M\u00fcnchen;Siemens AG",
        "aff_unique_dep": ";Corporate Technology",
        "aff_unique_url": "https://www.tum.de;https://www.siemens.com",
        "aff_unique_abbr": "TUM;Siemens",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "133",
        "author_site": "Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher",
        "author": "Jiasen Lu; Caiming Xiong; Devi Parikh; Richard Socher",
        "abstract": "Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as \"the\" and \"of\". Other words that may seem visual can often be predicted reliably just from the language model e.g., \"sign\" after \"behind a red stop\" or \"phone\" following \"talking on a cell\". In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Knowing_When_to_CVPR_2017_paper.pdf",
        "aff": "Virginia Tech; Salesforce Research; Georgia Institute of Technology; Salesforce Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.01887v2",
        "pdf_size": 4295555,
        "gs_citation": 2002,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3919912098681596308&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "vt.edu;salesforce.com;gatech.edu;salesforce.com",
        "email": "vt.edu;salesforce.com;gatech.edu;salesforce.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lu_Knowing_When_to_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Virginia Tech;Salesforce;Georgia Institute of Technology",
        "aff_unique_dep": ";Salesforce Research;",
        "aff_unique_url": "https://www.vt.edu;https://research.salesforce.com;https://www.gatech.edu",
        "aff_unique_abbr": "VT;Salesforce;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Knowledge Acquisition for Visual Question Answering via Iterative Querying",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "397",
        "author_site": "Yuke Zhu, Joseph J. Lim, Li Fei-Fei",
        "author": "Yuke Zhu; Joseph J. Lim; Li Fei-Fei",
        "abstract": "Humans possess an extraordinary ability to learn new skills and new knowledge for problem solving. Such learning ability is also required by an automatic model to deal with arbitrary, open-ended questions in the visual world. We propose a neural-based approach to acquiring task- driven information for visual question answering (VQA). Our model proposes queries to actively acquire relevant information from external auxiliary data. Supporting evidence from either human-curated or automatic sources is encoded and stored into a memory bank. We show that acquiring task-driven evidence effectively improves model performance on both the Visual7W and VQA datasets; moreover, these queries offer certain level of interpretability in our iterative QA model.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Knowledge_Acquisition_for_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, University of Southern California; Department of Computer Science, Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1580351,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10992270671604613331&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhu_Knowledge_Acquisition_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Stanford University;University of Southern California",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu;https://www.usc.edu",
        "aff_unique_abbr": "Stanford;USC",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Stanford;Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "L2-Net: Deep Learning of Discriminative Patch Descriptor in Euclidean Space",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "203",
        "author_site": "Yurun Tian, Bin Fan, Fuchao Wu",
        "author": "Yurun Tian; Bin Fan; Fuchao Wu",
        "abstract": "The research focus of designing local patch descriptors has gradually shifted from handcrafted ones (e.g., SIFT) to learned ones. In this paper, we propose to learn high per- formance descriptor in Euclidean space via the Convolu- tional Neural Network (CNN). Our method is distinctive in four aspects: (i) We propose a progressive sampling strat- egy which enables the network to access billions of train- ing samples in a few epochs. (ii) Derived from the ba- sic concept of local patch matching problem, we empha- size the relative distance between descriptors. (iii) Extra supervision is imposed on the intermediate feature maps. (iv) Compactness of the descriptor is taken into account. The proposed network is named as L2-Net since the out- put descriptor can be matched in Euclidean space by L2 distance. L2-Net achieves state-of-the-art performance on the Brown datasets [16], Oxford dataset [18] and the new- ly proposed Hpatches dataset [11]. The good generaliza- tion ability shown by experiments indicates that L2-Net can serve as a direct substitution of the existing handcrafted de- scriptors. The pre-trained L2-Net is publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tian_L2-Net_Deep_Learning_CVPR_2017_paper.pdf",
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Science, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China",
        "project": "",
        "github": "https://github.com/yuruntian/L2-Net",
        "supp": "",
        "arxiv": "",
        "pdf_size": 648447,
        "gs_citation": 698,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18331144065729754008&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tian_L2-Net_Deep_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "LCNN: Lookup-Based Convolutional Neural Network",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3345",
        "author_site": "Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi",
        "author": "Hessam Bagherinezhad; Mohammad Rastegari; Ali Farhadi",
        "abstract": "Porting state of the art deep learning algorithms to resource constrained compute platforms (e.g. VR, AR, wearables) is extremely challenging. We propose a fast, compact, and accurate model for convolutional neural networks that enables efficient learning and inference. We introduce LCNN, a lookup-based convolutional neural network that encodes convolutions by few lookups to a dictionary that is trained to cover the space of weights in CNNs. Training LCNN involves jointly learning a dictionary and a small set of linear combinations. The size of the dictionary naturally traces a spectrum of trade-offs between efficiency and accuracy. Our experimental results on ImageNet challenge show that LCNN can offer 3.2x speedup while achieving 55.1% top-1 accuracy using AlexNet architecture. Our fastest LCNN offers 37.6x speed up over AlexNet while maintaining 44.3% top-1 accuracy. LCNN not only offers dramatic speed ups at inference, but it also enables efficient training. In this paper, we show the benefits of LCNN in few-shot learning and few-iteration learning, two crucial aspects of on-device training of deep learning models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bagherinezhad_LCNN_Lookup-Based_Convolutional_CVPR_2017_paper.pdf",
        "aff": "University of Washington+XNOR.AI+Allen Institute for AI; XNOR.AI+Allen Institute for AI; University of Washington+XNOR.AI+Allen Institute for AI",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Bagherinezhad_LCNN_Lookup-Based_Convolutional_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.06473v2",
        "pdf_size": 764083,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10119503520040492401&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "xnor.ai;xnor.ai;xnor.ai",
        "email": "xnor.ai;xnor.ai;xnor.ai",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bagherinezhad_LCNN_Lookup-Based_Convolutional_CVPR_2017_paper.html",
        "aff_unique_index": "0+1+2;1+2;0+1+2",
        "aff_unique_norm": "University of Washington;XNOR.AI;Allen Institute for AI",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.washington.edu;https://xnor.ai;https://allenai.org",
        "aff_unique_abbr": "UW;XNOR.AI;AI2",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0;0+0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "LCR-Net: Localization-Classification-Regression for Human Pose",
        "session": "Analyzing Humans 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "1270",
        "author_site": "Gr\u00c3\u00a9gory Rogez, Philippe Weinzaepfel, Cordelia Schmid",
        "author": "Gregory Rogez; Philippe Weinzaepfel; Cordelia Schmid",
        "abstract": "We propose an end-to-end architecture for joint 2D and 3D human pose estimation in natural images. Key to our approach is the generation and scoring of a number of pose proposals per image, which allows us to predict 2D and 3D pose of multiple people simultaneously. Hence, our approach does not require an approximate localization of the humans for initialization. Our architecture, named LCR-Net, contains 3 main components: 1) the pose proposal generator that suggests potential poses at different locations in the image; 2) a classifier that scores the different pose proposals; and 3) a regressor that refines pose proposals both in 2D and 3D. All three stages share the convolutional feature layers and are trained jointly. The final pose estimation is obtained by integrating over neighboring pose hypotheses, which is shown to improve over a standard non maximum suppression algorithm. Our approach significantly outperforms the state of the art in 3D pose estimation on Human3.6M, a controlled environment. Moreover, it shows promising results on real images for both single and multi-person subsets of the MPII 2D pose benchmark.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rogez_LCR-Net_Localization-Classification-Regression_for_CVPR_2017_paper.pdf",
        "aff": "Inria\u2217; Xerox Research Centre Europe; Inria\u2217",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1399802,
        "gs_citation": 399,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5882225221066441347&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Rogez_LCR-Net_Localization-Classification-Regression_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "INRIA;Xerox Research Centre Europe",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.inria.fr;https://www.xerox.com/research-centre-europe.html",
        "aff_unique_abbr": "Inria;XRCE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "France;Unknown"
    },
    {
        "title": "LSTM Self-Supervision for Detailed Behavior Analysis",
        "session": "Biomedical Image/Video Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "2881",
        "author_site": "Biagio Brattoli, Uta B\u00c3\u00bcchler, Anna-Sophia Wahl, Martin E. Schwab, Bj\u00c3\u00b6rn Ommer",
        "author": "Biagio Brattoli; Uta Buchler; Anna-Sophia Wahl; Martin E. Schwab; Bjorn Ommer",
        "abstract": "Behavior analysis provides a crucial non-invasive and easily accessible diagnostic tool for biomedical research. A detailed analysis of posture changes during skilled motor tasks can reveal distinct functional deficits and their restoration during recovery. Our specific scenario is based on a neuroscientific study of rodents recovering from a large sensorimotor cortex stroke and skilled forelimb grasping is being recorded. Given large amounts of unlabeled videos that are recorded during such long-term studies, we seek an approach that captures fine-grained details of posture and its change during rehabilitation without costly manual supervision. Therefore, we utilize self-supervision to automatically learn accurate posture and behavior representations for analyzing motor function. Learning our model depends on the following fundamental elements: (i) limb detection based on a fully convolutional network is ini- tialized solely using motion information, (ii) a novel self- supervised training of LSTMs using only temporal permu- tation yields a detailed representation of behavior, and (iii) back-propagation of this sequence representation also im- proves the description of individual postures. We establish a novel test dataset with expert annotations for evaluation of fine-grained behavior analysis. Moreover, we demonstrate the generality of our approach by successfully applying it to self-supervised learning of human posture on two standard benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Brattoli_LSTM_Self-Supervision_for_CVPR_2017_paper.pdf",
        "aff": "HCI / IWR, Heidelberg University, Germany; HCI / IWR, Heidelberg University, Germany; Department of HST, ETH Zurich, Switzerland; Department of HST, ETH Zurich, Switzerland; HCI / IWR, Heidelberg University, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4831994,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14934291589558402615&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de;hifo.uzh.ch;hifo.uzh.ch;iwr.uni-heidelberg.de",
        "email": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de;hifo.uzh.ch;hifo.uzh.ch;iwr.uni-heidelberg.de",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Brattoli_LSTM_Self-Supervision_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Heidelberg University;ETH Zurich",
        "aff_unique_dep": "Human-Computer Interaction / Interdisciplinary Working Group on Research;Department of HST",
        "aff_unique_url": "https://www.uni-heidelberg.de;https://www.ethz.ch",
        "aff_unique_abbr": "Uni HD;ETHZ",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Heidelberg;",
        "aff_country_unique_index": "0;0;1;1;0",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "title": "Large Kernel Matters -- Improve Semantic Segmentation by Global Convolutional Network",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1770",
        "author_site": "Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, Jian Sun",
        "author": "Chao Peng; Xiangyu Zhang; Gang Yu; Guiming Luo; Jian Sun",
        "abstract": "Convolution Neural Network (CNN) has boosted the per- formanceofalotofcomputervisiontasks, likeimageclassi- fication [31], segmentation [25], and detection [28]. Based on the observations from [31, 32, 14], recent model design- ers prefer to employ stacking of small kernels, like 3 x 3 over large-size filters. However, in the field of semantic seg- mentation, where we need to perform dense per-pixel pre- diction, we find that large kernel plays an important role to relieve the contradictories when optimizing the classi- fication and localization tasks simultaneously. Following the design principle of large-size kernel, We propose the Global Convolutional Network to address both the classi- fication and localization issue in the semantic segmentation task. To further refine the object category boundaries, we presentBoundaryRefinementblockbasedonresidualstruc- ture. Qualitatively, our model achieves state-of-art perfor- mance on two public benchmarks and outperforms previous results on a large margin, 82.2% (vs 80.2%) on PASCAL VOC 2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Peng_Large_Kernel_Matters_CVPR_2017_paper.pdf",
        "aff": "School of Software, Tsinghua University; Megvii Inc. (Face++) + School of Software, Tsinghua University; Megvii Inc. (Face++) + School of Software, Tsinghua University; Megvii Inc. (Face++) + School of Software, Tsinghua University; Megvii Inc. (Face++)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.02719",
        "pdf_size": 1867866,
        "gs_citation": 2037,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17980729144178402832&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "mails.tsinghua.edu.cn;megvii.com;megvii.com;tsinghua.edu.cn;megvii.com",
        "email": "mails.tsinghua.edu.cn;megvii.com;megvii.com;tsinghua.edu.cn;megvii.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Peng_Large_Kernel_Matters_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+0;1+0;1+0;1",
        "aff_unique_norm": "Tsinghua University;Megvii Inc.;",
        "aff_unique_dep": "School of Software;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.megvii.com;",
        "aff_unique_abbr": "THU;Megvii;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0;0+0;0",
        "aff_country_unique": "China;"
    },
    {
        "title": "Large Margin Object Tracking With Circulant Feature Maps",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "1608",
        "author_site": "Mengmeng Wang, Yong Liu, Zeyi Huang",
        "author": "Mengmeng Wang; Yong Liu; Zeyi Huang",
        "abstract": "Structured output support vector machine (SVM) based tracking algorithms have shown favorable performance recently. Nonetheless, the time-consuming candidate sampling and complex optimization limit their real-time applications. In this paper, we propose a novel large margin object tracking method which absorbs the strong discriminative ability from structured output SVM and speeds up by the correlation filter algorithm significantly. Secondly, a multimodal target detection technique is proposed to improve the target localization precision and prevent model drift introduced by similar objects or background noise. Thirdly, we exploit the feedback from high-confidence tracking results to avoid the model corruption problem. We implement two versions of the proposed tracker with the representations from both conventional hand-crafted and deep convolution neural networks (CNNs) based features to validate the strong compatibility of the algorithm. The experimental results demonstrate that the proposed tracker performs superiorly against several state-of-the-art algorithms on the challenging benchmark sequences while runs at speed in excess of 80 frames per second.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Large_Margin_Object_CVPR_2017_paper.pdf",
        "aff": "Institute of Cyber-Systems and Control, Zhejiang University; Institute of Cyber-Systems and Control, Zhejiang University; Exacloud Limited, Zhejiang, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.05020v2",
        "pdf_size": 1322163,
        "gs_citation": 801,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18138603703196866305&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "zju.edu.cn;iipc.zju.edu.cn;qunhemail.com",
        "email": "zju.edu.cn;iipc.zju.edu.cn;qunhemail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Large_Margin_Object_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Zhejiang University;Exacloud Limited",
        "aff_unique_dep": "Institute of Cyber-Systems and Control;",
        "aff_unique_url": "http://www.zju.edu.cn;",
        "aff_unique_abbr": "ZJU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Latent Multi-View Subspace Clustering",
        "session": "Machine Learning 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "1728",
        "author_site": "Changqing Zhang, Qinghua Hu, Huazhu Fu, Pengfei Zhu, Xiaochun Cao",
        "author": "Changqing Zhang; Qinghua Hu; Huazhu Fu; Pengfei Zhu; Xiaochun Cao",
        "abstract": "In this paper, we propose a novel Latent Multi-view Subspace Clustering (LMSC) method, which clusters data points with latent representation and simultaneously explores underlying complementary information from multiple views. Unlike most existing single view subspace clustering methods that reconstruct data points using original features, our method seeks the underlying latent representation and simultaneously performs data reconstruction based on the learned latent representation.  With the complementarity of multiple views, the latent representation could depict data themselves more comprehensively than each single view individually, accordingly makes subspace representation more accurate and robust as well. The proposed method is intuitive and can be optimized efficiently by using the Augmented Lagrangian Multiplier with Alternating Direction Minimization (ALM-ADM) algorithm. Extensive experiments on benchmark datasets have validated the effectiveness of our proposed method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Latent_Multi-View_Subspace_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science and Technology, Tianjin University; School of Computer Science and Technology, Tianjin University; Institute for Infocomm Research, Agency for Science, Technology and Research; School of Computer Science and Technology, Tianjin University; State Key Laboratory of Information Security, IIE, Chinese Academy of Sciences + School of Cyber Security, University of Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1008581,
        "gs_citation": 631,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3842651598417617536&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tju.edu.cn;tju.edu.cn; ; ; ",
        "email": "tju.edu.cn;tju.edu.cn; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Latent_Multi-View_Subspace_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0;2+3",
        "aff_unique_norm": "Tianjin University;Agency for Science, Technology and Research;Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "School of Computer Science and Technology;Institute for Infocomm Research;State Key Laboratory of Information Security;School of Cyber Security",
        "aff_unique_url": "http://www.tju.edu.cn;https://www.a-star.edu.sg;http://www.ucas.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "Tianjin University;A*STAR;CAS;UCAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0+0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "title": "Lean Crowdsourcing: Combining Humans and Machines in an Online System",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3875",
        "author_site": "Steve Branson, Grant Van Horn, Pietro Perona",
        "author": "Steve Branson; Grant Van Horn; Pietro Perona",
        "abstract": "We introduce a method to greatly reduce the amount of redundant annotations required when crowdsourcing annotations such as bounding boxes, parts, and class labels. For example, if two Mechanical Turkers happen to click on the same pixel location when annotating a part in a given image--an event that is very unlikely to occur by random chance--, it is a strong indication that the location is correct. A similar type of confidence can be obtained if a single Turker happened to agree with a computer vision estimate. We thus incrementally collect a variable number of worker annotations per image based on online estimates of confidence. This is done using a sequential estimation of risk over a probabilistic model that combines worker skill, image difficulty, and an incrementally trained computer vision model. We develop specialized models and algorithms for binary annotation, part keypoint annotation, and sets of bounding box annotations. We show that our method can reduce annotation time by a factor of 4-11 for binary filtering of websearch results, 2-4 for annotation of boxes of pedestrians in images, while in many cases also reducing annotation error. We will make an end-to-end version of our system publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Branson_Lean_Crowdsourcing_Combining_CVPR_2017_paper.pdf",
        "aff": "Caltech; Caltech; Caltech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1115221,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9183797117290148044&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "caltech.edu;caltech.edu;caltech.edu",
        "email": "caltech.edu;caltech.edu;caltech.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Branson_Lean_Crowdsourcing_Combining_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "California Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.caltech.edu",
        "aff_unique_abbr": "Caltech",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pasadena",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learned Contextual Feature Reweighting for Image Geo-Localization",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "780",
        "author_site": "Hyo Jin Kim, Enrique Dunn, Jan-Michael Frahm",
        "author": "Hyo Jin Kim; Enrique Dunn; Jan-Michael Frahm",
        "abstract": "We address the problem of large scale image geo-localization where the location of an image is estimated by identifying geo-tagged reference images depicting the same place. We propose a novel model for learning image representations that integrates context-aware feature reweighting in order to effectively focus on regions that positively contribute to geo-localization. In particular, we introduce a Contextual Reweighting Network (CRN) that predicts the importance of each region in the feature map based on the image context. Our model is learned end-to-end for the image geo-localization task, and requires no annotation other than image geo-tags for training. In experimental results, the proposed approach significantly outperforms the previous state-of-the-art on the standard geo-localization benchmark datasets. We also demonstrate that our CRN discovers task-relevant contexts without any additional supervision.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kim_Learned_Contextual_Feature_CVPR_2017_paper.pdf",
        "aff": "UNC Chapel Hill; Stevens Institute of Technology; UNC Chapel Hill",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kim_Learned_Contextual_Feature_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1857692,
        "gs_citation": 276,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=537197948328040855&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.unc.edu;stevens.edu;cs.unc.edu",
        "email": "cs.unc.edu;stevens.edu;cs.unc.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kim_Learned_Contextual_Feature_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill;Stevens Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unc.edu;https://www.stevens.edu",
        "aff_unique_abbr": "UNC;SIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chapel Hill;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Adaptive Receptive Fields for Deep Image Parsing Network",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "888",
        "author_site": "Zhen Wei, Yao Sun, Jinqiao Wang, Hanjiang Lai, Si Liu",
        "author": "Zhen Wei; Yao Sun; Jinqiao Wang; Hanjiang Lai; Si Liu",
        "abstract": "In this paper, we introduce a novel approach to regulate receptive field in deep image parsing network automatically. Unlike previous works which have stressed much importance on obtaining better receptive fields using manually selected dilated convolutional kernels, our approach uses two affine transformation layers in the network's backbone and operates on feature maps. Feature maps will be inflated/shrinked by the new layer and therefore receptive fields in following layers are changed accordingly. By end-to-end training, the whole framework is data-driven without laborious manual intervention. The proposed method is generic across dataset and different tasks. We conduct extensive experiments on both general parsing task and face parsing task as concrete examples to demonstrate the method's superior regulation ability over manual designs.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wei_Learning_Adaptive_Receptive_CVPR_2017_paper.pdf",
        "aff": "State Key Laboratory of Information Security (SKLOIS), Institute of Information Engineering, Chinese Academy of Sciences, Beijing, 100093, China+University of Chinese Academy of Sciences, Beijing, 101408, China; State Key Laboratory of Information Security (SKLOIS), Institute of Information Engineering, Chinese Academy of Sciences, Beijing, 100093, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou 510275, China; State Key Laboratory of Information Security (SKLOIS), Institute of Information Engineering, Chinese Academy of Sciences, Beijing, 100093, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Wei_Learning_Adaptive_Receptive_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1031824,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7318306444330187840&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "iie.ac.cn;iie.ac.cn;nlpr.ia.ac.cn;gmail.com;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;nlpr.ia.ac.cn;gmail.com;iie.ac.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wei_Learning_Adaptive_Receptive_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;2;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Sun Yat-sen University",
        "aff_unique_dep": "State Key Laboratory of Information Security (SKLOIS);;School of Data and Computer Science",
        "aff_unique_url": "http://www.iiis.cas.cn;http://www.ucas.ac.cn;http://www.sysu.edu.cn",
        "aff_unique_abbr": "CAS;UCAS;SYSU",
        "aff_campus_unique_index": "0+0;0;0;1;0",
        "aff_campus_unique": "Beijing;Guangzhou",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Barycentric Representations of 3D Shapes for Sketch-Based 3D Shape Retrieval",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2084",
        "author_site": "Jin Xie, Guoxian Dai, Fan Zhu, Yi Fang",
        "author": "Jin Xie; Guoxian Dai; Fan Zhu; Yi Fang",
        "abstract": "Retrieving 3D shapes with sketches is a challenging problem since 2D sketches and 3D shapes are from two heterogeneous domains, which results in large discrepancy between them. In this paper, we propose to learn barycenters of 2D projections of 3D shapes for sketch-based 3D shape retrieval. Specifically, we first use two deep convolutional neural networks (CNNs) to extract deep features of sketches and 2D projections of 3D shapes. For 3D shapes, we then compute the Wasserstein barycenters of deep features of multiple projections to form a barycentric representation. Finally, by constructing a metric network, a discriminative loss is formulated on the Wasserstein barycenters of 3D shapes and sketches in the deep feature space to learn discriminative and compact 3D shape and sketch features for retrieval.  The proposed method is evaluated on the SHREC'13 and SHREC'14 sketch track benchmark datasets. Compared to the state-of-the-art methods, our proposed method can significantly improve the retrieval performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Learning_Barycentric_Representations_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 844444,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16368444446679905784&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Learning_Barycentric_Representations_CVPR_2017_paper.html"
    },
    {
        "title": "Learning Category-Specific 3D Shape Models From Weakly Labeled 2D Images",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1892",
        "author_site": "Dingwen Zhang, Junwei Han, Yang Yang, Dong Huang",
        "author": "Dingwen Zhang; Junwei Han; Yang Yang; Dong Huang",
        "abstract": "Recently, researchers have made great processes to build category-specific 3D shape models from 2D images with manual annotations consisting of class labels, keypoints, and ground truth figure-ground segmentations. However, the annotation of figure-ground segmentations is still labor-intensive and time-consuming. To further alleviate the burden of providing such manual annotations, we make the earliest effort to learn category-specific 3D shape models by only using weakly labeled 2D images. By revealing the underlying relationship between the tasks of common object segmentation and category-specific 3D shape reconstruction, we propose a novel framework to jointly solve these two problems along a cluster-level learning curriculum. Comprehensive experiments on the challenging PASCAL VOC benchmark demonstrate that the category-specific 3D shape models trained using our weakly supervised learning framework could, to some extent, approach the performance of the state-of-the-art methods using expensive manual segmentation annotations. In addition, the experiments also demonstrate the effectiveness of using 3D shape models for helping common object segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Learning_Category-Specific_3D_CVPR_2017_paper.pdf",
        "aff": "Northwestern Polytechnical University+Carnegie Mellon University; Northwestern Polytechnical University; Northwestern Polytechnical University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5119980,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1581811782285470483&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;gmail.com;gmail.com;andrew.cmu.edu",
        "email": "gmail.com;gmail.com;gmail.com;andrew.cmu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Learning_Category-Specific_3D_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;1",
        "aff_unique_norm": "Northwestern Polytechnical University;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nwpu.edu.cn;https://www.cmu.edu",
        "aff_unique_abbr": "NWPU;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Learning Cross-Modal Deep Representations for Robust Pedestrian Detection",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "2252",
        "author_site": "Dan Xu, Wanli Ouyang, Elisa Ricci, Xiaogang Wang, Nicu Sebe",
        "author": "Dan Xu; Wanli Ouyang; Elisa Ricci; Xiaogang Wang; Nicu Sebe",
        "abstract": "This paper presents a novel method for detecting pedestrians under adverse illumination conditions. Our approach relies on a novel cross-modality learning framework and it is based on two main phases. First, given a multimodal dataset, a deep convolutional network is employed to learn a non-linear mapping, modeling the relations between RGB and thermal data. Then, the learned feature representations are transferred to a second deep network, which receives as input an RGB image and outputs the detection results. In this way, features which are both discriminative and robust to bad illumination conditions are learned. Importantly, at test time, only the second pipeline is considered and no thermal data are required. Our extensive evaluation demonstrates that the proposed approach outperforms the state-of-the-art on the challenging KAIST multispectral pedestrian dataset and it is competitive with previous methods on the popular Caltech dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Learning_Cross-Modal_Deep_CVPR_2017_paper.pdf",
        "aff": "University of Trento; The Chinese University of Hong Kong+The University of Sydney; Fondazione Bruno Kessler+University of Perugia; The Chinese University of Hong Kong; University of Trento",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.02431v2",
        "pdf_size": 2473898,
        "gs_citation": 257,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6911915982790356442&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "unitn.it;unitn.it;fbk.eu;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "email": "unitn.it;unitn.it;fbk.eu;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xu_Learning_Cross-Modal_Deep_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+2;3+4;1;0",
        "aff_unique_norm": "University of Trento;Chinese University of Hong Kong;University of Sydney;Fondazione Bruno Kessler;University of Perugia",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.unitn.it;https://www.cuhk.edu.hk;https://www.sydney.edu.au;https://www.fbk.eu;https://www.unipg.it",
        "aff_unique_abbr": "UniTN;CUHK;USYD;FBK;Unipg",
        "aff_campus_unique_index": "1;;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;1+2;0+0;1;0",
        "aff_country_unique": "Italy;China;Australia"
    },
    {
        "title": "Learning Cross-Modal Embeddings for Cooking Recipes and Food Images",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1110",
        "author_site": "Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Ofli, Ingmar Weber, Antonio Torralba",
        "author": "Amaia Salvador; Nicholas Hynes; Yusuf Aytar; Javier Marin; Ferda Ofli; Ingmar Weber; Antonio Torralba",
        "abstract": "In this paper, we introduce Recipe1M, a new large-scale, structured corpus of over 1m cooking recipes and 800k food images. As the largest publicly available collection of recipe data, Recipe1M affords the ability to train high-capacity models on aligned, multi-modal data. Accordingly, we train a neural network to find a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task. Additionally, we demonstrate that regularization via the addition of a high-level, semantic classification objective improves performance to rival that of humans and enables semantic vector arithmetic. We postulate that these embeddings will provide a basis for further exploration of the Recipe1M dataset and food and cooking in general.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Salvador_Learning_Cross-Modal_Embeddings_CVPR_2017_paper.pdf",
        "aff": "Universitat Polit `ecnica de Catalunya; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Qatar Computing Research Institute, HBKU; Qatar Computing Research Institute, HBKU; Massachusetts Institute of Technology",
        "project": "http://im2recipe.csail.mit.edu",
        "github": "",
        "supp": "",
        "arxiv": "1810.06553v2",
        "pdf_size": 3530159,
        "gs_citation": 758,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10599433604267164175&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "upc.edu;mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu;qf.org.qa;qf.org.qa",
        "email": "upc.edu;mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu;qf.org.qa;qf.org.qa",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Salvador_Learning_Cross-Modal_Embeddings_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;2;2;1",
        "aff_unique_norm": "Universitat Polit\u00e8cnica de Catalunya;Massachusetts Institute of Technology;Qatar Computing Research Institute",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.upc.edu;https://web.mit.edu;https://www.qcri.org",
        "aff_unique_abbr": "UPC;MIT;QCRI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;2;2;1",
        "aff_country_unique": "Spain;United States;Qatar"
    },
    {
        "title": "Learning Deep Binary Descriptor With Multi-Quantization",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "411",
        "author_site": "Yueqi Duan, Jiwen Lu, Ziwei Wang, Jianjiang Feng, Jie Zhou",
        "author": "Yueqi Duan; Jiwen Lu; Ziwei Wang; Jianjiang Feng; Jie Zhou",
        "abstract": "In this paper, we propose an unsupervised feature learning method called deep binary descriptor with multi-quantization (DBD-MQ) for visual matching. Existing learning-based binary descriptors such as compact binary face descriptor (CBFD) and DeepBit utilize the rigid sign function for binarization despite of data distributions, thereby suffering from severe quantization loss. In order to address the limitation, our DBD-MQ considers the binarization as a multi-quantization task. Specifically, we apply a K-AutoEncoders (KAEs) network to jointly learn the parameters and the binarization functions under a deep learning framework, so that discriminative binary descriptors can be obtained with a fine-grained multi-quantization. Extensive experimental results on different visual analysis including patch retrieval, image matching and image retrieval show that our DBD-MQ outperforms most existing binary feature descriptors.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Duan_Learning_Deep_Binary_CVPR_2017_paper.pdf",
        "aff": "Department of Automation, Tsinghua University, Beijing, China+State Key Lab of Intelligent Technologies and Systems, Beijing, China+Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China; Department of Automation, Tsinghua University, Beijing, China+State Key Lab of Intelligent Technologies and Systems, Beijing, China+Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China; Department of Physics, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China+State Key Lab of Intelligent Technologies and Systems, Beijing, China+Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China; Department of Automation, Tsinghua University, Beijing, China+State Key Lab of Intelligent Technologies and Systems, Beijing, China+Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1634612,
        "gs_citation": 124,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14252920246824566747&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "mails.tsinghua.edu.cn;tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Duan_Learning_Deep_Binary_CVPR_2017_paper.html",
        "aff_unique_index": "0+1+0;0+1+0;0;0+1+0;0+1+0",
        "aff_unique_norm": "Tsinghua University;State Key Lab of Intelligent Technologies and Systems",
        "aff_unique_dep": "Department of Automation;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;",
        "aff_unique_abbr": "THU;",
        "aff_campus_unique_index": "0+0;0+0;0;0+0;0+0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0+0;0+0+0;0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Deep CNN Denoiser Prior for Image Restoration",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1535",
        "author_site": "Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang",
        "author": "Kai Zhang; Wangmeng Zuo; Shuhang Gu; Lei Zhang",
        "abstract": "Model-based optimization methods and discriminative learning methods have been the two dominant strategies for solving various inverse problems in low-level vision. Typically, those two kinds of methods have their respective merits and drawbacks, e.g., model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming with sophisticated priors for the purpose of good performance; in the meanwhile, discriminative learning methods have fast testing speed but their application range is greatly restricted by the specialized task. Recent works have revealed that, with the aid of variable splitting techniques, denoiser prior can be plugged in as a modular part of model-based optimization methods to solve other inverse problems (e.g., deblurring). Such an integration induces considerable advantage when the denoiser is obtained via discriminative learning. However, the study of integration with fast discriminative denoiser prior is still lacking. To this end, this paper aims to train a set of fast and effective CNN (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. Experimental results demonstrate that the learned set of denoisers can not only achieve promising Gaussian denoising results but also can be used as prior to deliver good performance for various low-level vision applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Learning_Deep_CNN_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China+Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03264v1",
        "pdf_size": 728767,
        "gs_citation": 2471,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11733015574117803846&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "gmail.com;hit.edu.cn;gmail.com;comp.polyu.edu.hk",
        "email": "gmail.com;hit.edu.cn;gmail.com;comp.polyu.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Learning_Deep_CNN_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;1;1",
        "aff_unique_norm": "Harbin Institute of Technology;Hong Kong Polytechnic University",
        "aff_unique_dep": "School of Computer Science and Technology;Dept. of Computing",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.polyu.edu.hk",
        "aff_unique_abbr": "HIT;PolyU",
        "aff_campus_unique_index": "0+1;0;1;1",
        "aff_campus_unique": "Harbin;Hong Kong",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Deep Context-Aware Features Over Body and Latent Parts for Person Re-Identification",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "135",
        "author_site": "Dangwei Li, Xiaotang Chen, Zhang Zhang, Kaiqi Huang",
        "author": "Dangwei Li; Xiaotang Chen; Zhang Zhang; Kaiqi Huang",
        "abstract": "Person Re-identification (ReID) is to identify the same person across different cameras. It is a challenging task due to the large variations in person pose, occlusion, background clutter, etc. How to extract powerful features is a fundamental problem in ReID and is still an open problem today. In this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learn powerful features over full body and body parts, which can well capture the local context knowledge by stacking multi-scale convolutions in each layer. Moreover, instead of using predefined rigid parts, we propose to learn and localize deformable pedestrian parts using Spatial Transformer Networks (STN) with novel spatial constraints. The learned body parts can release some difficulties, e.g. pose variations and background clutters, in part-based representation. Finally, we integrate the representation learning processes of full body and body parts into a unified framework for person ReID through multi-class person identification tasks. Extensive evaluations on current challenging large-scale person ReID datasets, including the image-based Market1501, CUHK03 and sequence-based MARS datasets, show that the proposed method achieves the state-of-the-art results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Learning_Deep_Context-Aware_CVPR_2017_paper.pdf",
        "aff": "CRIPAC &NLPR, CASIA+University of Chinese Academy of Sciences; CRIPAC &NLPR, CASIA+University of Chinese Academy of Sciences; CRIPAC &NLPR, CASIA+University of Chinese Academy of Sciences; CRIPAC &NLPR, CASIA+University of Chinese Academy of Sciences+CAS Center for Excellence in Brain Science and Intelligence Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Learning_Deep_Context-Aware_2017_CVPR_supplemental.pdf",
        "arxiv": "1710.06555v1",
        "pdf_size": 653564,
        "gs_citation": 829,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11787115398937943065&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Learning_Deep_Context-Aware_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;0+1+2",
        "aff_unique_norm": "Chinese Academy of Sciences Institute of Automation;University of Chinese Academy of Sciences;Chinese Academy of Sciences",
        "aff_unique_dep": "CRIPAC & NLPR;;Center for Excellence in Brain Science and Intelligence Technology",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;http://www.cas.cn/",
        "aff_unique_abbr": "CASIA;UCAS;CAS",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Deep Match Kernels for Image-Set Classification",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1234",
        "author_site": "Haoliang Sun, Xiantong Zhen, Yuanjie Zheng, Gongping Yang, Yilong Yin, Shuo Li",
        "author": "Haoliang Sun; Xiantong Zhen; Yuanjie Zheng; Gongping Yang; Yilong Yin; Shuo Li",
        "abstract": "Image-set classification has recently generated great popularity due to its widespread applications in computer vision. The great challenges arise from effectively and efficiently measuring the similarity between image sets with high inter-class ambiguity and huge intra-class variability. In this paper, we propose deep match kernels (DMK) to directly measure the similarity between image sets in the match kernel framework. Specifically, we build deep local match kernels between images upon arc-cosine kernels, which can faithfully characterize the similarity between images by mimicking deep neural networks; we introduce anchors to aggregate those deep local match kernels into a global match kernel between image sets, which is learned in a supervised way by kernel alignment and therefore more discriminative. The DMK provides the first match kernel framework for image-set classification, which removes specific assumptions usually required in previous approaches and is computationally more efficient. We conduct extensive experiments on four datasets for three diverse image-set classification tasks. The DMK achieves high performance and consistently surpasses state-of-the-art methods, showing its great effectiveness for image-set classification.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sun_Learning_Deep_Match_CVPR_2017_paper.pdf",
        "aff": "Shandong University; The University of Western Ontario; School of Information Science and Engineering, Key Lab of Intelligent Computing & Information Security in Universities of Shandong, Key Lab of Intelligent Information Processing, Institute of Life Sciences at Shandong Normal University; Shandong University; Shandong University of Finance and Economics; The University of Western Ontario",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 559793,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7296424823113225861&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;sdu.edu.cn;sdu.edu.cn",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;sdu.edu.cn;sdu.edu.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sun_Learning_Deep_Match_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0;3;1",
        "aff_unique_norm": "Shandong University;University of Western Ontario;Shandong Normal University;Shandong University of Finance and Economics",
        "aff_unique_dep": ";;School of Information Science and Engineering;",
        "aff_unique_url": "http://www.sdu.edu.cn;https://www.uwo.ca;http://www.sdu.edu.cn/;http://www.sdufe.edu.cn",
        "aff_unique_abbr": "SDU;UWO;SDU;SDUFE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;1",
        "aff_country_unique": "China;Canada"
    },
    {
        "title": "Learning Detailed Face Reconstruction From a Single Image",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Oral",
        "track": "main",
        "pid": "454",
        "author_site": "Elad Richardson, Matan Sela, Roy Or-El, Ron Kimmel",
        "author": "Elad Richardson; Matan Sela; Roy Or-El; Ron Kimmel",
        "abstract": "Reconstructing the detailed geometric structure of a face from a given image is a key to many computer vision and graphics applications, such as motion capture and reenactment. The reconstruction task is challenging as human faces vary extensively when considering expressions, poses, textures, and intrinsic geometries. While many approaches tackle this complexity by using additional data to reconstruct the face of a single subject, extracting facial surface from a single image remains a difficult problem. As a result, single-image based methods can usually provide only a rough estimate of the facial geometry. In contrast, we propose to leverage the power of convolutional neural networks to produce a highly detailed face reconstruction from a single image. For this purpose, we introduce an end-to-end CNN framework which derives the shape in a coarse-to-fine fashion. The proposed architecture is composed of two main blocks, a network that recovers the coarse facial geometry (CoarseNet), followed by a CNN that refines the facial features of that geometry (FineNet). The proposed networks are connected by a novel layer which renders a depth image given a mesh in 3D. Unlike object recognition and detection problems, there are no suitable datasets for training CNNs to perform face geometry reconstruction. Therefore, our training regime begins with a supervised phase, based on synthetic images, followed by an unsupervised phase that uses only unconstrained facial images. The accuracy and robustness of the proposed model is demonstrated by both qualitative and quantitative evaluation tests.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Richardson_Learning_Detailed_Face_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Richardson_Learning_Detailed_Face_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.05053",
        "gs_citation": 400,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16158879011118685648&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Richardson_Learning_Detailed_Face_CVPR_2017_paper.html"
    },
    {
        "title": "Learning Detection With Diverse Proposals",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3375",
        "author_site": "Samaneh Azadi, Jiashi Feng, Trevor Darrell",
        "author": "Samaneh Azadi; Jiashi Feng; Trevor Darrell",
        "abstract": "To predict a set of diverse and informative proposals with enriched representations, this paper introduces a differentiable Determinantal Point Process (DPP) layer that is able to augment the object detection architectures. Most modern object detection architectures, such as Faster R-CNN, learn to localize objects by minimizing deviations from the ground truth, but ignore correlation between multiple proposals and object categories. Non-Maximum Suppression (NMS) as a widely used proposal pruning scheme ignores label- and instance-level relations between object candidates resulting in multi-labeled detections. In the multi-class case, NMS selects boxes with the largest prediction scores ignoring the semantic relation between categories of potential election. In contrast, our trainable DPP layer, allowing for Learning Detection with Diverse Proposals (LDDP), considers both label-level contextual information and spatial layout relationships between proposals without increasing the number of parameters of the network, and thus improves location and category specifications of final detected bounding boxes substantially during both training and inference schemes. Furthermore, we show that LDDP keeps it superiority over Faster R-CNN even if the number of proposals generated by LDPP is only  30% as many as those for Faster R-CNN.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Azadi_Learning_Detection_With_CVPR_2017_paper.pdf",
        "aff": "University of California, Berkeley; National University of Singapore; University of California, Berkeley",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Azadi_Learning_Detection_With_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.03533v1",
        "pdf_size": 930976,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=928158358698163903&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "eecs.berkeley.edu;nus.edu.sg;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;nus.edu.sg;eecs.berkeley.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Azadi_Learning_Detection_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.nus.edu.sg",
        "aff_unique_abbr": "UC Berkeley;NUS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "title": "Learning Discriminative and Transformation Covariant Local Feature Detectors",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3129",
        "author_site": "Xu Zhang, Felix X. Yu, Svebor Karaman, Shih-Fu Chang",
        "author": "Xu Zhang; Felix X. Yu; Svebor Karaman; Shih-Fu Chang",
        "abstract": "Robust covariant local feature detectors are important for detecting local features that are (1) discriminative of the image content and (2) can be repeatably detected at consistent locations when the image undergoes diverse transformations. Such detectors are critical for applications such as image search and scene reconstruction. Many learning-based local feature detectors address one of these two problems while overlooking the other. In this work, we propose a novel learning-based method to simultaneously address both issues. Specifically, we extend the covariant constraint proposed by Lenc and Vedaldi by defining the concepts of \"standard patch\" and \"canonical feature\" and leverage these to train a novel robust covariant detector. We show that the introduction of these concepts greatly simplifies the learning stage of the covariant detector, and also makes the detector much more robust. Extensive experiments show that our method outperforms previous hand-crafted and learning-based detectors by large margins in terms of repeatability.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Learning_Discriminative_and_CVPR_2017_paper.pdf",
        "aff": "Columbia University; Google Research; Columbia University; Columbia University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 690974,
        "gs_citation": 157,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1443219612857892948&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "columbia.edu;google.com;columbia.edu;columbia.edu",
        "email": "columbia.edu;google.com;columbia.edu;columbia.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Learning_Discriminative_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Columbia University;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.columbia.edu;https://research.google",
        "aff_unique_abbr": "Columbia;Google Research",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Diverse Image Colorization",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3134",
        "author_site": "Aditya Deshpande, Jiajun Lu, Mao-Chuang Yeh, Min Jin Chong, David Forsyth",
        "author": "Aditya Deshpande; Jiajun Lu; Mao-Chuang Yeh; Min Jin Chong; David Forsyth",
        "abstract": "Colorization is an ambiguous problem, with multiple viable colorizations for a single grey-level image. However, previous methods only produce the single most probable colorization. Our goal is to model the diversity intrinsic to the problem of colorization and produce multiple colorizations that display long-scale spatial co-ordination. We learn a low dimensional embedding of color fields using a variational autoencoder (VAE). We construct loss terms for the VAE decoder that avoid blurry outputs and take into account the uneven distribution of pixel colors. Finally, we build a conditional model for the multi-modal distribution between grey-level image and the color field embeddings. Samples from this conditional model result in diverse colorization. We demonstrate that our method obtains better diverse colorizations than a standard conditional variational autoencoder (CVAE) model, as well as a recently proposed conditional generative adversarial network (cGAN).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Deshpande_Learning_Diverse_Image_CVPR_2017_paper.pdf",
        "aff": "University of Illinois at Urbana Champaign; University of Illinois at Urbana Champaign; University of Illinois at Urbana Champaign; University of Illinois at Urbana Champaign; University of Illinois at Urbana Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.01958v2",
        "pdf_size": 2221319,
        "gs_citation": 262,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13029067381672657111&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Deshpande_Learning_Diverse_Image_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Dynamic Guidance for Depth Image Enhancement",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1424",
        "author_site": "Shuhang Gu, Wangmeng Zuo, Shi Guo, Yunjin Chen, Chongyu Chen, Lei Zhang",
        "author": "Shuhang Gu; Wangmeng Zuo; Shi Guo; Yunjin Chen; Chongyu Chen; Lei Zhang",
        "abstract": "The depth images acquired by consumer depth sensors (e.g., Kinect and ToF) usually are of low resolution and insufficient quality. One natural solution is to incorporate with high resolution RGB camera for exploiting their statistical correlation. However, most existing methods are intuitive and limited in characterizing the complex and dynamic dependency between intensity and depth images. To address these limitations, we propose a weighted analysis representation model for guided depth image enhancement, which advances the conventional methods in two aspects: (i) task driven learning and (ii) dynamic guidance. First, we generalize the analysis representation model by including a guided weight function for dependency modeling. And the task-driven learning formulation is introduced to obtain the optimized guidance tailored to specific enhancement task. Second, the depth image is gradually enhanced along with the iterations, and thus the guidance should also be dynamically adjusted  to account for the updating of depth image. To this end, stage-wise parameters are learned for dynamic guidance. Experiments on guided depth image upsampling and noisy depth image restoration validate the effectiveness of our method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gu_Learning_Dynamic_Guidance_CVPR_2017_paper.pdf",
        "aff": "The Hong Kong Polytechnic University; Harbin Institute of Technology; Harbin Institute of Technology; ULSee Inc.; Sun Yat-sen University+The Hong Kong Polytechnic University; The Hong Kong Polytechnic University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4962801,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6798362995072276462&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;gmail.com; ; ; ;comp.polyu.edu.hk",
        "email": "gmail.com;gmail.com; ; ; ;comp.polyu.edu.hk",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gu_Learning_Dynamic_Guidance_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;2;3+0;0",
        "aff_unique_norm": "Hong Kong Polytechnic University;Harbin Institute of Technology;ULSee Inc.;Sun Yat-sen University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.polyu.edu.hk;http://www.hit.edu.cn/;;http://www.sysu.edu.cn/",
        "aff_unique_abbr": "PolyU;HIT;;SYSU",
        "aff_campus_unique_index": "0;1;1;0;0",
        "aff_campus_unique": "Hong Kong SAR;Harbin;",
        "aff_country_unique_index": "0;0;0;1;0+0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Learning Features by Watching Objects Move",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "980",
        "author_site": "Deepak Pathak, Ross Girshick, Piotr Doll\u00c3\u00a1r, Trevor Darrell, Bharath Hariharan",
        "author": "Deepak Pathak; Ross Girshick; Piotr Dollar; Trevor Darrell; Bharath Hariharan",
        "abstract": "This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Pathak_Learning_Features_by_CVPR_2017_paper.pdf",
        "aff": "Facebook AI Research (FAIR) + University of California, Berkeley; Facebook AI Research (FAIR); Facebook AI Research (FAIR); University of California, Berkeley; Facebook AI Research (FAIR)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.06370v2",
        "pdf_size": 2948229,
        "gs_citation": 640,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17299396636746877618&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Pathak_Learning_Features_by_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;1;0",
        "aff_unique_norm": "Meta;University of California, Berkeley",
        "aff_unique_dep": "Facebook AI Research;",
        "aff_unique_url": "https://research.facebook.com;https://www.berkeley.edu",
        "aff_unique_abbr": "FAIR;UC Berkeley",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning From Noisy Large-Scale Datasets With Minimal Supervision",
        "session": "Machine Learning for 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "275",
        "author_site": "Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, Serge Belongie",
        "author": "Andreas Veit; Neil Alldrin; Gal Chechik; Ivan Krasin; Abhinav Gupta; Serge Belongie",
        "abstract": "We present an approach to effectively use millions of images with noisy annotations in conjunction with a small subset of cleanly-annotated images to learn powerful image representations. One common approach to combine clean and noisy data is to first pre-train a network using the large noisy dataset and then fine-tune with the clean dataset. We show this approach does not fully leverage the information contained in the clean set. Thus, we demonstrate how to use the clean annotations to reduce the noise in the large dataset before fine-tuning the network using both the clean set and the full set with reduced noise. The approach comprises a multi-task network that jointly learns to clean noisy annotations and to accurately classify images. We evaluate our approach on the recently released Open Images dataset, containing  9 million images, multiple annotations per image and over 6000 unique classes. For the small clean set of annotations we use a quarter of the validation set with  40k images. Our results demonstrate that the proposed approach clearly outperforms direct fine-tuning across all major categories of classes in the Open Image dataset. Further, our approach is particularly effective for a large number of classes with wide range of noise in annotations (20-80% false positive annotations).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Veit_Learning_From_Noisy_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science & Cornell Tech, Cornell University; Google Inc; Google Inc; Google Inc; Google Inc + The Robotics Institute, Carnegie Mellon University; Department of Computer Science & Cornell Tech, Cornell University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.01619v2",
        "pdf_size": 1796081,
        "gs_citation": 606,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10881721383831570592&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Veit_Learning_From_Noisy_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;1+2;0",
        "aff_unique_norm": "Cornell University;Google;Carnegie Mellon University",
        "aff_unique_dep": "Department of Computer Science;Google;The Robotics Institute",
        "aff_unique_url": "https://www.cornell.edu;https://www.google.com;https://www.cmu.edu",
        "aff_unique_abbr": "Cornell;Google;CMU",
        "aff_campus_unique_index": "0;1;1;1;1;0",
        "aff_campus_unique": "Cornell Tech;Mountain View;",
        "aff_country_unique_index": "0;0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning From Simulated and Unsupervised Images Through Adversarial Training",
        "session": "Machine Learning 2",
        "status": "Oral",
        "track": "main",
        "pid": "767",
        "author_site": "Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, Russell Webb",
        "author": "Ashish Shrivastava; Tomas Pfister; Oncel Tuzel; Joshua Susskind; Wenda Wang; Russell Webb",
        "abstract": "With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shrivastava_Learning_From_Simulated_CVPR_2017_paper.pdf",
        "aff": "Apple Inc; Apple Inc; Apple Inc; Apple Inc; Apple Inc; Apple Inc",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Shrivastava_Learning_From_Simulated_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.07828v2",
        "pdf_size": 730156,
        "gs_citation": 2368,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5960837935511106924&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "apple.com;apple.com;apple.com;apple.com;apple.com;apple.com",
        "email": "apple.com;apple.com;apple.com;apple.com;apple.com;apple.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shrivastava_Learning_From_Simulated_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Apple",
        "aff_unique_dep": "Apple Inc",
        "aff_unique_url": "https://www.apple.com",
        "aff_unique_abbr": "Apple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning From Synthetic Humans",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "40",
        "author_site": "G\u00c3\u00bcl Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black, Ivan Laptev, Cordelia Schmid",
        "author": "Gul Varol; Javier Romero; Xavier Martin; Naureen Mahmood; Michael J. Black; Ivan Laptev; Cordelia Schmid",
        "abstract": "Estimating human pose, shape, and motion from images and video are fundamental challenges with many applications. Recent advances in 2D human pose estimation use large amounts of manually-labeled training data for learning convolutional neural networks (CNNs). Such data is time consuming to acquire and difficult to extend. Moreover, manual labeling of 3D pose, depth and motion is impractical. In this work we present SURREAL: a new large-scale dataset with synthetically-generated but realistic images of people rendered from 3D sequences of human motion capture data. We generate more than 6 million frames together with ground truth pose, depth maps, and segmentation masks. We show that CNNs trained on our synthetic dataset allow for accurate human depth estimation and human part segmentation in real RGB images. Our results and the new datast open up new possibilities for advancing person analysis using chap and large-scale synthetic data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Varol_Learning_From_Synthetic_CVPR_2017_paper.pdf",
        "aff": "Inria; Inria+MPI; Inria; MPI; Inria; Inria; Inria",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.01370v3",
        "pdf_size": 1164415,
        "gs_citation": 1234,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3070187880069123504&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Varol_Learning_From_Synthetic_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;0;1;0;0;0",
        "aff_unique_norm": "INRIA;Max Planck Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.inria.fr;https://www.mpg.de",
        "aff_unique_abbr": "Inria;MPI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;1;0;0;0",
        "aff_country_unique": "France;Germany"
    },
    {
        "title": "Learning Fully Convolutional Networks for Iterative Non-Blind Deconvolution",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1464",
        "author_site": "Jiawei Zhang, Jinshan Pan, Wei-Sheng Lai, Rynson W. H. Lau, Ming-Hsuan Yang",
        "author": "Jiawei Zhang; Jinshan Pan; Wei-Sheng Lai; Rynson W. H. Lau; Ming-Hsuan Yang",
        "abstract": "In this paper, we propose a fully convolutional network for iterative non-blind deconvolution. We decompose the non-blind deconvolution problem into image denoising and image deconvolution. We train a FCNN to remove noise in the gradient domain and use the learned gradients to guide the image deconvolution step. In contrast to the existing deep neural network based methods, we iteratively deconvolve the blurred images in a multi-stage framework. The proposed method is able to learn an adaptive image prior, which keeps both local (details) and global (structures) information. Both quantitative and qualitative evaluations on the benchmark datasets demonstrate that the proposed method performs favorably against state-of-the-art algorithms in terms of quality and speed.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Learning_Fully_Convolutional_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, City University of Hong Kong1; School of Mathematical Sciences, Dalian University of Technology2; Electrical Engineering and Computer Science, University of California, Merced3; Department of Computer Science, City University of Hong Kong1; Electrical Engineering and Computer Science, University of California, Merced3",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.06495v1",
        "pdf_size": 6164918,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12428437213629439568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "gmail.com; ; ; ; ",
        "email": "gmail.com; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Learning_Fully_Convolutional_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0;2",
        "aff_unique_norm": "City University of Hong Kong;Dalian University of Technology;University of California, Merced",
        "aff_unique_dep": "Department of Computer Science;School of Mathematical Sciences;Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.cityu.edu.hk;http://en.dlut.edu.cn/;https://www.ucmerced.edu",
        "aff_unique_abbr": "CityU;DUT;UC Merced",
        "aff_campus_unique_index": "0;2;0;2",
        "aff_campus_unique": "Hong Kong SAR;;Merced",
        "aff_country_unique_index": "0;0;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Learning Motion Patterns in Videos",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "1257",
        "author_site": "Pavel Tokmakov, Karteek Alahari, Cordelia Schmid",
        "author": "Pavel Tokmakov; Karteek Alahari; Cordelia Schmid",
        "abstract": "The problem of determining whether an object is in motion, irrespective of camera motion, is far from being solved. We address this challenging task by learning motion patterns in videos. The core of our approach is a fully convolutional network, which is learned entirely from synthetic video sequences, and their ground-truth optical flow and motion segmentation. This encoder-decoder style architecture first learns a coarse representation of the optical flow field features, and then refines it iteratively to produce motion labels at the original high-resolution. We further improve this labeling with an objectness map and a conditional random field, to account for errors in optical flow, and also to focus on moving \"things\" rather than \"stuff\". The output label of each pixel denotes whether it has undergone independent motion, i.e., irrespective of camera motion. We demonstrate the benefits of this learning framework on the moving object segmentation task, where the goal is to segment all objects in motion. Our approach outperforms the top method on the recently released DAVIS benchmark dataset, comprising real-world sequences, by 5.6%. We also evaluate on the Berkeley motion segmentation database, achieving state-of-the-art results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tokmakov_Learning_Motion_Patterns_CVPR_2017_paper.pdf",
        "aff": "Inria\u2217; Inria\u2217; Inria\u2217",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.07217v2",
        "pdf_size": 2179978,
        "gs_citation": 330,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7693817542104524159&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tokmakov_Learning_Motion_Patterns_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "Inria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Learning Multifunctional Binary Codes for Both Category and Attribute Oriented Retrieval Tasks",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1517",
        "author_site": "Haomiao Liu, Ruiping Wang, Shiguang Shan, Xilin Chen",
        "author": "Haomiao Liu; Ruiping Wang; Shiguang Shan; Xilin Chen",
        "abstract": "In this paper we propose a unified framework to address multiple realistic image retrieval tasks concerning both category and attributes. Considering the scale of modern datasets, hashing is favorable for its low complexity. However, most existing hashing methods are designed to preserve one single kind of similarity, thus incapable of dealing with the different tasks simultaneously. To overcome this limitation, we propose a new hashing method, named Dual Purpose Hashing (DPH), which jointly preserves the category and attribute similarities by exploiting the convolutional networks (CNN) to hierarchically capture the correlations between category and attributes. Since images with both category and attribute labels are scarce, our method is designed to take the abundant partially labelled images on the Internet as training inputs. With such a framework, the binary codes of new-coming images can be readily obtained by quantizing the network outputs of a binary-like layer, and the attributes can be recovered from the codes easily. Experiments on two large-scale datasets show that our dual purpose hash codes can achieve comparable or even better performance than those state-of-the-art methods specifically designed for each individual retrieval task, while being more compact than the compared methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Learning_Multifunctional_Binary_CVPR_2017_paper.pdf",
        "aff": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing, 100049, China + Cooperative Medianet Innovation Center, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing, 100049, China + Cooperative Medianet Innovation Center, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing, 100049, China + Cooperative Medianet Innovation Center, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing, 100049, China + Cooperative Medianet Innovation Center, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Liu_Learning_Multifunctional_Binary_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1079098,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11655731483213913662&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Learning_Multifunctional_Binary_CVPR_2017_paper.html",
        "aff_unique_index": "0+1+2;0+1+2;0+1+2;0+1+2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Cooperative Medianet Innovation Center",
        "aff_unique_dep": "Institute of Computing Technology;;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn;",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Non-Lambertian Object Intrinsics Across ShapeNet Categories",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "612",
        "author_site": "Jian Shi, Yue Dong, Hao Su, Stella X. Yu",
        "author": "Jian Shi; Yue Dong; Hao Su; Stella X. Yu",
        "abstract": "We focus on the non-Lambertian object-level intrinsic problem of recovering diffuse albedo, shading, and specular highlights from a single image of an object. Based on existing 3D models in the ShapeNet database, a large-scale object intrinsics database is rendered with HDR environment maps. Millions of synthetic images of objects and their corresponding albedo, shading, and specular ground-truth images are used to train an encoder-decoder CNN, which can decompose an image into the product of albedo and shading components along with an additive specular component. Our CNN delivers accurate and sharp results in this classical inverse problem of computer vision. Evaluated on our realistically synthetic dataset, our method consistently outperforms the state-of-the-art by a large margin. We train and test our CNN across different object categories. Perhaps surprising especially from the CNN classification perspective, our intrinsics CNN generalizes very well across categories. Our analysis shows that feature learning at the encoder stage is more crucial for developing a universal representation across categories. We apply our model to real images and videos from Internet, and observe robust and realistic intrinsics results. Quality non-Lambertian intrinsics could open up many interesting applications such as realistic product search based on material properties and image-based albedo / specular editing.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shi_Learning_Non-Lambertian_Object_CVPR_2017_paper.pdf",
        "aff": "SKLCS, Institute of Software, Chinese Academy of Sciences + University of Chinese Academy of Sciences; Microsoft Research Asia; Stanford University; UC Berkeley / ICSI",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Shi_Learning_Non-Lambertian_Object_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.08510v1",
        "pdf_size": 2487287,
        "gs_citation": 225,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15213897536485112269&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ios.ac.cn;microsoft.com;cs.stanford.edu;berkeley.edu",
        "email": "ios.ac.cn;microsoft.com;cs.stanford.edu;berkeley.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shi_Learning_Non-Lambertian_Object_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2;3;4",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Microsoft;Stanford University;University of California, Berkeley",
        "aff_unique_dep": "Institute of Software;;Research;;",
        "aff_unique_url": "http://www.ios.ac.cn;http://www.ucas.ac.cn;https://www.microsoft.com/en-us/research/group/asia;https://www.stanford.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "CAS;UCAS;MSR Asia;Stanford;UC Berkeley",
        "aff_campus_unique_index": ";1;2;3",
        "aff_campus_unique": ";Asia;Stanford;Berkeley",
        "aff_country_unique_index": "0+0;0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Learning Non-Maximum Suppression",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "1840",
        "author_site": "Jan Hosang, Rodrigo Benenson, Bernt Schiele",
        "author": "Jan Hosang; Rodrigo Benenson; Bernt Schiele",
        "abstract": "Object detectors have hugely profited from moving towards an end-to-end learning paradigm: proposals, fea tures, and the classifier becoming one neural network improved results two-fold on general object detection. One indispensable component is non-maximum suppression (NMS), a post-processing algorithm responsible for merging all detections that belong to the same object. The de facto standard NMS algorithm is still fully hand-crafted, suspiciously simple, and -- being based on greedy clustering with a fixed distance threshold -- forces a trade-off between recall and precision. We propose a new network architecture designed to perform NMS, using only boxes and their score. We report experiments for person detection on PETS and for general object categories on the COCO dataset. Our approach shows promise providing improved localization and occlusion handling.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hosang_Learning_Non-Maximum_Suppression_CVPR_2017_paper.pdf",
        "aff": "Max Planck Institut f\u00fcr Informatik; Max Planck Institut f\u00fcr Informatik; Max Planck Institut f\u00fcr Informatik",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Hosang_Learning_Non-Maximum_Suppression_2017_CVPR_supplemental.pdf",
        "arxiv": "1705.02950v2",
        "pdf_size": 819672,
        "gs_citation": 926,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=74136061828806591&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "email": "mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hosang_Learning_Non-Maximum_Suppression_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Max Planck Institute for Informatics",
        "aff_unique_dep": "Institute for Informatics",
        "aff_unique_url": "https://mpi-inf.mpg.de",
        "aff_unique_abbr": "MPII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Learning Object Interactions and Descriptions for Semantic Image Segmentation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2518",
        "author_site": "Guangrun Wang, Ping Luo, Liang Lin, Xiaogang Wang",
        "author": "Guangrun Wang; Ping Luo; Liang Lin; Xiaogang Wang",
        "abstract": "Recent advanced deep convolutional networks (CNNs) achieved great successes in many computer vision tasks, because of their compelling learning complexity and the presences of large-scale labeled data. However, as obtaining per-pixel annotations is expensive, performances of CNNs in semantic image segmentation are not fully exploited. This work significantly increases segmentation accuracy of CNNs by learning from an Image Descriptions in the Wild (IDW) dataset. Unlike previous image captioning datasets, where captions were manually and densely annotated, images and their descriptions in IDW are automatically downloaded from Internet without any manual cleaning and refinement. An IDW-CNN is proposed to jointly train IDW and existing image segmentation dataset such as Pascal VOC 2012 (VOC). It has two appealing properties. First, knowledge from different datasets can be fully explored and transferred from each other to improve performance. Second, segmentation accuracy in VOC can be constantly increased when selecting more data from IDW. Extensive experiments demonstrate the effectiveness and scalability of IDW-CNN, which outperforms existing best-performing system by 12% on VOC12 test set.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Learning_Object_Interactions_CVPR_2017_paper.pdf",
        "aff": "Sun Yat-sen University+The Chinese University of Hong Kong; The Chinese University of Hong Kong+Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Sun Yat-sen University+SenseTime Group (Limited); The Chinese University of Hong Kong+Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1627081,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4594040014658380040&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "mail2.sysu.edu.cn;ie.cuhk.edu.hk;ieee.org;ee.cuhk.edu.hk",
        "email": "mail2.sysu.edu.cn;ie.cuhk.edu.hk;ieee.org;ee.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Learning_Object_Interactions_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1+2;0+3;1+2",
        "aff_unique_norm": "Sun Yat-sen University;Chinese University of Hong Kong;Shenzhen Institute of Advanced Technology;SenseTime Group",
        "aff_unique_dep": ";;Shenzhen Key Lab of Comp. Vis. & Pat. Rec.;",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://www.cuhk.edu.hk;http://www.siat.ac.cn;https://www.sensetime.com",
        "aff_unique_abbr": "SYSU;CUHK;SIAT;SenseTime",
        "aff_campus_unique_index": "1;1+2;;1+2",
        "aff_campus_unique": ";Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Random-Walk Label Propagation for Weakly-Supervised Semantic Segmentation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3380",
        "author_site": "Paul Vernaza, Manmohan Chandraker",
        "author": "Paul Vernaza; Manmohan Chandraker",
        "abstract": "Large-scale training for semantic segmentation is challenging due to the expense of obtaining training data for this task relative to other vision tasks. We propose a novel training approach to address this difficulty. Given cheaply-obtained sparse image labelings, we propagate the sparse labels to produce guessed dense labelings. A standard CNN-based segmentation network is trained to mimic these labelings. The label-propagation process is defined via random-walk hitting probabilities, which leads to a differentiable parameterization with uncertainty estimates that are incorporated into our loss. We show that by learning the label-propagator jointly with the segmentation predictor, we are able to effectively learn semantic edges given no direct edge supervision. Experiments also show that training a segmentation network in this way outperforms the naive approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Vernaza_Learning_Random-Walk_Label_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Vernaza_Learning_Random-Walk_Label_2017_CVPR_supplemental.pdf",
        "arxiv": "1802.00470v1",
        "pdf_size": 1706575,
        "gs_citation": 315,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6430301706374433661&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Vernaza_Learning_Random-Walk_Label_CVPR_2017_paper.html"
    },
    {
        "title": "Learning Residual Images for Face Attribute Manipulation",
        "session": "Analyzing Humans 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "1626",
        "author_site": "Wei Shen, Rujie Liu",
        "author": "Wei Shen; Rujie Liu",
        "abstract": "Face attributes are interesting due to their detailed description of human faces. Unlike prior researches working on attribute prediction, we address an inverse and more challenging problem called face attribute manipulation which aims at modifying a face image according to a given attribute value. Instead of manipulating the whole image, we propose to learn the corresponding residual image defined as the difference between images before and after the manipulation. In this way, the manipulation can be operated efficiently with modest pixel modification. The framework of our approach is based on the Generative Adversarial Network. It consists of two image transformation networks and a discriminative network. The transformation networks are responsible for the attribute manipulation and its dual operation and the discriminative network is used to distinguish the generated images from real images. We also apply dual learning to allow transformation networks to learn from each other. Experiments show that residual images can be effectively learned and used for attribute manipulations. The generated images remain most of the details in attribute-irrelevant areas.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shen_Learning_Residual_Images_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.05363v2",
        "gs_citation": 323,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15095360696172031357&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shen_Learning_Residual_Images_CVPR_2017_paper.html"
    },
    {
        "title": "Learning Shape Abstractions by Assembling Volumetric Primitives",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "951",
        "author_site": "Shubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A. Efros, Jitendra Malik",
        "author": "Shubham Tulsiani; Hao Su; Leonidas J. Guibas; Alexei A. Efros; Jitendra Malik",
        "abstract": "We present a learning framework for abstracting complex shapes by learning to assemble objects using 3D volumetric primitives. In addition to generating simple and geometrically interpretable explanations of 3D objects, our framework also allows us to automatically discover and exploit consistent structure in the data.  We demonstrate that using our method allows predicting shape representations which can be leveraged for   obtaining a consistent parsing across the instances of a shape collection and constructing an interpretable shape similarity measure. We also examine applications for image-based prediction as well as shape manipulation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper.pdf",
        "aff": "University of California, Berkeley; Stanford University; Stanford University; University of California, Berkeley; University of California, Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.00404",
        "pdf_size": 1096618,
        "gs_citation": 402,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5684245627836603529&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "eecs.berkeley.edu;cs.stanford.edu;cs.stanford.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;cs.stanford.edu;cs.stanford.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tulsiani_Learning_Shape_Abstractions_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0;0",
        "aff_unique_norm": "University of California, Berkeley;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.stanford.edu",
        "aff_unique_abbr": "UC Berkeley;Stanford",
        "aff_campus_unique_index": "0;1;1;0;0",
        "aff_campus_unique": "Berkeley;Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Spatial Regularization With Image-Level Supervisions for Multi-Label Image Classification",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2328",
        "author_site": "Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, Xiaogang Wang",
        "author": "Feng Zhu; Hongsheng Li; Wanli Ouyang; Nenghai Yu; Xiaogang Wang",
        "abstract": "Multi-label image classification is a fundamental but challenging task in computer vision. Great progress has been achieved by exploiting semantic relations between labels in recent years. However, conventional approaches are unable to model the underlying spatial relations between labels in multi-label images, because spatial annotations of the labels are generally not provided. In this paper, we propose a unified deep neural network that exploits both semantic and spatial relations between labels with only image-level supervisions. Given a multi-label image, our proposed Spatial Regularization Network (SRN) generates attention maps for all labels and captures the underlying relations between them via learnable convolutions. By aggregating the regularized classification results with original results by a ResNet-101 network, the classification performance can be consistently improved. The whole deep neural network is trained end-to-end with only image-level annotations, thus requires no additional efforts on image annotations. Extensive evaluations on 3 public datasets with different types of labels show that our approach significantly outperforms state-of-the-arts and has strong generalization capability. Analysis of the learned SRN model demonstrates that it can effectively capture both semantic and spatial relations of labels for improving classification performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Learning_Spatial_Regularization_CVPR_2017_paper.pdf",
        "aff": "University of Science and Technology of China+Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong+University of Sydney; University of Science and Technology of China; Department of Electronic Engineering, The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.05891v2",
        "pdf_size": 1323186,
        "gs_citation": 476,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7760506825824213757&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "mail.ustc.edu.cn;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ustc.edu.cn;ee.cuhk.edu.hk",
        "email": "mail.ustc.edu.cn;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ustc.edu.cn;ee.cuhk.edu.hk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhu_Learning_Spatial_Regularization_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;1+2;0;1",
        "aff_unique_norm": "University of Science and Technology of China;Chinese University of Hong Kong;University of Sydney",
        "aff_unique_dep": ";Department of Electronic Engineering;",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.cuhk.edu.hk;https://www.sydney.edu.au",
        "aff_unique_abbr": "USTC;CUHK;USYD",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+0;0;0+1;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Learning Video Object Segmentation From Static Images",
        "session": "Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "967",
        "author_site": "Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt Schiele, Alexander Sorkine-Hornung",
        "author": "Federico Perazzi; Anna Khoreva; Rodrigo Benenson; Bernt Schiele; Alexander Sorkine-Hornung",
        "abstract": "Inspired by recent advances of deep learning in instance segmentation and object tracking, we introduce the concept of convnet-based guidance applied to video object segmentation. Our model proceeds on a per-frame basis, guided by the output of the previous frame towards the object of interest in the next frame. We demonstrate that highly accurate object segmentation in videos can be enabled by using a convolutional neural network (convnet) trained with static images only. The key component of our approach is a combination of offline and online learning strategies, where the former produces a refined mask from the previous' frame estimate and the latter allows to capture the appearance of the specific object instance. Our method can handle different types of input annotations such as bounding boxes and segments while leveraging an arbitrary amount of annotated frames. Therefore our system is suitable for diverse applications with different requirements in terms of accuracy and efficiency. In our extensive evaluation, we obtain competitive results on three different datasets, independently from the type of input annotation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Perazzi_Learning_Video_Object_CVPR_2017_paper.pdf",
        "aff": "Disney Research + ETH Zurich; Max Planck Institute for Informatics, Saarbr\u00fccken, Germany; Max Planck Institute for Informatics, Saarbr\u00fccken, Germany; Max Planck Institute for Informatics, Saarbr\u00fccken, Germany; Disney Research + ETH Zurich",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Perazzi_Learning_Video_Object_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.02646v1",
        "pdf_size": 1696119,
        "gs_citation": 651,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14113658434028068719&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "disneyresearch.com;mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;disneyresearch.com",
        "email": "disneyresearch.com;mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;disneyresearch.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Perazzi_Learning_Video_Object_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2;2;2;0+1",
        "aff_unique_norm": "Disney Research;ETH Zurich;Max Planck Institute for Informatics",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://research.disney.com;https://www.ethz.ch;https://mpi-inf.mpg.de",
        "aff_unique_abbr": "Disney Research;ETHZ;MPII",
        "aff_campus_unique_index": ";1;1;1;",
        "aff_campus_unique": ";Saarbr\u00fccken",
        "aff_country_unique_index": "0+1;2;2;2;0+1",
        "aff_country_unique": "United States;Switzerland;Germany"
    },
    {
        "title": "Learning a Deep Embedding Model for Zero-Shot Learning",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "741",
        "author_site": "Li Zhang, Tao Xiang, Shaogang Gong",
        "author": "Li Zhang; Tao Xiang; Shaogang Gong",
        "abstract": "Zero-shot learning (ZSL) models rely on learning a joint embedding space where both textual/semantic description of object classes and visual representation of object images can be projected to for nearest neighbour search. Despite the success of deep neural networks that learn an end-to-end model between text and images in other vision problems such as image captioning, very few deep ZSL model exists and they show little advantage over ZSL models that utilise deep feature representations but do not learn an end-to-end embedding. In this paper we argue that the key to make deep ZSL models succeed is to choose the right embedding space. Instead of embedding into a semantic space or an intermediate space, we propose to use the visual space as the embedding space. This is because that in this space, the subsequent nearest neighbour search would suffer much less from the hubness problem and thus become more effective. This  model design also provides a natural mechanism for multiple semantic modalities (e.g., attributes and sentence descriptions) to be fused and optimised jointly in an end-to-end manner. Extensive experiments on four benchmarks show that our model significantly outperforms the existing models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Learning_a_Deep_CVPR_2017_paper.pdf",
        "aff": "Queen Mary University of London; Queen Mary University of London; Queen Mary University of London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05088v4",
        "pdf_size": 4566192,
        "gs_citation": 895,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13116201652944140927&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk",
        "email": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Learning_a_Deep_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Queen Mary University of London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.qmul.ac.uk",
        "aff_unique_abbr": "QMUL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Learning an Invariant Hilbert Space for Domain Adaptation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1489",
        "author_site": "Samitha Herath, Mehrtash Harandi, Fatih Porikli",
        "author": "Samitha Herath; Mehrtash Harandi; Fatih Porikli",
        "abstract": "This paper introduces a learning scheme to construct a Hilbert space (i.e., a vector space along its inner product) to address both unsupervised and semi-supervised domain adaptation problems. This is achieved by learning projections from each domain to a latent space along the Mahalanobis metric of the latent space to simultaneously minimizing a notion of domain variance while maximizing a measure of discriminatory power. In particular, we make use of the Riemannian optimization techniques to match statistical properties (e.g., first and second order statistics) between samples projected into the latent space from different domains. Upon availability of class labels, we further deem samples sharing the same label to form more compact clusters while pulling away samples coming from different classes.We extensively evaluate and contrast our proposal against state-of-the-art methods for the task of visual domain adaptation using both handcrafted and deep-net features. Our experiments show that even with a simple nearest neighbor classifier, the proposed method can outperform several state-of-the-art methods benefitting from more involved classification schemes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Herath_Learning_an_Invariant_CVPR_2017_paper.pdf",
        "aff": "Australian National University + DATA61-CSIRO; Australian National University + DATA61-CSIRO; Australian National University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Herath_Learning_an_Invariant_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.08350v2",
        "pdf_size": 2311696,
        "gs_citation": 149,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17181793109319356129&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "data61.csiro.au;data61.csiro.au;anu.edu.au",
        "email": "data61.csiro.au;data61.csiro.au;anu.edu.au",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Herath_Learning_an_Invariant_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0",
        "aff_unique_norm": "Australian National University;Commonwealth Scientific and Industrial Research Organisation",
        "aff_unique_dep": ";DATA61",
        "aff_unique_url": "https://www.anu.edu.au;https://www.csiro.au",
        "aff_unique_abbr": "ANU;CSIRO",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Learning and Refining of Privileged Information-Based RNNs for Action Recognition From Depth Sequences",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1280",
        "author_site": "Zhiyuan Shi, Tae-Kyun Kim",
        "author": "Zhiyuan Shi; Tae-Kyun Kim",
        "abstract": "Existing RNN-based approaches for action recognition from depth sequences require either skeleton joints or hand-crafted depth features as inputs. An end-to-end manner, mapping from raw depth maps to action classes, is non-trivial to design due to the fact that: 1) single channel map lacks texture thus weakens the discriminative power; 2) relatively small set of depth training data. To address these challenges, we propose to learn an RNN driven by privileged information (PI) in three-steps: An encoder is pre-trained to learn a joint embedding of depth appearance and PI (i.e. skeleton joints). The learned embedding layers are then tuned in the learning step, aiming to optimize the network by exploiting PI in a form of multi-task loss. However, exploiting PI as a secondary task provides little help to improve the performance of a primary task (i.e. classification) due to the gap between them. Finally, a bridging matrix is defined to connect two tasks by discovering latent PI in the refining step. Our PI-based classification loss maintains a consistency between latent PI and predicted distribution. The latent PI and network are iteratively estimated and updated in an expectation-maximization procedure. The proposed learning process provides greater discriminative power to model subtle depth difference, while helping avoid overfitting the scarcer training data. Our experiments show significant performance gains over state-of-the-art methods on three public benchmark datasets and our newly collected Blanket dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shi_Learning_and_Refining_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.09625v4",
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1536085078783564658&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shi_Learning_and_Refining_CVPR_2017_paper.html"
    },
    {
        "title": "Learning by Association -- A Versatile Semi-Supervised Training Method for Neural Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "38",
        "author_site": "Philip Haeusser, Alexander Mordvintsev, Daniel Cremers",
        "author": "Philip Haeusser; Alexander Mordvintsev; Daniel Cremers",
        "abstract": "In many real-world scenarios, labeled data for a specific machine learning task is costly to obtain. Semi-supervised training methods make use of abundantly available unlabeled data and a smaller number of labeled examples. We propose a new framework for semi-supervised training of deep neural networks inspired by learning in humans. \"Associations\" are made from embeddings of labeled samples to those of unlabeled ones and back. The optimization schedule encourages correct association cycles that end up at the same class from which the association was started and penalizes wrong associations ending at a different class. The implementation is easy to use and can be added to any existing end-to-end training setup. We demonstrate the capabilities of learning by association on several data sets and show that it can improve performance on classification tasks tremendously by making use of additionally available unlabeled data. In particular, for cases with few labeled data, our training scheme outperforms the current state of the art on SVHN.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Haeusser_Learning_by_Association_CVPR_2017_paper.pdf",
        "aff": "Dept. of Informatics, TU Munich + Google, Inc.; Google, Inc.; Dept. of Informatics, TU Munich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 690010,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11265585094702023868&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "in.tum.de;google.com;in.tum.de",
        "email": "in.tum.de;google.com;in.tum.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Haeusser_Learning_by_Association_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "Technical University of Munich;Google",
        "aff_unique_dep": "Department of Informatics;Google",
        "aff_unique_url": "https://www.tum.de;https://www.google.com",
        "aff_unique_abbr": "TUM;Google",
        "aff_campus_unique_index": "0+1;1;0",
        "aff_campus_unique": "Munich;Mountain View",
        "aff_country_unique_index": "0+1;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "title": "Learning the Multilinear Structure of Visual Data",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1914",
        "author_site": "Mengjiao Wang, Yannis Panagakis, Patrick Snape, Stefanos Zafeiriou",
        "author": "Mengjiao Wang; Yannis Panagakis; Patrick Snape; Stefanos Zafeiriou",
        "abstract": "Statistical decomposition methods are of paramount importance in discovering the modes of variations of visual data. Probably the most prominent linear decomposition method is the Principal Component Analysis (PCA), which discovers a single mode of variation in the data. However, in practice, visual data exhibit several modes of variations. For instance, the appearance of faces varies in identity, expression, pose etc. To extract these modes of variations from visual data, several supervised methods, such as the TensorFaces, that rely on multilinear (tensor) decomposition (e.g., Higher Order SVD) have been developed. The main drawbacks of such methods is that they require both labels regarding the modes of variations and the same number of samples under all modes of variations (e.g., the same face under different expressions, poses etc.). Therefore, their applicability is limited to well-organised data, usually captured in well-controlled conditions. In this paper, we propose the first general multilinear method, to the best of our knowledge, that discovers the multilinear structure of visual data in unsupervised setting. That is, without the presence of labels. We demonstrate the applicability of the proposed method in two applications, namely Shape from Shading (SfS) and expression transfer.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Learning_the_Multilinear_CVPR_2017_paper.pdf",
        "aff": "Imperial College London; Imperial College London; Imperial College London; Imperial College London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1231992,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5781191028592381838&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Learning_the_Multilinear_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Imperial College London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.imperial.ac.uk",
        "aff_unique_abbr": "ICL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Learning to Align Semantic Segmentation and 2.5D Maps for Geolocalization",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1265",
        "author_site": "Anil Armagan, Martin Hirzer, Peter M. Roth, Vincent Lepetit",
        "author": "Anil Armagan; Martin Hirzer; Peter M. Roth; Vincent Lepetit",
        "abstract": "We present an efficient method for geolocalization in urban environments starting from a coarse estimate of the location provided by a GPS and using a simple untextured 2.5D model of the surrounding buildings. Our key contribution is a novel efficient and robust method to optimize the pose: We train a Deep Network to predict the best direction to improve a pose estimate, given a semantic segmentation of the input image and a rendering of the buildings from this estimate. We then iteratively apply this CNN until converging to a good pose. This approach avoids the use of reference images of the surroundings, which are difficult to acquire and match, while 2.5D models are broadly available. We can therefore apply it to places unseen during training.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Armagan_Learning_to_Align_CVPR_2017_paper.pdf",
        "aff": "Institute of Computer Graphics and Vision, Graz University of Technology, Austria; Institute of Computer Graphics and Vision, Graz University of Technology, Austria; Institute of Computer Graphics and Vision, Graz University of Technology, Austria; Institute of Computer Graphics and Vision, Graz University of Technology, Austria",
        "project": "https://www.openstreetmap.org",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 911349,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6662176953826695473&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "email": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Armagan_Learning_to_Align_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "Institute of Computer Graphics and Vision",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TU Graz",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Graz",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Austria"
    },
    {
        "title": "Learning to Detect Salient Objects With Image-Level Supervision",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "55",
        "author_site": "Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, Xiang Ruan",
        "author": "Lijun Wang; Huchuan Lu; Yifan Wang; Mengyang Feng; Dong Wang; Baocai Yin; Xiang Ruan",
        "abstract": "Deep Neural Networks (DNNs) have substantially improved the state-of-the-art in salient object detection. However, training DNNs requires costly pixel-level annotations. In this paper, we leverage the observation that image-level tags provide important cues of foreground salient objects, and develop a weakly supervised learning method for saliency detection using image-level tags only. The Foreground Inference Network (FIN) is introduced for this challenging task. In the first stage of our training method, FIN is jointly trained with a fully convolutional network (FCN) for image-level tag prediction. A global smooth pooling layer is proposed, enabling FCN to assign object category tags to corresponding object regions, while FIN is capable of capturing all potential foreground regions with the predicted saliency maps. In the second stage, FIN is fine-tuned with its predicted saliency maps as ground truth. For refinement of ground truth, an iterative Conditional Random Field is developed to enforce spatial label consistency and further boost performance. Our method alleviates annotation efforts and allows the usage of existing large scale training sets with image-level tags. Our model runs at 60 FPS, outperforms unsupervised ones with a large margin, and achieves comparable or even superior performance than fully supervised counterparts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Learning_to_Detect_CVPR_2017_paper.pdf",
        "aff": "Dalian University of Technology; Dalian University of Technology; Dalian University of Technology; Dalian University of Technology; Dalian University of Technology; Dalian University of Technology; Tiwaki Co., Ltd",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1083034,
        "gs_citation": 1450,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1658869462297327047&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "mail.dlut.edu.cn;dlut.edu.cn; ; ; ; ; ",
        "email": "mail.dlut.edu.cn;dlut.edu.cn; ; ; ; ; ",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Learning_to_Detect_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;1",
        "aff_unique_norm": "Dalian University of Technology;Tiwaki Co., Ltd",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.dlut.edu.cn/;",
        "aff_unique_abbr": "DUT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;1",
        "aff_country_unique": "China;Japan"
    },
    {
        "title": "Learning to Extract Semantic Structure From Documents Using Multimodal Fully Convolutional Neural Networks",
        "session": "Machine Learning 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "2231",
        "author_site": "Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel Kifer, C. Lee Giles",
        "author": "Xiao Yang; Ersin Yumer; Paul Asente; Mike Kraley; Daniel Kifer; C. Lee Giles",
        "abstract": "We present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that  we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_Learning_to_Extract_CVPR_2017_paper.pdf",
        "aff": "The Pennsylvania State University; Adobe Research; Adobe Research; Adobe Research; The Pennsylvania State University; The Pennsylvania State University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yang_Learning_to_Extract_2017_CVPR_supplemental.pdf",
        "arxiv": "1706.02337v1",
        "pdf_size": 1617157,
        "gs_citation": 320,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17247449626306863793&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "psu.edu;adobe.com;adobe.com;adobe.com;cse.psu.edu;ist.psu.edu",
        "email": "psu.edu;adobe.com;adobe.com;adobe.com;cse.psu.edu;ist.psu.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Learning_to_Extract_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;0;0",
        "aff_unique_norm": "Pennsylvania State University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.psu.edu;https://research.adobe.com",
        "aff_unique_abbr": "PSU;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning to Learn From Noisy Web Videos",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "2140",
        "author_site": "Serena Yeung, Vignesh Ramanathan, Olga Russakovsky, Liyue Shen, Greg Mori, Li Fei-Fei",
        "author": "Serena Yeung; Vignesh Ramanathan; Olga Russakovsky; Liyue Shen; Greg Mori; Li Fei-Fei",
        "abstract": "Understanding the simultaneously very diverse and intricately fine-grained set of possible human actions is a critical open problem in computer vision. Manually labeling training videos is feasible for some action classes but doesn't scale to the full long-tailed distribution of actions. A promising way to address this is to leverage noisy data from web queries to learn new actions, using semi-supervised or \"webly-supervised\" approaches. However, these methods typically do not learn domain-specific knowledge, or rely on iterative hand-tuned data labeling policies. In this work, we instead propose a reinforcement learning-based formulation for selecting the right examples for training a classifier from noisy web search results. Our method uses Q-learning to learn a data labeling policy on a small labeled training dataset, and then uses this to automatically label noisy web data for new visual concepts. Experiments on the challenging Sports-1M action recognition benchmark as well as on additional fine-grained and newly emerging action classes demonstrate that our method is able to learn good labeling policies for noisy data and use this to learn accurate visual concept classifiers.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yeung_Learning_to_Learn_CVPR_2017_paper.pdf",
        "aff": "Stanford University; Stanford University; Carnegie Mellon University; Stanford University; Simon Fraser University; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1706.02884v1",
        "pdf_size": 1306921,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5427086584949426577&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cs.stanford.edu;stanford.edu;cmu.edu;stanford.edu;cs.sfu.ca;cs.stanford.edu",
        "email": "cs.stanford.edu;stanford.edu;cmu.edu;stanford.edu;cs.sfu.ca;cs.stanford.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yeung_Learning_to_Learn_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0;2;0",
        "aff_unique_norm": "Stanford University;Carnegie Mellon University;Simon Fraser University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.cmu.edu;https://www.sfu.ca",
        "aff_unique_abbr": "Stanford;CMU;SFU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "Learning to Predict Stereo Reliability Enforcing Local Consistency of Confidence Maps",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "894",
        "author_site": "Matteo Poggi, Stefano Mattoccia",
        "author": "Matteo Poggi; Stefano Mattoccia",
        "abstract": "Confidence measures estimate unreliable disparity assignments performed by a stereo matching algorithm and, as recently proved, can be used for several purposes. This paper aims at increasing, by means of a deep network, the effectiveness of state-of-the-art confidence measures exploiting the local consistency assumption. We exhaustively evaluated our proposal on 23 confidence measures, including 5 top-performing ones based on random-forests and CNNs, training our networks with two popular stereo algorithms and a small subset (25 out of 194 frames) of the KITTI 2012 dataset. Experimental results show that our approach dramatically increases the effectiveness of all the 23 confidence measures on the remaining frames. Moreover, without re-training, we report a further cross-evaluation on KITTI 2015 and Middlebury 2014 confirming that our proposal provides remarkable improvements for each confidence measure even when dealing with significantly different input data. To the best of our knowledge, this is the first method to move beyond conventional pixel-wise confidence estimation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Poggi_Learning_to_Predict_CVPR_2017_paper.pdf",
        "aff": "University of Bologna, Department of Computer Science and Engineering (DISI), Viale del Risorgimento 2, Bologna, Italy; University of Bologna, Department of Computer Science and Engineering (DISI), Viale del Risorgimento 2, Bologna, Italy",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Poggi_Learning_to_Predict_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1414656,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13189160024773326345&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "unibo.it;unibo.it",
        "email": "unibo.it;unibo.it",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Poggi_Learning_to_Predict_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Bologna",
        "aff_unique_dep": "Department of Computer Science and Engineering (DISI)",
        "aff_unique_url": "https://www.unibo.it",
        "aff_unique_abbr": "UNIBO",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bologna",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy"
    },
    {
        "title": "Learning to Rank Retargeted Images",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "1599",
        "author_site": "Yang Chen, Yong-Jin Liu, Yu-Kun Lai",
        "author": "Yang Chen; Yong-Jin Liu; Yu-Kun Lai",
        "abstract": "Image retargeting techniques that adjust images into different sizes have attracted much attention recently. Objective quality assessment (OQA) of image retargeting results is often desired to automatically select the best results. Existing OQA methods output an absolute score for each retargeted image and use these scores to compare different results. Observing that it is challenging even for human subjects to give consistent scores for retargeting results of different source images, in this paper we propose a learning-based OQA method that predicts the ranking of a set of retargeted images with the same source image. We show that this more manageable task helps achieve more consistent prediction to human preference and is sufficient for most application scenarios. To compute the ranking, we propose a simple yet efficient machine learning framework that uses a General Regression Neural Network (GRNN) to model a combination of seven elaborate OQA metrics. We then propose a simple scheme to transform the relative scores output from GRNN into a global ranking. We train our GRNN model using human preference data collected in the elaborate RetargetMe benchmark and evaluate our method based on the subjective study in RetargetMe. Moreover, we introduce a further subjective benchmark to evaluate the generalizability of different OQA methods. Experimental results demonstrate that our method outperforms eight representative OQA methods in ranking prediction and has better generalizability to different datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Learning_to_Rank_CVPR_2017_paper.pdf",
        "aff": "Tsinghua University, China; Tsinghua University, China; Cardiff University, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1002471,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3143130688928669490&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "tsinghua.edu.cn;tsinghua.edu.cn;cs.cf.ac.uk",
        "email": "tsinghua.edu.cn;tsinghua.edu.cn;cs.cf.ac.uk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Learning_to_Rank_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Tsinghua University;Cardiff University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.cardiff.ac.uk",
        "aff_unique_abbr": "THU;Cardiff",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "title": "Level Playing Field for Million Scale Face Recognition",
        "session": "Analyzing Humans 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "3276",
        "author_site": "Aaron Nech, Ira Kemelmacher-Shlizerman",
        "author": "Aaron Nech; Ira Kemelmacher-Shlizerman",
        "abstract": "Face recognition has the perception of a solved problem, however when tested at the million-scale exhibits dramatic variation in accuracies across the different algorithms [??]. Are the algorithms very different? Is access to good/big training data their secret weapon? Where should face recognition improve?  To address those questions, we created a benchmark, MF2,  that requires all algorithms to be trained on same data, and tested at the million scale.  MF2 is a public large-scale  set  with 672K identities and 4.7M photos created with the goal to level playing field for large scale face recognition. We contrast our results with findings from the other two large-scale benchmarks MegaFace Challenge and MS-Celebs-1M where groups were allowed to train on any private/public/big/small set.  Some key  discoveries: 1)  algorithms, trained on MF2, were able to achieve state of the art and comparable results to algorithms trained on massive private sets, 2) some  outperformed themselves once trained on MF2, 3) invariance to aging suffers from low accuracies as in MegaFace, identifying the need for larger age variations possibly within identities or adjustment of algorithms in future testing.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Nech_Level_Playing_Field_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "http://megaface.cs.washington.edu",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Nech_Level_Playing_Field_2017_CVPR_supplemental.pdf",
        "arxiv": "1705.00393",
        "pdf_size": 720834,
        "gs_citation": 256,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12932836311624990730&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Nech_Level_Playing_Field_CVPR_2017_paper.html"
    },
    {
        "title": "Lifting From the Deep: Convolutional 3D Pose Estimation From a Single Image",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "909",
        "author_site": "Denis Tome, Chris Russell, Lourdes Agapito",
        "author": "Denis Tome; Chris Russell; Lourdes Agapito",
        "abstract": "We propose a unified formulation for the problem of 3D human pose estimation from a single raw RGB image that reasons jointly about 2D joint estimation and 3D pose reconstruction to improve both tasks. We take an integrated approach that fuses probabilistic knowledge of 3D human pose with a multi-stage CNN architecture and uses the knowledge of plausible 3D landmark locations to refine the search for better 2D locations. The entire process is trained end-to-end, is extremely efficient and obtains state-of-the-art results on Human3.6M outperforming previous approaches both on 2D and 3D errors.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tome_Lifting_From_the_CVPR_2017_paper.pdf",
        "aff": "University College London; The Turing Institute + The University of Edinburgh; University College London",
        "project": "http://visual.cs.ucl.ac.uk/pubs/liftingFromTheDeep",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Tome_Lifting_From_the_2017_CVPR_supplemental.pdf",
        "arxiv": "1701.00295v4",
        "pdf_size": 1930586,
        "gs_citation": 651,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6594653164669483811&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.ucl.ac.uk;turing.ac.uk;cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk;turing.ac.uk;cs.ucl.ac.uk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tome_Lifting_From_the_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "University College London;Turing Institute;University of Edinburgh",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucl.ac.uk;https://turing.ac.uk;https://www.ed.ac.uk",
        "aff_unique_abbr": "UCL;Turing;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Light Field Blind Motion Deblurring",
        "session": "Computational Photography",
        "status": "Oral",
        "track": "main",
        "pid": "1572",
        "author_site": "Pratul P. Srinivasan, Ren Ng, Ravi Ramamoorthi",
        "author": "Pratul P. Srinivasan; Ren Ng; Ravi Ramamoorthi",
        "abstract": "We study the problem of deblurring light fields of general 3D scenes captured under 3D camera motion and present both theoretical and practical contributions. By analyzing the motion-blurred light field in the primal and Fourier domains, we develop intuition into the effects of camera motion on the light field, show the advantages of capturing a 4D light field instead of a conventional 2D image for motion deblurring, and derive simple analytical methods of motion deblurring in certain cases. We then present an algorithm to blindly deblur light fields of general scenes without any estimation of scene geometry, and demonstrate that we can recover both the sharp light field and the 3D camera motion path of real and synthetically-blurred light fields.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Srinivasan_Light_Field_Blind_CVPR_2017_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, San Diego",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.05416",
        "pdf_size": 7778820,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14348538487378585565&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;cs.ucsd.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;cs.ucsd.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Srinivasan_Light_Field_Blind_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, Berkeley;University of California, San Diego",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "UC Berkeley;UCSD",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Berkeley;San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Light Field Reconstruction Using Deep Convolutional Network on EPI",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "2806",
        "author_site": "Gaochang Wu, Mandan Zhao, Liangyong Wang, Qionghai Dai, Tianyou Chai, Yebin Liu",
        "author": "Gaochang Wu; Mandan Zhao; Liangyong Wang; Qionghai Dai; Tianyou Chai; Yebin Liu",
        "abstract": "In this paper, we take advantage of the clear texture structure of the epipolar plane image (EPI) in the light field data and model the problem of light field reconstruction from a sparse set of views as a CNN-based angular detail restoration on EPI. We indicate that one of the main challenges in sparsely sampled light field reconstruction is the information asymmetry between the spatial and angular domain, where the detail portion in the angular domain is damaged by undersampling. To balance the spatial and angular information, the spatial high frequency components of an EPI is removed using EPI blur, before feeding to the network. Finally, a non-blind deblur operation is used to recover the spatial detail suppressed by the EPI blur. We evaluate our approach on several datasets including synthetic scenes, real-world scenes and challenging microscope light field data. We demonstrate the high performance and robustness of the proposed framework compared with the state-of-the-arts algorithms. We also show a further application for depth enhancement by using the reconstructed light field.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wu_Light_Field_Reconstruction_CVPR_2017_paper.pdf",
        "aff": "State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University; Department of Automation, Tsinghua University; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University; Department of Automation, Tsinghua University; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University; Department of Automation, Tsinghua University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Wu_Light_Field_Reconstruction_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4061745,
        "gs_citation": 254,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12233835181528518959&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "mail.tsinghua.edu.cn; ; ; ; ;mail.tsinghua.edu.cn",
        "email": "mail.tsinghua.edu.cn; ; ; ; ;mail.tsinghua.edu.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wu_Light_Field_Reconstruction_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1;0;1",
        "aff_unique_norm": "Northeastern University;Tsinghua University",
        "aff_unique_dep": "State Key Laboratory of Synthetical Automation for Process Industries;Department of Automation",
        "aff_unique_url": "http://www.neu.edu.cn/;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "NEU;THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Link the Head to the \"Beak\": Zero Shot Learning From Noisy Text Description at Part Precision",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2394",
        "author_site": "Mohamed Elhoseiny, Yizhe Zhu, Han Zhang, Ahmed Elgammal",
        "author": "Mohamed Elhoseiny; Yizhe Zhu; Han Zhang; Ahmed Elgammal",
        "abstract": "In this paper, we study learning visual classifiers from   unstructured text description at part precision with no training images. We show that visual text terms can be encouraged to attend to its relevant parts, while image connections to non-visual text terms vanishes without any supervision. This learning process enables terms like \"peak\" to be linked to parts like only head for instance , while non-visual terms like \"migrate\" not to affect classifier prediction without part-text annotation.  Images are encoded by a part-based CNN that detect bird parts and learn part-specific learning representation. Part-based visual classifiers are predicted from text descriptions of unseen visual classifiers to facilitate classification without training images (also known as zero-shot recognition ). We performed our experiments on CUB200 dataset and improves the zero-shot recognition results from 34.2% to 44.0%.  We also created a large scale benchmark on  404 North American Bird Images with text descriptions, where we also showed that our method outperforming existing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Elhoseiny_Link_the_Head_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Elhoseiny_Link_the_Head_2017_CVPR_supplemental.pdf",
        "arxiv": "1709.01148v1",
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9896811656802015763&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Elhoseiny_Link_the_Head_CVPR_2017_paper.html"
    },
    {
        "title": "Linking Image and Text With 2-Way Nets",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1916",
        "author_site": "Aviv Eisenschtat, Lior Wolf",
        "author": "Aviv Eisenschtat; Lior Wolf",
        "abstract": "Linking two data sources is a basic building block in numerous computer vision problems.  Canonical Correlation Analysis (CCA) achieves this by utilizing a linear optimizer in order to maximize the correlation between the two views.  Recent work makes use of non-linear models, including deep learning techniques, that optimize the CCA loss in some feature space. In this paper, we introduce a novel, bi-directional neural network architecture for the task of matching vectors from two data sources. Our approach employs two tied neural network channels that project the two views into a common, maximally correlated space using the Euclidean loss. We show a direct link between the correlation-based loss and Euclidean loss, enabling the use of Euclidean loss for correlation maximization. To overcome common Euclidean regression optimization problems, we modify well-known techniques to our problem, including batch normalization and dropout. We show state of the art results on a number of computer vision matching tasks including MNIST image matching and sentence-image matching on the Flickr8k, Flickr30k and COCO datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Eisenschtat_Linking_Image_and_CVPR_2017_paper.pdf",
        "aff": "The Blavatnik School of Computer Science, Tel Aviv University, Israel; The Blavatnik School of Computer Science, Tel Aviv University, Israel + Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Eisenschtat_Linking_Image_and_2017_CVPR_supplemental.pdf",
        "arxiv": "1608.07973",
        "pdf_size": 480363,
        "gs_citation": 208,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4239506599415631870&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Eisenschtat_Linking_Image_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Tel Aviv University;Meta",
        "aff_unique_dep": "Blavatnik School of Computer Science;Facebook AI Research",
        "aff_unique_url": "https://www.tau.ac.il;https://research.facebook.com",
        "aff_unique_abbr": "TAU;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tel Aviv;",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "title": "Lip Reading Sentences in the Wild",
        "session": "Analyzing Humans 2",
        "status": "Oral",
        "track": "main",
        "pid": "2873",
        "author_site": "Joon Son Chung, Andrew Senior, Oriol Vinyals, Andrew Zisserman",
        "author": "Joon Son Chung; Andrew Senior; Oriol Vinyals; Andrew Zisserman",
        "abstract": "The goal of this work is to recognise phrases and sentences being spoken by a talking face,  with or without the audio.  Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, and in the wild videos.  Our key contributions are: (1) a 'Watch, Listen, Attend and Spell' (WLAS) network that learns to transcribe videos of mouth motion to characters; (2) a curriculum learning strategy to accelerate training and to reduce overfitting; (3) a 'Lip Reading Sentences' (LRS) dataset for visual speech recognition, consisting of over 100,000 natural sentences from British television.  The WLAS model trained on the LRS dataset surpasses the performance of all previous work on standard lip reading benchmark datasets, often by a significant margin. This lip reading performance beats a professional lip reader on videos from BBC television, and we also demonstrate that if audio is available, then visual information helps to improve speech recognition performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chung_Lip_Reading_Sentences_CVPR_2017_paper.pdf",
        "aff": "Department of Engineering Science, University of Oxford; DeepMind; DeepMind; Department of Engineering Science, University of Oxford + DeepMind",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05358",
        "pdf_size": 1338311,
        "gs_citation": 1063,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8837383922585807467&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": "robots.ox.ac.uk;google.com;google.com;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;google.com;google.com;robots.ox.ac.uk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chung_Lip_Reading_Sentences_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0+1",
        "aff_unique_norm": "University of Oxford;DeepMind",
        "aff_unique_dep": "Department of Engineering Science;",
        "aff_unique_url": "https://www.ox.ac.uk;https://deepmind.com",
        "aff_unique_abbr": "Oxford;DeepMind",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oxford;",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Local Binary Convolutional Neural Networks",
        "session": "Machine Learning 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "19",
        "author_site": "Felix Juefei-Xu, Vishnu Naresh Boddeti, Marios Savvides",
        "author": "Felix Juefei-Xu; Vishnu Naresh Boddeti; Marios Savvides",
        "abstract": "We propose local binary convolution (LBC), an efficient alternative to convolutional layers in standard convolutional neural networks (CNN). The design principles of LBC are motivated by local binary patterns (LBP). The LBC layer comprises of a set of fixed sparse pre-defined binary convolutional filters that are not updated during the training process, a non-linear activation function and a set of learnable linear weights. The linear weights combine the activated filter responses to approximate the corresponding activated filter responses of a standard convolutional layer. The LBC layer affords significant parameter savings, 9x to 169x in the number of learnable parameters compared to a standard convolutional layer. Furthermore, the sparse and binary nature of the weights also results in up to 9x to 169x savings in model size compared to a standard convolutional layer. We demonstrate both theoretically and experimentally that our local binary convolution layer is a good approximation of a standard convolutional layer. Empirically, CNNs with LBC layers, called local binary convolutional neural networks (LBCNN), achieves performance parity with regular CNNs on a range of visual datasets (MNIST, SVHN, CIFAR-10, and ImageNet) while enjoying significant computational savings.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Juefei-Xu_Local_Binary_Convolutional_CVPR_2017_paper.pdf",
        "aff": "Carnegie Mellon University; Michigan State University; Carnegie Mellon University",
        "project": "http://xujuefei.com/lbcnn",
        "github": "",
        "supp": "",
        "arxiv": "1608.06049",
        "pdf_size": 2320710,
        "gs_citation": 351,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14717113966456221359&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "cmu.edu;msu.edu;ri.cmu.edu",
        "email": "cmu.edu;msu.edu;ri.cmu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Juefei-Xu_Local_Binary_Convolutional_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Michigan State University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.msu.edu",
        "aff_unique_abbr": "CMU;MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Locality-Sensitive Deconvolution Networks With Gated Fusion for RGB-D Indoor Semantic Segmentation",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1117",
        "author_site": "Yanhua Cheng, Rui Cai, Zhiwei Li, Xin Zhao, Kaiqi Huang",
        "author": "Yanhua Cheng; Rui Cai; Zhiwei Li; Xin Zhao; Kaiqi Huang",
        "abstract": "This paper focuses on indoor semantic segmentation using RGB-D data. Although the commonly used deconvolution networks (DeconvNet) have achieved impressive results on this task, we find there is still room for improvements in two aspects. One is about the boundary segmentation. DeconvNet aggregates large context to predict the label of each pixel, inherently limiting the segmentation precision of object boundaries. The other is about RGB-D fusion. Recent state-of-the-art methods generally fuse RGB and depth networks with equal-weight score fusion, regardless of the varying contributions of the two modalities on delineating different categories in different scenes. To address the two problems, we first propose a locality-sensitive DeconvNet (LS-DeconvNet) to refine the boundary segmentation over each modality. LS-DeconvNet incorporates locally visual and geometric cues from the raw RGB-D data into each DeconvNet, which is able to learn to upsample the coarse convolutional maps with large context whilst recovering sharp object boundaries. Towards RGB-D fusion, we introduce a gated fusion layer to effectively combine the two LS-DeconvNets. This layer can learn to adjust the contributions of RGB and depth over each pixel for high-performance object recognition. Experiments on the large-scale SUN RGB-D dataset and the popular NYU-Depth v2 dataset show that our approach achieves new state-of-the-art results for RGB-D indoor semantic segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cheng_Locality-Sensitive_Deconvolution_Networks_CVPR_2017_paper.pdf",
        "aff": "CRIPAC&NLPR, CASIA+University of Chinese Academy of Sciences; Microsoft Research; Microsoft Research; CRIPAC&NLPR, CASIA+University of Chinese Academy of Sciences; CRIPAC&NLPR, CASIA+University of Chinese Academy of Sciences+CAS Center for Excellence in Brain Science and Intelligence Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 823017,
        "gs_citation": 273,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2732598613105809720&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cheng_Locality-Sensitive_Deconvolution_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2;2;0+1;0+1+3",
        "aff_unique_norm": "Chinese Academy of Sciences Institute of Automation;University of Chinese Academy of Sciences;Microsoft;Chinese Academy of Sciences",
        "aff_unique_dep": "CRIPAC (Computational Intelligence & Pattern Analysis Group) & NLPR (National Laboratory of Pattern Recognition);;Microsoft Research;Center for Excellence in Brain Science and Intelligence Technology",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;https://www.microsoft.com/en-us/research;http://www.cas.cn/",
        "aff_unique_abbr": "CASIA;UCAS;MSR;CAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;1;0+0;0+0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-Grained Image Recognition",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Oral",
        "track": "main",
        "pid": "1813",
        "author_site": "Jianlong Fu, Heliang Zheng, Tao Mei",
        "author": "Jianlong Fu; Heliang Zheng; Tao Mei",
        "abstract": "Recognizing fine-grained categories (e.g., bird species) is difficult due to the challenges of discriminative region localization and fine-grained feature learning. Existing approaches predominantly solve these challenges independently, while neglecting the fact that region detection and fine-grained feature learning are mutually correlated and thus can reinforce each other. In this paper, we propose a novel recurrent attention convolutional neural network (RA-CNN) which recursively learns discriminative region attention and region-based feature representation at multiple scales in a mutual reinforced way. The learning at each scale consists of a classification sub-network and an attention proposal sub-network (APN). The APN starts from full images, and iteratively generates region attention from coarse to fine by taking previous prediction as a reference, while the finer scale network takes as input an amplified attended region from previous scale in a recurrent way. The proposed RA-CNN is optimized by an intra-scale classification loss and an inter-scale ranking loss, to mutually learn accurate region attention and fine-grained representation. RA-CNN does not need bounding box/part annotations and can be trained end-to-end. We conduct comprehensive experiments and show that RA-CNN achieves the best performance in three fine-grained tasks, with relative accuracy gains of 3.3%, 3.7%, 3.8%, on CUB Birds, Stanford Dogs and Stanford Cars, respectively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf",
        "aff": "Microsoft Research, Beijing, China; University of Science and Technology of China, Hefei, China; Microsoft Research, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2274951,
        "gs_citation": 1640,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15209829152531227222&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "microsoft.com;mail.ustc.edu.cn;microsoft.com",
        "email": "microsoft.com;mail.ustc.edu.cn;microsoft.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Fu_Look_Closer_to_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Microsoft;University of Science and Technology of China",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-asia;http://www.ustc.edu.cn",
        "aff_unique_abbr": "MSR;USTC",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Beijing;Hefei",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Look Into Person: Self-Supervised Structure-Sensitive Learning and a New Benchmark for Human Parsing",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "306",
        "author_site": "Ke Gong, Xiaodan Liang, Dongyu Zhang, Xiaohui Shen, Liang Lin",
        "author": "Ke Gong; Xiaodan Liang; Dongyu Zhang; Xiaohui Shen; Liang Lin",
        "abstract": "Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark \"Look into Person (LIP)\" that makes a significant advance in terms of scalability, diversity and difficulty, a contribution that we feel is crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analysis of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for specifically labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-Person-Part dataset demonstrate the superiority of our method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gong_Look_Into_Person_CVPR_2017_paper.pdf",
        "aff": "Sun Yat-sen University; Sun Yat-sen University+Carnegie Mellon University; Sun Yat-sen University; Adobe Research; Sun Yat-sen University+SenseTime Group (Limited)",
        "project": "http://hcp.sysu.edu.cn/lip",
        "github": "",
        "supp": "",
        "arxiv": "1703.05446v2",
        "pdf_size": 2270486,
        "gs_citation": 621,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11879150842411732238&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "mail2.sysu.edu.cn;cs.cmu.edu;mail.sysu.edu.cn;adobe.com;mail.sysu.edu.cn",
        "email": "mail2.sysu.edu.cn;cs.cmu.edu;mail.sysu.edu.cn;adobe.com;mail.sysu.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gong_Look_Into_Person_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;0;2;0+3",
        "aff_unique_norm": "Sun Yat-sen University;Carnegie Mellon University;Adobe;SenseTime Group",
        "aff_unique_dep": ";;Adobe Research;",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://www.cmu.edu;https://research.adobe.com;https://www.sensetime.com",
        "aff_unique_abbr": "SYSU;CMU;Adobe;SenseTime",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;1;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Loss Max-Pooling for Semantic Image Segmentation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "778",
        "author_site": "Samuel Rota Bul\u00c3\u00b2, Gerhard Neuhold, Peter Kontschieder",
        "author": "Samuel Rota Bulo; Gerhard Neuhold; Peter Kontschieder",
        "abstract": "In this work we introduce a novel loss max-pooling concept for handling imbalanced training data distributions, applicable as alternative loss layer in the context of deep neural networks for semantic image segmentation tasks. Most real-world semantic segmentation datasets exhibit long tail distributions with few object categories comprising the majority of data and consequently biasing the classifiers towards them. Our method adaptively re-weights the contributions of each pixel based on their observed losses, targeting under-performing classification results as often encountered for under-represented object classes. Moreover, our approach goes beyond conventional cost-sensitive learning attempts through adaptive considerations that allow us to indirectly address both, inter- and intra-class imbalances. We provide a theoretical justification of our approach, complementary to experimental analyses on standard semantic segmentation datasets. In our experiments on the challenging Cityscapes and Pascal VOC 2012 segmentation benchmarks we find consistently improved results, demonstrating the efficacy of our approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bulo_Loss_Max-Pooling_for_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.02966",
        "pdf_size": 705315,
        "gs_citation": 164,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8629449472809848690&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bulo_Loss_Max-Pooling_for_CVPR_2017_paper.html"
    },
    {
        "title": "Low-Rank Bilinear Pooling for Fine-Grained Classification",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "132",
        "author_site": "Shu Kong, Charless Fowlkes",
        "author": "Shu Kong; Charless Fowlkes",
        "abstract": "Pooling second-order local feature statistics to form a high-dimensional bilinear feature has been shown to achieve state-of-the-art performance on a variety of fine-grained classification tasks. To address the computational demands of high feature dimensionality, we propose to represent the covariance features as a matrix and apply a low-rank bilinear classifier. The resulting classifier can be evaluated without explicitly computing the bilinear feature map which allows for a large reduction in the compute time as well as decreasing the effective number of parameters to be learned. To further compress the model, we propose a classifier co-decomposition that factorizes the collection of bilinear classifiers into a common factor and compact per-class terms. The co-decomposition idea can be deployed through two convolutional layers and trained in an end-to-end architecture. We suggest a simple yet effective initialization that avoids explicitly first training and factorizing the larger bilinear classifiers. Through extensive experiments, we show that our model achieves state-of-the-art performance on several public datasets for fine-grained classification trained with only category labels. Importantly, our final model is an order of magnitude smaller than the recently proposed compact bilinear model [??], and three orders smaller than the standard bilinear CNN model [??].",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kong_Low-Rank_Bilinear_Pooling_CVPR_2017_paper.pdf",
        "aff": "Dept. of Computer Science, University of California, Irvine; Dept. of Computer Science, University of California, Irvine",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05109v2",
        "pdf_size": 3147858,
        "gs_citation": 460,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2514479181302231781&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "ics.uci.edu;ics.uci.edu",
        "email": "ics.uci.edu;ics.uci.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kong_Low-Rank_Bilinear_Pooling_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Dept. of Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Low-Rank Embedded Ensemble Semantic Dictionary for Zero-Shot Learning",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "754",
        "author_site": "Zhengming Ding, Ming Shao, Yun Fu",
        "author": "Zhengming Ding; Ming Shao; Yun Fu",
        "abstract": "Zero-shot learning for visual recognition has received much interest in the most recent years. However, the semantic gap across visual features and their underlying semantics is still the biggest obstacle in zero-shot learning. To fight off this hurdle, we propose an effective Low-rank Embedded Semantic Dictionary learning (LESD) through ensemble strategy. Specifically, we formulate a novel framework to jointly seek a low-rank embedding and semantic dictionary to link visual features with their semantic representations, which manages to capture shared features across different observed classes. Moreover, ensemble strategy is adopted to learn multiple semantic dictionaries to constitute the latent basis for the unseen classes. Consequently, our model could extract a variety of visual characteristics within objects, which can be well generalized to unknown categories. Extensive experiments on several zero-shot benchmarks verify that the proposed model can outperform the state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ding_Low-Rank_Embedded_Ensemble_CVPR_2017_paper.pdf",
        "aff": "Department of ECE, College of Engineering, Northeastern University, Boston, USA + Computer and Information Science, University of Massachusetts Dartmouth, USA; Department of ECE, College of Engineering, Northeastern University, Boston, USA; College of Computer and Information Science, Northeastern University, Boston, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1991929,
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8482248499212299884&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ece.neu.edu;umassd.edu;ece.neu.edu",
        "email": "ece.neu.edu;umassd.edu;ece.neu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ding_Low-Rank_Embedded_Ensemble_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Northeastern University;University of Massachusetts Dartmouth",
        "aff_unique_dep": "Department of ECE;Computer and Information Science",
        "aff_unique_url": "https://www.northeastern.edu;https://www.umassd.edu",
        "aff_unique_abbr": "NU;UMass Dartmouth",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Boston;Dartmouth",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Low-Rank-Sparse Subspace Representation for Robust Regression",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3854",
        "author_site": "Yongqiang Zhang, Daming Shi, Junbin Gao, Dansong Cheng",
        "author": "Yongqiang Zhang; Daming Shi; Junbin Gao; Dansong Cheng",
        "abstract": "Learning robust regression model from high-dimensional corrupted data is an essential and difficult problem in many practical applications. The state-of-the-art methods have studied low-rank regression models that are robust against typical noises (like Gaussian noise and out-sample sparse noise) or outliers, such that a regression model can be learned from clean data lying on underlying subspaces. However, few of the existing low-rank regression methods can handle the outliers/noise lying on the sparsely corrupted disjoint subspaces. To address this issue, we propose a low-rank-sparse subspace representation for robust regression, hereafter referred to as LRS-RR in this paper. The main contribution include the following: (1) Unlike most of the existing regression methods, we propose an approach with two phases of low-rank-sparse subspace recovery and regression optimization being carried out simultaneously;(2) we also apply the linearized alternating direction method with adaptive penalty to solved the formulated LRS-RR problem and prove the convergence of the algorithm and analyze its complexity; (3) we demonstrate the efficiency of our method for the high-dimensional corrupted data on both synthetic data and two benchmark datasets against several state-of-the-art robust methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Low-Rank-Sparse_Subspace_Representation_CVPR_2017_paper.pdf",
        "aff": "Harbin Institute of Technology; Harbin Institute of Technology+Shenzhen University; The University of Sydney; Harbin Institute of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_Low-Rank-Sparse_Subspace_Representation_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 661731,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6885830198155233468&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "foxmail.com;hotmail.com;sydney.edu.au;hit.edu.cn",
        "email": "foxmail.com;hotmail.com;sydney.edu.au;hit.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Low-Rank-Sparse_Subspace_Representation_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;2;0",
        "aff_unique_norm": "Harbin Institute of Technology;Shenzhen University;University of Sydney",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.szu.edu.cn;https://www.sydney.edu.au",
        "aff_unique_abbr": "HIT;SZU;USYD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Harbin;",
        "aff_country_unique_index": "0;0+0;1;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "MCMLSD: A Dynamic Programming Approach to Line Segment Detection",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "744",
        "author_site": "Emilio J. Almaz\u00c3\u00a0n, Ron Tal, Yiming Qian, James H. Elder",
        "author": "Emilio J. Almazan; Ron Tal; Yiming Qian; James H. Elder",
        "abstract": "Prior approaches to line segment detection typically involve perceptual grouping in the image domain or global accumulation in the Hough domain.  Here we propose a probabilistic algorithm that merges the advantages of both approaches.  In a first stage lines are detected using a global probabilistic Hough approach.  In the second stage each detected line is analyzed in the image domain to localize the line segments that generated the peak in the Hough map.  By limiting search to a line, the distribution of segments over the sequence of points on the line can be modeled as a Markov chain, and a probabilistically optimal labelling can be computed exactly using a standard dynamic programming algorithm, in linear time.  The Markov assumption also leads to an intuitive ranking method that uses the local marginal posterior probabilities to estimate the expected number of correctly labelled points on a segment.  To assess the resulting Markov Chain Marginal Line Segment Detector (MCMLSD) we develop and apply a novel quantitative evaluation methodology that controls for under- and over-segmentation.  Evaluation on the YorkUrbanDB dataset shows that the proposed MCMLSD method outperforms the state-of-the-art by a substantial margin.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Almazan_MCMLSD_A_Dynamic_CVPR_2017_paper.pdf",
        "aff": "Madrid, Spain; Uber Technologies, San Francisco, USA; York University, Toronto, Canada; York University, Toronto, Canada",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Almazan_MCMLSD_A_Dynamic_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2982729,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8599044077742764990&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;uber.com;cse.yorku.ca;yorku.ca",
        "email": "gmail.com;uber.com;cse.yorku.ca;yorku.ca",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Almazan_MCMLSD_A_Dynamic_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "University of Madrid;Uber Technologies;York University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucm.es;https://www.uber.com;https://yorku.ca",
        "aff_unique_abbr": "UCM;Uber;York U",
        "aff_campus_unique_index": "1;2;2",
        "aff_campus_unique": ";San Francisco;Toronto",
        "aff_country_unique_index": "0;1;2;2",
        "aff_country_unique": "Spain;United States;Canada"
    },
    {
        "title": "MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network",
        "session": "Applications",
        "status": "Oral",
        "track": "main",
        "pid": "2871",
        "author_site": "Zizhao Zhang, Yuanpu Xie, Fuyong Xing, Mason McGough, Lin Yang",
        "author": "Zizhao Zhang; Yuanpu Xie; Fuyong Xing; Mason McGough; Lin Yang",
        "abstract": "The inability to interpret the model prediction in semantically and visually meaningful ways is a well-known shortcoming of most existing computer-aided diagnosis methods. In this paper, we propose MDNet to establish a direct multimodal mapping between medical images and diagnostic reports that can read images, generate diagnostic reports, retrieve images by symptom descriptions, and visualize attention, to provide justifications of the network diagnosis process. MDNet includes an image model and a language model. The image model is proposed to enhance multi-scale feature ensembles and utilization efficiency. The language model, integrated with our improved attention mechanism, aims to read and explore discriminative image feature descriptions from reports to learn a direct mapping from sentence words to image pixels. The overall network is trained end-to-end by using our developed optimization strategy. Based on a pathology bladder cancer images and its diagnostic reports (BCIDR) dataset, we conduct sufficient experiments to demonstrate that MDNet outperforms comparative baselines. The proposed image model obtains state-of-the-art performance on two CIFAR datasets as well.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_MDNet_A_Semantically_CVPR_2017_paper.pdf",
        "aff": "University of Florida; University of Florida; University of Florida; University of Florida; University of Florida",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1707.02485v1",
        "pdf_size": 1305343,
        "gs_citation": 448,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12284121009778506880&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cise.ufl.edu; ; ; ; ",
        "email": "cise.ufl.edu; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_MDNet_A_Semantically_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Florida",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ufl.edu",
        "aff_unique_abbr": "UF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "MIML-FCN+: Multi-Instance Multi-Label Learning via Fully Convolutional Networks With Privileged Information",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "582",
        "author_site": "Hao Yang, Joey Tianyi Zhou, Jianfei Cai, Yew Soon Ong",
        "author": "Hao Yang; Joey Tianyi Zhou; Jianfei Cai; Yew Soon Ong",
        "abstract": "Multi-instance multi-label (MIML) learning has many interesting applications in computer visions, including multi-object recognition and automatic image tagging. In these applications, additional information such as bounding-boxes, image captions and descriptions is often available during training phrase, which is referred as privileged information (PI). However, as existing works on learning using PI only consider instance-level PI (privileged instances), they fail to make use of bag-level PI (privileged bags) available in MIML learning. Therefore, in this paper, we propose a two-stream fully convolutional network, named MIML-FCN+, unified by a novel PI loss to solve the problem of MIML learning with privileged bags. Compared to the previous works on PI, the proposed MIML-FCN+ utilizes the readily available privileged bags, instead of hard-to-obtain privileged instances, making the system more general and practical in real world applications. As the proposed PI loss is convex and SGD-compatible and the framework itself is a fully convolutional network, MIML FCN+ can be easily integrated with state-of-the-art deep learning networks. Moreover, the flexibility of convolutional layers allows us to exploit structured correlations among instances to facilitate more effective training and testing. Experimental results on three benchmark datasets demonstrate the effectiveness of the proposed MIML-FCN+, outperforming state-of-the-art methods in the application of multi-object recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_MIML-FCN_Multi-Instance_Multi-Label_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science and Engineering, NTU, Singapore; IHPC, A*STAR, Singapore; School of Computer Science and Engineering, NTU, Singapore; School of Computer Science and Engineering, NTU, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 735281,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=562862554188994735&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;gmail.com; ; ",
        "email": "gmail.com;gmail.com; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_MIML-FCN_Multi-Instance_Multi-Label_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Nanyang Technological University;A*STAR",
        "aff_unique_dep": "School of Computer Science and Engineering;Institute of High Performance Computing",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.a-star.edu.sg",
        "aff_unique_abbr": "NTU;A*STAR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Making 360deg Video Watchable in 2D: Learning Videography for Click Free Viewing",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Spotlight",
        "track": "main",
        "pid": "2894",
        "author_site": "Yu-Chuan Su, Kristen Grauman",
        "author": "Yu-Chuan Su; Kristen Grauman",
        "abstract": "360deg video requires human viewers to actively control \"where\" to look while watching the video. Although it provides a more immersive experience of the visual content, it also introduces additional burden for viewers; awkward interfaces to navigate the video lead to suboptimal viewing experiences. Virtual cinematography is an appealing direction to remedy these problems, but conventional methods are limited to virtual environments or rely on hand-crafted heuristics. We propose a new algorithm for virtual cinematography that automatically controls a virtual camera within a 360deg video. Compared to the state of the art, our algorithm allows more general camera control, avoids redundant outputs, and extracts its output videos substantially more efficiently. Experimental results on over 7 hours of real \"in the wild\" video show that our generalized camera control is crucial for viewing 360deg video, while the proposed efficient algorithm is essential for making the generalized control computationally tractable.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Su_Making_360deg_Video_CVPR_2017_paper.pdf",
        "aff": "The University of Texas at Austin; The University of Texas at Austin",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Su_Making_360deg_Video_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1637521,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6852394163438069513&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Su_Making_360deg_Video_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach",
        "session": "Machine Learning 2",
        "status": "Oral",
        "track": "main",
        "pid": "720",
        "author_site": "Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, Lizhen Qu",
        "author": "Giorgio Patrini; Alessandro Rozza; Aditya Krishna Menon; Richard Nock; Lizhen Qu",
        "abstract": "We present a theoretically grounded approach to train deep neural networks, including recurrent networks, subject to class-dependent label noise. We propose two procedures for loss correction that are agnostic to both application domain and network architecture. They simply amount to at most a matrix inversion and multiplication, provided that we know the probability of each class being corrupted into another. We further show how one can estimate these probabilities, adapting a recent technique for noise estimation to the multi-class setting, and thus providing an end-to-end framework. Extensive experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 and a large scale dataset of clothing images employing a diversity of architectures --- stacking dense, convolutional, pooling, dropout, batch normalization, word embedding, LSTM and residual layers --- demonstrate the noise robustness of our proposals. Incidentally, we also prove that, when ReLU is the only non-linearity, the loss curvature is immune to class-dependent label noise.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Patrini_Making_Deep_Neural_CVPR_2017_paper.pdf",
        "aff": "Australian National University+Data61; Waynaut; Australian National University+Data61; Australian National University+Data61+University of Sydney; Australian National University+Data61",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1609.03683",
        "pdf_size": 875354,
        "gs_citation": 1853,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15043338876316879408&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": "data61.csiro.au;waynaut.com;data61.csiro.au;data61.csiro.au;data61.csiro.au",
        "email": "data61.csiro.au;waynaut.com;data61.csiro.au;data61.csiro.au;data61.csiro.au",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Patrini_Making_Deep_Neural_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2;0+1;0+1+3;0+1",
        "aff_unique_norm": "Australian National University;Data61;Waynaut;University of Sydney",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.anu.edu.au;https://data61.csiro.au;;https://www.sydney.edu.au",
        "aff_unique_abbr": "ANU;Data61;;USYD",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0+0;0+0",
        "aff_country_unique": "Australia;"
    },
    {
        "title": "Making the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3177",
        "author_site": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh",
        "author": "Yash Goyal; Tejas Khot; Douglas Summers-Stay; Dhruv Batra; Devi Parikh",
        "abstract": "Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset (Antol et al., ICCV 2015) by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners.  Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Goyal_Making_the_v_CVPR_2017_paper.pdf",
        "aff": "Virginia Tech; Virginia Tech; Army Research Laboratory; Georgia Institute of Technology; Georgia Institute of Technology",
        "project": "http://visualqa.org/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1553981,
        "gs_citation": 3740,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18356113909117574420&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 16,
        "aff_domain": "vt.edu;vt.edu;mail.mil;gatech.edu;gatech.edu",
        "email": "vt.edu;vt.edu;mail.mil;gatech.edu;gatech.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Goyal_Making_the_v_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;2;2",
        "aff_unique_norm": "Virginia Tech;Army Research Laboratory;Georgia Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.vt.edu;https://www.arl.army.mil;https://www.gatech.edu",
        "aff_unique_abbr": "VT;ARL;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Material Classification Using Frequency- and Depth-Dependent Time-Of-Flight Distortion",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "36",
        "author_site": "Kenichiro Tanaka, Yasuhiro Mukaigawa, Takuya Funatomi, Hiroyuki Kubo, Yasuyuki Matsushita, Yasushi Yagi",
        "author": "Kenichiro Tanaka; Yasuhiro Mukaigawa; Takuya Funatomi; Hiroyuki Kubo; Yasuyuki Matsushita; Yasushi Yagi",
        "abstract": "This paper presents a material classification method using an off-the-shelf Time-of-Flight (ToF) camera. We use a key observation that the depth measurement by a ToF camera is distorted in objects with certain materials, especially with translucent materials. We show that this distortion is caused by the variations of time domain impulse responses across materials and also by the measurement mechanism of the existing ToF cameras. Specifically, we reveal that the amount of distortion varies according to the modulation frequency of the ToF camera, the material of the object, and the distance between the camera and object. Our method uses the depth distortion of ToF measurements as features and achieves material classification of a scene. Effectiveness of the proposed method is demonstrated by numerical evaluation and real-world experiments, showing its capability of even classifying visually similar objects.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tanaka_Material_Classification_Using_CVPR_2017_paper.pdf",
        "aff": "Nara Institute of Science and Technology (NAIST); Nara Institute of Science and Technology (NAIST); Nara Institute of Science and Technology (NAIST); Nara Institute of Science and Technology (NAIST); Osaka Univerisity; Osaka Univerisity",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Tanaka_Material_Classification_Using_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2681791,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16308010900873753554&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "is.naist.jp;is.naist.jp;is.naist.jp;is.naist.jp;ist.osaka-u.ac.jp;sanken.osaka-u.ac.jp",
        "email": "is.naist.jp;is.naist.jp;is.naist.jp;is.naist.jp;ist.osaka-u.ac.jp;sanken.osaka-u.ac.jp",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tanaka_Material_Classification_Using_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "Nara Institute of Science and Technology;Osaka University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.naist.jp;https://www.osaka-u.ac.jp",
        "aff_unique_abbr": "NAIST;Osaka U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Matrix Tri-Factorization With Manifold Regularizations for Zero-Shot Learning",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1440",
        "author_site": "Xing Xu, Fumin Shen, Yang Yang, Dongxiang Zhang, Heng Tao Shen, Jingkuan Song",
        "author": "Xing Xu; Fumin Shen; Yang Yang; Dongxiang Zhang; Heng Tao Shen; Jingkuan Song",
        "abstract": "Zero-shot learning (ZSL) aims to recognize objects of unseen classes with available training data from another set of seen classes. Existing solutions are focused on exploring knowledge transfer via an intermediate semantic embedding (e.g.s, attributes) shared between seen and unseen classes. In this paper, we propose a novel projection framework based on matrix tri-factorization with manifold regularizations. Specifically, we learn the semantic embedding projection by decomposing the visual feature matrix under the guidance of semantic embedding and class label matrices. By additionally introducing manifold regularizations on visual data and semantic embeddings, the learned projection can effectively captures the geometrical manifold structure residing in both visual and semantic spaces. To avoid the projection domain shift problem, we devise an effective prediction scheme by exploiting the test-time manifold structure. Extensive experiments on four benchmark datasets show that our approach significantly outperforms the state-of-the-arts, yielding an average improvement  ratio by 7.4% and 31.9% for the recognition and retrieval task, respectively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Matrix_Tri-Factorization_With_CVPR_2017_paper.pdf",
        "aff": "Center for Future Media & School of Computer Science and Engineering, University of Electronic Science and Technology of China, China; Center for Future Media & School of Computer Science and Engineering, University of Electronic Science and Technology of China, China; Center for Future Media & School of Computer Science and Engineering, University of Electronic Science and Technology of China, China; Center for Future Media & School of Computer Science and Engineering, University of Electronic Science and Technology of China, China; Center for Future Media & School of Computer Science and Engineering, University of Electronic Science and Technology of China, China; Center for Future Media & School of Computer Science and Engineering, University of Electronic Science and Technology of China, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2140382,
        "gs_citation": 158,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16009579465769305395&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xu_Matrix_Tri-Factorization_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Electronic Science and Technology of China",
        "aff_unique_dep": "Center for Future Media & School of Computer Science and Engineering",
        "aff_unique_url": "http://www.uestc.edu.cn",
        "aff_unique_abbr": "UESTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Matting and Depth Recovery of Thin Structures Using a Focal Stack",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "3231",
        "author_site": "Chao Liu, Srinivasa G. Narasimhan, Artur W. Dubrawski",
        "author": "Chao Liu; Srinivasa G. Narasimhan; Artur W. Dubrawski",
        "abstract": "Thin structures such as fence, grass and vessels are common in photography and scientific imaging. They exhibit complex 3D structures with sharp depth variations/discontinuities and mutual occlusions. In this paper, we develop a method to estimate the occlusion matte and depths of thin structures from a focal image stack, which is obtained either by varying the focus/aperture of the lens or computed from a one-shot light field image. We propose an image formation model that explicitly describes the spatially varying optical blur and mutual occlusions for structures located at different depths. Based on the model, we derive an efficient MCMC inference algorithm that enables direct and analytical computations of the iterative update for the model/images without re-rendering images in the sampling process. Then, the depths of the thin structures are recovered using gradient descent with the differential terms computed using the image formation model. We apply the proposed method to scenes at both macro and micro scales. For macro-scale, we evaluate our method on scenes with complex 3D thin structures such as tree branches and grass. For micro-scale, we apply our method to in-vivo microscopic images of micro-vessels with diameters less than 50 um. To our knowledge, the proposed method is the first approach to reconstruct the 3D structures of micro-vessels from non-invasive in-vivo image measurements.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Matting_and_Depth_CVPR_2017_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Liu_Matting_and_Depth_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2172927,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12250777553467070025&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Matting_and_Depth_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Memory-Augmented Attribute Manipulation Networks for Interactive Fashion Search",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "562",
        "author_site": "Bo Zhao, Jiashi Feng, Xiao Wu, Shuicheng Yan",
        "author": "Bo Zhao; Jiashi Feng; Xiao Wu; Shuicheng Yan",
        "abstract": "We introduce a new fashion search protocol where attribute manipulation is allowed within the interaction between users and search engines, e.g. manipulating the color attribute of the clothing from red to blue. It is particularly useful for image-based search when the query image cannot perfectly match user's expectation of the desired product. To build such a search engine, we propose a novel memory-augmented Attribute Manipulation Network (AMNet) which can manipulate image representation at the attribute level. Given a query image and some attributes that need to modify, AMNet can manipulate the intermediate representation encoding the unwanted attributes and change them to the desired ones through following four novel components: (1) a dual-path CNN architecture for discriminative deep attribute representation learning; (2) a memory block with an internal memory and a neural controller for prototype attribute representation learning and hosting; (3) an attribute manipulation network to modify the representation of the query image with the prototype feature retrieved from the memory block; (4) a loss layer which jointly optimizes the attribute classification loss and a triplet ranking loss over triplet images for facilitating precise attribute manipulation and image retrieving. Extensive experiments conducted on two large-scale fashion search datasets, i.e. DARN and DeepFashion, have demonstrated that AMNet is able to achieve remarkably good performance compared with well-designed baselines in terms of effectiveness of attribute manipulation and search accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Memory-Augmented_Attribute_Manipulation_CVPR_2017_paper.pdf",
        "aff": "Southwest Jiaotong University+National University of Singapore; National University of Singapore; Southwest Jiaotong University; 360 AI Institute",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1464751,
        "gs_citation": 173,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16859601895200726814&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "my.swjtu.edu.cn;nus.edu.sg;swjtu.edu.cn;360.cn",
        "email": "my.swjtu.edu.cn;nus.edu.sg;swjtu.edu.cn;360.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhao_Memory-Augmented_Attribute_Manipulation_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;0;2",
        "aff_unique_norm": "Southwest Jiao Tong University;National University of Singapore;360 AI Institute",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.swjtu.edu.cn;https://www.nus.edu.sg;",
        "aff_unique_abbr": "SWJTU;NUS;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "title": "Mimicking Very Efficient Network for Object Detection",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2820",
        "author_site": "Quanquan Li, Shengying Jin, Junjie Yan",
        "author": "Quanquan Li; Shengying Jin; Junjie Yan",
        "abstract": "Current CNN based object detectors need initialization from pre-trained ImageNet classification models, which are usually time-consuming. In this paper, we present a fully convolutional feature mimic framework to train very efficient CNN based detectors, which do not need ImageNet pre-training and achieve competitive performance as the large and slow models. We add supervision from high-level features of the large networks in training to help the small network better learn object representation. More specifically, we conduct a mimic method for the features sampled from the entire feature map and use a transform layer to map features from the small network onto the same dimension of the large network. In training the small network, we optimize the similarity between features sampled from the same region on the feature maps of both networks. Extensive experiments are conducted on pedestrian and common object detection tasks using VGG, Inception and ResNet. On both Caltech and Pascal VOC, we show that the modified 2.5x accelerated Inception network achieves competitive performance as the full Inception Network. Our faster model runs at 80 FPS for a 1000x1500 large input with only a minor degradation of performance on Caltech.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf",
        "aff": "SenseTime; Beihang University; SenseTime",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Mimicking_Very_Efficient_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 667080,
        "gs_citation": 434,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10538698953371841988&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "sensetime.com;gmail.com;outlook.com",
        "email": "sensetime.com;gmail.com;outlook.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Mimicking_Very_Efficient_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "SenseTime;Beihang University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sensetime.com;http://www.buaa.edu.cn/",
        "aff_unique_abbr": "SenseTime;BUAA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Mind the Class Weight Bias: Weighted Maximum Mean Discrepancy for Unsupervised Domain Adaptation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "828",
        "author_site": "Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang, Yong Xu, Wangmeng Zuo",
        "author": "Hongliang Yan; Yukang Ding; Peihua Li; Qilong Wang; Yong Xu; Wangmeng Zuo",
        "abstract": "In domain adaptation, maximum mean discrepancy (MMD) has been widely adopted as a discrepancy metric between the distributions of source and target domains. However, existing MMD-based domain adaptation methods generally ignore the changes of class prior distributions, i.e., class weight bias across domains. This remains an open problem but ubiquitous for domain adaptation, which can be caused by changes in sample selection criteria and application scenarios. We show that MMD cannot account for class weight bias and results in degraded domain adaptation performance. To address this issue, a weighted MMD model is proposed in this paper. Specifically, we introduce class-specific auxiliary weights into the original MMD for exploiting the class prior probability on source and target domains, whose challenge lies in the fact that the class label in target domain is unavailable. To account for it, our proposed weighted MMD model is defined by introducing an auxiliary weight for each class in the source domain, and a classification EM algorithm is suggested by alternating between assigning the pseudo-labels, estimating auxiliary weights and updating model parameters. Extensive experiments demonstrate the superiority of our weighted MMD over conventional MMD for domain adaptation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yan_Mind_the_Class_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; Bio-Computing Research Center, Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.00609v1",
        "pdf_size": 1003618,
        "gs_citation": 777,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3305832518774276595&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "hit.edu.cn;163.com;dlut.edu.cn;mail.dlut.edu.cn;hitsz.edu.cn;hit.edu.cn",
        "email": "hit.edu.cn;163.com;dlut.edu.cn;mail.dlut.edu.cn;hitsz.edu.cn;hit.edu.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yan_Mind_the_Class_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;1;0;0",
        "aff_unique_norm": "Harbin Institute of Technology;Dalian University of Technology",
        "aff_unique_dep": "School of Computer Science and Technology;School of Information and Communication Engineering",
        "aff_unique_url": "http://www.hit.edu.cn/;http://en.dlut.edu.cn/",
        "aff_unique_abbr": "HIT;DUT",
        "aff_campus_unique_index": "0;0;1;1;2;0",
        "aff_campus_unique": "Harbin;Dalian;Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Minimum Delay Moving Object Detection",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "1725",
        "author_site": "Dong Lao, Ganesh Sundaramoorthi",
        "author": "Dong Lao; Ganesh Sundaramoorthi",
        "abstract": "We present a general framework and method for detection of an object in a video based on apparent motion. The object moves relative to background motion at some unknown time in the video, and the goal is to detect and segment the object as soon it moves in an online manner. Due to unreliability of motion between frames, more than two frames are needed to reliably detect the object. Our method is designed to detect the object(s) with minimum delay, i.e., frames after the object moves, constraining the false alarms. Experiments on a new extensive dataset for moving object detection show that our method achieves less delay for all false alarm constraints than existing state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lao_Minimum_Delay_Moving_CVPR_2017_paper.pdf",
        "aff": "King Abdullah University of Science & Technology (KAUST), Saudi Arabia; King Abdullah University of Science & Technology (KAUST), Saudi Arabia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Lao_Minimum_Delay_Moving_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 18529130,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11467787134322474727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "kaust.edu.sa;kaust.edu.sa",
        "email": "kaust.edu.sa;kaust.edu.sa",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lao_Minimum_Delay_Moving_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "King Abdullah University of Science & Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaust.edu.sa",
        "aff_unique_abbr": "KAUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Saudi Arabia"
    },
    {
        "title": "Mining Object Parts From CNNs via Active Question-Answering",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "129",
        "author_site": "Quanshi Zhang, Ruiming Cao, Ying Nian Wu, Song-Chun Zhu",
        "author": "Quanshi Zhang; Ruiming Cao; Ying Nian Wu; Song-Chun Zhu",
        "abstract": "Given a convolutional neural network (CNN) that is pre-trained for object classification, this paper proposes to use active question-answering to semanticize neural patterns in conv-layers of the CNN and mine part concepts. For each part concept, we mine neural patterns in the pre-trained CNN, which are related to the target part, and use these patterns to construct an And-Or graph (AOG) to represent a four-layer semantic hierarchy of the part. As an interpretable model, the AOG associates different CNN units with different explicit object parts. We use an active human-computer communication to incrementally grow such an AOG on the pre-trained CNN as follows. We allow the computer to actively identify objects, whose neural patterns cannot be explained by the current AOG. Then, the computer asks human about the unexplained objects, and uses the answers to automatically discover certain CNN patterns corresponding to the missing knowledge. We incrementally grow the AOG to encode new knowledge discovered during the active-learning process. In experiments, our method exhibits high learning efficiency. Our method uses about 1/6--1/3 of the part annotations for training, but achieves similar or better part-localization performance than fast-RCNN methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Mining_Object_Parts_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03173",
        "pdf_size": 2036097,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7214624288157195988&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Mining_Object_Parts_CVPR_2017_paper.html"
    },
    {
        "title": "Missing Modalities Imputation via Cascaded Residual Autoencoder",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "527",
        "author_site": "Luan Tran, Xiaoming Liu, Jiayu Zhou, Rong Jin",
        "author": "Luan Tran; Xiaoming Liu; Jiayu Zhou; Rong Jin",
        "abstract": "Affordable sensors lead to an increasing interest in acquiring and modeling data with multiple modalities. Learning from multiple modalities has shown to significantly improve performance in object recognition. However, in practice it is common that the sensing equipment experiences unforeseeable malfunction or configuration issues, leading to corrupted data with missing modalities. Most existing multi-modal learning algorithms could not handle missing modalities, and would discard either all modalities with missing values or all corrupted data. To leverage the valuable information in the corrupted data, we propose to impute the missing data by leveraging the relatedness among different modalities. Specifically, we propose a novel Cascaded Residual Autoencoder (CRA) to impute missing modalities. By stacking residual autoencoders, CRA grows iteratively to model the residual between the current prediction and original data. Extensive experiments demonstrate the superior performance of CRA on both the data imputation and the object recognition task on imputed data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tran_Missing_Modalities_Imputation_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 262,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9611332148722086700&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tran_Missing_Modalities_Imputation_CVPR_2017_paper.html"
    },
    {
        "title": "Model-Based Iterative Restoration for Binary Document Image Compression With Dictionary Learning",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2606",
        "author_site": "Yandong Guo, Cheng Lu, Jan P. Allebach, Charles A. Bouman",
        "author": "Yandong Guo; Cheng Lu; Jan P. Allebach; Charles A. Bouman",
        "abstract": "The inherent noise in the observed (e.g., scanned) binary document image degrades the image quality and harms the compression ratio through breaking the pattern repentance and adding entropy to the document images. In this paper, we design a cost function in Bayesian framework with dictionary learning. Minimizing our cost function produces a restored image which has better quality than that of the observed noisy image, and a dictionary for representing and encoding the image. After the restoration, we use this dictionary (from the same cost function) to encode the restored image following the symbol-dictionary framework by JBIG2 standard with the lossless mode. Experimental results with a variety of document images demonstrate that our method improves the image quality compared with the observed image, and simultaneously improves the compression ratio. For the test images with synthetic noise, our method reduces the number of flipped pixels by 48.2% and improves the compression ratio by 36.36% as compared with the best encoding methods. For the test images with real noise, our method visually improves the image quality, and outperforms the cutting-edge method by 28.27% in terms of the compression ratio.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Guo_Model-Based_Iterative_Restoration_CVPR_2017_paper.pdf",
        "aff": "Microsoft Research; Sony Electronics Inc.; Purdue University at West Lafayette; Purdue University at West Lafayette",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.07019",
        "pdf_size": 1209590,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2141239547245907595&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "microsoft.com;am.sony.com;purdue.edu;purdue.edu",
        "email": "microsoft.com;am.sony.com;purdue.edu;purdue.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Guo_Model-Based_Iterative_Restoration_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Microsoft;Sony Electronics Inc.;Purdue University",
        "aff_unique_dep": "Microsoft Research;;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.sony.com/electronics;https://www.purdue.edu",
        "aff_unique_abbr": "MSR;SEI;Purdue",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";West Lafayette",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Modeling Relationships in Referential Expressions With Compositional Modular Networks",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "384",
        "author_site": "Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, Kate Saenko",
        "author": "Ronghang Hu; Marcus Rohrbach; Jacob Andreas; Trevor Darrell; Kate Saenko",
        "abstract": "People often refer to entities in an image in terms of their relationships with other entities. For example, \"the black cat sitting under the table\" refers to both a \"black cat\" entity and its relationship with another \"table\" entity. Understanding these relationships is essential for interpreting and grounding such natural language expressions. Most prior work focuses on either grounding entire referential expressions holistically to one region, or localizing relationships based on a fixed set of categories. In this paper we instead present a modular deep architecture capable of analyzing referential expressions into their component parts, identifying entities and relationships mentioned in the input expression and grounding them all in the scene. We call this approach Compositional Modular Networks (CMNs): a novel architecture that learns linguistic analysis and visual inference end-to-end. Our approach is built around two types of neural modules that inspect local regions and pairwise interactions between regions. We evaluate CMNs on multiple referential expression datasets, outperforming state-of-the-art approaches on all tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hu_Modeling_Relationships_in_CVPR_2017_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; Boston University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Hu_Modeling_Relationships_in_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.09978v1",
        "pdf_size": 1937275,
        "gs_citation": 454,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=588283919486114163&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;bu.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;bu.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hu_Modeling_Relationships_in_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "University of California, Berkeley;Boston University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.bu.edu",
        "aff_unique_abbr": "UC Berkeley;BU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Modeling Sub-Event Dynamics in First-Person Action Recognition",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "3469",
        "author_site": "Hasan F. M. Zaki, Faisal Shafait, Ajmal Mian",
        "author": "Hasan F. M. Zaki; Faisal Shafait; Ajmal Mian",
        "abstract": "First-person videos have unique characteristics such as heavy egocentric motion, strong preceding events, salient transitional activities and post-event impacts. Action recognition methods designed for third person videos may not optimally represent actions captured by first-person videos. We propose a method to represent the high level dynamics of sub-events in first-person videos by dynamically pooling features of sub-intervals of time series using a temporal feature pooling function. The sub-event dynamics are then temporally aligned to make a new series. To keep track of how the sub-event dynamics evolve over time, we recursively employ the Fast Fourier Transform on a pyramidal temporal structure. The Fourier coefficients of the segment define the overall video representation. We perform experiments on two existing benchmark first-person video datasets which have been captured in a controlled environment. Addressing this gap, we introduce a new dataset collected from YouTube which has a larger number of classes and a greater diversity of capture conditions thereby more closely depicting real-world challenges in first-person video analysis. We compare our method to state-of-the-art first person and generic video recognition algorithms. Our method consistently outperforms the nearest competitors by 10.3%, 3.3% and 11.7% respectively on the three datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zaki_Modeling_Sub-Event_Dynamics_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science and Software Engineering, The University of Western Australia + Department of Mechatronics Engineering, International Islamic University Malaysia; National University of Sciences and Technology, Pakistan; School of Computer Science and Software Engineering, The University of Western Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1226994,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9313937958992839820&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "research.uwa.edu.au;seeks.edu.pk;uwa.edu.au",
        "email": "research.uwa.edu.au;seeks.edu.pk;uwa.edu.au",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zaki_Modeling_Sub-Event_Dynamics_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2;0",
        "aff_unique_norm": "University of Western Australia;International Islamic University Malaysia;National University of Sciences and Technology",
        "aff_unique_dep": "School of Computer Science and Software Engineering;Department of Mechatronics Engineering;",
        "aff_unique_url": "https://www.uwa.edu.au;https://www.iiu.edu.my;https://www.nust.edu.pk",
        "aff_unique_abbr": "UWA;IIUM;NUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;2;0",
        "aff_country_unique": "Australia;Malaysia;Pakistan"
    },
    {
        "title": "Modeling Temporal Dynamics and Spatial Configurations of Actions Using Two-Stream Recurrent Neural Networks",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "158",
        "author_site": "Hongsong Wang, Liang Wang",
        "author": "Hongsong Wang; Liang Wang",
        "abstract": "Recently, skeleton based action recognition gains more popularity due to cost-effective depth sensors coupled with real-time skeleton estimation algorithms. Traditional approaches based on handcrafted features are limited to represent the complexity of motion patterns. Recent methods that use Recurrent Neural Networks (RNN) to handle raw skeletons only focus on the contextual dependency in the temporal domain and neglect the spatial configurations of articulated skeletons. In this paper, we propose a novel two-stream RNN architecture to model both temporal dynamics and spatial configurations for skeleton based action recognition. We explore two different structures for the temporal stream: stacked RNN and hierarchical RNN. Hierarchical RNN is designed according to human body kinematics. We also propose two effective methods to model the spatial structure by converting the spatial graph into a sequence of joints. To improve generalization of our model, we further exploit 3D transformation based data augmentation techniques including rotation and scaling transformation to transform the 3D coordinates of skeletons during training. Experiments on 3D action recognition benchmark datasets show that our method brings a considerable improvement for a variety of actions, i.e., generic actions, interaction activities and gestures.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Modeling_Temporal_Dynamics_CVPR_2017_paper.pdf",
        "aff": "Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR) + University of Chinese Academy of Sciences (UCAS); Center for Excellence in Brain Science and Intelligence Technology (CEBSIT), Institute of Automation, Chinese Academy of Sciences (CASIA) + University of Chinese Academy of Sciences (UCAS)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.02581v2",
        "pdf_size": 726266,
        "gs_citation": 511,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=482534204636685153&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Modeling_Temporal_Dynamics_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2+1",
        "aff_unique_norm": "National Laboratory of Pattern Recognition;University of Chinese Academy of Sciences;Chinese Academy of Sciences",
        "aff_unique_dep": "Center for Research on Intelligent Perception and Computing;;Institute of Automation",
        "aff_unique_url": ";http://www.ucas.ac.cn;http://www.ia.cas.cn",
        "aff_unique_abbr": "NLPR;UCAS;CASIA",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "More Is Less: A More Complicated Network With Less Inference Complexity",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2516",
        "author_site": "Xuanyi Dong, Junshi Huang, Yi Yang, Shuicheng Yan",
        "author": "Xuanyi Dong; Junshi Huang; Yi Yang; Shuicheng Yan",
        "abstract": "In this paper, we present a novel and general network structure towards accelerating the inference process of convolutional neural networks, which is more complicated in network structure yet with less inference complexity. The core idea is to equip each original convolutional layer with another low-cost collaborative layer (LCCL), and the element-wise multiplication of the ReLU outputs of these two parallel layers produces the layer-wise output. The combined layer is potentially more discriminative than the original convolutional layer, and its inference is faster for two reasons: 1) the zero cells of the LCCL feature maps will remain zero after element-wise multiplication, and thus it is safe to skip the calculation of the corresponding high-cost convolution in the original convolutional layer; 2) LCCL is very fast if it is implemented as a 1*1 convolution or only a single filter shared by all channels. Extensive experiments on the CIFAR-10, CIFAR-100 and ILSCRC-2012 benchmarks show that our proposed network structure can accelerate the inference process by 32% on average with negligible performance drop.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dong_More_Is_Less_CVPR_2017_paper.pdf",
        "aff": "CAI, University of Technology Sydney; 2360 AI Institute; CAI, University of Technology Sydney; 2360 AI Institute + National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.08651v2",
        "pdf_size": 730846,
        "gs_citation": 390,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13439018732223603071&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "icloud.com;360.cn;uts.edu.au;360.cn",
        "email": "icloud.com;360.cn;uts.edu.au;360.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dong_More_Is_Less_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1+2",
        "aff_unique_norm": "University of Technology Sydney;AI Institute;National University of Singapore",
        "aff_unique_dep": "CAI;;",
        "aff_unique_url": "https://www.uts.edu.au;;https://www.nus.edu.sg",
        "aff_unique_abbr": "UTS;;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;2",
        "aff_country_unique": "Australia;;Singapore"
    },
    {
        "title": "MuCaLe-Net: Multi Categorical-Level Networks to Generate More Discriminating Features",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3088",
        "author_site": "Youssef Tamaazousti, Herv\u00c3\u00a9 Le Borgne, C\u00c3\u00a9line Hudelot",
        "author": "Youssef Tamaazousti; Herve Le Borgne; Celine Hudelot",
        "abstract": "In a transfer-learning scheme, the intermediate layers of a pre-trained CNN are employed as universal image representation to tackle many visual classification problems. The current trend to generate such representation is to learn a CNN on a large set of images labeled among the most specific categories. Such processes ignore potential relations between categories, as well as the categorical-levels used by humans to classify. In this paper, we propose Multi Categorical-Level Networks (MuCaLe-Net) that include human-categorization knowledge into the CNN learning process. A MuCaLe-Net separates generic categories from each other while it independently distinguishes specific ones. It thereby generates different features in the intermediate layers that are complementary when combined together. Advantageously, our method does not require additive data nor annotation to train the network. The extensive experiments over four publicly available benchmarks of image classification exhibit state-of-the-art performances.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tamaazousti_MuCaLe-Net_Multi_Categorical-Level_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Tamaazousti_MuCaLe-Net_Multi_Categorical-Level_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 948595,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11109323139569865461&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tamaazousti_MuCaLe-Net_Multi_Categorical-Level_CVPR_2017_paper.html"
    },
    {
        "title": "Multi-Attention Network for One Shot Learning",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "989",
        "author_site": "Peng Wang, Lingqiao Liu, Chunhua Shen, Zi Huang, Anton van den Hengel, Heng Tao Shen",
        "author": "Peng Wang; Lingqiao Liu; Chunhua Shen; Zi Huang; Anton van den Hengel; Heng Tao Shen",
        "abstract": "One-shot learning is a challenging problem where the aim is to recognize a class identified by a single training image. Given the practical importance of one-shot learning, it seems surprising that the rich information present in the class tag itself has largely been ignored. Most existing approaches restrict the use of the class tag to finding similar classes and transferring classifiers or metrics learned thereon. We demonstrate here, in contrast, that the class tag can inform one-shot learning as a guide to visual attention on the training image for creating the image representation. This is motivated by the fact that human beings can better interpret a training image if the class tag of the image is understood. Specifically, we design a neural network architecture which takes the semantic embedding of the class tag to generate attention maps and uses those attention maps to create the image features for one-shot learning. Note that unlike other applications, our task requires that the learned attention generator can be generalized to novel classes. We show that this can be realized by representing class tags with distributed word embeddings and learning the attention map generator from an auxiliary training set. Also, we design a multiple-attention scheme to extract richer information from the exemplar image and this leads to substantial performance improvement. Through comprehensive experiments, we show that the proposed approach leads to superior performance over the baseline methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Multi-Attention_Network_for_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1139125,
        "gs_citation": 112,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5743096347331865589&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Multi-Attention_Network_for_CVPR_2017_paper.html"
    },
    {
        "title": "Multi-Context Attention for Human Pose Estimation",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "678",
        "author_site": "Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L. Yuille, Xiaogang Wang",
        "author": "Xiao Chu; Wei Yang; Wanli Ouyang; Cheng Ma; Alan L. Yuille; Xiaogang Wang",
        "abstract": "In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on detailed descriptions for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semantic consistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive field of the network. These units are extensions of residual units with a side branch incorporating filters with larger receptive field, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts. Code has been made publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chu_Multi-Context_Attention_for_CVPR_2017_paper.pdf",
        "aff": "The Chinese University of Hong Kong, Hong Kong SAR, China; The Chinese University of Hong Kong, Hong Kong SAR, China; The Chinese University of Hong Kong, Hong Kong SAR, China + The University of Sydney, Sydney, Australia; Tsinghua University, Beijing, China; Johns Hopkins University, Baltimore, USA; The Chinese University of Hong Kong, Hong Kong SAR, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.07432v1",
        "pdf_size": 2392868,
        "gs_citation": 909,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8990777784067552312&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;mails.tsinghua.edu.cn;jhu.edu;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;mails.tsinghua.edu.cn;jhu.edu;ee.cuhk.edu.hk",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chu_Multi-Context_Attention_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0+1;2;3;0",
        "aff_unique_norm": "Chinese University of Hong Kong;University of Sydney;Tsinghua University;Johns Hopkins University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.sydney.edu.au;https://www.tsinghua.edu.cn;https://www.jhu.edu",
        "aff_unique_abbr": "CUHK;USYD;THU;JHU",
        "aff_campus_unique_index": "0;0;0+1;2;3;0",
        "aff_campus_unique": "Hong Kong SAR;Sydney;Beijing;Baltimore",
        "aff_country_unique_index": "0;0;0+1;0;2;0",
        "aff_country_unique": "China;Australia;United States"
    },
    {
        "title": "Multi-Level Attention Networks for Visual Question Answering",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1955",
        "author_site": "Dongfei Yu, Jianlong Fu, Tao Mei, Yong Rui",
        "author": "Dongfei Yu; Jianlong Fu; Tao Mei; Yong Rui",
        "abstract": "Inspired by the recent success of text-based question answering, visual question answering (VQA) is proposed to automatically answer natural language questions with the reference to a given image. Compared with text-based QA, VQA is more challenging because the reasoning process on visual domain needs both effective semantic embedding and fine-grained visual understanding. Existing approaches predominantly infer answers from the abstract low-level visual features, while neglecting the modeling of high-level image semantics and the rich spatial context of regions. To solve the challenges, we propose a multi-level attention network for visual question answering that can simultaneously reduce the semantic gap by semantic attention and benefit fine-grained spatial inference by visual attention. First, we generate semantic concepts from high-level semantics in convolutional neural networks (CNN) and select those question-related concepts as semantic attention. Second, we encode region-based middle-level outputs from CNN into spatially-embedded representation by a bidirectional recurrent neural network, and further pinpoint the answer-related regions by multiple layer perceptron as visual attention. Third, we jointly optimize semantic attention, visual attention and question embedding by a softmax classifier to infer the final answer. Extensive experiments show the proposed approach outperforms the-state-of-arts on two challenging VQA datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_Multi-Level_Attention_Networks_CVPR_2017_paper.pdf",
        "aff": "University of Science and Technology of China; Microsoft Research; Microsoft Research; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 531659,
        "gs_citation": 275,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12972350228828661166&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mail.ustc.edu.cn;microsoft.com;microsoft.com;outlook.com",
        "email": "mail.ustc.edu.cn;microsoft.com;microsoft.com;outlook.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_Multi-Level_Attention_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Science and Technology of China;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "USTC;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Multi-Modal Mean-Fields via Cardinality-Based Clamping",
        "session": "Machine Learning 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "634",
        "author_site": "Pierre Baqu\u00c3\u00a9, Fran\u00c3\u00a7ois Fleuret, Pascal Fua",
        "author": "Pierre Baque; Francois Fleuret; Pascal Fua",
        "abstract": "Mean Field inference is central to statistical physics. It has attracted much  interest in the Computer Vision community to efficiently solve problems  expressible in terms of large Conditional Random Fields. However,  since it models the posterior probability distribution as a product of marginal probabilities,  it may fail to properly account for important dependencies between variables.  We therefore replace the fully factorized distribution of Mean Field  by a weighted mixture of such distributions, that similarly minimizes the  KL-Divergence to the true posterior.  By introducing two new ideas, namely,  conditioning on groups of variables instead of single ones and using a  parameter of the conditional random field potentials, that we identify to the  temperature in the sense of statistical physics to select such groups, we can  perform this minimization efficiently. Our extension of the clamping method  proposed in previous works allows us to both produce a more descriptive  approximation of the true posterior and, inspired by the diverse MAP paradigms,  fit a mixture of Mean Field approximations. We demonstrate that this  positively impacts real-world algorithms that initially relied on mean fields.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Baque_Multi-Modal_Mean-Fields_via_CVPR_2017_paper.pdf",
        "aff": "CVLab, EPFL, Lausanne, Switzerland; CVLab, EPFL, Lausanne, Switzerland + IDIAP, Martigny, Switzerland; CVLab, EPFL, Lausanne, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.07941v1",
        "pdf_size": 1366659,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8331449771121758071&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Baque_Multi-Modal_Mean-Fields_via_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "EPFL;IDIAP",
        "aff_unique_dep": "CVLab;",
        "aff_unique_url": "https://www.epfl.ch;https://www.idiap.ch",
        "aff_unique_abbr": "EPFL;",
        "aff_campus_unique_index": "0;0+1;0",
        "aff_campus_unique": "Lausanne;Martigny",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Multi-Object Tracking With Quadruplet Convolutional Neural Networks",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "2386",
        "author_site": "Jeany Son, Mooyeol Baek, Minsu Cho, Bohyung Han",
        "author": "Jeany Son; Mooyeol Baek; Minsu Cho; Bohyung Han",
        "abstract": "We propose Quadruplet Convolutional Neural Networks (Quad-CNN) for multi-object tracking, which learn to associate object detections across frames using quadruplet losses. The proposed networks consider target appearances together with their temporal adjacencies for data association. Unlike conventional ranking losses, the quadruplet loss enforces an additional  constraint that makes temporally adjacent detections more closely located than the ones with large temporal gaps. We also employ a multi-task loss to jointly learn object association and bounding box regression for better localization. The whole network is trained end-to-end. For tracking, the target association is performed by minimax label propagation using the metric learned from the proposed network. We evaluate performance of our multi-object tracking algorithm on public MOT Challenge datasets, and achieve outstanding results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Son_Multi-Object_Tracking_With_CVPR_2017_paper.pdf",
        "aff": "Dept. of Computer Science and Engineering, POSTECH, Korea; Dept. of Computer Science and Engineering, POSTECH, Korea; Dept. of Computer Science and Engineering, POSTECH, Korea; Dept. of Computer Science and Engineering, POSTECH, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 736332,
        "gs_citation": 320,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12894047541038404532&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "postech.ac.kr;postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Son_Multi-Object_Tracking_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "POSTECH",
        "aff_unique_dep": "Dept. of Computer Science and Engineering",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation",
        "session": "3D Vision 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "2250",
        "author_site": "Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, Nicu Sebe",
        "author": "Dan Xu; Elisa Ricci; Wanli Ouyang; Xiaogang Wang; Nicu Sebe",
        "abstract": "This paper addresses the problem of depth estimation from a single still image. Inspired by recent works on multi-scale convolutional neural networks (CNN), we propose a deep model which fuses complementary information derived from multiple CNN side outputs. Different from previous methods, the integration is obtained by means of continuous Conditional Random Fields (CRFs). In particular, we propose two different variations, one based on a cascade of multiple CRFs, the other on a unified graphical model. By designing a novel CNN implementation of mean-field updates for continuous CRFs, we show that both proposed models can be regarded as sequential deep networks and that training can be performed end-to-end. Through extensive experimental evaluation we demonstrate the effectiveness of the proposed approach and establish new state of the art results on publicly available datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Multi-Scale_Continuous_CRFs_CVPR_2017_paper.pdf",
        "aff": "University of Trento; Fondazione Bruno Kessler+University of Perugia; The Chinese University of Hong Kong+The University of Sydney; The Chinese University of Hong Kong; University of Trento",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.02157v1",
        "pdf_size": 1513683,
        "gs_citation": 561,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7841432366579581463&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "unitn.it;fbk.eu;ee.cuhk.edu.hk;ee.cuhk.edu.hk;unitn.it",
        "email": "unitn.it;fbk.eu;ee.cuhk.edu.hk;ee.cuhk.edu.hk;unitn.it",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xu_Multi-Scale_Continuous_CRFs_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+2;3+4;3;0",
        "aff_unique_norm": "University of Trento;Fondazione Bruno Kessler;University of Perugia;Chinese University of Hong Kong;University of Sydney",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.unitn.it;https://www.fbk.eu;https://www.unipg.it;https://www.cuhk.edu.hk;https://www.sydney.edu.au",
        "aff_unique_abbr": "UniTN;FBK;Unipg;CUHK;USYD",
        "aff_campus_unique_index": ";1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0+0;1+2;1;0",
        "aff_country_unique": "Italy;China;Australia"
    },
    {
        "title": "Multi-Scale FCN With Cascaded Instance Aware Segmentation for Arbitrary Oriented Word Spotting in the Wild",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "1304",
        "author_site": "Dafang He, Xiao Yang, Chen Liang, Zihan Zhou, Alexander G. Ororbi II, Daniel Kifer, C. Lee Giles",
        "author": "Dafang He; Xiao Yang; Chen Liang; Zihan Zhou; Alexander G. Ororbi II; Daniel Kifer; C. Lee Giles",
        "abstract": "Scene text detection has attracted great attention these years.  Text potentially exist in a wide variety of images or videos and play an important role in understanding the scene.  In this paper, we present a novel text detection algorithm which is composed of two cascaded steps:  (1) a multi-scale fully convolutional neural network (FCN) is proposed to extract text block regions;  (2) a novel instance (word or line) aware segmentation is designed to further remove false positives and obtain word instances.  The proposed algorithm can accurately localize word or text line in arbitrary orientations, including curved text lines which cannot be handled in a lot of other frameworks.  Our algorithm achieved state-of-the-art performance in ICDAR 2013 (IC13), ICDAR 2015 (IC15) and CUTE80 and Street View Text (SVT) benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/He_Multi-Scale_FCN_With_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/He_Multi-Scale_FCN_With_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2104896,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2594491644151256130&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/He_Multi-Scale_FCN_With_CVPR_2017_paper.html"
    },
    {
        "title": "Multi-Task Clustering of Human Actions by Sharing Information",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2849",
        "author_site": "Xiaoqiang Yan, Shizhe Hu, Yangdong Ye",
        "author": "Xiaoqiang Yan; Shizhe Hu; Yangdong Ye",
        "abstract": "Sharing information between multiple tasks can enhance the accuracy of human action recognition systems. However, using shared information to improve multi-task human action clustering has never been considered before, and cannot be achieved using existing clustering methods. In this work, we present a novel and effective Multi-Task Information Bottleneck (MTIB) clustering method, which is capable of exploring the shared information between multiple action clustering tasks to improve the performance of individual task. Our motivation is that, different action collections always share many similar action patterns, and exploiting the shared information can lead to improved performance. Specifically, MTIB generally formulates this problem as an information loss minimization function. In this function, the shared information can be quantified by the distributional correlation of clusters in different tasks, which is based on a high-level common vocabulary constructed through a novel agglomerative information maximization method. Extensive experiments on two kinds of challenging data sets, including realistic action data sets (HMDB & UCF50, Olympic & YouTube), and cross-view data sets (IXMAS, WVU), show that the proposed approach compares favorably to the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yan_Multi-Task_Clustering_of_CVPR_2017_paper.pdf",
        "aff": "School of Information Engineering, Zhengzhou University; School of Information Engineering, Zhengzhou University; School of Information Engineering, Zhengzhou University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1733366,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2080729373467502524&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;gs.zzu.edu.cn;zzu.edu.cn",
        "email": "gmail.com;gs.zzu.edu.cn;zzu.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yan_Multi-Task_Clustering_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Zhengzhou University",
        "aff_unique_dep": "School of Information Engineering",
        "aff_unique_url": "http://www.zzu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-Task Correlation Particle Filter for Robust Object Tracking",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "1758",
        "author_site": "Tianzhu Zhang, Changsheng Xu, Ming-Hsuan Yang",
        "author": "Tianzhu Zhang; Changsheng Xu; Ming-Hsuan Yang",
        "abstract": "In this paper, we propose a multi-task correlation particle filter (MCPF) for robust visual tracking. We first present the multi-task correlation filter (MCF) that takes the interdependencies among different features into account to learn correlation filters jointly. The proposed MCPF is designed to exploit and complement the strength of a MCF and a particle filter. Compared with existing tracking methods based on correlation filters and particle filters, the proposed tracker has several advantages. First, it can shepherd the sampled particles toward the modes of the target state distribution via the MCF, thereby resulting in robust tracking performance. Second, it can effectively handle large-scale variation via a particle sampling strategy. Third, it can effectively maintain multiple modes in the posterior density using fewer particles than conventional particle filters, thereby lowering the computational cost. Extensive experimental results on three benchmark datasets demonstrate that the proposed MCPF performs favorably against the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Multi-Task_Correlation_Particle_CVPR_2017_paper.pdf",
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences+University of Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences+University of Chinese Academy of Sciences; University of California at Merced",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1668028,
        "gs_citation": 498,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13534947624945305499&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "; ;",
        "email": "; ;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Multi-Task_Correlation_Particle_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;University of California, Merced",
        "aff_unique_dep": "Institute of Automation;;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;https://www.ucmerced.edu",
        "aff_unique_abbr": "CAS;UCAS;UC Merced",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Merced",
        "aff_country_unique_index": "0+0;0+0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Multi-View 3D Object Detection Network for Autonomous Driving",
        "session": "Machine Learning for 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "704",
        "author_site": "Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, Tian Xia",
        "author": "Xiaozhi Chen; Huimin Ma; Ji Wan; Bo Li; Tian Xia",
        "abstract": "This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf",
        "aff": "Department of Electronic Engineering, Tsinghua University; Department of Electronic Engineering, Tsinghua University; Baidu Inc.; Baidu Inc.; Baidu Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.07759v3",
        "pdf_size": 1425047,
        "gs_citation": 3953,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4264986223142283168&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "mails.tsinghua.edu.cn;tsinghua.edu.cn;baidu.com;baidu.com;baidu.com",
        "email": "mails.tsinghua.edu.cn;tsinghua.edu.cn;baidu.com;baidu.com;baidu.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Multi-View_3D_Object_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;1;1",
        "aff_unique_norm": "Tsinghua University;Baidu",
        "aff_unique_dep": "Department of Electronic Engineering;Baidu Inc.",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.baidu.com",
        "aff_unique_abbr": "THU;Baidu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-View Supervision for Single-View Reconstruction via Differentiable Ray Consistency",
        "session": "3D Vision 1",
        "status": "Oral",
        "track": "main",
        "pid": "950",
        "author_site": "Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, Jitendra Malik",
        "author": "Shubham Tulsiani; Tinghui Zhou; Alexei A. Efros; Jitendra Malik",
        "abstract": "We study the notion of consistency between a 3D shape and a 2D observation and propose a differentiable formulation which allows computing gradients of the 3D shape given an observation from an arbitrary view. We do so by reformulating view consistency using a differentiable ray consistency (DRC) term. We show that this formulation can be incorporated in a learning framework to leverage different types of multi-view observations e.g. foreground masks, depth, color images, semantics etc. as supervision for learning single-view 3D prediction. We present empirical analysis of our technique in a controlled setting. We also show that this approach allows us to improve over existing techniques for single-view reconstruction of objects from the PASCAL VOC dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tulsiani_Multi-View_Supervision_for_CVPR_2017_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "project": "https://shubhtuls.github.io/drc/",
        "github": "",
        "supp": "",
        "arxiv": "1704.06254",
        "pdf_size": 1050483,
        "gs_citation": 646,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14665868878676865845&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tulsiani_Multi-View_Supervision_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multi-Way Multi-Level Kernel Modeling for Neuroimaging Classification",
        "session": "Biomedical Image/Video Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "130",
        "author_site": "Lifang He, Chun-Ta Lu, Hao Ding, Shen Wang, Linlin Shen, Philip S. Yu, Ann B. Ragin",
        "author": "Lifang He; Chun-Ta Lu; Hao Ding; Shen Wang; Linlin Shen; Philip S. Yu; Ann B. Ragin",
        "abstract": "Owing to prominence as a diagnostic tool for probing the neural correlates of cognition, neuroimaging tensor data has been the focus of intense investigation. Although many supervised tensor learning approaches have been proposed, they either cannot capture the nonlinear relationships of tensor data or cannot preserve the complex multi-way structural information. In this paper, we propose a Multi-way Multi-level Kernel (MMK) model that can extract discriminative, nonlinear and structural preserving representations of tensor data. Specifically, we introduce a kernelized CP tensor factorization technique, which is equivalent to performing the low-rank tensor factorization in a possibly much higher dimensional space that is implicitly defined by the kernel function. We further employ a multi-way nonlinear feature mapping to derive the dual structural preserving kernels, which are used in conjunction with kernel machines (e.g., SVM). Extensive experiments on real-world neuroimages demonstrate that the proposed MMK method can effectively boost the classification performance on diverse brain disorders (i.e., Alzheimer's disease, ADHD, and HIV).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/He_Multi-Way_Multi-Level_Kernel_CVPR_2017_paper.pdf",
        "aff": "Shenzhen University, Shenzhen, China; University of Illinois at Chicago, Chicago, IL, USA; Beijing University of Posts and Telecommunications, Beijing, China; University of Illinois at Chicago, Chicago, IL, USA; Shenzhen University, Shenzhen, China; University of Illinois at Chicago, Chicago, IL, USA + Tsinghua University, Beijing, China; Northwestern University, Chicago, IL, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4482691,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9769520247033671819&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "gmail.com;gmail.com;szu.edu.cn;uic.edu;uic.edu;uic.edu;northwestern.edu",
        "email": "gmail.com;gmail.com;szu.edu.cn;uic.edu;uic.edu;uic.edu;northwestern.edu",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/He_Multi-Way_Multi-Level_Kernel_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;1;0;1+3;4",
        "aff_unique_norm": "Shenzhen University;University of Illinois at Chicago;Beijing University of Posts and Telecommunications;Tsinghua University;Northwestern University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.szu.edu.cn;https://www.uic.edu;http://www.bupt.edu.cn/;https://www.tsinghua.edu.cn;https://www.northwestern.edu",
        "aff_unique_abbr": "SZU;UIC;BUPT;THU;NU",
        "aff_campus_unique_index": "0;1;2;1;0;1+2;1",
        "aff_campus_unique": "Shenzhen;Chicago;Beijing",
        "aff_country_unique_index": "0;1;0;1;0;1+0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Multigrid Neural Architectures",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3053",
        "author_site": "Tsung-Wei Ke, Michael Maire, Stella X. Yu",
        "author": "Tsung-Wei Ke; Michael Maire; Stella X. Yu",
        "abstract": "We propose a multigrid extension of convolutional neural networks (CNNs). Rather than manipulating representations living on a single spatial grid, our network layers operate across scale space, on a pyramid of grids.  They consume multigrid inputs and produce multigrid outputs; convolutional filters themselves have both within-scale and cross-scale extent.  This aspect is distinct from simple multiscale designs, which only process the input at different scales.  Viewed in terms of information flow, a multigrid network passes messages across a spatial pyramid.  As a consequence, receptive field size grows exponentially with depth, facilitating rapid integration of context. Most critically, multigrid structure enables networks to learn internal attention and dynamic routing mechanisms, and use them to accomplish tasks on which modern CNNs fail.  Experiments demonstrate wide-ranging performance advantages of multigrid.  On CIFAR and ImageNet classification tasks, flipping from a single grid to multigrid within the standard CNN paradigm improves accuracy, while being compute and parameter efficient.  Multigrid is independent of other architectural choices; we show synergy in combination with residual connections.  Multigrid yields dramatic improvement on a synthetic semantic segmentation dataset.  Most strikingly, relatively shallow multigrid networks can learn to directly perform spatial transformation tasks, where, in contrast, current CNNs fail.  Together, our results suggest that continuous evolution of features on a multigrid pyramid is a more powerful alternative to existing CNN designs on a flat grid.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ke_Multigrid_Neural_Architectures_CVPR_2017_paper.pdf",
        "aff": "UC Berkeley / ICSI; TTI Chicago; UC Berkeley / ICSI",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ke_Multigrid_Neural_Architectures_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.07661",
        "pdf_size": 1108274,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11916179939561042521&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 15,
        "aff_domain": "icsi.berkeley.edu;ttic.edu;berkeley.edu",
        "email": "icsi.berkeley.edu;ttic.edu;berkeley.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ke_Multigrid_Neural_Architectures_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UC Berkeley;TTI",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Berkeley;Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2191",
        "author_site": "Xin Wang, Geoffrey Oxholm, Da Zhang, Yuan-Fang Wang",
        "author": "Xin Wang; Geoffrey Oxholm; Da Zhang; Yuan-Fang Wang",
        "abstract": "Transferring artistic styles onto everyday photographs has become an extremely popular task in both academia and industry.  Recently, offline training has replaced online iterative optimization, enabling nearly real-time stylization.  When those stylization networks are applied directly to high-resolution images, however, the style of localized regions often appears less similar to the desired artistic style.  This is because the transfer process fails to capture small, intricate textures and maintain correct texture scales of the artworks.  Here we propose a multimodal convolutional neural network that takes into consideration faithful representations of both color and luminance channels, and performs stylization hierarchically with multiple losses of increasing scales.  Compared to state-of-the-art networks, our network can also perform style transfer in nearly real-time by performing much more sophisticated training offline.  By properly handling style and texture cues at multiple scales using several modalities, we can transfer not just large-scale, obvious style cues but also subtle, exquisite ones.  That is, our scheme can generate results that are visually pleasing and more similar to multiple desired artistic styles with color and texture cues at multiple scales.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Multimodal_Transfer_A_CVPR_2017_paper.pdf",
        "aff": "University of California, Santa Barbara, CA + Adobe Research, San Francisco, CA; Adobe Research, San Francisco, CA; University of California, Santa Barbara, CA; University of California, Santa Barbara, CA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Wang_Multimodal_Transfer_A_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.01895v2",
        "pdf_size": 4810529,
        "gs_citation": 217,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17157984360378123899&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cs.ucsb.edu;adobe.com;cs.ucsb.edu;cs.ucsb.edu",
        "email": "cs.ucsb.edu;adobe.com;cs.ucsb.edu;cs.ucsb.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Multimodal_Transfer_A_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;0;0",
        "aff_unique_norm": "University of California, Santa Barbara;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.ucsb.edu;https://research.adobe.com",
        "aff_unique_abbr": "UCSB;Adobe",
        "aff_campus_unique_index": "0+1;1;0;0",
        "aff_campus_unique": "Santa Barbara;San Francisco",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multiple Instance Detection Network With Online Instance Classifier Refinement",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1036",
        "author_site": "Peng Tang, Xinggang Wang, Xiang Bai, Wenyu Liu",
        "author": "Peng Tang; Xinggang Wang; Xiang Bai; Wenyu Liu",
        "abstract": "Of late, weakly supervised object detection is with great importance in object recognition. Based on deep learning, weakly supervised detectors have achieved many promising results. However, compared with fully supervised detection, it is more challenging to train deep network based detectors in a weakly supervised manner. Here we formulate weakly supervised detection as a Multiple Instance Learning (MIL) problem, where instance classifiers (object detectors) are put into the network as hidden nodes. We propose a novel online instance classifier refinement algorithm to integrate MIL and the instance classifier refinement procedure into a single deep network, and train the network end-to-end with only image-level supervision, i.e., without object location information. More precisely, instance labels inferred from weak supervision are propagated to their spatially overlapped instances to refine instance classifier online. The iterative instance classifier refinement procedure is implemented using multiple streams in deep network, where each stream supervises its latter stream. Weakly supervised object detection experiments are carried out on the challenging PASCAL VOC 2007 and 2012 benchmarks. We obtain 47% mAP on VOC 2007 that significantly outperforms the previous state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tang_Multiple_Instance_Detection_CVPR_2017_paper.pdf",
        "aff": "School of EIC, Huazhong University of Science and Technology; School of EIC, Huazhong University of Science and Technology; School of EIC, Huazhong University of Science and Technology; School of EIC, Huazhong University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Tang_Multiple_Instance_Detection_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.00138v1",
        "pdf_size": 1322133,
        "gs_citation": 564,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4997816747796012577&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "hust.edu.cn;hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "email": "hust.edu.cn;hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tang_Multiple_Instance_Detection_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Huazhong University of Science and Technology",
        "aff_unique_dep": "School of EIC",
        "aff_unique_url": "http://www.hust.edu.cn",
        "aff_unique_abbr": "HUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multiple People Tracking by Lifted Multicut and Person Re-Identification",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1309",
        "author_site": "Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, Bernt Schiele",
        "author": "Siyu Tang; Mykhaylo Andriluka; Bjoern Andres; Bernt Schiele",
        "abstract": "Tracking multiple persons in a monocular video of a crowded scene is a challenging task. Humans can master it even if they loose track of a person locally by re-identifying the same person based on their appearance. Care must be taken across long distances, as similar-looking persons need not be identical. In this work, we propose a novel graph-based formulation that links and clusters person hypotheses over time by solving an instance of a minimum cost lifted multicut problem. Our model generalizes previous works by introducing a mechanism for adding long-range attractive connections between nodes in the graph without modifying the original set of feasible solutions. This allows us to reward tracks that assign detections of similar appearance to the same person in a way that does not introduce implausible solutions. To effectively match hypotheses over longer temporal gaps we develop new deep architectures for re-identification of people. They combine holistic representations extracted with deep networks and body pose layout obtained with a state-of-the-art pose estimation model. We demonstrate the effectiveness of our formulation by reporting a new state-of-the-art for the MOT16 benchmark.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tang_Multiple_People_Tracking_CVPR_2017_paper.pdf",
        "aff": "Max Planck Institute for Informatics, Saarbr \u00a8ucken, Germany + Max Planck Institute for Intelligent Systems, T \u00a8ubingen, Germany; Max Planck Institute for Informatics, Saarbr \u00a8ucken, Germany; Max Planck Institute for Informatics, Saarbr \u00a8ucken, Germany; Max Planck Institute for Informatics, Saarbr \u00a8ucken, Germany",
        "project": "http://mpi-inf.mpg.de/multicut_track",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1128257,
        "gs_citation": 703,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15533250553525552602&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tuebingen.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "email": "tuebingen.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tang_Multiple_People_Tracking_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Max Planck Institute for Informatics;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://mpi-inf.mpg.de;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "MPII;MPI-IS",
        "aff_campus_unique_index": "0+1;0;0;0",
        "aff_campus_unique": "Saarbr\u00fccken;T\u00fcbingen",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Multiple-Scattering Microphysics Tomography",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "3099",
        "author_site": "Aviad Levis, Yoav Y. Schechner, Anthony B. Davis",
        "author": "Aviad Levis; Yoav Y. Schechner; Anthony B. Davis",
        "abstract": "Scattering effects in images, including those related to haze, fog and appearance of clouds, are fundamentally dictated by microphysical characteristics of the scatterers. This work defines and derives recovery of these characteristics, in a three-dimensional (3D) heterogeneous medium. Recovery is based on a novel tomography approach. Multi-view (multi-angular) and multi-spectral data are linked to the underlying microphysics using 3D radiative transfer, accounting for multiple-scattering. Despite the nonlinearity of the tomography model, inversion is enabled using a few approximations that we describe. As a case study, we focus on passive remote sensing of the atmosphere, where scatterer retrieval can benefit modeling and forecasting of weather, climate and pollution.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Levis_Multiple-Scattering_Microphysics_Tomography_CVPR_2017_paper.pdf",
        "aff": "Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of Technology, Haifa, Israel; Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of Technology, Haifa, Israel; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1479753,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13490530128279600538&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;ee.technion.ac.il;jpl.nasa.gov",
        "email": "gmail.com;ee.technion.ac.il;jpl.nasa.gov",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Levis_Multiple-Scattering_Microphysics_Tomography_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Technion - Israel Institute of Technology;California Institute of Technology",
        "aff_unique_dep": "Viterbi Faculty of Electrical Engineering;Jet Propulsion Laboratory",
        "aff_unique_url": "https://www.technion.ac.il;https://www.caltech.edu",
        "aff_unique_abbr": "Technion;Caltech",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Haifa;Pasadena",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "title": "NID-SLAM: Robust Monocular SLAM Using Normalised Information Distance",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "536",
        "author_site": "Geoffrey Pascoe, Will Maddern, Michael Tanner, Pedro Pini\u00c3\u00a9s, Paul Newman",
        "author": "Geoffrey Pascoe; Will Maddern; Michael Tanner; Pedro Pinies; Paul Newman",
        "abstract": "We propose a direct monocular SLAM algorithm based on the Normalised Information Distance (NID) metric. In contrast to current state-of-the-art direct methods based on photometric error minimisation, our information-theoretic NID metric provides robustness to appearance variation due to lighting, weather and structural changes in the scene. We demonstrate successful localisation and mapping across changes in lighting with a synthetic indoor scene, and across changes in weather (direct sun, rain, snow) using real-world data collected from a vehicle-mounted camera. Our approach runs in real-time on a consumer GPU using OpenGL, and provides comparable localisation accuracy to state-of-the-art photometric methods but significantly outperforms both direct and feature-based methods in robustness to appearance changes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Pascoe_NID-SLAM_Robust_Monocular_CVPR_2017_paper.pdf",
        "aff": "Oxford Robotics Institute; Oxford Robotics Institute; Oxford Robotics Institute; Oxford Robotics Institute; Oxford Robotics Institute",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Pascoe_NID-SLAM_Robust_Monocular_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1127233,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15652211577783117220&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Pascoe_NID-SLAM_Robust_Monocular_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Oxford Robotics Institute",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Oral",
        "track": "main",
        "pid": "2955",
        "author_site": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba",
        "author": "David Bau; Bolei Zhou; Aditya Khosla; Aude Oliva; Antonio Torralba",
        "abstract": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a data set of concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are labeled across a broad range of visual concepts including objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability is an axis-independent property of the representation space, then we apply the method to compare the latent representations of various networks when trained to solve different classification problems.  We further analyze the effect of training iterations, compare networks trained with different initializations, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bau_Network_Dissection_Quantifying_CVPR_2017_paper.pdf",
        "aff": "CSAIL, MIT; CSAIL, MIT; CSAIL, MIT; CSAIL, MIT; CSAIL, MIT",
        "project": "http://netdissect.csail.mit.edu",
        "github": "",
        "supp": "",
        "arxiv": "1704.05796v1",
        "pdf_size": 2644088,
        "gs_citation": 1943,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18069685615852396783&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bau_Network_Dissection_Quantifying_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Network Sketching: Exploiting Binary Structure in Deep CNNs",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2575",
        "author_site": "Yiwen Guo, Anbang Yao, Hao Zhao, Yurong Chen",
        "author": "Yiwen Guo; Anbang Yao; Hao Zhao; Yurong Chen",
        "abstract": "Convolutional neural networks (CNNs) with deep architectures have substantially advanced the state-of-the-art in computer vision tasks. However, deep networks are typically resource-intensive and thus difficult to be deployed on mobile devices. Recently, CNNs with binary weights have shown compelling efficiency to the community, whereas the accuracy of such models is usually unsatisfactory in practice. In this paper, we introduce network sketching as a novel technique of pursuing binary-weight CNNs, targeting at more faithful inference and better trade-off for practical applications. Our basic idea is to exploit binary structure directly in pre-trained filter banks and to produce binary-weight models via tensor expansion. The whole process can be treated as a coarse-to-fine model approximation, akin to the pencil drawing steps of outlining and shading. To further speedup the generated models, namely the sketches, we also propose an associative implementation of binary tensor convolutions. Experimental results demonstrate that a proper sketch of AlexNet (or ResNet) outperforms the existing binary-weight models by large margins on the ImageNet large scale classification task, while the committed memory for network parameters only exceeds a little.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Guo_Network_Sketching_Exploiting_CVPR_2017_paper.pdf",
        "aff": "Intel Labs China; Intel Labs China; Department of Electronic Engineering, Tsinghua University + Intel Labs China; Intel Labs China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1706.02021v1",
        "pdf_size": 1838153,
        "gs_citation": 159,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11046710759848755875&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "intel.com;intel.com;intel.com;intel.com",
        "email": "intel.com;intel.com;intel.com;intel.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Guo_Network_Sketching_Exploiting_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1+0;0",
        "aff_unique_norm": "Intel;Tsinghua University",
        "aff_unique_dep": "Intel Labs;Department of Electronic Engineering",
        "aff_unique_url": "https://www.intel.cn;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "Intel;THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Neural Aggregation Network for Video Face Recognition",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1773",
        "author_site": "Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen, Hongdong Li, Gang Hua",
        "author": "Jiaolong Yang; Peiran Ren; Dongqing Zhang; Dong Chen; Fang Wen; Hongdong Li; Gang Hua",
        "abstract": "This paper presents a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with a variable number of face images as its input, and produces a compact, fixed-dimension feature representation for recognition. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN) which maps each face image to a feature vector. The aggregation module consists of two attention blocks which adaptively aggregate the feature vectors to form a single feature inside the convex hull spanned by them. Due to the attention mechanism, the aggregation is invariant to the image order. Our NAN is trained with a standard classification or verification loss without any extra supervision signal, and we found that it automatically learns to advocate high-quality face images while repelling low-quality ones such as blurred, occluded and improperly exposed faces. The experiments on IJB-A, YouTube Face, Celebrity-1000 video face recognition benchmarks show that it consistently outperforms naive aggregation methods and achieves the state-of-the-art accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_Neural_Aggregation_Network_CVPR_2017_paper.pdf",
        "aff": "Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; The Australian National University; Microsoft Research+Beijing Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1603.05474v4",
        "pdf_size": 2988010,
        "gs_citation": 495,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15408895152379008454&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Neural_Aggregation_Network_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;1;0+2",
        "aff_unique_norm": "Microsoft;Australian National University;Beijing Institute of Technology",
        "aff_unique_dep": "Microsoft Research;;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.anu.edu.au;http://www.bit.edu.cn/",
        "aff_unique_abbr": "MSR;ANU;BIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;1;0+2",
        "aff_country_unique": "United States;Australia;China"
    },
    {
        "title": "Neural Face Editing With Intrinsic Image Disentangling",
        "session": "Machine Learning 4",
        "status": "Oral",
        "track": "main",
        "pid": "2340",
        "author_site": "Zhixin Shu, Ersin Yumer, Sunil Hadap, Kalyan Sunkavalli, Eli Shechtman, Dimitris Samaras",
        "author": "Zhixin Shu; Ersin Yumer; Sunil Hadap; Kalyan Sunkavalli; Eli Shechtman; Dimitris Samaras",
        "abstract": "Traditional face editing methods often require a number of sophisticated and task specific algorithms to be applied one after the other --- a process that is tedious, fragile, and computationally intensive. In this paper, we propose an end-to-end generative adversarial network that infers a face-specific disentangled representation of intrinsic face properties, including shape (i.e. normals), albedo, and lighting, and an alpha matte. We show that this network can be trained on \"in-the-wild\" images by incorporating an in-network physically-based image formation module and appropriate loss functions. Our disentangling latent representation allows for semantically relevant edits, where one aspect of facial appearance can be manipulated while keeping orthogonal properties fixed, and we demonstrate its use for a number of facial editing applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shu_Neural_Face_Editing_CVPR_2017_paper.pdf",
        "aff": "Stony Brook University; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Stony Brook University+CentraleSup\u00e9lec, Universit\u00e9 Paris-Saclay",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Shu_Neural_Face_Editing_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.04131v1",
        "pdf_size": 1771271,
        "gs_citation": 340,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13118037453145658943&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.stonybrook.edu;adobe.com;adobe.com;adobe.com;adobe.com;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;adobe.com;adobe.com;adobe.com;adobe.com;cs.stonybrook.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shu_Neural_Face_Editing_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;1;0+2",
        "aff_unique_norm": "Stony Brook University;Adobe;CentraleSup\u00e9lec",
        "aff_unique_dep": ";Adobe Research;",
        "aff_unique_url": "https://www.stonybrook.edu;https://research.adobe.com;https://www.centralesupelec.fr",
        "aff_unique_abbr": "SBU;Adobe;CS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Paris-Saclay",
        "aff_country_unique_index": "0;0;0;0;0;0+1",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Neural Scene De-Rendering",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "220",
        "author_site": "Jiajun Wu, Joshua B. Tenenbaum, Pushmeet Kohli",
        "author": "Jiajun Wu; Joshua B. Tenenbaum; Pushmeet Kohli",
        "abstract": "e study the problem of holistic scene understanding. We would like to obtain a compact, expressive, and interpretable representation of scenes that encodes information such as the number of objects and their categories, poses, positions, etc. Such a representation would allow us to reason about and even reconstruct or manipulate elements of the scene. Previous works have used encoder-decoder based neural architectures to learn image representations; however, representations obtained in this way are typically uninterpretable, or only explain a single object in the scene.  In this work, we propose a new approach to learn an interpretable distributed representation of scenes. Our approach employs a deterministic rendering function as the decoder, mapping a naturally structured and disentangled scene description, which we named scene XML, to an image. By doing so, the encoder is forced to perform the inverse of the rendering operation (a.k.a. de-rendering) to transform an input image to the structured scene XML that the decoder used to produce the image. We use a object proposal based encoder that is trained by minimizing both the supervised prediction and the unsupervised reconstruction errors. Experiments demonstrate that our approach works well on scene de-rendering with two different graphics engines, and our learned representation can be easily adapted for a wide range of applications like image editing, inpainting, visual analogy-making, and image captioning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wu_Neural_Scene_De-Rendering_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2168992,
        "gs_citation": 173,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16649897812055201036&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wu_Neural_Scene_De-Rendering_CVPR_2017_paper.html"
    },
    {
        "title": "Newton-Type Methods for Inference in Higher-Order Markov Random Fields",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3555",
        "author_site": "Hariprasad Kannan, Nikos Komodakis, Nikos Paragios",
        "author": "Hariprasad Kannan; Nikos Komodakis; Nikos Paragios",
        "abstract": "Linear programming relaxations are central to MAP in- ference in discrete Markov Random Fields. The ability to properly solve the Lagrangian dual is a critical component of such methods. In this paper, we study the benefit of us- ing Newton-type methods to solve the Lagrangian dual of a smooth version of the problem. We investigate their abil- ity to achieve superior convergence behavior and to bet- ter handle the ill-conditioned nature of the formulation, as compared to first order methods. We show that it is indeed possible to efficiently apply a trust region Newton method for a broad range of MAP inference problems. In this pa- per we propose a provably globally efficient framework that includes (i) excellent compromise between computational complexity and precision concerning the Hessian matrix construction, (ii) a damping strategy that aids efficient opti- mization, (iii) a truncation strategy coupled with a generic pre-conditioner for Conjugate Gradients, (iv) efficient sum- product computation for sparse clique potentials. Results for higher-order Markov Random Fields demonstrate the potential of this approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kannan_Newton-Type_Methods_for_CVPR_2017_paper.pdf",
        "aff": "CentraleSup\u00e9lec-INRIA Saclay + Universit\u00e9 Paris-Saclay; Ecole des Ponts ParisTech + Universit\u00e9 Paris Est; CentraleSup\u00e9lec-INRIA Saclay + Universit\u00e9 Paris-Saclay",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kannan_Newton-Type_Methods_for_2017_CVPR_supplemental.pdf",
        "arxiv": "1709.01237v1",
        "pdf_size": 650453,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8413488800945837054&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "gmail.com;enpc.fr;ecp.fr",
        "email": "gmail.com;enpc.fr;ecp.fr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kannan_Newton-Type_Methods_for_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2+3;0+1",
        "aff_unique_norm": "CentraleSup\u00e9lec;Universit\u00e9 Paris-Saclay;Ecole des Ponts ParisTech;Universit\u00e9 Paris Est",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.centralesupelec.fr;https://www.universite-paris-saclay.fr;https://www.ponts.org;https://www.univ-Paris-est.fr",
        "aff_unique_abbr": "CS;UPSaclay;ENPC;UPE",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Saclay;",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "France"
    },
    {
        "title": "Noise Robust Depth From Focus Using a Ring Difference Filter",
        "session": "3D Vision 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "2807",
        "author_site": "Jaeheung Surh, Hae-Gon Jeon, Yunwon Park, Sunghoon Im, Hyowon Ha, In So Kweon",
        "author": "Jaeheung Surh; Hae-Gon Jeon; Yunwon Park; Sunghoon Im; Hyowon Ha; In So Kweon",
        "abstract": "Depth from focus (DfF) is a method of estimating depth of a scene by using the information acquired through the change of the focus of a camera. Within the framework of DfF, the focus measure (FM) forms the foundation on which the accuracy of the output is determined. With the result from the FM, the role of a DfF pipeline is to determine and recalculate unreliable measurements while enhancing those that are reliable. In this paper, we propose a new FM that more accurately and robustly measures focus, which we call the \"ring difference filter\" (RDF). FMs can usually be categorized as confident local methods or noise robust non-local methods. RDF's unique ring-and-disk structure allows it to have the advantageous sides of both local and non-local FMs. We then describe an efficient pipeline that utilizes the properties that the RDF brings. Our method is able to reproduce results that are on par with or even better than those of the state-of-the-art, while spending less time in computation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Surh_Noise_Robust_Depth_CVPR_2017_paper.pdf",
        "aff": "Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3523335,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15522273613978055660&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr",
        "email": "rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr;rcv.kaist.ac.kr",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Surh_Noise_Robust_Depth_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "KAIST",
        "aff_unique_dep": "Robotics and Computer Vision Lab.",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Noise-Blind Image Deblurring",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1300",
        "author_site": "Meiguang Jin, Stefan Roth, Paolo Favaro",
        "author": "Meiguang Jin; Stefan Roth; Paolo Favaro",
        "abstract": "We present a novel approach to noise-blind deblurring, the problem of deblurring an image with known blur, but unknown noise level. We introduce an efficient and robust solution based on a Bayesian framework using a smooth generalization of the 0-1 loss. A novel bound allows the calculation of very high-dimensional integrals in closed form. It avoids the degeneracy of Maximum a-Posteriori (MAP) estimates and leads to an effective noise-adaptive scheme. Moreover, we drastically accelerate our algorithm by using Majorization Minimization (MM) without introducing any approximation or boundary artifacts. We further speed up convergence by turning our algorithm into a neural network termed GradNet, which is highly parallelizable and can be efficiently trained. We demonstrate that our noise-blind formulation can be integrated with different priors and significantly improves existing deblurring algorithms in the noise-blind and in the known-noise case. Furthermore, GradNet leads to state-of-the-art performance across different noise levels, while retaining high computational efficiency.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jin_Noise-Blind_Image_Deblurring_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Jin_Noise-Blind_Image_Deblurring_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 18397291,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4441697517607265552&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jin_Noise-Blind_Image_Deblurring_CVPR_2017_paper.html"
    },
    {
        "title": "Noisy Softmax: Improving the Generalization Ability of DCNN via Postponing the Early Softmax Saturation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2255",
        "author_site": "Binghui Chen, Weihong Deng, Junping Du",
        "author": "Binghui Chen; Weihong Deng; Junping Du",
        "abstract": "Over the past few years, softmax and SGD have become a commonly used component and the default training strategy in CNN frameworks, respectively. However, when optimizing CNNs with SGD, the saturation behavior behind softmax always gives us an illusion of training well and then is omitted. In this paper, we first emphasize that the early saturation behavior of softmax will impede the exploration of SGD, which sometimes is a reason for model converging at a bad local-minima, then propose Noisy Softmax to mitigating this early saturation issue by injecting annealed noise in softmax during each iteration. This operation based on noise injection aims at postponing the early saturation and further bringing continuous gradients propagation so as to significantly encourage SGD solver to be more exploratory and help to find a better local-minima. This paper empirically verifies the superiority of the early softmax desaturation, and our method indeed improves the generalization ability of CNN model by regularization. We experimentally find that this early desaturation helps optimization in many tasks, yielding state-of-the-art or competitive results on several popular benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Noisy_Softmax_Improving_CVPR_2017_paper.pdf",
        "aff": "School of Information and Communication Engineering, Beijing University of Posts and Telecommunications; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications; School of Computer Science, Beijing University of Posts and Telecommunications",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1708.03769v1",
        "pdf_size": 670529,
        "gs_citation": 170,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3898462588333428186&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Noisy_Softmax_Improving_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "School of Information and Communication Engineering",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Non-Contact Full Field Vibration Measurement Based on Phase-Shifting",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1343",
        "author_site": "Hiroyuki Kayaba, Yuji Kokumai",
        "author": "Hiroyuki Kayaba; Yuji Kokumai",
        "abstract": "Vibration measurement systems are widely used in the industry.  A variety of vibration measurement techniques are proposed, including methods using an acceleration sensor, a laser displacement meter, and tracking a marker using a camera. However, these methods have limitations that allow only one point to be measured and require markers. We present a novel, non-contact full field joint measurement technique both of vibrations and shape based on phase-shifting. Our key idea is to acquire the frequency of vibrating objects using FFT to analyze the phase-shift error of vibrating objects. Our proposed algorithm estimates the phase-shift error by iterating frame-to-frame optimization and pixel-to-pixel optimization. A feature of our approach is to measure the surface of vibrating at different frequencies without markers or texture in full fields.  Our developed system is a low cost system, which is composed of a digital-light-processing (DLP) projector and camera (100 frames per second). The results of our experiments show that low frequency vibration of objects can be measured with high accuracy in non-contact.  Also, reconstruction of the vibrating object surface can be performed with high accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kayaba_Non-Contact_Full_Field_CVPR_2017_paper.pdf",
        "aff": "Nikon Corporation, Japan; Nikon Corporation, Japan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3503029,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6982669582350978260&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "nikon.com;nikon.com",
        "email": "nikon.com;nikon.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kayaba_Non-Contact_Full_Field_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Nikon Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nikon.com",
        "aff_unique_abbr": "Nikon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Non-Local Color Image Denoising With Convolutional Neural Networks",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1320",
        "author": "Stamatios Lefkimmiatis",
        "abstract": "We propose a novel deep network architecture for grayscale and color image denoising that is based on a non-local image model. Our motivation for the overall design of the proposed network stems from   variational methods that exploit the inherent non-local self-similarity property of natural  images. We build on this concept and introduce deep networks that perform non-local processing and  at the same time they significantly benefit from discriminative learning. Experiments on the Berkeley segmentation dataset, comparing several state-of-the-art methods, show that the proposed non-local models achieve the best reported denoising performance both for grayscale and color images for all the tested noise levels. It is also worth noting that this increase in performance comes at no extra cost on the capacity of the network compared to existing alternative deep network architectures. In addition, we highlight a direct link of the proposed non-local models to convolutional neural networks. This connection is of significant importance since it allows our models to take full advantage of the latest advances on GPU computing in deep learning and makes them amenable to efficient implementations through their inherent parallelism.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lefkimmiatis_Non-Local_Color_Image_CVPR_2017_paper.pdf",
        "aff": "Skolkovo Institute of Science and Technology (Skoltech), Moscow, Russia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Lefkimmiatis_Non-Local_Color_Image_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.06757v2",
        "pdf_size": 1929803,
        "gs_citation": 432,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5962346180042305607&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "skoltech.ru",
        "email": "skoltech.ru",
        "author_num": 1,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lefkimmiatis_Non-Local_Color_Image_CVPR_2017_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Skolkovo Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.skoltech.ru",
        "aff_unique_abbr": "Skoltech",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Moscow",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Russian Federation"
    },
    {
        "title": "Non-Local Deep Features for Salient Object Detection",
        "session": "Machine Learning for 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "2999",
        "author_site": "Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin Eichel, Shaozi Li, Pierre-Marc Jodoin",
        "author": "Zhiming Luo; Akshaya Mishra; Andrew Achkar; Justin Eichel; Shaozi Li; Pierre-Marc Jodoin",
        "abstract": "Saliency detection aims to highlight the most relevant objects in an image.  Methods using conventional models struggle whenever salient objects are pictured on top of a cluttered background while deep neural nets suffer from excess complexity and slow evaluation speeds.  In this paper, we propose a simplified convolutional neural network  which combines local and global information through a multi-resolution 4x5 grid structure. Instead of enforcing spacial coherence with a CRF or superpixels as is usually the case, we implemented a loss function inspired by the Mumford-Shah functional which penalizes errors on the boundary.  We trained our model on the MSRA-B dataset, and tested it on six different saliency benchmark datasets. Results show that our method is on par with the state-of-the-art while reducing computation time by a factor of 18 to 100 times, enabling near real-time, high performance saliency detection.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Luo_Non-Local_Deep_Features_CVPR_2017_paper.pdf",
        "aff": "Department of Cognitive Science, Xiamen University, China + Fujian Key Laboratory of Brain-Inspired Computing Technique and Applications, Xiamen University, China + Department of Computer Science, University of Sherbrooke, Canada; Miovision Technologies Inc., Canada; Miovision Technologies Inc., Canada; Miovision Technologies Inc., Canada; Department of Cognitive Science, Xiamen University, China + Fujian Key Laboratory of Brain-Inspired Computing Technique and Applications, Xiamen University, China; Department of Computer Science, University of Sherbrooke, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1727193,
        "gs_citation": 736,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16428186249695992880&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "usherbrooke.ca;miovision.com;miovision.com;miovision.com;xmu.edu.cn;usherbrooke.ca",
        "email": "usherbrooke.ca;miovision.com;miovision.com;miovision.com;xmu.edu.cn;usherbrooke.ca",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Luo_Non-Local_Deep_Features_CVPR_2017_paper.html",
        "aff_unique_index": "0+0+1;2;2;2;0+0;1",
        "aff_unique_norm": "Xiamen University;University of Sherbrooke;Miovision Technologies Inc.",
        "aff_unique_dep": "Department of Cognitive Science;Department of Computer Science;",
        "aff_unique_url": "https://www.xmu.edu.cn;https://www.usherbrooke.ca;https://www.miovision.com",
        "aff_unique_abbr": "XMU;USherb;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+1;1;1;1;0+0;1",
        "aff_country_unique": "China;Canada"
    },
    {
        "title": "Non-Uniform Subset Selection for Active Learning in Structured Data",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3138",
        "author_site": "Sujoy Paul, Jawadul H. Bappy, Amit K. Roy-Chowdhury",
        "author": "Sujoy Paul; Jawadul H. Bappy; Amit K. Roy-Chowdhury",
        "abstract": "Several works have shown that relationships between data points (i.e., context) in structured data can be exploited to obtain better recognition performance. In this paper, we explore a different, but related, problem: how can these inter-relationships be used to efficiently learn and continuously update a recognition model, with minimal human labeling effort. Towards this goal, we propose an active learning framework to select an optimal subset of data points for manual labeling by exploiting the relationships between them. We construct a graph from the unlabeled data to represent the underlying structure, such that each node represents a data point, and edges represent the inter-relationships between them. Thereafter, considering the flow of beliefs in this graph, we choose those samples for labeling which minimize the joint entropy of the nodes of the graph. This results in significant reduction in manual labeling effort without compromising recognition performance. Our method chooses non-uniform number of samples from each batch of streaming data depending on its information content. Also, the submodular property of our objective function makes it computationally efficient to optimize. The proposed framework is demonstrated in various applications, including document analysis, scene-object recognition, and activity recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Paul_Non-Uniform_Subset_Selection_CVPR_2017_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, University of California, Riverside, CA 92521; Department of Electrical and Computer Engineering, University of California, Riverside, CA 92521; Department of Electrical and Computer Engineering, University of California, Riverside, CA 92521",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 739301,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5121206153521495279&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ece.ucr.edu;ece.ucr.edu;ece.ucr.edu",
        "email": "ece.ucr.edu;ece.ucr.edu;ece.ucr.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Paul_Non-Uniform_Subset_Selection_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Riverside",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucr.edu",
        "aff_unique_abbr": "UCR",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Riverside",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Nonnegative Matrix Underapproximation for Robust Multiple Model Fitting",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "755",
        "author_site": "Mariano Tepper, Guillermo Sapiro",
        "author": "Mariano Tepper; Guillermo Sapiro",
        "abstract": "In this work, we introduce a highly efficient algorithm to address the nonnegative matrix underapproximation (NMU) problem, i.e., nonnegative matrix factorization (NMF) with an additional underapproximation constraint. NMU results are interesting as, compared to traditional NMF, they present additional sparsity and part-based behavior, explaining unique data features. To show these features in practice, we first present an application to the analysis of climate data. We then present an NMU-based algorithm to robustly fit multiple parametric models to a dataset. The proposed approach delivers state-of-the-art results for the estimation of multiple fundamental matrices and homographies, outperforming other alternatives in the literature and exemplifying the use of efficient NMU computations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tepper_Nonnegative_Matrix_Underapproximation_CVPR_2017_paper.pdf",
        "aff": "Flatiron Institute, Simons Foundation\u2020; ECE, Duke University",
        "project": "https://goo.gl/xSqKQ4",
        "github": "",
        "supp": "",
        "arxiv": "1611.01408v5",
        "pdf_size": 7891219,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13724864535885741746&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "flatironinstitute.org;duke.edu",
        "email": "flatironinstitute.org;duke.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tepper_Nonnegative_Matrix_Underapproximation_CVPR_2017_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Flatiron Institute;Duke University",
        "aff_unique_dep": ";Electrical and Computer Engineering",
        "aff_unique_url": "https://flatironinstitute.org;https://www.duke.edu",
        "aff_unique_abbr": "Flatiron;Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Not Afraid of the Dark: NIR-VIS Face Recognition via Cross-Spectral Hallucination and Low-Rank Embedding",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "3026",
        "author_site": "Jos\u00c3\u00a9 Lezama, Qiang Qiu, Guillermo Sapiro",
        "author": "Jose Lezama; Qiang Qiu; Guillermo Sapiro",
        "abstract": "Surveillance cameras today often capture NIR (near infrared) images in low-light environments. However, most face datasets accessible for training and verification are only collected in the VIS (visible light) spectrum. It remains a challenging problem to match NIR to VIS face images due to the different light spectrum.  Recently, breakthroughs have been made for VIS face recognition by applying deep learning on a huge amount of labeled VIS face samples. The same deep learning approach cannot be simply applied to NIR face recognition for two main reasons: First, much limited NIR face images are available for training compared to the VIS spectrum. Second, face galleries to be matched are mostly available only in the VIS spectrum.  In this paper, we propose an approach to extend the deep learning breakthrough for VIS face recognition to the NIR spectrum, without retraining the underlying deep models that see only VIS faces. Our approach consists of two core components, cross-spectral hallucination and low-rank embedding, to optimize respectively input and output of a VIS deep model for cross-spectral face recognition. Cross-spectral hallucination produces VIS faces from NIR images through a deep learning approach. Low-rank embedding restores a low-rank structure for faces deep features across both NIR and VIS spectrum.  We observe that it is often equally effective to perform hallucination to input NIR images or low-rank embedding to output deep features for a VIS deep model for cross-spectral recognition. When hallucination and low-rank embedding are deployed together, we observe significant further improvement; we obtain state-of-the-art accuracy on the CASIA NIR-VIS v2.0 benchmark, without the need at all to re-train the recognition system.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lezama_Not_Afraid_of_CVPR_2017_paper.pdf",
        "aff": "IIE, Universidad de la Rep\u00fablica, Uruguay; ECE, Duke University, USA; ECE, Duke University, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.06638v1",
        "pdf_size": 2417231,
        "gs_citation": 169,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7506547143263619277&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff_domain": "; ; ",
        "email": "; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lezama_Not_Afraid_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Universidad de la Rep\u00fablica;Duke University",
        "aff_unique_dep": "Instituto de Ingenier\u00eda El\u00e9ctrica;Electrical and Computer Engineering",
        "aff_unique_url": "https://www.universidad.edu.uy;https://www.duke.edu",
        "aff_unique_abbr": "Udelar;Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Uruguay;United States"
    },
    {
        "title": "Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "1176",
        "author_site": "Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, Xiaoou Tang",
        "author": "Xiaoxiao Li; Ziwei Liu; Ping Luo; Chen Change Loy; Xiaoou Tang",
        "abstract": "We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and 'difficulty-aware' learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an end-to-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Not_All_Pixels_CVPR_2017_paper.pdf",
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong+Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong+Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong+Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong+Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong+Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.01344v1",
        "pdf_size": 2211432,
        "gs_citation": 353,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6618547112372348206&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Not_All_Pixels_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese University of Hong Kong;Shenzhen Institute of Advanced Technology",
        "aff_unique_dep": "Department of Information Engineering;Shenzhen Key Lab of Comp. Vis. & Pat. Rec.",
        "aff_unique_url": "https://www.cuhk.edu.hk;http://www.siat.ac.cn",
        "aff_unique_abbr": "CUHK;SIAT",
        "aff_campus_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_campus_unique": "Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Object Co-Skeletonization With Co-Segmentation",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2725",
        "author_site": "Koteswar Rao Jerripothula, Jianfei Cai, Jiangbo Lu, Junsong Yuan",
        "author": "Koteswar Rao Jerripothula; Jianfei Cai; Jiangbo Lu; Junsong Yuan",
        "abstract": "Recent advances in the joint processing of images have certainly shown its advantages over the individual processing. Different from the existing works geared towards co-segmentation or co-localization, in this paper, we explore a new joint processing topic: co-skeletonization, which is defined as joint skeleton extraction of common objects in a set of semantically similar images. Object skeletonization in real world images is a challenging problem, because there is no prior knowledge of the object's shape if we consider only a single image. This motivates us to resort to the idea of object co-skeletonization hoping that the commonness prior existing across the similar images may help, just as it does for other joint processing problems such as co-segmentation. Noting that skeleton can provide good scribbles for segmentation, and skeletonization, in turn, needs good segmentation, we propose a coupled framework for co-skeletonization and co-segmentation tasks so that they are well informed by each other, and benefit each other synergistically. Since it is a new problem, we also construct a benchmark dataset for the co-skeletonization task. Extensive experiments demonstrate that proposed method achieves very competitive results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jerripothula_Object_Co-Skeletonization_With_CVPR_2017_paper.pdf",
        "aff": "Graphic Era University, India+Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Advanced Digital Sciences Center, Singapore+Shenzhen Cloudream Technology, China; Nanyang Technological University, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2041791,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8766384658620517740&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff_domain": "geu.ac.in;ntu.edu.sg;gmail.com;ntu.edu.sg",
        "email": "geu.ac.in;ntu.edu.sg;gmail.com;ntu.edu.sg",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jerripothula_Object_Co-Skeletonization_With_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;2+3;1",
        "aff_unique_norm": "Graphic Era University;Nanyang Technological University;Advanced Digital Sciences Center;Shenzhen Cloudream Technology",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.graphicera.com;https://www.ntu.edu.sg;;",
        "aff_unique_abbr": "GEU;NTU;;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1+2;1",
        "aff_country_unique": "India;Singapore;China"
    },
    {
        "title": "Object Detection in Videos With Tubelet Proposal Networks",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "225",
        "author_site": "Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie Yan, Xihui Liu, Xiaogang Wang",
        "author": "Kai Kang; Hongsheng Li; Tong Xiao; Wanli Ouyang; Junjie Yan; Xihui Liu; Xiaogang Wang",
        "abstract": "Object detection in videos has drawn increasing attention recently with the introduction of the large-scale ImageNet VID dataset. Different from object detection in static images, temporal information in videos is vital for object detection. To fully utilize temporal information, state-of-the-art methods are based on spatiotemporal tubelets, which are essentially sequences of associated bounding boxes across time. However, the existing methods have major limitations in generating tubelets in terms of quality and efficiency. Motion-based methods are able to obtain dense tubelets efficiently, but the lengths are generally only several frames, which is not optimal for incorporating long-term temporal information. Appearance-based methods, usually involving generic object tracking, could generate long tubelets, but are usually computationally expensive. In this work, we propose a framework for object detection in videos, which consists of a novel tubelet proposal network to efficiently generate spatiotemporal proposals, and a Long Short-term Memory (LSTM) network that incorporates temporal information from tubelet proposals for achieving high object detection accuracy in videos. Experiments on the large-scale ImageNet VID dataset demonstrate the effectiveness of the proposed framework for object detection in videos.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kang_Object_Detection_in_CVPR_2017_paper.pdf",
        "aff": "Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China+The Chinese University of Hong Kong; The Chinese University of Hong Kong; Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China+The Chinese University of Hong Kong; The Chinese University of Hong Kong+The University of Sydney; SenseTime Group Limited; Tsinghua University; The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.06355v2",
        "pdf_size": 4991073,
        "gs_citation": 255,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13169945530592615629&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;sensetime.com;mails.tsinghua.edu.cn;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;sensetime.com;mails.tsinghua.edu.cn;ee.cuhk.edu.hk",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kang_Object_Detection_in_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;0+1;1+2;3;4;1",
        "aff_unique_norm": "Shenzhen Institute of Advanced Technology;Chinese University of Hong Kong;University of Sydney;SenseTime Group Limited;Tsinghua University",
        "aff_unique_dep": "Shenzhen Key Lab of Comp. Vis. & Pat. Rec.;;;;",
        "aff_unique_url": "http://www.siat.ac.cn;https://www.cuhk.edu.hk;https://www.sydney.edu.au;https://www.sensetime.com;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "SIAT;CUHK;USYD;SenseTime;THU",
        "aff_campus_unique_index": "0+1;1;0+1;1;1",
        "aff_campus_unique": "Shenzhen;Hong Kong SAR;",
        "aff_country_unique_index": "0+0;0;0+0;0+1;0;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Object Region Mining With Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Oral",
        "track": "main",
        "pid": "581",
        "author_site": "Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan",
        "author": "Yunchao Wei; Jiashi Feng; Xiaodan Liang; Ming-Ming Cheng; Yao Zhao; Shuicheng Yan",
        "abstract": "We investigate a principle way to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems. Classification networks are only responsive to small and sparse discriminative regions from the object of interest, which deviates from the requirement of the segmentation task that needs to localize dense, interior and integral regions for pixel-wise inference. To mitigate this gap, we propose a new adversarial erasing approach for localizing and expanding object regions progressively. Starting with a single small object region, our proposed approach drives the classification network to sequentially discover new and complement object regions by erasing the current mined regions in an adversarial manner. These localized regions eventually constitute a dense and complete object region for learning semantic segmentation. To further enhance the quality of the discovered regions by adversarial erasing, an online prohibitive segmentation learning approach is developed to collaborate with adversarial erasing by providing auxiliary segmentation supervision modulated by the more reliable classification scores. Despite its apparent simplicity, the proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union (mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wei_Object_Region_Mining_CVPR_2017_paper.pdf",
        "aff": "National University of Singapore; National University of Singapore; CMU; Naikai University; Beijing Jiaotong University; 360 AI Institute",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.08448v3",
        "pdf_size": 755904,
        "gs_citation": 1023,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=90754286231777746&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "nus.edu.sg;nus.edu.sg;cs.cmu.edu;nankai.edu.cn;bjtu.edu.cn;360.cn",
        "email": "nus.edu.sg;nus.edu.sg;cs.cmu.edu;nankai.edu.cn;bjtu.edu.cn;360.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wei_Object_Region_Mining_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;2;3;4",
        "aff_unique_norm": "National University of Singapore;Carnegie Mellon University;Naikai University;Beijing Jiao Tong University;360 AI Institute",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.cmu.edu;http://www.nkau.edu.cn/;http://www.njtu.edu.cn/en;",
        "aff_unique_abbr": "NUS;CMU;;BJTU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2;2;2",
        "aff_country_unique": "Singapore;United States;China"
    },
    {
        "title": "Object-Aware Dense Semantic Correspondence",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1010",
        "author_site": "Fan Yang, Xin Li, Hong Cheng, Jianping Li, Leiting Chen",
        "author": "Fan Yang; Xin Li; Hong Cheng; Jianping Li; Leiting Chen",
        "abstract": "This work aims to build pixel-to-pixel correspondences between images from the same visual class but with different geometries and visual similarities. This task is particularly challenging because (i) their visual content is similar only on the high-level structure, and (ii) background clutters keep bringing in noises.  To address these problems, this paper proposes an object-aware method to estimate per-pixel correspondences from semantic to low-level by learning a classifier for each selected discriminative grid cell and guiding the localization of every pixel under the semantic constraint. Specifically, an Object-aware Hierarchical Graph (OHG) model is constructed to regulate matching consistency from one coarse grid cell containing whole object(s), to fine grid cells covering smaller semantic elements, and finally to every pixel. A guidance layer is introduced as the semantic constraint on local structure matching. In addition, we propose to learn the important high-level structure for each grid cell in an \"objectness-driven\" way as an alternative to handcrafted descriptors in defining a better visual similarity. The proposed method has been extensively evaluated on various challenging benchmarks and real-world images. The results show that our method significantly outperforms the state-of-the-arts in terms of semantic flow accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_Object-Aware_Dense_Semantic_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science & Engineering, UESTC; School of Computer Science & Engineering, UESTC; Center for Robotics, School of Automation Engineering, UESTC; School of Computer Science & Engineering, UESTC; School of Computer Science & Engineering, UESTC",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2445148,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16647265783067215703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": "hotmail.com;hotmail.com;uestc.edu.cn; ; ",
        "email": "hotmail.com;hotmail.com;uestc.edu.cn; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Object-Aware_Dense_Semantic_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Electronic Science and Technology of China",
        "aff_unique_dep": "School of Computer Science & Engineering",
        "aff_unique_url": "http://www.uestc.edu.cn",
        "aff_unique_abbr": "UESTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "OctNet: Learning Deep 3D Representations at High Resolutions",
        "session": "Machine Learning for 3D Vision",
        "status": "Oral",
        "track": "main",
        "pid": "1319",
        "author_site": "Gernot Riegler, Ali Osman Ulusoy, Andreas Geiger",
        "author": "Gernot Riegler; Ali Osman Ulusoy; Andreas Geiger",
        "abstract": "We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Riegler_OctNet_Learning_Deep_CVPR_2017_paper.pdf",
        "aff": "Institute for Computer Graphics and Vision, Graz University of Technology; Autonomous Vision Group, MPI for Intelligent Systems T\u00fcbingen; Autonomous Vision Group, MPI for Intelligent Systems T\u00fcbingen + Computer Vision and Geometry Group, ETH Z\u00fcrich",
        "project": "https://3dwarehouse.sketchup.com",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Riegler_OctNet_Learning_Deep_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.05009",
        "pdf_size": 9262066,
        "gs_citation": 1982,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5787747825600741306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "icg.tugraz.at;tue.mpg.de;tue.mpg.de",
        "email": "icg.tugraz.at;tue.mpg.de;tue.mpg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Riegler_OctNet_Learning_Deep_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1+2",
        "aff_unique_norm": "Graz University of Technology;Max Planck Institute for Intelligent Systems;ETH Zurich",
        "aff_unique_dep": "Institute for Computer Graphics and Vision;Autonomous Vision Group;Computer Vision and Geometry Group",
        "aff_unique_url": "https://www.tugraz.at;https://www.mpituebingen.mpg.de;https://www.ethz.ch",
        "aff_unique_abbr": "TU Graz;MPI-IS;ETHZ",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Graz;T\u00fcbingen;",
        "aff_country_unique_index": "0;1;1+2",
        "aff_country_unique": "Austria;Germany;Switzerland"
    },
    {
        "title": "On Compressing Deep Models by Low Rank and Sparse Decomposition",
        "session": "Machine Learning 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "3653",
        "author_site": "Xiyu Yu, Tongliang Liu, Xinchao Wang, Dacheng Tao",
        "author": "Xiyu Yu; Tongliang Liu; Xinchao Wang; Dacheng Tao",
        "abstract": "Deep compression refers to removing the redundancy of parameters and feature maps for deep learning models. Low-rank approximation and pruning for sparse structures play a vital role in many compression works. However, weight filters tend to be both low-rank and sparse. Neglecting either part of these structure information in previous methods results in iteratively retraining, compromising accuracy, and low compression rates. Here we propose a unified framework integrating the low-rank and sparse decomposition of weight matrices with the feature map reconstructions. Our model includes methods like pruning connections as special cases, and is optimized by a fast SVD-free algorithm. It has been theoretically proven that, with a small sample, due to its generalizability, our model can well reconstruct the feature maps on both training and test data, which results in less compromising accuracy prior to the subsequent retraining. With such a \"warm start\" to retrain, the compression method always possesses several merits: (a) higher compression rates, (b) little loss of accuracy, and (c) fewer rounds to compress deep models. The experimental results on several popular models such as AlexNet, VGG-16, and GoogLeNet show that our model can significantly reduce the parameters for both convolutional and fully-connected layers. As a result, our model reduces the size of VGG-16 by 15x, better than other recent compression methods that use a single strategy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_On_Compressing_Deep_CVPR_2017_paper.pdf",
        "aff": "UBTech Sydney AI Institute and SIT, FEIT, The University of Sydney; UBTech Sydney AI Institute and SIT, FEIT, The University of Sydney; IFP, Beckman Institute, The University of Illinois Urbana-Champaign (UIUC); UBTech Sydney AI Institute and SIT, FEIT, The University of Sydney",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 695346,
        "gs_citation": 562,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13746460964689633682&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;gmail.com;illinois.edu;sydney.edu.au",
        "email": "gmail.com;gmail.com;illinois.edu;sydney.edu.au",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_On_Compressing_Deep_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Sydney;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Sydney AI Institute and SIT, FEIT;Beckman Institute",
        "aff_unique_url": "https://www.sydney.edu.au;https://wwwillinois.edu",
        "aff_unique_abbr": "USYD;UIUC",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Sydney;Urbana-Champaign",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Australia;United States"
    },
    {
        "title": "On Human Motion Prediction Using Recurrent Neural Networks",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1057",
        "author_site": "Julieta Martinez, Michael J. Black, Javier Romero",
        "author": "Julieta Martinez; Michael J. Black; Javier Romero",
        "abstract": "Human motion modelling is a classical problem at the intersection of graphics and computer vision, with applications spanning human-computer interaction, motion synthesis, and motion prediction for virtual and augmented reality.        Following the success of deep learning methods in several computer vision tasks, recent work has focused on using deep recurrent neural networks (RNNs) to model human motion, with the goal of learning time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion synthesis. We examine recent work, with a focus on the evaluation methodologies commonly used in the literature, and show that, surprisingly, state of the art performance can be achieved by a simple baseline that does not attempt to model motion at all. We investigate this result, and analyze recent RNN methods by looking at the architectures, loss functions, and training procedures used in state-of-the-art approaches. We propose three changes to the standard RNN models typically used for human motion, which results in a simple and scalable RNN architecture that obtains state-of-the-art performance on human motion prediction.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Martinez_On_Human_Motion_CVPR_2017_paper.pdf",
        "aff": "University of British Columbia, Vancouver, Canada; MPI for Intelligent Systems, T\u00fcbingen, Germany; Body Labs Inc., New York, NY",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.02445",
        "pdf_size": 2917964,
        "gs_citation": 1252,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1667028470840876283&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.ubc.ca;tuebingen.mpg.de;bodylabs.com",
        "email": "cs.ubc.ca;tuebingen.mpg.de;bodylabs.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Martinez_On_Human_Motion_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of British Columbia;Max Planck Institute for Intelligent Systems;Body Labs Inc.",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ubc.ca;https://www.mpituebingen.mpg.de;https://www.bodylabs.com",
        "aff_unique_abbr": "UBC;MPI-IS;",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Vancouver;T\u00fcbingen;",
        "aff_country_unique_index": "0;1;2",
        "aff_country_unique": "Canada;Germany;United States"
    },
    {
        "title": "On the Effectiveness of Visible Watermarks",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "781",
        "author_site": "Tali Dekel, Michael Rubinstein, Ce Liu, William T. Freeman",
        "author": "Tali Dekel; Michael Rubinstein; Ce Liu; William T. Freeman",
        "abstract": "Visible watermarking is a widely-used technique for marking and protecting copyrights of many millions of images on the web, yet it suffers from an inherent security flaw---watermarks are typically added in a consistent manner to many images. We show that this consistency allows to automatically estimate the watermark and recover the original images with high accuracy. Specifically, we present a generalized multi-image matting algorithm that  takes a watermarked image collection as input and automatically estimates the \"foreground\" (watermark), its alpha matte, and  the \"background\" (original) images.  Since such an attack relies on the consistency of watermarks across image collection, we explore and evaluate how it is affected by various types of inconsistencies in the watermark embedding that could potentially be used to make watermarking more secured.  We demonstrate the algorithm on stock imagery available on the web, and provide extensive quantitative analysis on synthetic watermarked data. A key takeaway message of this paper is that visible watermarks should be designed to not only be robust against removal from a single image, but to be more resistant to mass-scale removal from image collections as well.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dekel_On_the_Effectiveness_CVPR_2017_paper.pdf",
        "aff": "Google Research; Google Research; Google Research; Google Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3696152,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9521120946478938037&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 11,
        "aff_domain": "google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dekel_On_the_Effectiveness_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On the Global Geometry of Sphere-Constrained Sparse Blind Deconvolution",
        "session": "Machine Learning 3",
        "status": "Oral",
        "track": "main",
        "pid": "2020",
        "author_site": "Yuqian Zhang, Yenson Lau, Han-wen Kuo, Sky Cheung, Abhay Pasupathy, John Wright",
        "author": "Yuqian Zhang; Yenson Lau; Han-wen Kuo; Sky Cheung; Abhay Pasupathy; John Wright",
        "abstract": "Blind deconvolution is the problem of recovering a convolutional kernel and an activation signal from their convolution. This problem is ill-posed without further constraints or priors. This paper studies the situation where the nonzero entries in the activation signal are sparsely and randomly populated.. We normalize the convolution kernel to have unit Frobenius norm and cast the sparse blind deconvolution problem as a nonconvex optimization problem over the sphere. With this spherical constraint, every spurious local minimum turns out to be close to some signed shift truncation of the ground truth, under certain hypotheses. This benign property motivates an effective two stage algorithm that recovers the ground truth from the partial information offered by a suboptimal local minimum. This geometry-inspired algorithm recovers the ground truth for certain microscopy problems, also exhibits promising performance in the more challenging image deblurring problem. Our insights into the global geometry and the two stage algorithm extend to the convolutional dictionary learning problem, where a superposition of multiple convolution signals is observed.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_On_the_Global_CVPR_2017_paper.pdf",
        "aff": "Department of Electrical Engineering, Columbia University, New York, USA; Department of Electrical Engineering, Columbia University, New York, USA; Department of Electrical Engineering, Columbia University, New York, USA; Department of Physics, Columbia University, New York, USA; Department of Physics, Columbia University, New York, USA; Department of Electrical Engineering, Columbia University, New York, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_On_the_Global_2017_CVPR_supplemental.pdf",
        "arxiv": "1901.01913v1",
        "pdf_size": 22604526,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10786073769223427833&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "columbia.edu;columbia.edu;columbia.edu;columbia.edu;columbia.edu;columbia.edu",
        "email": "columbia.edu;columbia.edu;columbia.edu;columbia.edu;columbia.edu;columbia.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_On_the_Global_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "New York",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "On the Two-View Geometry of Unsynchronized Cameras",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2003",
        "author_site": "Cenek Albl, Zuzana Kukelova, Andrew Fitzgibbon, Jan Heller, Matej Smid, Tomas Pajdla",
        "author": "Cenek Albl; Zuzana Kukelova; Andrew Fitzgibbon; Jan Heller; Matej Smid; Tomas Pajdla",
        "abstract": "We present new methods of simultaneously estimating camera geometry and time shift from video sequences from multiple unsynchronized cameras.  Algorithms for simultaneous computation of a fundamental matrix or a homography with unknown time shift between images are developed. Our methods use minimal correspondence sets  (eight for fundamental matrix and four and a half for homography) and therefore are suitable for robust estimation using RANSAC. Furthermore, we present an iterative algorithm that extends the applicability on sequences which are significantly unsynchronized, finding the correct time shift up to several seconds. We evaluated the methods on synthetic and wide range of real world datasets and the results show a broad applicability to the problem of camera synchronization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Albl_On_the_Two-View_CVPR_2017_paper.pdf",
        "aff": "Czech Technical University in Prague; Czech Technical University in Prague; HoloLens, Microsoft; Magik Eye Inc.; Czech Technical University in Prague; Czech Technical University in Prague",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Albl_On_the_Two-View_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.06843v1",
        "pdf_size": 3669470,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10495745978230176222&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cmp.felk.cvut.cz;cmp.felk.cvut.cz;microsoft.com;magik-eye.com;cmp.felk.cvut.cz;cvut.cz",
        "email": "cmp.felk.cvut.cz;cmp.felk.cvut.cz;microsoft.com;magik-eye.com;cmp.felk.cvut.cz;cvut.cz",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Albl_On_the_Two-View_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;2;0;0",
        "aff_unique_norm": "Czech Technical University;Microsoft;Magik Eye Inc.",
        "aff_unique_dep": ";HoloLens;",
        "aff_unique_url": "https://www.ctu.cz;https://www.microsoft.com;",
        "aff_unique_abbr": "CTU;Microsoft;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Prague;",
        "aff_country_unique_index": "0;0;1;1;0;0",
        "aff_country_unique": "Czech Republic;United States"
    },
    {
        "title": "On-The-Fly Adaptation of Regression Forests for Online Camera Relocalisation",
        "session": "3D Vision 1",
        "status": "Oral",
        "track": "main",
        "pid": "1821",
        "author_site": "Tommaso Cavallari, Stuart Golodetz, Nicholas A. Lord, Julien Valentin, Luigi Di Stefano, Philip H. S. Torr",
        "author": "Tommaso Cavallari; Stuart Golodetz; Nicholas A. Lord; Julien Valentin; Luigi Di Stefano; Philip H. S. Torr",
        "abstract": "Camera relocalisation is an important problem in computer vision, with applications in simultaneous localisation and mapping, virtual/augmented reality and navigation. Common techniques either match the current image against keyframes with known poses coming from a tracker, or establish 2D-to-3D correspondences between keypoints in the current image and points in the scene in order to estimate the camera pose. Recently, regression forests have become a popular alternative to establish such correspondences. They achieve accurate results, but must be trained offline on the target scene, preventing relocalisation in new environments. In this paper, we show how to circumvent this limitation by adapting a pre-trained forest to a new scene on the fly. Our adapted forests achieve relocalisation performance that is on par with that of offline forests, and our approach runs in under 150ms, making it desirable for real-time systems that require online relocalisation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cavallari_On-The-Fly_Adaptation_of_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science and Engineering, University of Bologna; Department of Engineering Science, University of Oxford; Department of Engineering Science, University of Oxford; perceptiveio, Inc.; Department of Computer Science and Engineering, University of Bologna; Department of Engineering Science, University of Oxford",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Cavallari_On-The-Fly_Adaptation_of_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1495979,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4800529197028427786&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "unibo.it;robots.ox.ac.uk;robots.ox.ac.uk;perceptiveio.com;unibo.it;robots.ox.ac.uk",
        "email": "unibo.it;robots.ox.ac.uk;robots.ox.ac.uk;perceptiveio.com;unibo.it;robots.ox.ac.uk",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cavallari_On-The-Fly_Adaptation_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;2;0;1",
        "aff_unique_norm": "University of Bologna;University of Oxford;perceptiveio, Inc.",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Engineering Science;",
        "aff_unique_url": "https://www.unibo.it;https://www.ox.ac.uk;",
        "aff_unique_abbr": "UNIBO;Oxford;",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Oxford",
        "aff_country_unique_index": "0;1;1;2;0;1",
        "aff_country_unique": "Italy;United Kingdom;United States"
    },
    {
        "title": "One-Shot Hyperspectral Imaging Using Faced Reflectors",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "1627",
        "author_site": "Tsuyoshi Takatani, Takahito Aoto, Yasuhiro Mukaigawa",
        "author": "Tsuyoshi Takatani; Takahito Aoto; Yasuhiro Mukaigawa",
        "abstract": "Hyperspectral imaging is a useful technique for various computer vision tasks such as material recognition. However, such technique usually requires an expensive and professional setup and is time-consuming because a conventional hyperspectral image consists of a large number of observations. In this paper, we propose a novel technique of one-shot hyperspectral imaging using faced reflectors on which color filters are attached. The key idea is based on the principle that each of multiple reflections on the filters has a different spectrum, which allows us to observe multiple intensities through different spectra. Our technique can be implemented either by a coupled mirror or a kaleidoscope geometry. Experimental results show that our technique is capable of accurately capturing a hyperspectral image by using a coupled mirror setup which is readily available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Takatani_One-Shot_Hyperspectral_Imaging_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1521144511322328101&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Takatani_One-Shot_Hyperspectral_Imaging_CVPR_2017_paper.html"
    },
    {
        "title": "One-Shot Metric Learning for Person Re-Identification",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1100",
        "author_site": "Slawomir B\u00c4\u0085k, Peter Carr",
        "author": "Slawomir Bak; Peter Carr",
        "abstract": "Re-identification of people in surveillance footage must cope with drastic variations in color, background, viewing angle and a person's pose.  Supervised techniques are often the most effective, but require extensive annotation which is infeasible for large camera networks.  Unlike previous supervised learning approaches that require hundreds of annotated subjects,  we learn a metric using a novel one-shot learning approach.  We first learn a deep texture representation from intensity images with Convolutional Neural Networks (CNNs). When training a CNN using only intensity images, the learned embedding is color-invariant and shows high performance even on unseen datasets without fine-tuning. To account for differences in camera color distributions, we learn a color metric using a single pair of ColorChecker images. The proposed one-shot learning achieves performance that is competitive with supervised methods, but uses only a single example rather than the hundreds required for the fully supervised case.  Compared with semi-supervised and unsupervised state-of-the-art methods, our approach yields significantly higher accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bak_One-Shot_Metric_Learning_CVPR_2017_paper.pdf",
        "aff": "Disney Research; Disney Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1844241,
        "gs_citation": 160,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5672681909017764318&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "disneyresearch.com;disneyresearch.com",
        "email": "disneyresearch.com;disneyresearch.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bak_One-Shot_Metric_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Disney Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://research.disney.com",
        "aff_unique_abbr": "Disney Research",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "One-Shot Video Object Segmentation",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "81",
        "author_site": "Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taix\u00c3\u00a9, Daniel Cremers, Luc Van Gool",
        "author": "Sergi Caelles; Kevis-Kokitsi Maninis; Jordi Pont-Tuset; Laura Leal-Taixe; Daniel Cremers; Luc Van Gool",
        "abstract": "This paper tackles the task of semi-supervised video object segmentation, i.e., the separation of an object from the background in a video, given the mask of the first frame. We present One-Shot Video Object Segmentation (OSVOS), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one-shot). Although all frames are processed independently, the results are temporally coherent and stable. We perform experiments on two annotated video segmentation databases, which show that OSVOS is fast and improves the state of the art by a significant margin (79.8% vs 68.0%).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Caelles_One-Shot_Video_Object_CVPR_2017_paper.pdf",
        "aff": "ETH Z\u00fcrich; ETH Z\u00fcrich; ETH Z\u00fcrich; TU M\u00fcnchen; TU M\u00fcnchen; ETH Z\u00fcrich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05198",
        "pdf_size": 2754510,
        "gs_citation": 1167,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1190653439359688733&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Caelles_One-Shot_Video_Object_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;1;0",
        "aff_unique_norm": "ETH Zurich;Technical University of Munich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.tum.de",
        "aff_unique_abbr": "ETHZ;TUM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;1;0",
        "aff_country_unique": "Switzerland;Germany"
    },
    {
        "title": "One-To-Many Network for Visually Pleasing Compression Artifacts Reduction",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1125",
        "author_site": "Jun Guo, Hongyang Chao",
        "author": "Jun Guo; Hongyang Chao",
        "abstract": "We consider the compression artifacts reduction problem, where a compressed image is transformed into an artifact-free image. Recent approaches for this problem typically train a one-to-one mapping using a per-pixel L_2 loss between the outputs and the ground-truths. We point out that these approaches used to produce overly smooth results, and PSNR doesn't reflect their real performance. In this paper, we propose a one-to-many network, which measures output quality using a perceptual loss, a naturalness loss, and a JPEG loss. We also avoid grid-like artifacts during deconvolution using a \"shift-and-average\" strategy. Extensive experimental results demonstrate the dramatic visual improvement of our approach over the state of the arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Guo_One-To-Many_Network_for_CVPR_2017_paper.pdf",
        "aff": "School of Data and Computer Science, Sun Yat-sen University + SYSU-CMU Shunde International Joint Research Institute; School of Data and Computer Science, Sun Yat-sen University + SYSU-CMU Shunde International Joint Research Institute",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.04994v2",
        "pdf_size": 1654468,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1590338739817033663&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "outlook.com;mail.sysu.edu.cn",
        "email": "outlook.com;mail.sysu.edu.cn",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Guo_One-To-Many_Network_for_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Sun Yat-sen University;SYSU-CMU Shunde International Joint Research Institute",
        "aff_unique_dep": "School of Data and Computer Science;",
        "aff_unique_url": "http://www.sysu.edu.cn/;",
        "aff_unique_abbr": "SYSU;",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Shunde",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Online Asymmetric Similarity Learning for Cross-Modal Retrieval",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1727",
        "author_site": "Yiling Wu, Shuhui Wang, Qingming Huang",
        "author": "Yiling Wu; Shuhui Wang; Qingming Huang",
        "abstract": "Cross-modal retrieval has attracted intensive attention in recent years. Measuring the semantic similarity between heterogeneous data objects is an essential yet challenging problem in cross-modal retrieval. In this paper, we propose an online learning method to learn the similarity function between heterogeneous modalities by preserving the relative similarity in the training data, which is modeled as a set of bi-directional hinge loss constraints on the cross-modal training triplets. The overall online similarity function learning problem is optimized by the margin based Passive-Aggressive algorithm. We further extend the approach to learn similarity function in reproducing kernel Hilbert spaces by kernelizing the approach and combining multiple kernels derived from different layers of the CNN features using the Hedging algorithm. Theoretical mistake bounds are given for our methods. Experiments conducted on real world datasets well demonstrate the effectiveness of our methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wu_Online_Asymmetric_Similarity_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3554597,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2440492855566872221&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wu_Online_Asymmetric_Similarity_CVPR_2017_paper.html"
    },
    {
        "title": "Online Graph Completion: Multivariate Signal Recovery in Computer Vision",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1315",
        "author_site": "Won Hwa Kim, Mona Jalal, Seongjae Hwang, Sterling C. Johnson, Vikas Singh",
        "author": "Won Hwa Kim; Mona Jalal; Seongjae Hwang; Sterling C. Johnson; Vikas Singh",
        "abstract": "The adoption of \"human-in-the-loop\" paradigms in computer vision and machine learning is leading to various applications where the actual data acquisition (e.g., human supervision) and the underlying inference algorithms are closely interwined. While classical work in active learning provides effective solutions when the learning module involves classification and regression tasks, many practical issues such as partially observed measurements, financial constraints and even additional distributional or structural aspects of the data typically fall outside the scope of this treatment. For instance, with sequential acquisition of partial measurements of data that manifest as a matrix (or tensor), novel strategies for completion (or collaborative filtering) of the remaining entries have only been studied recently. Motivated by vision problems where we seek to annotate a large dataset of images via a crowdsourced platform or alternatively, complement results from a state-of-the-art object detector using human feedback, we study the \"completion\" problem defined on graphs, where requests for additional measurements must be made sequentially. We design the optimization model in the Fourier domain of the graph describing how ideas based on adaptive submodularity provide algorithms that work well in practice. On a large set of images collected from Imgur, we see promising results on images that are otherwise difficult to categorize. We also show applications to an experimental design problem in neuroimaging.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kim_Online_Graph_Completion_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "http://pages.cs.wisc.edu/~wonhwa",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2862012,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17634950096889163293&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kim_Online_Graph_Completion_CVPR_2017_paper.html"
    },
    {
        "title": "Online Summarization via Submodular and Convex Optimization",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "661",
        "author_site": "Ehsan Elhamifar, M. Clara De Paolis Kaluza",
        "author": "Ehsan Elhamifar; M. Clara De Paolis Kaluza",
        "abstract": "We consider the problem of subset selection in the online setting, where data arrive incrementally. Instead of storing and running subset selection on the entire dataset, we propose an incremental subset selection framework that, at each time instant, uses the previously selected set of representatives and the new batch of data in order to update the set of representatives. We cast the problem as an integer binary optimization minimizing the encoding cost of the data via representatives regularized by the number of selected items. As the proposed optimization is, in general, NP-hard and non-convex, we study a greedy approach based on unconstrained submodular optimization and also propose an efficient convex relaxation. We show that, under appropriate conditions, the solution of our proposed convex algorithm achieves the global optimal solution of the non-convex problem. Our results also address the conventional problem of subset selection in the offline setting, as a special case. By extensive experiments on the problem of video summarization, we demonstrate that our proposed online subset selection algorithms perform well on real data, capturing diverse representative events in videos, while they obtain objective function values close to the offline setting.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Elhamifar_Online_Summarization_via_CVPR_2017_paper.pdf",
        "aff": "College of Computer and Information Science, Northeastern University; College of Computer and Information Science, Northeastern University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4674871,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3765658778383337119&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ccs.neu.edu;ccs.neu.edu",
        "email": "ccs.neu.edu;ccs.neu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Elhamifar_Online_Summarization_via_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "College of Computer and Information Science",
        "aff_unique_url": "https://www.northeastern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Online Video Object Segmentation via Convolutional Trident Network",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "2517",
        "author_site": "Won-Dong Jang, Chang-Su Kim",
        "author": "Won-Dong Jang; Chang-Su Kim",
        "abstract": "A semi-supervised online video object segmentation algorithm, which accepts user annotations about a target object at the first frame, is proposed in this work. We propagate the segmentation labels at the previous frame to the current frame using optical flow vectors. However, the propagation is error-prone. Therefore, we develop the convolutional trident network (CTN), which has three decoding branches: separative, definite foreground, and definite background decoders. Then, we perform  Markov random field optimization based on outputs of the three decoders. We sequentially carry out these processes from the second to the last frames to extract a segment track of the target object. Experimental results demonstrate that the proposed algorithm significantly outperforms the state-of-the-art conventional algorithms on the DAVIS benchmark dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jang_Online_Video_Object_CVPR_2017_paper.pdf",
        "aff": "Korea University; Korea University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2015893,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11835522730454977059&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mcl.korea.ac.kr;korea.ac.kr",
        "email": "mcl.korea.ac.kr;korea.ac.kr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jang_Online_Video_Object_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Korea University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.korea.ac.kr",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Optical Flow Estimation Using a Spatial Pyramid Network",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "1696",
        "author_site": "Anurag Ranjan, Michael J. Black",
        "author": "Anurag Ranjan; Michael J. Black",
        "abstract": "We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning. This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow. Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions; these are dealt with by the pyramid. This has several advantages. First, our Spatial Pyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms of model parameters. This makes it more efficient and appropriate for embedded applications. Second, since the flow at each pyramid level is small ( < 1 pixel), a convolutional approach applied to pairs of warped images is appropriate. Third, unlike FlowNet, the learned convolution filters appear similar to classical spatio-temporal filters, giving insight into the method and how to improve it. Our results are more accurate than FlowNet on most standard benchmarks, suggesting a new direction of combining classical flow methods with deep learning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ranjan_Optical_Flow_Estimation_CVPR_2017_paper.pdf",
        "aff": "Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.00850",
        "pdf_size": 2544785,
        "gs_citation": 1582,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1776399510420952171&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ranjan_Optical_Flow_Estimation_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "T\u00fcbingen",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Optical Flow Requires Multiple Strategies (but Only One Network)",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "2038",
        "author_site": "Tal Schuster, Lior Wolf, David Gadot",
        "author": "Tal Schuster; Lior Wolf; David Gadot",
        "abstract": "We show that the matching problem that underlies optical flow requires multiple strategies, depending on the amount of image motion  and other factors. We then study the implications of this observation on training a deep neural network for representing image patches in the context of descriptor based optical flow. We propose a metric learning method, which selects suitable negative samples based on the nature of the true match. This type of training produces a network that displays multiple strategies depending on the input and leads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Schuster_Optical_Flow_Requires_CVPR_2017_paper.pdf",
        "aff": "The Blavatnik School of Computer Science, Tel Aviv University, Israel; The Blavatnik School of Computer Science, Tel Aviv University, Israel + Facebook AI Research; The Blavatnik School of Computer Science, Tel Aviv University, Israel",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05607v3",
        "pdf_size": 765118,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11367940704293971807&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "gmail.com;cs.tau.ac.il;gmail.com",
        "email": "gmail.com;cs.tau.ac.il;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Schuster_Optical_Flow_Requires_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Tel Aviv University;Meta",
        "aff_unique_dep": "Blavatnik School of Computer Science;Facebook AI Research",
        "aff_unique_url": "https://www.tau.ac.il;https://research.facebook.com",
        "aff_unique_abbr": "TAU;FAIR",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Tel Aviv;",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "title": "Optical Flow in Mostly Rigid Scenes",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "1941",
        "author_site": "Jonas Wulff, Laura Sevilla-Lara, Michael J. Black",
        "author": "Jonas Wulff; Laura Sevilla-Lara; Michael J. Black",
        "abstract": "The optical flow of natural scenes is a combination of the motion of the observer and the independent motion of objects. Existing algorithms typically focus on either recovering motion and structure under the assumption of a purely static world or optical flow for general unconstrained scenes. We combine these approaches in an optical flow algorithm that estimates an explicit segmentation of moving objects from appearance and physical constraints. In static regions we take advantage of strong constraints to jointly estimate the camera motion and the 3D structure of the scene over multiple frames. This allows us to also regularize the structure instead of the motion. Our formulation uses a Plane+Parallax framework, which works even under small baselines, and reduces the motion estimation to a one-dimensional search problem, resulting in more accurate estimation. In moving regions the flow is treated as unconstrained, and computed with an existing optical flow method. The resulting Mostly-Rigid Flow (MR-Flow) method achieves state-of-the-art results on both the MPI-Sintel and KITTI-2015 benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wulff_Optical_Flow_in_CVPR_2017_paper.pdf",
        "aff": "Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.01352",
        "pdf_size": 4322950,
        "gs_citation": 148,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10436260802448551219&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "tue.mpg.de;tue.mpg.de;tue.mpg.de",
        "email": "tue.mpg.de;tue.mpg.de;tue.mpg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wulff_Optical_Flow_in_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "T\u00fcbingen",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Order-Preserving Wasserstein Distance for Sequence Matching",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "365",
        "author_site": "Bing Su, Gang Hua",
        "author": "Bing Su; Gang Hua",
        "abstract": "We present a new distance measure between sequences that can tackle local temporal distortion and periodic sequences with arbitrary starting points. Through viewing the instances of sequences as empirical samples of an unknown distribution, we cast the calculation of the distance between sequences as the optimal transport problem. To preserve the inherent temporal relationships of the instances in sequences, we smooth the optimal transport problem with two novel temporal regularization terms. The inverse difference moment regularization enforces transport with local homogeneous structures, and the KL-divergence with a prior distribution regularization prevents transport between instances with far temporal positions. We show that this problem can be efficiently optimized through the matrix scaling algorithm. Extensive experiments on different datasets with different classifiers show that the proposed distance outperforms the traditional DTW variants and the smoothed optimal transport distance without temporal regularization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Su_Order-Preserving_Wasserstein_Distance_CVPR_2017_paper.pdf",
        "aff": "Science & Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing 100190, China; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1350231,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14281075474524551491&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;gmail.com",
        "email": "gmail.com;gmail.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Su_Order-Preserving_Wasserstein_Distance_CVPR_2017_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Chinese Academy of Sciences;Microsoft",
        "aff_unique_dep": "Institute of Software;Microsoft Research",
        "aff_unique_url": "http://www.cas.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "CAS;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Oriented Response Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "160",
        "author_site": "Yanzhao Zhou, Qixiang Ye, Qiang Qiu, Jianbin Jiao",
        "author": "Yanzhao Zhou; Qixiang Ye; Qiang Qiu; Jianbin Jiao",
        "abstract": "Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During back-propagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Oriented_Response_Networks_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.01833v2",
        "gs_citation": 337,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11577141454912475571&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Oriented_Response_Networks_CVPR_2017_paper.html"
    },
    {
        "title": "Outlier-Robust Tensor PCA",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "816",
        "author_site": "Pan Zhou, Jiashi Feng",
        "author": "Pan Zhou; Jiashi Feng",
        "abstract": "Low-rank tensor analysis is important for various real applications in computer vision. However, existing methods focus on recovering a low-rank tensor contaminated by Gaussian or gross sparse noise and hence cannot effectively handle outliers that are common in practical tensor data. To solve this issue, we propose an outlier-robust tensor principle component analysis (OR-TPCA) method for simultaneous low-rank tensor recovery and outlier detection. For intrinsically low-rank tensor observations with arbitrary outlier corruption, OR-TPCA is the first method that has provable performance guarantee for exactly recovering the tensor subspace and detecting outliers  under mild conditions.  Since tensor data are naturally high-dimensional and multi-way, we further develop a fast randomized algorithm that requires small sampling size yet can substantially accelerate OR-TPCA without performance drop. Experimental results on four tasks: outlier detection, clustering, semi-supervised and supervised learning, clearly demonstrate the advantages of our method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Outlier-Robust_Tensor_PCA_CVPR_2017_paper.pdf",
        "aff": "National University of Singapore, Singapore; National University of Singapore, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 975749,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10048007764850847599&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "u.nus.edu;nus.edu.sg",
        "email": "u.nus.edu;nus.edu.sg",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Outlier-Robust_Tensor_PCA_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "National University of Singapore",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nus.edu.sg",
        "aff_unique_abbr": "NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "POSEidon: Face-From-Depth for Driver Pose Estimation",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "1940",
        "author_site": "Guido Borghi, Marco Venturelli, Roberto Vezzani, Rita Cucchiara",
        "author": "Guido Borghi; Marco Venturelli; Roberto Vezzani; Rita Cucchiara",
        "abstract": "Fast and accurate upper-body and head pose estimation is a key task for automatic monitoring of driver attention, a challenging context characterized by severe illumination changes, occlusions and extreme poses.  In this work, we present a new deep learning framework for head localization and pose estimation on depth images. The core of the proposal is a regressive neural network, called POSEidon, which is composed of three independent convolutional nets followed by a fusion layer, specially conceived for understanding the pose by depth.  In addition, to recover the intrinsic value of face appearance for understanding head position and orientation, we propose a new Face-from-Depth model for learning image faces from depth.  Results in face reconstruction are qualitatively impressive.  We test the proposed framework on two public datasets, namely Biwi Kinect Head Pose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by the automotive setup. Results show that our method overcomes all recent state-of-art works, running in real time at more than 30 frames per second.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Borghi_POSEidon_Face-From-Depth_for_CVPR_2017_paper.pdf",
        "aff": "University of Modena and Reggio Emilia; University of Modena and Reggio Emilia; University of Modena and Reggio Emilia; University of Modena and Reggio Emilia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.10195v3",
        "pdf_size": 852694,
        "gs_citation": 231,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3505109138137530533&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "unimore.it;unimore.it;unimore.it;unimore.it",
        "email": "unimore.it;unimore.it;unimore.it;unimore.it",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Borghi_POSEidon_Face-From-Depth_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Modena and Reggio Emilia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unimore.it",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "title": "Parametric T-Spline Face Morphable Model for Detailed Fitting in Shape Subspace",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "2699",
        "author_site": "Weilong Peng, Zhiyong Feng, Chao Xu, Yong Su",
        "author": "Weilong Peng; Zhiyong Feng; Chao Xu; Yong Su",
        "abstract": "Pre-learnt subspace methods, e.g., 3DMMs, are significant exploration for the synthesis of 3D faces by assuming that faces are in a linear class. However, the human face is in a nonlinear manifold, and a new test are always not in the pre-learnt subspace accurately because of the disparity brought by ethnicity, age, gender, etc. In the paper, we propose a parametric T-spline morphable model (T-splineMM) for 3D face representation, which has great advantages of fitting data from an unknown source accurately. In the model, we describe a face by C^2 T-spline surface, and divide the face surface into several shape units (SUs), according to facial action coding system (FACS), on T-mesh instead of on the surface directly. A fitting algorithm is proposed to optimize coefficients of T-spline control point components along pre-learnt identity and expression subspaces, as well as to optimize the details in refinement progress. As any pre-learnt subspace is not complete to handle the variety and details of faces and expressions, it covers a limited span of morphing. SUs division and detail refinement make the model fitting the facial muscle deformation in a larger span of morphing subspace. We conduct experiments on face scan data, kinect data as well as the space-time data to test the performance of detail fitting, robustness to missing data and noise, and to demonstrate the effectiveness of our model. Convincing results are illustrated to demonstrate the effectiveness of our model compared with the popular methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Peng_Parametric_T-Spline_Face_CVPR_2017_paper.pdf",
        "aff": "School of Computer Science and Technology, Tianjin University; School of Computer Software, Tianjin University; School of Computer Software, Tianjin University; School of Computer Science and Technology, Tianjin University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1763558,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8185990339513151699&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Peng_Parametric_T-Spline_Face_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Parsing Images of Overlapping Organisms With Deep Singling-Out Networks",
        "session": "Biomedical Image/Video Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "2782",
        "author_site": "Victor Yurchenko, Victor Lempitsky",
        "author": "Victor Yurchenko; Victor Lempitsky",
        "abstract": "This work is motivated by the mostly unsolved task of parsing biological images with multiple overlapping articulated model organisms (such as worms or larvae). We present a general approach that separates the two main challenges associated with such data, individual object shape estimation and object groups disentangling. At the core of the approach is a deep feed-forward singling-out network (SON) that is trained to map each local patch to a vectorial descriptor that is sensitive to the characteristics (e.g. shape) of a central object, while being invariant to the variability of all other surrounding elements. Given a SON, a local image patch can be matched to a gallery of isolated elements using their SON-descriptors, thus producing a hypothesis about the shape of the central element in that patch. The image-level optimization based on integer programming can then pick a subset of the hypotheses to explain (parse) the whole image and disentangle groups of organisms.   While sharing many similarities with existing \"analysis-by-synthesis\" approaches, our method avoids the need for stochastic search in the high-dimensional configuration space and numerous rendering operations at test-time. We show that our approach can parse microscopy images of three popular model organisms (the C.Elegans roundworms, the Drosophila larvae, and the E.Coli bacteria) even under significant crowding and overlaps between organisms. We speculate that the overall approach is applicable to a wider class of image parsing problems concerned with crowded articulated objects, for which rendering training images is possible.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yurchenko_Parsing_Images_of_CVPR_2017_paper.pdf",
        "aff": "Skolkovo Institute of Science and Technology (Skoltech) + Yandex; Skolkovo Institute of Science and Technology (Skoltech)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.06017v1",
        "pdf_size": 678976,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15721887694136213273&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "skoltech.ru;skoltech.ru",
        "email": "skoltech.ru;skoltech.ru",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yurchenko_Parsing_Images_of_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Skolkovo Institute of Science and Technology;Yandex",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.skoltech.ru;https://yandex.com",
        "aff_unique_abbr": "Skoltech;Yandex",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Russian Federation"
    },
    {
        "title": "Perceptual Generative Adversarial Networks for Small Object Detection",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "446",
        "author_site": "Jianan Li, Xiaodan Liang, Yunchao Wei, Tingfa Xu, Jiashi Feng, Shuicheng Yan",
        "author": "Jianan Li; Xiaodan Liang; Yunchao Wei; Tingfa Xu; Jiashi Feng; Shuicheng Yan",
        "abstract": "Detecting small objects is notoriously challenging due to their low resolution and noisy representation. Existing object detection pipelines usually detect small objects through learning representations of all the objects at multiple scales. However, the performance gain of such ad hoc architectures is usually limited to pay off the computational cost. In this work, we address the small object detection problem by developing a single architecture that internally lifts representations of small objects to super-resolved ones, achieving similar characteristics as large objects and thus more discriminative for detection. For this purpose, we propose a new Perceptual Generative Adversarial Network (Perceptual GAN) model that improves small object detection through narrowing representation difference of small objects from the large ones. Specifically, its generator learns to transfer perceived poor representations of the small objects to super-resolved ones that are similar enough to real large objects to fool a competing discriminator. Meanwhile its discriminator competes with the generator to identify the generated representation and imposes an additional perceptual requirement - generated representations of small objects must be beneficial for detection purpose - on the generator. Extensive evaluations on the challenging Tsinghua-Tencent 100K and the Caltech benchmark well demonstrate the superiority of Perceptual GAN in detecting small objects, including traffic signs and pedestrians, over well-established state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Perceptual_Generative_Adversarial_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1706.05274v2",
        "pdf_size": 1753347,
        "gs_citation": 1052,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8946676958079599000&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Perceptual_Generative_Adversarial_CVPR_2017_paper.html"
    },
    {
        "title": "Person Re-Identification in the Wild",
        "session": "Analyzing Humans 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "505",
        "author_site": "Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Yi Yang, Qi Tian",
        "author": "Liang Zheng; Hengheng Zhang; Shaoyan Sun; Manmohan Chandraker; Yi Yang; Qi Tian",
        "abstract": "This paper presents a novel large-scale dataset and comprehensive baselines for end-to-end pedestrian detection and person recognition in raw video frames. Our baselines address three issues: the performance of various combinations of detectors and recognizers, mechanisms for pedestrian detection to help improve overall re-identification (re-ID) accuracy and assessing the effectiveness of different detectors for re-ID. We make three distinct contributions. First, a new dataset, PRW, is introduced to evaluate Person Re-identification in the Wild, using videos acquired through six synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities. Extensive benchmarking results are presented on this dataset. Second, we show that pedestrian detection aids re-ID through two simple yet effective improvements: a cascaded fine-tuning strategy that trains a detection model first and then the classification model, and a Confidence Weighted Similarity (CWS) metric that incorporates detection scores into similarity measurement. Third, we derive insights in evaluating detector performance for the particular scenario of accurate person re-ID.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zheng_Person_Re-Identification_in_CVPR_2017_paper.pdf",
        "aff": "University of Technology Sydney; UTSA; USTC; UCSD & NEC Labs; University of Technology Sydney; UTSA",
        "project": "http://www.liangzheng.com.cn",
        "github": "",
        "supp": "",
        "arxiv": "1604.02531v2",
        "pdf_size": 2546536,
        "gs_citation": 1011,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2451674287677049201&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com; ; ",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com; ; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zheng_Person_Re-Identification_in_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;3;0;1",
        "aff_unique_norm": "University of Technology Sydney;University of Texas at San Antonio;University of Science and Technology of China;University of California, San Diego",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.uts.edu.au;https://www.utsa.edu;https://www.ustc.edu.cn;https://ucsd.edu",
        "aff_unique_abbr": "UTS;UTSA;USTC;UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";La Jolla",
        "aff_country_unique_index": "0;1;2;1;0;1",
        "aff_country_unique": "Australia;United States;China"
    },
    {
        "title": "Person Search With Natural Language Description",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "729",
        "author_site": "Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu Yue, Xiaogang Wang",
        "author": "Shuang Li; Tong Xiao; Hongsheng Li; Bolei Zhou; Dayu Yue; Xiaogang Wang",
        "abstract": "Searching persons in large-scale image databases with the query of natural language description has important applications in video surveillance. Existing methods mainly focused on searching persons with image-based or attribute-based queries, which have major limitations for a practical usage. In this paper, we study the problem of person search with natural language description. Given the textual description of a person, the algorithm of the person search is required to rank all the samples in the person database then retrieve the most relevant sample corresponding to the queried description. Since there is no person dataset or benchmark with textual description available, we collect a large-scale person description dataset with detailed natural language annotations and person samples from various sources, termed as CUHK Person Description Dataset (CUHK-PEDES). A wide range of possible models and baselines have been evaluated and compared on the person search benchmark. An Recurrent Neural Network with Gated Neural Attention mechanism (GNA-RNN) is proposed to establish the state-of-the art performance on person search.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Person_Search_With_CVPR_2017_paper.pdf",
        "aff": "The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong+Massachuate Institute of Technology; Massachuate Institute of Technology; SenseTime Group Limited; The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.05729v2",
        "pdf_size": 1901428,
        "gs_citation": 537,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14469118557598103766&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;mit.edu;sensetime.com;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;mit.edu;sensetime.com;ee.cuhk.edu.hk",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Person_Search_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0+1;1;2;0",
        "aff_unique_norm": "Chinese University of Hong Kong;Massachusetts Institute of Technology;SenseTime Group Limited",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://web.mit.edu;https://www.sensetime.com",
        "aff_unique_abbr": "CUHK;MIT;SenseTime",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0+1;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Personalizing Gesture Recognition Using Hierarchical Bayesian Neural Networks",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2928",
        "author_site": "Ajjen Joshi, Soumya Ghosh, Margrit Betke, Stan Sclaroff, Hanspeter Pfister",
        "author": "Ajjen Joshi; Soumya Ghosh; Margrit Betke; Stan Sclaroff; Hanspeter Pfister",
        "abstract": "Building robust classifiers trained on data susceptible to group or subject-specific variations is a challenging pattern recognition problem. We develop hierarchical Bayesian neural networks to capture subject-specific variations and share statistical strength across subjects.  Leveraging recent work on learning Bayesian neural networks, we build fast, scalable algorithms for inferring the posterior distribution over all network weights in the hierarchy. We also develop methods for adapting our model to new subjects when a small number of subject-specific personalization data is available. Finally, we investigate active learning algorithms for interactively labeling personalization data in resource-constrained scenarios. Focusing on the problem of gesture recognition where inter-subject variations are commonplace, we demonstrate the effectiveness of our proposed techniques. We test our framework on three widely used gesture recognition datasets, achieving personalization performance competitive with the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Joshi_Personalizing_Gesture_Recognition_CVPR_2017_paper.pdf",
        "aff": "Boston University; IBM T.J. Watson Research Center; Boston University; Boston University; Harvard University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Joshi_Personalizing_Gesture_Recognition_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 758410,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6976752218843963892&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "bu.edu;us.ibm.com;bu.edu;bu.edu;seas.harvard.edu",
        "email": "bu.edu;us.ibm.com;bu.edu;bu.edu;seas.harvard.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Joshi_Personalizing_Gesture_Recognition_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0;2",
        "aff_unique_norm": "Boston University;IBM;Harvard University",
        "aff_unique_dep": ";Research Center;",
        "aff_unique_url": "https://www.bu.edu;https://www.ibm.com/research/watson;https://www.harvard.edu",
        "aff_unique_abbr": "BU;IBM;Harvard",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";T.J. Watson",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
        "session": "Machine Learning 1",
        "status": "Oral",
        "track": "main",
        "pid": "1948",
        "author_site": "Christian Ledig, Lucas Theis, Ferenc Husz\u00c3\u00a1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi",
        "author": "Christian Ledig; Lucas Theis; Ferenc Huszar; Jose Caballero; Andrew Cunningham; Alejandro Acosta; Andrew Aitken; Alykhan Tejani; Johannes Totz; Zehan Wang; Wenzhe Shi",
        "abstract": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf",
        "aff": ";;;;;;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1609.04802v5",
        "pdf_size": 946407,
        "gs_citation": 14895,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1219263946448760936&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": ";;;;;;;;;;",
        "email": ";;;;;;;;;;",
        "author_num": 11,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html"
    },
    {
        "title": "Photorealistic Facial Texture Inference Using Deep Neural Networks",
        "session": "Computational Photography",
        "status": "Spotlight",
        "track": "main",
        "pid": "2133",
        "author_site": "Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, Hao Li",
        "author": "Shunsuke Saito; Lingyu Wei; Liwen Hu; Koki Nagano; Hao Li",
        "abstract": "We present a data-driven inference method that can synthesize a photorealistic texture map of a complete 3D face model given a partial 2D view of a person in the wild.  After an initial estimation of shape and low-frequency albedo, we compute a high-frequency partial texture map, without the shading component, of the visible face area.  To extract the fine appearance details from this incomplete input, we introduce a multi-scale detail analysis technique based on mid-layer feature correlations extracted from a deep convolutional neural network.  We demonstrate that fitting a convex combination of feature correlations from a high-resolution face database can yield a semantically plausible facial detail description of the entire face. A complete and photorealistic texture map can then be synthesized by iteratively optimizing for the reconstructed feature correlations. Using these high-resolution textures and a commercial rendering framework, we can produce high-fidelity 3D renderings that are visually comparable to those obtained with state-of-the-art multi-view face capture systems. We demonstrate successful face reconstructions from a wide range of low resolution input images, including those of historical figures. In addition to extensive evaluations, we validate the realism of our results using a crowdsourced user study.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Saito_Photorealistic_Facial_Texture_CVPR_2017_paper.pdf",
        "aff": "Pinscreen+University of Southern California+USC Institute for Creative Technologies; Pinscreen+University of Southern California+USC Institute for Creative Technologies; Pinscreen+University of Southern California+USC Institute for Creative Technologies; USC Institute for Creative Technologies; Pinscreen+University of Southern California+USC Institute for Creative Technologies",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Saito_Photorealistic_Facial_Texture_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.00523v1",
        "pdf_size": 3064524,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18221632182035315678&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Saito_Photorealistic_Facial_Texture_CVPR_2017_paper.html",
        "aff_unique_index": "0+1+1;0+1+1;0+1+1;1;0+1+1",
        "aff_unique_norm": "Pinscreen;University of Southern California",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.pinscreen.com;https://www.usc.edu",
        "aff_unique_abbr": ";USC",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0+1+1;0+1+1;0+1+1;1;0+1+1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "title": "Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2212",
        "author_site": "Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, Thomas Funkhouser",
        "author": "Yinda Zhang; Shuran Song; Ersin Yumer; Manolis Savva; Joon-Young Lee; Hailin Jin; Thomas Funkhouser",
        "abstract": "Indoor scene understanding is central to applications such as robot navigation and human companion assistance. Over the last years, data-driven deep neural networks have outperformed many traditional approaches thanks to their representation learning capabilities. One of the bottlenecks in training for better representations is the amount of available per-pixel ground truth data that is required for core scene understanding tasks such as semantic segmentation, normal prediction, and object boundary detection. To address this problem, a number of works proposed using synthetic data. However, a systematic study of how such synthetic data is generated is missing.  In this work, we introduce a large-scale synthetic dataset with 500K physically-based rendered images from 45K realistic 3D indoor scenes. We study the effects of rendering methods and scene lighting on training for three computer vision tasks: surface normal prediction, semantic segmentation, and object boundary detection. This study provides insights into the best practices for training with synthetic data (more realistic rendering is worth it) and shows that pretraining with our new synthetic dataset can improve results beyond the current state of the art on all three tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Physically-Based_Rendering_for_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_Physically-Based_Rendering_for_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.07429v3",
        "pdf_size": 5042123,
        "gs_citation": 329,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10519725307300171189&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Physically-Based_Rendering_for_CVPR_2017_paper.html"
    },
    {
        "title": "Physics Inspired Optimization on Semantic Transfer Features: An Alternative Method for Room Layout Estimation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "17",
        "author_site": "Hao Zhao, Ming Lu, Anbang Yao, Yiwen Guo, Yurong Chen, Li Zhang",
        "author": "Hao Zhao; Ming Lu; Anbang Yao; Yiwen Guo; Yurong Chen; Li Zhang",
        "abstract": "In this paper, we propose an alternative method to estimate room layouts of cluttered indoor scenes. This method enjoys the benefits of two novel techniques. The first one is semantic transfer (ST), which is: (1) a formulation to integrate the relationship between scene clutter and room layout into convolutional neural networks; (2) an architecture that can be end-to-end trained; (3) a practical strategy to initialize weights for very deep networks under unbalanced training data distribution. ST allows us to extract highly robust features under various circumstances, and in order to address the computation redundance hidden in these features we develop a principled and efficient inference scheme named physics inspired optimization (PIO). PIO's basic idea is to formulate some phenomena observed in ST features into mechanics concepts. Evaluations on public datasets LSUN and Hedau show that the proposed method is more accurate than state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Physics_Inspired_Optimization_CVPR_2017_paper.pdf",
        "aff": "Department of Electronic Engineering, Tsinghua University; Department of Electronic Engineering, Tsinghua University; Cognitive Computing Laboratory, Intel Labs China; Cognitive Computing Laboratory, Intel Labs China; Cognitive Computing Laboratory, Intel Labs China; Department of Electronic Engineering, Tsinghua University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhao_Physics_Inspired_Optimization_2017_CVPR_supplemental.zip",
        "arxiv": "1707.00383v1",
        "pdf_size": 1975879,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10804205328289371571&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;intel.com;intel.com;intel.com;mail.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;intel.com;intel.com;intel.com;mail.tsinghua.edu.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhao_Physics_Inspired_Optimization_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;1;1;0",
        "aff_unique_norm": "Tsinghua University;Intel",
        "aff_unique_dep": "Department of Electronic Engineering;Cognitive Computing Laboratory",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.intel.com/content/www/us/en/research/labs.html",
        "aff_unique_abbr": "THU;Intel",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Pixelwise Instance Segmentation With a Dynamically Instantiated Network",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "144",
        "author_site": "Anurag Arnab, Philip H. S. Torr",
        "author": "Anurag Arnab; Philip H. S. Torr",
        "abstract": "Semantic segmentation and object detection research have recently achieved rapid progress. However, the former task has no notion of different instances of the same object, and the latter operates at a coarse, bounding-box level. We propose an Instance Segmentation system that produces a segmentation map where each pixel is assigned an object class and instance identity label. Most approaches adapt object detectors to produce segments instead of boxes. In contrast, our method is based on an initial semantic segmentation module, which feeds into an instance subnetwork. This subnetwork uses the initial category-level segmentation, along with cues from the output of an object detector, within an end-to-end CRF to predict instances. This part of our model is dynamically instantiated to produce a variable number of instances per image. Our end-to-end approach requires no post-processing and considers the image holistically, instead of processing independent proposals. Therefore, unlike some related work, a pixel cannot belong to multiple instances. Furthermore, far more precise segmentations are achieved, as shown by our state-of-the-art results (particularly at high IoU thresholds) on the Pascal VOC and Cityscapes datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Arnab_Pixelwise_Instance_Segmentation_CVPR_2017_paper.pdf",
        "aff": "University of Oxford; University of Oxford",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Arnab_Pixelwise_Instance_Segmentation_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.02386",
        "pdf_size": 751067,
        "gs_citation": 311,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17843465810057852859&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "eng.ox.ac.uk;eng.ox.ac.uk",
        "email": "eng.ox.ac.uk;eng.ox.ac.uk",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Arnab_Pixelwise_Instance_Segmentation_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space",
        "session": "Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "1822",
        "author_site": "Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, Jason Yosinski",
        "author": "Anh Nguyen; Jeff Clune; Yoshua Bengio; Alexey Dosovitskiy; Jason Yosinski",
        "abstract": "Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. 2016 showed one interesting way to synthesize novel images by performing gradient descent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network.  In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227x227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models \"Plug and Play Generative Networks\". PPGNs are composed of (1) a generator network G that is capable of drawing a wide range of image types and (2) a replaceable \"condition\" network C that tells the generator what to draw. We demonstrate generation of images conditioned on a class - when C is an ImageNet classification network - and also conditioned on a caption - when C is an image captioning network. Our method also improves the state of the art of Deep Multifaceted Feature Visualization, which involves synthetically generating the set of inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While we operate on images in this paper, the approach is modality agnostic and can be applied to many types of data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Nguyen_Plug__Play_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Nguyen_Plug__Play_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.00005v2",
        "gs_citation": 1035,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8069456009591620263&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Nguyen_Plug__Play_CVPR_2017_paper.html"
    },
    {
        "title": "Point to Set Similarity Based Deep Feature Learning for Person Re-Identification",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1397",
        "author_site": "Sanping Zhou, Jinjun Wang, Jiayun Wang, Yihong Gong, Nanning Zheng",
        "author": "Sanping Zhou; Jinjun Wang; Jiayun Wang; Yihong Gong; Nanning Zheng",
        "abstract": "Person re-identification (Re-ID) remains a challenging problem due to significant appearance changes caused by variations in  view angle, background clutter, illumination condition and mutual occlusion. To address these issues, conventional methods usually focus on proposing robust feature representation or learning metric transformation based on pairwise similarity, using Fisher-type criterion. The recent development in deep learning based approaches address the two processes in a joint fashion and have achieved promising progress. One of the key issues for deep learning based person Re-ID is the selection of proper similarity comparison criteria, and the performance of learned features using existing criterion based on pairwise similarity is still limited, because only P2P distances are mostly considered. In this paper, we present a novel person Re-ID method based on P2S similarity comparison. The P2S metric can jointly minimize the intra-class distance and maximize the inter-class distance, while back-propagating the gradient to optimize parameters of the deep model. By utilizing our proposed P2S metric, the learned deep model can effectively distinguish different persons by learning discriminative and stable feature representations. Comprehensive experimental evaluations on 3DPeS, CUHK01, PRID2011 and Market1501 datasets demonstrate the advantages of our method over the state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Point_to_Set_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 191,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7829420717951652202&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Point_to_Set_CVPR_2017_paper.html"
    },
    {
        "title": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
        "session": "Machine Learning 1",
        "status": "Oral",
        "track": "main",
        "pid": "201",
        "author_site": "Charles R. Qi, Hao Su, Kaichun Mo, Leonidas J. Guibas",
        "author": "Charles R. Qi; Hao Su; Kaichun Mo; Leonidas J. Guibas",
        "abstract": "Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Qi_PointNet_Deep_Learning_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.00593",
        "pdf_size": 2288792,
        "gs_citation": 19941,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1958213547177282284&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html"
    },
    {
        "title": "Polarimetric Multi-View Stereo",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "579",
        "author_site": "Zhaopeng Cui, Jinwei Gu, Boxin Shi, Ping Tan, Jan Kautz",
        "author": "Zhaopeng Cui; Jinwei Gu; Boxin Shi; Ping Tan; Jan Kautz",
        "abstract": "Multi-view stereo relies on feature correspondences for 3D reconstruction, and thus is fundamentally flawed in dealing with featureless scenes. In this paper, we propose polarimetric multi-view stereo, which combines per-pixel photometric information from polarization with epipolar constraints from multiple views for 3D reconstruction. Polarization reveals surface normal information, and is thus helpful to propagate depth to featureless regions. Polarimetric multi-view stereo is completely passive and can be applied outdoors in uncontrolled illumination, since the data capture can be done simply with either a polarizer or a polarization camera. Unlike previous work on shape-from-polarization which is limited to either diffuse polarization or specular polarization only, we propose a novel polarization imaging model that can handle real-world objects with mixed polarization. We prove there are exactly two types of ambiguities on estimating surface azimuth angles from polarization, and we resolve them with graph optimization and iso-depth contour tracing. This step significantly improves the initial depth map estimate, which are later fused together for complete 3D reconstruction. Extensive experimental results demonstrate high-quality 3D reconstruction and better performance than state-of-the-art multi-view stereo methods, especially on featureless 3D objects, such as ceramic tiles, office room with white walls, and highly reflective cars in the outdoors.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cui_Polarimetric_Multi-View_Stereo_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Cui_Polarimetric_Multi-View_Stereo_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "gs_citation": 174,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3089882665360886290&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cui_Polarimetric_Multi-View_Stereo_CVPR_2017_paper.html"
    },
    {
        "title": "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "224",
        "author_site": "Xingcheng Zhang, Zhizhong Li, Chen Change Loy, Dahua Lin",
        "author": "Xingcheng Zhang; Zhizhong Li; Chen Change Loy; Dahua Lin",
        "abstract": "A number of studies have shown that increasing the depth or width of convolutional networks is a rewarding approach to improve the performance of image recognition. In our study, however, we observed difficulties along both directions. On one hand, the pursuit for very deep networks is met with a diminishing return and increased training difficulty; on the other hand, widening a network would result in a quadratic growth in both computational cost and memory demand. These difficulties motivate us to explore structural diversity in designing deep networks, a new dimension beyond just depth and width. Specifically, we present a new family of modules, namely the PolyInception, which can be flexibly inserted in isolation or in a composition as replacements of different parts of a network. Choosing PolyInception modules with the guidance of architectural efficiency can improve the expressive power while preserving comparable computational cost. The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_PolyNet_A_Pursuit_CVPR_2017_paper.pdf",
        "aff": "The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong",
        "project": "",
        "github": "https://github.com/CUHK-MMLAB/polynet",
        "supp": "",
        "arxiv": "1611.05725",
        "pdf_size": 607767,
        "gs_citation": 329,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=988258480855586208&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_PolyNet_A_Pursuit_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Polyhedral Conic Classifiers for Visual Object Detection and Classification",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "100",
        "author_site": "Hakan Cevikalp, Bill Triggs",
        "author": "Hakan Cevikalp; Bill Triggs",
        "abstract": "We propose a family of quasi-linear discriminants that outperform  current large-margin methods in sliding window visual object  detection and open set recognition tasks. In these tasks the  classification problems are both numerically imbalanced -- positive (object class) training and test windows are much rarer than  negative (non-class) ones -- and geometrically asymmetric -- the  positive samples typically form compact, visually-coherent groups  while negatives are much more diverse, including anything at all  that is not a well-centred sample from the target class. It is  difficult to cover such negative classes using training samples,  and doubly so in `open set' applications where run-time negatives  may stem from classes that were not seen at all during training. So  there is a need for discriminants whose decision regions focus on  tightly circumscribing the positive class, while still taking  account of negatives in zones where the two classes overlap. This  paper introduces a family of quasi-linear \"polyhedral conic\"  discriminants whose positive regions are distorted L1 balls. The  methods have properties and run-time complexities comparable to linear Support Vector Machines (SVMs), and they can be trained from either binary or positive-only samples using constrained quadratic programs related to SVMs. Our experiments show that they significantly outperform both linear SVMs and existing one-class  discriminants on a wide range of object detection, open set  recognition and conventional closed-set classification tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cevikalp_Polyhedral_Conic_Classifiers_CVPR_2017_paper.pdf",
        "aff": "Eskisehir Osmangazi University; Laboratoire Jean Kuntzmann",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 928910,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2681605933266046576&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;imag.fr",
        "email": "gmail.com;imag.fr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cevikalp_Polyhedral_Conic_Classifiers_CVPR_2017_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Eskisehir Osmangazi University;Laboratoire Jean Kuntzmann",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ogu.edu.tr;https://www.ljk.ij.cnrs.fr",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "T\u00fcrkiye;France"
    },
    {
        "title": "Pose-Aware Person Recognition",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2747",
        "author_site": "Vijay Kumar, Anoop Namboodiri, Manohar Paluri, C. V. Jawahar",
        "author": "Vijay Kumar; Anoop Namboodiri; Manohar Paluri; C. V. Jawahar",
        "abstract": "Person recognition methods that use multiple body regions have shown significant improvements over traditional face-based recognition. One of the primary challenges in full-body person recognition is the extreme variation in pose and view point. In this work,  (i) we present an approach that tackles pose variations utilizing multiple models that are trained on specific poses, and combined using pose-aware weights during testing. (ii) For learning a person representation, we propose a network that jointly optimizes a single loss over multiple body regions. (iii) Finally, we introduce new benchmarks to evaluate person recognition in diverse scenarios and show significant improvements over previously proposed approaches on all the benchmarks including the photo album setting of PIPA.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kumar_Pose-Aware_Person_Recognition_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kumar_Pose-Aware_Person_Recognition_2017_CVPR_supplemental.pdf",
        "arxiv": "1705.10120v1",
        "pdf_size": 1533634,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13406805647220156874&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kumar_Pose-Aware_Person_Recognition_CVPR_2017_paper.html"
    },
    {
        "title": "PoseAgent: Budget-Constrained 6D Object Pose Estimation via Reinforcement Learning",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3085",
        "author_site": "Alexander Krull, Eric Brachmann, Sebastian Nowozin, Frank Michel, Jamie Shotton, Carsten Rother",
        "author": "Alexander Krull; Eric Brachmann; Sebastian Nowozin; Frank Michel; Jamie Shotton; Carsten Rother",
        "abstract": "State-of-the-art computer vision algorithms often achieve efficiency by making discrete choices about which hypotheses to explore next. This allows allocation of computational resources to promising candidates, however, such decisions are non-differentiable.  As a result, these algorithms are hard to train in an end-to-end fashion. In this work we propose to learn an efficient algorithm for the task of 6D object pose estimation. Our system optimizes the parameters of an existing state-of-the art pose estimation system using reinforcement learning, where the pose estimation system now becomes the stochastic policy, parametrized by a CNN. Additionally, we present an efficient training algorithm that dramatically reduces computation time. We show empirically that our learned pose estimation procedure makes better use of limited resources and improves upon the state-of-the-art on a challenging dataset. Our approach enables differentiable end-to-end training of complex algorithmic pipelines and learns to make optimal use of a given computational budget.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Krull_PoseAgent_Budget-Constrained_6D_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.03779v2",
        "pdf_size": 734181,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6551415250984070856&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Krull_PoseAgent_Budget-Constrained_6D_CVPR_2017_paper.html"
    },
    {
        "title": "PoseTrack: Joint Multi-Person Pose Estimation and Tracking",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "740",
        "author_site": "Umar Iqbal, Anton Milan, Juergen Gall",
        "author": "Umar Iqbal; Anton Milan; Juergen Gall",
        "abstract": "In this work, we introduce the challenging problem of joint multi-person pose estimation and tracking of an unknown number of persons in unconstrained videos. Existing methods for multi-person pose estimation in images cannot be applied directly to this problem, since it also requires to solve the problem of person association over time in addition to the pose estimation for each person. We therefore propose a novel method that jointly models multi-person pose estimation and tracking in a single formulation. To this end, we represent body joint detections in a video by a spatio-temporal graph and solve an integer linear program to partition the graph into sub-graphs that correspond to plausible body pose trajectories for each person. The proposed approach implicitly handles occlusion and truncation of persons. Since the problem has not been addressed quantitatively in the literature, we introduce a challenging \"Multi-Person PoseTrack\" dataset, and also propose a completely unconstrained evaluation protocol that does not make any assumptions about the scale, size, location or the number of persons. Finally, we evaluate the proposed approach and several baseline methods on our new dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Iqbal_PoseTrack_Joint_Multi-Person_CVPR_2017_paper.pdf",
        "aff": "Computer Vision Group, University of Bonn, Germany; Australian Centre for Visual Technologies, University of Adelaide, Australia; Computer Vision Group, University of Bonn, Germany",
        "project": "http://pages.iai.uni-bonn.de/iqbal_umar/PoseTrack/",
        "github": "",
        "supp": "",
        "arxiv": "1611.07727v3",
        "pdf_size": 978226,
        "gs_citation": 301,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12880997048796780627&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "; ; ",
        "email": "; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Iqbal_PoseTrack_Joint_Multi-Person_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Bonn;University of Adelaide",
        "aff_unique_dep": "Computer Vision Group;Australian Centre for Visual Technologies",
        "aff_unique_url": "https://www.uni-bonn.de;https://www.adelaide.edu.au",
        "aff_unique_abbr": ";Adelaide",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Adelaide",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;Australia"
    },
    {
        "title": "Position Tracking for Virtual Reality Using Commodity WiFi",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "32",
        "author_site": "Manikanta Kotaru, Sachin Katti",
        "author": "Manikanta Kotaru; Sachin Katti",
        "abstract": "Today, experiencing virtual reality (VR) is a cumbersome experience which either requires dedicated infrastructure like infrared cameras to track the headset and hand-motion controllers (e.g., Oculus Rift, HTC Vive), or provides only 3-DoF (Degrees of Freedom) tracking which severely limits the user experience (e.g., Samsung Gear). To truly enable VR everywhere, we need position tracking to be available as a ubiquitous service. This paper presents WiCapture, a novel approach which leverages commodity WiFi infrastructure, which is ubiquitous today, for tracking purposes. We prototype WiCapture using off-the-shelf WiFi radios and show that it achieves an accuracy of 0.88 cm compared to sophisticated infrared-based tracking systems like the Oculus, while providing much higher range, resistance to occlusion, ubiquity and ease of deployment.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kotaru_Position_Tracking_for_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kotaru_Position_Tracking_for_2017_CVPR_supplemental.zip",
        "arxiv": "1703.03468v2",
        "gs_citation": 139,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14687522647021221159&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kotaru_Position_Tracking_for_CVPR_2017_paper.html"
    },
    {
        "title": "Predicting Behaviors of Basketball Players From First Person Videos",
        "session": "Analyzing Humans 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "554",
        "author_site": "Shan Su, Jung Pyo Hong, Jianbo Shi, Hyun Soo Park",
        "author": "Shan Su; Jung Pyo Hong; Jianbo Shi; Hyun Soo Park",
        "abstract": "This paper presents a method to predict the future movements (location and gaze direction) of basketball players as a whole from their first person videos. The predicted behaviors reflect an individual physical space that affords to take the next actions while conforming to social behaviors by engaging to joint attention. Our key innovation is to use the 3D reconstruction of multiple first person cameras to automatically annotate each other's visual semantics of social configurations. We leverage two learning signals uniquely embedded in first person videos. Individually, a first person video records the visual semantics of a spatial and social layout around a person that allows associating with past similar situations. Collectively, first person videos follow joint attention that can link the individuals to a group. We learn the egocentric visual semantics of group movements using a Siamese neural network to retrieve future trajectories. We consolidate the retrieved trajectories from all players by maximizing a measure of social compatibility---the gaze alignment towards joint attention predicted by their social formation, where the dynamics of joint attention is learned by a long-term recurrent convolutional network. This allows us to characterize which social configuration is more plausible and predict future group trajectories.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Su_Predicting_Behaviors_of_CVPR_2017_paper.pdf",
        "aff": "University of Pennsylvania; KAIST; University of Pennsylvania; University of Minnesota",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Su_Predicting_Behaviors_of_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 18989703,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17256294781359809454&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "seas.upenn.edu;ai.kaist.ac.kr;seas.upenn.edu;umn.edu",
        "email": "seas.upenn.edu;ai.kaist.ac.kr;seas.upenn.edu;umn.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Su_Predicting_Behaviors_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of Pennsylvania;Korea Advanced Institute of Science and Technology;University of Minnesota",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.upenn.edu;https://www.kaist.ac.kr;https://www.minnesota.edu",
        "aff_unique_abbr": "UPenn;KAIST;UMN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "title": "Predicting Ground-Level Scene Layout From Aerial Imagery",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "283",
        "author_site": "Menghua Zhai, Zachary Bessinger, Scott Workman, Nathan Jacobs",
        "author": "Menghua Zhai; Zachary Bessinger; Scott Workman; Nathan Jacobs",
        "abstract": "We introduce a novel strategy for learning to extract semantically meaningful features from aerial imagery. Instead of manually labeling the aerial imagery, we propose to predict (noisy) semantic features automatically extracted from co-located ground imagery. Our network architecture takes an aerial image as input, extracts features using a convolutional neural network, and then applies an adaptive transformation to map these features into the ground-level perspective. We use an end-to-end learning approach to minimize the difference between the semantic segmentation extracted directly from the ground image and the semantic segmentation predicted solely based on the aerial image.  We show that a model learned using this strategy, with no additional training, is already capable of rough semantic labeling of aerial imagery. Furthermore, we demonstrate that by finetuning this model we can achieve more accurate semantic segmentation than two baseline initialization strategies. We use our network to address the task of estimating the geolocation and geo-orientation of a ground image. Finally, we show how features extracted from an aerial image can be used to hallucinate a plausible ground-level panorama.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhai_Predicting_Ground-Level_Scene_CVPR_2017_paper.pdf",
        "aff": "Computer Science, University of Kentucky; Computer Science, University of Kentucky; Computer Science, University of Kentucky; Computer Science, University of Kentucky",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhai_Predicting_Ground-Level_Scene_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.02709v1",
        "pdf_size": 4842532,
        "gs_citation": 282,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17311320291098371037&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.uky.edu;cs.uky.edu;cs.uky.edu;cs.uky.edu",
        "email": "cs.uky.edu;cs.uky.edu;cs.uky.edu;cs.uky.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhai_Predicting_Ground-Level_Scene_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Kentucky",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "https://www.uky.edu",
        "aff_unique_abbr": "UK",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Predicting Salient Face in Multiple-Face Videos",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1805",
        "author_site": "Yufan Liu, Songyang Zhang, Mai Xu, Xuming He",
        "author": "Yufan Liu; Songyang Zhang; Mai Xu; Xuming He",
        "abstract": "Although the recent success of convolutional neural network (CNN) advances state-of-the-art saliency prediction in static images, few work has addressed the problem of predicting attention in videos. On the other hand, we find that the attention of different subjects consistently focuses on a single face in each frame of videos involving multiple faces. Therefore, we propose in this paper a novel deep learning (DL) based method to predict salient face in multiple-face videos, which  is capable of learning features and transition of salient faces across video frames. In particular, we first learn a CNN for each frame to locate salient face. Taking CNN features as input, we develop a multiple-stream long short-term memory (M-LSTM) network to predict the temporal transition of salient faces in video sequences. To evaluate our DL-based method, we build a new eye-tracking database of multiple-face videos.  The experimental results show that our method outperforms the prior state-of-the-art methods in predicting visual attention on faces in multiple-face videos.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Predicting_Salient_Face_CVPR_2017_paper.pdf",
        "aff": "Beihang University; Beihang University + ShanghaiTech University; Beihang University; ShanghaiTech University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3309027,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1853065526769564536&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;shanghaitech.edu.cn",
        "email": "buaa.edu.cn;buaa.edu.cn;buaa.edu.cn;shanghaitech.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Predicting_Salient_Face_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;0;1",
        "aff_unique_norm": "Beihang University;ShanghaiTech University",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.buaa.edu.cn/;https://www.shanghaitech.edu.cn",
        "aff_unique_abbr": "BUAA;ShanghaiTech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Predictive-Corrective Networks for Action Detection",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "328",
        "author_site": "Achal Dave, Olga Russakovsky, Deva Ramanan",
        "author": "Achal Dave; Olga Russakovsky; Deva Ramanan",
        "abstract": "While deep feature learning has revolutionized techniques for static-image understanding, the same does not quite hold for video processing. Architectures and optimization techniques used for video are largely based off those for static images, potentially underutilizing rich video information. In this work, we rethink both the underlying network architecture and the stochastic learning paradigm for temporal data. To do so, we draw inspiration from classic theory on linear dynamic systems for modeling time series. By extending such models to include nonlinear mappings, we derive a series of novel recurrent neural networks that sequentially make top-down predictions about the future and then correct those predictions with bottom-up observations. Predictive-corrective networks have a number of desirable properties: (1) they can adaptively focus computation on \"surprising\" frames where predictions require large corrections, (2) they simplify learning in that only \"residual-like\" corrective terms need to be learned over time and (3) they naturally decorrelate an input data stream in a hierarchical fashion, producing a more reliable signal for learning at each layer of a network. We provide an extensive analysis of our lightweight and interpretable framework, and demonstrate that our model is competitive with the two-stream network on three challenging datasets without the need for computationally expensive optical flow.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dave_Predictive-Corrective_Networks_for_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03615v2",
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1378162483376360395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dave_Predictive-Corrective_Networks_for_CVPR_2017_paper.html"
    },
    {
        "title": "Primary Object Segmentation in Videos Based on Region Augmentation and Reduction",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1272",
        "author_site": "Yeong Jun Koh, Chang-Su Kim",
        "author": "Yeong Jun Koh; Chang-Su Kim",
        "abstract": "A novel algorithm to segment a primary object in a video sequence is proposed in this work. First, we generate candidate regions for the primary object using both color and motion edges. Second,we estimate initial primary object regions, by exploiting the recurrence property of the primary object. Third, we augment the initial regions with missing parts or reducing them by excluding noisy parts repeatedly. This augmentation and reduction process (ARP) identifies the primary object region in each frame. Experimental results demonstrate that the proposed algorithm significantly outperforms the state-of-the-art conventional algorithms on recent benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Koh_Primary_Object_Segmentation_CVPR_2017_paper.pdf",
        "aff": "Korea University; Korea University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3482900,
        "gs_citation": 186,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6428283019357774066&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "mcl.korea.ac.kr;korea.ac.kr",
        "email": "mcl.korea.ac.kr;korea.ac.kr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Koh_Primary_Object_Segmentation_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Korea University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.korea.ac.kr",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Probabilistic Temporal Subspace Clustering",
        "session": "Machine Learning 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "1132",
        "author_site": "Behnam Gholami, Vladimir Pavlovic",
        "author": "Behnam Gholami; Vladimir Pavlovic",
        "abstract": "Subspace clustering is a common modeling paradigm used to identify constituent modes of variation in data with locally linear structure.  These structures are common to many problems in computer vision, including modeling time series of complex human motion. However classical subspace clustering algorithms learn the relationships within a set of data without considering the temporal dependency and then use a separate clustering step (e.g., spectral clustering) for final segmentation. Moreover, these, frequently optimization-based, algorithms assume that all observations have complete features. In contrast in real-world applications, some features are often missing, which results in incomplete data and substantial performance degeneration of these approaches. In this paper, we propose a unified non-parametric generative framework for temporal subspace clustering to segment data drawn from a sequentially ordered union of subspaces that deals with the missing features in a principled way. The non-parametric nature of our generative model makes it possible to infer the number of subspaces and their dimension automatically from data. Experimental results on human action datasets demonstrate that the proposed model consistently outperforms other state-of-the-art subspace clustering approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gholami_Probabilistic_Temporal_Subspace_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, Rutgers University; Department of Computer Science, Rutgers University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Gholami_Probabilistic_Temporal_Subspace_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2134692,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11425597872657988597&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cs.rutgers.edu;cs.rutgers.edu",
        "email": "cs.rutgers.edu;cs.rutgers.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gholami_Probabilistic_Temporal_Subspace_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rutgers University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.rutgers.edu",
        "aff_unique_abbr": "Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Procedural Generation of Videos to Train Deep Action Recognition Networks",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1978",
        "author_site": "C\u00c3\u00a9sar Roberto de Souza, Adrien Gaidon, Yohann Cabon, Antonio Manuel L\u00c3\u00b3pez",
        "author": "Cesar Roberto de Souza; Adrien Gaidon; Yohann Cabon; Antonio Manuel Lopez",
        "abstract": "Deep learning for human action recognition in videos is making significant progress, but is slowed down by its dependency on expensive manual labeling of large video collections. In this work, we investigate the generation of synthetic training data for action recognition, as it has recently shown promising results for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation and other computer graphics techniques of modern game engines. We generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for \"Procedural Human Action Videos\". It contains a total of 39,982 videos, with more than 1,000 examples for each action of 35 categories. Our approach is not limited to existing motion capture sequences, and we procedurally define 14 synthetic actions. We introduce a deep multi-task representation learning architecture to mix synthetic and real videos, even if the action categories differ. Our experiments on the UCF101 and HMDB51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance, significantly outperforming fine-tuning state-of-the-art unsupervised generative models of videos.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/de_Souza_Procedural_Generation_of_CVPR_2017_paper.pdf",
        "aff": "Computer Vision Group, Xerox Research Center Europe, Meylan, France+Centre de Visi \u00b4o per Computador, Universitat Aut `onoma de Barcelona, Bellaterra, Spain; Toyota Research Institute, Los Altos, CA, USA; Computer Vision Group, Xerox Research Center Europe, Meylan, France; Centre de Visi \u00b4o per Computador, Universitat Aut `onoma de Barcelona, Bellaterra, Spain",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/de_Souza_Procedural_Generation_of_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.00881v2",
        "pdf_size": 2742338,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5259776868376784898&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "xrce.xerox.com;tri.global;xrce.xerox.com;cvc.uab.es",
        "email": "xrce.xerox.com;tri.global;xrce.xerox.com;cvc.uab.es",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/de_Souza_Procedural_Generation_of_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2;0;1",
        "aff_unique_norm": "Xerox Research Center Europe;Universitat Autonoma de Barcelona;Toyota Research Institute",
        "aff_unique_dep": "Computer Vision Group;Centre de Visio per Computador;",
        "aff_unique_url": "https://www.xerox.com/research-centers/europe.html;https://www.uab.cat;https://www.tri.global",
        "aff_unique_abbr": "XRC Europe;UAB;TRI",
        "aff_campus_unique_index": "0+1;2;0;1",
        "aff_campus_unique": "Meylan;Bellaterra;Los Altos",
        "aff_country_unique_index": "0+1;2;0;1",
        "aff_country_unique": "France;Spain;United States"
    },
    {
        "title": "Product Manifold Filter: Non-Rigid Shape Correspondence via Kernel Density Estimation in the Product Space",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1244",
        "author_site": "Matthias Vestner, Roee Litman, Emanuele Rodol\u00c3\u00a0, Alex Bronstein, Daniel Cremers",
        "author": "Matthias Vestner; Roee Litman; Emanuele Rodola; Alex Bronstein; Daniel Cremers",
        "abstract": "Many algorithms for the computation of correspondences between deformable shapes rely on some variant of nearest neighbor matching in a descriptor space. Such are, for example, various point-wise correspondence recovery algorithms used as a post-processing stage in the functional correspondence framework. Such frequently used techniques implicitly make restrictive assumptions (e.g., nearisometry) on the considered shapes and in practice suffer from lack of accuracy and result in poor surjectivity. We propose an alternative recovery technique capable of guaranteeing a bijective correspondence and producing significantly higher accuracy and smoothness. Unlike other methods our approach does not depend on the assumption that the analyzed shapes are isometric. We derive the proposed method from the statistical framework of kernel density estimation and demonstrate its performance on several challenging deformable 3D shape matching datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Vestner_Product_Manifold_Filter_CVPR_2017_paper.pdf",
        "aff": "Technical University Munich; Tel-Aviv University; USI Lugano; Technion, Israel Institute of Technology + Perceptual Computing Group, Intel, Israel; Technical University Munich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.00669v2",
        "pdf_size": 2147263,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15372370349211701897&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Vestner_Product_Manifold_Filter_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;3+4;0",
        "aff_unique_norm": "Technical University of Munich;Tel Aviv University;Universit\u00e0 della Svizzera italiana;Israel Institute of Technology;Intel",
        "aff_unique_dep": ";;;;Perceptual Computing Group",
        "aff_unique_url": "https://www.tum.de;https://www.tau.ac.il;https://www.usi.ch;https://www.technion.ac.il/en/;https://www.intel.com",
        "aff_unique_abbr": "TUM;TAU;USI;Technion;Intel",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Lugano",
        "aff_country_unique_index": "0;1;2;1+1;0",
        "aff_country_unique": "Germany;Israel;Switzerland"
    },
    {
        "title": "Product Split Trees",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2743",
        "author_site": "Artem Babenko, Victor Lempitsky",
        "author": "Artem Babenko; Victor Lempitsky",
        "abstract": "In this work, we introduce a new kind of spatial partition trees for efficient nearest-neighbor search. Our approach first identifies a set of useful data splitting directions, and then learns a codebook that can be used to encode such directions. We use the product-quantization idea in order to make the effective codebook large, the evaluation of scalar products between the query and the encoded splitting direction very fast, and the encoding itself compact. As a result, the proposed data srtucture (Product Split tree) achieves compact clustering of data points, while keeping the  traversal very efficient. In the nearest-neighbor search experiments on high-dimensional data, product split trees achieved state-of-the-art performance, demonstrating better speed-accuracy tradeoff than other spatial partition trees.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Babenko_Product_Split_Trees_CVPR_2017_paper.pdf",
        "aff": "Yandex, National Research University Higher School of Economics; Skolkovo Institute of Science and Technology (Skoltech)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1126738,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11618952552513156529&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "phystech.edu;skoltech.ru",
        "email": "phystech.edu;skoltech.ru",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Babenko_Product_Split_Trees_CVPR_2017_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "National Research University Higher School of Economics;Skolkovo Institute of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://hse.ru;https://www.skoltech.ru",
        "aff_unique_abbr": "HSE;Skoltech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Russian Federation"
    },
    {
        "title": "Provable Self-Representation Based Outlier Detection in a Union of Subspaces",
        "session": "Machine Learning 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "1258",
        "author_site": "Chong You, Daniel P. Robinson, Ren\u00c3\u00a9 Vidal",
        "author": "Chong You; Daniel P. Robinson; Rene Vidal",
        "abstract": "Many computer vision tasks involve processing large amounts of data contaminated by outliers, which need to be detected and rejected. While outlier detection methods based on robust statistics have existed for decades, only recently have methods based on sparse and low-rank representation been developed along with guarantees of correct outlier detection when the inliers lie in one or more low-dimensional subspaces. This paper proposes a new outlier detection method that combines tools from sparse representation with random walks on a graph. By exploiting the property that data points can be expressed as sparse linear combinations of each other, we obtain an asymmetric affinity matrix among data points, which we use to construct a weighted directed graph. By defining a suitable Markov Chain from this graph, we establish a connection between inliers/outliers and essential/inessential states of the Markov chain, which allows us to detect outliers by using random walks. We provide a theoretical analysis that justifies the correctness of our method under geometric and connectivity assumptions. Experimental results on image databases demonstrate its superiority with respect to state-of-the-art sparse and low-rank outlier detection methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/You_Provable_Self-Representation_Based_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03925",
        "pdf_size": 892485,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12997389039960184817&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/You_Provable_Self-Representation_Based_CVPR_2017_paper.html"
    },
    {
        "title": "Pyramid Scene Parsing Network",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1054",
        "author_site": "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia",
        "author": "Hengshuang Zhao; Jianping Shi; Xiaojuan Qi; Xiaogang Wang; Jiaya Jia",
        "abstract": "Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Pyramid_Scene_Parsing_CVPR_2017_paper.pdf",
        "aff": "The Chinese University of Hong Kong; SenseTime Group Limited; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhao_Pyramid_Scene_Parsing_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.01105v2",
        "pdf_size": 4788287,
        "gs_citation": 17601,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14280592669518971589&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 28,
        "aff_domain": "cse.cuhk.edu.hk;sensetime.com;cse.cuhk.edu.hk;ee.cuhk.edu.hk;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;sensetime.com;cse.cuhk.edu.hk;ee.cuhk.edu.hk;cse.cuhk.edu.hk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhao_Pyramid_Scene_Parsing_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong;SenseTime Group Limited",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.sensetime.com",
        "aff_unique_abbr": "CUHK;SenseTime",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Quad-Networks: Unsupervised Learning to Rank for Interest Point Detection",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "673",
        "author_site": "Nikolay Savinov, Akihito Seki, \u00c4\u00bdubor Ladick\u00c3\u00bd, Torsten Sattler, Marc Pollefeys",
        "author": "Nikolay Savinov; Akihito Seki; Lubor Ladicky; Torsten Sattler; Marc Pollefeys",
        "abstract": "Several machine learning tasks require to represent the data using only a sparse set of interest points. An ideal detector is able to find the corresponding interest points even if the data undergo a transformation typical for a given domain. Since the task is of high practical interest in computer vision, many hand-crafted solutions were proposed. In this paper, we ask a fundamental question: can we learn such detectors from scratch? Since it is often unclear what points are \"interesting\", human labelling cannot be used to find a truly unbiased solution. Therefore, the task requires an unsupervised formulation. We are the first to propose such a formulation: training a neural network to rank points in a transformation-invariant manner. Interest points are then extracted from the top/bottom quantiles of this ranking. We validate our approach on two tasks: standard RGB image interest point detection and challenging cross-modal interest point detection between RGB and depth images. We quantitatively show that our unsupervised method performs better or on-par with baselines.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Savinov_Quad-Networks_Unsupervised_Learning_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science at ETH Zurich; Toshiba Corporation; Department of Computer Science at ETH Zurich; Department of Computer Science at ETH Zurich; Department of Computer Science at ETH Zurich + Microsoft",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2789519,
        "gs_citation": 230,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8978347724364149294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "inf.ethz.ch;toshiba.co.jp;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;toshiba.co.jp;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Savinov_Quad-Networks_Unsupervised_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0;0+2",
        "aff_unique_norm": "ETH Zurich;Toshiba Corporation;Microsoft",
        "aff_unique_dep": "Department of Computer Science;;Microsoft Corporation",
        "aff_unique_url": "https://www.ethz.ch;https://www.toshiba.co.jp;https://www.microsoft.com",
        "aff_unique_abbr": "ETHZ;Toshiba;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0+2",
        "aff_country_unique": "Switzerland;Japan;United States"
    },
    {
        "title": "Quality Aware Network for Set to Set Recognition",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2493",
        "author_site": "Yu Liu, Junjie Yan, Wanli Ouyang",
        "author": "Yu Liu; Junjie Yan; Wanli Ouyang",
        "abstract": "This paper targets on the problem of set to set recognition, which learns the metric between two image sets.  Images in each set belong to the same identity.  Since images in a set can be complementary, they hopefully lead to higher accuracy in practical applications. However, the quality of each sample cannot be guaranteed, and samples with poor quality will hurt the metric. In this paper, the quality aware network (QAN) is proposed to confront this problem, where the quality of each sample can be automatically learned although such information is not explicitly provided in the training stage. The network has two branches, where the first branch extracts appearance feature embedding for each sample and the other branch predicts quality score for each sample. Features and quality scores of all samples in a set are then aggregated to generate the final feature embedding. We show that the two branches can be trained in an end-to-end manner given only the set-level identity annotation. Analysis on gradient spread of this mechanism indicates that the quality learned by the network is beneficial to set-to-set recognition and simplifies the distribution that the network needs to fit. Experiments on both face verification and person re-identification show advantages of the proposed QAN. The source code and network structure can be downloaded at GitHub.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Quality_Aware_Network_CVPR_2017_paper.pdf",
        "aff": "SenseTime Group Limited; SenseTime Group Limited; University of Sydney",
        "project": "",
        "github": "github.com/sciencefans/Quality-Aware-Network",
        "supp": "",
        "arxiv": "1704.03373v1",
        "pdf_size": 1361428,
        "gs_citation": 396,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9870426671476481464&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gmail.com;sensetime.com;gmail.com",
        "email": "gmail.com;sensetime.com;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Quality_Aware_Network_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "SenseTime Group Limited;University of Sydney",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sensetime.com;https://www.sydney.edu.au",
        "aff_unique_abbr": "SenseTime;USYD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Query-Focused Video Summarization: Dataset, Evaluation, and a Memory Network Based Approach",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1990",
        "author_site": "Aidean Sharghi, Jacob S. Laurel, Boqing Gong",
        "author": "Aidean Sharghi; Jacob S. Laurel; Boqing Gong",
        "abstract": "Recent years have witnessed a resurgence of interest in video summarization. However, one of the main obstacles to the research on video summarization is the user subjectivity --- users have various preferences over the summaries. The subjectiveness causes at least two problems. First, no single video summarizer fits all users unless it interacts with and adapts to the individual users. Second, it is very challenging to evaluate the performance of a video summarizer.   To tackle the first problem, we explore the recently proposed query-focused video summarization which  introduces user preferences in the form of text queries about the video into the summarization process. We propose a memory network parameterized sequential determinantal point process in order to attend the user query onto different video frames and shots. To address the second challenge, we contend that a good evaluation metric for video summarization should focus on the semantic information that humans can perceive rather than the visual features or temporal overlaps. To this end, we collect dense per-video-shot concept annotations, compile a new dataset, and suggest an efficient  evaluation method defined upon the  concept annotations. We conduct extensive experiments contrasting our video summarizer to existing ones and present detailed analyses about the dataset and the new evaluation method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sharghi_Query-Focused_Video_Summarization_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Sharghi_Query-Focused_Video_Summarization_2017_CVPR_supplemental.pdf",
        "arxiv": "1707.04960",
        "gs_citation": 164,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6255883079853596917&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sharghi_Query-Focused_Video_Summarization_CVPR_2017_paper.html"
    },
    {
        "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2790",
        "author_site": "Jo\u00c3\u00a3o Carreira, Andrew Zisserman",
        "author": "Joao Carreira; Andrew Zisserman",
        "abstract": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2% on HMDB-51 and 97.9% on UCF-101.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Carreira_Quo_Vadis_Action_CVPR_2017_paper.pdf",
        "aff": "DeepMind; DeepMind + Department of Engineering Science, University of Oxford",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.07750v3",
        "pdf_size": 1112797,
        "gs_citation": 10976,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9581219496538221166&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "google.com;google.com",
        "email": "google.com;google.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Carreira_Quo_Vadis_Action_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "DeepMind;University of Oxford",
        "aff_unique_dep": ";Department of Engineering Science",
        "aff_unique_url": "https://deepmind.com;https://www.ox.ac.uk",
        "aff_unique_abbr": "DeepMind;Oxford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Oxford",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "ROAM: A Rich Object Appearance Model With Application to Rotoscoping",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1953",
        "author_site": "Ondrej Miksik, Juan-Manuel P\u00c3\u00a9rez-R\u00c3\u00baa, Philip H. S. Torr, Patrick P\u00c3\u00a9rez",
        "author": "Ondrej Miksik; Juan-Manuel Perez-Rua; Philip H. S. Torr; Patrick Perez",
        "abstract": "Rotoscoping, the detailed delineation of scene elements through a video shot, is a painstaking task of tremendous importance in professional post-production pipelines. While pixel-wise segmentation techniques can help for this task, professional rotoscoping tools rely on parametric curves that offer the artists a much better interactive control on the definition, editing and manipulation of the segments of interest. Sticking to this prevalent rotoscoping paradigm, we propose a novel framework to capture and track the visual aspect of an arbitrary object in a scene, given a first closed outline of this object. This model combines a collection of local foreground/background appearance models spread along the outline, a global appearance model of the enclosed object and a set of distinctive foreground landmarks. The structure of this rich appearance model allows simple initialization, efficient iterative optimization with exact minimization at each step, and on-line adaptation in videos. We demonstrate qualitatively and quantitatively the merit of this framework through comparisons with tools based on either dynamic segmentation with a closed curve or pixel-wise binary labelling.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Miksik_ROAM_A_Rich_CVPR_2017_paper.pdf",
        "aff": "University of Oxford; Technicolor Research & Innovation + Inria (Centre Rennes - Bretagne Atlantique, France); University of Oxford; Technicolor Research & Innovation",
        "project": "http://www.miksik.co.uk",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Miksik_ROAM_A_Rich_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.01495",
        "pdf_size": 1121504,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12701756760548711676&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Miksik_ROAM_A_Rich_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+2;0;1",
        "aff_unique_norm": "University of Oxford;Technicolor;INRIA",
        "aff_unique_dep": ";Research & Innovation;",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.technicolor.com/en;https://www.inria.fr",
        "aff_unique_abbr": "Oxford;Technicolor;Inria",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Rennes - Bretagne Atlantique",
        "aff_country_unique_index": "0;1+1;0;1",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "title": "RON: Reverse Connection With Objectness Prior Networks for Object Detection",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2562",
        "author_site": "Tao Kong, Fuchun Sun, Anbang Yao, Huaping Liu, Ming Lu, Yurong Chen",
        "author": "Tao Kong; Fuchun Sun; Anbang Yao; Huaping Liu; Ming Lu; Yurong Chen",
        "abstract": "We present RON, an efficient and effective framework for generic object detection. Our motivation is to smartly associate the best of the region-based (e.g., Faster R-CNN) and region-free (e.g., SSD) methodologies. Under fully convolutional architecture, RON mainly focuses on two fundamental problems: (a) multi-scale object localization and (b) negative sample mining. To address (a), we design the reverse connection, which enables the network to detect objects on multi-levels of CNNs. To deal with (b), we propose the objectness prior to significantly reduce the searching space of objects. We optimize the reverse connection, objectness prior and object detector jointly by a multi-task loss function, thus RON can directly predict final detection results from all locations of various feature maps. Extensive experiments on the challenging PASCAL VOC 2007, PASCAL VOC 2012 and MS COCO benchmarks demonstrate the competitive performance of RON. Specifically, with VGG-16 and low resolution 384*384 input size, the network gets 81.3% mAP on PASCAL VOC 2007, 80.7% mAP on PASCAL VOC 2012 datasets. Its superiority increases when datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. With 1.5G GPU memory at test phase, the speed of the network is 15 FPS, 3 times faster than the Faster R-CNN counterpart. Code will be made publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kong_RON_Reverse_Connection_CVPR_2017_paper.pdf",
        "aff": "State Key Lab. of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Computer Science and Technology, Tsinghua University; State Key Lab. of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Computer Science and Technology, Tsinghua University; Intel Labs China; State Key Lab. of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Department of Computer Science and Technology, Tsinghua University; Department of Electronic Engineering, Tsinghua University; Intel Labs China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1707.01691v1",
        "pdf_size": 2136262,
        "gs_citation": 539,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14409309550915943732&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;intel.com;intel.com;mails.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;intel.com;intel.com;mails.tsinghua.edu.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kong_RON_Reverse_Connection_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0;0;1",
        "aff_unique_norm": "Tsinghua University;Intel",
        "aff_unique_dep": "Department of Computer Science and Technology;Intel Labs",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.intel.cn",
        "aff_unique_abbr": "Tsinghua;Intel",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Radiometric Calibration From Faces in Images",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1149",
        "author_site": "Chen Li, Stephen Lin, Kun Zhou, Katsushi Ikeuchi",
        "author": "Chen Li; Stephen Lin; Kun Zhou; Katsushi Ikeuchi",
        "abstract": "We present a method for radiometric calibration of cameras from a single image that contains a human face. This technique takes advantage of a low-rank property that exists among certain skin albedo gradients because of the pigments within the skin. This property becomes distorted in images that are captured with a non-linear camera response function, and we perform radiometric calibration by solving for the inverse response function that best restores this low-rank property in an image. Although this work makes use of the color properties of skin pigments, we show that this calibration is unaffected by the color of scene illumination or the sensitivities of the camera's color filters. Our experiments validate this approach on a variety of images containing human faces, and show that faces can provide an important source of calibration data in images where existing radiometric calibration techniques perform poorly.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Radiometric_Calibration_From_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Radiometric_Calibration_From_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3232986,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3114513386728650282&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Radiometric_Calibration_From_CVPR_2017_paper.html"
    },
    {
        "title": "Radiometric Calibration for Internet Photo Collections",
        "session": "Low- & Mid-Level Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "1969",
        "author_site": "Zhipeng Mo, Boxin Shi, Sai-Kit Yeung, Yasuyuki Matsushita",
        "author": "Zhipeng Mo; Boxin Shi; Sai-Kit Yeung; Yasuyuki Matsushita",
        "abstract": "Radiometrically calibrating the images from Internet photo collections brings photometric analysis from lab data to big image data in the wild, but conventional calibration methods cannot be directly applied to such image data. This paper presents a method to jointly perform radiometric calibration for a set of images in an Internet photo collection. By incorporating the consistency of scene reflectance for corresponding pixels in multiple images, the proposed method estimates radiometric response functions of all the images using a rank minimization framework. Our calibration aligns all response functions in an image set up to the same exponential ambiguity in a robust manner. Quantitative results using both synthetic and real data show the effectiveness of the proposed method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Mo_Radiometric_Calibration_for_CVPR_2017_paper.pdf",
        "aff": "Singapore University of Technology and Design; Artificial Intelligence Research Center, National Institute of AIST; Singapore University of Technology and Design; Artificial Intelligence Research Center, National Institute of AIST + Osaka University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3585585,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12341267791364722250&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "sutd.edu.sg;aist.go.jp;sutd.edu.sg;osaka-u.ac.jp",
        "email": "sutd.edu.sg;aist.go.jp;sutd.edu.sg;osaka-u.ac.jp",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Mo_Radiometric_Calibration_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1+2",
        "aff_unique_norm": "Singapore University of Technology and Design;National Institute of Advanced Industrial Science and Technology;Osaka University",
        "aff_unique_dep": ";Artificial Intelligence Research Center;",
        "aff_unique_url": "https://www.sutd.edu.sg;https://www.aist.go.jp;https://www.osaka-u.ac.jp",
        "aff_unique_abbr": "SUTD;AIST;Osaka U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1+1",
        "aff_country_unique": "Singapore;Japan"
    },
    {
        "title": "Re-Ranking Person Re-Identification With k-Reciprocal Encoding",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "477",
        "author_site": "Zhun Zhong, Liang Zheng, Donglin Cao, Shaozi Li",
        "author": "Zhun Zhong; Liang Zheng; Donglin Cao; Shaozi Li",
        "abstract": "When considering person re-identification (re-ID) as a retrieval process, re-ranking is a critical step to improve its accuracy. Yet in the re-ID community, limited effort has been devoted to re-ranking, especially those fully automatic, unsupervised solutions. In this paper, we propose a k-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is that if a gallery image is similar to the probe in the k-reciprocal nearest neighbors, it is more likely to be a true match. Specifically, given an image, a k-reciprocal feature is calculated by encoding its k-reciprocal nearest neighbors into a single vector, which is used for re-ranking under the Jaccard distance. The final distance is computed as the combination of the original distance and the Jaccard distance. Our re-ranking method does not require any human interaction or any labeled data, so it is applicable to large-scale datasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW datasets confirm the effectiveness of our method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.08398",
        "gs_citation": 1916,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5132863931120457431&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.html"
    },
    {
        "title": "Re-Sign: Re-Aligned End-To-End Sequence Modelling With Deep Recurrent CNN-HMMs",
        "session": "Analyzing Humans 2",
        "status": "Oral",
        "track": "main",
        "pid": "1734",
        "author_site": "Oscar Koller, Sepehr Zargaran, Hermann Ney",
        "author": "Oscar Koller; Sepehr Zargaran; Hermann Ney",
        "abstract": "This work presents an iterative re-alignment approach applicable to visual sequence labelling tasks such as gesture recognition, activity recognition and continuous sign language recognition.  Previous methods dealing with video data usually rely on given frame labels to train their classifiers. However, looking at recent data sets, these labels often tend to be noisy which is commonly overseen.  We propose an algorithm that treats the provided training labels as weak labels and refines the label-to-image alignment on-the-fly in a weakly supervised fashion.  Given a series of frames and sequence-level labels,  a deep recurrent CNN-BLSTM network is trained end-to-end. Embedded into an HMM the resulting deep model corrects the frame labels and continuously improves its performance in several re-alignments.   We evaluate on two challenging publicly available sign recognition benchmark data sets featuring over 1000 classes. We outperform the state-of-the-art by up to 10% absolute and 30% relative.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Koller_Re-Sign_Re-Aligned_End-To-End_CVPR_2017_paper.pdf",
        "aff": "Human Language Technology & Pattern Recognition Group, RWTH Aachen University, Germany; Human Language Technology & Pattern Recognition Group, RWTH Aachen University, Germany; Human Language Technology & Pattern Recognition Group, RWTH Aachen University, Germany",
        "project": "http://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 598082,
        "gs_citation": 336,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6948645759915763728&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.rwth-aachen.de; ;cs.rwth-aachen.de",
        "email": "cs.rwth-aachen.de; ;cs.rwth-aachen.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Koller_Re-Sign_Re-Aligned_End-To-End_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "RWTH Aachen University",
        "aff_unique_dep": "Human Language Technology & Pattern Recognition Group",
        "aff_unique_url": "https://www.rwth-aachen.de",
        "aff_unique_abbr": "RWTH",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Aachen",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Real-Time 3D Model Tracking in Color and Depth on a Single CPU Core",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "237",
        "author_site": "Wadim Kehl, Federico Tombari, Slobodan Ilic, Nassir Navab",
        "author": "Wadim Kehl; Federico Tombari; Slobodan Ilic; Nassir Navab",
        "abstract": "We present a novel method to track 3D models in color and depth data. To this end, we introduce approximations that accelerate the state-of-the-art in region-based tracking by an order of magnitude while retaining similar accuracy. Furthermore, we show how the method can be made more robust in the presence of depth data and consequently formulate a new joint contour and ICP tracking energy. We present better results than the state-of-the-art while being much faster then most other methods and achieving all of the above on a single CPU core.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kehl_Real-Time_3D_Model_CVPR_2017_paper.pdf",
        "aff": "Toyota Research Institute, Los Altos+TUM, Munich; TUM, Munich; TUM, Munich+Siemens R&D, Munich; TUM, Munich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1911.10249v1",
        "pdf_size": 1831635,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3971796825969634378&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tri.global; ; ; ",
        "email": "tri.global; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kehl_Real-Time_3D_Model_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;1+2;1",
        "aff_unique_norm": "Toyota Research Institute;Technical University of Munich;Siemens AG",
        "aff_unique_dep": ";;Research and Development",
        "aff_unique_url": "https://www.tri.global;https://www.tum.de;https://www.siemens.com",
        "aff_unique_abbr": "TRI;TUM;Siemens",
        "aff_campus_unique_index": "0+1;1;1+1;1",
        "aff_campus_unique": "Los Altos;Munich",
        "aff_country_unique_index": "0+1;1;1+1;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "Real-Time Neural Style Transfer for Videos",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "252",
        "author_site": "Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wenhao Jiang, Xiaolong Zhu, Zhifeng Li, Wei Liu",
        "author": "Haozhi Huang; Hao Wang; Wenhan Luo; Lin Ma; Wenhao Jiang; Xiaolong Zhu; Zhifeng Li; Wei Liu",
        "abstract": "Recent research endeavors have shown the potential of using feed-forward convolutional neural networks to accomplish fast style transfer for images. In this work, we take one step further to explore the possibility of exploiting a feed-forward network to perform style transfer for videos and simultaneously maintain temporal consistency among stylized video frames. Our feed-forward network is trained by enforcing the outputs of consecutive frames to be both well stylized and temporally consistent. More specifically, a hybrid loss is proposed to capitalize on the content information of input frames, the style information of a given style image, and the temporal information of consecutive frames. To calculate the temporal loss during the training stage, a novel two-frame synergic training mechanism is proposed. Compared with directly applying an existing image style transfer method to videos, our proposed method employs the trained network to yield temporally consistent stylized videos which are much more visually pleasant. In contrast to the prior video style transfer method which relies on time-consuming optimization on the fly, our method runs in real time while generating competitive visual results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Real-Time_Neural_Style_CVPR_2017_paper.pdf",
        "aff": "Tsinghua University\u2021Tencent AI Lab; Tsinghua University\u2021Tencent AI Lab; Tsinghua University\u2021Tencent AI Lab; Tsinghua University\u2021Tencent AI Lab; Tsinghua University\u2021Tencent AI Lab; Tsinghua University\u2021Tencent AI Lab; Tsinghua University\u2021Tencent AI Lab; Tsinghua University\u2021Tencent AI Lab",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Huang_Real-Time_Neural_Style_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 3056610,
        "gs_citation": 287,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18286186187040741967&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gmail.com; ; ; ; ; ;ee.columbia.edu; ",
        "email": "gmail.com; ; ; ; ; ;ee.columbia.edu; ",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Real-Time_Neural_Style_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Real-Time Video Super-Resolution With Spatio-Temporal Networks and Motion Compensation",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1989",
        "author_site": "Jose Caballero, Christian Ledig, Andrew Aitken, Alejandro Acosta, Johannes Totz, Zehan Wang, Wenzhe Shi",
        "author": "Jose Caballero; Christian Ledig; Andrew Aitken; Alejandro Acosta; Johannes Totz; Zehan Wang; Wenzhe Shi",
        "abstract": "Convolutional neural networks have enabled accurate image super-resolution in real-time. However, recent attempts to benefit from temporal correlations in video super-resolution have been limited to naive or inefficient architectures. In this paper, we introduce spatio-temporal sub-pixel convolution networks that effectively exploit temporal redundancies and improve reconstruction accuracy while maintaining real-time speed. Specifically, we discuss the use of early fusion, slow fusion and 3D convolutions for the joint processing of multiple consecutive video frames. We also propose a novel joint motion compensation and video super-resolution algorithm that is orders of magnitude more efficient than competing methods, relying on a fast multi-resolution spatial transformer module that is end-to-end trainable. These contributions provide both higher accuracy and temporally more consistent videos, which we confirm qualitatively and quantitatively. Relative to single-frame models, spatio-temporal networks can either reduce the computational cost by 30% whilst maintaining the same quality or provide a 0.2dB gain for a similar computational cost. Results on publicly available datasets demonstrate that the proposed algorithms surpass current state-of-the-art performance in both accuracy and efficiency.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Caballero_Real-Time_Video_Super-Resolution_CVPR_2017_paper.pdf",
        "aff": "Twitter; Twitter; Twitter; Twitter; Twitter; Twitter; Twitter",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05250v2",
        "pdf_size": 2946950,
        "gs_citation": 888,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6978869732496111774&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "twitter.com;twitter.com;twitter.com;twitter.com;twitter.com;twitter.com;twitter.com",
        "email": "twitter.com;twitter.com;twitter.com;twitter.com;twitter.com;twitter.com;twitter.com",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Caballero_Real-Time_Video_Super-Resolution_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Twitter, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://twitter.com",
        "aff_unique_abbr": "Twitter",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields",
        "session": "Analyzing Humans 1",
        "status": "Oral",
        "track": "main",
        "pid": "3550",
        "author_site": "Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh",
        "author": "Zhe Cao; Tomas Simon; Shih-En Wei; Yaser Sheikh",
        "abstract": "We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.pdf",
        "aff": "The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",
        "project": "https://youtu.be/pW6nZXeWlGM",
        "github": "",
        "supp": "",
        "arxiv": "1611.08050",
        "pdf_size": 4501440,
        "gs_citation": 9379,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14150631212008688559&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cmu.edu;cs.cmu.edu;cmu.edu;cs.cmu.edu",
        "email": "cmu.edu;cs.cmu.edu;cmu.edu;cs.cmu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "The Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Reconstructing Transient Images From Single-Photon Sensors",
        "session": "Computational Photography",
        "status": "Spotlight",
        "track": "main",
        "pid": "571",
        "author_site": "Matthew O'Toole, Felix Heide, David B. Lindell, Kai Zang, Steven Diamond, Gordon Wetzstein",
        "author": "Matthew O'Toole; Felix Heide; David B. Lindell; Kai Zang; Steven Diamond; Gordon Wetzstein",
        "abstract": "Computer vision algorithms build on 2D images or 3D videos that capture dynamic events at the millisecond time scale. However, capturing and analyzing \"transient images\" at the picosecond scale---i.e., at one trillion frames per second---reveals unprecedented information about a scene and light transport within. This is not only crucial for time-of-flight range imaging, but it also helps further our understanding of light transport phenomena at a more fundamental level and potentially allows to revisit many assumptions made in different computer vision algorithms.  In this work, we design and evaluate an imaging system that builds on single photon avalanche diode (SPAD) sensors to capture multi-path responses with picosecond-scale active illumination. We develop inverse methods that use modern approaches to deconvolve and denoise measurements in the presence of Poisson noise, and compute transient images at a higher quality than previously reported. The small form factor, fast acquisition rates, and relatively low cost of our system potentially makes transient imaging more practical for a range of applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/OToole_Reconstructing_Transient_Images_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/OToole_Reconstructing_Transient_Images_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 848950,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15845021332101349518&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/OToole_Reconstructing_Transient_Images_CVPR_2017_paper.html"
    },
    {
        "title": "Recurrent 3D Pose Sequence Machines",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Oral",
        "track": "main",
        "pid": "263",
        "author_site": "Mude Lin, Liang Lin, Xiaodan Liang, Keze Wang, Hui Cheng",
        "author": "Mude Lin; Liang Lin; Xiaodan Liang; Keze Wang; Hui Cheng",
        "abstract": "3D human articulated pose recovery from monocular image sequences is very challenging due to the diverse appearances, viewpoints, occlusions, and also the human 3D pose is inherently ambiguous from the monocular imagery. It is thus critical to exploit rich spatial and temporal long-range dependencies among body joints for accurate 3D pose sequence prediction. Existing approaches usually manually design some elaborate prior terms and human body kinematic constraints for capturing structures, which are often insufficient to exploit all intrinsic structures and not scalable for all scenarios. In contrast, this paper presents a Recurrent 3D Pose Sequence Machine(RPSM) to automatically learn the image-dependent structural constraint and sequence-dependent temporal context by using a multi-stage sequential refinement. At each stage, our RPSM is composed of three modules to predict the 3D pose sequences based on the previously learned 2D pose representations and 3D poses: (i) a 2D pose module extracting the image-dependent pose representations, (ii) a 3D pose recurrent module regressing 3D poses and (iii) a feature adaption module serving as a bridge between module (i) and (ii) to enable the representation transformation from 2D to 3D domain. These three modules are then assembled into a sequential prediction framework to refine the predicted poses with multiple recurrent stages. Extensive evaluations on the Human3.6M dataset and HumanEva-I dataset show that our RPSM outperforms all state-of-the-art approaches for 3D pose estimation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Recurrent_3D_Pose_CVPR_2017_paper.pdf",
        "aff": "School of Data and Computer Science, Sun Yat-sen University; School of Data and Computer Science, Sun Yat-sen University; School of Data and Computer Science, Sun Yat-sen University; School of Data and Computer Science, Sun Yat-sen University; School of Data and Computer Science, Sun Yat-sen University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1707.09695v1",
        "pdf_size": 1028859,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13769547602229866209&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "foxmail.com;ieee.org;cs.cmu.edu;mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "email": "foxmail.com;ieee.org;cs.cmu.edu;mail2.sysu.edu.cn;mail.sysu.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Recurrent_3D_Pose_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Sun Yat-sen University",
        "aff_unique_dep": "School of Data and Computer Science",
        "aff_unique_url": "http://www.sysu.edu.cn/",
        "aff_unique_abbr": "SYSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Recurrent Convolutional Neural Networks for Continuous Sign Language Recognition by Staged Optimization",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "3624",
        "author_site": "Runpeng Cui, Hu Liu, Changshui Zhang",
        "author": "Runpeng Cui; Hu Liu; Changshui Zhang",
        "abstract": "This work presents a weakly supervised framework with deep neural networks for vision-based continuous sign language recognition, where the ordered gloss labels but no exact temporal locations are available with the video of sign sentence, and the amount of labeled sentences for training is limited. Our approach addresses the mapping of video segments to glosses by introducing recurrent convolutional neural network for spatio-temporal feature extraction and sequence learning. We design a three-stage optimization process for our architecture. First, we develop an end-to-end sequence learning scheme and employ connectionist temporal classification (CTC) as the objective function for alignment proposal. Second, we take the alignment proposal as stronger supervision to tune our feature extractor. Finally, we optimize the sequence learning model with the improved feature representations, and design a weakly supervised detection network for regularization. We apply the proposed approach to a real-world continuous sign language recognition benchmark, and our method, with no extra supervision, achieves results comparable to the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cui_Recurrent_Convolutional_Neural_CVPR_2017_paper.pdf",
        "aff": "Department of Automation, Tsinghua University; Department of Automation, Tsinghua University; Department of Automation, Tsinghua University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1310018,
        "gs_citation": 422,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15572081776573199004&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mail.tsinghua.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cui_Recurrent_Convolutional_Neural_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Automation",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Recurrent Modeling of Interaction Context for Collective Activity Recognition",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1126",
        "author_site": "Minsi Wang, Bingbing Ni, Xiaokang Yang",
        "author": "Minsi Wang; Bingbing Ni; Xiaokang Yang",
        "abstract": "Modeling of high order interactional context, e.g., group interaction, lies in the central of collective/group activity recognition. However, most of the previous activity recognition  methods do not offer a flexible and  scalable scheme to handle the high order context modeling problem. To explicitly address this fundamental bottleneck, we propose a recurrent interactional context modeling scheme based on LSTM network. By utilizing the information propagation/aggregation capability of LSTM, the proposed scheme unifies the interactional feature modeling process for single person dynamics, intra-group (e.g., persons within a group) and inter-group(e.g., group to group)interactions. The proposed high order context modeling scheme produces more discriminative/descriptive interactional features. It is very flexible to handle a varying number of input instances (e.g., different number of persons in a group or different number of groups) and linearly scalable to high order context modeling problem. Extensive experiments on two benchmark collective/group activity datasets demonstrate the effectiveness of the proposed method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Recurrent_Modeling_of_CVPR_2017_paper.pdf",
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1014358,
        "gs_citation": 183,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13020606612089785774&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;sjtu.edu.cn;sjtu.edu.cn",
        "email": "gmail.com;sjtu.edu.cn;sjtu.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Recurrent_Modeling_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "707",
        "author_site": "Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid",
        "author": "Guosheng Lin; Anton Milan; Chunhua Shen; Ian Reid",
        "abstract": "Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.pdf",
        "aff": "Nanyang Technological University; University of Adelaide; University of Adelaide + Australian Centre for Robotic Vision; University of Adelaide + Australian Centre for Robotic Vision",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.06612v3",
        "pdf_size": 1037764,
        "gs_citation": 4039,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9682999547844662749&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ntu.edu.sg;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "ntu.edu.sg;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1+2;1+2",
        "aff_unique_norm": "Nanyang Technological University;University of Adelaide;Australian Centre for Robotic Vision",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.adelaide.edu.au;https://roboticvision.org/",
        "aff_unique_abbr": "NTU;Adelaide;ACRV",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1+1;1+1",
        "aff_country_unique": "Singapore;Australia"
    },
    {
        "title": "Reflectance Adaptive Filtering Improves Intrinsic Image Estimation",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3113",
        "author_site": "Thomas Nestmeyer, Peter V. Gehler",
        "author": "Thomas Nestmeyer; Peter V. Gehler",
        "abstract": "Separating an image into reflectance and shading layers poses a challenge for learning approaches because no large corpus of precise and realistic ground truth decompositions exists. The Intrinsic Images in the Wild (IIW) dataset provides a sparse set of relative human reflectance judgments, which serves as a standard benchmark for intrinsic images. A number of methods use IIW to learn statistical dependencies between the images and their reflectance layer. Although learning plays an important role for high performance, we show that a standard signal processing technique achieves performance on par with current state-of-the-art. We propose a loss function for CNN learning of dense reflectance predictions. Our results show a simple pixel-wise decision, without any context or prior knowledge, is sufficient to provide a strong baseline on IIW. This sets a competitive baseline which only two other approaches surpass. We then develop a joint bilateral filtering method that implements strong prior knowledge about reflectance constancy. This filtering operation can be applied to any intrinsic image algorithm and we improve several previous results achieving a new state-of-the-art on IIW. Our findings suggest that the effect of learning-based approaches may have been over-estimated so far. Explicit prior knowledge is still at least as important to obtain high performance in intrinsic image decompositions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Nestmeyer_Reflectance_Adaptive_Filtering_CVPR_2017_paper.pdf",
        "aff": "University of W\u00fcrzburg, Germany+Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Bernstein Center for Computational Neuroscience, T\u00fcbingen, Germany+Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.05062",
        "pdf_size": 1018406,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7122826816705681773&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Nestmeyer_Reflectance_Adaptive_Filtering_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2+1",
        "aff_unique_norm": "University of W\u00fcrzburg;Max Planck Institute for Intelligent Systems;Bernstein Center for Computational Neuroscience",
        "aff_unique_dep": ";;Computational Neuroscience",
        "aff_unique_url": "https://www.uni-wuerzburg.de;https://www.mpi-is.mpg.de;",
        "aff_unique_abbr": "UWue;MPI-IS;",
        "aff_campus_unique_index": "1;1+1",
        "aff_campus_unique": ";T\u00fcbingen",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Reflection Removal Using Low-Rank Matrix Completion",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2302",
        "author_site": "Byeong-Ju Han, Jae-Young Sim",
        "author": "Byeong-Ju Han; Jae-Young Sim",
        "abstract": "The images taken through glass often capture a target transmitted scene as well as undesired reflected scenes. In this paper, we propose a low-rank matrix completion algorithm to remove reflection artifacts automatically from multiple glass images taken at slightly different camera locations. We assume that the transmitted scenes are more dominant than the reflected scenes in typical glass images. We first warp the multiple glass images to a reference image, where the gradients are consistent in the transmission images while the gradients are varying across the reflection images. Based on this observation, we compute a gradient reliability such that the pixels belonging to the salient edges of the transmission image are assigned high reliability. Then we suppress the gradients of the reflection images and recover the gradients of the transmission images only, by solving a low-rank matrix completion problem in gradient domain. We reconstruct an original transmission image using the resulting optimal gradient map. Experimental results show that the proposed algorithm removes the reflection artifacts from glass images faithfully and outperforms the existing algorithms on typical glass images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Han_Reflection_Removal_Using_CVPR_2017_paper.pdf",
        "aff": "School of Electrical Engineering, Ulsan National Institute of Science and Technology, Ulsan, Korea; School of Electrical Engineering, Ulsan National Institute of Science and Technology, Ulsan, Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Han_Reflection_Removal_Using_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1033975,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8849159911371014331&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "unist.ac.kr;unist.ac.kr",
        "email": "unist.ac.kr;unist.ac.kr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Han_Reflection_Removal_Using_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Ulsan National Institute of Science and Technology",
        "aff_unique_dep": "School of Electrical Engineering",
        "aff_unique_url": "https://www.unist.ac.kr",
        "aff_unique_abbr": "UNIST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ulsan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Regressing Robust and Discriminative 3D Morphable Models With a Very Deep Neural Network",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2144",
        "author_site": "Anh Tu\u00e1\u00ba\u00a5n Tr\u00e1\u00ba\u00a7n, Tal Hassner, Iacopo Masi, G\u00c3\u00a9rard Medioni",
        "author": "Anh Tuan Tran; Tal Hassner; Iacopo Masi; Gerard Medioni",
        "abstract": "The 3D shapes of faces are well known to be discriminative. Yet despite this, they are rarely used for face recognition and always under controlled viewing conditions. We claim that this is a symptom of a serious but often overlooked problem with existing methods for single view 3D face reconstruction: when applied \"in the wild\", their 3D estimates are either unstable and change for different photos of the same subject or they are over-regularized and generic. In response, we describe a robust method for regressing discriminative 3D morphable face models (3DMM). We use a convolutional neural network (CNN) to regress 3DMM shape and texture parameters directly from an input photo. We overcome the shortage of training data required for this purpose by offering a method for generating huge numbers of labeled  examples. The 3D estimates produced by our CNN surpass state of the art accuracy on the MICC data set. Coupled with a 3D-3D face matching pipeline, we show the first competitive face recognition results on the LFW, YTF and IJB-A benchmarks using 3D face shapes as representations, rather than the opaque deep feature vectors used by other modern systems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tran_Regressing_Robust_and_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.04904",
        "pdf_size": 3298103,
        "gs_citation": 612,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15965123435012620773&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tran_Regressing_Robust_and_CVPR_2017_paper.html"
    },
    {
        "title": "Relationship Proposal Networks",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2414",
        "author_site": "Ji Zhang, Mohamed Elhoseiny, Scott Cohen, Walter Chang, Ahmed Elgammal",
        "author": "Ji Zhang; Mohamed Elhoseiny; Scott Cohen; Walter Chang; Ahmed Elgammal",
        "abstract": "Image scene understanding requires learning the relationships between objects in the scene. A scene with many objects may have only a few individual interacting objects (e.g., in a party image with many people, only a handful of people might be speaking with each other).  To detect all relationships, it would be inefficient to first detect all individual objects and then classify all pairs; not only is the number of all pairs quadratic, but classification requires limited object categories, which is not scalable for real-world images. In this paper we address these challenges by using pairs of related regions in images to train a relationship proposer that at test time produces a manageable number of related regions. We name our model the Relationship Proposal Network (Rel-PN). Like object proposals, our Rel-PN is class-agnostic and thus scalable to an open vocabulary of objects. We demonstrate the ability of our Rel-PN to localize relationships with only a few thousand proposals.  We demonstrate its performance on the Visual Genome dataset and compare to other baselines that we designed. We also conduct experiments on a smaller subset of 5,000 images with over 37,000 related regions and show promising results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Relationship_Proposal_Networks_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, Rutgers University; Facebook AI Research; Adobe Research; Adobe Research; Department of Computer Science, Rutgers University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1464361,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4320008986797971281&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "rutgers.edu;fb.com;adobe.com;adobe.com;rutgers.edu",
        "email": "rutgers.edu;fb.com;adobe.com;adobe.com;rutgers.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Relationship_Proposal_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;2;0",
        "aff_unique_norm": "Rutgers University;Meta;Adobe",
        "aff_unique_dep": "Department of Computer Science;Facebook AI Research;Adobe Research",
        "aff_unique_url": "https://www.rutgers.edu;https://research.facebook.com;https://research.adobe.com",
        "aff_unique_abbr": "Rutgers;FAIR;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1040",
        "author_site": "Shan Li, Weihong Deng, JunPing Du",
        "author": "Shan Li; Weihong Deng; JunPing Du",
        "abstract": "Past research on facial expressions have used relatively limited datasets, which makes it unclear whether current methods can be employed in real world. In this paper, we present a novel database, RAF-DB, which contains about 30000 facial images from thousands of individuals. Each image has been individually labeled about 40 times, then EM algorithm was used to filter out unreliable labels. Crowdsourcing reveals that real-world faces often express compound emotions, or even mixture ones. For all we know, RAF-DB is the first database that contains compound expressions in the wild. Our cross-database study shows that the action units of basic emotions in RAF-DB are much more diverse than, or even deviate from, those of lab-controlled ones. To address this problem, we propose a new DLP-CNN (Deep Locality-Preserving CNN) method, which aims to enhance the discriminative power of deep features by preserving the locality closeness while maximizing the inter-class scatters. The benchmark experiments on the 7-class basic expressions and 11-class compound expressions, as well as the additional experiments on SFEW and CK+ databases, show that the proposed DLP-CNN outperforms the state-of-the-art handcrafted features and deep learning based methods for the expression recognition in the wild.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Reliable_Crowdsourcing_and_CVPR_2017_paper.pdf",
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications",
        "project": "http://whdeng.cn/RAF/model1.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2655677,
        "gs_citation": 1966,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8987464787690009644&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Reliable_Crowdsourcing_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Removing Rain From Single Images via a Deep Detail Network",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1495",
        "author_site": "Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao Ding, John Paisley",
        "author": "Xueyang Fu; Jiabin Huang; Delu Zeng; Yue Huang; Xinghao Ding; John Paisley",
        "abstract": "We propose a new deep network architecture for removing rain streaks from individual images based on the deep convolutional neural network (CNN). Inspired by the deep residual network (ResNet) that simplifies the learning process by changing the mapping form, we propose a deep detail network to directly reduce the mapping range from input to output, which makes the learning process easier. To further improve the de-rained result, we use a priori image domain knowledge by focusing on high frequency detail during training, which removes background interference and focuses the model on the structure of rain in images. This demonstrates that a deep architecture not only has benefits for high-level vision tasks but also can be used to solve low-level imaging problems. Though we train the network on synthetic data, we find that the learned network generalizes well to real-world test images. Experiments show that the proposed method significantly outperforms state-of-the-art methods on both synthetic and real-world images in terms of both qualitative and quantitative measures. We discuss applications of this structure to denoising and JPEG artifact reduction at the end of the paper.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Removing_Rain_From_CVPR_2017_paper.pdf",
        "aff": "Key Laboratory of Underwater Acoustic Communication and Marine Information Technology, Ministry of Education & School of Information Science and Engineering, Xiamen University, China; Key Laboratory of Underwater Acoustic Communication and Marine Information Technology, Ministry of Education & School of Information Science and Engineering, Xiamen University, China; School of Mathematics, South China University of Technology, China; Key Laboratory of Underwater Acoustic Communication and Marine Information Technology, Ministry of Education & School of Information Science and Engineering, Xiamen University, China; Key Laboratory of Underwater Acoustic Communication and Marine Information Technology, Ministry of Education & School of Information Science and Engineering, Xiamen University, China; Department of Electrical Engineering & Data Science Institute, Columbia University, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2965914,
        "gs_citation": 1377,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7781340610262067559&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "xmu.edu.cn; ; ; ; ; ",
        "email": "xmu.edu.cn; ; ; ; ; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Fu_Removing_Rain_From_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0;0;2",
        "aff_unique_norm": "Xiamen University;South China University of Technology;Columbia University",
        "aff_unique_dep": "School of Information Science and Engineering;School of Mathematics;Department of Electrical Engineering & Data Science Institute",
        "aff_unique_url": "https://www.xmu.edu.cn;https://www.scut.edu.cn/en/;https://www.columbia.edu",
        "aff_unique_abbr": "XMU;SCUT;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Residual Attention Network for Image Classification",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "1163",
        "author_site": "Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang",
        "author": "Fei Wang; Mengqing Jiang; Chen Qian; Shuo Yang; Cheng Li; Honggang Zhang; Xiaogang Wang; Xiaoou Tang",
        "abstract": "In this work, we propose \"Residual Attention Network\", a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers. Extensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and ImageNet (4.8% single model and single crop, top-5 error). Note that, our method achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69% forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Residual_Attention_Network_CVPR_2017_paper.pdf",
        "aff": ";;;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.06904v1",
        "gs_citation": 4712,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9108679738301713484&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Residual_Attention_Network_CVPR_2017_paper.html"
    },
    {
        "title": "Residual Expansion Algorithm: Fast and Effective Optimization for Nonconvex Least Squares Problems",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2506",
        "author_site": "Daiki Ikami, Toshihiko Yamasaki, Kiyoharu Aizawa",
        "author": "Daiki Ikami; Toshihiko Yamasaki; Kiyoharu Aizawa",
        "abstract": "We propose the residual expansion (RE) algorithm: a global (or near-global) optimization method for nonconvex least squares problems. Unlike most existing nonconvex optimization techniques, the RE algorithm is not based on either stochastic or multi-point searches; therefore, it can achieve fast global optimization. Moreover, the RE algorithm is easy to implement and successful in high-dimensional optimization. The RE algorithm exhibits excellent empirical performance in terms of k-means clustering, point-set registration, optimized product quantization, and blind image deblurring.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ikami_Residual_Expansion_Algorithm_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.09549v1",
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12416690926398663293&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ikami_Residual_Expansion_Algorithm_CVPR_2017_paper.html"
    },
    {
        "title": "Revisiting Metric Learning for SPD Matrix Based Visual Representation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1197",
        "author_site": "Luping Zhou, Lei Wang, Jianjia Zhang, Yinghuan Shi, Yang Gao",
        "author": "Luping Zhou; Lei Wang; Jianjia Zhang; Yinghuan Shi; Yang Gao",
        "abstract": "The success of many visual recognition tasks largely depends on a good similarity measure, and distance metric learning plays an important role in this regard. Meanwhile, Symmetric Positive Definite (SPD) matrix is receiving increased attention for feature representation in multiple computer vision applications. However, distance metric learning on SPD matrices has not been sufficiently researched. A few existing works approached this by learning either d^2 xp or d xk transformation matrix for dxd SPD matrices. Different from these methods, this paper proposes a new member to the family of distance metric learning for SPD matrices. It learns only d parameters to adjust the eigenvalues of the SPD matrices through an efficient optimisation scheme. Also, it is shown that the proposed method can be interpreted as learning a sample-specific transformation matrix, instead of the fixed transformation matrix learned for all the samples in the existing works. The optimised d parameters can be used to \"massage\" the SPD matrices for better discrimination while still keeping them in the original space. From this perspective, the proposed method complements, rather than competes with, the existing linear-transformation-based methods, as the latter can always be applied to the output of the former to perform distance metric learning in further. The proposed method has been tested on multiple SPD-based visual representation data sets used in the literature, and the results demonstrate its interesting properties and attractive performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Revisiting_Metric_Learning_CVPR_2017_paper.pdf",
        "aff": "University of Wollongong, Australia; University of Wollongong, Australia; CSIRO, Australia; Nanjing University, China; Nanjing University, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhou_Revisiting_Metric_Learning_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 566665,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6138187743907780838&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "uow.edu.au;uow.edu.au;gmail.com;nju.edu.cn;nju.edu.cn",
        "email": "uow.edu.au;uow.edu.au;gmail.com;nju.edu.cn;nju.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Revisiting_Metric_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;2;2",
        "aff_unique_norm": "University of Wollongong;Commonwealth Scientific and Industrial Research Organisation;Nanjing University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uow.edu.au;https://www.csiro.au;http://www.nju.edu.cn",
        "aff_unique_abbr": "UOW;CSIRO;Nanjing U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;1",
        "aff_country_unique": "Australia;China"
    },
    {
        "title": "Revisiting the Variable Projection Method for Separable Nonlinear Least Squares Problems",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "52",
        "author_site": "Je Hyeong Hong, Christopher Zach, Andrew Fitzgibbon",
        "author": "Je Hyeong Hong; Christopher Zach; Andrew Fitzgibbon",
        "abstract": "Variable Projection (VarPro) is a framework to solve optimization problems efficiently by optimally eliminating a subset of the unknowns. It is in particular adapted for Separable Nonlinear Least Squares (SNLS) problems, a class of optimization problems including low-rank matrix factorization with missing data and affine bundle adjustment as instances. VarPro-based methods have received much attention over the last decade due to the experimentally observed large convergence basin for certain problem classes, where they have a clear advantage over standard methods based on Joint optimization over all unknowns. Yet no clear answers have been found in the literature as to why VarPro outperforms others and why Joint optimization, which has been successful in solving many computer vision tasks, fails on this type of problems. Also, the fact that VarPro has been mainly tested on small to medium-sized datasets has raised questions about its scalability. This paper intends to address these unsolved puzzles.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hong_Revisiting_the_Variable_CVPR_2017_paper.pdf",
        "aff": "University of Cambridge; Toshiba Research Europe; Microsoft",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 885782,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3561719332284119701&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "cam.ac.uk;gmail.com;microsoft.com",
        "email": "cam.ac.uk;gmail.com;microsoft.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hong_Revisiting_the_Variable_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Cambridge;Toshiba Research Europe;Microsoft",
        "aff_unique_dep": ";;Microsoft Corporation",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.toshiba-research.eu;https://www.microsoft.com",
        "aff_unique_abbr": "Cambridge;TRE;Microsoft",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Richer Convolutional Features for Edge Detection",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1104",
        "author_site": "Yun Liu, Ming-Ming Cheng, Xiaowei Hu, Kai Wang, Xiang Bai",
        "author": "Yun Liu; Ming-Ming Cheng; Xiaowei Hu; Kai Wang; Xiang Bai",
        "abstract": "In this paper, we propose an accurate edge detector using richer convolutional features (RCF). Since objects in natural images possess various scales and aspect ratios, learning the rich hierarchical representations is very critical for edge detection. CNNs have been proved to be effective for this task. In addition, the convolutional features in CNNs gradually become coarser with the increase of the receptive fields. According to these observations, we attempt to adopt richer convolutional features in such a challenging vision task. The proposed network fully exploits multiscale and multilevel information of objects to perform the image-to-image prediction by combining all the meaningful convolutional features in a holistic manner. Using VGG16 network, we achieve state-of-the-art performance on several available datasets. When evaluating on the well-known BSDS500 benchmark, we achieve ODS F-measure of 0.811 while retaining a fast speed (8 FPS). Besides, our fast version of RCF achieves ODS F-measure of 0.806 with 30 FPS.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Richer_Convolutional_Features_CVPR_2017_paper.pdf",
        "aff": "Nankai University; Nankai University; Nankai University; Nankai University; HUST",
        "project": "https://mmcheng.net/rcfEdge/",
        "github": "",
        "supp": "",
        "arxiv": "1612.02103v3",
        "pdf_size": 1962456,
        "gs_citation": 1433,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16811934375694575588&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "nankai.edu.cn; ; ; ; ",
        "email": "nankai.edu.cn; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Richer_Convolutional_Features_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Nankai University;Huazhong University of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.nankai.edu.cn;http://www.hust.edu.cn",
        "aff_unique_abbr": "NKU;HUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Riemannian Nonlinear Mixed Effects Models: Analyzing Longitudinal Deformations in Neuroimaging",
        "session": "Biomedical Image/Video Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "918",
        "author_site": "Hyunwoo J. Kim, Nagesh Adluru, Heemanshu Suri, Baba C. Vemuri, Sterling C. Johnson, Vikas Singh",
        "author": "Hyunwoo J. Kim; Nagesh Adluru; Heemanshu Suri; Baba C. Vemuri; Sterling C. Johnson; Vikas Singh",
        "abstract": "Statistical machine learning models that operate on manifold-valued data are being extensively studied in vision, motivated by applications in activity recognition, feature tracking and medical imaging. While non-parametric methods have been relatively well studied in the literature, efficient formulations for parametric models (which may offer benefits in small sample size regimes) have only emerged recently. So far, manifold-valued regression models (such as geodesic regression) are restricted to the analysis of cross-sectional data, i.e., the so-called \"fixed effects\" in statistics. But in most \"longitudinal analysis\" (e.g., when a participant provides multiple measurements, over time) the application of fixed effects models is problematic. In an effort to answer this need, this paper generalizes non-linear mixed effects model to the regime where the response variable is manifold-valued, i.e., f:R^d -> M. We derive the underlying model and estimation schemes and demonstrate the immediate benefits such a model can provide --- both for group level and individual level analysis --- on longitudinal brain imaging data. The direct consequence of our results is that longitudinal analysis of manifold-valued measurements (especially, the symmetric positive definite manifold) can be conducted in a computationally tractable manner.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kim_Riemannian_Nonlinear_Mixed_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1888838,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=202219405111454534&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kim_Riemannian_Nonlinear_Mixed_CVPR_2017_paper.html"
    },
    {
        "title": "Robust Energy Minimization for BRDF-Invariant Shape From Light Fields",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2360",
        "author_site": "Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Manmohan Chandraker",
        "author": "Zhengqin Li; Zexiang Xu; Ravi Ramamoorthi; Manmohan Chandraker",
        "abstract": "Highly effective optimization frameworks have been developed for traditional multiview stereo relying on lambertian photoconsistency. However, they do not account for complex material properties. On the other hand, recent works have explored PDE invariants for shape recovery with complex BRDFs, but they have not been incorporated into robust numerical optimization frameworks. We present a variational energy minimization framework for robust recovery of shape in multiview stereo with complex, unknown BRDFs. While our formulation is general, we demonstrate its efficacy on shape recovery using a single light field image, where the microlens array may be considered as a realization of a purely translational multiview stereo setup. Our formulation automatically balances contributions from texture gradients, traditional Lambertian photoconsistency, an appropriate BRDF-invariant PDE and a smoothness prior. Unlike prior works, our energy function inherently handles spatially-varying BRDFs and albedos. Extensive experiments with synthetic and real data show that our optimization framework consistently achieves errors lower than Lambertian baselines and further, is more robust than prior BRDF-invariant reconstruction methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Robust_Energy_Minimization_CVPR_2017_paper.pdf",
        "aff": "University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Robust_Energy_Minimization_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1130588,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4395983511299209573&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "eng.ucsd.edu;eng.ucsd.edu;eng.ucsd.edu;eng.ucsd.edu",
        "email": "eng.ucsd.edu;eng.ucsd.edu;eng.ucsd.edu;eng.ucsd.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Robust_Energy_Minimization_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Robust Interpolation of Correspondences for Large Displacement Optical Flow",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "155",
        "author_site": "Yinlin Hu, Yunsong Li, Rui Song",
        "author": "Yinlin Hu; Yunsong Li; Rui Song",
        "abstract": "The interpolation of correspondences (EpicFlow) was widely used for optical flow estimation in most-recent works. It has the advantage of edge-preserving and efficiency. However, it is vulnerable to input matching noise, which is inevitable in modern matching techniques. In this paper, we present a Robust Interpolation method of Correspondences (called RicFlow) to overcome the weakness. First, the scene is over-segmented into superpixels to revitalize an early idea of piecewise flow model. Then, each model is estimated robustly from its support neighbors based on a graph constructed on superpixels. We propose a propagation mechanism among the pieces in the estimation of models. The propagation of models is significantly more efficient than the independent estimation of each model, yet retains the accuracy. Extensive experiments on three public datasets demonstrate that RicFlow is more robust than EpicFlow, and it outperforms state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hu_Robust_Interpolation_of_CVPR_2017_paper.pdf",
        "aff": "State Key Laboratory of Integrated Service Networks, Xidian University, China; State Key Laboratory of Integrated Service Networks, Xidian University, China; State Key Laboratory of Integrated Service Networks, Xidian University, China + Key Laboratory of Infrared System Detection and Imaging Technology, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2416745,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13718541291892724835&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;mail.xidian.edu.cn;xidian.edu.cn",
        "email": "gmail.com;mail.xidian.edu.cn;xidian.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hu_Robust_Interpolation_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Xidian University;Chinese Academy of Sciences",
        "aff_unique_dep": "State Key Laboratory of Integrated Service Networks;Key Laboratory of Infrared System Detection and Imaging Technology",
        "aff_unique_url": "http://www.xidian.edu.cn/;http://www.cas.cn",
        "aff_unique_abbr": "Xidian;CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Robust Joint and Individual Variance Explained",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2209",
        "author_site": "Christos Sagonas, Yannis Panagakis, Alina Leidinger, Stefanos Zafeiriou",
        "author": "Christos Sagonas; Yannis Panagakis; Alina Leidinger; Stefanos Zafeiriou",
        "abstract": "Discovering the common (joint) and individual subspaces is crucial for analysis of multiple data sets, including multi-view and multi-modal data. Several statistical machine learning methods have been developed for discovering the common features across multiple data sets. The most well studied family of the methods is that of Canonical Correlation Analysis (CCA) and its variants. Even though the CCA is a powerful tool, it has several drawbacks that render its application challenging for computer vision applications. That is, it discovers only common features and not individual ones, and it is sensitive to gross errors present in visual data.   Recently, efforts have been made in order to develop methods that discover individual and common components. Nevertheless, these methods are mainly applicable in two sets of data. In this paper, we investigate the use of a recently proposed statistical  method, the so-called Joint and Individual Variance Explained (JIVE) method, for the recovery of joint and individual components in an arbitrary number of data sets. Since, the JIVE is not robust to gross errors, we propose alternatives, which are both robust to non-Gaussian noise of large magnitude, as well as able to automatically find the rank of the individual components. We demonstrate the effectiveness of the proposed approach to two computer vision applications, namely facial expression synthesis and face age progression in-the-wild.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sagonas_Robust_Joint_and_CVPR_2017_paper.pdf",
        "aff": "Imperial College London, UK + Onfido, UK; Imperial College London, UK + Middlesex University London, UK; Imperial College London, UK; Imperial College London, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3558424,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9640642096663574040&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "onfido.com;imperial.ac.uk;imperial.ac.uk; ",
        "email": "onfido.com;imperial.ac.uk;imperial.ac.uk; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sagonas_Robust_Joint_and_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+2;0;0",
        "aff_unique_norm": "Imperial College London;Onfido;Middlesex University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.onfido.com;https://www.mdx.ac.uk",
        "aff_unique_abbr": "ICL;;Middlesex",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Robust Visual Tracking Using Oblique Random Forests",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "2371",
        "author_site": "Le Zhang, Jagannadan Varadarajan, Ponnuthurai Nagaratnam Suganthan, Narendra Ahuja, Pierre Moulin",
        "author": "Le Zhang; Jagannadan Varadarajan; Ponnuthurai Nagaratnam Suganthan; Narendra Ahuja; Pierre Moulin",
        "abstract": "Random forest has emerged as a powerful classification technique with promising results in various vision tasks including image classification, pose estimation and object detection. However, current techniques have shown little improvements in visual tracking as they mostly rely on piece wise orthogonal hyperplanes to create decision nodes and lack a robust incremental learning mechanism that is much needed for online tracking. In this paper, we propose a discriminative tracker based on a novel incremental oblique random forest. Unlike conventional orthogonal decision trees that use a single feature and heuristic measures to obtain a split at each node, we propose to use a more powerful proximal SVM to obtain oblique hyperplanes to capture the geometric structure of the data better. The resulting decision surface is not restricted to be axis aligned and hence has the ability to represent and classify the input data better. Furthermore, in order to generalize to online tracking scenarios, we derive incremental update steps that enable the hyperplanes in each node to be updated recursively, efficiently and in a closed-form fashion. We demonstrate the effectiveness of our method using two large scale benchmark datasets (OTB-51 and OTB-100) and show that our method gives competitive results on several challenging cases by relying on simple HOG features as well as in combination with more sophisticated deep neural network based models. The implementations of the proposed random forest are available at https://github.com/ZhangLeUestc/ Incremental-Oblique-Random-Forest.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Robust_Visual_Tracking_CVPR_2017_paper.pdf",
        "aff": "Advanced Digital Sciences Center, Singapore; Advanced Digital Sciences Center, Singapore; Nanyang Technology University, Singapore; Advanced Digital Sciences Center, Singapore + University of Illinois at Urbana-Champaign, IL USA; Advanced Digital Sciences Center, Singapore + University of Illinois at Urbana-Champaign, IL USA",
        "project": "",
        "github": "https://github.com/ZhangLeUestc/Incremental-Oblique-Random-Forest",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_Robust_Visual_Tracking_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1548677,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1943350668350893219&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "adsc.com.sg;adsc.com.sg;ntu.edu.sg;illinois.edu;illinois.edu",
        "email": "adsc.com.sg;adsc.com.sg;ntu.edu.sg;illinois.edu;illinois.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Robust_Visual_Tracking_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0+2;0+2",
        "aff_unique_norm": "Advanced Digital Sciences Center;Nanyang Technological University;University of Illinois Urbana-Champaign",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.ntu.edu.sg;https://illinois.edu",
        "aff_unique_abbr": ";NTU;UIUC",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0+1;0+1",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "title": "S2F: Slow-To-Fast Interpolator Flow",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "765",
        "author_site": "Yanchao Yang, Stefano Soatto",
        "author": "Yanchao Yang; Stefano Soatto",
        "abstract": "We introduce a method to compute optical flow at multiple scales of motion, without resorting to multi- resolution or combinatorial methods. It addresses the key problem of small objects moving fast, and resolves the artificial binding between how large an object is and how fast it can move before being diffused away by clas- sical scale-space. Even with no learning, it achieves top performance on the most challenging optical flow benchmark. Moreover, the results are interpretable, and indeed we list the assumptions underlying our method explicitly. The key to our approach is the matching pro- gression from slow to fast, as well as the choice of in- terpolation method, or equivalently the prior, to fill in regions where the data allows it. We use several off- the-shelf components, with relatively low sensitivity to parameter tuning. Computational cost is comparable to the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_S2F_Slow-To-Fast_Interpolator_CVPR_2017_paper.pdf",
        "aff": "UCLA Vision Lab, University of California, Los Angeles, CA 90095; UCLA Vision Lab, University of California, Los Angeles, CA 90095",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yang_S2F_Slow-To-Fast_Interpolator_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1049327,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5610951678971055832&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_S2F_Slow-To-Fast_Interpolator_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "UCLA Vision Lab",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "S3Pool: Pooling With Stochastic Spatial Sampling",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2051",
        "author_site": "Shuangfei Zhai, Hui Wu, Abhishek Kumar, Yu Cheng, Yongxi Lu, Zhongfei Zhang, Rogerio Feris",
        "author": "Shuangfei Zhai; Hui Wu; Abhishek Kumar; Yu Cheng; Yongxi Lu; Zhongfei Zhang; Rogerio Feris",
        "abstract": "Feature pooling layers (e.g., max pooling) in convolutional neural networks (CNNs) serve the dual purpose of providing increasingly abstract representations as well as yielding computational savings in subsequent convolutional layers. We view the pooling operation in CNNs as a two step procedure: first, a pooling window (e.g., 2x2) slides over the feature map with stride one which leaves the spatial resolution intact,  and second, downsampling is performed by selecting one pixel from each non-overlapping pooling window in an often uniform and deterministic (e.g., top-left) manner. Our starting point in this work is the observation that this regularly spaced downsampling arising from non-overlapping windows, although intuitive from a signal processing perspective (which has the goal of signal reconstruction), is not necessarily optimal for learning (where the goal is to generalize).                     We study this aspect and propose a novel pooling strategy with stochastic spatial sampling (S3Pool), where the regular downsampling is replaced by a more general stochastic version.      We observe that this general stochasticity acts as a strong regularizer, and can also be seen as doing implicit data augmentation by introducing distortions in the feature maps.  We further introduce a mechanism to control the amount of distortion to suit different datasets and architectures. To demonstrate the effectiveness of the proposed approach, we perform extensive experiments on several popular image classification benchmarks,  observing excellent improvements over baseline models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhai_S3Pool_Pooling_With_CVPR_2017_paper.pdf",
        "aff": "Binghamton Univeristy;IBM Research;IBM Research;IBM Research;UC San Diego;Binghamton Univeristy;IBM Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05138v1",
        "pdf_size": 568016,
        "gs_citation": 106,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1248412472100457258&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "binghamton.edu;us.ibm.com;us.ibm.com;us.ibm.com;ucsd.edu;binghamton.edu;us.ibm.com",
        "email": "binghamton.edu;us.ibm.com;us.ibm.com;us.ibm.com;ucsd.edu;binghamton.edu;us.ibm.com",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhai_S3Pool_Pooling_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;2;0;1",
        "aff_unique_norm": "Binghamton University;IBM;University of California, San Diego",
        "aff_unique_dep": ";IBM Research;",
        "aff_unique_url": "https://www.binghamton.edu;https://www.ibm.com/research;https://www.ucsd.edu",
        "aff_unique_abbr": "Binghamton;IBM;UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2402",
        "author_site": "Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, Tat-Seng Chua",
        "author": "Long Chen; Hanwang Zhang; Jun Xiao; Liqiang Nie; Jian Shao; Wei Liu; Tat-Seng Chua",
        "abstract": "Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism --- a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning  methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 2297,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6724303383476915310&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.html"
    },
    {
        "title": "SCC: Semantic Context Cascade for Efficient Action Detection",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "543",
        "author_site": "Fabian Caba Heilbron, Wayner Barrios, Victor Escorcia, Bernard Ghanem",
        "author": "Fabian Caba Heilbron; Wayner Barrios; Victor Escorcia; Bernard Ghanem",
        "abstract": "Despite the recent advances in large-scale video analysis, action detection remains as one of the most challenging unsolved problems in computer vision. This snag is in part due to the large volume of data that needs to be analyzed to detect actions in videos. Existing approaches have mitigated the computational cost, but still, these methods lack rich high-level semantics that helps them to localize the actions quickly. In this paper, we introduce a Semantic Cascade Context (SCC) model that aims to detect action in long video sequences. By embracing semantic priors associated with human activities, SCC produces high-quality class-specific action proposals and prune unrelated activities in a cascade fashion. Experimental results in ActivityNet unveils that SCC achieves state-of-the-art performance for action detection while operating at real time.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Heilbron_SCC_Semantic_Context_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2650305,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7450454011015834709&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Heilbron_SCC_Semantic_Context_CVPR_2017_paper.html"
    },
    {
        "title": "SGM-Nets: Semi-Global Matching With Neural Networks",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "87",
        "author_site": "Akihito Seki, Marc Pollefeys",
        "author": "Akihito Seki; Marc Pollefeys",
        "abstract": "This paper deals with deep neural networks for predicting accurate dense disparity map with Semi-global matching (SGM). SGM is a widely used regularization method for real scenes because of its high accuracy and fast computation speed. Even though SGM can obtain accurate results, tuning of SGM's penalty-parameters, which control a smoothness and discontinuity of a disparity map, is uneasy and empirical methods have been proposed. We propose a learning based penalties estimation method, which we call SGM-Nets that consist of Convolutional Neural Networks. A small image patch and its position are input into SGMNets to predict the penalties for the 3D object structures. In order to train the networks, we introduce a novel loss function which is able to use sparsely annotated disparity maps such as captured by a LiDAR sensor in real environments. Moreover, we propose a novel SGM parameterization, which deploys different penalties depending on either positive or negative disparity changes in order to represent the object structures more discriminatively. Our SGM-Nets outperformed state of the art accuracy on KITTI benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Seki_SGM-Nets_Semi-Global_Matching_CVPR_2017_paper.pdf",
        "aff": "Toshiba Corporation; ETH Z\u00fcrich+Microsoft",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Seki_SGM-Nets_Semi-Global_Matching_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5099417,
        "gs_citation": 328,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6610716315511346020&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "toshiba.co.jp;inf.ethz.ch",
        "email": "toshiba.co.jp;inf.ethz.ch",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Seki_SGM-Nets_Semi-Global_Matching_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Toshiba Corporation;ETH Zurich;Microsoft",
        "aff_unique_dep": ";;Microsoft Corporation",
        "aff_unique_url": "https://www.toshiba.co.jp;https://www.ethz.ch;https://www.microsoft.com",
        "aff_unique_abbr": "Toshiba;ETHZ;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+2",
        "aff_country_unique": "Japan;Switzerland;United States"
    },
    {
        "title": "SPFTN: A Self-Paced Fine-Tuning Network for Segmenting Objects in Weakly Labelled Videos",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1808",
        "author_site": "Dingwen Zhang, Le Yang, Deyu Meng, Dong Xu, Junwei Han",
        "author": "Dingwen Zhang; Le Yang; Deyu Meng; Dong Xu; Junwei Han",
        "abstract": "Object segmentation in weakly labelled videos is an interesting yet challenging task, which aims at learning to perform category-specific video object segmentation by only using video-level tags. Existing works in this research area might still have some limitations, e.g., lack of effective DNN-based learning frameworks, under-exploring the context information, and requiring to leverage the unstable negative video collection, which prevent them from obtaining more promising performance. To this end, we propose a novel self-paced fine-tuning network (SPFTN)-based framework, which could learn to explore the context information within the video frames and capture adequate object semantics without using the negative videos. To perform weakly supervised learning based on the deep neural network, we make the earliest effort to integrate the self-paced learning regime and the deep neural network into a unified and compatible framework, leading to the self-paced fine-tuning network. Comprehensive experiments on the large-scale YouTube-Objects and DAVIS datasets demonstrate that the proposed approach achieves superior performance as compared with other state-of-the-art methods as well as the baseline networks and models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_SPFTN_A_Self-Paced_CVPR_2017_paper.pdf",
        "aff": "Northwestern Polytechincal University; Northwestern Polytechincal University; Xi\u2019an Jiaotong University; University of Sydney; Northwestern Polytechincal University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2722844,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2228141679105536193&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "mail.nwpu.edu.cn;gmail.com;mail.xjtu.edu.cn;sydney.edu.au;gmail.com",
        "email": "mail.nwpu.edu.cn;gmail.com;mail.xjtu.edu.cn;sydney.edu.au;gmail.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_SPFTN_A_Self-Paced_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Northwestern Polytechnic University;Xi'an Jiao Tong University;University of Sydney",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nwpu.edu.cn;https://www.xjtu.edu.cn;https://www.sydney.edu.au",
        "aff_unique_abbr": "NWPU;XJTU;USYD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2;0",
        "aff_country_unique": "United States;China;Australia"
    },
    {
        "title": "SRN: Side-output Residual Network for Object Symmetry Detection in the Wild",
        "session": "Low- & Mid-Level Vision",
        "status": "Oral",
        "track": "main",
        "pid": "369",
        "author_site": "Wei Ke, Jie Chen, Jianbin Jiao, Guoying Zhao, Qixiang Ye",
        "author": "Wei Ke; Jie Chen; Jianbin Jiao; Guoying Zhao; Qixiang Ye",
        "abstract": "In this paper, we establish a baseline for object symmetry detection in complex backgrounds by presenting a new benchmark and an end-to-end deep learning approach, opening up a promising direction for symmetry detection in the wild. The new benchmark, named Sym-PASCAL, spans challenges including object diversity, multi-objects, part-invisibility, and various complex backgrounds that are far beyond those in existing datasets. The proposed symmetry detection approach, named Side-output Residual Network (SRN), leverages output Residual Units (RUs) to fit the errors between the object symmetry ground-truth and the outputs of RUs. By stacking RUs in a deep-to-shallow manner, SRN exploits the 'flow' of errors among multiple scales to ease the problems of fitting complex outputs with limited layers, suppressing the complex backgrounds, and effectively matching object symmetry of different scales. Experimental results validate both the benchmark and its challenging aspects related to real-world images, and the state-of-the-art performance of our symmetry detection approach. The benchmark and the code for SRN are publicly available at https://github.com/KevinKecc/SRN.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ke_SRN_Side-output_Residual_CVPR_2017_paper.pdf",
        "aff": "University of Chinese Academy of Sciences; CMVS, University of Oulu; University of Chinese Academy of Sciences; CMVS, University of Oulu; University of Chinese Academy of Sciences",
        "project": "",
        "github": "https://github.com/KevinKecc/SRN",
        "supp": "",
        "arxiv": "1703.02243v2",
        "pdf_size": 1762485,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8677714818345806583&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "mails.ucas.ac.cn;ee.oulu.fi;ucas.ac.cn;ee.oulu.fi;ucas.ac.cn",
        "email": "mails.ucas.ac.cn;ee.oulu.fi;ucas.ac.cn;ee.oulu.fi;ucas.ac.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ke_SRN_Side-output_Residual_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "University of Chinese Academy of Sciences;University of Oulu",
        "aff_unique_dep": ";CMVS",
        "aff_unique_url": "http://www.ucas.ac.cn;https://www.oulu.fi",
        "aff_unique_abbr": "UCAS;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "China;Finland"
    },
    {
        "title": "SST: Single-Stream Temporal Action Proposals",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1063",
        "author_site": "Shyamal Buch, Victor Escorcia, Chuanqi Shen, Bernard Ghanem, Juan Carlos Niebles",
        "author": "Shyamal Buch; Victor Escorcia; Chuanqi Shen; Bernard Ghanem; Juan Carlos Niebles",
        "abstract": "Our paper presents a new approach for temporal detection of human actions in long, untrimmed video sequences. We introduce Single-Stream Temporal Action Proposals (SST), a new effective and efficient deep architecture for the generation of temporal action proposals. Our network can run continuously in a single stream over very long input video sequences, without the need to divide input into short overlapping clips or temporal windows for batch processing. We demonstrate empirically that our model outperforms the state-of-the-art on the task of temporal action proposal generation, while achieving some of the fastest processing speeds in the literature. Finally, we demonstrate that using SST proposals in conjunction with existing action classifiers results in improved state-of-the-art temporal action detection performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Buch_SST_Single-Stream_Temporal_CVPR_2017_paper.pdf",
        "aff": "Stanford University; King Abdullah University of Science and Technology (KAUST); Stanford University; King Abdullah University of Science and Technology (KAUST); Stanford University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Buch_SST_Single-Stream_Temporal_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1521241,
        "gs_citation": 544,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10040369624776760454&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.stanford.edu;kaust.edu.sa;cs.stanford.edu;kaust.edu.sa;cs.stanford.edu",
        "email": "cs.stanford.edu;kaust.edu.sa;cs.stanford.edu;kaust.edu.sa;cs.stanford.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Buch_SST_Single-Stream_Temporal_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "Stanford University;King Abdullah University of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.kaust.edu.sa",
        "aff_unique_abbr": "Stanford;KAUST",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "United States;Saudi Arabia"
    },
    {
        "title": "STD2P: RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven Pooling",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2002",
        "author_site": "Yang He, Wei-Chen Chiu, Margret Keuper, Mario Fritz",
        "author": "Yang He; Wei-Chen Chiu; Margret Keuper; Mario Fritz",
        "abstract": "We propose a novel superpixel-based multi-view convolutional neural network for semantic image segmentation. The proposed network produces a high quality segmentation of a single image by leveraging information from additional views of the same scene. Particularly in indoor videos such as captured by robotic platforms or handheld and bodyworn RGBD cameras, nearby video frames provide diverse viewpoints and additional context of objects and scenes. To leverage such information, we first compute region correspondences by optical flow and image boundary-based superpixels. Given these region correspondences, we propose a novel spatio-temporal pooling layer to aggregate information over space and time. We evaluate our approach on the NYU-Depth-V2 and the SUN3D datasets and compare it to various state-of-the-art single-view and multi-view approaches. Besides a general improvement over the state-of-the-art, we also show the benefits of making use of unlabeled frames during training for multi-view as well as single-view prediction.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/He_STD2P_RGBD_Semantic_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/He_STD2P_RGBD_Semantic_2017_CVPR_supplemental.pdf",
        "arxiv": "1604.02388v3",
        "pdf_size": 2016747,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11552982465580005015&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/He_STD2P_RGBD_Semantic_CVPR_2017_paper.html"
    },
    {
        "title": "Saliency Revisited: Analysis of Mouse Movements Versus Fixations",
        "session": "Theory",
        "status": "Poster",
        "track": "main",
        "pid": "650",
        "author_site": "Hamed R. Tavakoli, Fawad Ahmed, Ali Borji, Jorma Laaksonen",
        "author": "Hamed R. Tavakoli; Fawad Ahmed; Ali Borji; Jorma Laaksonen",
        "abstract": "This paper revisits visual saliency prediction by evaluating the recent advancements in this field such as crowd-sourced mouse tracking-based databases and contextual annotations. We pursue a critical and quantitative approach towards some of the new challenges including the quality of mouse tracking versus eye tracking for model training and evaluation. We extend quantitative evaluation of models in order to incorporate contextual information by proposing an evaluation methodology that allows accounting for contextual factors such as text, faces, and object attributes. The proposed contextual evaluation scheme facilitates detailed analysis of models and helps identify their pros and cons. Through several experiments, we find that (1) mouse tracking data has lower inter-participant visual congruency and higher dispersion, compared to the eye tracking data, (2) mouse tracking data does not totally agree with eye tracking in general and in terms of different contextual regions in specific, and (3) mouse tracking data leads to acceptable results in training current existing models, and (4) mouse tracking data is less reliable for model selection and evaluation. The contextual evaluation also reveals that, among the studied models, there is no single model that performs best on all the tested annotations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tavakoli_Saliency_Revisited_Analysis_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.10546",
        "pdf_size": 1053294,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12934874504378030308&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tavakoli_Saliency_Revisited_Analysis_CVPR_2017_paper.html"
    },
    {
        "title": "Scalable Person Re-Identification on Supervised Smoothed Manifold",
        "session": "Analyzing Humans 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "915",
        "author_site": "Song Bai, Xiang Bai, Qi Tian",
        "author": "Song Bai; Xiang Bai; Qi Tian",
        "abstract": "Most existing person re-identification algorithms either extract robust visual features or learn discriminative metrics for person images. However, the underlying manifold which those images reside on is rarely investigated. That arises a problem that the learned metric is not smooth with respect to the local geometry structure of the data manifold.  In this paper, we study person re-identification with manifold-based affinity learning, which did not receive enough attention from this area. An unconventional manifold-preserving algorithm is proposed, which can 1) make best use of supervision from training data, whose label information is given as pairwise constraints; 2) scale up to large repositories with low on-line time complexity; and 3) be plunged into most existing algorithms, serving as a generic postprocessing procedure to further boost the identification accuracies. Extensive experimental results on five popular person re-identification benchmarks consistently demonstrate the effectiveness of our method. Especially, on the largest CUHK03 and Market-1501, our method outperforms the state-of-the-art alternatives by a large margin with high efficiency, which is more appropriate for practical applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bai_Scalable_Person_Re-Identification_CVPR_2017_paper.pdf",
        "aff": "Huazhong University of Science and Technology; Huazhong University of Science and Technology; University of Texas at San Antonio",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Bai_Scalable_Person_Re-Identification_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.08359v1",
        "pdf_size": 795486,
        "gs_citation": 390,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15152737632152076086&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "hust.edu.cn;hust.edu.cn;utsa.edu",
        "email": "hust.edu.cn;hust.edu.cn;utsa.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bai_Scalable_Person_Re-Identification_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Huazhong University of Science and Technology;University of Texas at San Antonio",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.hust.edu.cn;https://www.utsa.edu",
        "aff_unique_abbr": "HUST;UTSA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Antonio",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Scalable Surface Reconstruction From Point Clouds With Extreme Scale and Density Diversity",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "299",
        "author_site": "Christian Mostegel, Rudolf Prettenthaler, Friedrich Fraundorfer, Horst Bischof",
        "author": "Christian Mostegel; Rudolf Prettenthaler; Friedrich Fraundorfer; Horst Bischof",
        "abstract": "In this paper we present a scalable approach for robustly computing a 3D surface mesh from multi-scale multi-view stereo point clouds that can handle extreme jumps of point density (in our experiments three orders of magnitude). The backbone of our approach is a combination of octree data partitioning, local Delaunay tetrahedralization and graph cut optimization. Graph cut optimization is used twice, once to extract surface hypotheses from local Delaunay tetrahedralizations and once to merge overlapping surface hypotheses even when the local tetrahedralizations do not share the same topology. This formulation allows us to obtain a constant memory consumption per sub-problem while at the same time retaining the density independent interpolation properties of the Delaunay-based optimization. On multiple public datasets, we demonstrate that our approach is highly competitive with the state-of-the-art in terms of accuracy, completeness and outlier resilience. Further, we demonstrate the multi-scale potential of our approach by processing a newly recorded dataset with 2 billion points and a point density variation of more than four orders of magnitude - requiring less than 9GB of RAM per process.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Mostegel_Scalable_Surface_Reconstruction_CVPR_2017_paper.pdf",
        "aff": "Institute for Computer Graphics and Vision, Graz University of Technology; Institute for Computer Graphics and Vision, Graz University of Technology; Institute for Computer Graphics and Vision, Graz University of Technology; Institute for Computer Graphics and Vision, Graz University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.00949v1",
        "pdf_size": 7152019,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12097438925407237616&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "email": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Mostegel_Scalable_Surface_Reconstruction_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "Institute for Computer Graphics and Vision",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TU Graz",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Graz",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Austria"
    },
    {
        "title": "Scale-Aware Face Detection",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2721",
        "author_site": "Zekun Hao, Yu Liu, Hongwei Qin, Junjie Yan, Xiu Li, Xiaolin Hu",
        "author": "Zekun Hao; Yu Liu; Hongwei Qin; Junjie Yan; Xiu Li; Xiaolin Hu",
        "abstract": "Convolutional neural network (CNN) based face detectors are inefficient in handling faces of diverse scales. They rely on either fitting a large single model to faces across a large scale range or multi-scale testing. Both are computationally expensive. We propose Scale-aware Face Detection (SAFD) to handle scale explicitly using CNN, and achieve better performance with less computation cost. Prior to detection, an efficient CNN predicts the scale distribution histogram of the faces. Then the scale histogram guides the zoom-in and zoom-out of the image. Since the faces will be approximately in uniform scale after zoom, they can be detected accurately even with much smaller CNN. Actually, more than 99% of the faces in AFW can be covered with less than two zooms per image. Extensive experiments on FDDB, MALF and AFW show advantages of SAFD.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hao_Scale-Aware_Face_Detection_CVPR_2017_paper.pdf",
        "aff": "SenseTime; SenseTime; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1706.09876v1",
        "pdf_size": 1687239,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8418331331858166201&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "outlook.com;outlook.com;gmail.com;mails.tsinghua.edu.cn;tsinghua.edu.cn;sz.tsinghua.edu.cn",
        "email": "outlook.com;outlook.com;gmail.com;mails.tsinghua.edu.cn;tsinghua.edu.cn;sz.tsinghua.edu.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hao_Scale-Aware_Face_Detection_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;1;1;1",
        "aff_unique_norm": "SenseTime;Tsinghua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sensetime.com;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "SenseTime;THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes",
        "session": "3D Vision 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "2513",
        "author_site": "Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nie\u00c3\u009fner",
        "author": "Angela Dai; Angel X. Chang; Manolis Savva; Maciej Halber; Thomas Funkhouser; Matthias Niessner",
        "abstract": "A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -- current datasets cover a small range of scene views and have limited semantic annotations.   To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations.  To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dai_ScanNet_Richly-Annotated_3D_CVPR_2017_paper.pdf",
        "aff": "Stanford University; Princeton University; Princeton University; Princeton University; Princeton University; Stanford University+Technical University of Munich",
        "project": "www.scan-net.org",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Dai_ScanNet_Richly-Annotated_3D_2017_CVPR_supplemental.zip",
        "arxiv": "1702.04405",
        "pdf_size": 2089027,
        "gs_citation": 5003,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11711174654424692449&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dai_ScanNet_Richly-Annotated_3D_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;1;0+2",
        "aff_unique_norm": "Stanford University;Princeton University;Technical University of Munich",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.princeton.edu;https://www.tum.de",
        "aff_unique_abbr": "Stanford;Princeton;TUM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0;0+1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "Scene Flow to Action Map: A New Representation for RGB-D Based Action Recognition With Convolutional Neural Networks",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "189",
        "author_site": "Pichao Wang, Wanqing Li, Zhimin Gao, Yuyao Zhang, Chang Tang, Philip Ogunbona",
        "author": "Pichao Wang; Wanqing Li; Zhimin Gao; Yuyao Zhang; Chang Tang; Philip Ogunbona",
        "abstract": "Scene flow describes the motion of 3D objects in real world and potentially could be the basis of a good feature for 3D action recognition. However, its use for action recognition, especially in the context of convolutional neural networks (ConvNets), has not been previously studied. In this paper, we propose the extraction and use of scene flow for action recognition from RGB-D data. Previous works have considered the depth and RGB modalities as separate channels and extract features for later fusion. We take a different approach and consider the modalities as one entity, thus allowing feature extraction for action recognition at the beginning. Two key questions about the use of scene flow for action recognition are addressed: how to organize the scene flow vectors and how to represent the long term dynamics of videos based on scene flow. In order to calculate the scene flow correctly on the available datasets, we propose an effective self-calibration method to align the RGB and depth data spatially without knowledge of the camera parameters. Based on the scene flow vectors, we propose a new representation, namely, Scene Flow to Action Map (SFAM), that describes several long term spatio-temporal dynamics for action recognition. We adopt a channel transform kernel to transform the scene flow vectors to an optimal color space analogous to RGB.  This transformation takes better advantage of the trained ConvNets models over ImageNet. Experimental results indicate that this new representation can surpass the performance of state-of-the-art methods on two large public datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Scene_Flow_to_CVPR_2017_paper.pdf",
        "aff": "Advanced Multimedia Research Lab, University of Wollongong, Australia; Advanced Multimedia Research Lab, University of Wollongong, Australia; Advanced Multimedia Research Lab, University of Wollongong, Australia; Advanced Multimedia Research Lab, University of Wollongong, Australia; School of Computer Science, China University of Geosciences, Wuhan, China; Advanced Multimedia Research Lab, University of Wollongong, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.08652",
        "pdf_size": 754732,
        "gs_citation": 176,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17913482200547778367&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "uowmail.edu.au;uow.edu.au;uowmail.edu.au;uowmail.edu.au;gmail.com;uow.edu.au",
        "email": "uowmail.edu.au;uow.edu.au;uowmail.edu.au;uowmail.edu.au;gmail.com;uow.edu.au",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Scene_Flow_to_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "University of Wollongong;China University of Geosciences",
        "aff_unique_dep": "Advanced Multimedia Research Lab;School of Computer Science",
        "aff_unique_url": "https://www.uow.edu.au;http://www.cug.edu.cn",
        "aff_unique_abbr": ";CUG",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Wuhan",
        "aff_country_unique_index": "0;0;0;0;1;0",
        "aff_country_unique": "Australia;China"
    },
    {
        "title": "Scene Graph Generation by Iterative Message Passing",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2289",
        "author_site": "Danfei Xu, Yuke Zhu, Christopher B. Choy, Li Fei-Fei",
        "author": "Danfei Xu; Yuke Zhu; Christopher B. Choy; Li Fei-Fei",
        "abstract": "Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel end-to-end model that generates such structured scene representation from an input image. Our key insight is that the graph generation problem can be formulated as message passing between the primal node graph and its dual edge graph. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods on the Visual Genome dataset as well as support relation inference in NYU Depth V2 dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Scene_Graph_Generation_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Electrical Engineering, Stanford University; Department of Computer Science, Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.02426",
        "pdf_size": 1885989,
        "gs_citation": 1568,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12812416044799283808&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xu_Scene_Graph_Generation_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Scene Parsing Through ADE20K Dataset",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "194",
        "author_site": "Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba",
        "author": "Bolei Zhou; Hang Zhao; Xavier Puig; Sanja Fidler; Adela Barriuso; Antonio Torralba",
        "abstract": "Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A scene parsing benchmark is built upon the ADE20K with 150 object and stuff classes included. Several segmentation baseline models are evaluated on the benchmark. A novel network design called Cascade Segmentation Module is proposed to parse a scene into stuff, objects, and object parts in a cascade and improve over the baselines. We further show that the trained scene parsing networks can lead to applications such as image content removal and scene synthesis(Dataset and pretrained models are available at http://groups.csail.mit.edu/vision/datasets/ADE20K/).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Scene_Parsing_Through_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2427210,
        "gs_citation": 4105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16858687614404160649&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Scene_Parsing_Through_CVPR_2017_paper.html"
    },
    {
        "title": "Scribbler: Controlling Deep Image Synthesis With Sketch and Color",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "2275",
        "author_site": "Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, James Hays",
        "author": "Patsorn Sangkloy; Jingwan Lu; Chen Fang; Fisher Yu; James Hays",
        "abstract": "Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adverserial image synthesis architecture that is conditioned on coarse sketches and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to 'scribble' over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sangkloy_Scribbler_Controlling_Deep_CVPR_2017_paper.pdf",
        "aff": "Georgia Institute of Technology; Adobe Research; Adobe Research; Princeton University; Georgia Institute of Technology",
        "project": "scribbler.eye.gatech.edu",
        "github": "",
        "supp": "",
        "arxiv": "1612.00835v2",
        "pdf_size": 1765146,
        "gs_citation": 643,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13445531679657986508&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sangkloy_Scribbler_Controlling_Deep_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;2;0",
        "aff_unique_norm": "Georgia Institute of Technology;Adobe;Princeton University",
        "aff_unique_dep": ";Adobe Research;",
        "aff_unique_url": "https://www.gatech.edu;https://research.adobe.com;https://www.princeton.edu",
        "aff_unique_abbr": "Georgia Tech;Adobe;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-Based Person Re-Identification",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1970",
        "author_site": "Zhen Zhou, Yan Huang, Wei Wang, Liang Wang, Tieniu Tan",
        "author": "Zhen Zhou; Yan Huang; Wei Wang; Liang Wang; Tieniu Tan",
        "abstract": "Surveillance cameras have been widely used in different scenes. Accordingly, a demanding need is to recognize a person under different cameras, which is called person re-identification. This topic has gained increasing interests in computer vision recently. However, less attention has been paid to video-based approaches, compared with image-based ones. Two steps are usually involved in previous approaches, namely feature learning and metric learning. But most of the existing approaches only focus on either feature learning or metric learning. Meanwhile, many of them do not take full use of the temporal and spatial information. In this paper, we concentrate on video-based person re-identification and build an end-to-end deep neural network architecture to jointly learn features and metrics. The proposed method can automatically pick out the most discriminative frames in a given video by a temporal attention model. Moreover, it integrates the surrounding information at each location by a spatial recurrent model when measuring the similarity with another pedestrian video. That is, our method handles spatial and temporal information simultaneously in a unified manner. The carefully designed experiments on three public datasets show the effectiveness of each component of the proposed deep network, performing better in comparison with the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_See_the_Forest_CVPR_2017_paper.pdf",
        "aff": "Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR) + University of Chinese Academy of Sciences (UCAS); Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR) + University of Chinese Academy of Sciences (UCAS); Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR) + University of Chinese Academy of Sciences (UCAS); Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR) + Center for Excellence in Brain Science and Intelligence Technology (CEBSIT), Institute of Automation, Chinese Academy of Sciences (CASIA) + University of Chinese Academy of Sciences (UCAS); Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR) + Center for Excellence in Brain Science and Intelligence Technology (CEBSIT), Institute of Automation, Chinese Academy of Sciences (CASIA) + University of Chinese Academy of Sciences (UCAS)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1350538,
        "gs_citation": 395,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9473997489770572987&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_See_the_Forest_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;0+2+1;0+2+1",
        "aff_unique_norm": "National Laboratory of Pattern Recognition;University of Chinese Academy of Sciences;Chinese Academy of Sciences",
        "aff_unique_dep": "Center for Research on Intelligent Perception and Computing;;Institute of Automation",
        "aff_unique_url": ";http://www.ucas.ac.cn;http://www.ia.cas.cn",
        "aff_unique_abbr": "NLPR;UCAS;CASIA",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Seeing Into Darkness: Scotopic Visual Recognition",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1470",
        "author_site": "Bo Chen, Pietro Perona",
        "author": "Bo Chen; Pietro Perona",
        "abstract": "Images are formed by counting how many photons traveling from a given set of directions hit an image sensor during a given time interval. When photons are few and far in between, the concept of `image' breaks down and it is best to consider directly the flow of photons. Computer vision in this regime, which we call `scotopic', is radically different from the classical image-based paradigm in that visual computations (classification, control, search) have to take place while the stream of photons is captured and decisions may be taken as soon as enough information is available. The scotopic regime is important for biomedical imaging, security, astronomy and many other fields. Here we develop a framework that allows a machine to classify objects with as few photons as possible, while maintaining the error rate below an acceptable threshold. A dynamic and asymptotically optimal speed-accuracy tradeoff is a key feature of this framework. We propose and study an algorithm to optimize the tradeoff of a convolutional network directly from lowlight images and evaluate on simulated images from standard datasets. Surprisingly, scotopic systems can achieve comparable classification performance as traditional vision systems while using less than 0.1% of the photons in a conventional image. In addition, we demonstrate that our algorithms work even when the illuminance of the environment is unknown and varying. Last, we outline a spiking neural network coupled with photon-counting sensors as a power-efficient hardware realization of scotopic algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Seeing_Into_Darkness_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Chen_Seeing_Into_Darkness_2017_CVPR_supplemental.pdf",
        "arxiv": "1610.00405v1",
        "pdf_size": 4235302,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6208847812670454330&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Seeing_Into_Darkness_CVPR_2017_paper.html"
    },
    {
        "title": "Seeing Invisible Poses: Estimating 3D Body Pose From Egocentric Video",
        "session": "Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "1579",
        "author_site": "Hao Jiang, Kristen Grauman",
        "author": "Hao Jiang; Kristen Grauman",
        "abstract": "Understanding the camera wearer's activity is central to egocentric vision, yet one key facet of that activity is inherently invisible to the camera--the wearer's body pose. Prior work focuses on estimating the pose of hands and arms when they come into view, but this 1) gives an incomplete view of the full body posture, and 2) prevents any pose estimate at all in many frames, since the hands are only visible in a fraction of daily life activities. We propose to infer the \"invisible pose\" of a person behind the egocentric camera. Given a single video, our efficient learning-based approach returns the full body 3D joint positions for each frame. Our method exploits cues from the dynamic motion signatures of the surrounding scene--which change predictably as a function of body pose--as well as static scene structures that reveal the viewpoint (e.g., sitting vs. standing). We further introduce a novel energy minimization scheme to infer the pose sequence. It uses soft predictions of the poses per time instant together with a non-parametric model of human pose dynamics over longer windows. Our method outperforms an array of possible alternatives, including typical deep learning approaches for direct pose regression from images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jiang_Seeing_Invisible_Poses_CVPR_2017_paper.pdf",
        "aff": "Boston College, USA; University of Texas at Austin, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1603.07763v1",
        "pdf_size": 43534034,
        "gs_citation": 117,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4255109979978512705&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.bc.edu;cs.utexas.edu",
        "email": "cs.bc.edu;cs.utexas.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jiang_Seeing_Invisible_Poses_CVPR_2017_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Boston College;University of Texas at Austin",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bostoncollege.edu;https://www.utexas.edu",
        "aff_unique_abbr": "BC;UT Austin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Seeing What Is Not There: Learning Context to Determine Where Objects Are Missing",
        "session": "Analyzing Humans 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "2433",
        "author_site": "Jin Sun, David W. Jacobs",
        "author": "Jin Sun; David W. Jacobs",
        "abstract": "Most of computer vision focuses on what is in an image. We propose to train a standalone object-centric context representation to perform the opposite task: seeing what is not there. Given an image, our context model can predict where objects should exist, even when no object instances are present. Combined with object detection results, we can perform a novel vision task: finding where objects are missing in an image. Our model is based on a convolutional neural network structure. With a specially designed training strategy, the model learns to ignore objects and focus on context only. It is fully convolutional thus highly efficient. Experiments show the effectiveness of the proposed approach in one important accessibility task: finding city street regions where curb ramps are missing, which could help millions of people with mobility disabilities.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sun_Seeing_What_Is_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, University of Maryland; Department of Computer Science, University of Maryland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Sun_Seeing_What_Is_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1200547,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9245715277973624540&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.umd.edu;cs.umd.edu",
        "email": "cs.umd.edu;cs.umd.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sun_Seeing_What_Is_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Self-Calibration-Based Approach to Critical Motion Sequences of Rolling-Shutter Structure From Motion",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "259",
        "author_site": "Eisuke Ito, Takayuki Okatani",
        "author": "Eisuke Ito; Takayuki Okatani",
        "abstract": "In this paper we consider critical motion sequences (CMSs) of rolling-shutter (RS) SfM. Employing an RS camera model with linearized pure rotation, we show that the RS distortion can be approximately expressed by two internal parameters of an \"imaginary\" camera plus one-parameter nonlinear transformation similar to lens distortion. We then reformulate the problem as self-calibration of the imaginary camera, in which its skew and aspect ratio are unknown and varying in the image sequence. In the formulation, we derive a general representation of CMSs. We also show that our method can explain the CMS that was recently reported in the literature, and then present a new remedy to deal with the degeneracy. Our theoretical results agree well with experimental results; it explains degeneracies observed when we employ naive bundle adjustment, and how they are resolved by our method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ito_Self-Calibration-Based_Approach_to_CVPR_2017_paper.pdf",
        "aff": "Tohoku University; Tohoku University + RIKEN Center for Advanced Intelligence Project",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05476v1",
        "pdf_size": 2038225,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3204277684569232390&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "vision.is.tohoku.ac.jp;vision.is.tohoku.ac.jp",
        "email": "vision.is.tohoku.ac.jp;vision.is.tohoku.ac.jp",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ito_Self-Calibration-Based_Approach_to_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Tohoku University;RIKEN",
        "aff_unique_dep": ";Center for Advanced Intelligence Project",
        "aff_unique_url": "https://www.tohoku.ac.jp;https://www.riken.jp/en/",
        "aff_unique_abbr": "Tohoku U;RIKEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Self-Critical Sequence Training for Image Captioning",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Oral",
        "track": "main",
        "pid": "3266",
        "author_site": "Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, Vaibhava Goel",
        "author": "Steven J. Rennie; Etienne Marcheret; Youssef Mroueh; Jerret Ross; Vaibhava Goel",
        "abstract": "Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a \"baseline\" to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective.  Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper.pdf",
        "aff": "IBM T.J. Watson Research Center, NY, USA; IBM T.J. Watson Research Center, NY, USA + Watson Multimodal Algorithms and Engines Group; IBM T.J. Watson Research Center, NY, USA; IBM T.J. Watson Research Center, NY, USA; IBM T.J. Watson Research Center, NY, USA + Watson Multimodal Algorithms and Engines Group",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.00563",
        "pdf_size": 2322837,
        "gs_citation": 2619,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4288352136730370416&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "us.ibm.com;gmail.com;us.ibm.com;us.ibm.com;gmail.com",
        "email": "us.ibm.com;gmail.com;us.ibm.com;us.ibm.com;gmail.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+0;0;0;0+0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM T.J. Watson Research Center",
        "aff_unique_url": "https://www.ibm.com/research/watson",
        "aff_unique_abbr": "IBM Watson",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Yorktown Heights;",
        "aff_country_unique_index": "0;0+0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Self-Learning Scene-Specific Pedestrian Detectors Using a Progressive Latent Model",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "159",
        "author_site": "Qixiang Ye, Tianliang Zhang, Wei Ke, Qiang Qiu, Jie Chen, Guillermo Sapiro, Baochang Zhang",
        "author": "Qixiang Ye; Tianliang Zhang; Wei Ke; Qiang Qiu; Jie Chen; Guillermo Sapiro; Baochang Zhang",
        "abstract": "In this paper, a self-learning approach is proposed towards solving scene-specific pedestrian detection problem without any human' annotation involved. The self-learning approach is deployed as progressive steps of object discovery, object enforcement, and label propagation. In the learning procedure, object locations in each frame are treated as latent variables that are solved with a progressive latent model (PLM). Compared with conventional latent models, the proposed PLM incorporates a spatial regularization term to reduce ambiguities in object proposals and to enforce object localization, and also a graph-based label propagation to discover harder instances in adjacent frames. With the difference of convex (DC) objective functions, PLM can be efficiently optimized with a concave-convex programming and thus guaranteeing the stability of self-learning. Extensive experiments demonstrate that even without annotation the proposed self-learning approach outperforms weakly supervised learning approaches, while achieving comparable performance with transfer learning and fully supervised approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ye_Self-Learning_Scene-Specific_Pedestrian_CVPR_2017_paper.pdf",
        "aff": "EECE, University of Chinese Academy of Sciences; EECE, University of Chinese Academy of Sciences; EECE, University of Chinese Academy of Sciences; ECE, Duke University; University of Oulu, Finland; ECE, Duke University; ASEE, Beihang University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.07544",
        "pdf_size": 1738253,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12270948429097786158&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ucas.ac.cn;ucas.ac.cn;mails.ucas.ac.cn;duke.edu;oulu.fi;duke.edu;buaa.edu.cn",
        "email": "ucas.ac.cn;ucas.ac.cn;mails.ucas.ac.cn;duke.edu;oulu.fi;duke.edu;buaa.edu.cn",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ye_Self-Learning_Scene-Specific_Pedestrian_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;2;1;3",
        "aff_unique_norm": "University of Chinese Academy of Sciences;Duke University;University of Oulu;Beihang University",
        "aff_unique_dep": "EECE;Electrical and Computer Engineering;;ASEE",
        "aff_unique_url": "http://www.ucas.ac.cn;https://www.duke.edu;https://www.oulu.fi;http://www.buaa.edu.cn",
        "aff_unique_abbr": "UCAS;Duke;UOulu;Beihang",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;2;1;0",
        "aff_country_unique": "China;United States;Finland"
    },
    {
        "title": "Self-Supervised Learning of Visual Features Through Embedding Images Into Text Topic Spaces",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1721",
        "author_site": "Lluis Gomez, Yash Patel, Mar\u00c3\u00a7al Rusi\u00c3\u00b1ol, Dimosthenis Karatzas, C. V. Jawahar",
        "author": "Lluis Gomez; Yash Patel; Marcal Rusinol; Dimosthenis Karatzas; C. V. Jawahar",
        "abstract": "End-to-end training from scratch of current deep architectures for new computer vision problems would require Imagenet-scale datasets, and this is not always possible. In this paper we present a method that is able to take advantage of freely available multi-modal content to train computer vision algorithms without human supervision. We put forward the idea of performing self-supervised learning of visual features by mining a large scale corpus of multi-modal (text and image) documents. We show that discriminative visual features can be learnt efficiently by training a CNN to predict the semantic context in which a particular image is more probable to appear as an illustration. For this we leverage the hidden semantic structures discovered in the text corpus with a well-known topic modeling technique. Our experiments demonstrate state of the art performance in image classification, object detection, and multi-modal retrieval compared to recent self-supervised or natural-supervised approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gomez_Self-Supervised_Learning_of_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.08631v1",
        "pdf_size": 1996136,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8610937182014010454&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gomez_Self-Supervised_Learning_of_CVPR_2017_paper.html"
    },
    {
        "title": "Self-Supervised Video Representation Learning With Odd-One-Out Networks",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1337",
        "author_site": "Basura Fernando, Hakan Bilen, Efstratios Gavves, Stephen Gould",
        "author": "Basura Fernando; Hakan Bilen; Efstratios Gavves; Stephen Gould",
        "abstract": "We propose a new self-supervised CNN pre-training technique based on a novel auxiliary task called odd-one-out learning. In this task, the machine is asked to identify the unrelated or odd element from a set of otherwise related elements. We apply this technique to self-supervised video representation learning where we sample subsequences from videos and ask the network to learn to predict the odd video subsequence. The odd video subsequence is sampled such that it has wrong temporal order of frames while the even ones have the correct temporal order. Therefore, to generate a odd-one-out question no manual annotation is required. Our learning machine is implemented as multi-stream convolutional neural network, which is learned end-to-end. Using odd-one-out networks, we learn temporal representations for videos that generalizes to other related tasks such as action recognition.  On action classification, our method obtains 60.3% on the UCF101 dataset using only UCF101 data for training which is approximately 10% better than current state-of-the-art self-supervised learning methods. Similarly, on HMDB51 dataset we outperform self-supervised state-of-the art methods by 12.7% on action classification task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Fernando_Self-Supervised_Video_Representation_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.06646v4",
        "pdf_size": 622768,
        "gs_citation": 562,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15451087969194820727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Fernando_Self-Supervised_Video_Representation_CVPR_2017_paper.html"
    },
    {
        "title": "Semantic Amodal Segmentation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "547",
        "author_site": "Yan Zhu, Yuandong Tian, Dimitris Metaxas, Piotr Doll\u00c3\u00a1r",
        "author": "Yan Zhu; Yuandong Tian; Dimitris Metaxas; Piotr Dollar",
        "abstract": "Common visual recognition tasks such as classification, object detection, and semantic segmentation are rapidly reaching maturity, and given the recent rate of progress, it is not unreasonable to conjecture that techniques for many of these problems will approach human levels of performance in the next few years. In this paper we look to the future: what is the next frontier in visual recognition? We offer one possible answer to this question. We propose a detailed image annotation that captures information beyond the visible pixels and requires complex reasoning about full scene structure. Specifically, we create an amodal segmentation of each image: the full extent of each region is marked, not just the visible pixels. Annotators outline and name all salient regions in the image and specify a partial depth order. The result is a rich scene structure, including visible and occluded portions of each region, figure-ground edge information, semantic labels, and object overlap. We create two datasets for semantic amodal segmentation. First, we label 500 images in the BSDS dataset with multiple annotators per image, allowing us to study the statistics of human annotations. We show that the proposed full scene annotation is surprisingly consistent between annotators, including for regions and edges. Second, we annotate 5000 images from COCO. This larger dataset allows us to explore a number of algorithmic ideas for amodal segmentation and depth ordering. We introduce novel metrics for these tasks, and along with our strong baselines, define concrete new challenges for the community.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Semantic_Amodal_Segmentation_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1509.01329",
        "pdf_size": 2110487,
        "gs_citation": 212,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3708080030250311277&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhu_Semantic_Amodal_Segmentation_CVPR_2017_paper.html"
    },
    {
        "title": "Semantic Autoencoder for Zero-Shot Learning",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "1173",
        "author_site": "Elyor Kodirov, Tao Xiang, Shaogang Gong",
        "author": "Elyor Kodirov; Tao Xiang; Shaogang Gong",
        "abstract": "Existing zero-shot learning (ZSL) models typically learn a projection function from a feature space to a semantic embedding space (e.g. attribute space). However, such a projection function is only concerned with predicting the training seen class  semantic representation (e.g. attribute prediction) or classification. When applied to test data, which in the context of ZSL contains different (unseen) classes without training data, a ZSL model typically suffers from the project domain shift problem. In this work, we present a novel solution to ZSL based on learning a Semantic AutoEncoder (SAE). Taking the encoder-decoder paradigm, an encoder aims to project a visual feature vector into the semantic space as in the existing ZSL models. However, the decoder exerts an additional constraint, that is, the projection/code must be able to reconstruct the original visual feature. We show that with this additional reconstruction constraint, the learned  projection function from the seen classes is able to generalise better to the new unseen classes. Importantly, the encoder and decoder are linear and symmetric which enable us to develop an extremely efficient learning algorithm. Extensive experiments on six benchmark datasets demonstrate that the proposed SAE outperforms significantly the existing ZSL models with the additional benefit of lower computational cost. Furthermore, when the SAE is applied to supervised clustering problem, it also beats the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kodirov_Semantic_Autoencoder_for_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.08345v1",
        "gs_citation": 1099,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14392129101848795987&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kodirov_Semantic_Autoencoder_for_CVPR_2017_paper.html"
    },
    {
        "title": "Semantic Compositional Networks for Visual Captioning",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Spotlight",
        "track": "main",
        "pid": "2390",
        "author_site": "Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin, Li Deng",
        "author": "Zhe Gan; Chuang Gan; Xiaodong He; Yunchen Pu; Kenneth Tran; Jianfeng Gao; Lawrence Carin; Li Deng",
        "abstract": "A Semantic Compositional Network (SCN) is developed for image captioning, in which semantic concepts (i.e., tags) are detected from the image, and the probability of each tag is used to compose the parameters in a long short-term memory (LSTM) network. The SCN extends each weight matrix of the LSTM to an ensemble of tag-dependent weight matrices. The degree to which each member of the ensemble is used to generate an image caption is tied to the image-dependent probability of the corresponding tag. In addition to captioning images, we also extend the SCN to generate captions for video clips. We qualitatively analyze semantic composition in SCNs, and quantitatively evaluate the algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text. Experimental results show that the proposed method significantly outperforms prior state-of-the-art approaches, across multiple evaluation metrics.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gan_Semantic_Compositional_Networks_CVPR_2017_paper.pdf",
        "aff": "Duke University; Tsinghua University; Microsoft Research, Redmond, WA 98052, USA; Duke University; Microsoft Research, Redmond, WA 98052, USA; Microsoft Research, Redmond, WA 98052, USA; Duke University; Microsoft Research, Redmond, WA 98052, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Gan_Semantic_Compositional_Networks_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.08002v2",
        "pdf_size": 1022292,
        "gs_citation": 561,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16533415689613421354&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "duke.edu;gmail.com;microsoft.com;duke.edu;microsoft.com;microsoft.com;duke.edu;microsoft.com",
        "email": "duke.edu;gmail.com;microsoft.com;duke.edu;microsoft.com;microsoft.com;duke.edu;microsoft.com",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gan_Semantic_Compositional_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0;2;2;0;2",
        "aff_unique_norm": "Duke University;Tsinghua University;Microsoft",
        "aff_unique_dep": ";;Microsoft Research",
        "aff_unique_url": "https://www.duke.edu;https://www.tsinghua.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Duke;THU;MSR",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Redmond",
        "aff_country_unique_index": "0;1;0;0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Semantic Image Inpainting With Deep Generative Models",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "2322",
        "author_site": "Raymond A. Yeh, Chen Chen, Teck Yian Lim, Alexander G. Schwing, Mark Hasegawa-Johnson, Minh N. Do",
        "author": "Raymond A. Yeh; Chen Chen; Teck Yian Lim; Alexander G. Schwing; Mark Hasegawa-Johnson; Minh N. Do",
        "abstract": "Semantic image inpainting is a challenging task where large missing regions have to be filled based on the available visual data. Existing methods which extract information from only a single image generally produce unsatisfactory results due to the lack of high level context.  In this paper, we propose a novel method for  semantic image inpainting, which generates the missing content by conditioning on the available data. Given a trained generative model, we search for the closest encoding of the corrupted image in the latent image manifold using our context and prior losses. This encoding is then passed through the generative model to infer the missing content. In our method, inference is possible irrespective of how the missing content is structured, while the state-of-the-art learning based method requires  specific information about the holes in the training phase. Experiments on three datasets show that our method successfully predicts information in large missing regions and achieves pixel-level photorealism, significantly outperforming the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yeh_Semantic_Image_Inpainting_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1607.07539",
        "gs_citation": 1485,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16696847548454034293&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yeh_Semantic_Image_Inpainting_CVPR_2017_paper.html"
    },
    {
        "title": "Semantic Multi-View Stereo: Jointly Estimating Objects and Voxels",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "885",
        "author_site": "Ali Osman Ulusoy, Michael J. Black, Andreas Geiger",
        "author": "Ali Osman Ulusoy; Michael J. Black; Andreas Geiger",
        "abstract": "Dense 3D reconstruction from RGB images is a highly ill-posed problem due to occlusions, textureless or reflective surfaces, as well as other challenges. We propose object-level shape priors to address these ambiguities. Towards this goal, we formulate a probabilistic model that integrates multi-view image evidence with 3D shape information from multiple objects. Inference in this model yields a dense 3D reconstruction of the scene as well as the existence and precise 3D pose of the objects in it. Our approach is able to recover fine details not captured in the input shapes while defaulting to the input models in occluded regions where image evidence is weak. Due to its probabilistic nature, the approach is able to cope with the approximate geometry of the 3D models as well as input shapes that are not present in the scene. We evaluate the approach quantitatively on several challenging indoor and outdoor datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ulusoy_Semantic_Multi-View_Stereo_CVPR_2017_paper.pdf",
        "aff": "Perceiving Systems Department, MPI for Intelligent Systems T\u00fcbingen + Autonomous Vision Group, MPI for Intelligent Systems T\u00fcbingen; Perceiving Systems Department, MPI for Intelligent Systems T\u00fcbingen; Autonomous Vision Group, MPI for Intelligent Systems T\u00fcbingen + Computer Vision and Geometry Group, ETH Z\u00fcrich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2036316,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8849766798594501054&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tue.mpg.de;tue.mpg.de;tue.mpg.de",
        "email": "tue.mpg.de;tue.mpg.de;tue.mpg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ulusoy_Semantic_Multi-View_Stereo_CVPR_2017_paper.html",
        "aff_unique_index": "0+0;0;0+1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;ETH Zurich",
        "aff_unique_dep": "Perceiving Systems Department;Computer Vision and Geometry Group",
        "aff_unique_url": "https://www.mpituebingen.mpg.de;https://www.ethz.ch",
        "aff_unique_abbr": "MPI-IS;ETHZ",
        "aff_campus_unique_index": "0+0;0;0",
        "aff_campus_unique": "T\u00fcbingen;",
        "aff_country_unique_index": "0+0;0;0+1",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "title": "Semantic Regularisation for Recurrent Image Annotation",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1050",
        "author_site": "Feng Liu, Tao Xiang, Timothy M. Hospedales, Wankou Yang, Changyin Sun",
        "author": "Feng Liu; Tao Xiang; Timothy M. Hospedales; Wankou Yang; Changyin Sun",
        "abstract": "The \"CNN-RNN\" design pattern is increasingly widely applied in a variety of image annotation tasks including  multi-label classification and captioning. Existing models use the weakly semantic CNN hidden layer or its transform as the image embedding that provides the interface between the CNN and RNN. This leaves the RNN overstretched with two jobs: predicting the visual concepts and modelling their correlations for generating structured annotation output. Importantly this makes the end-to-end training of the CNN and RNN slow and ineffective due to the difficulty of back propagating gradients through the RNN to train the CNN. We propose a simple modification to the design pattern that makes learning  more effective and efficient. Specifically, we propose to use a semantically regularised embedding layer as the interface between the CNN and RNN. Regularising the interface can partially or completely decouple the learning problems, allowing each to be  more effectively trained and jointly training much more efficient. Extensive experiments show that  state-of-the art performance is achieved on multi-label classification  as well as image captioning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Semantic_Regularisation_for_CVPR_2017_paper.pdf",
        "aff": "Southeast University, China+Queen Mary University of London, UK; Queen Mary University of London, UK; University of Edinburgh, UK; Southeast University, China; Southeast University, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Liu_Semantic_Regularisation_for_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.05490",
        "pdf_size": 564180,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16936524375601629910&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "seu.edu.cn;qmul.ac.uk;ed.ac.uk;seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;qmul.ac.uk;ed.ac.uk;seu.edu.cn;seu.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Semantic_Regularisation_for_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;2;0;0",
        "aff_unique_norm": "Southeast University;Queen Mary University of London;University of Edinburgh",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.seu.edu.cn/;https://www.qmul.ac.uk;https://www.ed.ac.uk",
        "aff_unique_abbr": "SEU;QMUL;Edinburgh",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0+1;1;1;0;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "title": "Semantic Scene Completion From a Single Depth Image",
        "session": "3D Vision 1",
        "status": "Oral",
        "track": "main",
        "pid": "646",
        "author_site": "Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Manolis Savva, Thomas Funkhouser",
        "author": "Shuran Song; Fisher Yu; Andy Zeng; Angel X. Chang; Manolis Savva; Thomas Funkhouser",
        "abstract": "This paper focuses on semantic scene completion, a task for producing a complete 3D voxel representation of volumetric occupancy and semantic labels for a scene from a single-view depth map observation. Previous work has considered scene completion and semantic labeling of depth maps separately. However, we observe that these two problems are tightly intertwined. To leverage the coupled nature of these two tasks, we introduce the semantic scene completion network (SSCNet), an end-to-end 3D convolutional network that takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum. Our network uses a dilation-based 3D context module to efficiently expand the receptive field and enable 3D context learning. To train our network, we construct SUNCG - a manually created largescale dataset of synthetic 3D scenes with dense volumetric annotations. Our experiments demonstrate that the joint model outperforms methods addressing each task in isolation and outperforms alternative approaches on the semantic scene completion task. The dataset and code is available at http://sscnet.cs.princeton.edu.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Song_Semantic_Scene_Completion_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Song_Semantic_Scene_Completion_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.08974v1",
        "pdf_size": 4464635,
        "gs_citation": 1504,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3370839631979937893&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Song_Semantic_Scene_Completion_CVPR_2017_paper.html"
    },
    {
        "title": "Semantic Segmentation via Structured Patch Prediction, Context CRF and Guidance CRF",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "725",
        "author_site": "Falong Shen, Rui Gan, Shuicheng Yan, Gang Zeng",
        "author": "Falong Shen; Rui Gan; Shuicheng Yan; Gang Zeng",
        "abstract": "This paper describes a fast and accurate semantic image segmentation approach that encodes not only segmentation-specified features but also high-order context compatibilities and boundary guidance constraints. We introduce a structured patch prediction technique to make a trade-off between classification discriminability and boundary sensibility for features. Both label and feature contexts are embedded to ensure recognition accuracy and compatibility, while the complexity of the high order cliques is reduced by a distance-aware sampling and pooling strategy. The proposed joint model also employs a guidance CRF to further enhance the segmentation performance. The message passing step is augmented with the guided filtering which enables an efficient and joint training of the whole system in an end-to-end fashion. Our proposed joint model outperforms the state-of-art on Pascal VOC 2012 and Cityscapes, with mIoU(%) of 82.5 and 79.2 respectively. It also reaches a leading performance on ADE20K, which is the dataset of the scene parsing track in ILSVRC 2016.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shen_Semantic_Segmentation_via_CVPR_2017_paper.pdf",
        "aff": "Peking University; Peking University; 360 AI Institute+National University of Singapore; Peking University",
        "project": "",
        "github": "https://github.com/FalongShen/SegModel",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1715338,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6184470041056362416&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "pku.edu.cn;pku.edu.cn;360.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;360.cn;pku.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shen_Semantic_Segmentation_via_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1+2;0",
        "aff_unique_norm": "Peking University;360 AI Institute;National University of Singapore",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.pku.edu.cn;;https://www.nus.edu.sg",
        "aff_unique_abbr": "Peking U;;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "title": "Semantically Coherent Co-Segmentation and Reconstruction of Dynamic Scenes",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "142",
        "author_site": "Armin Mustafa, Adrian Hilton",
        "author": "Armin Mustafa; Adrian Hilton",
        "abstract": "In this paper we propose a framework for spatially and temporally coherent semantic co-segmentation and reconstruction of complex dynamic scenes from multiple static or moving cameras. Semantic co-segmentation exploits the coherence in semantic class labels both spatially, between views at a single time instant, and temporally, between widely spaced time instants of dynamic objects with similar shape and appearance. We demonstrate that semantic coherence results in improved segmentation and reconstruction for complex scenes. A joint formulation is proposed for semantically coherent object-based co-segmentation and reconstruction of scenes by enforcing consistent semantic labelling between views and over time. Semantic tracklets are introduced to enforce temporal coherence in semantic labelling and reconstruction between widely spaced instances of dynamic objects. Tracklets of dynamic objects enable unsupervised learning of appearance and shape priors that are exploited in joint segmentation and reconstruction. Evaluation on challenging indoor and outdoor sequences with hand-held moving cameras shows improved accuracy in segmentation, temporally coherent semantic labelling and 3D reconstruction of dynamic scenes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Mustafa_Semantically_Coherent_Co-Segmentation_CVPR_2017_paper.pdf",
        "aff": "CVSSP, University of Surrey, United Kingdom; CVSSP, University of Surrey, United Kingdom",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Mustafa_Semantically_Coherent_Co-Segmentation_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1792272,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1399584901922637491&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "surrey.ac.uk;surrey.ac.uk",
        "email": "surrey.ac.uk;surrey.ac.uk",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Mustafa_Semantically_Coherent_Co-Segmentation_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Surrey",
        "aff_unique_dep": "CVSSP",
        "aff_unique_url": "https://www.surrey.ac.uk",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Semantically Consistent Regularization for Zero-Shot Recognition",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2650",
        "author_site": "Pedro Morgado, Nuno Vasconcelos",
        "author": "Pedro Morgado; Nuno Vasconcelos",
        "abstract": "The role of semantics in zero-shot learning is considered. The effectiveness of previous approaches is analyzed according to the form of supervision provided. While some learn semantics independently, others only supervise the semantic subspace explained by training classes. Thus, the former is able to constrain the whole space but lacks the ability to model semantic correlations. The latter addresses this issue but leaves part of the semantic space unsupervised. This complementarity is exploited in a new convolutional neural network (CNN) framework, which proposes the use of semantics as constraints for recognition.Although a CNN trained for classification has no transfer ability, this can be encouraged by learning an hidden semantic layer together with a semantic code for classification. Two forms of semantic constraints are then introduced. The first is a loss-based regularizer that introduces a generalization constraint on each semantic predictor. The second is a codeword regularizer that favors semantic-to-class mappings consistent with prior semantic knowledge while allowing these to be learned from data. Significant improvements over the state-of-the-art are achieved on several datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Morgado_Semantically_Consistent_Regularization_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03039v1",
        "gs_citation": 167,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7940680561462833637&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Morgado_Semantically_Consistent_Regularization_CVPR_2017_paper.html"
    },
    {
        "title": "Semi-Calibrated Near Field Photometric Stereo",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "311",
        "author_site": "Fotios Logothetis, Roberto Mecca, Roberto Cipolla",
        "author": "Fotios Logothetis; Roberto Mecca; Roberto Cipolla",
        "abstract": "3D reconstruction from shading information through Photometric Stereo is considered a very challenging problem in Computer Vision. Although this technique can potentially provide highly detailed shape recovery, its accuracy is critically dependent on a numerous set of factors among them the reliability of the light sources in emitting a constant amount of light. In this work, we propose a novel variational approach to solve the so called semi-calibrated near field Photometric Stereo problem, where the positions but not the brightness of the light sources are known. Additionally, we take into account realistic modeling features such as perspective viewing geometry and heterogeneous scene composition, containing both diffuse and specular objects. Furthermore, we also relax the point light source assumption that usually constraints the near field formulation by explicitly calculating the light attenuation maps. Synthetic experiments are performed for quantitative evaluation for a wide range of cases whilst real experiments provide comparisons, qualitatively outperforming the state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Logothetis_Semi-Calibrated_Near_Field_CVPR_2017_paper.pdf",
        "aff": "Department of Engineering, University of Cambridge, United Kingdom + Department of Mathematics, University of Bologna, Italy; Department of Engineering, University of Cambridge, United Kingdom + Department of Mathematics, University of Bologna, Italy; Department of Engineering, University of Cambridge, United Kingdom",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Logothetis_Semi-Calibrated_Near_Field_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2175991,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3973034056760562866&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Logothetis_Semi-Calibrated_Near_Field_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0",
        "aff_unique_norm": "University of Cambridge;University of Bologna",
        "aff_unique_dep": "Department of Engineering;Department of Mathematics",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.unibo.it",
        "aff_unique_abbr": "Cambridge;Unibo",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0+1;0+1;0",
        "aff_country_unique": "United Kingdom;Italy"
    },
    {
        "title": "Semi-Supervised Deep Learning for Monocular Depth Map Prediction",
        "session": "Machine Learning 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "3050",
        "author_site": "Yevhen Kuznietsov, J\u00c3\u00b6rg St\u00c3\u00bcckler, Bastian Leibe",
        "author": "Yevhen Kuznietsov; Jorg Stuckler; Bastian Leibe",
        "abstract": "Supervised deep learning often suffers from the lack of sufficient training data. Specifically in the context of monocular depth map prediction, it is barely possible to determine dense ground truth depth images in realistic dynamic outdoor environments. When using LiDAR sensors, for instance, noise is present in the distance measurements, the calibration between sensors cannot be perfect, and the measurements are typically much sparser than the camera images. In this paper, we propose a novel approach to depth map prediction from monocular images that learns in a semi-supervised way. While we use sparse ground-truth depth for supervised learning, we also enforce our deep network to produce photoconsistent dense depth maps in a stereo setup using a direct image alignment loss. In experiments we demonstrate superior performance in depth map prediction from single images compared to the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kuznietsov_Semi-Supervised_Deep_Learning_CVPR_2017_paper.pdf",
        "aff": "Computer Vision Group, Visual Computing Institute, RWTH Aachen University; Computer Vision Group, Visual Computing Institute, RWTH Aachen University; Computer Vision Group, Visual Computing Institute, RWTH Aachen University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kuznietsov_Semi-Supervised_Deep_Learning_2017_CVPR_supplemental.pdf",
        "arxiv": "1702.02706v3",
        "pdf_size": 4990386,
        "gs_citation": 897,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6028523748057348493&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de",
        "email": "rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kuznietsov_Semi-Supervised_Deep_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "RWTH Aachen University",
        "aff_unique_dep": "Visual Computing Institute",
        "aff_unique_url": "https://www.rwth-aachen.de",
        "aff_unique_abbr": "RWTH",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Aachen",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Sequential Person Recognition in Photo Albums With a Recurrent Network",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "489",
        "author_site": "Yao Li, Guosheng Lin, Bohan Zhuang, Lingqiao Liu, Chunhua Shen, Anton van den Hengel",
        "author": "Yao Li; Guosheng Lin; Bohan Zhuang; Lingqiao Liu; Chunhua Shen; Anton van den Hengel",
        "abstract": "Recognizing the identities of people in everyday photos is still a very challenging problem for machine vision, due to issues such as non-frontal faces, changes in clothing, location, lighting. Recent studies have shown that rich relational information between people in the same photo can help in recognizing their identities. In this work, we propose to model the relational information between people as a sequence prediction task. At the core of our work is a novel recurrent network architecture, in which relational information between instances' labels and appearance are modeled jointly. In addition to relational cues, scene context is incorporated in our sequence prediction model with no additional cost. In this sense, our approach is a unified framework for modeling both contextual cues and visual appearance of person instances. Our model is trained end-to-end with a sequence of annotated instances in a photo as inputs, and a sequence of corresponding labels as targets. We demonstrate that this simple but elegant formulation achieves state-of-the-art performance on the newly released People In Photo Albums (PIPA) dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Sequential_Person_Recognition_CVPR_2017_paper.pdf",
        "aff": "The University of Adelaide; Nanyang Technological University + The University of Adelaide; The University of Adelaide; The University of Adelaide; The University of Adelaide; The University of Adelaide",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.09967v1",
        "pdf_size": 2912071,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17246434640780690519&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "adelaide.edu.au;ntu.edu.sg;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;ntu.edu.sg;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Sequential_Person_Recognition_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+0;0;0;0;0",
        "aff_unique_norm": "University of Adelaide;Nanyang Technological University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.adelaide.edu.au;https://www.ntu.edu.sg",
        "aff_unique_abbr": "Adelaide;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0;0;0;0",
        "aff_country_unique": "Australia;Singapore"
    },
    {
        "title": "Shading Annotations in the Wild",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3260",
        "author_site": "Balazs Kovacs, Sean Bell, Noah Snavely, Kavita Bala",
        "author": "Balazs Kovacs; Sean Bell; Noah Snavely; Kavita Bala",
        "abstract": "Understanding shading effects in images is critical for a variety of vision and graphics problems, including intrinsic image decomposition, shadow removal, image relighting, and inverse rendering. As is the case with other vision tasks, machine learning is a promising approach to understanding shading - but there is little ground truth shading data available for real-world images. We introduce Shading Annotations in the Wild (SAW), a new large-scale, public dataset of shading annotations in indoor scenes, comprised of multiple forms of shading judgments obtained via crowdsourcing, along with shading annotations automatically generated from RGB-D imagery. We use this data to train a convolutional neural network to predict per-pixel shading information in an image. We demonstrate the value of our data and network in an application to intrinsic images, where we can reduce decomposition artifacts produced by existing algorithms. Our database is available at http://opensurfaces.cs.cornell.edu/saw/.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kovacs_Shading_Annotations_in_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kovacs_Shading_Annotations_in_2017_CVPR_supplemental.pdf",
        "arxiv": "1705.01156v1",
        "pdf_size": 1385835,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8558658250837344233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kovacs_Shading_Annotations_in_CVPR_2017_paper.html"
    },
    {
        "title": "Shape Completion Using 3D-Encoder-Predictor CNNs and Shape Synthesis",
        "session": "Machine Learning for 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "2519",
        "author_site": "Angela Dai, Charles Ruizhongtai Qi, Matthias Nie\u00c3\u009fner",
        "author": "Angela Dai; Charles Ruizhongtai Qi; Matthias Niessner",
        "abstract": "We introduce a data-driven approach to complete partial 3D shapes through a combination of volumetric deep neural networks and 3D shape synthesis. From a partially-scanned input shape, our method first infers a low-resolution -- but complete -- output. To this end, we introduce a 3D-Encoder-Predictor Network (3D-EPN) which is composed of 3D convolutional layers. The network is trained to predict and fill in missing data, and operates on an implicit surface representation that encodes both known and unknown space. This allows us to predict global structure in unknown areas at high accuracy. We then correlate these intermediary results with 3D geometry from a shape database at test time. In a final pass, we propose a patch-based 3D shape synthesis method that imposes the 3D geometry from these retrieved shapes as constraints on the coarsely-completed mesh.  This synthesis process enables us to reconstruct fine-scale detail and generate high-resolution output while respecting the global mesh structure obtained by the 3D-EPN. Although our 3D-EPN outperforms state-of-the-art completion method, the main contribution in our work lies in the combination of a data-driven shape predictor and analytic 3D shape synthesis. In our results, we show extensive evaluations on a newly-introduced shape completion benchmark for both real-world and synthetic data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dai_Shape_Completion_Using_CVPR_2017_paper.pdf",
        "aff": "Stanford University; Stanford University; Stanford University + Technical University of Munich",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Dai_Shape_Completion_Using_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1681119,
        "gs_citation": 804,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4295806478094597130&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dai_Shape_Completion_Using_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Stanford University;Technical University of Munich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.tum.de",
        "aff_unique_abbr": "Stanford;TUM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "ShapeOdds: Variational Bayesian Learning of Generative Shape Models",
        "session": "Machine Learning 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "804",
        "author_site": "Shireen Elhabian, Ross Whitaker",
        "author": "Shireen Elhabian; Ross Whitaker",
        "abstract": "Shape models provide a compact parameterization of a class of shapes, and have been shown to be important to a variety of vision problems, including object detection, tracking, and image segmentation. Learning generative shape models from grid-structured representations, aka silhouettes, is usually hindered by (1) data likelihoods with intractable marginals and posteriors, (2) high-dimensional shape spaces with limited training samples (and the associated risk of overfitting), and (3) estimation of hyperparameters relating to model complexity that often entails computationally expensive grid searches. In this paper, we propose a Bayesian treatment that relies on direct probabilistic formulation for learning generative shape models in the silhouettes space. We propose a variational approach for learning a latent variable model in which we make use of, and extend, recent works on variational bounds of logistic-Gaussian integrals to circumvent intractable marginals and posteriors. Spatial coherency and sparsity priors are also incorporated to lend stability to the optimization problem by regularizing the solution space while avoiding overfitting in this high-dimensional, low-sample-size scenario. We deploy a type-II maximum likelihood estimate of the model hyperparameters to avoid grid searches. We demonstrate that the proposed model generates realistic samples, generalizes to unseen examples, and is able to handle missing regions and/or background clutter, while comparing favorably with recent, neural-network-based approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Elhabian_ShapeOdds_Variational_Bayesian_CVPR_2017_paper.pdf",
        "aff": "Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, UT, USA; Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, UT, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2076197,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15982949427771027065&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "sci.utah.edu;sci.utah.edu",
        "email": "sci.utah.edu;sci.utah.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Elhabian_ShapeOdds_Variational_Bayesian_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Utah",
        "aff_unique_dep": "Scientific Computing and Imaging Institute",
        "aff_unique_url": "https://www.sci.utah.edu",
        "aff_unique_abbr": "Utah SCI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Salt Lake City",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Simple Does It: Weakly Supervised Instance and Semantic Segmentation",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "286",
        "author_site": "Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, Bernt Schiele",
        "author": "Anna Khoreva; Rodrigo Benenson; Jan Hosang; Matthias Hein; Bernt Schiele",
        "abstract": "Semantic labelling and instance segmentation are two tasks that require particularly costly annotations. Starting from weak supervision in the form of bounding box detection annotations, we propose a new approach that does not require modification of the segmentation training procedure. We show that when carefully designing the input labels from given bounding boxes, even a single round of training is enough to improve over previously reported weakly supervised results. Overall, our weak supervision approach reaches  95% of the quality of the fully supervised model, both for semantic labelling and instance segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Khoreva_Simple_Does_It_CVPR_2017_paper.pdf",
        "aff": "Max Planck Institute for Informatics, Saarbr\u00fccken, Germany; Max Planck Institute for Informatics, Saarbr\u00fccken, Germany; Max Planck Institute for Informatics, Saarbr\u00fccken, Germany; Saarland University, Saarbr\u00fccken, Germany; Max Planck Institute for Informatics, Saarbr\u00fccken, Germany",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Khoreva_Simple_Does_It_2017_CVPR_supplemental.pdf",
        "arxiv": "1603.07485v2",
        "pdf_size": 1053083,
        "gs_citation": 971,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=146460674909646908&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Khoreva_Simple_Does_It_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Max Planck Institute for Informatics;Saarland University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://mpi-inf.mpg.de;https://www.uni-saarland.de",
        "aff_unique_abbr": "MPII;UdS",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Saarbr\u00fccken",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Simultaneous Facial Landmark Detection, Pose and Deformation Estimation Under Facial Occlusion",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1281",
        "author_site": "Yue Wu, Chao Gou, Qiang Ji",
        "author": "Yue Wu; Chao Gou; Qiang Ji",
        "abstract": "Facial landmark detection, head pose estimation, and facial deformation analysis are typical facial behavior analysis tasks in computer vision. The existing methods usually perform each task independently and sequentially, ignoring their interactions. To tackle this problem, we propose a unified framework for simultaneous facial landmark detection, head pose estimation, and facial deformation analysis, and the proposed model is robust to facial occlusion. Following a cascade procedure augmented with model-based head pose estimation, we iteratively update the facial landmark locations, facial occlusion, head pose and facial deformation until convergence. The experimental results on benchmark databases demonstrate the effectiveness of the proposed method for simultaneous facial landmark detection, head pose and facial deformation estimation, even if the images are under facial occlusion.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wu_Simultaneous_Facial_Landmark_CVPR_2017_paper.pdf",
        "aff": "ECSE Department, Rensselaer Polytechnic Institute; Institute of Automation, Chinese Academy of Sciences; ECSE Department, Rensselaer Polytechnic Institute",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1709.08130v1",
        "pdf_size": 945547,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=896446059052713015&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;ic.ac.cn;rpi.edu",
        "email": "gmail.com;ic.ac.cn;rpi.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wu_Simultaneous_Facial_Landmark_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute;Chinese Academy of Sciences",
        "aff_unique_dep": "ECSE Department;Institute of Automation",
        "aff_unique_url": "https://www.rpi.edu;http://www.ia.cas.cn",
        "aff_unique_abbr": "RPI;CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Simultaneous Feature Aggregating and Hashing for Large-Scale Image Search",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3012",
        "author_site": "Thanh-Toan Do, Dang-Khoa Le Tan, Trung T. Pham, Ngai-Man Cheung",
        "author": "Thanh-Toan Do; Dang-Khoa Le Tan; Trung T. Pham; Ngai-Man Cheung",
        "abstract": "In most state-of-the-art hashing-based visual search systems, local image descriptors of an image are first aggregated as a single feature vector. This feature vector is then subjected to a hashing function that produces a binary hash code. In previous work, the aggregating and the hashing processes are designed independently. In this paper, we propose a novel framework where feature aggregating and hashing are designed simultaneously and optimized jointly. Specifically, our joint optimization produces aggregated representations that can be better reconstructed by some binary codes.  This leads to more discriminative binary hash codes and improved retrieval accuracy. In addition, we also propose a fast version of the recently-proposed Binary Autoencoder to be used in our proposed framework. We perform extensive retrieval experiments on several benchmark datasets with both SIFT and convolutional features. Our results suggest that the proposed framework achieves significant improvements over the state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Do_Simultaneous_Feature_Aggregating_CVPR_2017_paper.pdf",
        "aff": "The University of Adelaide; Singapore University of Technology and Design; The University of Adelaide; Singapore University of Technology and Design",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.00860",
        "pdf_size": 621812,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14144652328022676263&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "adelaide.edu.au;sutd.edu.sg;adelaide.edu.au;sutd.edu.sg",
        "email": "adelaide.edu.au;sutd.edu.sg;adelaide.edu.au;sutd.edu.sg",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Do_Simultaneous_Feature_Aggregating_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Adelaide;Singapore University of Technology and Design",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.adelaide.edu.au;https://www.sutd.edu.sg",
        "aff_unique_abbr": "Adelaide;SUTD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "Australia;Singapore"
    },
    {
        "title": "Simultaneous Geometric and Radiometric Calibration of a Projector-Camera Pair",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2017",
        "author_site": "Marjan Shahpaski, Luis Ricardo Sapaico, Gaspard Chevassus, Sabine S\u00c3\u00bcsstrunk",
        "author": "Marjan Shahpaski; Luis Ricardo Sapaico; Gaspard Chevassus; Sabine Susstrunk",
        "abstract": "We present a novel method that allows for simultaneous geometric and radiometric calibration of a projector-camera pair. It is simple and does not require specialized hardware. We prewarp and align a specially designed projection pattern onto a printed pattern of different colorimetric properties. After capturing the patterns in several orientations, we perform geometric calibration by estimating the corner locations of the two patterns in different color channels. We perform radiometric calibration of the projector by using the information contained inside the projected squares. We show that our method performs on par with current approaches that all require separate geometric and radiometric calibration, while being more efficient and user friendly.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shahpaski_Simultaneous_Geometric_and_CVPR_2017_paper.pdf",
        "aff": "School of Computer and Communication Sciences, EPFL; Oc \u00e9 Print Logic Technologies S.A.; School of Computer and Communication Sciences, EPFL; School of Computer and Communication Sciences, EPFL",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2598958,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15780445445399818679&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "epfl.ch;oce.com;epfl.ch;epfl.ch",
        "email": "epfl.ch;oce.com;epfl.ch;epfl.ch",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shahpaski_Simultaneous_Geometric_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Ecole Polytechnique Federale de Lausanne;Print Logic Technologies S.A.",
        "aff_unique_dep": "School of Computer and Communication Sciences;",
        "aff_unique_url": "https://www.epfl.ch;",
        "aff_unique_abbr": "EPFL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland;"
    },
    {
        "title": "Simultaneous Stereo Video Deblurring and Scene Flow Estimation",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1776",
        "author_site": "Liyuan Pan, Yuchao Dai, Miaomiao Liu, Fatih Porikli",
        "author": "Liyuan Pan; Yuchao Dai; Miaomiao Liu; Fatih Porikli",
        "abstract": "Videos for outdoor scene often show unpleasant blur effects due to the large relative motion between the camera and the dynamic objects and large depth variations. Existing works typically focus monocular video deblurring. In this paper, we propose a novel approach to deblurring from stereo videos. In particular, we exploit the piece-wise planar assumption about the scene and leverage the scene flow information to deblur the image. Unlike the existing approach [31] which used a pre-computed scene flow, we propose a single framework to jointly estimate the scene flow and deblur the image, where the motion cues from scene flow estimation and blur information could reinforce each other, and produce superior results than the conventional scene flow estimation or stereo deblurring methods. We evaluate our method extensively on two available datasets and achieve significant improvement in flow estimation and removing the blur effect over the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Pan_Simultaneous_Stereo_Video_CVPR_2017_paper.pdf",
        "aff": "School of Automation, Northwestern Polytechnical University, Xi\u2019an, China + Research School of Engineering, Australian National University, Canberra, Australia; Research School of Engineering, Australian National University, Canberra, Australia; Data61, CSIRO, Canberra, Australia + Research School of Engineering, Australian National University, Canberra, Australia; Research School of Engineering, Australian National University, Canberra, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03273v1",
        "pdf_size": 2632359,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1873951331207917132&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "mail.nwpu.edu.cn;anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "mail.nwpu.edu.cn;anu.edu.au;anu.edu.au;anu.edu.au",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Pan_Simultaneous_Stereo_Video_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;2+1;1",
        "aff_unique_norm": "Northwestern Polytechnical University;Australian National University;CSIRO",
        "aff_unique_dep": "School of Automation;Research School of Engineering;Data61",
        "aff_unique_url": "https://www.nwpu.edu.cn;https://www.anu.edu.au;https://www.csiro.au",
        "aff_unique_abbr": "NPU;ANU;CSIRO",
        "aff_campus_unique_index": "0+1;1;1+1;1",
        "aff_campus_unique": "Xi'an;Canberra",
        "aff_country_unique_index": "0+1;1;1+1;1",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Images Using Weakly-Supervised Joint Convolutional Sparse Coding",
        "session": "Biomedical Image/Video Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "2661",
        "author_site": "Yawen Huang, Ling Shao, Alejandro F. Frangi",
        "author": "Yawen Huang; Ling Shao; Alejandro F. Frangi",
        "abstract": "Magnetic Resonance Imaging (MRI) offers high-resolution in vivo imaging and rich functional and anatomical multimodality tissue contrast. In practice, however, there are challenges associated with considerations of scanning costs, patient comfort, and scanning time that constrain how much data can be acquired in clinical or research studies. In this paper, we explore the possibility of generating high-resolution and multimodal images from low-resolution single-modality imagery. We propose the weakly-supervised joint convolutional sparse coding to simultaneously solve the problems of super-resolution (SR) and cross-modality image synthesis. The learning process requires only a few registered multimodal image pairs as the training set. Additionally, the quality of the joint dictionary learning can be improved using a larger set of unpaired images. To combine unpaired data from different image resolutions/modalities, a hetero-domain image alignment term is proposed. Local image neighborhoods are naturally preserved by operating on the whole image domain (as opposed to image patches) and using joint convolutional sparse coding. The paired images are enhanced in the joint learning process with unpaired data and an additional maximum mean discrepancy term, which minimizes the dissimilarity between their feature distributions. Experiments show that the proposed method outperforms state-of-the-art techniques on both SR reconstruction and simultaneous SR and cross-modality synthesis.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Simultaneous_Super-Resolution_and_CVPR_2017_paper.pdf",
        "aff": "Department of Electronic and Electrical Engineering, The University of Sheffield, UK; School of Computing Sciences, University of East Anglia, UK; Department of Electronic and Electrical Engineering, The University of Sheffield, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.02596",
        "pdf_size": 2784620,
        "gs_citation": 251,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17768273071738512542&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "sheffield.ac.uk;uea.ac.uk;sheffield.ac.uk",
        "email": "sheffield.ac.uk;uea.ac.uk;sheffield.ac.uk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Simultaneous_Super-Resolution_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Sheffield;University of East Anglia",
        "aff_unique_dep": "Department of Electronic and Electrical Engineering;School of Computing Sciences",
        "aff_unique_url": "https://www.sheffield.ac.uk;https://www.uea.ac.uk",
        "aff_unique_abbr": "Sheffield;UEA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Simultaneous Visual Data Completion and Denoising Based on Tensor Rank and Total Variation Minimization and Its Primal-Dual Splitting Algorithm",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1387",
        "author_site": "Tatsuya Yokota, Hidekata Hontani",
        "author": "Tatsuya Yokota; Hidekata Hontani",
        "abstract": "Tensor completion has attracted attention because of its promising ability and generality.  However, there are few studies on noisy scenarios which directly solve an optimization problem consisting of a \"noise inequality constraint\".  In this paper, we propose a new tensor completion and denoising model including tensor total variation and tensor nuclear norm minimization with a range of values and noise inequalities.  Furthermore, we developed its solution algorithm based on a primal-dual splitting method, which is computationally efficient as compared to tensor decomposition based non-convex optimization.  Lastly, extensive experiments demonstrated the advantages of the proposed method for visual data retrieval such as for color images, movies, and 3D-volumetric data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yokota_Simultaneous_Visual_Data_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2552660,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2433453861337131396&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yokota_Simultaneous_Visual_Data_CVPR_2017_paper.html"
    },
    {
        "title": "Single Image Reflection Suppression",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1825",
        "author_site": "Nikolaos Arvanitopoulos, Radhakrishna Achanta, Sabine S\u00c3\u00bcsstrunk",
        "author": "Nikolaos Arvanitopoulos; Radhakrishna Achanta; Sabine Susstrunk",
        "abstract": "Reflections are a common artifact in images taken through glass windows. Automatically removing the reflection artifacts after the picture is taken is an ill-posed problem. Attempts to solve this problem using optimization schemes therefore rely on various prior assumptions from the physical world. Instead of removing reflections from a single image, which has met with limited success so far, we propose a novel approach to suppress reflections. It is based on a Laplacian data fidelity term and an l-zero gradient sparsity term imposed on the output. With experiments on artificial and real-world images we show that our reflection suppression method performs better than the state-of-the-art reflection removal techniques.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Arvanitopoulos_Single_Image_Reflection_CVPR_2017_paper.pdf",
        "aff": "School of Computer and Communication Sciences (IC) \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL), Switzerland; School of Computer and Communication Sciences (IC) \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL), Switzerland; School of Computer and Communication Sciences (IC) \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL), Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1155750,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17077753693012716927&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Arvanitopoulos_Single_Image_Reflection_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "School of Computer and Communication Sciences",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3491",
        "author_site": "Yufei Wang, Zhe Lin, Xiaohui Shen, Scott Cohen, Garrison W. Cottrell",
        "author": "Yufei Wang; Zhe Lin; Xiaohui Shen; Scott Cohen; Garrison W. Cottrell",
        "abstract": "Recently, there has been a lot of interest in automatically generating descriptions for an image. Most existing language-model based approaches for this task learn to generate an image description word by word in its original word order. However, for humans, it is more natural to locate the objects and their relationships first, and then elaborate on each object, describing notable attributes. We present a coarse-to-fine method that decomposes the original image description into a skeleton sentence and its attributes, and generates the skeleton sentence and attribute phrases separately. By this decomposition, our method can generate more accurate and novel descriptions than the previous state-of-the-art. Experimental results on the MS-COCO and a larger scale Stock3M datasets show that our algorithm yields consistent improvements across different evaluation metrics, especially on the SPICE metric, which has much higher correlation with human ratings than the conventional metrics. Furthermore, our algorithm can generate descriptions with varied length, benefiting from the separate control of the skeleton and attributes. This enables image description generation that better accommodates user preferences.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Skeleton_Key_Image_CVPR_2017_paper.pdf",
        "aff": "University of California, San Diego; Adobe Research; Adobe Research; Adobe Research; University of California, San Diego",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Wang_Skeleton_Key_Image_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.06972v1",
        "pdf_size": 1175905,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2569530831106621415&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ucsd.edu;adobe.com;adobe.com;adobe.com;ucsd.edu",
        "email": "ucsd.edu;adobe.com;adobe.com;adobe.com;ucsd.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Skeleton_Key_Image_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of California, San Diego;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.ucsd.edu;https://research.adobe.com",
        "aff_unique_abbr": "UCSD;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Slow Flow: Exploiting High-Speed Cameras for Accurate and Diverse Optical Flow Reference Data",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Oral",
        "track": "main",
        "pid": "1321",
        "author_site": "Joel Janai, Fatma G\u00c3\u00bcney, Jonas Wulff, Michael J. Black, Andreas Geiger",
        "author": "Joel Janai; Fatma Guney; Jonas Wulff; Michael J. Black; Andreas Geiger",
        "abstract": "Existing optical flow datasets are limited in size and variability due to the difficulty of capturing dense ground truth. In this paper, we tackle this problem by tracking pixels through densely sampled space-time volumes recorded with a high-speed video camera. Our model exploits the linearity of small motions and reasons about occlusions from multiple frames. Using our technique, we are able to establish accurate reference flow fields outside the laboratory in natural environments. Besides, we show how our predictions can be used to augment the input images with realistic motion blur. We demonstrate the quality of the produced flow fields on synthetic and real-world datasets. Finally, we collect a novel challenging optical flow dataset by applying our technique on data from a high-speed camera and analyze the performance of the state-of-the-art in optical flow under various levels of motion blur.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Janai_Slow_Flow_Exploiting_CVPR_2017_paper.pdf",
        "aff": "Autonomous Vision Group, MPI for Intelligent Systems T\u00a8ubingen; Autonomous Vision Group, MPI for Intelligent Systems T\u00a8ubingen; Perceiving Systems Department, MPI for Intelligent Systems T\u00a8ubingen; Perceiving Systems Department, MPI for Intelligent Systems T\u00a8ubingen; Autonomous Vision Group, MPI for Intelligent Systems T\u00a8ubingen + Computer Vision and Geometry Group, ETH Z\u00a8urich",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Janai_Slow_Flow_Exploiting_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1639677,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=288366238493764555&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "tue.mpg.de;tue.mpg.de;tue.mpg.de;tue.mpg.de;tue.mpg.de",
        "email": "tue.mpg.de;tue.mpg.de;tue.mpg.de;tue.mpg.de;tue.mpg.de",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Janai_Slow_Flow_Exploiting_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;ETH Zurich",
        "aff_unique_dep": "Autonomous Vision Group;Computer Vision and Geometry Group",
        "aff_unique_url": "https://www.mpituebingen.mpg.de;https://www.ethz.ch",
        "aff_unique_abbr": "MPI-IS;ETHZ",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "T\u00fcbingen;",
        "aff_country_unique_index": "0;0;0;0;0+1",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "title": "Snapshot Hyperspectral Light Field Imaging",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "1219",
        "author_site": "Zhiwei Xiong, Lizhi Wang, Huiqun Li, Dong Liu, Feng Wu",
        "author": "Zhiwei Xiong; Lizhi Wang; Huiqun Li; Dong Liu; Feng Wu",
        "abstract": "This paper presents the first snapshot hyperspectral light field imager in practice. Specifically, we design a novel hybrid camera system to obtain two complementary measurements that sample the angular and spectral dimensions respectively. To recover the full 5D hyperspectral light field from the severely undersampled measurements, we then propose an efficient computational reconstruction algorithm by exploiting the large correlations across the angular and spectral dimensions through self-learned dictionaries. Simulation on an elaborate hyperspectral light field dataset validates the effectiveness of the proposed approach. Hardware experimental results demonstrate that, for the first time to our knowledge, a 5D hyperspectral light field containing 9x9 angular views and 27 spectral bands can be acquired in a single shot.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xiong_Snapshot_Hyperspectral_Light_CVPR_2017_paper.pdf",
        "aff": "University of Science and Technology of China; Beijing Institute of Technology; University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1234719,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5540613979750828196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xiong_Snapshot_Hyperspectral_Light_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "University of Science and Technology of China;Beijing Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.ustc.edu.cn;http://www.bit.edu.cn/",
        "aff_unique_abbr": "USTC;BIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Social Scene Understanding: End-To-End Multi-Person Action Localization and Collective Activity Recognition",
        "session": "Analyzing Humans 2",
        "status": "Oral",
        "track": "main",
        "pid": "1744",
        "author_site": "Timur Bagautdinov, Alexandre Alahi, Fran\u00c3\u00a7ois Fleuret, Pascal Fua, Silvio Savarese",
        "author": "Timur Bagautdinov; Alexandre Alahi; Francois Fleuret; Pascal Fua; Silvio Savarese",
        "abstract": "We present a unified framework for understanding human social behaviors in raw image sequences. Our model jointly detects multiple individuals, infers their social actions, and estimates the collective actions with a single feed-forward pass through a neural network. We propose a single architecture that does not rely on external detection algorithms but rather is trained end-to-end to generate dense proposal maps that are refined via a novel inference scheme. The temporal consistency is handled via a person-level matching Recurrent Neural Network. The complete model takes as input a sequence of frames and outputs detections along with the estimates of individual actions and collective activities. We demonstrate state-of-the-art performance of our algorithm on multiple publicly available benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bagautdinov_Social_Scene_Understanding_CVPR_2017_paper.pdf",
        "aff": "\u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne (EPFL) + IDIAP Research Institute; Stanford University; \u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne (EPFL) + IDIAP Research Institute; \u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne (EPFL); Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.09078v1",
        "pdf_size": 849803,
        "gs_citation": 296,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4279496718362240840&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "epfl.ch;stanford.edu;epfl.ch;epfl.ch;stanford.edu",
        "email": "epfl.ch;stanford.edu;epfl.ch;epfl.ch;stanford.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bagautdinov_Social_Scene_Understanding_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2;0+1;0;2",
        "aff_unique_norm": "EPFL;Idiap Research Institute;Stanford University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.epfl.ch;https://www.idiap.ch;https://www.stanford.edu",
        "aff_unique_abbr": "EPFL;IDIAP;Stanford",
        "aff_campus_unique_index": "0;2;0;0;2",
        "aff_campus_unique": "Lausanne;;Stanford",
        "aff_country_unique_index": "0+0;1;0+0;0;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "Soft-Margin Mixture of Regressions",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2951",
        "author_site": "Dong Huang, Longfei Han, Fernando De la Torre",
        "author": "Dong Huang; Longfei Han; Fernando De la Torre",
        "abstract": "Nonlinear regression is a common statistical tool to solve many computer vision problems (e.g., age estimation, pose estimation).  Existing approaches to nonlinear regression fall into two main categories: (1) The universal approach provides an implicit or explicit homogeneous feature mapping (e.g., kernel ridge regression, Gaussian process regression, neural networks). These approaches may fail when data is heterogeneous or discontinuous. (2) Divide-and-conquer approaches partition a heterogeneous input feature space and learn multiple local regressors. However, existing divide-and-conquer approaches fail to deal with discontinuities between partitions (e.g., Gaussian mixture of regressions) and they cannot guarantee that the partitioned input space will be homogeneously modeled by local regressors (e.g., ordinal regression). To address these issues, this paper proposes Soft-Margin Mixture of Regressions (SMMR), a method that directly learns homogeneous partitions of the input space and is able to deal with discontinuities. SMMR outperforms the state-of-the-art methods on three popular computer vision tasks: age estimation, crowd counting and viewpoint estimation from images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Soft-Margin_Mixture_of_CVPR_2017_paper.pdf",
        "aff": "Carnegie Mellon University; Beijing Institute of Technology + Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4877987,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17240193824246674338&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cmu.edu;hotmail.com;cs.cmu.edu",
        "email": "cmu.edu;hotmail.com;cs.cmu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Soft-Margin_Mixture_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Carnegie Mellon University;Beijing Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;http://www.bit.edu.cn/",
        "aff_unique_abbr": "CMU;BIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Spatial-Semantic Image Search by Visual Feature Synthesis",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Spotlight",
        "track": "main",
        "pid": "1962",
        "author_site": "Long Mai, Hailin Jin, Zhe Lin, Chen Fang, Jonathan Brandt, Feng Liu",
        "author": "Long Mai; Hailin Jin; Zhe Lin; Chen Fang; Jonathan Brandt; Feng Liu",
        "abstract": "The performance of image retrieval has been improved tremendously in recent years through the use of deep feature representations. Most existing methods, however, aim to retrieve images that are visually similar or semantically relevant to the query, irrespective of spatial configuration. In this paper, we develop a spatial-semantic image search technology that enables users to search for images with both semantic and spatial constraints by manipulating concept text-boxes on a 2D query canvas. We train a convolutional neural network to synthesize appropriate visual features that captures the spatial-semantic constraints from the user canvas query. We directly optimize the retrieval performance of the visual features when training our deep neural network. These visual features then are used to retrieve images that are both spatially and semantically relevant to the user query. The experiments on large-scale datasets such as MS-COCO and Visual Genome show that our method outperforms other baseline and state-of-the-art methods in spatial-semantic image search.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Mai_Spatial-Semantic_Image_Search_CVPR_2017_paper.pdf",
        "aff": "Portland State University; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Portland State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2242688,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3681476810273946680&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.pdx.com;cs.pdx.com;adobe.com;adobe.com;adobe.com;adobe.com",
        "email": "cs.pdx.com;cs.pdx.com;adobe.com;adobe.com;adobe.com;adobe.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Mai_Spatial-Semantic_Image_Search_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "Portland State University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.pdx.edu;https://research.adobe.com",
        "aff_unique_abbr": "PSU;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Spatially Adaptive Computation Time for Residual Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "360",
        "author_site": "Michael Figurnov, Maxwell D. Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, Ruslan Salakhutdinov",
        "author": "Michael Figurnov; Maxwell D. Collins; Yukun Zhu; Li Zhang; Jonathan Huang; Dmitry Vetrov; Ruslan Salakhutdinov",
        "abstract": "This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Figurnov_Spatially_Adaptive_Computation_CVPR_2017_paper.pdf",
        "aff": "National Research University Higher School of Economics; Google Inc.; Google Inc.; Google Inc.; Google Inc.; National Research University Higher School of Economics+Yandex; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Figurnov_Spatially_Adaptive_Computation_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.02297",
        "pdf_size": 1917764,
        "gs_citation": 431,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13202224366050669196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "figurnov.ru;google.com;google.com;google.com;google.com;yandex.ru;cs.cmu.edu",
        "email": "figurnov.ru;google.com;google.com;google.com;google.com;yandex.ru;cs.cmu.edu",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Figurnov_Spatially_Adaptive_Computation_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;1;0+2;3",
        "aff_unique_norm": "National Research University Higher School of Economics;Google;Yandex;Carnegie Mellon University",
        "aff_unique_dep": ";Google;;",
        "aff_unique_url": "https://hse.ru;https://www.google.com;https://yandex.com;https://www.cmu.edu",
        "aff_unique_abbr": "HSE;Google;Yandex;CMU",
        "aff_campus_unique_index": "1;1;1;1;",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;1;1;1;0+0;1",
        "aff_country_unique": "Russian Federation;United States"
    },
    {
        "title": "Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted Transform Coefficients of Gradient Magnitudes",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2497",
        "author_site": "S. Alireza Golestaneh, Lina J. Karam",
        "author": "S. Alireza Golestaneh; Lina J. Karam",
        "abstract": "The detection of spatially-varying blur without having any information about the blur type is a challenging task. In this paper, we propose a novel effective approach to address this blur detection problem from a single image without requiring any knowledge about the blur type, level, or camera settings. Our approach computes blur detection maps based on a novel High-frequency multiscale Fusion and Sort Transform (HiFST) of gradient magnitudes.  The evaluations of the proposed approach on a diverse set of blurry images with different blur types, levels, and contents demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods qualitatively and quantitatively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Golestaneh_Spatially-Varying_Blur_Detection_CVPR_2017_paper.pdf",
        "aff": "Arizona State University; Arizona State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.07478",
        "pdf_size": 2176452,
        "gs_citation": 140,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6542991413556311642&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "asu.edu;asu.edu",
        "email": "asu.edu;asu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Golestaneh_Spatially-Varying_Blur_Detection_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Spatio-Temporal Alignment of Non-Overlapping Sequences From Independently Panning Cameras",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1442",
        "author_site": "Seyed Morteza Safdarnejad, Xiaoming Liu",
        "author": "Seyed Morteza Safdarnejad; Xiaoming Liu",
        "abstract": "This paper addresses the problem of spatio-temporal alignment of multiple video sequences. We identify and tackle a novel scenario of this problem referred to as Nonoverlapping Sequences (NOS). NOS are captured by multiple freely panning handheld cameras whose field of views (FOV) might have no direct spatial overlap. With the popularity of mobile sensors, NOS rise when multiple cooperative users capture a public event to create a panoramic video, or when consolidating multiple footages of an incident into a single video. To tackle this novel scenario, we first spatially align the sequences by reconstructing the background of each sequence and registering these backgrounds, even if the backgrounds are not overlapping. Given the spatial alignment, we temporally synchronize the sequences, such that the trajectories of moving objects (e.g., cars or pedestrians) are consistent across sequences. Experimental results demonstrate the performance of our algorithm in this novel and challenging scenario, quantitatively and qualitatively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Safdarnejad_Spatio-Temporal_Alignment_of_CVPR_2017_paper.pdf",
        "aff": "Michigan State University; Michigan State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2575867,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7026306817761716986&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff_domain": "egr.msu.edu;cse.msu.edu",
        "email": "egr.msu.edu;cse.msu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Safdarnejad_Spatio-Temporal_Alignment_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Spatio-Temporal Naive-Bayes Nearest-Neighbor (ST-NBNN) for Skeleton-Based Action Recognition",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "1698",
        "author_site": "Junwu Weng, Chaoqun Weng, Junsong Yuan",
        "author": "Junwu Weng; Chaoqun Weng; Junsong Yuan",
        "abstract": "Motivated by previous success of using non-parametric methods to recognize objects, e.g., NBNN, we extend it to recognize actions using skeletons. Each 3D action is presented by a sequence of 3D poses. Similar to NBNN, our proposed Spatio-Temporal-NBNN applies stage-to-class distance to classify actions. However, ST-NBNN takes the spatio-temporal structure of 3D actions into consideration and relaxes the Naive Bayes assumption of NBNN. Specifically, ST-NBNN adopts bilinear classifiers to identify both key temporal stages as well as spatial joints for action classification. Although only using a linear classifier, experiments on three benchmark datasets show that by combining the strength of both non-parametric and parametric models, ST-NBNN can achieve competitive performance compared with state-of-the-art results using sophisticated models such as deep learning. Moreover, by identifying key skeleton joints and temporal stages for each action class, our ST-NBNN can capture the essential spatio-temporal patterns that play key roles of recognizing actions, which is not always achievable by using end-to-end models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Weng_Spatio-Temporal_Naive-Bayes_Nearest-Neighbor_CVPR_2017_paper.pdf",
        "aff": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5194825,
        "gs_citation": 164,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14717434906044102686&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg",
        "email": "e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Weng_Spatio-Temporal_Naive-Bayes_Nearest-Neighbor_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Spatio-Temporal Self-Organizing Map Deep Network for Dynamic Object Detection From Videos",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "2321",
        "author_site": "Yang Du, Chunfeng Yuan, Bing Li, Weiming Hu, Stephen Maybank",
        "author": "Yang Du; Chunfeng Yuan; Bing Li; Weiming Hu; Stephen Maybank",
        "abstract": "In dynamic object detection, it is challenging to construct an effective model to sufficiently characterize the spatial-temporal properties of the background. This paper proposes a new Spatio-Temporal Self-Organizing Map (STSOM) deep network to detect dynamic objects in complex scenarios. The proposed approach has several contributions: First, a novel STSOM shared by all pixels in a video frame is presented to efficiently model complex background. We exploit the fact that the motions of complex background have the global variation in the space and the local variation in the time, to train STSOM using the whole frames and the sequence of a pixel over time to tackle the variance of complex background. Second, a Bayesian parameter estimation based method is presented to learn thresholds automatically for all pixels to filter out the background. Last, in order to model the complex background more accurately, we extend the single-layer STSOM to the deep network. Then the background is filtered out layer by layer. Experimental results on  CDnet 2014 dataset demonstrate that the proposed STSOM deep network outperforms numerous recently proposed methods in the overall performance and in most categories of scenarios.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Du_Spatio-Temporal_Self-Organizing_Map_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Du_Spatio-Temporal_Self-Organizing_Map_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 935430,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7729578724601780482&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Du_Spatio-Temporal_Self-Organizing_Map_CVPR_2017_paper.html"
    },
    {
        "title": "Spatio-Temporal Vector of Locally Max Pooled Features for Action Recognition in Videos",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1146",
        "author_site": "Ionut Cosmin Duta, Bogdan Ionescu, Kiyoharu Aizawa, Nicu Sebe",
        "author": "Ionut Cosmin Duta; Bogdan Ionescu; Kiyoharu Aizawa; Nicu Sebe",
        "abstract": "We introduce Spatio-Temporal Vector of Locally Max Pooled Features (ST-VLMPF), a super vector-based encoding method specifically designed for local deep features encoding.  The proposed method addresses an important problem of video understanding: how to build a video representation that incorporates the CNN features over the entire video. Feature assignment is carried out at two levels, by using the similarity and spatio-temporal information. For each assignment we build a specific encoding, focused on the nature of deep features,  with the goal to capture the highest feature responses from the highest neuron activation of the network. Our ST-VLMPF clearly provides a more reliable video representation than  some of the most widely used and powerful encoding approaches (Improved Fisher Vectors and Vector of Locally  Aggregated Descriptors), while maintaining a low computational complexity. We conduct experiments on three action recognition datasets: HMDB51, UCF50 and UCF101. Our pipeline obtains  state-of-the-art results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Duta_Spatio-Temporal_Vector_of_CVPR_2017_paper.pdf",
        "aff": "University of Trento, Italy; University Politehnica of Bucharest, Romania; University of Tokyo, Japan; University of Trento, Italy",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 648790,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16256084954195907836&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "unitn.it;imag.pub.ro;hal.t.u-tokyo.ac.jp;unitn.it",
        "email": "unitn.it;imag.pub.ro;hal.t.u-tokyo.ac.jp;unitn.it",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Duta_Spatio-Temporal_Vector_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Trento;University Politehnica of Bucharest;University of Tokyo",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unitn.it;https://www.upb.ro;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UniTN;UPB;UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "Italy;Romania;Japan"
    },
    {
        "title": "Spatiotemporal Multiplier Networks for Video Action Recognition",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1982",
        "author_site": "Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes",
        "author": "Christoph Feichtenhofer; Axel Pinz; Richard P. Wildes",
        "abstract": "This paper presents a general ConvNet architecture for video action recognition based on multiplicative interactions of spacetime features. Our model combines the appearance and motion pathways of a two-stream architecture by motion gating and is trained end-to-end. We theoretically motivate multiplicative gating functions for residual networks and empirically study their effect on classification accuracy. To capture long-term dependencies we inject  identity mapping kernels for learning temporal relationships. Our architecture is fully convolutional in spacetime and able to evaluate a video in a single forward pass. Empirical investigation reveals that our model produces state-of-the-art results on two standard action recognition datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Feichtenhofer_Spatiotemporal_Multiplier_Networks_CVPR_2017_paper.pdf",
        "aff": "Graz University of Technology; Graz University of Technology; York University, Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1074884,
        "gs_citation": 1281,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17812047066454194940&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 24,
        "aff_domain": "tugraz.at;tugraz.at;cse.yorku.ca",
        "email": "tugraz.at;tugraz.at;cse.yorku.ca",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Feichtenhofer_Spatiotemporal_Multiplier_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Graz University of Technology;York University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tugraz.at;https://yorku.ca",
        "aff_unique_abbr": "TUGraz;York U",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Toronto",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Austria;Canada"
    },
    {
        "title": "Spatiotemporal Pyramid Network for Video Action Recognition",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "569",
        "author_site": "Yunbo Wang, Mingsheng Long, Jianmin Wang, Philip S. Yu",
        "author": "Yunbo Wang; Mingsheng Long; Jianmin Wang; Philip S. Yu",
        "abstract": "Two-stream convolutional networks have shown strong performance in video action recognition tasks. The key idea is to learn spatiotemporal features by fusing convolutional networks spatially and temporally. However, it remains unclear how to model the correlations between the spatial and temporal structures at multiple abstraction levels. First, the spatial stream tends to fail if two videos share similar backgrounds. Second, the temporal stream may be fooled if two actions resemble in short snippets, though appear to be distinct in the long term. We propose a novel spatiotemporal pyramid network to fuse the spatial and temporal features in a pyramid structure such that they can reinforce each other. From the architecture perspective, our network constitutes hierarchical fusion strategies which can be trained as a whole using a unified spatiotemporal loss. A series of ablation experiments support the importance of each fusion strategy. From the technical perspective, we introduce the spatiotemporal compact bilinear operator into video analysis tasks. This operator enables efficient training of bilinear fusion operations which can capture full interactions between the spatial and temporal features. Our final network achieves state-of-the-art results on standard video datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Spatiotemporal_Pyramid_Network_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1903.01038v1",
        "pdf_size": 746722,
        "gs_citation": 312,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16228459825132970579&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Spatiotemporal_Pyramid_Network_CVPR_2017_paper.html"
    },
    {
        "title": "Specular Highlight Removal in Facial Images",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1148",
        "author_site": "Chen Li, Stephen Lin, Kun Zhou, Katsushi Ikeuchi",
        "author": "Chen Li; Stephen Lin; Kun Zhou; Katsushi Ikeuchi",
        "abstract": "We present a method for removing specular highlight reflections in facial images that may contain varying illumination colors. This is accurately achieved through the use of physical and statistical properties of human skin and faces. We employ a melanin and hemoglobin based model to represent the diffuse color variations in facial skin, and utilize this model to constrain the highlight removal solution in a manner that is effective even for partially saturated pixels. The removal of highlights is further facilitated through estimation of directionally variant illumination colors over the face, which is done while taking advantage of a statistically-based approximation of facial geometry. An important practical feature of the proposed method is that the skin color model is utilized in a way that does not require color calibration of the camera. Moreover, this approach does not require assumptions commonly needed in previous highlight removal techniques, such as uniform illumination color or piecewise-constant surface colors. We validate this technique through comparisons to existing methods for removing specular highlights.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Specular_Highlight_Removal_CVPR_2017_paper.pdf",
        "aff": "State Key Lab of CAD&CG, Zhejiang University; Microsoft Research; State Key Lab of CAD&CG, Zhejiang University; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Specular_Highlight_Removal_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4611546,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9439794597289009742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Specular_Highlight_Removal_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Zhejiang University;Microsoft",
        "aff_unique_dep": "State Key Lab of CAD&CG;Microsoft Research",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "ZJU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "3562",
        "author_site": "Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy",
        "author": "Jonathan Huang; Vivek Rathod; Chen Sun; Menglong Zhu; Anoop Korattikara; Alireza Fathi; Ian Fischer; Zbigniew Wojna; Yang Song; Sergio Guadarrama; Kevin Murphy",
        "abstract": "The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN (Ren et al., 2015), R-FCN (Dai et al., 2016) and SSD (Liu et al., 2016) systems, which we view as \"meta-architectures\" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.pdf",
        "aff": ";;;;;;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.10012v3",
        "pdf_size": 2065104,
        "gs_citation": 3693,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8642196741698958427&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;;;;;;",
        "email": ";;;;;;;;;;",
        "author_num": 11,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.html"
    },
    {
        "title": "SphereFace: Deep Hypersphere Embedding for Face Recognition",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "76",
        "author_site": "Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, Le Song",
        "author": "Weiyang Liu; Yandong Wen; Zhiding Yu; Ming Li; Bhiksha Raj; Le Song",
        "abstract": "This paper addresses deep face recognition (FR) problem under open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. However, few existing algorithms can effectively achieve this criterion. To this end, we propose the angular softmax (A-Softmax) loss that enables convolutional neural networks (CNNs) to learn angularly discriminative features. Geometrically, A-Softmax loss can be viewed as imposing discriminative constraints on a hypersphere manifold, which intrinsically matches the prior that faces also lie on a manifold. Moreover, the size of angular margin can be quantitatively adjusted by a parameter m. We further derive specific m to approximate the ideal feature criterion. Extensive analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace Challenge 1 show the superiority of A-Softmax loss in FR tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper.pdf",
        "aff": "Georgia Institute of Technology; Carnegie Mellon University; Carnegie Mellon University; Sun Yat-Sen University; Carnegie Mellon University; Georgia Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.08063v4",
        "pdf_size": 1646430,
        "gs_citation": 3736,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11996860204567761072&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "gatech.edu;andrew.cmu.edu;andrew.cmu.edu; ;andrew.cmu.edu;cc.gatech.edu",
        "email": "gatech.edu;andrew.cmu.edu;andrew.cmu.edu; ;andrew.cmu.edu;cc.gatech.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;2;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Carnegie Mellon University;Sun Yat-sen University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.gatech.edu;https://www.cmu.edu;http://www.sysu.edu.cn/",
        "aff_unique_abbr": "Georgia Tech;CMU;SYSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Spindle Net: Person Re-Identification With Human Body Region Guided Feature Decomposition and Fusion",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "372",
        "author_site": "Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, Xiaogang Wang, Xiaoou Tang",
        "author": "Haiyu Zhao; Maoqing Tian; Shuyang Sun; Jing Shao; Junjie Yan; Shuai Yi; Xiaogang Wang; Xiaoou Tang",
        "abstract": "Person re-identification (ReID) is an important task in video surveillance and has various applications. It is non-trivial due to complex background clutters, varying illumination conditions, and uncontrollable camera settings. Moreover, the person body misalignment caused by detectors or pose variations is sometimes too severe for feature matching across images. In this study, we propose a novel Convolutional Neural Network (CNN), called Spindle Net, based on human body region guided multi-stage feature decomposition and tree-structured competitive feature fusion. It is the first time human body structure information is considered in a CNN framework to facilitate feature learning. The proposed Spindle Net brings unique advantages: 1) it separately captures semantic features from different body regions thus the macro- and micro-body features can be well aligned across images, 2) the learned region features from different semantic regions are merged with a competitive scheme and discriminative features can be well preserved. State of the art performance can be achieved on multiple datasets by large margins. We further demonstrate the robustness and effectiveness of the proposed Spindle Net on our proposed dataset SenseReID without fine-tuning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Spindle_Net_Person_CVPR_2017_paper.pdf",
        "aff": "SenseTime Group Limited; SenseTime Group Limited; SenseTime Group Limited; SenseTime Group Limited; SenseTime Group Limited; SenseTime Group Limited; The Chinese University of Hong Kong; The Chinese University of Hong Kong",
        "project": "",
        "github": "https://github.com/yokattame/SpindleNet",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1002141,
        "gs_citation": 1102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10597448056612590924&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 7,
        "aff_domain": "sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com;sensetime.com;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhao_Spindle_Net_Person_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;1;1",
        "aff_unique_norm": "SenseTime Group Limited;Chinese University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sensetime.com;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "SenseTime;CUHK",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "367",
        "author_site": "Richard Zhang, Phillip Isola, Alexei A. Efros",
        "author": "Richard Zhang; Phillip Isola; Alexei A. Efros",
        "abstract": "We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Split-Brain_Autoencoders_Unsupervised_CVPR_2017_paper.pdf",
        "aff": "Berkeley AI Research (BAIR) Laboratory; Berkeley AI Research (BAIR) Laboratory; Berkeley AI Research (BAIR) Laboratory",
        "project": "",
        "github": "https://richzhang.github.io/splitbrainauto",
        "supp": "",
        "arxiv": "1611.09842",
        "pdf_size": 915272,
        "gs_citation": 843,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=845318398040643748&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Split-Brain_Autoencoders_Unsupervised_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Berkeley AI Research (BAIR) Laboratory",
        "aff_unique_url": "https://bair.berkeley.edu",
        "aff_unique_abbr": "BAIR",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Sports Field Localization via Deep Structured Models",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2173",
        "author_site": "Namdar Homayounfar, Sanja Fidler, Raquel Urtasun",
        "author": "Namdar Homayounfar; Sanja Fidler; Raquel Urtasun",
        "abstract": "In this work, we propose a novel way of efficiently localizing a sports field from a single broadcast image of the game. Related work in this area relies on manually annotating a few key frames and extending the localization to similar images, or installing fixed specialized cameras in the stadium from which the layout of the field can be obtained. In contrast, we formulate this problem as a branch and bound inference in a Markov random field where an energy function is defined in terms of  semantic cues such as the field surface, lines and circles obtained from a deep semantic segmentation network. Moreover, our approach is fully automatic and depends only on a single image from the broadcast video of the game. We demonstrate the effectiveness of our method by applying it to soccer and hockey.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Homayounfar_Sports_Field_Localization_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4611133,
        "gs_citation": 163,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1649885157901566696&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Homayounfar_Sports_Field_Localization_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Stacked Generative Adversarial Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2100",
        "author_site": "Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, Serge Belongie",
        "author": "Xun Huang; Yixuan Li; Omid Poursaeed; John Hopcroft; Serge Belongie",
        "abstract": "In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality  than GANs without stacking.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, Cornell University; School of Electrical and Computer Engineering, Cornell University; School of Electrical and Computer Engineering, Cornell University; Department of Computer Science, Cornell University; Department of Computer Science, Cornell University+Cornell Tech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.04357v4",
        "pdf_size": 918324,
        "gs_citation": 751,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12809025073819241356&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "cornell.edu;cornell.edu;cornell.edu;cs.cornell.edu;cornell.edu",
        "email": "cornell.edu;cornell.edu;cornell.edu;cs.cornell.edu;cornell.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Stacked_Generative_Adversarial_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0+0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Ithaca;New York City",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Stereo-Based 3D Reconstruction of Dynamic Fluid Surfaces by Global Optimization",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "455",
        "author_site": "Yiming Qian, Minglun Gong, Yee-Hong Yang",
        "author": "Yiming Qian; Minglun Gong; Yee-Hong Yang",
        "abstract": "3D reconstruction of dynamic fluid surfaces is an open and challenging problem in computer vision. Unlike previous approaches that reconstruct each surface point independently and often return noisy depth maps, we propose a novel global optimization-based approach that recovers both depths and normals of all 3D points simultaneously. Using the traditional refraction stereo setup, we capture the wavy appearance of a pre-generated random pattern, and then estimate the correspondences between the captured images and the known background by tracking the pattern. Assuming that the light is refracted only once through the fluid interface, we minimize an objective function that incorporates both the cross-view normal consistency constraint and the single-view normal consistency constraints. The key idea is that the normals required for light refraction based on Snell's law from one view should agree with not only the ones from the second view, but also the ones estimated from local 3D geometry. Moreover, an effective reconstruction error metric is designed for estimating the refractive index of the fluid. We report experimental results on both synthetic and real data demonstrating that the proposed approach is accurate and shows superiority over the conventional stereo-based method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Qian_Stereo-Based_3D_Reconstruction_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10916075601357575884&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Qian_Stereo-Based_3D_Reconstruction_CVPR_2017_paper.html"
    },
    {
        "title": "Straight to Shapes: Real-Time Detection of Encoded Shapes",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2960",
        "author_site": "Saumya Jetley, Michael Sapienza, Stuart Golodetz, Philip H. S. Torr",
        "author": "Saumya Jetley; Michael Sapienza; Stuart Golodetz; Philip H. S. Torr",
        "abstract": "Current object detection approaches predict bounding boxes that provide little instance-specific information beyond location, scale and aspect ratio.   In this work, we propose to regress directly to objects' shapes in addition to their bounding boxes and categories.  It is crucial to find an appropriate shape representation that is compact and decodable, and in which objects can be compared for higher-order concepts such as view similarity, pose variation and occlusion.   To achieve this, we use a denoising convolutional auto-encoder to learn a low-dimensional shape embedding space. We place the decoder network after a fast end-to-end deep convolutional network that is trained to regress directly to the shape vectors provided by the auto-encoder.   This yields what to the best of our knowledge is the first real-time shape prediction network, running at  35 FPS on a high-end desktop.   With higher-order shape reasoning well-integrated into the network pipeline, the network shows the useful practical quality of generalising to unseen categories that are similar to the ones in the training set, something that most existing approaches fail to handle.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jetley_Straight_to_Shapes_CVPR_2017_paper.pdf",
        "aff": "Department of Engineering Science, University of Oxford; Department of Engineering Science, University of Oxford + Samsung Research America, Mountain View CA; Department of Engineering Science, University of Oxford; Department of Engineering Science, University of Oxford",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Jetley_Straight_to_Shapes_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.07932v2",
        "pdf_size": 1905355,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4830468594814989898&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "robots.ox.ac.uk;samsung.com;robots.ox.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;samsung.com;robots.ox.ac.uk;robots.ox.ac.uk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jetley_Straight_to_Shapes_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "University of Oxford;Samsung",
        "aff_unique_dep": "Department of Engineering Science;Samsung Research America",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.samsung.com/us/",
        "aff_unique_abbr": "Oxford;SRA",
        "aff_campus_unique_index": "0;0+1;0;0",
        "aff_campus_unique": "Oxford;Mountain View",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "StyleBank: An Explicit Representation for Neural Image Style Transfer",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "703",
        "author_site": "Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, Gang Hua",
        "author": "Dongdong Chen; Lu Yuan; Jing Liao; Nenghai Yu; Gang Hua",
        "abstract": "We propose StyleBank, which is composed of multiple convolution filter banks and each filter bank explicitly represents one style, for neural image style transfer. To transfer an image to a specific style, the corresponding filter bank is operated on top of the intermediate feature embedding produced by a single auto-encoder. The StyleBank and the auto-encoder are jointly learnt, where the learning is conducted in such a way that the auto-encoder does not encode any style information thanks to the flexibility introduced by the explicit filter bank representation. It also enables us to conduct incremental learning to add a new image style by learning a new filter bank while holding the auto-encoder fixed. The explicit style representation along with the flexible network design enables us to fuse styles at not only the image level, but also the region level. Our method is the first style transfer network that links back to traditional texton mapping methods, and hence provides new understanding on neural style transfer. Our method is easy to train, runs in real-time, and produces results that qualitatively better or at least comparable to existing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_StyleBank_An_Explicit_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.09210v2",
        "gs_citation": 602,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=932276203729600060&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_StyleBank_An_Explicit_CVPR_2017_paper.html"
    },
    {
        "title": "StyleNet: Generating Attractive Visual Captions With Styles",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1161",
        "author_site": "Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, Li Deng",
        "author": "Chuang Gan; Zhe Gan; Xiaodong He; Jianfeng Gao; Li Deng",
        "abstract": "We propose a novel framework named StyleNet to address the task of generating attractive captions for images and videos with different styles.  To this end, we devise a novel model component, named factored LSTM, which automatically distills the style factors in the monolingual text corpus. Then at runtime, we can explicitly control the style in the caption generation process so as to produce attractive visual captions with the desired style. Our approach achieves this goal by leveraging two sets of data: 1) factual image/video-caption paired data, and 2) stylized monolingual text data (e.g., romantic and humorous sentences). We show experimentally that StyleNet outperforms existing approaches for generating visual captions with different styles, measured in both automatic and human evaluation metrics on the newly collected  FlickrStyle10K image caption dataset, which contains 10K Flickr images with corresponding humorous and romantic captions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Gan_StyleNet_Generating_Attractive_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 397,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4319472844510388115&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Gan_StyleNet_Generating_Attractive_CVPR_2017_paper.html"
    },
    {
        "title": "Subspace Clustering via Variance Regularized Ridge Regression",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1065",
        "author_site": "Chong Peng, Zhao Kang, Qiang Cheng",
        "author": "Chong Peng; Zhao Kang; Qiang Cheng",
        "abstract": "Spectral clustering based subspace clustering methods have emerged recently. When the inputs are 2-dimensional (2D) data, most existing clustering methods convert such data to vectors as preprocessing, which severely damages spatial information of the data. In this paper, we propose a novel subspace clustering method for 2D data with enhanced capability of retaining spatial information for clustering. It seeks two projection matrices and simultaneously constructs a linear representation of the projected data, such that the sought projections help construct the most expressive representation with the most variational information. We regularize our method based on covariance matrices directly obtained from 2D data, which have much smaller size and are more computationally amiable. Moreover, to exploit nonlinear structures of the data, a nonlinear version is proposed, which constructs an adaptive manifold according to updated projections. The learning processes of projections, representation, and manifold thus mutually enhance each other, leading to a powerful data representation. Efficient optimization procedures are proposed, which generate non-increasing objective value sequence with theoretical convergence guarantee. Extensive experimental results confirm the effectiveness of proposed method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Peng_Subspace_Clustering_via_CVPR_2017_paper.pdf",
        "aff": "Southern Illinois University, Carbondale, IL, 62901, USA; Southern Illinois University, Carbondale, IL, 62901, USA; Southern Illinois University, Carbondale, IL, 62901, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1009122,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15729743783558552626&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "siu.edu;siu.edu;siu.edu",
        "email": "siu.edu;siu.edu;siu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Peng_Subspace_Clustering_via_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Southern Illinois University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.siu.edu",
        "aff_unique_abbr": "SIU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Carbondale",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Superpixel-Based Tracking-By-Segmentation Using Markov Chains",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "670",
        "author_site": "Donghun Yeo, Jeany Son, Bohyung Han, Joon Hee Han",
        "author": "Donghun Yeo; Jeany Son; Bohyung Han; Joon Hee Han",
        "abstract": "We propose a simple but effective tracking-by-segmentation algorithm using Absorbing Markov Chain (AMC) on superpixel segmentation, where target state is estimated by a combination of bottom-up and top-down approaches, and target segmentation is propagated to subsequent frames in a recursive manner. Our algorithm constructs a graph for AMC using the superpixels identified in two consecutive frames, where background superpixels in the previous frame correspond to absorbing vertices while all other superpixels create transient ones. The weight of each edge depends on the similarity of scores in the end superpixels, which are learned by support vector regression. Once graph construction is completed, target segmentation is estimated using the absorption time of each superpixel. The proposed tracking algorithm achieves substantially improved performance compared to the state-of-the-art segmentation-based tracking techniques in multiple challenging datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yeo_Superpixel-Based_Tracking-By-Segmentation_Using_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3329307907350735979&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yeo_Superpixel-Based_Tracking-By-Segmentation_Using_CVPR_2017_paper.html"
    },
    {
        "title": "Superpixels and Polygons Using Simple Non-Iterative Clustering",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1936",
        "author_site": "Radhakrishna Achanta, Sabine S\u00c3\u00bcsstrunk",
        "author": "Radhakrishna Achanta; Sabine Susstrunk",
        "abstract": "We present an improved version of the Simple Linear Iterative Clustering (SLIC) superpixel segmentation. Unlike SLIC, our algorithm is non-iterative, enforces connectivity from the start, requires lesser memory, and is faster. Relying on the superpixel boundaries obtained using our algorithm, we also present a polygonal partitioning algorithm. We demonstrate that our superpixels as well as the polygonal partitioning are superior to the respective state-of-the-art algorithms on quantitative benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Achanta_Superpixels_and_Polygons_CVPR_2017_paper.pdf",
        "aff": "School of Computer and Communication Sciences (IC) \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL); School of Computer and Communication Sciences (IC) \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2280000,
        "gs_citation": 575,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17683992366473129599&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Achanta_Superpixels_and_Polygons_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "School of Computer and Communication Sciences",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Supervising Neural Attention Models for Video Captioning by Human Gaze Data",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "156",
        "author_site": "Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo, Sang-Hun Lee, Gunhee Kim",
        "author": "Youngjae Yu; Jongwook Choi; Yeonhwa Kim; Kyung Yoo; Sang-Hun Lee; Gunhee Kim",
        "abstract": "The attention mechanisms in deep neural networks are inspired by human's attention that sequentially focuses on the most relevant parts of the information over time to generate prediction output. The attention parameters in those models are implicitly trained in an end-to-end manner, yet there have been few trials to explicitly incorporate human gaze tracking to supervise the attention models. In this paper, we investigate whether attention models can benefit from explicit human gaze labels, especially for the task of video captioning. We collect a new dataset called VAS, consisting of movie clips, and corresponding multiple descriptive sentences along with human gaze tracking data. We propose a video captioning model named Gaze Encoding Attention Network (GEAN) that can leverage gaze tracking information to provide the spatial and temporal attention for sentence generation. Through evaluation of language similarity metrics and human assessment via Amazon mechanical Turk, we demonstrate that spatial attentions guided by human gaze data indeed improve the performance of multiple captioning methods. Moreover, we show that the proposed approach achieves the state-of-the-art performance for both gaze prediction and video captioning not only in our VAS dataset but also in standard datasets (e.g. LSMDC and Hollywood2)",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yu_Supervising_Neural_Attention_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1707.06029v1",
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18170327085267671145&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yu_Supervising_Neural_Attention_CVPR_2017_paper.html"
    },
    {
        "title": "SurfNet: Generating 3D Shape Surfaces Using Deep Residual Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2635",
        "author_site": "Ayan Sinha, Asim Unmesh, Qixing Huang, Karthik Ramani",
        "author": "Ayan Sinha; Asim Unmesh; Qixing Huang; Karthik Ramani",
        "abstract": "3D shape models are naturally parameterized using vertices and faces, i.e, composed on polygons forming a surface. However, current 3D learning paradigms for predictive and generative tasks using convolutional neural networks focus on a voxelized representation of the object. Lifting convolution operators from the traditional 2D to 3D results in high computational overhead with little additional benefit as most of the geometry information is contained on the surface boundary. Here we study the problem of directly generating the 3D shape surface of rigid and non-rigid shapes using deep convolutional neural networks. We develop a procedure to create consistent `geometry images' representing the 3D shape surface of a category of shapes. We then use this consistent representation for category-specific shape generation from a parametric representation or an image by developing novel extensions of deep residual networks for the task of 3D surface generation. Our experiments indicate that our network learns a meaningful representation of shape surfaces allowing it to interpolate between shape orientations and poses, invent new shape surfaces, reconstruct 3D shape surfaces from previously unseen images, and rectify noisy correspondence between 3D shapes belonging to the same class.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sinha_SurfNet_Generating_3D_CVPR_2017_paper.pdf",
        "aff": "MIT; IIT Kanpur; UT Austin; Purdue",
        "project": "",
        "github": "https://github.com/sinhayan/surfnet",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Sinha_SurfNet_Generating_3D_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.04079v1",
        "pdf_size": 1575733,
        "gs_citation": 213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15053387007127335834&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mit.edu;gmail.com;cs.utexas.edu;purdue.edu",
        "email": "mit.edu;gmail.com;cs.utexas.edu;purdue.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sinha_SurfNet_Generating_3D_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Massachusetts Institute of Technology;Indian Institute of Technology Kanpur;University of Texas at Austin;Purdue University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://web.mit.edu;https://www.iitk.ac.in;https://www.utexas.edu;https://www.purdue.edu",
        "aff_unique_abbr": "MIT;IITK;UT Austin;Purdue",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Kanpur;Austin",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;India"
    },
    {
        "title": "Surface Motion Capture Transfer With Gaussian Process Regression",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "68",
        "author_site": "Adnane Boukhayma, Jean-S\u00c3\u00a9bastien Franco, Edmond Boyer",
        "author": "Adnane Boukhayma; Jean-Sebastien Franco; Edmond Boyer",
        "abstract": "We address the problem of transferring motion between captured 4D models. We particularly focus on human subjects for which the ability to automatically augment 4D datasets, by propagating movements between subjects, is of interest in a great deal of recent vision applications that builds on human visual corpus. Given 4D training sets for two subjects for which a sparse set of corresponding keyposes are known, our method is able to transfer a newly captured motion from one subject to the other. With the aim to generalize transfers to input motions possibly very diverse with respect to the training sets, the method contributes with a new transfer model based on non-linear pose interpolation. Building on Gaussian process regression, this model intends to capture and preserve individual motion properties, and thereby realism, by accounting for pose inter-dependencies during motion transfers. Our experiments show visually qualitative, and quantitative, improvements over existing pose-mapping methods and confirm the generalization capabilities of our method compared to state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Boukhayma_Surface_Motion_Capture_CVPR_2017_paper.pdf",
        "aff": "Inria, LJK, Univ. Grenoble Alpes; Inria, LJK, Univ. Grenoble Alpes; Inria, LJK, Univ. Grenoble Alpes",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3919348,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11225083697467409158&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "inria.fr;inria.fr;inria.fr",
        "email": "inria.fr;inria.fr;inria.fr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Boukhayma_Surface_Motion_Capture_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "LJK",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "Inria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Surveillance Video Parsing With Single Frame Supervision",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "141",
        "author_site": "Si Liu, Changhu Wang, Ruihe Qian, Han Yu, Renda Bao, Yao Sun",
        "author": "Si Liu; Changhu Wang; Ruihe Qian; Han Yu; Renda Bao; Yao Sun",
        "abstract": "Surveillance video parsing, which segments the video frames into several labels, i.e.,  face, pants, left-leg, has wide applications. However, annotating all frames pixel-wisely is tedious and inefficient. In this paper, we develop a Single frame Video Parsing (SVP) method which requires only one labeled frame per video in training stage.   To parse one particular frame, the video segment preceding the frame is jointly considered. SVP 1: roughly parses the frames within the video segment, 2: estimates the optical flow between frames and 3: fuses the rough parsing results warped by optical flow to produce the refined parsing result. The three components of SVP, namely frame parsing, optical flow estimation and temporal fusion are integrated in an end-to-end manner. Experimental results on two surveillance video datasets reveal that SVP is superior than state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_Surveillance_Video_Parsing_CVPR_2017_paper.pdf",
        "aff": "State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences; Toutiao AI Lab; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences",
        "project": "http://liusi-group.com/projects/SVP",
        "github": "",
        "supp": "",
        "arxiv": "1611.09587v1",
        "pdf_size": 1388372,
        "gs_citation": 73,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18382332525926733986&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "iie.ac.cn;toutiao.com;iie.ac.cn;iie.ac.cn;iie.ac.cn; ",
        "email": "iie.ac.cn;toutiao.com;iie.ac.cn;iie.ac.cn;iie.ac.cn; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Liu_Surveillance_Video_Parsing_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;Toutiao",
        "aff_unique_dep": "State Key Laboratory of Information Security, Institute of Information Engineering;AI Lab",
        "aff_unique_url": "http://www.cas.cn;https://www.toutiao.com",
        "aff_unique_abbr": "CAS;Toutiao",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Switching Convolutional Neural Network for Crowd Counting",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2451",
        "author_site": "Deepak Babu Sam, Shiv Surya, R. Venkatesh Babu",
        "author": "Deepak Babu Sam; Shiv Surya; R. Venkatesh Babu",
        "abstract": "We propose a novel crowd counting model that maps a given crowd scene to its density. Crowd analysis is compounded by myriad of factors like inter-occlusion between people due to extreme crowding, high similarity of appearance between people and background elements, and large variability of camera view-points. Current state-of-the art approaches tackle these factors by using multi-scale CNN architectures, recurrent networks and late fusion of features from multi-column CNN with different receptive fields. We propose switching convolutional neural network that leverages variation of crowd density within an image to improve the accuracy and localization of the predicted crowd count. Patches from a grid within a crowd scene are relayed to independent CNN regressors based on crowd count prediction quality of the CNN established during training. The independent CNN regressors are designed to have different receptive fields and a switch classifier is trained to relay the crowd scene patch to the best CNN regressor. We perform extensive experiments on all major crowd counting datasets and evidence better performance compared to current state-of-the-art methods. We provide interpretable representations of the multichotomy of space of crowd scene patches inferred from the switch. It is observed that the switch relays an image patch to a particular CNN column based on density of crowd.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sam_Switching_Convolutional_Neural_CVPR_2017_paper.pdf",
        "aff": "Indian Institute of Science; Indian Institute of Science; Indian Institute of Science",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Sam_Switching_Convolutional_Neural_2017_CVPR_supplemental.pdf",
        "arxiv": "1708.00199",
        "pdf_size": 1929433,
        "gs_citation": 1200,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10313407600813817221&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "grads.cds.iisc.ac.in;gmail.com;cds.iisc.ac.in",
        "email": "grads.cds.iisc.ac.in;gmail.com;cds.iisc.ac.in",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sam_Switching_Convolutional_Neural_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation",
        "session": "Machine Learning for 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "832",
        "author_site": "Li Yi, Hao Su, Xingwen Guo, Leonidas J. Guibas",
        "author": "Li Yi; Hao Su; Xingwen Guo; Leonidas J. Guibas",
        "abstract": "In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs.  A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parametrizing kernels in the spectral domain spanned by graph Laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strives to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parametrization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested SyncSpecCNN on various tasks, including 3D shape part segmentation and keypoint prediction.  State-of-the-art performance has been achieved on all benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yi_SyncSpecCNN_Synchronized_Spectral_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yi_SyncSpecCNN_Synchronized_Spectral_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.00606v1",
        "pdf_size": 20278145,
        "gs_citation": 560,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12816663187843359630&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yi_SyncSpecCNN_Synchronized_Spectral_CVPR_2017_paper.html"
    },
    {
        "title": "Synthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes With Deep Generative Networks",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "555",
        "author_site": "Amir Arsalan Soltani, Haibin Huang, Jiajun Wu, Tejas D. Kulkarni, Joshua B. Tenenbaum",
        "author": "Amir Arsalan Soltani; Haibin Huang; Jiajun Wu; Tejas D. Kulkarni; Joshua B. Tenenbaum",
        "abstract": "We study the problem of learning generative models of 3D shapes. Voxels or 3D parts have been widely used as the underlying representations to build complex 3D shapes; however, voxel-based representations suffer from high memory requirements, and parts-based models require a large collection of cached or richly parametrized parts. We take an alternative approach: learning a generative model over multi-view depth maps or their corresponding silhouettes, and using a deterministic rendering function to produce 3D shapes from these images. A multi-view representation of shapes enables generation of 3D models with fine details, as 2D depth maps and silhouettes can be modeled at a much higher resolution than 3D voxels. Moreover, our approach naturally brings the ability to recover the underlying 3D representation from depth maps of one or a few viewpoints. Experiments show that our framework can generate 3D shapes with variations and details. We also demonstrate that our model has out-of-sample generalization power for real-world tasks with occluded objects.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Soltani_Synthesizing_3D_Shapes_CVPR_2017_paper.pdf",
        "aff": "Massachusetts Institute of Technology; University of Massachusetts, Amherst; Massachusetts Institute of Technology; Google DeepMind; Massachusetts Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3000712,
        "gs_citation": 256,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7819886472262311114&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "mit.edu;cs.umass.edu;mit.edu;gmail.com;mit.edu",
        "email": "mit.edu;cs.umass.edu;mit.edu;gmail.com;mit.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Soltani_Synthesizing_3D_Shapes_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Massachusetts Amherst;Google",
        "aff_unique_dep": ";;Google DeepMind",
        "aff_unique_url": "https://web.mit.edu;https://www.umass.edu;https://deepmind.com",
        "aff_unique_abbr": "MIT;UMass Amherst;DeepMind",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Amherst",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "3327",
        "author_site": "Jianwen Xie, Song-Chun Zhu, Ying Nian Wu",
        "author": "Jianwen Xie; Song-Chun Zhu; Ying Nian Wu",
        "abstract": "Video sequences contain rich dynamic patterns, such as dynamic texture patterns that exhibit stationarity in the temporal domain, and action  patterns that are non-stationary in either spatial or temporal domain. We show that a spatial-temporal generative ConvNet can be used to model and synthesize  dynamic patterns. The model defines a probability distribution on the video sequence, and the log probability is defined by a spatial-temporal ConvNet that consists of multiple layers of spatial-temporal filters to capture spatial-temporal patterns of different scales. The model can be learned from the training video sequences by an \"analysis by synthesis\" learning algorithm that iterates the following two steps. Step 1 synthesizes video sequences from the currently learned model. Step 2 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences. We show that the learning algorithm can synthesize realistic dynamic patterns.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.pdf",
        "aff": "University of California, Los Angeles (UCLA), USA; University of California, Los Angeles (UCLA), USA; University of California, Los Angeles (UCLA), USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1606.00972",
        "pdf_size": 1851467,
        "gs_citation": 117,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6016985797168347299&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ucla.edu;stat.ucla.edu;stat.ucla.edu",
        "email": "ucla.edu;stat.ucla.edu;stat.ucla.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Synthesizing_Dynamic_Patterns_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Synthesizing Normalized Faces From Facial Identity Features",
        "session": "Analyzing Humans 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "1362",
        "author_site": "Forrester Cole, David Belanger, Dilip Krishnan, Aaron Sarna, Inbar Mosseri, William T. Freeman",
        "author": "Forrester Cole; David Belanger; Dilip Krishnan; Aaron Sarna; Inbar Mosseri; William T. Freeman",
        "abstract": "We present a method for synthesizing a frontal, neutral-expression image of a person's face, given an input face photograph. This is achieved by learning to generate facial landmarks and textures from features extracted from a facial-recognition network. Unlike previous generative approaches, our encoding feature vector is largely invariant to lighting, pose, and facial expression. Exploiting this invariance, we train our decoder network using only frontal, neutral-expression photographs. Since these photographs are well aligned, we can decompose them into a sparse set of landmark points and aligned texture maps. The decoder then predicts landmarks and textures independently and combines them using a differentiable image warping operation. The resulting images can be used for a number of applications, such as analyzing facial attributes, exposure and white balance adjustment, or creating a 3-D avatar.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cole_Synthesizing_Normalized_Faces_CVPR_2017_paper.pdf",
        "aff": "Google, Inc.; Google, Inc. + University of Massachusetts Amherst; Google, Inc.; Google, Inc.; Google, Inc.; Google, Inc. + MIT CSAIL",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Cole_Synthesizing_Normalized_Faces_2017_CVPR_supplemental.pdf",
        "arxiv": "1701.04851v4",
        "pdf_size": 1510681,
        "gs_citation": 190,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1624988979715798568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cole_Synthesizing_Normalized_Faces_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;0;0;0;0+2",
        "aff_unique_norm": "Google;University of Massachusetts Amherst;Massachusetts Institute of Technology",
        "aff_unique_dep": "Google;;Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.google.com;https://www.umass.edu;https://www.csail.mit.edu",
        "aff_unique_abbr": "Google;UMass Amherst;MIT CSAIL",
        "aff_campus_unique_index": "0;0+1;0;0;0;0+2",
        "aff_campus_unique": "Mountain View;Amherst;Cambridge",
        "aff_country_unique_index": "0;0+0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Spotlight",
        "track": "main",
        "pid": "1002",
        "author_site": "Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, Gunhee Kim",
        "author": "Yunseok Jang; Yale Song; Youngjae Yu; Youngjin Kim; Gunhee Kim",
        "abstract": "Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a  dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jang_TGIF-QA_Toward_Spatio-Temporal_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 676,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13686605722049018608&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jang_TGIF-QA_Toward_Spatio-Temporal_CVPR_2017_paper.html"
    },
    {
        "title": "Task-Driven Dynamic Fusion: Reducing Ambiguity in Video Description",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1370",
        "author_site": "Xishan Zhang, Ke Gao, Yongdong Zhang, Dongming Zhang, Jintao Li, Qi Tian",
        "author": "Xishan Zhang; Ke Gao; Yongdong Zhang; Dongming Zhang; Jintao Li; Qi Tian",
        "abstract": "Integrating complementary features from multiple channels is expected to solve the description ambiguity problem in video captioning, whereas inappropriate fusion strategies often harm rather than help the performance. Existing static fusion methods in video captioning such as concatenation and summation cannot attend to appropriate feature channels, thus fail to adaptively support the recognition of various kinds of visual entities such as actions and objects. This paper contributes to: 1)The first in-depth study of the weakness inherent in data-driven static fusion methods for video captioning. 2) The establishment of a task-driven dynamic fusion (TDDF) method. It can adaptively choose different fusion patterns according to model status. 3) The improvement of video captioning. Extensive experiments conducted on two well-known benchmarks demonstrate that our dynamic fusion method outperforms the state-of-the-art results on MSVD with METEOR scores 0.333, and achieves superior METEOR scores 0.278 on MSR-VTT-10K. Compared to single features, the relative improvement derived from our fusion method are 10.0% and 5.7% respectively on two datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Task-Driven_Dynamic_Fusion_CVPR_2017_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6646540710465693215&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Task-Driven_Dynamic_Fusion_CVPR_2017_paper.html"
    },
    {
        "title": "Teaching Compositionality to CNNs",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2083",
        "author_site": "Austin Stone, Huayan Wang, Michael Stark, Yi Liu, D. Scott Phoenix, Dileep George",
        "author": "Austin Stone; Huayan Wang; Michael Stark; Yi Liu; D. Scott Phoenix; Dileep George",
        "abstract": "Convolutional neural networks (CNNs) have shown great success in computer vision, approaching human-level performance when trained for specific tasks via application- specific loss functions. In this paper, we propose a method for augmenting and training CNNs so that their learned features are compositional. It encourages networks to form representations that disentangle objects from their surroundings and from each other, thereby promoting better generalization. Our method is agnostic to the specific de- tails of the underlying CNN to which it is applied and can in principle be used with any CNN. As we show in our experiments, the learned representations lead to feature activations that are more localized and improve performance over non-compositional baselines in object recognition tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Stone_Teaching_Compositionality_to_CVPR_2017_paper.pdf",
        "aff": "Vicarious FPC, San Francisco, CA, USA; Vicarious FPC, San Francisco, CA, USA; Vicarious FPC, San Francisco, CA, USA; Vicarious FPC, San Francisco, CA, USA; Vicarious FPC, San Francisco, CA, USA; Vicarious FPC, San Francisco, CA, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Stone_Teaching_Compositionality_to_2017_CVPR_supplemental.pdf",
        "arxiv": "1706.04313v1",
        "pdf_size": 2432085,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9081573647465855079&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "vicarious.com;vicarious.com;vicarious.com;vicarious.com;vicarious.com;vicarious.com",
        "email": "vicarious.com;vicarious.com;vicarious.com;vicarious.com;vicarious.com;vicarious.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Stone_Teaching_Compositionality_to_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Vicarious FPC",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "San Francisco",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Template Matching With Deformable Diversity Similarity",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Spotlight",
        "track": "main",
        "pid": "67",
        "author_site": "Itamar Talmi, Roey Mechrez, Lihi Zelnik-Manor",
        "author": "Itamar Talmi; Roey Mechrez; Lihi Zelnik-Manor",
        "abstract": "We propose a novel measure for template matching named Deformable Diversity Similarity -- based on the diversity of feature matches between a target image window and the template. We rely on both local appearance and geometric information that jointly lead to a powerful approach for matching. Our key contribution is a similarity measure, that is robust to complex deformations, significant background clutter, and occlusions. Empirical evaluation on the most up-to-date benchmark shows that our method outperforms the current state-of-the-art in its detection accuracy while improving computational complexity.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Talmi_Template_Matching_With_CVPR_2017_paper.pdf",
        "aff": "Technion; Technion; Technion",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Talmi_Template_Matching_With_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.02190",
        "pdf_size": 1896176,
        "gs_citation": 143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13242273865204453198&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "campus.technion.ac.il;tx.technion.ac.il;ee.technion.ac.il",
        "email": "campus.technion.ac.il;tx.technion.ac.il;ee.technion.ac.il",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Talmi_Template_Matching_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Template-Based Monocular 3D Recovery of Elastic Shapes Using Lagrangian Multipliers",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1646",
        "author_site": "Nazim Haouchine, Stephane Cotin",
        "author": "Nazim Haouchine; Stephane Cotin",
        "abstract": "We present in this paper an efficient template-based method for 3D recovery of elastic shapes from a fixed monocular camera. By exploiting the object's elasticity, in contrast to isometric methods that use inextensibility constraints, a large range of deformations can be handled. Our method is expressed as a saddle point problem using Lagrangian multipliers resulting in a linear system which unifies both mechanical and optical constraints and integrates Dirichlet boundary conditions, whether they are fixed or free. We experimentally show that no prior knowledge on material properties is needed, which exhibit the generic usability of our method with elastic and inelastic objects with different kinds of materials. Comparisons with existing techniques are conducted on synthetic and real elastic objects with strains ranging from 25% to 130% resulting to low errors.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Haouchine_Template-Based_Monocular_3D_CVPR_2017_paper.pdf",
        "aff": "Inria - Mimesis Group; Inria - Mimesis Group",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5541914,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16070816795525835707&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "inria.fr;inria.fr",
        "email": "inria.fr;inria.fr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Haouchine_Template-Based_Monocular_3D_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "INRIA",
        "aff_unique_dep": "Mimesis Group",
        "aff_unique_url": "https://www.inria.fr",
        "aff_unique_abbr": "Inria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Temporal Action Co-Segmentation in 3D Motion Capture Data and Videos",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "3131",
        "author_site": "Konstantinos Papoutsakis, Costas Panagiotakis, Antonis A. Argyros",
        "author": "Konstantinos Papoutsakis; Costas Panagiotakis; Antonis A. Argyros",
        "abstract": "Given two action sequences, we are interested in spotting/co-segmenting all pairs of sub-sequences that represent the same action. We propose a totally unsupervised solution to this problem. No a-priori model of the actions is assumed to be available. The number of common sub-sequences may be unknown. The sub-sequences can be located anywhere in the original sequences, may differ in duration and the corresponding actions may be performed by a different person, in different style. We treat this type of temporal action co-segmentation as a stochastic optimization problem that is solved by employing Particle Swarm Optimization (PSO). The objective function that is minimized by PSO capitalizes on Dynamic Time Warping (DTW) to compare two action sub-sequences. Due to the generic problem formulation and solution, the proposed method can be applied to motion capture (i.e., 3D skeletal) data or to conventional RGB videos acquired in the wild. We present extensive quantitative experiments on standard data sets as well as on data sets we introduced in this paper. The obtained results demonstrate that the proposed method achieves a remarkable increase in co-segmentation quality compared to all tested state of the art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Papoutsakis_Temporal_Action_Co-Segmentation_CVPR_2017_paper.pdf",
        "aff": "Computational Vision and Robotics Laboratory, Institute of Computer Science, FORTH, Greece+Computer Science Department, University of Crete, Greece; Computational Vision and Robotics Laboratory, Institute of Computer Science, FORTH, Greece+Business Administration Department (Agios Nikolaos), TEI of Crete, Greece; Computational Vision and Robotics Laboratory, Institute of Computer Science, FORTH, Greece+Computer Science Department, University of Crete, Greece",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Papoutsakis_Temporal_Action_Co-Segmentation_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1518494,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1128024002974226733&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ics.forth.gr;ics.forth.gr;ics.forth.gr",
        "email": "ics.forth.gr;ics.forth.gr;ics.forth.gr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Papoutsakis_Temporal_Action_Co-Segmentation_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+2;0+1",
        "aff_unique_norm": "Foundation for Research and Technology - Hellas;University of Crete;TEI of Crete",
        "aff_unique_dep": "Institute of Computer Science;Computer Science Department;Business Administration Department",
        "aff_unique_url": "https://www.forth.gr;https://www.uoc.gr;http://www.teicrete.gr",
        "aff_unique_abbr": "FORTH;;",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Agios Nikolaos",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Greece"
    },
    {
        "title": "Temporal Action Localization by Structured Maximal Sums",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1357",
        "author_site": "Zehuan Yuan, Jonathan C. Stroud, Tong Lu, Jia Deng",
        "author": "Zehuan Yuan; Jonathan C. Stroud; Tong Lu; Jia Deng",
        "abstract": "We address the problem of temporal action localization in videos. We pose action localization as a structured prediction over arbitrary-length temporal windows, where each window is scored as the sum of frame-wise classification scores. Additionally, our model classifies the start, middle, and end of each action as separate components, allowing our system to explicitly model each action's temporal evolution and take advantage of informative temporal dependencies present in that structure. In this framework, we localize actions by searching for the structured maximal sum, a problem for which we develop a novel, provably-efficient algorithmic solution. The frame-wise classification scores are computed using features from a deep Convolutional Neural Network (CNN), which are trained end-to-end to directly optimize for a novel structured objective. We evaluate our system on the THUMOS '14 action detection benchmark and achieve competitive performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yuan_Temporal_Action_Localization_CVPR_2017_paper.pdf",
        "aff": "State Key Laboratory for Novel Software Technology, Nanjing University, China+University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; State Key Laboratory for Novel Software Technology, Nanjing University, China; University of Michigan, Ann Arbor",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.04671",
        "pdf_size": 1067652,
        "gs_citation": 174,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8731437054413490707&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yuan_Temporal_Action_Localization_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;1;0;1",
        "aff_unique_norm": "Nanjing University;University of Michigan",
        "aff_unique_dep": "State Key Laboratory for Novel Software Technology;",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.umich.edu",
        "aff_unique_abbr": "Nanjing U;UM",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Ann Arbor",
        "aff_country_unique_index": "0+1;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Temporal Attention-Gated Model for Robust Sequence Classification",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3096",
        "author_site": "Wenjie Pei, Tadas Baltru\u00c5\u00a1aitis, David M.J. Tax, Louis-Philippe Morency",
        "author": "Wenjie Pei; Tadas Baltrusaitis; David M.J. Tax; Louis-Philippe Morency",
        "abstract": "Typical techniques for sequence classification are designed for well-segmented sequences which have been edited to remove noisy or irrelevant parts. Therefore, such methods cannot be easily applied on noisy sequences expected in real-world applications. In this paper,  we present the Temporal Attention-Gated Model (TAGM) which integrates ideas from attention models and gated recurrent networks to better deal with noisy or unsegmented sequences. Specifically, we extend the concept of attention model to measure the relevance of each observation (time step) of a sequence. We then use a novel gated recurrent network to learn the hidden representation for the final prediction. An important advantage of our approach is interpretability since the temporal attention weights provide a meaningful value for the salience of each time step in the sequence. We demonstrate the merits of our TAGM approach, both for prediction accuracy and interpretability, on three different tasks: spoken digit recognition, text-based sentiment analysis and visual event recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Pei_Temporal_Attention-Gated_Model_CVPR_2017_paper.pdf",
        "aff": "Pattern Recognition Laboratory, Delft University of Technology; Language Technologies Institute, Carnegie Mellon University; Pattern Recognition Laboratory, Delft University of Technology; Language Technologies Institute, Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Pei_Temporal_Attention-Gated_Model_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.00385",
        "pdf_size": 1153737,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14044189976007854907&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "tudelft.nl;cs.cmu.edu;tudelft.nl;cs.cmu.edu",
        "email": "tudelft.nl;cs.cmu.edu;tudelft.nl;cs.cmu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Pei_Temporal_Attention-Gated_Model_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Delft University of Technology;Carnegie Mellon University",
        "aff_unique_dep": "Pattern Recognition Laboratory;Language Technologies Institute",
        "aff_unique_url": "https://www.tudelft.nl;https://www.cmu.edu",
        "aff_unique_abbr": "TUDelft;CMU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "Netherlands;United States"
    },
    {
        "title": "Temporal Convolutional Networks for Action Segmentation and Detection",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "60",
        "author_site": "Colin Lea, Michael D. Flynn, Ren\u00c3\u00a9 Vidal, Austin Reiter, Gregory D. Hager",
        "author": "Colin Lea; Michael D. Flynn; Rene Vidal; Austin Reiter; Gregory D. Hager",
        "abstract": "The ability to identify and temporally segment fine-grained human actions throughout a video is crucial for robotics, surveillance, education, and beyond. Typical approaches decouple this problem by first extracting local spatiotemporal features from video frames and then feeding them into a temporal classifier that captures high-level temporal patterns. We describe a class of temporal models, which we call Temporal Convolutional Networks (TCNs), that use a hierarchy of temporal convolutions to perform fine-grained action segmentation or detection. Our Encoder-Decoder TCN uses pooling and upsampling to efficiently capture long-range temporal patterns whereas our Dilated TCN uses dilated convolutions. We show that TCNs are capable of capturing action compositions, segment durations, and long-range dependencies, and are over a magnitude faster to train than competing LSTM-based Recurrent Neural Networks. We apply these models to three challenging fine-grained datasets and show large improvements over the state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lea_Temporal_Convolutional_Networks_CVPR_2017_paper.pdf",
        "aff": "Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.05267",
        "pdf_size": 733157,
        "gs_citation": 2196,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11702474179718689716&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "jhu.edu;jhu.edu;cis.jhu.edu;cs.jhu.edu;cs.jhu.edu",
        "email": "jhu.edu;jhu.edu;cis.jhu.edu;cs.jhu.edu;cs.jhu.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lea_Temporal_Convolutional_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Temporal Residual Networks for Dynamic Scene Recognition",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1967",
        "author_site": "Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes",
        "author": "Christoph Feichtenhofer; Axel Pinz; Richard P. Wildes",
        "abstract": "This paper combines three contributions to establish a new state-of-the-art in dynamic scene recognition. First, we present a novel ConvNet architecture based on temporal residual units that is fully convolutional in spacetime. Our model augments spatial ResNets with convolutions across time to hierarchically add temporal residuals as the depth of the network increases. Second, existing approaches to video-based recognition are categorized and a baseline of seven previously top performing algorithms is selected for comparative evaluation on dynamic scenes. Third, we introduce a new and challenging video database of dynamic scenes that more than doubles the size of those previously available. This dataset is explicitly split into two subsets of equal size that contain videos with and without camera motion to allow for systematic study of how this variable interacts with the defining dynamics of the scene per se. Our evaluations verify the particular strengths and weaknesses of the baseline algorithms with respect to various scene classes and camera motion parameters. Finally, our temporal ResNet boosts recognition performance and establishes a new state-of-the-art on dynamic scene recognition, as well as on the complementary task of action recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Feichtenhofer_Temporal_Residual_Networks_CVPR_2017_paper.pdf",
        "aff": "Graz University of Technology; Graz University of Technology; York University, Toronto",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Feichtenhofer_Temporal_Residual_Networks_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1335805,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11624743279425253512&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "tugraz.at;tugraz.at;cse.yorku.ca",
        "email": "tugraz.at;tugraz.at;cse.yorku.ca",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Feichtenhofer_Temporal_Residual_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Graz University of Technology;York University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tugraz.at;https://yorku.ca",
        "aff_unique_abbr": "TUGraz;York U",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Toronto",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Austria;Canada"
    },
    {
        "title": "The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "3408",
        "author_site": "Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan Boyd-Graber, Hal Daum\u00c3\u00a9 III, Larry S. Davis",
        "author": "Mohit Iyyer; Varun Manjunatha; Anupam Guha; Yogarshi Vyas; Jordan Boyd-Graber; Hal Daume III; Larry S. Davis",
        "abstract": "Visual narrative is often a combination of explicit information and judicious omissions, relying on the viewer to supply missing details. In comics, most movements in time and space are hidden in the \"gutters\" between panels. To follow the story, readers logically connect panels together by inferring unseen actions through a process called \"closure\". While computers can now describe the content of natural images, in this paper we examine whether they can understand the closure-driven narratives conveyed by stylized artwork and dialogue in comic book panels. We collect a dataset, COMICS, that consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis of COMICS demonstrates that neither text nor image alone can tell a comic book story, so a computer must understand both modalities to keep up with the plot. We introduce three cloze-style tasks that ask models to predict narrative and character-centric aspects of a panel given n preceding panels as context. Various deep neural architectures underperform human baselines on these tasks, suggesting that COMICS contains fundamental challenges for both vision and language.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Iyyer_The_Amazing_Mysteries_CVPR_2017_paper.pdf",
        "aff": "University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Colorado, Boulder; University of Maryland, College Park; University of Maryland, College Park",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Iyyer_The_Amazing_Mysteries_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.05118v2",
        "pdf_size": 2558557,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17784095767785774241&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;colorado.edu;umiacs.umd.edu;umiacs.umd.edu",
        "email": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;colorado.edu;umiacs.umd.edu;umiacs.umd.edu",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Iyyer_The_Amazing_Mysteries_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;1;0;0",
        "aff_unique_norm": "University of Maryland;University of Colorado",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;https://www.colorado.edu",
        "aff_unique_abbr": "UMD;CU",
        "aff_campus_unique_index": "0;0;0;0;1;0;0",
        "aff_campus_unique": "College Park;Boulder",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "The Geometry of First-Returning Photons for Non-Line-Of-Sight Imaging",
        "session": "Computational Photography",
        "status": "Spotlight",
        "track": "main",
        "pid": "3428",
        "author_site": "Chia-Yin Tsai, Kiriakos N. Kutulakos, Srinivasa G. Narasimhan, Aswin C. Sankaranarayanan",
        "author": "Chia-Yin Tsai; Kiriakos N. Kutulakos; Srinivasa G. Narasimhan; Aswin C. Sankaranarayanan",
        "abstract": "Non-line-of-sight (NLOS) imaging utilizes the full 5D light transient measurements to reconstruct scenes beyond the camera's field of view. Mathematically, this requires solving an elliptical tomography problem that unmixes the shape and albedo from spatially-multiplexed measurements of the NLOS scene. In this paper, we propose a new approach for NLOS imaging by studying the properties of first-returning photons from three-bounce light paths. We show that the times of flight of first-returning photons are dependent only on the geometry of the NLOS scene and each observation is almost always generated from a single NLOS scene point. Exploiting these properties, we derive a space carving algorithm for NLOS scenes. In addition, by assuming local planarity, we derive an algorithm to localize NLOS scene points in 3D and estimate their surface normals. Our methods do not require either the full transient measurements or solving the hard elliptical tomography problem. We demonstrate the effectiveness of our methods through simulations as well as real data captured from a SPAD sensor.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Tsai_The_Geometry_of_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Tsai_The_Geometry_of_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18151937512158716845&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Tsai_The_Geometry_of_CVPR_2017_paper.html"
    },
    {
        "title": "The Impact of Typicality for Informative Representative Selection",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2539",
        "author_site": "Jawadul H. Bappy, Sujoy Paul, Ertem Tuncel, Amit K. Roy-Chowdhury",
        "author": "Jawadul H. Bappy; Sujoy Paul; Ertem Tuncel; Amit K. Roy-Chowdhury",
        "abstract": "In computer vision, selection of the most informative samples from a huge pool of training data in order to learn a good recognition model is an active research problem. Furthermore, it is also useful to reduce the annotation cost, as it is time consuming to annotate unlabeled samples. In this paper, motivated by the theories in data compression, we propose a novel sample selection strategy which exploits the concept of typicality from the domain of information theory. Typicality is a simple and powerful technique which can be applied to compress the training data to learn a good classification model. In this work, typicality is used to identify a subset of the most informative samples for labeling, which is then used to update the model using active learning. The proposed model can take advantage of the inter-relationships between data samples. Our approach leads to a significant reduction of manual labeling cost while achieving similar or better recognition performance compared to a model trained with entire training set. This is demonstrated through rigorous experimentation on five datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bappy_The_Impact_of_CVPR_2017_paper.pdf",
        "aff": "Department of ECE, University of California, Riverside, CA 92521, USA; Department of ECE, University of California, Riverside, CA 92521, USA; Department of ECE, University of California, Riverside, CA 92521, USA; Department of ECE, University of California, Riverside, CA 92521, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1226620,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13963624235853181973&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ece.ucr.edu;ece.ucr.edu;ece.ucr.edu;ece.ucr.edu",
        "email": "ece.ucr.edu;ece.ucr.edu;ece.ucr.edu;ece.ucr.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bappy_The_Impact_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Riverside",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.ucr.edu",
        "aff_unique_abbr": "UCR",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Riverside",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "The Incremental Multiresolution Matrix Factorization Algorithm",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1074",
        "author_site": "Vamsi K. Ithapu, Risi Kondor, Sterling C. Johnson, Vikas Singh",
        "author": "Vamsi K. Ithapu; Risi Kondor; Sterling C. Johnson; Vikas Singh",
        "abstract": "Multiresolution analysis and matrix factorization are foundational tools in computer vision. In this work, we study the interface between these two distinct topics and obtain techniques to uncover hierarchical block structure in symmetric matrices -- an important aspect in the success of many vision problems. Our new algorithm, the incremental multiresolution matrix factorization, uncovers such structure one feature at a time, and hence scales well to large matrices. We describe how this multiscale analysis goes much farther than what a direct \"global\" factorization of the data can identify.  We evaluate the efficacy of the resulting factorizations for relative leveraging within regression tasks using medical imaging data.   We also use the factorization on representations learned by popular deep networks,  providing evidence of their ability to infer semantic relationships even when they are not explicitly trained to do so. We show that this algorithm can be used as an exploratory tool to improve the network architecture, and within numerous other settings in vision.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ithapu_The_Incremental_Multiresolution_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ithapu_The_Incremental_Multiresolution_2017_CVPR_supplemental.pdf",
        "arxiv": "1705.05804",
        "pdf_size": 1205765,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3440066516874001062&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ithapu_The_Incremental_Multiresolution_CVPR_2017_paper.html"
    },
    {
        "title": "The Misty Three Point Algorithm for Relative Pose",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1021",
        "author_site": "Tobias Palm\u00c3\u00a9r, Kalle \u00c3\u0085str\u00c3\u00b6m, Jan-Michael Frahm",
        "author": "Tobias Palmer; Kalle Astrom; Jan-Michael Frahm",
        "abstract": "There is a significant interest in scene reconstruction from underwater images given its utility for oceanic research and for recreational image manipulation. In this paper we propose a novel algorithm for two view camera motion estimation for underwater imagery. Our method leverages the constraints provided by the attenuation properties of water and its effects on the appearance of the color to determine the depth difference of a point with respect to the two observing views of the underwater cameras. Additionally, we propose an algorithm, leveraging the depth differences of three such observed points, to estimate the relative pose of the cameras. Given the unknown underwater attenuation coefficients, our method estimates the relative motion up to scale. The results are represented as a generalized camera. We evaluate our method on both real data and simulated data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Palmer_The_Misty_Three_CVPR_2017_paper.pdf",
        "aff": "Center for Mathematical Sciences, Lund University; Center for Mathematical Sciences, Lund University; Department of Computer Science, The University of North Carolina at Chapel Hill",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Palmer_The_Misty_Three_2017_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 5096516,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4028562825594379680&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "maths.lth.se;maths.lth.se;cs.unc.edu",
        "email": "maths.lth.se;maths.lth.se;cs.unc.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Palmer_The_Misty_Three_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Lund University;University of North Carolina at Chapel Hill",
        "aff_unique_dep": "Center for Mathematical Sciences;Department of Computer Science",
        "aff_unique_url": "https://www.lunduniversity.lu.se;https://www.unc.edu",
        "aff_unique_abbr": "LU;UNC Chapel Hill",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Lund;Chapel Hill",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Sweden;United States"
    },
    {
        "title": "The More You Know: Using Knowledge Graphs for Image Classification",
        "session": "Machine Learning 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "968",
        "author_site": "Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta",
        "author": "Kenneth Marino; Ruslan Salakhutdinov; Abhinav Gupta",
        "abstract": "One characteristic that sets humans apart from modern learning-based computer vision algorithms is the ability to acquire knowledge about the world and use that knowledge to reason about the visual world. Humans can learn about the characteristics of objects and the relationships that occur between them to learn a large variety of visual concepts, often with few examples. This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification. We build on recent work on end-to-end learning on graphs, introducing the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. We show in a number of experiments that our method outperforms standard neural network baselines for multi-label classification.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Marino_The_More_You_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1612.04844v2",
        "pdf_size": 51120208,
        "gs_citation": 451,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6456039702951516339&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Marino_The_More_You_CVPR_2017_paper.html"
    },
    {
        "title": "The Surfacing of Multiview 3D Drawings via Lofting and Occlusion Reasoning",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1095",
        "author_site": "Anil Usumezbas, Ricardo Fabbri, Benjamin B. Kimia",
        "author": "Anil Usumezbas; Ricardo Fabbri; Benjamin B. Kimia",
        "abstract": "The three-dimensional reconstruction of scenes from multiple views has made impressive strides in recent years, chiefly by methods correlating isolated feature points, intensities, or curvilinear structure. In the general setting, i.e., without requiring controlled acquisition, limited number of objects, abundant patterns on objects, or object curves to follow particular models, the majority of these methods produce unorganized point clouds, meshes, or voxel representations of the reconstructed scene, with some exceptions producing 3D drawings as networks of curves. Many applications, e.g., robotics, urban planning, industrial design, and hard surface modeling, however, require structured representations which make explicit 3D curves, surfaces, and their spatial relationships. Reconstructing surface representations can now be constrained by the 3D drawing acting like a scaffold to hang on the computed representations, leading to increased robustness and quality of reconstruction. This paper presents one way of completing such 3D drawings with surface reconstructions, by exploring occlusion reasoning through lofting algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Usumezbas_The_Surfacing_of_CVPR_2017_paper.pdf",
        "aff": "SRI International; State University of Rio de Janeiro; Brown University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1707.03946",
        "pdf_size": 3288275,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17657581496144196092&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "sri.com;gmail.com;brown.edu",
        "email": "sri.com;gmail.com;brown.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Usumezbas_The_Surfacing_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "SRI International;State University of Rio de Janeiro;Brown University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.sri.com;http://www.uerj.br;https://www.brown.edu",
        "aff_unique_abbr": "SRI;UERJ;Brown",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Rio de Janeiro",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Brazil"
    },
    {
        "title": "The VQA-Machine: Learning How to Use Existing Vision Algorithms to Answer New Questions",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "402",
        "author_site": "Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel",
        "author": "Peng Wang; Qi Wu; Chunhua Shen; Anton van den Hengel",
        "abstract": "One of the most intriguing features of the Visual Question Answering (VQA) challenge is the unpredictability of the questions. Extracting the information required to answer them demands a variety of image operations from detection and counting, to segmentation and reconstruction. To train a method to perform even one of these operations accurately from image,question,answer tuples would be challenging, but to aim to achieve them all with a limited set of such training data seems ambitious at best. Our method thus learns how to exploit a set of external off-the-shelf algorithms to achieve its goal, an approach that has something in common with the Neural Turing Machine. The core of our proposed method is a new co-attention model. In addition, the proposed approach generates human-readable reasons for its decision, and can still be trained end-to-end without ground truth reasons being given. We demonstrate the effectiveness on two publicly available datasets, Visual Genome and VQA, and show that it produces the state-of-the-art results in both cases.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_The_VQA-Machine_Learning_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1190224,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4385004433222977639&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_The_VQA-Machine_Learning_CVPR_2017_paper.html"
    },
    {
        "title": "The World of Fast Moving Objects",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "2157",
        "author_site": "Denys Rozumnyi, Jan Kotera, Filip \u00c5\u00a0roubek, Luk\u00c3\u00a1\u00c5\u00a1 Novotn\u00c3\u00bd, Ji\u00c5\u0099\u00c3\u00ad Matas",
        "author": "Denys Rozumnyi; Jan Kotera; Filip Sroubek; Lukas Novotny; Jiri Matas",
        "abstract": "The notion of a Fast Moving Object (FMO), i.e. an object that moves over a distance exceeding its size within the exposure time, is introduced. FMOs may, and typically do, rotate with high angular speed. FMOs are very common in sports videos, but are not rare elsewhere. In a single frame, such objects are often barely visible and appear as semitransparent streaks. A method for the detection and tracking of FMOs is proposed. The method consists of three distinct algorithms, which form an efficient localization pipeline that operates successfully in a broad range of conditions. We show that it is possible to recover the appearance of the object and its axis of rotation, despite its blurred appearance. The proposed method is evaluated on a new annotated dataset. The results show that existing trackers are inadequate for the problem of FMO localization and a new approach is required. Two applications of localization, temporal superresolution and highlighting, are presented.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rozumnyi_The_World_of_CVPR_2017_paper.pdf",
        "aff": "Center for Machine Perception, Faculty of Electrical Engineering, Czech Technical University in Prague + Department of Signal Processing, Tampere University of Technology; Institute of Information Theory and Automation, Czech Academy of Sciences; Institute of Information Theory and Automation, Czech Academy of Sciences; Center for Machine Perception, Faculty of Electrical Engineering, Czech Technical University in Prague; Center for Machine Perception, Faculty of Electrical Engineering, Czech Technical University in Prague",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.07889v1",
        "pdf_size": 5501475,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10443871812525178856&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Rozumnyi_The_World_of_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2;2;0;0",
        "aff_unique_norm": "Czech Technical University in Prague;Tampere University of Technology;Czech Academy of Sciences",
        "aff_unique_dep": "Faculty of Electrical Engineering;Department of Signal Processing;Institute of Information Theory and Automation",
        "aff_unique_url": "https://www.cvut.cz;https://www.tut.fi;https://www.cas.cz",
        "aff_unique_abbr": "CTU;TUT;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Prague;",
        "aff_country_unique_index": "0+1;0;0;0;0",
        "aff_country_unique": "Czech Republic;Finland"
    },
    {
        "title": "Thin-Slicing Network: A Deep Structured Model for Pose Estimation in Videos",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Oral",
        "track": "main",
        "pid": "1719",
        "author_site": "Jie Song, Limin Wang, Luc Van Gool, Otmar Hilliges",
        "author": "Jie Song; Limin Wang; Luc Van Gool; Otmar Hilliges",
        "abstract": "Deep ConvNets have been shown to be effective for the task of human pose estimation from single images. However, several challenging issues arise in the video-based case such as self-occlusion, motion blur, and uncommon poses with few or no examples in the training data. Temporal information can provide additional cues about the location of body joints and help to alleviate these issues. In this paper, we propose a deep structured model to estimate a sequence of human poses in unconstrained videos. This model can be efficiently trained in an end-to-end manner and is capable of representing the appearance of body joints and their spatio-temporal relationships simultaneously. Domain knowledge about the human body is explicitly incorporated into the network providing effective priors to regularize the skeletal structure and to enforce temporal consistency. The proposed end-to-end architecture is evaluated on two widely used benchmarks for video-based pose estimation (Penn Action and JHMDB datasets). Our approach  outperforms several state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Song_Thin-Slicing_Network_A_CVPR_2017_paper.pdf",
        "aff": "AIT Lab, ETH Zurich; Computer Vision Lab, ETH Zurich; Computer Vision Lab, ETH Zurich; AIT Lab, ETH Zurich",
        "project": "",
        "github": "https://github.com/JieSong89/thin-slicing-network",
        "supp": "",
        "arxiv": "1703.10898v1",
        "pdf_size": 1256236,
        "gs_citation": 159,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1767321015926737689&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Song_Thin-Slicing_Network_A_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "AIT Lab",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Zurich",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Top-Down Visual Saliency Guided by Captions",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3426",
        "author_site": "Vasili Ramanishka, Abir Das, Jianming Zhang, Kate Saenko",
        "author": "Vasili Ramanishka; Abir Das; Jianming Zhang; Kate Saenko",
        "abstract": "Neural image/video captioning models can generate accurate descriptions, but their internal process of mapping regions to words is a black box and therefore difficult to explain. Top-down neural saliency methods can find important regions given a high-level semantic task such as object classification, but cannot use a natural language sentence as the top-down input for the task. In this paper, we propose Caption-Guided Visual Saliency to expose the region-to-word mapping in modern encoder-decoder networks and demonstrate that it is learned implicitly from caption training data, without any pixel-level annotations. Our approach can produce spatial or spatiotemporal heatmaps for both predicted captions, and for arbitrary query sentences. It recovers saliency without the overhead of introducing explicit attention layers, and can be used to analyze a variety of existing model architectures and improve their design. Evaluation on large-scale video and image datasets demonstrates that our approach achieves comparable captioning performance with existing methods while providing more accurate saliency heatmaps. Our code is available at visionlearninggroup.github.io/caption-guided-saliency/",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ramanishka_Top-Down_Visual_Saliency_CVPR_2017_paper.pdf",
        "aff": "Boston University; Boston University; Adobe Research; Boston University",
        "project": "",
        "github": "visionlearninggroup.github.io/caption-guided-saliency/",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Ramanishka_Top-Down_Visual_Saliency_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.07360v2",
        "pdf_size": 1230488,
        "gs_citation": 181,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11965974300107586554&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "bu.edu;bu.edu;adobe.com;bu.edu",
        "email": "bu.edu;bu.edu;adobe.com;bu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ramanishka_Top-Down_Visual_Saliency_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Boston University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.bu.edu;https://research.adobe.com",
        "aff_unique_abbr": "BU;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Toroidal Constraints for Two-Point Localization Under High Outlier Ratios",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1874",
        "author_site": "Federico Camposeco, Torsten Sattler, Andrea Cohen, Andreas Geiger, Marc Pollefeys",
        "author": "Federico Camposeco; Torsten Sattler; Andrea Cohen; Andreas Geiger; Marc Pollefeys",
        "abstract": "Localizing a query image against a 3D model at large scale is a hard problem, since 2D-3D matches become more and more ambiguous as the model size increases. This creates a need for pose estimation strategies that can handle very low inlier ratios. In this paper, we draw new insights on the geometric information available from the 2D-3D matching process. As modern descriptors are not invariant against large variations in viewpoint, we are able to find the rays in space used to triangulate a given point that are closest to a query descriptor. It is well known that two correspondences constrain the camera to lie on the surface of a torus. Adding the knowledge of direction of triangulation, we are able to approximate the position of the camera from two matches alone. We derive a geometric solver that can compute this position in under 1 microsecond. Using this solver, we propose a simple yet powerful outlier filter which scales quadratically in the number of matches. We validate the accuracy of our solver and demonstrate the usefulness of our method in real world settings.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Camposeco_Toroidal_Constraints_for_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, ETH Z\u00fcrich; Department of Computer Science, ETH Z\u00fcrich; Department of Computer Science, ETH Z\u00fcrich; Department of Computer Science, ETH Z\u00fcrich + Autonomous Vision Group, MPI for Intelligent Systems T\u00fcbingen; Department of Computer Science, ETH Z\u00fcrich",
        "project": "www.cvg.ethz.ch/research/toroidal-constraints/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1274234,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10157595119989012314&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Camposeco_Toroidal_Constraints_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0+1;0",
        "aff_unique_norm": "ETH Zurich;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Department of Computer Science;Autonomous Vision Group",
        "aff_unique_url": "https://www.ethz.ch;https://www.mpituebingen.mpg.de",
        "aff_unique_abbr": "ETHZ;MPI-IS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";T\u00fcbingen",
        "aff_country_unique_index": "0;0;0;0+1;0",
        "aff_country_unique": "Switzerland;Germany"
    },
    {
        "title": "Towards Accurate Multi-Person Pose Estimation in the Wild",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2022",
        "author_site": "George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander Toshev, Jonathan Tompson, Chris Bregler, Kevin Murphy",
        "author": "George Papandreou; Tyler Zhu; Nori Kanazawa; Alexander Toshev; Jonathan Tompson; Chris Bregler; Kevin Murphy",
        "abstract": "We propose a method for multi-person detection and 2-D pose estimation that achieves state-of-art results on the challenging COCO keypoints task. It is a simple, yet powerful, top-down approach consisting of two stages.  In the first stage, we predict the location and scale of boxes which are likely to contain people; for this we use the Faster RCNN detector. In the second stage, we estimate the keypoints of the person potentially contained in each proposed bounding box. For each keypoint type we predict dense heatmaps and offsets using a fully convolutional ResNet. To combine these outputs we introduce a novel aggregation procedure to obtain highly localized keypoint predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression (NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based confidence score estimation, instead of box-level scoring.  Trained on COCO data alone, our final system achieves average precision of 0.649 on the COCO test-dev set and the 0.643 test-standard sets, outperforming the winner of the 2016 COCO keypoints challenge and other recent state-of-art. Further, by using additional in-house labeled data we obtain an even higher average precision of 0.685 on the test-dev set and 0.673 on the test-standard set, more than 5% absolute improvement compared to the previous best performing method on the same dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Papandreou_Towards_Accurate_Multi-Person_CVPR_2017_paper.pdf",
        "aff": "Google, Inc.; Google, Inc.; Google, Inc.; Google, Inc.; Google, Inc.; Google, Inc.; Google, Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.01779v2",
        "pdf_size": 4061600,
        "gs_citation": 1144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6244481413102220403&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Papandreou_Towards_Accurate_Multi-Person_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Towards a Quality Metric for Dense Light Fields",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "28",
        "author_site": "Vamsi Kiran Adhikarla, Marek Vinkler, Denis Sumin, Rafa\u00c5\u0082 K. Mantiuk, Karol Myszkowski, Hans-Peter Seidel, Piotr Didyk",
        "author": "Vamsi Kiran Adhikarla; Marek Vinkler; Denis Sumin; Rafal K. Mantiuk; Karol Myszkowski; Hans-Peter Seidel; Piotr Didyk",
        "abstract": "Light fields become a popular representation of three-dimensional scenes, and there is interest in their processing, resampling, and compression. As those operations often result in loss of quality, there is a need to quantify it. In this work, we collect a new dataset of dense reference and distorted light fields as well as the corresponding quality scores which are scaled in perceptual units. The scores were acquired in a subjective experiment using an interactive light-field viewing setup. The dataset contains typical artifacts that occur in light-field processing chain due to light-field reconstruction, multi-view compression, and limitations of automultiscopic displays. We test a number of existing objective quality metrics to determine how well they can predict the quality of light fields. We find that the existing image quality metrics provide good measures of light-field quality, but require dense reference light- fields for optimal performance. For more complex tasks of comparing two distorted light fields, their performance drops significantly, which reveals the need for new, light-field-specific metrics.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Adhikarla_Towards_a_Quality_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Adhikarla_Towards_a_Quality_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.07576",
        "pdf_size": 3176948,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2869854144340338140&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Adhikarla_Towards_a_Quality_CVPR_2017_paper.html"
    },
    {
        "title": "Tracking by Natural Language Specification",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2925",
        "author_site": "Zhenyang Li, Ran Tao, Efstratios Gavves, Cees G. M. Snoek, Arnold W.M. Smeulders",
        "author": "Zhenyang Li; Ran Tao; Efstratios Gavves; Cees G. M. Snoek; Arnold W.M. Smeulders",
        "abstract": "This paper strives to track a target object in a video. Rather than specifying the target in the first frame of a video by a bounding box, we propose to track the object based on a natural language specification of the target, which provides a more natural human-machine interaction as well as a means to improve tracking results. We define three variants of tracking by language specification: one relying on lingual target specification only, one relying on visual target specification based on language, and one leveraging their joint capacity. To show the potential of tracking by natural language specification we extend two popular tracking datasets with lingual descriptions and report experiments. Finally, we also sketch new tracking scenarios in surveillance and other live video streams that become feasible with a lingual specification of the target.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Tracking_by_Natural_CVPR_2017_paper.pdf",
        "aff": "QUV A Lab, University of Amsterdam; QUV A Lab, University of Amsterdam; QUV A Lab, University of Amsterdam; QUV A Lab, University of Amsterdam; QUV A Lab, University of Amsterdam",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4824771,
        "gs_citation": 206,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2173929726176688980&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Tracking_by_Natural_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Amsterdam",
        "aff_unique_dep": "QUV A Lab",
        "aff_unique_url": "https://www.uva.nl",
        "aff_unique_abbr": "UvA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "title": "Training Object Class Detectors With Click Supervision",
        "session": "3D Vision 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "2827",
        "author_site": "Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, Vittorio Ferrari",
        "author": "Dim P. Papadopoulos; Jasper R. R. Uijlings; Frank Keller; Vittorio Ferrari",
        "abstract": "Training object class detectors typically requires a large set of images with objects annotated by bounding boxes. However, manually drawing bounding boxes is very time consuming. In this paper we greatly reduce annotation time by proposing center-click annotations: we ask annotators to click on the center of an imaginary bounding box which tightly encloses the object instance. We then incorporate these clicks into existing Multiple Instance Learning techniques for weakly supervised object localization, to jointly localize object bounding boxes over all training images. Extensive experiments on PASCAL VOC 2007 and MS COCO show that: (1) our scheme delivers high-quality detectors, performing substantially better than those produced by weakly supervised techniques, with a modest extra annotation effort; (2) these detectors in fact perform in a range close to those trained from manually drawn bounding boxes; (3) as the center-click task is very fast, our scheme reduces total annotation time by 9x to 18x.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Papadopoulos_Training_Object_Class_CVPR_2017_paper.pdf",
        "aff": "University of Edinburgh; Google Research; University of Edinburgh; University of Edinburgh+Google Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.06189",
        "pdf_size": 1849924,
        "gs_citation": 157,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3316849494373406788&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ed.ac.uk;google.com;inf.ed.ac.uk;inf.ed.ac.uk",
        "email": "ed.ac.uk;google.com;inf.ed.ac.uk;inf.ed.ac.uk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Papadopoulos_Training_Object_Class_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0+1",
        "aff_unique_norm": "University of Edinburgh;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.ed.ac.uk;https://research.google",
        "aff_unique_abbr": "Edinburgh;Google Research",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;0;0+1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Transformation-Grounded Image Generation Network for Novel 3D View Synthesis",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1299",
        "author_site": "Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan, Alexander C. Berg",
        "author": "Eunbyung Park; Jimei Yang; Ersin Yumer; Duygu Ceylan; Alexander C. Berg",
        "abstract": "We present a transformation-grounded image generation network for novel 3D view synthesis from a single image. Our approach first explicitly infers the parts of the geometry visible both in the input and novel views and then casts the remaining synthesis problem as image completion. Specifically, we both predict a flow to move the pixels from the input to the novel view along with a novel visibility map that helps deal with occulsion/disocculsion. Next, conditioned on those intermediate results, we hallucinate (infer) parts of the object invisible in the input image. In addition to the new network structure, training with a combination of adversarial and perceptual loss results in a reduction in common artifacts of novel view synthesis such as distortions and holes, while successfully generating high frequency details and preserving visual aspects of the input image. We evaluate our approach on a wide range of synthetic and real examples. Both qualitative and quantitative results show our method achieves significantly better results compared to existing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Park_Transformation-Grounded_Image_Generation_CVPR_2017_paper.pdf",
        "aff": "University of North Carolina at Chapel Hill; Adobe Research; Adobe Research; Adobe Research; University of North Carolina at Chapel Hill",
        "project": "http://www.cs.unc.edu/~eunbyung/tvsnserved",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Park_Transformation-Grounded_Image_Generation_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.02921v1",
        "pdf_size": 2435063,
        "gs_citation": 346,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=972043207656112313&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.unc.edu;adobe.com;adobe.com;adobe.com;cs.unc.edu",
        "email": "cs.unc.edu;adobe.com;adobe.com;adobe.com;cs.unc.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Park_Transformation-Grounded_Image_Generation_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of North Carolina;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.unc.edu;https://research.adobe.com",
        "aff_unique_abbr": "UNC;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chapel Hill;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Transition Forests: Learning Discriminative Temporal Transitions for Action Recognition and Detection",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "143",
        "author_site": "Guillermo Garcia-Hernando, Tae-Kyun Kim",
        "author": "Guillermo Garcia-Hernando; Tae-Kyun Kim",
        "abstract": "A human action can be seen as transitions between one's body poses over time, where the transition depicts a temporal relation between two poses. Recognizing actions thus involves learning a classifier sensitive to these pose transitions as well as to static poses. In this paper, we introduce a novel method called transitions forests, an ensemble of decision trees that both learn to discriminate static poses and transitions between pairs of two independent frames. During training, node splitting is driven by alternating two criteria: the standard classification objective that maximizes the discrimination power in individual frames, and the proposed one in pairwise frame transitions. Growing the trees tends to group frames that have similar associated transitions and share same action label incorporating temporal information that was not available otherwise. Unlike conventional decision trees where the best split in a node is determined independently of other nodes, the transition forests try to find the best split of nodes jointly (within a layer) for incorporating distant node transitions. When inferring the class label of a new frame, it is passed down the trees and the prediction is made based on previous frame predictions and the current one in an efficient and online manner. We apply our method on varied skeleton action recognition and online detection datasets showing its suitability over several baselines and state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Garcia-Hernando_Transition_Forests_Learning_CVPR_2017_paper.pdf",
        "aff": "Imperial College London; Imperial College London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1607.02737",
        "pdf_size": 2941234,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8379675958144602788&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Garcia-Hernando_Transition_Forests_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Imperial College London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.imperial.ac.uk",
        "aff_unique_abbr": "ICL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Truncated Max-Of-Convex Models",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "875",
        "author_site": "Pankaj Pansari, M. Pawan Kumar",
        "author": "Pankaj Pansari; M. Pawan Kumar",
        "abstract": "Truncated convex models (TCM) are a special case of pair- wise random fields that have been widely used in computer vision. However, by restricting the order of the potentials to be at most two, they fail to capture useful image statistics. We propose a natural generalization of TCM to high-order random fields, which we call truncated max-of-convex models (TMCM). The energy function of TMCM consists of two types of potentials: (i) unary potential, which has no restriction on its form; and (ii) high-order potential, which is the sum of the truncation of the m largest convex distances over disjoint pairs of random variables in an arbitrary size clique. The use of a convex distance function encourages smoothness, while truncation allows for discontinuities in the labeling. By using m > 1, TMCM provides robustness towards errors in the definition of the cliques. In order to minimize the energy function of a TMCM over all possible labelings, we design an efficient st-mincut based range expansion algorithm. We prove the accuracy of our algorithm by establishing strong multiplicative bounds for several special cases of interest. Using synthetic and standard real datasets, we demonstrate the benefit of our high-order TMCM over pairwise TCM, as well as the benefit of our range expansion algorithm over other st-mincut based approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Pansari_Truncated_Max-Of-Convex_Models_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Pansari_Truncated_Max-Of-Convex_Models_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6644855549553597627&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Pansari_Truncated_Max-Of-Convex_Models_CVPR_2017_paper.html"
    },
    {
        "title": "Turning an Urban Scene Video Into a Cinemagraph",
        "session": "Computational Photography",
        "status": "Poster",
        "track": "main",
        "pid": "137",
        "author_site": "Hang Yan, Yebin Liu, Yasutaka Furukawa",
        "author": "Hang Yan; Yebin Liu; Yasutaka Furukawa",
        "abstract": "This paper proposes an algorithm that turns a regular video capturing urban scenes into a high-quality endless animation, known as a Cinemagraph. The creation of a Cinemagraph usually requires a static camera in a carefully configured scene. The task becomes challenging for a regular video with a moving camera and objects. Our approach first warps an input video into the viewpoint of a reference camera. Based on the warped video, we propose effective temporal analysis algorithms to detect regions with static geometry and dynamic appearance, where geometric modeling is reliable and visually attractive animations can be created. Lastly, the algorithm applies a sequence of video processing techniques to produce a Cinemagraph movie. We have tested the proposed approach on numerous challenging real scenes. To our knowledge, this work is the first to automatically generate Cinemagraph animations from regular movies in the wild.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yan_Turning_an_Urban_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "http://yanhangpublic.github.io/cinemagraph",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yan_Turning_an_Urban_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.01235v1",
        "pdf_size": 1225461,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5070132512084216515&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yan_Turning_an_Urban_CVPR_2017_paper.html"
    },
    {
        "title": "Ubernet: Training a Universal Convolutional Neural Network for Low-, Mid-, and High-Level Vision Using Diverse Datasets and Limited Memory",
        "session": "Machine Learning 4",
        "status": "Oral",
        "track": "main",
        "pid": "2694",
        "author": "Iasonas Kokkinos",
        "abstract": "In this work we  train in an end-to-end manner a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a  unified architecture.  Such a network can act like a `swiss knife' for vision tasks;  we call it an \"UberNet\" to indicate its overarching nature.    The main contribution of this work consists in handling challenges that emerge when scaling up to many tasks. We introduce  techniques that facilitate (i) training a deep architecture while relying on diverse training sets and (ii) training many (potentially unlimited) tasks with a limited memory budget.  This allows us to train in an end-to-end manner a unified CNN architecture that jointly handles (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) human part segmentation (f) semantic boundary detection, (g) region proposal generation and object detection. We obtain competitive performance while jointly addressing all tasks in 0.7 seconds on a GPU.  Our system will be made publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kokkinos_Ubernet_Training_a_CVPR_2017_paper.pdf",
        "aff": "University College London & Facebook Artificial Intelligence Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1153229,
        "gs_citation": 855,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9004211426848593866&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk",
        "author_num": 1,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kokkinos_Ubernet_Training_a_CVPR_2017_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "UltraStereo: Efficient Learning-Based Matching for Active Stereo Systems",
        "session": "Machine Learning for 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "977",
        "author_site": "Sean Ryan Fanello, Julien Valentin, Christoph Rhemann, Adarsh Kowdle, Vladimir Tankovich, Philip Davidson, Shahram Izadi",
        "author": "Sean Ryan Fanello; Julien Valentin; Christoph Rhemann; Adarsh Kowdle; Vladimir Tankovich; Philip Davidson; Shahram Izadi",
        "abstract": "Efficient estimation of depth from pairs of stereo images is one of the core problems in computer vision. We efficiently solve the specialized problem of stereo matching under active illumination using a new learning-based algorithm. This type of 'active' stereo i.e. stereo matching where scene texture is augmented by an active light projector is proving compelling for designing depth cameras, largely due to improved robustness when compared to time of flight or traditional structured light techniques. Our algorithm uses an unsupervised greedy optimization scheme that learns features that are discriminative for estimating correspondences in infrared images. The proposed method optimizes a series of sparse hyperplanes that are used at test time to remap all the image patches into a compact binary representation in O(1). The proposed algorithm is cast in a PatchMatch Stereo-like framework, producing depth maps at 500Hz. In contrast to standard structured light methods, our approach generalizes to different scenes, does not require tedious per camera calibration procedures and is not adversely affected by interference from overlapping sensors. Extensive evaluations show we surpass the quality and overcome the limitations of current depth sensing technologies.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Fanello_UltraStereo_Efficient_Learning-Based_CVPR_2017_paper.pdf",
        "aff": "perceptiveIO; perceptiveIO; perceptiveIO; perceptiveIO; perceptiveIO; perceptiveIO; perceptiveIO",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2392118,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15072124232607504265&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Fanello_UltraStereo_Efficient_Learning-Based_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "perceptiveIO",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Unambiguous Text Localization and Retrieval for Cluttered Scenes",
        "session": "Object Recognition & Scene Understanding 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "2326",
        "author_site": "Xuejian Rong, Chucai Yi, Yingli Tian",
        "author": "Xuejian Rong; Chucai Yi; Yingli Tian",
        "abstract": "Text instance as one category of self-described objects provides valuable information for understanding and describing cluttered scenes. In this paper, we explore the task of unambiguous text localization and retrieval, to accurately localize a specific targeted text instance in a cluttered image given a natural language description that refers to it. To address this issue, first a novel recurrent Dense Text Localization Network (DTLN) is proposed to sequentially decode the intermediate convolutional representations of a cluttered scene image into a set of distinct text instance detections. Our approach avoids repeated detections at multiple scales of the same text instance by recurrently memorizing previous detections, and effectively tackles crowded text instances in close proximity. Second, we propose a Context Reasoning Text Retrieval (CRTR) model, which jointly encodes text instances and their context information through a recurrent network, and ranks localized text bounding boxes by a scoring function of context compatibility. Quantitative evaluations on standard scene text localization benchmarks and a newly collected scene text retrieval dataset demonstrate the effectiveness and advantages of our models for both scene text localization and retrieval.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rong_Unambiguous_Text_Localization_CVPR_2017_paper.pdf",
        "aff": "The City College, City University of New York, USA; HERE North America LLC, USA; The City College, City University of New York, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2848148,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17309482402722624908&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "ccny.cuny.edu;here.com;ccny.cuny.edu",
        "email": "ccny.cuny.edu;here.com;ccny.cuny.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Rong_Unambiguous_Text_Localization_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "City University of New York;HERE North America LLC",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cuny.edu;",
        "aff_unique_abbr": "CUNY;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "City College;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Understanding Traffic Density From Large-Scale Web Camera Data",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "2551",
        "author_site": "Shanghang Zhang, Guanhang Wu, Jo\u00c3\u00a3o P. Costeira, Jos\u00c3\u00a9 M. F. Moura",
        "author": "Shanghang Zhang; Guanhang Wu; Joao P. Costeira; Jose M. F. Moura",
        "abstract": "Understanding traffic density from large-scale web camera (webcam) videos is a challenging problem because such videos have low spatial and temporal resolution, high occlusion and large perspective. To deeply understand traffic density, we explore both optimization based and deep learning based methods. To avoid individual vehicle detection or tracking, both methods map the dense image feature into vehicle density, one based on rank constrained regression and the other based on fully convolutional networks (FCN). The regression based method learns different weights for different blocks of the image to embed road geometry and significantly reduce the error induced by camera perspective. The FCN based method jointly estimates vehicle density and vehicle count with a residual learning framework to perform end-to-end dense prediction, allowing arbitrary image resolution, and adapting to different vehicle scales and perspectives. We analyze and compare both methods, and get insights from optimization based method to improve deep model. Since existing datasets do not cover all the challenges in our work, we collected and labelled a large-scale traffic video dataset, containing 60 million frames from 212 webcams. Both methods are extensively evaluated and compared on different counting tasks and datasets. FCN based method significantly reduces the mean absolute error (MAE) from 10.99 to 5.31 on the public dataset TRANCOS compared with the state-of-the-art baseline.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Understanding_Traffic_Density_CVPR_2017_paper.pdf",
        "aff": "Carnegie Mellon University, Pittsburgh, PA, USA+ISR - IST, Universidade de Lisboa, Lisboa, Portugal; Carnegie Mellon University, Pittsburgh, PA, USA; ISR - IST, Universidade de Lisboa, Lisboa, Portugal; Carnegie Mellon University, Pittsburgh, PA, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1162482,
        "gs_citation": 191,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2522222511256078972&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;isr.ist.utl.pt;andrew.cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;isr.ist.utl.pt;andrew.cmu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Understanding_Traffic_Density_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Universidade de Lisboa",
        "aff_unique_dep": ";ISR - IST",
        "aff_unique_url": "https://www.cmu.edu;https://www.ulusiada.pt",
        "aff_unique_abbr": "CMU;UL",
        "aff_campus_unique_index": "0+1;0;1;0",
        "aff_campus_unique": "Pittsburgh;Lisboa",
        "aff_country_unique_index": "0+1;0;1;0",
        "aff_country_unique": "United States;Portugal"
    },
    {
        "title": "Unified Embedding and Metric Learning for Zero-Exemplar Event Detection",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "379",
        "author_site": "Noureldien Hussein, Efstratios Gavves, Arnold W.M. Smeulders",
        "author": "Noureldien Hussein; Efstratios Gavves; Arnold W.M. Smeulders",
        "abstract": "Event detection in unconstrained videos is conceived as a content-based video retrieval with two modalities: textual and visual. Given a text describing a novel event, the goal is to rank related videos accordingly. This task is zero-exemplar, no video examples are given to the novel event. Related works train a bank of concept detectors on external data sources. These detectors predict confidence scores for test videos, which are ranked and retrieved accordingly. In contrast, we learn a joint space in which the visual and textual representations are embedded. The space casts a novel event as a probability of pre-defined events. Also, it learns to measure the distance between an event and its related videos. Our model is trained end-to-end on publicly available EventNet. When applied to TRECVID Multimedia Event Detection dataset, it outperforms the state-of-the-art by a considerable margin.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hussein_Unified_Embedding_and_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.02148",
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4957700641328572316&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hussein_Unified_Embedding_and_CVPR_2017_paper.html"
    },
    {
        "title": "Unite the People: Closing the Loop Between 3D and 2D Human Representations",
        "session": "Analyzing Humans in Images",
        "status": "Poster",
        "track": "main",
        "pid": "2644",
        "author_site": "Christoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael J. Black, Peter V. Gehler",
        "author": "Christoph Lassner; Javier Romero; Martin Kiefel; Federica Bogo; Michael J. Black; Peter V. Gehler",
        "abstract": "3D models provide a common ground for different representations of human bodies. In turn, robust 2D estimation has proven to be a powerful tool to obtain 3D fits \"in-the-wild\". However, depending on the level of detail, it can be hard to impossible to acquire labeled data for training 2D estimators on large scale. We propose a hybrid approach to this problem: with an extended version of the recently introduced SMPLify method, we obtain high quality 3D body model fits for multiple human pose datasets. Human annotators solely sort good and bad fits. This procedure leads to an initial dataset, UP-3D, with rich annotations. With a comprehensive set of experiments, we show how this data can be used to train discriminative models that produce results with an unprecedented level of detail: our models predict 31 segments and 91 landmark locations on the body. Using the 91 landmark pose estimator, we present state-of-the art results for 3D human pose and shape estimation using an order of magnitude less training data and without assumptions about gender or pose in the fitting procedure. We show that UP-3D can be enhanced with these improved fits to grow in quantity and quality, which makes the system deployable on large scale. The data, code and models are available for research purposes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Lassner_Unite_the_People_CVPR_2017_paper.pdf",
        "aff": "Bernstein Center for Computational Neuroscience, T\u00fcbingen, Germany+MPI for Intelligent Systems, T\u00fcbingen, Germany; Body Labs Inc., New York, United States; MPI for Intelligent Systems, T\u00fcbingen, Germany; Microsoft, Cambridge, UK; MPI for Intelligent Systems, T\u00fcbingen, Germany; University of W\u00fcrzburg, Germany+MPI for Intelligent Systems, T\u00fcbingen, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.02468v3",
        "pdf_size": 784619,
        "gs_citation": 680,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8996340826671872379&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "tue.mpg.de;bodylabs.com;tue.mpg.de;microsoft.com;tue.mpg.de;tue.mpg.de",
        "email": "tue.mpg.de;bodylabs.com;tue.mpg.de;microsoft.com;tue.mpg.de;tue.mpg.de",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Lassner_Unite_the_People_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;2;1;3;1;4+1",
        "aff_unique_norm": "Bernstein Center for Computational Neuroscience;Max Planck Institute for Intelligent Systems;Body Labs Inc.;Microsoft;University of W\u00fcrzburg",
        "aff_unique_dep": "Computational Neuroscience;;;Microsoft;",
        "aff_unique_url": ";https://www.mpituebingen.mpg.de;;https://www.microsoft.com;https://www.uni-wuerzburg.de",
        "aff_unique_abbr": ";MPI-IS;;MSFT;UWue",
        "aff_campus_unique_index": "0+0;0;2;0;0",
        "aff_campus_unique": "T\u00fcbingen;;Cambridge",
        "aff_country_unique_index": "0+0;1;0;2;0;0+0",
        "aff_country_unique": "Germany;United States;United Kingdom"
    },
    {
        "title": "Universal Adversarial Perturbations",
        "session": "Machine Learning 1",
        "status": "Oral",
        "track": "main",
        "pid": "649",
        "author_site": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard",
        "author": "Seyed-Mohsen Moosavi-Dezfooli; Alhussein Fawzi; Omar Fawzi; Pascal Frossard",
        "abstract": "Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_CVPR_2017_paper.pdf",
        "aff": "\u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Switzerland+\u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Switzerland; \u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Switzerland+\u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Switzerland; ENS de Lyon, LIP, UMR 5668 ENS Lyon - CNRS - UCBL - INRIA, Universit\u00b4e de Lyon, France; \u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Switzerland",
        "project": "https://youtu.be/jhOu5yhe0rc",
        "github": "https://github.com/LTS4/universal",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_2017_CVPR_supplemental.pdf",
        "arxiv": "1610.08401",
        "pdf_size": 2134607,
        "gs_citation": 3430,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9478919746797532610&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "epfl.ch;gmail.com;ens-lyon.fr;epfl.ch",
        "email": "epfl.ch;gmail.com;ens-lyon.fr;epfl.ch",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_CVPR_2017_paper.html",
        "aff_unique_index": "0+0;0+0;1;0",
        "aff_unique_norm": "EPFL;ENS de Lyon",
        "aff_unique_dep": ";LIP, UMR 5668 ENS Lyon - CNRS - UCBL - INRIA",
        "aff_unique_url": "https://www.epfl.ch;https://www.ens-lyon.fr",
        "aff_unique_abbr": "EPFL;ENS de Lyon",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;1;0",
        "aff_country_unique": "Switzerland;France"
    },
    {
        "title": "Unrolling the Shutter: CNN to Correct Motion Distortions",
        "session": "Computational Photography",
        "status": "Oral",
        "track": "main",
        "pid": "833",
        "author_site": "Vijay Rengarajan, Yogesh Balaji, A. N. Rajagopalan",
        "author": "Vijay Rengarajan; Yogesh Balaji; A. N. Rajagopalan",
        "abstract": "Row-wise exposure delay present in CMOS cameras is responsible for skew and curvature distortions known as the rolling shutter (RS) effect while imaging under camera motion. Existing RS correction methods resort to using multiple images or tailor scene-specific correction schemes. We propose a convolutional neural network (CNN) architecture that automatically learns essential scene features from a single RS image to estimate the row-wise camera motion and undo RS distortions back to the time of first-row exposure. We employ long rectangular kernels to specifically learn the effects produced by the row-wise exposure. Experiments reveal that our proposed architecture performs better than the conventional CNN employing square kernels. Our single-image correction method fares well even operating in a frame-by-frame manner against video-based methods and performs better than scene-specific correction schemes even under challenging situations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rengarajan_Unrolling_the_Shutter_CVPR_2017_paper.pdf",
        "aff": "Indian Institute of Technology Madras; University of Maryland; Indian Institute of Technology Madras",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Rengarajan_Unrolling_the_Shutter_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3863779,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=683863316164611260&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ee.iitm.ac.in; ; ",
        "email": "ee.iitm.ac.in; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Rengarajan_Unrolling_the_Shutter_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Indian Institute of Technology Madras;University of Maryland",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitm.ac.in;https://www/umd.edu",
        "aff_unique_abbr": "IIT Madras;UMD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madras;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "Unsupervised Adaptive Re-Identification in Open World Dynamic Camera Networks",
        "session": "Image Motion & Tracking; Video Analysis",
        "status": "Spotlight",
        "track": "main",
        "pid": "3289",
        "author_site": "Rameswar Panda, Amran Bhuiyan, Vittorio Murino, Amit K. Roy-Chowdhury",
        "author": "Rameswar Panda; Amran Bhuiyan; Vittorio Murino; Amit K. Roy-Chowdhury",
        "abstract": "Person re-identification is an open and challenging problem in computer vision. Existing approaches have concentrated on either designing the best feature representation or learning optimal matching metrics in a static setting where the number of cameras are fixed in a network. Most approaches have neglected the dynamic and open world nature of the re-identification problem, where a new camera may be temporarily inserted into an existing system to get additional information. To address such a novel and very practical problem, we propose an unsupervised adaptation scheme for re-identification models in a dynamic camera network. First, we formulate a domain perceptive re-identification method based on geodesic flow kernel that can effectively find the best source camera (already installed) to adapt with a newly introduced target camera, without requiring a very expensive training phase. Second, we introduce a transitive inference algorithm for re-identification that can exploit the information from best source camera to improve the accuracy across other camera pairs in a network of multiple cameras. Extensive experiments on four benchmark datasets demonstrate that the proposed approach significantly outperforms the state-of-the-art unsupervised learning based alternatives whilst being extremely efficient to compute.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Panda_Unsupervised_Adaptive_Re-Identification_CVPR_2017_paper.pdf",
        "aff": "Department of ECE, UC Riverside; Pattern Analysis and Computer Vision (PA VIS), Istituto Italiano di Tecnologia, Italy; Pattern Analysis and Computer Vision (PA VIS), Istituto Italiano di Tecnologia, Italy; Department of ECE, UC Riverside",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Panda_Unsupervised_Adaptive_Re-Identification_2017_CVPR_supplemental.pdf",
        "arxiv": "1706.03112v1",
        "pdf_size": 811094,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14569163096691867540&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ucr.edu;iit.it;iit.it;ucr.edu",
        "email": "ucr.edu;iit.it;iit.it;ucr.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Panda_Unsupervised_Adaptive_Re-Identification_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "University of California, Riverside;Istituto Italiano di Tecnologia",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Pattern Analysis and Computer Vision (PA VIS)",
        "aff_unique_url": "https://www.ucr.edu;https://www.iit.it",
        "aff_unique_abbr": "UCR;IIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Riverside;",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;Italy"
    },
    {
        "title": "Unsupervised Learning of Depth and Ego-Motion From Video",
        "session": "Machine Learning for 3D Vision",
        "status": "Oral",
        "track": "main",
        "pid": "687",
        "author_site": "Tinghui Zhou, Matthew Brown, Noah Snavely, David G. Lowe",
        "author": "Tinghui Zhou; Matthew Brown; Noah Snavely; David G. Lowe",
        "abstract": "We present an unsupervised learning framework for the task of dense 3D geometry and camera motion estimation from unstructured video sequences. In common with recent work, we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to these works, our method is completely unsupervised, requiring only a sequence of images as input. We achieve this with a network that estimates the 6-DoF camera pose parameters of the input set, along with dense depth for a reference view using single-view inference. Our loss is constructed by projecting the nearby posed views into the reference view via the depth map. Results using the KITTI dataset demonstrate the effectiveness of our approach, which performs on par with another deep learning approach that assumes ground-truth pose information at training time.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.pdf",
        "aff": "UC Berkeley; Google; Google; Google",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.07813v2",
        "pdf_size": 1677491,
        "gs_citation": 3364,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11440379944978300844&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 11,
        "aff_domain": "berkeley.edu;google.com;google.com;google.com",
        "email": "berkeley.edu;google.com;google.com;google.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of California, Berkeley;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.berkeley.edu;https://www.google.com",
        "aff_unique_abbr": "UC Berkeley;Google",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Berkeley;Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Unsupervised Learning of Long-Term Motion Dynamics for Videos",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "793",
        "author_site": "Zelun Luo, Boya Peng, De-An Huang, Alexandre Alahi, Li Fei-Fei",
        "author": "Zelun Luo; Boya Peng; De-An Huang; Alexandre Alahi; Li Fei-Fei",
        "abstract": "We present an unsupervised representation learning approach that compactly encodes the motion dependencies in videos. Given a pair of images from a video clip, our framework learns to predict the long-term 3D motions. To reduce the complexity of the learning framework, we propose to describe the motion as a sequence of atomic 3D flows computed with RGB-D modality. We  use a Recurrent Neural Network based Encoder-Decoder framework to predict these sequences of flows. We argue that in order for the decoder to reconstruct these sequences, the encoder must learn a robust video representation that captures long-term motion dependencies and spatial-temporal relations. We demonstrate the effectiveness of our learned temporal representations on  activity classification across multiple modalities and datasets such as NTU RGB+D and  MSR Daily Activity 3D. Our framework is generic to any input modality, i.e., RGB, depth, and RGB-D videos.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Luo_Unsupervised_Learning_of_CVPR_2017_paper.pdf",
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.01821v3",
        "pdf_size": 2039717,
        "gs_citation": 254,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9847765407432878222&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Luo_Unsupervised_Learning_of_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Unsupervised Monocular Depth Estimation With Left-Right Consistency",
        "session": "Machine Learning for 3D Vision",
        "status": "Oral",
        "track": "main",
        "pid": "104",
        "author_site": "Cl\u00c3\u00a9ment Godard, Oisin Mac Aodha, Gabriel J. Brostow",
        "author": "Clement Godard; Oisin Mac Aodha; Gabriel J. Brostow",
        "abstract": "Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convo- lutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Ex- ploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone re- sults in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency be- tween the disparities produced relative to both the left and right images, leading to improved performance and robustness com- pared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Godard_Unsupervised_Monocular_Depth_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1609.03677",
        "pdf_size": 4284562,
        "gs_citation": 3871,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17687680169908617872&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Godard_Unsupervised_Monocular_Depth_CVPR_2017_paper.html"
    },
    {
        "title": "Unsupervised Part Learning for Visual Recognition",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2780",
        "author_site": "Ronan Sicre, Yannis Avrithis, Ewa Kijak, Fr\u00c3\u00a9d\u00c3\u00a9ric Jurie",
        "author": "Ronan Sicre; Yannis Avrithis; Ewa Kijak; Frederic Jurie",
        "abstract": "Part-based image classification aims at representing categories by small sets of learned discriminative parts, upon which an image representation is built. Considered as a promising avenue a decade ago, this direction has been neglected since the advent of deep neural networks. In this context, this paper brings two contributions:  first, this work proceeds one step further compared to recent part-based models (PBM), focusing on how to learn parts without using any labeled data. Instead of learning a set of parts per class, as generally performed in the PBM  literature, the proposed approach both constructs a partition of a given set of images into visually similar groups, and subsequently learns a set of discriminative parts per group in a fully unsupervised fashion.  This strategy opens the door to the use of PBM in new applications where labeled data are typically not available, such as instance-based image retrieval.  Second, this paper shows that despite the recent success of end-to-end models, explicit part learning can still boost classification performance. We experimentally show that our learned parts can help building efficient image representations, which outperform state-of-the art Deep Convolutional Neural Networks (DCNN) on both classification and retrieval tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sicre_Unsupervised_Part_Learning_CVPR_2017_paper.pdf",
        "aff": "INRIA / IRISA; INRIA / IRISA; INRIA / IRISA; Normandie Univ, UNICAEN, ENSICAEN, CNRS - UMR GREYC",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03755v1",
        "pdf_size": 525748,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14404666341134082725&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "inria.fr;inria.fr;irisa.fr;unicaen.fr",
        "email": "inria.fr;inria.fr;irisa.fr;unicaen.fr",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sicre_Unsupervised_Part_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "INRIA;Normandie University",
        "aff_unique_dep": ";ENSICAEN",
        "aff_unique_url": "https://www.inria.fr;https://www.unicaen.fr",
        "aff_unique_abbr": "INRIA;UNICAEN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Unsupervised Pixel-Level Domain Adaptation With Generative Adversarial Networks",
        "session": "Machine Learning 1",
        "status": "Oral",
        "track": "main",
        "pid": "1385",
        "author_site": "Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan",
        "author": "Konstantinos Bousmalis; Nathan Silberman; David Dohan; Dumitru Erhan; Dilip Krishnan",
        "abstract": "Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that have tried to either map representations between the two domains, or learn to extract features that are domain-invariant. In this work, we approach the problem in a new light by learning  in an unsupervised manner a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Bousmalis_Unsupervised_Pixel-Level_Domain_CVPR_2017_paper.pdf",
        "aff": "Google Brain, London, UK; Google Research, New York, NY; Google Brain, Mountain View, CA; Google Brain, San Francisco, CA; Google Research, Cambridge, MA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Bousmalis_Unsupervised_Pixel-Level_Domain_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.05424v2",
        "pdf_size": 697039,
        "gs_citation": 2021,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11774400096015894119&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Bousmalis_Unsupervised_Pixel-Level_Domain_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Brain",
        "aff_unique_url": "https://brain.google.com",
        "aff_unique_abbr": "Google Brain",
        "aff_campus_unique_index": "0;1;2;3;4",
        "aff_campus_unique": "London;New York;Mountain View;San Francisco;Cambridge",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Unsupervised Semantic Scene Labeling for Streaming Data",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1923",
        "author_site": "Maggie Wigness, John G. Rogers III",
        "author": "Maggie Wigness; John G. Rogers III",
        "abstract": "We introduce an unsupervised semantic scene labeling approach that continuously learns and adapts semantic models discovered within a data stream. While closely related to unsupervised video segmentation, our algorithm is not designed to be an early video processing strategy that produces coherent over-segmentations, but instead, to directly learn higher-level semantic concepts. This is achieved with an ensemble-based approach, where each learner clusters data from a local window in the data stream. Overlapping local windows are processed and encoded in a graph structure to create a label mapping across windows and reconcile the labelings to reduce unsupervised learning noise. Additionally, we iteratively learn a merging threshold criteria from observed data similarities to automatically determine the number of learned labels without human provided parameters. Experiments show that our approach semantically labels video streams with a high degree of accuracy, and achieves a better balance of under and over-segmentation entropy than existing video segmentation algorithms given similar numbers of label outputs.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wigness_Unsupervised_Semantic_Scene_CVPR_2017_paper.pdf",
        "aff": "U.S. Army Research Laboratory; U.S. Army Research Laboratory",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 922026,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10571021859529920425&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "mail.mil;mail.mil",
        "email": "mail.mil;mail.mil",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wigness_Unsupervised_Semantic_Scene_CVPR_2017_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "U.S. Army Research Laboratory",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.arl.army.mil",
        "aff_unique_abbr": "ARL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Unsupervised Vanishing Point Detection and Camera Calibration From a Single Manhattan Image With Radial Distortion",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1732",
        "author_site": "Michel Antunes, Jo\u00c3\u00a3o P. Barreto, Djamila Aouada, Bj\u00c3\u00b6rn Ottersten",
        "author": "Michel Antunes; Joao P. Barreto; Djamila Aouada; Bjorn Ottersten",
        "abstract": "The article concerns the automatic calibration of a camera with radial distortion from a single image. It is known that, under the mild assumption of square pixels and zero skew, lines in the scene project into circles in the image, and three lines suffice to calibrate the camera up to an ambiguity between focal length and radial distortion. The calibration results highly depend on accurate circle estimation, which is hard to accomplish because lines tend to project into short circular arcs. To overcome this problem, we show that, given a short circular arc edge, it is possible to robustly determine a line that goes through the center of the corresponding circle. These lines, henceforth called Lines of Circle Centres (LCCs), are used in a new method that detects sets of parallel lines and estimates the calibration parameters, including the center and amount of distortion, focal length, and camera orientation with respect to the Manhattan frame. Extensive experiments in both semi-synthetic and real images show that our algorithm outperforms state-of-the-art approaches in unsupervised calibration from a single image, while providing more information.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Antunes_Unsupervised_Vanishing_Point_CVPR_2017_paper.pdf",
        "aff": "Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg; Institute of Systems and Robotics (ISR), University of Coimbra; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 10884629,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6302179644858209328&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "uni.lu;deec.uc.pt;uni.lu;uni.lu",
        "email": "uni.lu;deec.uc.pt;uni.lu;uni.lu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Antunes_Unsupervised_Vanishing_Point_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Luxembourg;University of Coimbra",
        "aff_unique_dep": "Interdisciplinary Centre for Security, Reliability and Trust (SnT);Institute of Systems and Robotics",
        "aff_unique_url": "https://wwwen.uniluxembourg.lu;https://www.uc.pt",
        "aff_unique_abbr": "Uni Lu;UC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Luxembourg;Portugal"
    },
    {
        "title": "Unsupervised Video Summarization With Adversarial LSTM Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "71",
        "author_site": "Behrooz Mahasseni, Michael Lam, Sinisa Todorovic",
        "author": "Behrooz Mahasseni; Michael Lam; Sinisa Todorovic",
        "abstract": "This paper addresses the problem of unsupervised video summarization, formulated as selecting a sparse subset of video frames that optimally represent the input video. Our key idea is to learn a deep summarizer network to minimize distance between training videos and a distribution of their summarizations, in an unsupervised way. Such a summarizer can then be applied on a new video for estimating its optimal summarization. For learning, we specify a novel generative adversarial framework, consisting of the summarizer and discriminator. The summarizer is the autoencoder long short-term memory network (LSTM) aimed at, first, selecting video frames, and then decoding the obtained summarization for reconstructing the input video. The discriminator is another LSTM aimed at distinguishing between the original video and its reconstruction from the summarizer. The summarizer LSTM is cast as an adversary of the discriminator, i.e., trained so as to maximally confuse the discriminator. This learning is also regularized for sparsity. Evaluation on four benchmark datasets, consisting of videos showing diverse events in first- and third-person views, demonstrates our competitive performance in comparison to fully supervised state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper.pdf",
        "aff": "Oregon State University; Oregon State University; Oregon State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1026965,
        "gs_citation": 704,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10348829294026652890&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;oregonstate.edu;oregonstate.edu",
        "email": "gmail.com;oregonstate.edu;oregonstate.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Mahasseni_Unsupervised_Video_Summarization_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "790",
        "author_site": "De-An Huang, Joseph J. Lim, Li Fei-Fei, Juan Carlos Niebles",
        "author": "De-An Huang; Joseph J. Lim; Li Fei-Fei; Juan Carlos Niebles",
        "abstract": "We propose an unsupervised method for reference resolution in instructional videos, where the goal is to temporally link an entity (e.g., \"dressing\") to the action (e.g., \"mix yogurt\") that produced it. The key challenge is the inevitable visual-linguistic ambiguities arising from the changes in both visual appearance and referring expression of an entity in the video. This challenge is amplified by the fact that we aim to resolve references with no supervision. We address these challenges by learning a joint visual-linguistic model, where linguistic cues can help resolve visual ambiguities and vice versa. We verify our approach by learning our model unsupervisedly using more than two thousand unstructured cooking videos from YouTube, and show that our visual-linguistic model can substantially improve upon state-of-the-art linguistic only model on reference resolution in instructional videos.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Unsupervised_Visual-Linguistic_Reference_CVPR_2017_paper.pdf",
        "aff": "Stanford University; University of Southern California; Stanford University; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.02521",
        "pdf_size": 1694266,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12660100536691274703&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": "cs.stanford.edu;usc.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;usc.edu;cs.stanford.edu;cs.stanford.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_Unsupervised_Visual-Linguistic_Reference_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Stanford University;University of Southern California",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.usc.edu",
        "aff_unique_abbr": "Stanford;USC",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Stanford;Los Angeles",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "UntrimmedNets for Weakly Supervised Action Recognition and Detection",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1750",
        "author_site": "Limin Wang, Yuanjun Xiong, Dahua Lin, Luc Van Gool",
        "author": "Limin Wang; Yuanjun Xiong; Dahua Lin; Luc Van Gool",
        "abstract": "Current action recognition methods heavily rely on trimmed videos for model training. However, it is expensive and time-consuming to acquire a large-scale trimmed video dataset. This paper presents a new weakly supervised architecture, called UntrimmedNet, which is able to directly learn action recognition models from untrimmed videos without the requirement of temporal annotations of action instances. Our UntrimmedNet couples two important components, the classification module and the selection module, to learn the action models and reason about the temporal duration of action instances, respectively. These two components are implemented with feed-forward networks, and UntrimmedNet is therefore an end-to-end trainable architecture. We exploit the learned models for action recognition (WSR) and detection (WSD) on the untrimmed video datasets of THUMOS14 and ActivityNet. Although our UntrimmedNet only employs weak supervision, our method achieves performance superior or comparable to that of those strongly supervised approaches on these two datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_UntrimmedNets_for_Weakly_CVPR_2017_paper.pdf",
        "aff": "Computer Vision Laboratory, ETH Zurich, Switzerland; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Computer Vision Laboratory, ETH Zurich, Switzerland",
        "project": "",
        "github": "https://github.com/wanglimin/UntrimmedNet",
        "supp": "",
        "arxiv": "1703.03329v2",
        "pdf_size": 1303547,
        "gs_citation": 651,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11313421283676135984&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_UntrimmedNets_for_Weakly_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "ETH Zurich;Chinese University of Hong Kong",
        "aff_unique_dep": "Computer Vision Laboratory;Department of Information Engineering",
        "aff_unique_url": "https://www.ethz.ch;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "ETHZ;CUHK",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Switzerland;China"
    },
    {
        "title": "Using Locally Corresponding CAD Models for Dense 3D Reconstructions From a Single Image",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2007",
        "author_site": "Chen Kong, Chen-Hsuan Lin, Simon Lucey",
        "author": "Chen Kong; Chen-Hsuan Lin; Simon Lucey",
        "abstract": "We investigate the problem of estimating the dense 3D shape of an object, given a set of 2D landmarks and silhouette in a single image. An obvious prior to employ in such a problem is a dictionary of dense CAD models. Employing a sufficiently large enough dictionary of CAD models, however, is in general computationally infeasible. A common strategy in dictionary learning to encourage generalization is to allow for linear combinations of dictionary elements. This too, however, is problematic as most CAD models cannot be readily placed in global dense correspondence. In this paper, we propose a two-step strategy. First, we employ orthogonal matching pursuit to rapidly choose the \"closest\" single CAD model in our dictionary to the projected image. Second, we employ a novel graph embedding based on local dense correspondence to allow for sparse linear combinations of CAD models. We validate our framework experimentally in both synthetic and real world scenario and demonstrate the superiority of our approach to both 3D mesh reconstruction and volumetric representation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Kong_Using_Locally_Corresponding_CVPR_2017_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Kong_Using_Locally_Corresponding_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3924736,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8579961634180033490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Kong_Using_Locally_Corresponding_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Using Ranking-CNN for Age Estimation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2148",
        "author_site": "Shixing Chen, Caojin Zhang, Ming Dong, Jialiang Le, Mike Rao",
        "author": "Shixing Chen; Caojin Zhang; Ming Dong; Jialiang Le; Mike Rao",
        "abstract": "Human age is considered an important biometric trait for human identification or search. Recent research shows that the aging features deeply learned from large-scale data lead to significant performance improvement on facial image-based age estimation. However, age-related ordinal information is totally ignored in these approaches. In this paper, we propose a novel Convolutional Neural Network (CNN)-based framework, ranking-CNN, for age estimation. Ranking-CNN contains a series of basic CNNs, each of which is trained with ordinal age labels. Then, their binary outputs are aggregated for the final age prediction. We theoretically obtain a much tighter error bound for ranking-based age estimation. Moreover, we rigorously prove that ranking-CNN is more likely to get smaller estimation errors when compared with multi-class classification approaches. Through extensive experiments, we show that statistically, ranking-CNN significantly outperforms other state-of-the-art age estimation models on benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Using_Ranking-CNN_for_CVPR_2017_paper.pdf",
        "aff": "Department of Computer Science, Wayne State University; Department of Mathematics, Wayne State University; Department of Computer Science, Wayne State University; Research & Innovation Center, Ford Motor Company; Research & Innovation Center, Ford Motor Company",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1345953,
        "gs_citation": 327,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13121549543951466678&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "wayne.edu;wayne.edu;wayne.edu;ford.com;ford.com",
        "email": "wayne.edu;wayne.edu;wayne.edu;ford.com;ford.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_Using_Ranking-CNN_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Wayne State University;Ford Motor Company",
        "aff_unique_dep": "Department of Computer Science;Research & Innovation Center",
        "aff_unique_url": "https://wayne.edu;https://www.ford.com",
        "aff_unique_abbr": "WSU;Ford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Variational Autoencoded Regression: High Dimensional Regression of Visual Data on Complex Manifold",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1355",
        "author_site": "YoungJoon Yoo, Sangdoo Yun, Hyung Jin Chang, Yiannis Demiris, Jin Young Choi",
        "author": "YoungJoon Yoo; Sangdoo Yun; Hyung Jin Chang; Yiannis Demiris; Jin Young Choi",
        "abstract": "This paper proposes a new high dimensional regression method by merging Gaussian process regression into a variational autoencoder framework. In contrast to other regression methods, the proposed method focuses on the case where output responses are on a complex high dimensional manifold, such as images. Our contributions are summarized as follows: (i) A new regression method estimating high dimensional image responses, which is not handled by existing regression algorithms, is proposed. (ii) The proposed regression method introduces a strategy to learn the latent space as well as the encoder and decoder so that the result of the regressed response in the latent space coincide with the corresponding response in the data space. (iii) The proposed regression is embedded into a generative model, and the whole procedure is developed by the variational autoencoder framework. We demonstrate the robustness and effectiveness of our method through a number of experiments on various visual data regression problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yoo_Variational_Autoencoded_Regression_CVPR_2017_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yoo_Variational_Autoencoded_Regression_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3830724,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17418698512692759003&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yoo_Variational_Autoencoded_Regression_CVPR_2017_paper.html"
    },
    {
        "title": "Variational Bayesian Multiple Instance Learning With Gaussian Processes",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2966",
        "author_site": "Manuel Hau\u00c3\u009fmann, Fred A. Hamprecht, Melih Kandemir",
        "author": "Manuel Haussmann; Fred A. Hamprecht; Melih Kandemir",
        "abstract": "Gaussian Processes (GPs) are effective Bayesian predictors. We here show for the first time that instance labels of a GP classifier can be inferred in the multiple instance learning (MIL) setting using variational Bayes. We achieve this via a new construction of the bag likelihood that assumes a large value if the instance predictions obey the MIL constraints and a small value otherwise. This construction lets us derive the update rules for the variational parameters analytically, assuring both scalable learning and fast convergence. We observe this model to improve the state of the art in instance label prediction from bag-level supervision in the 20 Newsgroups benchmark, as well as in Barrett's cancer tumor localization from histopathology tissue microarray images. Furthermore, we introduce a novel pipeline for weakly supervised object detection naturally complemented with our model, which improves the state of the art on the PASCAL VOC 2007 and 2012 data sets. Last but not least, the performance of our model can be further boosted up using mixed supervision: a combination of weak (bag) and strong (instance) labels.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Haussmann_Variational_Bayesian_Multiple_CVPR_2017_paper.pdf",
        "aff": "HCI/IWR, Heidelberg University; HCI/IWR, Heidelberg University; HCI/IWR, Heidelberg University+\u00a8Ozye \u02d8gin University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Haussmann_Variational_Bayesian_Multiple_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 563773,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3597290600565625726&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de;ozyegin.edu.tr",
        "email": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de;ozyegin.edu.tr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Haussmann_Variational_Bayesian_Multiple_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Heidelberg University;Ozyegin University",
        "aff_unique_dep": "Human-Computer Interaction / Institute for Workflow Research;",
        "aff_unique_url": "https://www.uni-heidelberg.de;https://www.ozyegin.edu.tr",
        "aff_unique_abbr": "Uni Heidelberg;Ozyegin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Heidelberg;",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "Germany;T\u00fcrkiye"
    },
    {
        "title": "ViP-CNN: Visual Phrase Guided Convolutional Neural Network",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "490",
        "author_site": "Yikang Li, Wanli Ouyang, Xiaogang Wang, Xiao'ou Tang",
        "author": "Yikang Li; Wanli Ouyang; Xiaogang Wang; Xiao'ou Tang",
        "abstract": "As the intermediate level task connecting image captioning and object detection, visual relationship detection started to catch researchers' attention because of its descriptive power and clear structure. It detects the objects and captures their pair-wise interactions with a subject-predicate-object triplet, e.g. person-ride-horse. In this paper, each visual relationship is considered as a phrase with three components. We formulate the visual relationship detection as three inter-connected recognition problems and propose a Visual Phrase guided Convolutional Neural Network (ViP-CNN) to address them simultaneously. In ViP-CNN, we present a Phrase-guided Message Passing Structure (PMPS) to establish the connection among relationship components and help the model consider the three problems jointly. Corresponding non-maximum suppression method and model training strategy are also proposed. Experimental results show that our ViP-CNN outperforms the state-of-art method both in speed and accuracy. We further pretrain ViP-CNN on our cleansed Visual Genome Relationship dataset, which is found to perform better than the pretraining on the ImageNet for this task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_ViP-CNN_Visual_Phrase_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 314,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1699655188995825967&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_ViP-CNN_Visual_Phrase_CVPR_2017_paper.html"
    },
    {
        "title": "VidLoc: A Deep Spatio-Temporal Model for 6-DoF Video-Clip Relocalization",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "3151",
        "author_site": "Ronald Clark, Sen Wang, Andrew Markham, Niki Trigoni, Hongkai Wen",
        "author": "Ronald Clark; Sen Wang; Andrew Markham; Niki Trigoni; Hongkai Wen",
        "abstract": "Machine learning techniques, namely convolutional neural networks (CNN) and regression forests, have recently shown great promise in performing 6-DoF localization of monocular images. However, in most cases image-sequences, rather only single images, are readily available. To this extent, none of the proposed learning-based approaches exploit the valuable constraint of temporal smoothness, often leading to situations where the per-frame error is larger than the camera motion. In this paper we propose a recurrent model for performing 6-DoF localization of video-clips. We find that, even by considering only short sequences (20 frames), the pose estimates are smoothed and the localization error can be drastically reduced. Finally, we consider means of obtaining probabilistic pose estimates from our model. We evaluate our method on openly-available real-world autonomous driving and indoor localization datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Clark_VidLoc_A_Deep_CVPR_2017_paper.pdf",
        "aff": "University of Oxford; University of Oxford; University of Oxford; University of Oxford; University of Warwick",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.06521",
        "pdf_size": 30510352,
        "gs_citation": 325,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4829407158436653485&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "cs.ox.ac.uk;cs.ox.ac.uk;cs.ox.ac.uk;cs.ox.ac.uk;warwick.ac.uk",
        "email": "cs.ox.ac.uk;cs.ox.ac.uk;cs.ox.ac.uk;cs.ox.ac.uk;warwick.ac.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Clark_VidLoc_A_Deep_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "University of Oxford;University of Warwick",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.warwick.ac.uk",
        "aff_unique_abbr": "Oxford;Warwick",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Video Acceleration Magnification",
        "session": "Image Motion & Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "162",
        "author_site": "Yichao Zhang, Silvia L. Pintea, Jan C. van Gemert",
        "author": "Yichao Zhang; Silvia L. Pintea; Jan C. van Gemert",
        "abstract": "The ability to amplify or reduce subtle image changes over time is useful in contexts such as video editing, medical video analysis, product quality control and sports. In these contexts there is often large motion present which severely distorts current video amplification methods that magnify change linearly. In this work we propose a method to cope with large motions while still magnifying small changes. We make the following two observations: i) large motions are linear on the temporal scale of the small changes; ii) small changes deviate from this linearity. We ignore linear motion and propose to magnify acceleration. Our method is pure Eulerian and does not require any optical flow, temporal alignment or region annotations. We link temporal second-order derivative filtering to spatial acceleration magnification. We apply our method to moving objects where we show motion magnification and color magnification. We provide quantitative as well as qualitative evidence for our method while comparing to the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Video_Acceleration_Magnification_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Zhang_Video_Acceleration_Magnification_2017_CVPR_supplemental.zip",
        "arxiv": "1704.04186",
        "pdf_size": 1411819,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12349163894813002048&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Video_Acceleration_Magnification_CVPR_2017_paper.html"
    },
    {
        "title": "Video Captioning With Transferred Semantic Attributes",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2927",
        "author_site": "Yingwei Pan, Ting Yao, Houqiang Li, Tao Mei",
        "author": "Yingwei Pan; Ting Yao; Houqiang Li; Tao Mei",
        "abstract": "Automatically generating natural language descriptions of videos plays a fundamental challenge for computer vision community. Most recent progress in this problem has been achieved through employing 2-D and/or 3-D Convolutional Neural Networks (CNNs) to encode video content and Recurrent Neural Networks (RNNs) to decode a sentence. In this paper, we present Long Short-Term Memory with Transferred Semantic Attributes (LSTM-TSA)---a novel deep architecture that incorporates the transferred semantic attributes learnt from images and videos into the CNN plus RNN framework, by training them in an end-to-end manner. The design of LSTM-TSA is highly inspired by the facts that 1) semantic attributes play a significant contribution to captioning, and 2) images and videos carry complementary semantics and thus can reinforce each other for captioning. To boost video captioning, we propose a novel transfer unit to model the mutually correlated attributes learnt from images and videos. Extensive experiments are conducted on three public datasets, i.e., MSVD, M-VAD and MPII-MD. Our proposed LSTM-TSA achieves to-date the best published performance in sentence generation on MSVD: 52.8% and 74.0% in terms of BLEU@4 and CIDEr-D. Superior results are also reported on M-VAD and MPII-MD when compared to state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Pan_Video_Captioning_With_CVPR_2017_paper.pdf",
        "aff": "University of Science and Technology of China, Hefei, China; Microsoft Research, Beijing, China; University of Science and Technology of China, Hefei, China; Microsoft Research, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.07675v1",
        "pdf_size": 1017298,
        "gs_citation": 427,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16814075306562750459&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;microsoft.com;ustc.edu.cn;microsoft.com",
        "email": "gmail.com;microsoft.com;ustc.edu.cn;microsoft.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Pan_Video_Captioning_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Science and Technology of China;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.microsoft.com/en-us/research/group/microsoft-research-asia",
        "aff_unique_abbr": "USTC;MSR",
        "aff_campus_unique_index": "0;1;0;1",
        "aff_campus_unique": "Hefei;Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Video Desnowing and Deraining Based on Matrix Decomposition",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1710",
        "author_site": "Weihong Ren, Jiandong Tian, Zhi Han, Antoni Chan, Yandong Tang",
        "author": "Weihong Ren; Jiandong Tian; Zhi Han; Antoni Chan; Yandong Tang",
        "abstract": "The existing snow/rain removal methods often fail for heavy snow/rain and dynamic scene. One reason for the failure is due to the assumption that all the snowflakes/rain streaks are sparse in snow/rain scenes. The other is that the existing methods often can not differentiate moving objects and snowflakes/rain streaks.  In this paper, we propose a model based on matrix decomposition for video desnowing and deraining to solve the problems mentioned above.  We divide snowflakes/rain streaks into two categories: sparse ones and dense ones. With background fluctuations and optical flow information, the detection of moving objects and sparse snowflakes/rain streaks is formulated as a multi-label Markov Random Fields (MRFs). As for dense snowflakes/rain streaks, they are considered to obey Gaussian distribution. The snowflakes/rain streaks, including sparse ones and dense ones, in scene backgrounds are removed by low-rank representation of the backgrounds. Meanwhile, a group sparsity term in our model is designed to filter snow/rain pixels within the moving objects. Experimental results show that our proposed model performs better than the state-of-the-art methods for snow and rain removal.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ren_Video_Desnowing_and_CVPR_2017_paper.pdf",
        "aff": "State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences + University of Chinese Academy of Sciences; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences; City University of Hong Kong; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1272993,
        "gs_citation": 199,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9553682666241685409&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "sia.cn;sia.cn;sia.cn;cityu.edu.hk;sia.cn",
        "email": "sia.cn;sia.cn;sia.cn;cityu.edu.hk;sia.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ren_Video_Desnowing_and_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;2;0",
        "aff_unique_norm": "Shenyang Institute of Automation;University of Chinese Academy of Sciences;City University of Hong Kong",
        "aff_unique_dep": "State Key Laboratory of Robotics;;",
        "aff_unique_url": "http://www.sia.cas.cn;http://www.ucas.ac.cn;https://www.cityu.edu.hk",
        "aff_unique_abbr": ";UCAS;CityU",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Video Frame Interpolation via Adaptive Convolution",
        "session": "Computational Photography",
        "status": "Spotlight",
        "track": "main",
        "pid": "205",
        "author_site": "Simon Niklaus, Long Mai, Feng Liu",
        "author": "Simon Niklaus; Long Mai; Feng Liu",
        "abstract": "Video frame interpolation typically involves two steps: motion estimation and pixel synthesis. Such a two-step approach heavily depends on the quality of motion estimation. This paper presents a robust video frame interpolation method that combines these two steps into a single process. Specifically, our method considers pixel synthesis for the interpolated frame as local convolution over two input frames. The convolution kernel captures both the local motion between the input frames and the coefficients for pixel synthesis. Our method employs a deep fully convolutional neural network to estimate a spatially-adaptive convolution kernel for each pixel. This deep neural network can be directly trained end to end using widely available video data without any difficult-to-obtain ground-truth data like optical flow. Our experiments show that the formulation of video interpolation as a single convolution process allows our method to gracefully handle challenges like occlusion, blur, and abrupt brightness change and enables high-quality video frame interpolation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Niklaus_Video_Frame_Interpolation_CVPR_2017_paper.pdf",
        "aff": "Portland State University; Portland State University; Portland State University",
        "project": "http://graphics.cs.pdx.edu/project/adaconv",
        "github": "",
        "supp": "",
        "arxiv": "1703.07514v1",
        "pdf_size": 1956516,
        "gs_citation": 614,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9638940283730060772&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "pdx.edu;cs.pdx.edu;cs.pdx.edu",
        "email": "pdx.edu;cs.pdx.edu;cs.pdx.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Niklaus_Video_Frame_Interpolation_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Portland State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.pdx.edu",
        "aff_unique_abbr": "PSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Video Propagation Networks",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "146",
        "author_site": "Varun Jampani, Raghudeep Gadde, Peter V. Gehler",
        "author": "Varun Jampani; Raghudeep Gadde; Peter V. Gehler",
        "abstract": "We propose a technique that propagates information forward through video data. The method is conceptually simple and can be applied to tasks that require the propagation of structured information, such as semantic labels, based on video content. We propose a \"Video Propagation Network\" that processes video frames in an adaptive manner. The model is applied online: it propagates information forward without the need to access future frames. In particular we combine two components, a temporal bilateral network for dense and video adaptive filtering, followed by a spatial network to refine features and increased flexibility. We present experiments on video object segmentation and semantic video segmentation and show increased performance comparing to the best previous task-specific methods, while having favorable runtime. Additionally we demonstrate our approach on an example regression task of color propagation in a grayscale video.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jampani_Video_Propagation_Networks_CVPR_2017_paper.pdf",
        "aff": "Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany+Bernstein Center for Computational Neuroscience, T\u00fcbingen, Germany; Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany+Bernstein Center for Computational Neuroscience, T\u00fcbingen, Germany; Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany+Bernstein Center for Computational Neuroscience, T\u00fcbingen, Germany",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Jampani_Video_Propagation_Networks_2017_CVPR_supplemental.pdf",
        "arxiv": "1612.05478",
        "pdf_size": 1745337,
        "gs_citation": 298,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8977685089584741274&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jampani_Video_Propagation_Networks_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Bernstein Center for Computational Neuroscience",
        "aff_unique_dep": ";Computational Neuroscience",
        "aff_unique_url": "https://www.mpi-is.mpg.de;",
        "aff_unique_abbr": "MPI-IS;",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "T\u00fcbingen",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Video Segmentation via Multiple Granularity Analysis",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1105",
        "author_site": "Rui Yang, Bingbing Ni, Chao Ma, Yi Xu, Xiaokang Yang",
        "author": "Rui Yang; Bingbing Ni; Chao Ma; Yi Xu; Xiaokang Yang",
        "abstract": "We introduce a Multiple Granularity Analysis framework for video segmentation in a coarse-to-fine manner. We cast video segmentation as a spatio-temporal superpixel labeling problem. Benefited from the bounding volume provided by off-the-shelf object trackers, we estimate the foreground/ background super-pixel labeling using the spatiotemporal multiple instance learning algorithm to obtain coarse foreground/background separation within the volume. We further refine the segmentation mask in the pixel level using the graph-cut model. Extensive experiments on benchmark video datasets demonstrate the superior performance of the proposed video segmentation algorithm.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yang_Video_Segmentation_via_CVPR_2017_paper.pdf",
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; The University of Adelaide; Shanghai Jiao Tong University; Shanghai Jiao Tong University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yang_Video_Segmentation_via_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1467932,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2159916222977647067&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;adelaide.edu.au;sjtu.edu.cn;sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;adelaide.edu.au;sjtu.edu.cn;sjtu.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yang_Video_Segmentation_via_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;University of Adelaide",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.adelaide.edu.au",
        "aff_unique_abbr": "SJTU;Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Video2Shop: Exact Matching Clothes in Videos to Online Shopping Images",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1633",
        "author_site": "Zhi-Qi Cheng, Xiao Wu, Yang Liu, Xian-Sheng Hua",
        "author": "Zhi-Qi Cheng; Xiao Wu; Yang Liu; Xian-Sheng Hua",
        "abstract": "In recent years, both online retail and video hosting service have been exponentially grown. In this paper, a novel deep neural network, called AsymNet, is proposed to explore a new cross-domain task, Video2Shop, targeting for matching clothes appeared in videos to the exactly same items in online shops. For the image side, well-established methods are used to detect and extract features for clothing patches with arbitrary sizes. For the video side, deep visual features are extracted from detected object regions in each frame, and further fed into a Long Short-Term Memory (LSTM) framework for sequence modeling, which captures the temporal dynamics in videos. To conduct exact matching between videos and online shopping images, LSTM hidden states for videos and image features extracted from static images are jointly modeled, under the similarity network with reconfigurable deep tree structure. Moreover, an approximate training method is proposed to achieve the efficiency when training. Extensive experiments conducted on a large cross-domain dataset have demonstrated the effectiveness and efficiency of the proposed AsymNet, which outperforms the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Cheng_Video2Shop_Exact_Matching_CVPR_2017_paper.pdf",
        "aff": "Southwest Jiaotong University; Southwest Jiaotong University; Alibaba Group; Alibaba Group",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1804.05287v2",
        "pdf_size": 952689,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7247219941969347680&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gmail.com;swjtu.edu.cn;alibaba-inc.com;gmail.com",
        "email": "gmail.com;swjtu.edu.cn;alibaba-inc.com;gmail.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Cheng_Video2Shop_Exact_Matching_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "Southwest Jiao Tong University;Alibaba Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.swjtu.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "SWJTU;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Viraliency: Pooling Local Virality",
        "session": "Applications",
        "status": "Poster",
        "track": "main",
        "pid": "2662",
        "author_site": "Xavier Alameda-Pineda, Andrea Pilzer, Dan Xu, Nicu Sebe, Elisa Ricci",
        "author": "Xavier Alameda-Pineda; Andrea Pilzer; Dan Xu; Nicu Sebe; Elisa Ricci",
        "abstract": "In our overly-connected world, the automatic recognition of virality -- the quality of an image or video to be rapidly and widely spread -- is of crucial importance, and has recently awaken the interest of the computer vision community Concurrently, recent progress in deep learning architectures showed that global (average) pooling strategies allow to extract class activation maps, which highlight the part of the image most likely to contain a certain class. We extend this concept by introducing a pooling layer that learns the size of the average support: the learned top-N average (LENA) pooling. We hypothesize that the latent concepts (feature maps) describing virality may require such a rich pooling strategy and perform an extensive evaluation to assess the validity of this hypothesis. Moreover, we also appraise the use of objectness maps at predicting and localizing the virality of an image. Experiments are shown in two publicly available datasets annotated for virality.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Alameda-Pineda_Viraliency_Pooling_Local_CVPR_2017_paper.pdf",
        "aff": "University of Trento+Perception Team, INRIA; University of Trento; University of Trento; University of Trento; University of Perugia+Fondazione Bruno Kessler",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Alameda-Pineda_Viraliency_Pooling_Local_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.03937",
        "pdf_size": 1340243,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8153699512124546545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "inria.fr;unitn.it;unitn.it;unitn.it;fbk.eu",
        "email": "inria.fr;unitn.it;unitn.it;unitn.it;fbk.eu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Alameda-Pineda_Viraliency_Pooling_Local_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;0;0;2+3",
        "aff_unique_norm": "University of Trento;INRIA;University of Perugia;Fondazione Bruno Kessler",
        "aff_unique_dep": ";Perception Team;;",
        "aff_unique_url": "https://www.unitn.it;https://www.inria.fr;https://www.unipg.it;https://www.fbk.eu",
        "aff_unique_abbr": "UniTN;INRIA;Unipg;FBK",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0;0;0+0",
        "aff_country_unique": "Italy;France"
    },
    {
        "title": "Visual Dialog",
        "session": "Object Recognition & Scene Understanding - Computer Vision & Language",
        "status": "Spotlight",
        "track": "main",
        "pid": "121",
        "author_site": "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00c3\u00a9 M. F. Moura, Devi Parikh, Dhruv Batra",
        "author": "Abhishek Das; Satwik Kottur; Khushi Gupta; Avi Singh; Deshraj Yadav; Jose M. F. Moura; Devi Parikh; Dhruv Batra",
        "abstract": "We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial contains 1 dialog (10 question- answer pairs) on  140k images from the COCO dataset, with a total of  1.4M dialog question-answer pairs. We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders (Late Fusion, Hierarchical Recurrent Encoder and Memory Network) and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Our dataset, code, and trained models will be released publicly at https://visualdialog.org. Putting it all together, we demonstrate the first 'visual chatbot'!",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Das_Visual_Dialog_CVPR_2017_paper.pdf",
        "aff": "Georgia Institute of Technology; Carnegie Mellon University; Carnegie Mellon University+Virginia Tech; UC Berkeley+Virginia Tech; Virginia Tech; Carnegie Mellon University; Georgia Institute of Technology; Georgia Institute of Technology",
        "project": "visualdialog.org",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Das_Visual_Dialog_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.08669v5",
        "pdf_size": 1352651,
        "gs_citation": 1254,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1741770872944079783&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": "gatech.edu;andrew.cmu.edu;andrew.cmu.edu;cs.berkeley.edu;vt.edu;andrew.cmu.edu;gatech.edu;gatech.edu",
        "email": "gatech.edu;andrew.cmu.edu;andrew.cmu.edu;cs.berkeley.edu;vt.edu;andrew.cmu.edu;gatech.edu;gatech.edu",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Das_Visual_Dialog_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1+2;3+2;2;1;0;0",
        "aff_unique_norm": "Georgia Institute of Technology;Carnegie Mellon University;Virginia Tech;University of California, Berkeley",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.gatech.edu;https://www.cmu.edu;https://www.vt.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "Georgia Tech;CMU;VT;UC Berkeley",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0+0;0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Visual Translation Embedding Network for Visual Relation Detection",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2334",
        "author_site": "Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, Tat-Seng Chua",
        "author": "Hanwang Zhang; Zawlin Kyaw; Shih-Fu Chang; Tat-Seng Chua",
        "abstract": "Visual relations, such as \"person ride bike\" and \"bike next to car\", offer a comprehensive scene understanding of an image, and have already shown their great utility in connecting computer vision and natural language. However, due to the challenging combinatorial complexity of modeling subject-predicate-object relation triplets, very little work has been done to localize and predict visual relations. Inspired by the recent advances in relational representation learning of knowledge bases and convolutional object detection networks, we propose a Visual Translation Embedding network (VTransE) for visual relation detection. VTransE places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation, i.e., subject + predicate = object. We propose a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass. To the best of our knowledge, VTransE is the first end-to-end relation detection network. We demonstrate the effectiveness of VTransE over other state-of-the-art methods on two large-scale datasets: Visual Relationship and Visual Genome. Note that even though VTransE is a purely visual model, it is still competitive to the Lu's multi-modal model with language",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Visual_Translation_Embedding_CVPR_2017_paper.pdf",
        "aff": "Columbia University; National University of Singapore; Columbia University; National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1702.08319v1",
        "pdf_size": 1717163,
        "gs_citation": 676,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1588186222785179425&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;gmail.com;ee.columbia.edu;nus.edu.sg",
        "email": "gmail.com;gmail.com;ee.columbia.edu;nus.edu.sg",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Visual_Translation_Embedding_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Columbia University;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.columbia.edu;https://www.nus.edu.sg",
        "aff_unique_abbr": "Columbia;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "title": "Visual-Inertial-Semantic Scene Representation for 3D Object Detection",
        "session": "3D Computer Vision",
        "status": "Poster",
        "track": "main",
        "pid": "326",
        "author_site": "Jingming Dong, Xiaohan Fei, Stefano Soatto",
        "author": "Jingming Dong; Xiaohan Fei; Stefano Soatto",
        "abstract": "We describe a system to detect objects in three-dimensional space using video and inertial sensors (accelerometer and gyrometer), ubiquitous in modern mobile platforms from phones to drones. Inertials afford the ability to impose class-specific scale priors for objects, and provide a global orientation reference. A minimal sufficient representation, the posterior of semantic (identity) and syntactic (pose) attributes of objects in space, can be decomposed into a geometric term, which can be maintained by a localization-and-mapping filter, and a likelihood function, which can be approximated by a discriminatively-trained convolutional neural network The resulting system can process the video stream causally in real time, and provides a representation of objects in the scene that is persistent: Confidence in the presence of objects grows with evidence, and objects previously seen are kept in memory even when temporarily occluded, with their return into view automatically predicted to prime re-detection.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Dong_Visual-Inertial-Semantic_Scene_Representation_CVPR_2017_paper.pdf",
        "aff": "UCLA Vision Lab, University of California, Los Angeles, CA 90095; UCLA Vision Lab, University of California, Los Angeles, CA 90095; UCLA Vision Lab, University of California, Los Angeles, CA 90095",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1050733,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6373498353742110325&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Dong_Visual-Inertial-Semantic_Scene_Representation_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "UCLA Vision Lab",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "200",
        "author_site": "Thibaut Durand, Taylor Mordan, Nicolas Thome, Matthieu Cord",
        "author": "Thibaut Durand; Taylor Mordan; Nicolas Thome; Matthieu Cord",
        "abstract": "This paper introduces WILDCAT, a deep learning method which jointly aims at aligning image regions for gaining spatial invariance and learning strongly localized features. Our model is trained using only global image labels and is devoted to three main visual recognition tasks: image classification, weakly supervised object localization and semantic segmentation. WILDCAT extends state-of-the-art Convolutional Neural Networks at three main levels: the use of Fully Convolutional Networks for maintaining spatial resolution, the explicit design in the network of local features related to different class modalities, and a new way to pool these features to provide a global image prediction required for weakly supervised training. Extensive experiments show that our model significantly outperforms state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Durand_WILDCAT_Weakly_Supervised_CVPR_2017_paper.pdf",
        "aff": "Sorbonne Universit \u00b4es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris; Sorbonne Universit \u00b4es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris + Thales Optronique S.A.S., 2 Avenue Gay Lussac, 78990 \u00b4Elancourt, France; CEDRIC - Conservatoire National des Arts et M \u00b4etiers, 292 rue St Martin, 75003 Paris, France; Sorbonne Universit \u00b4es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Durand_WILDCAT_Weakly_Supervised_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2411495,
        "gs_citation": 419,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5997063811530804850&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": "lip6.fr;lip6.fr;lip6.fr;lip6.fr",
        "email": "lip6.fr;lip6.fr;lip6.fr;lip6.fr",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Durand_WILDCAT_Weakly_Supervised_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+1;2;0",
        "aff_unique_norm": "Sorbonne Universit\u00e9s;Thales Optronique S.A.S.;Conservatoire National des Arts et M\u00e9tiers",
        "aff_unique_dep": ";;CEDRIC",
        "aff_unique_url": "https://www.sorbonne-universite.fr;;https://www.cnam.fr",
        "aff_unique_abbr": "Sorbonne;;CNAM",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "WSISA: Making Survival Prediction From Whole Slide Histopathological Images",
        "session": "Biomedical Image/Video Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "3455",
        "author_site": "Xinliang Zhu, Jiawen Yao, Feiyun Zhu, Junzhou Huang",
        "author": "Xinliang Zhu; Jiawen Yao; Feiyun Zhu; Junzhou Huang",
        "abstract": "Image-based precision medicine techniques can be used to better treat cancer patients. However, the gigapixel resolution of Whole Slide Histopathological Images (WSIs) makes traditional survival models computationally impossible. These models usually adopt manually labeled discriminative patches from region of interests (ROIs) and are unable to directly learn discriminative patches from WSIs. We argue that only a small set of patches cannot fully represent the patients' survival status due to the heterogeneity of tumor. Another challenge is that survival prediction usually comes with insufficient training patient samples. In this paper, we propose an effective Whole Slide Histopathological Images Survival Analysis framework (WSISA) to overcome above challenges. To exploit survival-discriminative patterns from WSIs, we first extract hundreds of patches from each WSI by adaptive sampling and then group these images into different clusters. Then we propose to train an aggregation model to make patient-level predictions based on cluster-level Deep Convolutional Survival (DeepConvSurv) prediction results. Different from existing state-of-the-arts image-based survival models which extract features using some patches from small regions of WSIs, the proposed framework can efficiently exploit and utilize all discriminative patterns in WSIs to predict patients' survival status. To the best of our knowledge, this has not been shown before. We apply our method to the survival predictions of glioma and non-small-cell lung cancer using three datasets. Results demonstrate the proposed framework can significantly improve the prediction performance compared with the existing state-of-the-arts survival methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_WSISA_Making_Survival_CVPR_2017_paper.pdf",
        "aff": "University of Texas at Arlington; University of Texas at Arlington; University of Texas at Arlington; University of Texas at Arlington + Tencent AI Lab",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1402231,
        "gs_citation": 360,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15567038490291328893&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "uta.edu;uta.edu;uta.edu;uta.edu",
        "email": "uta.edu;uta.edu;uta.edu;uta.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Zhu_WSISA_Making_Survival_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "University of Texas at Arlington;Tencent",
        "aff_unique_dep": ";Tencent AI Lab",
        "aff_unique_url": "https://www.uta.edu;https://ai.tencent.com",
        "aff_unique_abbr": "UTA;Tencent AI Lab",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Arlington;",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Weakly Supervised Action Learning With RNN Based Fine-To-Coarse Modeling",
        "session": "Analyzing Humans 1",
        "status": "Oral",
        "track": "main",
        "pid": "244",
        "author_site": "Alexander Richard, Hilde Kuehne, Juergen Gall",
        "author": "Alexander Richard; Hilde Kuehne; Juergen Gall",
        "abstract": "We present an approach for weakly supervised learning of human actions. Given a set of videos and an ordered list of the occurring actions, the goal is to infer start and end frames of the related action classes within the video and to train the respective action classifiers without any need for hand labeled frame boundaries. To address this task, we propose a combination of a discriminative representation of subactions, modeled by a recurrent neural network, and a coarse probabilistic model to allow for a temporal alignment and inference over long sequences. While this system alone already generates good results, we show that the performance can be further improved by approximating the number of subactions to the characteristics of the different action classes. To this end, we adapt the number of subaction classes by iterating realignment and reestimation during training. The proposed system is evaluated on two benchmark datasets, the Breakfast and the Hollywood extended dataset, showing a competitive performance on various weak learning tasks such as temporal action segmentation and action alignment.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Richard_Weakly_Supervised_Action_CVPR_2017_paper.pdf",
        "aff": "University of Bonn, Germany; University of Bonn, Germany; University of Bonn, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1703.08132v3",
        "pdf_size": 1095605,
        "gs_citation": 264,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2430389239473787034&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "iai.uni-bonn.de;iai.uni-bonn.de;iai.uni-bonn.de",
        "email": "iai.uni-bonn.de;iai.uni-bonn.de;iai.uni-bonn.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Richard_Weakly_Supervised_Action_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Bonn",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-bonn.de",
        "aff_unique_abbr": "UBonn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Weakly Supervised Actor-Action Segmentation via Robust Multi-Task Ranking",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "471",
        "author_site": "Yan Yan, Chenliang Xu, Dawen Cai, Jason J. Corso",
        "author": "Yan Yan; Chenliang Xu; Dawen Cai; Jason J. Corso",
        "abstract": "Fine-grained activity understanding in videos has attracted considerable recent attention with a shift from action classification to detailed actor and action understanding that provides compelling results for perceptual needs of cutting-edge autonomous systems. However, current methods for detailed understanding of actor and action have significant limitations: they require large amounts of finely labeled data, and they fail to capture any internal relationship among actors and actions. To address these issues, in this paper, we propose a novel, robust multi-task ranking model for weakly supervised actor-action segmentation where only video-level tags are given for training samples. Our model is able to share useful information among different actors and actions while learning a ranking matrix to select representative supervoxels for actors and actions respectively. Final segmentation results are generated by a conditional random field that considers various ranking scores for different video parts. Extensive experimental results on the Actor-Action Dataset (A2D) demonstrate that the proposed approach outperforms the state-of-the-art weakly supervised methods and performs as well as the top-performing fully supervised method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Yan_Weakly_Supervised_Actor-Action_CVPR_2017_paper.pdf",
        "aff": "Department of Electrical Engineering and Computer Science, University of Michigan; Department of Computer Science, University of Rochester; Department of Cell and Developmental Biology, Biophysics, University of Michigan; Department of Electrical Engineering and Computer Science, University of Michigan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1324342,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5792417522884761901&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "umich.edu;rochester.edu;umich.edu;umich.edu",
        "email": "umich.edu;rochester.edu;umich.edu;umich.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Yan_Weakly_Supervised_Actor-Action_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Michigan;University of Rochester",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.umich.edu;https://www.rochester.edu",
        "aff_unique_abbr": "UM;U of R",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Ann Arbor",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Weakly Supervised Affordance Detection",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1026",
        "author_site": "Johann Sawatzky, Abhilash Srikantha, Juergen Gall",
        "author": "Johann Sawatzky; Abhilash Srikantha; Juergen Gall",
        "abstract": "Localizing functional regions of objects or affordances is an important aspect of scene understanding and relevant for many robotics applications. In this work, we introduce a pixel-wise annotated affordance dataset of 3090 images containing 9916 object instances. Since parts of an object can have multiple affordances, we address this by a convo- lutional neural network for multilabel affordance segmen- tation. We also propose an approach to train the network from very few keypoint annotations. Our approach achieves a higher affordance detection accuracy than other weakly supervised methods that also rely on keypoint annotations or image annotations as weak supervision.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Sawatzky_Weakly_Supervised_Affordance_CVPR_2017_paper.pdf",
        "aff": "University of Bonn; Carl Zeiss AG + University of Bonn; University of Bonn",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 830190,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8478897356613488157&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "iai.uni-bonn.de;zeiss.com;iai.uni-bonn.de",
        "email": "iai.uni-bonn.de;zeiss.com;iai.uni-bonn.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Sawatzky_Weakly_Supervised_Affordance_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "University of Bonn;Carl Zeiss AG",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-bonn.de/;https://www.zeiss.com",
        "aff_unique_abbr": "UBonn;Zeiss",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Weakly Supervised Cascaded Convolutional Networks",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "303",
        "author_site": "Ali Diba, Vivek Sharma, Ali Pazandeh, Hamed Pirsiavash, Luc Van Gool",
        "author": "Ali Diba; Vivek Sharma; Ali Pazandeh; Hamed Pirsiavash; Luc Van Gool",
        "abstract": "Object detection is a challenging task in visual understanding domain, and even more so if the supervision is to be weak. Recently, few efforts to handle the task without expensive human annotations is established by promising deep neural network. A new architecture of cascaded networks is proposed to learn a convolutional neural network (CNN) under such conditions. We introduce two such architectures, with either two cascade stages or three which are trained in an end-to-end pipeline. The first stage of both architectures extracts best candidate of class specific region proposals by training a fully convolutional network. In the case of the three stage architecture, the middle stage provides object segmentation, using the output of the activation maps of first stage. The final stage of both architectures is a part of a convolutional neural network that performs multiple instance learning on proposals extracted in the previous stage(s). Our experiments on the PASCAL VOC 2007, 2010, 2012 and large scale object datasets, ILSVRC 2013, 2014 datasets show improvements in the areas of weakly-supervised object detection, classification and localization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Diba_Weakly_Supervised_Cascaded_CVPR_2017_paper.pdf",
        "aff": "ESAT-PSI, KU Leuven; CV:HCI, Karlsruhe Institute of Technology + ESAT-PSI, KU Leuven; Sharif University; University of Maryland Baltimore County; CVL, ETH Z\u00fcrich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1611.08258v1",
        "pdf_size": 1033853,
        "gs_citation": 370,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12519234615816350164&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "kuleuven.be;kit.edu;ee.sharif.edu;umbc.edu; ",
        "email": "kuleuven.be;kit.edu;ee.sharif.edu;umbc.edu; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Diba_Weakly_Supervised_Cascaded_CVPR_2017_paper.html",
        "aff_unique_index": "0;1+0;2;3;4",
        "aff_unique_norm": "KU Leuven;Karlsruhe Institute of Technology;Sharif University of Technology;University of Maryland, Baltimore County;ETH Zurich",
        "aff_unique_dep": "ESAT-PSI;CV:HCI;;;Computer Vision Laboratory",
        "aff_unique_url": "https://www.kuleuven.be;https://www.kit.edu;https://www.sharif.edu;https://www.umbc.edu;https://www.ethz.ch",
        "aff_unique_abbr": "KU Leuven;KIT;Sharif;UMBC;ETHZ",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Baltimore County",
        "aff_country_unique_index": "0;1+0;2;3;4",
        "aff_country_unique": "Belgium;Germany;Iran;United States;Switzerland"
    },
    {
        "title": "Weakly Supervised Dense Video Captioning",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "705",
        "author_site": "Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, Xiangyang Xue",
        "author": "Zhiqiang Shen; Jianguo Li; Zhou Su; Minjun Li; Yurong Chen; Yu-Gang Jiang; Xiangyang Xue",
        "abstract": "This paper focuses on a novel and challenging vision task, dense video captioning, which aims to automatically describe a video clip with multiple informative and diverse caption sentences. The proposed method is trained without explicit annotation of fine-grained sentence to video region-sequence correspondence, but is only based on weak video-level sentence annotations. It differs from existing video captioning systems in three technical aspects. First, we propose lexical fully convolutional neural networks (Lexical-FCN) with weakly supervised multi-instance multi-label learning to weakly link video regions with lexical labels. Second, we introduce a novel submodular maximization scheme to generate multiple informative and diverse region-sequences based on the Lexical-FCN outputs. A winner-takes-all scheme is adopted to weakly associate sentences to region-sequences in the training phase. Third, a sequence-to-sequence learning based language model is trained with the weakly supervised information obtained through the association process. We show that the proposed method can not only produce informative and diverse dense captions, but also outperform state-of-the-art single video captioning methods by a large margin.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shen_Weakly_Supervised_Dense_CVPR_2017_paper.pdf",
        "aff": "Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University; Intel Labs China; Intel Labs China; Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University; Intel Labs China; Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University; Shanghai Key Laboratory of Intelligent Information Processing, School of Computer Science, Fudan University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Shen_Weakly_Supervised_Dense_2017_CVPR_supplemental.pdf",
        "arxiv": "1704.01502v1",
        "pdf_size": 1642145,
        "gs_citation": 165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18332471947874271486&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "fudan.edu.cn;intel.com;intel.com;fudan.edu.cn;intel.com;fudan.edu.cn;fudan.edu.cn",
        "email": "fudan.edu.cn;intel.com;intel.com;fudan.edu.cn;intel.com;fudan.edu.cn;fudan.edu.cn",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shen_Weakly_Supervised_Dense_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0;1;0;0",
        "aff_unique_norm": "Fudan University;Intel",
        "aff_unique_dep": "School of Computer Science;Intel Labs",
        "aff_unique_url": "https://www.fudan.edu.cn;https://www.intel.cn",
        "aff_unique_abbr": "Fudan;Intel",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Weakly Supervised Semantic Segmentation Using Web-Crawled Videos",
        "session": "Machine Learning 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "3568",
        "author_site": "Seunghoon Hong, Donghun Yeo, Suha Kwak, Honglak Lee, Bohyung Han",
        "author": "Seunghoon Hong; Donghun Yeo; Suha Kwak; Honglak Lee; Bohyung Han",
        "abstract": "We propose a novel algorithm for weakly supervised semantic segmentation based on image-level class labels only. In weakly supervised setting, it is commonly observed that trained model overly focuses on discriminative parts rather than the entire object area. Our goal is to overcome this limitation with no additional human intervention by retrieving videos relevant to target class labels from web repository, and generating segmentation labels from the retrieved videos to simulate strong supervision for semantic segmentation. During this process, we take advantage of image classification with discriminative localization technique to reject false alarms in retrieved videos and identify relevant spatio-temporal volumes within retrieved videos. Although the entire procedure does not require any additional supervision, the segmentation annotations obtained from videos are sufficiently strong to learn a model for semantic segmentation. The proposed algorithm substantially outperforms existing methods based on the same level of supervision and is even as competitive as the approaches relying on extra annotations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Hong_Weakly_Supervised_Semantic_CVPR_2017_paper.pdf",
        "aff": "POSTECH; POSTECH; DGIST; University of Michigan; POSTECH",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1701.00352v3",
        "pdf_size": 1091962,
        "gs_citation": 191,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5226572212213865103&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "postech.ac.kr;postech.ac.kr;dgist.ac.kr;umich.edu;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr;dgist.ac.kr;umich.edu;postech.ac.kr",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Hong_Weakly_Supervised_Semantic_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Pohang University of Science and Technology;Daegu Gyeongbuk Institute of Science and Technology;University of Michigan",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.postech.ac.kr;https://www.dgist.ac.kr;https://www.umich.edu",
        "aff_unique_abbr": "POSTECH;DGIST;UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pohang;",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "title": "Weakly-Supervised Visual Grounding of Phrases With Linguistic Structures",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "2570",
        "author_site": "Fanyi Xiao, Leonid Sigal, Yong Jae Lee",
        "author": "Fanyi Xiao; Leonid Sigal; Yong Jae Lee",
        "abstract": "We propose a weakly-supervised approach that takes image-sentence pairs as input and learns to visually ground (i.e., localize) arbitrary linguistic phrases, in the form of spatial attention masks. Specifically, the model is trained with images and their associated image-level captions, without any explicit region-to-phrase correspondence annotations. To this end, we introduce an end-to-end model which learns visual groundings of phrases with two types of carefully designed loss functions. In addition to the standard discriminative loss, which enforces that attended image regions and phrases are consistently encoded, we propose a novel structural loss which makes use of the parse tree structures induced by the sentences. In particular, we ensure complementarity among the attention masks that correspond to sibling noun phrases, and compositionality of attention masks among the children and parent phrases, as defined by the sentence parse tree. We validate the effectiveness of our approach on the Microsoft COCO and Visual Genome datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xiao_Weakly-Supervised_Visual_Grounding_CVPR_2017_paper.pdf",
        "aff": "University of California, Davis; Disney Research; University of California, Davis",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.01371",
        "pdf_size": 2382434,
        "gs_citation": 152,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3502603648792712231&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "ucdavis.edu;disneyresearch.com;ucdavis.edu",
        "email": "ucdavis.edu;disneyresearch.com;ucdavis.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xiao_Weakly-Supervised_Visual_Grounding_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Davis;Disney Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucdavis.edu;https://research.disney.com",
        "aff_unique_abbr": "UC Davis;Disney Research",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Davis;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Webly Supervised Semantic Segmentation",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "1333",
        "author_site": "Bin Jin, Maria V. Ortiz Segovia, Sabine S\u00c3\u00bcsstrunk",
        "author": "Bin Jin; Maria V. Ortiz Segovia; Sabine Susstrunk",
        "abstract": "We propose a weakly supervised semantic segmentation algorithm that uses image tags for supervision. We apply the tags in queries to collect three sets of web images, which encode the clean foregrounds, the common back- grounds, and realistic scenes of the classes. We introduce a novel three-stage training pipeline to progressively learn semantic segmentation models. We first train and refine a class-specific shallow neural network to obtain segmentation masks for each class. The shallow neural networks of all classes are then assembled into one deep convolutional neural network for end-to-end training and testing. Experiments show that our method notably outperforms previous state-of-the-art weakly supervised semantic segmentation approaches on the PASCAL VOC 2012 segmentation bench- mark. We further apply the class-specific shallow neural networks to object segmentation and obtain excellent results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Jin_Webly_Supervised_Semantic_CVPR_2017_paper.pdf",
        "aff": "IC, EPFL; Oc\u00e9 Print Logic Technologies; IC, EPFL",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 840955,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6496530380704138750&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "epfl.ch;oce.com;epfl.ch",
        "email": "epfl.ch;oce.com;epfl.ch",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Jin_Webly_Supervised_Semantic_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "EPFL;Oc\u00e9",
        "aff_unique_dep": "IC (Computer Science Department);",
        "aff_unique_url": "https://www.epfl.ch;https://www.oce.com",
        "aff_unique_abbr": "EPFL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Switzerland;Netherlands"
    },
    {
        "title": "Weighted-Entropy-Based Quantization for Deep Neural Networks",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "2315",
        "author_site": "Eunhyeok Park, Junwhan Ahn, Sungjoo Yoo",
        "author": "Eunhyeok Park; Junwhan Ahn; Sungjoo Yoo",
        "abstract": "Quantization is considered as one of the most effective methods to optimize the inference cost of neural network models for their deployment to mobile and embedded systems, which have tight resource constraints. In such approaches, it is critical to provide low-cost quantization under a tight accuracy loss constraint (e.g., 1%). In this paper, we propose a novel method for quantizing weights and activations based on the concept of weighted entropy. Unlike recent work on binary-weight neural networks, our approach is multi-bit quantization, in which weights and activations can be quantized by any number of bits depending on the target accuracy. This facilitates much more flexible exploitation of accuracy-performance trade-off provided by different levels of quantization. Moreover, our scheme provides an automated quantization flow based on conventional training algorithms, which greatly reduces the design-time effort to quantize the network. According to our extensive evaluations based on practical neural network models for image classification (AlexNet, GoogLeNet and ResNet-50/101), object detection (R-FCN with 50-layer ResNet), and language modeling (an LSTM network), our method achieves significant reductions in both the model size and the amount of computation with minimal accuracy loss. Also, compared to existing quantization schemes, ours provides higher accuracy with a similar resource constraint and requires much lower design effort.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Park_Weighted-Entropy-Based_Quantization_for_CVPR_2017_paper.pdf",
        "aff": "Seoul National University; Seoul National University\u00a7Design Automation Laboratory; Seoul National University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 454584,
        "gs_citation": 336,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13512991371949992375&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;snu.ac.kr;gmail.com",
        "email": "gmail.com;snu.ac.kr;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Park_Weighted-Entropy-Based_Quantization_for_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Wetness and Color From a Single Multispectral Image",
        "session": "Low- & Mid-Level Vision",
        "status": "Oral",
        "track": "main",
        "pid": "1574",
        "author_site": "Mihoko Shimano, Hiroki Okawa, Yuta Asano, Ryoma Bise, Ko Nishino, Imari Sato",
        "author": "Mihoko Shimano; Hiroki Okawa; Yuta Asano; Ryoma Bise; Ko Nishino; Imari Sato",
        "abstract": "Visual recognition of wet surfaces and their degrees of wetness is important for many computer vision applications. It can inform slippery spots on a road to autonomous vehicles, muddy areas of a trail to humanoid robots, and the freshness of groceries to us. In the past, monochromatic appearance change, the fact that surfaces darken when wet, has been modeled to recognize wet surfaces. In this paper, we show that color change, particularly in its spectral behavior, carries rich information about a wet surface. We derive an analytical spectral appearance model of wet surfaces that expresses the characteristic spectral sharpening due to multiple scattering and absorption in the surface. We derive a novel method for estimating key parameters of this spectral appearance model, which enables the recovery of the original surface color and the degree of wetness from a single observation. Applied to a multispectral image, the method estimates the spatial map of wetness together with the dry spectral distribution of the surface. To our knowledge, this work is the first to model and leverage the spectral characteristics of wet surfaces to revert its appearance. We conduct comprehensive experimental validation with a number of wet real surfaces. The results demonstrate the accuracy of our model and the effectiveness of our method for surface wetness and color estimation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Shimano_Wetness_and_Color_CVPR_2017_paper.pdf",
        "aff": "National Institute of Informatics\u2020; Tokyo Institute of Technology\u2021; Tokyo Institute of Technology\u2021; National Institute of Informatics\u2020; Drexel University\u22c6; National Institute of Informatics\u2020",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 11383302,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13026230100870829868&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "nii.ac.jp;m.titech.ac.jp;m.titech.ac.jp;nii.ac.jp;drexel.edu;nii.ac.jp",
        "email": "nii.ac.jp;m.titech.ac.jp;m.titech.ac.jp;nii.ac.jp;drexel.edu;nii.ac.jp",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Shimano_Wetness_and_Color_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;0;2;0",
        "aff_unique_norm": "National Institute of Informatics;Tokyo Institute of Technology;Drexel University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nii.ac.jp;https://www.titech.ac.jp;https://www.drexel.edu",
        "aff_unique_abbr": "NII;Titech;Drexel",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "title": "What Can Help Pedestrian Detection?",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "1152",
        "author_site": "Jiayuan Mao, Tete Xiao, Yuning Jiang, Zhimin Cao",
        "author": "Jiayuan Mao; Tete Xiao; Yuning Jiang; Zhimin Cao",
        "abstract": "Aggregating extra features has been considered as an effective approach to boost traditional pedestrian detection methods. However, there is still a lack of studies on whether and how CNN-based pedestrian detectors can benefit from these extra features. The first contribution of this paper is exploring this issue by aggregating extra features into CNN-based pedestrian detection framework. Through extensive experiments, we evaluate the effects of different kinds of extra features quantitatively. Moreover, we propose a novel network architecture, namely HyperLearner, to jointly learn pedestrian detection as well as the given extra feature. By multi-task training, HyperLearner is able to utilize the information of given features and improve detection performance without extra inputs in inference. The experimental results on multiple pedestrian benchmarks validate the effectiveness of the proposed HyperLearner.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Mao_What_Can_Help_CVPR_2017_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1705.02757v1",
        "pdf_size": 1989462,
        "gs_citation": 363,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17025961347538531040&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Mao_What_Can_Help_CVPR_2017_paper.html"
    },
    {
        "title": "What Is and What Is Not a Salient Object? Learning Salient Object Detector by Ensembling Linear Exemplar Regressors",
        "session": "Object Recognition & Scene Understanding 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "1686",
        "author_site": "Changqun Xia, Jia Li, Xiaowu Chen, Anlin Zheng, Yu Zhang",
        "author": "Changqun Xia; Jia Li; Xiaowu Chen; Anlin Zheng; Yu Zhang",
        "abstract": "Finding what is and what is not a salient object can be helpful in developing better features and models in salient object detection (SOD). In this paper, we investigate the images that are selected and discarded in constructing a new SOD dataset and find that many similar candidates, complex shape and low objectness are three main attributes of many non-salient objects. Moreover, objects may have diversified attributes that make them salient. As a result, we propose a novel salient object detector by ensembling linear exemplar regressors. We first select reliable foreground and background seeds using the boundary prior and then adopt locally linear embedding (LLE) to conduct manifold-preserving foregroundness propagation. In this manner, a foregroundness map can be generated to roughly pop-out salient objects and suppress non-salient ones with many similar candidates. Moreover, we extract the shape, foregroundness and attention descriptors to characterize the extracted object proposals, and a linear exemplar regressor is trained to encode how to detect salient proposals in a specific image. Finally, various linear exemplar regressors are ensembled to form a single detector that adapts to various scenarios. Extensive experimental results on 5 dataset and the new SOD dataset show that our approach outperforms 9 state-of-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xia_What_Is_and_CVPR_2017_paper.pdf",
        "aff": "State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University + International Research Institute for Multidisciplinary Science, Beihang University; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1455741,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10429326392737413299&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xia_What_Is_and_CVPR_2017_paper.html",
        "aff_unique_index": "0;0+0;0;0;0",
        "aff_unique_norm": "Beihang University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.buaa.edu.cn",
        "aff_unique_abbr": "Beihang",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "What Is the Space of Attenuation Coefficients in Underwater Computer Vision?",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "2033",
        "author_site": "Derya Akkaynak, Tali Treibitz, Tom Shlesinger, Yossi Loya, Raz Tamir, David Iluz",
        "author": "Derya Akkaynak; Tali Treibitz; Tom Shlesinger; Yossi Loya; Raz Tamir; David Iluz",
        "abstract": "Underwater image reconstruction methods require the knowledge of wideband attenuation coefficients per color channel. Current estimation methods for these coefficients require specialized hardware or multiple images, and none of them leverage the multitude of existing ocean optical measurements as priors. Here, we aim to constrain the set of physically-feasible wideband attenuation coefficients in the ocean by utilizing water attenuation measured worldwide by oceanographers. We calculate the space of valid wideband effective attenuation coefficients in the 3D RGB domain and find that a bound manifold in 3-space sufficiently represents the variation from the clearest to murkiest waters. We validate our model using in situ experiments in two different optical water bodies, the Red Sea and the Mediterranean. Moreover, we show that contradictory to the common image formation model, the coefficients depend on the imaging range and object reflectance, and quantify the errors resulting from ignoring these dependencies.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Akkaynak_What_Is_the_CVPR_2017_paper.pdf",
        "aff": "University of Haifa+Inter-University Institute of Marine Sciences, Eilat; University of Haifa; Tel Aviv University; Inter-University Institute of Marine Sciences, Eilat+Tel Aviv University; Tel Aviv University; Bar Ilan University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2982839,
        "gs_citation": 221,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15902977265247754820&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;univ.haifa.ac.il;gmail.com;gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;univ.haifa.ac.il;gmail.com;gmail.com;gmail.com;gmail.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Akkaynak_What_Is_the_CVPR_2017_paper.html",
        "aff_unique_index": "0+1;0;2;1+2;2;3",
        "aff_unique_norm": "University of Haifa;Inter-University Institute of Marine Sciences;Tel Aviv University;Bar-Ilan University",
        "aff_unique_dep": ";Marine Sciences;;",
        "aff_unique_url": "https://www.haifa.ac.il;http://www.iui-eilat.ac.il;https://www.tau.ac.il;https://www.biu.ac.il",
        "aff_unique_abbr": "UoH;;TAU;BIU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Eilat",
        "aff_country_unique_index": "0+0;0;0;0+0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "What's in a Question: Using Visual Questions as a Form of Supervision",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Spotlight",
        "track": "main",
        "pid": "90",
        "author_site": "Siddha Ganju, Olga Russakovsky, Abhinav Gupta",
        "author": "Siddha Ganju; Olga Russakovsky; Abhinav Gupta",
        "abstract": "Collecting fully annotated image datasets is challenging and expensive. Many types of weak supervision have been explored: weak manual annotations, web search results, temporal continuity, ambient sound and others. We focus on one particular unexplored mode: visual questions that are asked about images. The key observation that inspires our work is that the question itself provides useful information about the image (even without the answer being available). For instance, the question \"what is the breed of the dog?\" informs the AI that the animal in the scene is a dog and that there is only one dog present. We make three contributions: (1) providing an extensive qualitative and quantitative analysis of the information contained in human visual questions, (2) proposing two simple but surprisingly effective modifications to the standard visual question answering models that allow them to make use of weak supervision in the form of unanswered questions associated with images and (3) demonstrating that a simple data augmentation strategy inspired by our insights results in a 7.1% improvement on the standard VQA benchmark.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ganju_Whats_in_a_CVPR_2017_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1704.03895",
        "pdf_size": 2501829,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18123483776113842345&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ganju_Whats_in_a_CVPR_2017_paper.html"
    },
    {
        "title": "Why You Should Forget Luminance Conversion and Do Something Better",
        "session": "Low- & Mid-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "3102",
        "author_site": "Rang M. H. Nguyen, Michael S. Brown",
        "author": "Rang M. H. Nguyen; Michael S. Brown",
        "abstract": "One of the most frequently applied low-level operations in computer vision is the conversion of an RGB camera image into its luminance representation.   This is also one of the most incorrectly applied operations. Even our most trusted softwares, Matlab and OpenCV, do not perform luminance conversion correctly.  In this paper, we examine the main factors that make proper RGB to luminance conversion difficult, in particular: 1) incorrect white-balance, 2) incorrect gamma/tone-curve correction, and 3) incorrect equations.  Our analysis shows errors up to 50% for various colors are not uncommon.   As a result, we argue that for most computer vision problems there is no need to attempt luminance conversion; instead, there are better alternatives depending on the task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Nguyen_Why_You_Should_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Nguyen_Why_You_Should_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5485995181827066080&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Nguyen_Why_You_Should_CVPR_2017_paper.html"
    },
    {
        "title": "Xception: Deep Learning With Depthwise Separable Convolutions",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "451",
        "author_site": "Fran\u00c3\u00a7ois Chollet",
        "author": "Francois Chollet",
        "abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf",
        "aff": "Google, Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "1610.02357v3",
        "pdf_size": 988439,
        "gs_citation": 22485,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3110565860331647079&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "google.com",
        "email": "google.com",
        "author_num": 1,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "title": "YOLO9000: Better, Faster, Stronger",
        "session": "Object Recognition & Scene Understanding 3",
        "status": "Oral",
        "track": "main",
        "pid": "3485",
        "author_site": "Joseph Redmon, Ali Farhadi",
        "author": "Joseph Redmon; Ali Farhadi",
        "abstract": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf",
        "aff": ";",
        "project": "http://pjreddie.com/yolo9000/",
        "github": "",
        "supp": "",
        "arxiv": "1612.08242v1",
        "pdf_size": 5392639,
        "gs_citation": 25398,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10688536692178239560&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 23,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.html"
    },
    {
        "title": "YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "2227",
        "author_site": "Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, Vincent Vanhoucke",
        "author": "Esteban Real; Jonathon Shlens; Stefano Mazzocchi; Xin Pan; Vincent Vanhoucke",
        "abstract": "We introduce a new large-scale data set of video URLs with densely-sampled object bounding box annotations called YouTube-BoundingBoxes (YT-BB). The data set consists of approximately 380,000 video segments about 19s long, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera. The objects represent a subset of the COCO label set. All video segments were human-annotated with high-precision classification labels and bounding boxes at 1 frame per second. The use of a cascade of increasingly precise human annotations ensures a label accuracy above 95% for every class and tight bounding boxes. Finally, we train and evaluate well-known deep network architectures and report baseline figures for per-frame classification and localization. We also demonstrate how the temporal contiguity of video can potentially be used to improve such inferences. The data set can be found at https://research.google.com/youtube-bb. We hope the availability of such large curated corpus will spur new advances in video object detection and tracking.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Real_YouTube-BoundingBoxes_A_Large_CVPR_2017_paper.pdf",
        "aff": "Google Brain; Google Brain; Google Research; Google Brain; Google Brain",
        "project": "https://research.google.com/youtube-bb",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Real_YouTube-BoundingBoxes_A_Large_2017_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1550142,
        "gs_citation": 738,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11574116814475502004&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Real_YouTube-BoundingBoxes_A_Large_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Brain",
        "aff_unique_url": "https://brain.google.com",
        "aff_unique_abbr": "Google Brain",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Zero Shot Learning via Multi-Scale Manifold Regularization",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "3339",
        "author_site": "Shay Deutsch, Soheil Kolouri, Kyungnam Kim, Yuri Owechko, Stefano Soatto",
        "author": "Shay Deutsch; Soheil Kolouri; Kyungnam Kim; Yuri Owechko; Stefano Soatto",
        "abstract": "We address zero-shot learning using a new manifold alignment framework based on a localized multi-scale transform on graphs. Our inference approach includes a smoothness criterion for a function mapping nodes on a graph (visual representation) onto a linear space (semantic representation), which we optimize using multi-scale graph wavelets. The robustness of the ensuing scheme allows us to operate with automatically generated semantic annotations, resulting in an algorithm that is entirely free of manual supervision, and yet improves the state-of-the-art as measured on benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Deutsch_Zero_Shot_Learning_CVPR_2017_paper.pdf",
        "aff": "UCLA Vision Lab, University of California, Los Angeles, CA 90095 + Department of Mathematics, University of California Los Angeles; HRL Laboratories, LLC, Malibu, CA; HRL Laboratories, LLC, Malibu, CA; HRL Laboratories, LLC, Malibu, CA; UCLA Vision Lab, University of California, Los Angeles, CA 90095",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 732259,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17186722882932472560&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "math.ucla.edu;cs.ucla.edu;hrl.com;hrl.com;hrl.com",
        "email": "math.ucla.edu;cs.ucla.edu;hrl.com;hrl.com;hrl.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Deutsch_Zero_Shot_Learning_CVPR_2017_paper.html",
        "aff_unique_index": "0+0;1;1;1;0",
        "aff_unique_norm": "University of California, Los Angeles;HRL Laboratories, LLC",
        "aff_unique_dep": "UCLA Vision Lab;",
        "aff_unique_url": "https://www.ucla.edu;https://www.hrl.com",
        "aff_unique_abbr": "UCLA;HRL",
        "aff_campus_unique_index": "0+0;1;1;1;0",
        "aff_campus_unique": "Los Angeles;Malibu",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Zero-Shot Action Recognition With Error-Correcting Output Codes",
        "session": "Video Analytics",
        "status": "Poster",
        "track": "main",
        "pid": "1035",
        "author_site": "Jie Qin, Li Liu, Ling Shao, Fumin Shen, Bingbing Ni, Jiaxin Chen, Yunhong Wang",
        "author": "Jie Qin; Li Liu; Ling Shao; Fumin Shen; Bingbing Ni; Jiaxin Chen; Yunhong Wang",
        "abstract": "Recently, zero-shot action recognition (ZSAR) has emerged with the explosive growth of action categories. In this paper, we explore ZSAR from a novel perspective by adopting the Error-Correcting Output Codes (dubbed ZSECOC). Our ZSECOC equips the conventional ECOC with the additional capability of ZSAR, by addressing the domain shift problem. In particular, we learn discriminative ZSECOC for seen categories from both category-level semantics and intrinsic data structures. This procedure deals with domain shift implicitly by transferring the well-established correlations among seen categories to unseen ones. Moreover, a simple semantic transfer strategy is developed for explicitly transforming the learned embeddings of seen categories to better fit the underlying structure of unseen categories. As a consequence, our ZSECOC inherits the promising characteristics from ECOC as well as overcomes domain shift, making it more discriminative for ZSAR. We systematically evaluate ZSECOC on three realistic action benchmarks, i.e. Olympic Sports, HMDB51 and UCF101. The experimental results clearly show the superiority of ZSECOC over the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Qin_Zero-Shot_Action_Recognition_CVPR_2017_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2468216,
        "gs_citation": 186,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15467173015380245251&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Qin_Zero-Shot_Action_Recognition_CVPR_2017_paper.html"
    },
    {
        "title": "Zero-Shot Classification With Discriminative Semantic Representation Learning",
        "session": "Machine Learning",
        "status": "Poster",
        "track": "main",
        "pid": "3361",
        "author_site": "Meng Ye, Yuhong Guo",
        "author": "Meng Ye; Yuhong Guo",
        "abstract": "Zero-shot learning, a special case of unsupervised domain adaptation where the source and target domains have disjoint label spaces, has become increasingly popular in the computer vision community. In this paper, we propose a novel zero-shot learning method based on discriminative sparse non-negative matrix factorization. The proposed approach aims to identify a set of common high-level semantic components across the two domains via non-negative sparse matrix factorization, while enforcing the representation vectors of the images in this common component-based space to be discriminatively aligned with the attribute-based label representation vectors. To fully exploit the aligned semantic information contained in the learned representation vectors of the instances, we develop a label propagation based testing procedure to classify the unlabeled instances from the unseen classes in the target domain. We conduct experiments on four standard zero-shot learning image datasets, by comparing the proposed approach to the state-of-the-art zero-shot learning methods. The empirical results demonstrate the efficacy of the proposed approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Ye_Zero-Shot_Classification_With_CVPR_2017_paper.pdf",
        "aff": "Computer and Information Sciences, Temple University; School of Computer Science, Carleton University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 515597,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2621084896648316436&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff_domain": "temple.edu;carleton.ca",
        "email": "temple.edu;carleton.ca",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ye_Zero-Shot_Classification_With_CVPR_2017_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Temple University;Carleton University",
        "aff_unique_dep": "Computer and Information Sciences;School of Computer Science",
        "aff_unique_url": "https://www.temple.edu;https://carleton.ca",
        "aff_unique_abbr": "Temple;Carleton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "Zero-Shot Learning - the Good, the Bad and the Ugly",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1896",
        "author_site": "Yongqin Xian, Bernt Schiele, Zeynep Akata",
        "author": "Yongqin Xian; Bernt Schiele; Zeynep Akata",
        "abstract": "Due to the importance of zero-shot learning, the number of proposed approaches has increased steadily recently. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss limitations of the current status of the area which can be taken as a basis for advancing it.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Xian_Zero-Shot_Learning_-_CVPR_2017_paper.pdf",
        "aff": "Max Planck Institute for Informatics; Max Planck Institute for Informatics; Max Planck Institute for Informatics + Amsterdam Machine Learning Lab",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 708596,
        "gs_citation": 1015,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14543995956671751974&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Xian_Zero-Shot_Learning_-_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Max Planck Institute for Informatics;Amsterdam Machine Learning Lab",
        "aff_unique_dep": ";Machine Learning",
        "aff_unique_url": "https://mpi-inf.mpg.de;https://amlab.nl",
        "aff_unique_abbr": "MPII;AMLab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1",
        "aff_country_unique": "Germany;Netherlands"
    },
    {
        "title": "Zero-Shot Recognition Using Dual Visual-Semantic Mapping Paths",
        "session": "Object Recognition & Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "1224",
        "author_site": "Yanan Li, Donghui Wang, Huanhang Hu, Yuetan Lin, Yueting Zhuang",
        "author": "Yanan Li; Donghui Wang; Huanhang Hu; Yuetan Lin; Yueting Zhuang",
        "abstract": "Zero-shot recognition aims to accurately recognize objects of unseen classes by using a shared visual-semantic mapping between the image feature space and the semantic embedding space. This  mapping is learned on training data of seen classes and is expected to have transfer ability to unseen classes. In this paper, we tackle this problem by exploiting the intrinsic relationship between the semantic space manifold and the transfer ability of visual-semantic mapping. We formalize their connection and cast zero-shot recognition as a joint optimization problem. Motivated by this, we propose a novel framework for zero-shot recognition, which contains dual visual-semantic mapping paths. Our analysis shows this framework can not only apply prior semantic knowledge to infer underlying semantic manifold in the image feature space, but also generate optimized semantic embedding space, which can enhance the transfer ability of the visual-semantic mapping to unseen classes. The proposed method is evaluated for zero-shot recognition on four benchmark datasets, achieving outstanding results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Zero-Shot_Recognition_Using_CVPR_2017_paper.pdf",
        "aff": "Institute of Artificial Intelligence, Zhejiang University; Institute of Artificial Intelligence, Zhejiang University; Institute of Artificial Intelligence, Zhejiang University; Institute of Artificial Intelligence, Zhejiang University; Institute of Artificial Intelligence, Zhejiang University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Zero-Shot_Recognition_Using_2017_CVPR_supplemental.pdf",
        "arxiv": "1703.05002v2",
        "pdf_size": 4105244,
        "gs_citation": 187,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13142330732398283713&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Li_Zero-Shot_Recognition_Using_CVPR_2017_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "Institute of Artificial Intelligence",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "iCaRL: Incremental Classifier and Representation Learning",
        "session": "Analyzing Humans with 3D Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "739",
        "author_site": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, Christoph H. Lampert",
        "author": "Sylvestre-Alvise Rebuffi; Alexander Kolesnikov; Georg Sperl; Christoph H. Lampert",
        "abstract": "A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2017/papers/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.pdf",
        "aff": "University of Oxford/IST Austria; IST Austria; IST Austria; IST Austria",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2017/supplemental/Rebuffi_iCaRL_Incremental_Classifier_2017_CVPR_supplemental.pdf",
        "arxiv": "1611.07725v2",
        "pdf_size": 1184899,
        "gs_citation": 4974,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13522192663157931909&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2017/html/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Oxford;Institute of Science and Technology Austria",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.ist.ac.at",
        "aff_unique_abbr": "Oxford;IST Austria",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Oxford;",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "United Kingdom;Austria"
    }
]