[
    {
        "title": "3D Action Recognition From Novel Viewpoints",
        "session": "Recognition and Parsing In 3D",
        "status": "Oral",
        "track": "main",
        "pid": "1",
        "author_site": "Hossein Rahmani, Ajmal Mian",
        "author": "Hossein Rahmani; Ajmal Mian",
        "abstract": "We propose a human pose representation model that transfers human poses acquired from different unknown views to a view-invariant high-level space. The model is a deep convolutional neural network and requires a large corpus of multiview training data which is very expensive to acquire. Therefore, we propose a method to generate this data by fitting synthetic 3D human models to real motion capture data and rendering the human poses from numerous viewpoints. While learning the CNN model, we do not use action labels but only the pose labels after clustering all training poses into k clusters. The proposed model is able to generalize to real depth images of unseen poses without the need for re-training or fine-tuning. Real depth videos are passed through the model frame-wise to extract view-invariant features. For spatio-temporal representation, we propose group sparse Fourier Temporal Pyramid which robustly encodes the action specific most discriminative output features of the proposed human pose model. Experiments on two multiview and three single-view benchmark datasets show that the proposed method dramatically outperforms existing state-of-the-art in action recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Rahmani_3D_Action_Recognition_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science and Software Engineering, The University of Western Australia; School of Computer Science and Software Engineering, The University of Western Australia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Rahmani_3D_Action_Recognition_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1624578,
        "gs_citation": 243,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15168279017664879441&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "csse.uwa.edu.au;uwa.edu.au",
        "email": "csse.uwa.edu.au;uwa.edu.au",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Rahmani_3D_Action_Recognition_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Western Australia",
        "aff_unique_dep": "School of Computer Science and Software Engineering",
        "aff_unique_url": "https://www.uwa.edu.au",
        "aff_unique_abbr": "UWA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "3D Part-Based Sparse Tracker With Automatic Synchronization and Registration",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "72",
        "author_site": "Adel Bibi, Tianzhu Zhang, Bernard Ghanem",
        "author": "Adel Bibi; Tianzhu Zhang; Bernard Ghanem",
        "abstract": "In this paper, we present a part-based sparse tracker in a particle filter framework where both the motion and appearance model are formulated in 3D. The motion model is adaptive and directed according to a simple yet powerful occlusion handling paradigm, which is intrinsically fused  in the motion model. Also, since 3D trackers are sensitive to synchronization and registration noise in the RGB and depth streams, we propose automated methods to solve these two issues. Extensive experiments are conducted on a popular RGBD tracking benchmark, which demonstrate that our tracker can achieve superior results, outperforming many other recent and state-of-the-art RGBD trackers.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bibi_3D_Part-Based_Sparse_CVPR_2016_paper.pdf",
        "aff": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1197503,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12176270526398664934&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "kaust.edu.sa;kaust.edu.sa;kaust.edu.sa",
        "email": "kaust.edu.sa;kaust.edu.sa;kaust.edu.sa",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bibi_3D_Part-Based_Sparse_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaust.edu.sa",
        "aff_unique_abbr": "KAUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Saudi Arabia"
    },
    {
        "title": "3D Reconstruction of Transparent Objects With Position-Normal Consistency",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "62",
        "author_site": "Yiming Qian, Minglun Gong, Yee Hong Yang",
        "author": "Yiming Qian; Minglun Gong; Yee Hong Yang",
        "abstract": "Estimating the shape of transparent and refractive objects is one of the few open problems in 3D reconstruction. Under the assumption that the rays refract only twice when traveling through the object, we present the first approach to simultaneously reconstructing the 3D positions and normals of the object's surface at both refraction locations. Our acquisition setup requires only two cameras and one monitor, which serves as the light source. After acquiring the ray-ray correspondences between each camera and the monitor, we solve an optimization function which enforces a new position-normal consistency constraint. That is, the 3D positions of surface points shall agree with the normals required to refract the rays under Snell's law. Experimental results using both synthetic and real data demonstrate the robustness and accuracy of the proposed approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Qian_3D_Reconstruction_of_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11073663866506909039&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Qian_3D_Reconstruction_of_CVPR_2016_paper.html"
    },
    {
        "title": "3D Semantic Parsing of Large-Scale Indoor Spaces",
        "session": "Recognition and Parsing In 3D",
        "status": "Oral",
        "track": "main",
        "pid": "4",
        "author_site": "Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, Silvio Savarese",
        "author": "Iro Armeni; Ozan Sener; Amir R. Zamir; Helen Jiang; Ioannis Brilakis; Martin Fischer; Silvio Savarese",
        "abstract": "In this paper, we propose a method for semantic parsing the 3D point cloud of an entire building using a hierarchical approach: first, the raw data is parsed into semantically meaningful spaces (e.g. rooms, etc) that are aligned into a canonical reference coordinate system. Second, the spaces are parsed into their structural and building elements (e.g. walls, columns, etc). Performing these with a strong notation of global 3D space is the backbone of our method. The alignment in the first step injects strong 3D priors from the canonical coordinate system into the second step for dis- covering elements. This allows diverse challenging scenarios as man-made indoor spaces often show recurrent geo- metric patterns while the appearance features can change drastically. We also argue that identification of structural elements in indoor spaces is essentially a detection problem, rather than segmentation which is commonly used. We evaluated our method on a new dataset of several buildings with a covered area of over 6, 000m2 and over 215 million points, demonstrating robust results readily useful for practical applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Armeni_3D_Semantic_Parsing_CVPR_2016_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Armeni_3D_Semantic_Parsing_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 3016124,
        "gs_citation": 2242,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6251144525214379784&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Armeni_3D_Semantic_Parsing_CVPR_2016_paper.html"
    },
    {
        "title": "3D Shape Attributes",
        "session": "Recognition and Parsing In 3D",
        "status": "Oral",
        "track": "main",
        "pid": "2",
        "author_site": "David F. Fouhey, Abhinav Gupta, Andrew Zisserman",
        "author": "David F. Fouhey; Abhinav Gupta; Andrew Zisserman",
        "abstract": "In this paper we investigate 3D attributes as a means to understand the shape of an object in a single image.  To this end, we make a number of contributions: (i) we introduce and define a set of 3D Shape attributes, including planarity, symmetry and occupied space; (ii) we show that such properties can be successfully inferred from a single image using a Convolutional Neural Network (CNN); (iii) we introduce a 143K image dataset of sculptures with 2197 works over 242 artists for training and evaluating the CNN; (iv) we show that the 3D attributes trained on this dataset generalize to images of other (non-sculpture) object classes; and furthermore (v) we show that the CNN also provides a shape embedding that can be used to match previously unseen sculptures largely independent of viewpoint.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Fouhey_3D_Shape_Attributes_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3309522,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3899456741998707323&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Fouhey_3D_Shape_Attributes_CVPR_2016_paper.html"
    },
    {
        "title": "6D Dynamic Camera Relocalization From Single Reference Image",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "28",
        "author_site": "Wei Feng, Fei-Peng Tian, Qian Zhang, Jizhou Sun",
        "author": "Wei Feng; Fei-Peng Tian; Qian Zhang; Jizhou Sun",
        "abstract": "Dynamic relocalization of 6D camera pose from single reference image is a costly and challenging task that requires delicate hand-eye calibration and precision positioning platform to do 3D mechanical rotation and translation. In this paper, we show that high-quality camera relocalization can be achieved in a much less expensive way. Based on inexpensive platform with unreliable absolute repositioning accuracy (ARA), we propose a hand-eye calibration free strategy to actively relocate camera into the same 6D pose that produces the input reference image, by sequentially correcting 3D relative rotation and translation. We theoretically prove that, by this strategy, both rotational and translational relative pose can be effectively reduced to zero, with bounded unknown hand-eye pose displacement. To conquer 3D rotation and translation ambiguity, this theoretical strategy is further revised to a practical relocalization algorithm with faster convergence rate and more reliability by jointly adjusting 3D relative rotation and translation. Extensive experiments validate the effectiveness and superior accuracy of the proposed approach on laboratory tests and challenging real-world applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Feng_6D_Dynamic_Camera_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science and Technology, Tianjin University, Tianjin, China+Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China; School of Computer Science and Technology, Tianjin University, Tianjin, China+Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China; School of Computer Science and Technology, Tianjin University, Tianjin, China+Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China; School of Computer Science and Technology, Tianjin University, Tianjin, China+Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, Tianjin, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 11148479,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12123736399785262881&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "email": "tju.edu.cn;tju.edu.cn;tju.edu.cn;tju.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Feng_6D_Dynamic_Camera_CVPR_2016_paper.html",
        "aff_unique_index": "0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Tianjin University",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.tju.edu.cn",
        "aff_unique_abbr": "Tianjin University",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Tianjin",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "A 3D Morphable Model Learnt From 10,000 Faces",
        "session": "People and Faces",
        "status": "Spotlight",
        "track": "main",
        "pid": "27",
        "author_site": "James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan Ponniah, David Dunaway",
        "author": "James Booth; Anastasios Roussos; Stefanos Zafeiriou; Allan Ponniah; David Dunaway",
        "abstract": "We present Large Scale Facial Model (LSFM) -- a 3D Morphable Model (3DMM) automatically constructed from 9,663 distinct facial identities. To the best of our knowledge LSFM is the largest-scale Morphable Model ever constructed, containing statistical information from a huge variety of the human population. To build such a large model we introduce a novel fully automated and robust Morphable Model construction pipeline. The dataset that LSFM is trained on includes rich demographic information about each subject, allowing for the construction of not only a global 3DMM but also models tailored for specific age, gender or ethnicity groups. As an application example, we utilise the proposed model to perform age classification from 3D shape alone. Furthermore, we perform a systematic analysis of the constructed 3DMMs that showcases their quality and descriptive power. The presented extensive qualitative and quantitative evaluations reveal that the proposed 3DMM achieves state-of-the-art results, outperforming existing models by a large margin. Finally, for the benefit of the research community, we make publicly available the source code of the proposed automatic 3DMM construction pipeline. In addition, the constructed global 3DMM and a variety of bespoke models tailored by age, gender and ethnicity are available on application to researchers involved in medically oriented research.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Booth_A_3D_Morphable_CVPR_2016_paper.pdf",
        "aff": "Imperial College London, UK+Center for Machine Vision and Signal Analysis (CMVS), University of Oulu, Finland; Imperial College London, UK+Center for Machine Vision and Signal Analysis (CMVS), University of Oulu, Finland; Imperial College London, UK+Center for Machine Vision and Signal Analysis (CMVS), University of Oulu, Finland; Great Ormond Street Hospital, UK; Great Ormond Street Hospital, UK",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Booth_A_3D_Morphable_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 3493119,
        "gs_citation": 424,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9589000559197937280&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;gosh.nhs.uk;gosh.nhs.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;gosh.nhs.uk;gosh.nhs.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Booth_A_3D_Morphable_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;2;2",
        "aff_unique_norm": "Imperial College London;University of Oulu;Great Ormond Street Hospital",
        "aff_unique_dep": ";Center for Machine Vision and Signal Analysis (CMVS);",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.oulu.fi;https://www.gosh.org",
        "aff_unique_abbr": "ICL;;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0+1;0;0",
        "aff_country_unique": "United Kingdom;Finland"
    },
    {
        "title": "A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation",
        "session": "Video Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "78",
        "author_site": "Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, Alexander Sorkine-Hornung",
        "author": "Federico Perazzi; Jordi Pont-Tuset; Brian McWilliams; Luc Van Gool; Markus Gross; Alexander Sorkine-Hornung",
        "abstract": "Over the years, datasets and benchmarks have proven their fundamental importance in computer vision research, enabling targeted progress and objective comparisons in many fields. At the same time, legacy datasets may impend the evolution of a field due to saturated algorithm performance and the lack of contemporary, high quality data. In this work we present a new benchmark dataset and evaluation methodology for the area of video object segmentation. The dataset, named DAVIS (Densely Annotated VIdeo Segmentation), consists of fifty high quality, Full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motion-blur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation. In addition, we provide a comprehensive analysis of several state-of-the-art segmentation approaches using three complementary metrics that measure the spatial extent of the segmentation, the accuracy of the silhouette contours and the temporal coherence. The results uncover strengths and weaknesses of current approaches, opening up promising directions for future works.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2152156,
        "gs_citation": 2422,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15997908011077742822&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.html"
    },
    {
        "title": "A Benchmark Dataset and Evaluation for Non-Lambertian and Uncalibrated Photometric Stereo",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "73",
        "author_site": "Boxin Shi, Zhe Wu, Zhipeng Mo, Dinglong Duan, Sai-Kit Yeung, Ping Tan",
        "author": "Boxin Shi; Zhe Wu; Zhipeng Mo; Dinglong Duan; Sai-Kit Yeung; Ping Tan",
        "abstract": "Recent progress on photometric stereo extends the technique to deal with general materials and unknown illumination conditions. However, due to the lack of suitable benchmark data with ground truth shapes (normals), quantitative comparison and evaluation is difficult to achieve. In this paper, we first survey and categorize existing methods using a photometric stereo taxonomy emphasizing on non-Lambertian and uncalibrated methods. We then introduce the 'DiLiGenT' photometric stereo image dataset with calibrated Directional Lightings, objects of General reflectance, and 'ground Truth' shapes (normals). Based on our dataset, we quantitatively evaluate state-of-the-art photometric stereo methods for general non-Lambertian materials and unknown lightings to analyze their strengths and limitations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shi_A_Benchmark_Dataset_CVPR_2016_paper.pdf",
        "aff": "Artificial Intelligence Research Center, National Institute of AIST + Singapore University of Technology and Design; Singapore University of Technology and Design + National University of Singapore; Singapore University of Technology and Design; National University of Singapore; Singapore University of Technology and Design; Simon Fraser University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Shi_A_Benchmark_Dataset_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1524303,
        "gs_citation": 358,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2387030137278821293&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "aist.go.jp;u.nus.edu;u.nus.edu;u.nus.edu; sutd.edu.sg;sfu.ca",
        "email": "aist.go.jp;u.nus.edu;u.nus.edu;u.nus.edu; sutd.edu.sg;sfu.ca",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shi_A_Benchmark_Dataset_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1+2;1;2;1;3",
        "aff_unique_norm": "National Institute of Advanced Industrial Science and Technology;Singapore University of Technology and Design;National University of Singapore;Simon Fraser University",
        "aff_unique_dep": "Artificial Intelligence Research Center;;;",
        "aff_unique_url": "https://www.aist.go.jp;https://www.sutd.edu.sg;https://www.nus.edu.sg;https://www.sfu.ca",
        "aff_unique_abbr": "AIST;SUTD;NUS;SFU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1+1;1;1;1;2",
        "aff_country_unique": "Japan;Singapore;Canada"
    },
    {
        "title": "A Comparative Study for Single Image Blind Deblurring",
        "session": "Image Processing and Restoration",
        "status": "Spotlight",
        "track": "main",
        "pid": "22",
        "author_site": "Wei-Sheng Lai, Jia-Bin Huang, Zhe Hu, Narendra Ahuja, Ming-Hsuan Yang",
        "author": "Wei-Sheng Lai; Jia-Bin Huang; Zhe Hu; Narendra Ahuja; Ming-Hsuan Yang",
        "abstract": "Numerous single image blind deblurring algorithms have been proposed to restore latent sharp images under camera motion. However, these algorithms are mainly evaluated using either synthetic datasets or few selected real blurred images. It is thus unclear how these algorithms would perform on images acquired \"in the wild\" and how we could gauge the progress in the field. In this paper, we aim to bridge this gap. We present the first comprehensive perceptual study and analysis of single image blind deblurring using real-world blurred images. First, we collect a dataset of real blurred images and a dataset of synthetically blurred images. Using these datasets, we conduct a large-scale user study to quantify the performance of several representative state-of-the-art blind deblurring algorithms. Second, we systematically analyze subject preferences, including the level of agreement, significance tests of score differences, and rationales for preferring one method over another. Third, we study the correlation between human subjective scores and several full-reference and no-reference image quality metrics. Our evaluation and analysis indicate the performance gap between synthetically blurred images and real blurred image and sheds light on future research in single image blind deblurring.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lai_A_Comparative_Study_CVPR_2016_paper.pdf",
        "aff": "University of California, Merced; University of Illinois, Urbana-Champaign; University of California, Merced; University of Illinois, Urbana-Champaign; University of California, Merced",
        "project": "http://vllab.ucmerced.edu/~wlai24/cvpr16_deblur_study",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Lai_A_Comparative_Study_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2715446,
        "gs_citation": 524,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12191465174374248661&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lai_A_Comparative_Study_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "University of California, Merced;University of Illinois",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucmerced.edu;https://illinois.edu",
        "aff_unique_abbr": "UC Merced;UIUC",
        "aff_campus_unique_index": "0;1;0;1;0",
        "aff_campus_unique": "Merced;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Consensus-Based Framework for Distributed Bundle Adjustment",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "28",
        "author_site": "Anders Eriksson, John Bastian, Tat-Jun Chin, Mats Isaksson",
        "author": "Anders Eriksson; John Bastian; Tat-Jun Chin; Mats Isaksson",
        "abstract": "In this paper we study large-scale optimization problems in multi-view geometry, in particular the Bundle Adjustment problem. In its conventional formulation, the complexity of existing solvers scale poorly with problem size, hence this component of the Structure-from-Motion pipeline can quickly become a bottle-neck. Here we present a novel formulation for solving bundle adjustment in a truly distributed manner using consensus based optimization methods. Our algorithm is presented with a concise derivation based on proximal splitting, along with a theoretical proof of convergence and brief discussions on complexity and implementation. Experiments on a number of real image datasets convincingly demonstrates the potential of the proposed method by outperforming the conventional bundle adjustment formulation by orders of magnitude.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Eriksson_A_Consensus-Based_Framework_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Eriksson_A_Consensus-Based_Framework_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1683026,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=25860913143424691&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Eriksson_A_Consensus-Based_Framework_CVPR_2016_paper.html"
    },
    {
        "title": "A Continuous Occlusion Model for Road Scene Understanding",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "58",
        "author_site": "Vikas Dhiman, Quoc-Huy Tran, Jason J. Corso, Manmohan Chandraker",
        "author": "Vikas Dhiman; Quoc-Huy Tran; Jason J. Corso; Manmohan Chandraker",
        "abstract": "We present a physically interpretable, continuous 3D model for handling occlusions with applications to road scene understanding. We probabilistically assign each point in space to an object with a theoretical modeling of the reflection and transmission probabilities for the corresponding camera ray. Our modeling is unified in handling occlusions across a variety of scenarios, such as associating structure from motion point tracks with potentially occluded objects or modeling object detection scores in applications such as 3D localization. For point track association, our model uniformly handles static and dynamic objects, which is an advantage over motion segmentation approaches traditionally used in multibody SFM. Detailed experiments on the KITTI dataset show the superiority of the proposed method over both state-of-the-art motion segmentation and a baseline that heuristically uses detection bounding boxes for resolving occlusions. We also demonstrate how our continuous occlusion model may be applied to the task of 3D localization in road scenes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Dhiman_A_Continuous_Occlusion_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Dhiman_A_Continuous_Occlusion_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 845582,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=44931347907854353&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Dhiman_A_Continuous_Occlusion_CVPR_2016_paper.html"
    },
    {
        "title": "A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "55",
        "author_site": "Neil D. B. Bruce, Christopher Catton, Sasa Janjic",
        "author": "Neil D. B. Bruce; Christopher Catton; Sasa Janjic",
        "abstract": "In this paper we consider the problem of visual saliency modeling, including both human gaze prediction and salient object segmentation. The overarching goal of the paper is to identify high level considerations relevant to deriving more sophisticated visual saliency models. A deep learning model based on fully convolutional networks (FCNs) is presented, which shows very favorable performance across a wide variety of benchmarks relative to existing proposals. We also demonstrate that the manner in which training data is selected, and ground truth treated is critical to resulting model behaviour. Recent efforts have explored the relationship between human gaze and salient objects, and we also examine this point further in the context of FCNs. Close examination of the proposed and alternative models serves as a vehicle for identifying problems important to developing more comprehensive models going forward.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bruce_A_Deeper_Look_CVPR_2016_paper.pdf",
        "aff": "University of Manitoba; University of Manitoba; University of Manitoba",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6019941,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10763419248994272392&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.umanitoba.ca; ; ",
        "email": "cs.umanitoba.ca; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bruce_A_Deeper_Look_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Manitoba",
        "aff_unique_dep": "",
        "aff_unique_url": "https://umanitoba.ca",
        "aff_unique_abbr": "U of M",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "A Direct Least-Squares Solution to the PnP Problem With Unknown Focal Length",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "32",
        "author_site": "Yinqiang Zheng, Laurent Kneip",
        "author": "Yinqiang Zheng; Laurent Kneip",
        "abstract": "In this work, we propose a direct least-squares solution to the perspective-(n)-point (P(n)P) pose estimation problem of a partially calibrated camera, whose intrinsic parameters except the focal length are known. The basic idea is to construct a proper objective function with respect to the target variables and extract all its stationary points so as to find the global minimum. The advantages of our proposed solution over existing ones are that (i) the objective function is directly built upon the imaging equation, such that all the 3D-to-2D correspondences are treated with balance, and that (ii) the proposed solution is noniterative, in the sense that the stationary points are retrieved by means of standard eigenvalue factorization and the common iterative refinement step is not needed. In addition, the proposed solution has (O(n)) complexity, and can be used to handle both planar and nonplanar 3D points. Experimental results have shown that the proposed solution is much more accurate than the existing state-of-the-art solutions, and is even comparable to the maximum likelihood estimation by minimizing the reprojection error.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zheng_A_Direct_Least-Squares_CVPR_2016_paper.pdf",
        "aff": "National Institute of Informatics, Tokyo, JAPAN; Australian National University, Canberra, AUSTRALIA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4246218,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8649295273219855118&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "nii.ac.jp;anu.edu.au",
        "email": "nii.ac.jp;anu.edu.au",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zheng_A_Direct_Least-Squares_CVPR_2016_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "National Institute of Informatics;Australian National University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nii.ac.jp;https://www.anu.edu.au",
        "aff_unique_abbr": "NII;ANU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Tokyo;Canberra",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Japan;Australia"
    },
    {
        "title": "A Dual-Source Approach for 3D Pose Estimation From a Single Image",
        "session": "Human Pose Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "43",
        "author_site": "Hashim Yasin, Umar Iqbal, Bj\u00f6rn Kr\u00fcger, Andreas Weber, Juergen Gall",
        "author": "Hashim Yasin; Umar Iqbal; Bjorn Kruger; Andreas Weber; Juergen Gall",
        "abstract": "One major challenge for 3D pose estimation from a single RGB image is the acquisition of sufficient training data. In particular, collecting large amounts of training data that contain unconstrained images and are annotated with accurate 3D poses is infeasible. We therefore propose to use two independent training sources. The first source consists of images with annotated 2D poses and the second source consists of accurate 3D motion capture data. To integrate both sources, we propose a dual-source approach that combines 2D pose estimation with efficient and robust 3D pose retrieval. In our experiments, we show that our approach achieves state-of-the-art results and is even competitive when the skeleton structure of the two sources differ substantially.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yasin_A_Dual-Source_Approach_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 260,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11016851658732282467&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yasin_A_Dual-Source_Approach_CVPR_2016_paper.html"
    },
    {
        "title": "A Field Model for Repairing 3D Shapes",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "41",
        "author_site": "Duc Thanh Nguyen, Binh-Son Hua, Khoi Tran, Quang-Hieu Pham, Sai-Kit Yeung",
        "author": "Duc Thanh Nguyen; Binh-Son Hua; Khoi Tran; Quang-Hieu Pham; Sai-Kit Yeung",
        "abstract": "This paper proposes a field model for repairing 3D shapes constructed from multi-view RGB data. Specifically, we represent a 3D shape in a Markov random field (MRF) in which the geometric information is encoded by random binary variables and the appearance information is retrieved from a set of RGB images captured at multiple viewpoints. The local priors in the MRF model capture the local structures of object shapes and are learnt from 3D shape templates using a convolutional deep belief network. Repairing a 3D shape is formulated as the maximum a posteriori (MAP) estimation in the corresponding MRF. Variational mean field approximation technique is adopted for the MAP estimation. The proposed method was evaluated on both artificial data and real data obtained from reconstruction of practical scenes. Experimental results have shown the robustness and efficiency of the proposed method in repairing noisy and incomplete 3D shapes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Nguyen_A_Field_Model_CVPR_2016_paper.pdf",
        "aff": "School of Information Technology, Deakin University, Australia + Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1258122,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5348355733895832385&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Nguyen_A_Field_Model_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;1;1;1",
        "aff_unique_norm": "Deakin University;Singapore University of Technology and Design",
        "aff_unique_dep": "School of Information Technology;",
        "aff_unique_url": "https://www.deakin.edu.au;https://www.sutd.edu.sg",
        "aff_unique_abbr": "Deakin;SUTD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1;1",
        "aff_country_unique": "Australia;Singapore"
    },
    {
        "title": "A Hierarchical Deep Temporal Model for Group Activity Recognition",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "51",
        "author_site": "Mostafa S. Ibrahim, Srikanth Muralidharan, Zhiwei Deng, Arash Vahdat, Greg Mori",
        "author": "Mostafa S. Ibrahim; Srikanth Muralidharan; Zhiwei Deng; Arash Vahdat; Greg Mori",
        "abstract": "In group activity recognition, the temporal dynamics of the whole activity can be inferred based on the dynamics of the individual people representing the activity. We build a deep model to capture these dynamics based on LSTM (long short-term memory) models. To make use of these observations, we present a 2-stage deep temporal model for the group activity recognition problem. In our model, a LSTM model is designed to represent action dynamics of individual people in a sequence and another LSTM model is designed to aggregate person-level information for whole activity understanding.   We evaluate our model over two datasets: the Collective Activity Dataset and a new volleyball dataset. Experimental results demonstrate that our proposed model improves group activity recognition performance  compared to baseline methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ibrahim_A_Hierarchical_Deep_CVPR_2016_paper.pdf",
        "aff": "School of Computing Science, Simon Fraser University, Burnaby, Canada; School of Computing Science, Simon Fraser University, Burnaby, Canada; School of Computing Science, Simon Fraser University, Burnaby, Canada; School of Computing Science, Simon Fraser University, Burnaby, Canada; School of Computing Science, Simon Fraser University, Burnaby, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 949526,
        "gs_citation": 642,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16767198570026032282&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "sfu.ca;sfu.ca;sfu.ca;sfu.ca;cs.sfu.ca",
        "email": "sfu.ca;sfu.ca;sfu.ca;sfu.ca;cs.sfu.ca",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ibrahim_A_Hierarchical_Deep_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Simon Fraser University",
        "aff_unique_dep": "School of Computing Science",
        "aff_unique_url": "https://www.sfu.ca",
        "aff_unique_abbr": "SFU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Burnaby",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "A Hierarchical Pose-Based Approach to Complex Action Understanding Using Dictionaries of Actionlets and Motion Poselets",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "52",
        "author_site": "Ivan Lillo, Juan Carlos Niebles, Alvaro Soto",
        "author": "Ivan Lillo; Juan Carlos Niebles; Alvaro Soto",
        "abstract": "In this paper, we introduce a new hierarchical model for human action recognition that is able to categorize complex actions performed in videos. Our model is also able to perform spatio-temporal annotation of the atomic actions that compose the overall complex action. That is, for each atomic action, the model generates temporal atomic action annotations by inferring the starting and ending times of the atomic action, as well spatial annotations by inferring the human body parts that are involved in each atomic action. Our model has three key properties: (i) it can be trained with no spatial supervision, as it is able to automatically discover the relevant body parts from temporal action annotations only; (ii) its jointly learned poselet and actionlet representation encodes the visual variability of actions with good generalization power; (iii) its mechanism for handling noisy body pose estimates make it robust to common pose estimation errors. We experimentally evaluate the performance of our method in multiple action recognition benchmarks. Our model consistently outperform baselines and state-of-the-art action recognition methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lillo_A_Hierarchical_Pose-Based_CVPR_2016_paper.pdf",
        "aff": "P. Universidad Catolica de Chile, Santiago, Chile; Stanford University, USA + Universidad del Norte, Colombia; P. Universidad Catolica de Chile, Santiago, Chile",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1147828,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9796229998463968578&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "uc.cl;cs.stanford.edu;ing.uc.cl",
        "email": "uc.cl;cs.stanford.edu;ing.uc.cl",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lillo_A_Hierarchical_Pose-Based_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Pontificia Universidad Cat\u00f3lica de Chile;Stanford University;Universidad del Norte",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.puc.cl;https://www.stanford.edu;https://www.uninorte.edu.co",
        "aff_unique_abbr": "PUC;Stanford;",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Santiago;Stanford;",
        "aff_country_unique_index": "0;1+2;0",
        "aff_country_unique": "Chile;United States;Colombia"
    },
    {
        "title": "A Hole Filling Approach Based on Background Reconstruction for View Synthesis in 3D Video",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "31",
        "author_site": "Guibo Luo, Yuesheng Zhu, Zhaotian Li, Liming Zhang",
        "author": "Guibo Luo; Yuesheng Zhu; Zhaotian Li; Liming Zhang",
        "abstract": "The depth image based rendering (DIBR) plays a key role in 3D video synthesis, by which other virtual views can be generated from a 2D video and its depth map. However, in the synthesis process, the background occluded by the foreground objects might be exposed in the new view, resulting in some holes in the synthetized video. In this paper, a hole filling approach based on background reconstruction is proposed, in which the temporal correlation information in both the 2D video and its corresponding depth map are exploited to construct a background video. To construct a clean background video, the foreground objects are detected and removed. Also motion compensation is applied to make the background reconstruction model suitable for moving camera scenario. Each frame is projected to the current plane where a modified Gaussian mixture model is performed. The constructed background video is used to eliminate the holes in the synthetized video. Our experimental results have indicated that the proposed approach has better quality of the synthetized 3D video compared with the other methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Luo_A_Hole_Filling_CVPR_2016_paper.pdf",
        "aff": "Commun. Inf. Secur. Lab, Shenzhen Graduate School, Peking University, China; Commun. Inf. Secur. Lab, Shenzhen Graduate School, Peking University, China; Commun. Inf. Secur. Lab, Shenzhen Graduate School, Peking University, China; Faculty of Science and Technology, University of Macau, Macao, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Luo_A_Hole_Filling_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 935505,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=233288301476475838&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff_domain": "sz.pku.edu.cn;pkusz.edu.cn;sz.pku.edu.cn;umac.mo",
        "email": "sz.pku.edu.cn;pkusz.edu.cn;sz.pku.edu.cn;umac.mo",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Luo_A_Hole_Filling_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Peking University;University of Macau",
        "aff_unique_dep": "Commun. Inf. Secur. Lab;Faculty of Science and Technology",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.um.edu.mo",
        "aff_unique_abbr": "Peking U;UM",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Shenzhen Graduate School;Macao",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Holistic Approach to Cross-Channel Image Noise Modeling and Its Application to Image Denoising",
        "session": "Image Processing and Restoration",
        "status": "Spotlight",
        "track": "main",
        "pid": "20",
        "author_site": "Seonghyeon Nam, Youngbae Hwang, Yasuyuki Matsushita, Seon Joo Kim",
        "author": "Seonghyeon Nam; Youngbae Hwang; Yasuyuki Matsushita; Seon Joo Kim",
        "abstract": "Modelling and analyzing noise in images is a fundamental task in many computer vision systems. Traditionally, noise has been modelled per color channel assuming that the color channels are independent. Although the color channels can be considered as mutually independent in camera RAW images, signals from different color channels get mixed during the imaging process inside the camera due to gamut mapping, tone-mapping, and compression. We show the influence of the in-camera imaging pipeline on noise and propose a new noise model in the 3D RGB space to accounts for the color channel mix-ups. A data-driven approach for determining the parameters of the new noise model is introduced as well as its application to image denoising. The experiments show that our noise model represents the noise in regular JPEG images more accurately compared to the previous models and is advantageous in image denoising.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Nam_A_Holistic_Approach_CVPR_2016_paper.pdf",
        "aff": "Yonsei University; KETI; Osaka University; Yonsei University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Nam_A_Holistic_Approach_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5644774,
        "gs_citation": 310,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4334936717689802868&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Nam_A_Holistic_Approach_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Yonsei University;Korea Electronics and Telecommunications Institute;Osaka University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.yonsei.ac.kr;http://www.keti.re.kr;https://www.osaka-u.ac.jp",
        "aff_unique_abbr": "Yonsei;KETI;Osaka U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "South Korea;Japan"
    },
    {
        "title": "A Key Volume Mining Deep Framework for Action Recognition",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "53",
        "author_site": "Wangjiang Zhu, Jie Hu, Gang Sun, Xudong Cao, Yu Qiao",
        "author": "Wangjiang Zhu; Jie Hu; Gang Sun; Xudong Cao; Yu Qiao",
        "abstract": "Recently, deep learning approaches have demonstrated remarkable progresses for action recognition in videos. Most existing deep frameworks equally treat every volume i.e. spatial-temporal video clip, and directly assign a video label to all volumes sampled from it. However, within a video, discriminative actions may occur sparsely in a few key volumes, and most other volumes are irrelevant to the labeled action category. Training with a large proportion of irrelevant volumes will hurt performance. To address this issue, we propose a key volume mining deep framework to identify key volumes and conduct classification simultaneously. Specifically, our framework is trained end-to-end in an EM-like loop. In the forward pass, our network mines key volumes for each action class. In the backward pass, it updates network parameters with the help of these mined key volumes. In addition, we propose \"Stochastic out\" to handle key volumes from multi-modalities, and an effective yet simple \"unsupervised key volume proposal\" method for high quality volume sampling. Our experiments show that action recognition performance can be significantly improved by mining key volumes, and our methods achieve state-of-the-art performance on UCF101 (93.1%).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_A_Key_Volume_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 321,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8542292819823771509&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_A_Key_Volume_CVPR_2016_paper.html"
    },
    {
        "title": "A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "27",
        "author_site": "Nikolaus Mayer, Eddy Ilg, Philip H\u00e4usser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox",
        "author": "Nikolaus Mayer; Eddy Ilg; Philip Hausser; Philipp Fischer; Daniel Cremers; Alexey Dosovitskiy; Thomas Brox",
        "abstract": "Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks.  Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation.  To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks.  Our datasets are the first large-scale datasets to enable training and evaluation of scene flow methods.  Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mayer_A_Large_Dataset_CVPR_2016_paper.pdf",
        "aff": "University of Freiburg+Technical University of Munich; University of Freiburg+Technical University of Munich; Technical University of Munich; University of Freiburg+Technical University of Munich; Technical University of Munich; University of Freiburg; University of Freiburg",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Mayer_A_Large_Dataset_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1880248,
        "gs_citation": 3436,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5567272789746439114&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.tum.edu;cs.uni-freiburg.de;tum.de;cs.uni-freiburg.de;cs.uni-freiburg.de",
        "email": "cs.uni-freiburg.de;cs.uni-freiburg.de;cs.tum.edu;cs.uni-freiburg.de;tum.de;cs.uni-freiburg.de;cs.uni-freiburg.de",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mayer_A_Large_Dataset_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1;1;0+1;1;0;0",
        "aff_unique_norm": "University of Freiburg;Technical University of Munich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-freiburg.de;https://www.tum.de",
        "aff_unique_abbr": "UoF;TUM",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "A Multi-Level Contextual Model For Person Recognition in Photo Albums",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "57",
        "author_site": "Haoxiang Li, Jonathan Brandt, Zhe Lin, Xiaohui Shen, Gang Hua",
        "author": "Haoxiang Li; Jonathan Brandt; Zhe Lin; Xiaohui Shen; Gang Hua",
        "abstract": "In this work, we present a new framework for person recognition in photo albums that exploits contextual cues at multiple levels, spanning individual persons, individual photos, and photo groups.  Through experiments, we show that the information available at each of these distinct contextual levels provides complementary cues as to person identities.  At the person level, we leverage clothing and body appearance in addition to facial appearance, and to compensate for instances where the faces are not visible.  At the photo level we leverage a learned prior on the joint distribution of identities on the same photo to guide the identity assignments. Going beyond a single photo, we are able to infer natural groupings of photos with shared context in an unsupervised manner. By exploiting this shared contextual information, we are able to reduce the identity search space and exploit higher intra-personal appearance consistency within photo groups. Our new framework enables efficient use of these complementary multi-level contextual cues to improve overall recognition rates on the photo album person recognition task, as demonstrated through state-of-the-art results on a challenging public dataset.  Our results outperform competing methods by a significant margin, while being computationally efficient and practical in a real world application.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_A_Multi-Level_Contextual_CVPR_2016_paper.pdf",
        "aff": "Stevens Institute of Technology; Adobe Research; Adobe Research; Adobe Research; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1985271,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4428589751980728016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "stevens.edu;adobe.com;adobe.com;adobe.com;microsoft.com",
        "email": "stevens.edu;adobe.com;adobe.com;adobe.com;microsoft.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_A_Multi-Level_Contextual_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;1;2",
        "aff_unique_norm": "Stevens Institute of Technology;Adobe;Microsoft",
        "aff_unique_dep": ";Adobe Research;Microsoft Research",
        "aff_unique_url": "https://www.stevens.edu;https://research.adobe.com;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "SIT;Adobe;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Multi-Stream Bi-Directional Recurrent Neural Network for Fine-Grained Action Detection",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "50",
        "author_site": "Bharat Singh, Tim K. Marks, Michael Jones, Oncel Tuzel, Ming Shao",
        "author": "Bharat Singh; Tim K. Marks; Michael Jones; Oncel Tuzel; Ming Shao",
        "abstract": "We present a multi-stream bi-directional recurrent neural network for fine-grained action detection. Recently, two-stream convolutional neural networks (CNNs) trained on stacked optical flow and image frames have been successful for action recognition in videos. Our system uses a tracking algorithm to locate a bounding box around the person, which provides a frame of reference for appearance and motion and also suppresses background noise that is not within the bounding box.  We train two additional streams on motion and appearance cropped to the tracked bounding box, along with full-frame streams.  Our motion streams use pixel trajectories of a frame as raw features, in which the displacement values corresponding to a moving scene point are at the same spatial position across several frames.  To model long-term temporal dynamics within and between actions, the multi-stream CNN is followed by a bi-directional Long Short-Term Memory (LSTM) layer.  We show that our bi-directional LSTM network utilizes about 8 seconds of the video sequence to predict an action label. We test on two action detection datasets: the MPII Cooking 2 Dataset, and a new MERL Shopping Dataset that we introduce and make available to the community with this paper.  The results demonstrate that our method significantly outperforms state-of-the-art action detection methods on both datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Singh_A_Multi-Stream_Bi-Directional_CVPR_2016_paper.pdf",
        "aff": "U. of Maryland; Mitsubishi Electric Research Labs (MERL); Mitsubishi Electric Research Labs (MERL); Mitsubishi Electric Research Labs (MERL); Northeastern University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Singh_A_Multi-Stream_Bi-Directional_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 688856,
        "gs_citation": 606,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14725238622371340469&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.umd.edu;merl.com;merl.com;merl.com;ccs.neu.edu",
        "email": "cs.umd.edu;merl.com;merl.com;merl.com;ccs.neu.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Singh_A_Multi-Stream_Bi-Directional_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;1;2",
        "aff_unique_norm": "University of Maryland;Mitsubishi Electric Research Labs;Northeastern University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www/umd.edu;https://www.merl.com;https://www.northeastern.edu",
        "aff_unique_abbr": "UMD;MERL;NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A New Finsler Minimal Path Model With Curvature Penalization for Image Segmentation and Closed Contour Detection",
        "session": "Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "38",
        "author_site": "Da Chen, Jean-Marie Mirebeau, Laurent D. Cohen",
        "author": "Da Chen; Jean-Marie Mirebeau; Laurent D. Cohen",
        "abstract": "In this paper, we propose a new curvature penalized minimal path model for image segmentation via closed contour detection based on the weighted Euler elastica curves, firstly introduced to the field of computer vision in [22]. Our image segmentation method extracts a collection of curvature penalized minimal geodesics, concatenated to form a closed contour, by connecting a set of user-specified points. Globally optimal minimal paths can be computed by solving an Eikonal equation. This first order PDE is traditionally regarded as unable to penalize curvature, which is related to the path acceleration in active contour models. We introduce here a new approach that enables finding a global minimum of the geodesic energy including a curvature term. We achieve this through the use of a novel Finsler metric adding to the image domain the orientation as an extra space dimension. This metric is non-Riemannian and asymmetric, defined on an orientation lifted space, incorporating the curvature penalty in the geodesic energy. Experiments show that the proposed Finsler minimal path model indeed outperforms state-of-the-art minimal path models in both synthetic and real images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_A_New_Finsler_CVPR_2016_paper.pdf",
        "aff": "CEREMADE, CNRS University Paris Dauphine, PSL Research University, UMR 7534, 75016 PARIS, FRANCE; Laboratoire de math \u00b4ematiques d\u2019Orsay, CNRS Universit \u00b4e Paris-Sud, Universit \u00b4e Paris-Saclay, 91405 ORSAY , FRANCE; CEREMADE, CNRS University Paris Dauphine, PSL Research University, UMR 7534, 75016 PARIS, FRANCE",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1974251,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10066031272978471294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "ceremade.dauphine.fr;math.u-psud.fr;ceremade.dauphine.fr",
        "email": "ceremade.dauphine.fr;math.u-psud.fr;ceremade.dauphine.fr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_A_New_Finsler_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University Paris Dauphine;CNRS Universite Paris-Sud",
        "aff_unique_dep": "CEREMADE;Laboratoire de mathematiques d'Orsay",
        "aff_unique_url": "https://www.univ-paris-dauphine.fr;https://www.universite-paris-sud.fr",
        "aff_unique_abbr": "UPD;UPS",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Paris;Orsay",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "A Nonlinear Regression Technique for Manifold Valued Data With Applications to Medical Image Analysis",
        "session": "Statistical Methods and Learning",
        "status": "Poster",
        "track": "main",
        "pid": "68",
        "author_site": "Monami Banerjee, Rudrasis Chakraborty, Edward Ofori, Michael S. Okun, David E. Viallancourt, Baba C. Vemuri",
        "author": "Monami Banerjee; Rudrasis Chakraborty; Edward Ofori; Michael S. Okun; David E. Viallancourt; Baba C. Vemuri",
        "abstract": "Regression is an essential tool in Statistical analysis of data with many applications in Computer Vision, Machine Learning, Medical Imaging and various disciplines of Science and Engineering. Linear and nonlinear regression in a vector space setting has been well studied in literature. However, generalizations to manifold-valued data are only recently gaining popularity. With the exception of a few, most existing methods of regression for manifold valued data are limited to geodesic regression which is a generalization of the linear regression in vector-spaces. In this paper, we present a novel nonlinear kernel-based regression method that is applicable to manifold valued data. Our method is applicable to cases when the independent and dependent variables in the regression model are both manifold-valued or one is manifold-valued and the other is vector or scalar valued. Further, unlike most methods, our method does not require any imposed ordering on the manifold-valued data. The performance of our model is tested on a large number of real data sets acquired from Alzhiemers and movement disorder (Parkinsons and Essential Tremor) patients. We present an extensive set of results along with statistical validation and comparisons.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Banerjee_A_Nonlinear_Regression_CVPR_2016_paper.pdf",
        "aff": "Department of CISE, University of Florida, FL 32611, USA; Department of CISE, University of Florida, FL 32611, USA; Department of Applied Physiology and Kinesiology, University of Florida, FL 32611, USA; Department of Neurology, University of Florida, FL 32611, USA; Department of Applied Physiology and Kinesiology, University of Florida, FL 32611, USA; Department of CISE, University of Florida, FL 32611, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 794340,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6644023660804127444&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cise.ufl.edu;cise.ufl.edu;ufl.edu;neurology.ufl.edu;ufl.edu;cise.ufl.edu",
        "email": "cise.ufl.edu;cise.ufl.edu;ufl.edu;neurology.ufl.edu;ufl.edu;cise.ufl.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Banerjee_A_Nonlinear_Regression_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Florida",
        "aff_unique_dep": "Department of CISE",
        "aff_unique_url": "https://www.ufl.edu",
        "aff_unique_abbr": "UF",
        "aff_campus_unique_index": "0;0;1;0;1;0",
        "aff_campus_unique": "Gainesville;FL",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Paradigm for Building Generalized Models of Human Image Perception Through Data Fusion",
        "session": "Biologically Inspired Vision",
        "status": "Poster",
        "track": "main",
        "pid": "50",
        "author_site": "Shaojing Fan, Tian-Tsong Ng, Bryan L. Koenig, Ming Jiang, Qi Zhao",
        "author": "Shaojing Fan; Tian-Tsong Ng; Bryan L. Koenig; Ming Jiang; Qi Zhao",
        "abstract": "In many sub-fields, researchers collect datasets of human ground truth that are used to create a new algorithm. For example, in research on image perception, datasets have been collected for topics such as what makes an image aesthetic or memorable. Despite high costs for human data collection, datasets are infrequently reused beyond their own fields of interest. Moreover, the algorithms built from them are domain-specific (predict a small set of attributes) and usually unconnected to one another. In this paper, we present a paradigm for building generalized and expandable models of human image perception. First, we fuse multiple fragmented and partially-overlapping datasets through data imputation. We then create a theoretically-structured statistical model of human image perception that is fit to the fused datasets. The resulting model has many advantages. (1) It is generalized, going beyond the content of the constituent datasets, and can be easily expanded by fusing additional datasets. (2) It provides a new ontology usable as a network to expand human data in a cost-effective way. (3) It can guide the design of a generalized computational algorithm for multi-dimensional visual perception. Indeed, experimental results show that a model-based algorithm outperforms state-of-the-art methods on predicting visual sentiment, visual realism and interestingness. Our paradigm can be used in various visual tasks (e.g., video summarization).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Fan_A_Paradigm_for_CVPR_2016_paper.pdf",
        "aff": "National University of Singapore; Institute for Infocomm Research; Washington University in St. Louis; National University of Singapore; National University of Singapore",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Fan_A_Paradigm_for_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1977116,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3346012048537674928&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "nus.edu.sg;i2r.a-star.edu.sg;wustl.edu;nus.edu.sg;nus.edu.sg",
        "email": "nus.edu.sg;i2r.a-star.edu.sg;wustl.edu;nus.edu.sg;nus.edu.sg",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Fan_A_Paradigm_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "National University of Singapore;Institute for Infocomm Research;Washington University in St. Louis",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.i2r.a-star.edu.sg;https://wustl.edu",
        "aff_unique_abbr": "NUS;I2R;WashU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";St. Louis",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "title": "A Probabilistic Collaborative Representation Based Approach for Pattern Classification",
        "session": "Scene and Image Classification",
        "status": "Poster",
        "track": "main",
        "pid": "74",
        "author_site": "Sijia Cai, Lei Zhang, Wangmeng Zuo, Xiangchu Feng",
        "author": "Sijia Cai; Lei Zhang; Wangmeng Zuo; Xiangchu Feng",
        "abstract": "Conventional representation based classifiers, ranging from the classical nearest neighbor classifier and nearest subspace classifier to the recently developed sparse representation based classifier (SRC) and collaborative representation based classifier (CRC), are essentially distance based classifiers. Though SRC and CRC have shown interesting classification results, their intrinsic classification mechanism remains unclear. In this paper we propose a probabilistic collaborative representation framework, where the probability that a test sample belongs to the collaborative subspace of all classes can be well defined and computed. Consequently, we present a probabilistic collaborative representation based classifier (ProCRC), which jointly maximizes the likelihood that a test sample belongs to each of the multiple classes. The final classification is performed by checking which class has the maximum likelihood. The proposed ProCRC has a clear probabilistic interpretation, and it shows superior performance to many popular classifiers, including SRC, CRC and SVM. Coupled with the CNN features, it also leads to state-of-the-art classification results on a variety of challenging visual datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cai_A_Probabilistic_Collaborative_CVPR_2016_paper.pdf",
        "aff": "Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Dept. of Applied Mathematics, Xidian University, Xi\u2019an, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 555544,
        "gs_citation": 337,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9385171952026423452&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "comp.polyu.edu.hk;comp.polyu.edu.hk;gmail.com;mail.xidian.edu.cn",
        "email": "comp.polyu.edu.hk;comp.polyu.edu.hk;gmail.com;mail.xidian.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cai_A_Probabilistic_Collaborative_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Hong Kong Polytechnic University;Harbin Institute of Technology;Xidian University",
        "aff_unique_dep": "Dept. of Computing;School of Computer Science and Technology;Dept. of Applied Mathematics",
        "aff_unique_url": "https://www.polyu.edu.hk;http://www.hit.edu.cn/;http://www.xidian.edu.cn/",
        "aff_unique_abbr": "PolyU;HIT;Xidian",
        "aff_campus_unique_index": "0;0;1;2",
        "aff_campus_unique": "Hong Kong;Harbin;Xi'an",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "A Probabilistic Framework for Color-Based Point Set Registration",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "35",
        "author_site": "Martin Danelljan, Giulia Meneghetti, Fahad Shahbaz Khan, Michael Felsberg",
        "author": "Martin Danelljan; Giulia Meneghetti; Fahad Shahbaz Khan; Michael Felsberg",
        "abstract": "In recent years, sensors capable of measuring both color and depth information have become increasingly popular. Despite the abundance of colored point set data, state-of-the-art probabilistic registration techniques ignore the available color information. In this paper, we propose a probabilistic point set registration framework that exploits available color information associated with the points. Our method is based on a model of the joint distribution of 3D-point observations and their color information. The proposed model captures discriminative color information, while being computationally efficient. We derive an EM algorithm for jointly estimating the model parameters and the relative transformations.  Comprehensive experiments are performed on the Stanford Lounge dataset, captured by an RGB-D camera, and two point sets captured by a Lidar sensor. Our results demonstrate a significant gain in robustness and accuracy when incorporating color information. On the Stanford Lounge dataset, our approach achieves a relative reduction of the failure rate by 78% compared to the baseline. Furthermore, our proposed model outperforms standard strategies for combining color and 3D-point information, leading to state-of-the-art results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Danelljan_A_Probabilistic_Framework_CVPR_2016_paper.pdf",
        "aff": "Computer Vision Laboratory, Department of Electrical Engineering, Link \u00a8oping University, Sweden; Computer Vision Laboratory, Department of Electrical Engineering, Link \u00a8oping University, Sweden; Computer Vision Laboratory, Department of Electrical Engineering, Link \u00a8oping University, Sweden; Computer Vision Laboratory, Department of Electrical Engineering, Link \u00a8oping University, Sweden",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Danelljan_A_Probabilistic_Framework_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1564021,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14088480980952581489&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "liu.se;liu.se;liu.se;liu.se",
        "email": "liu.se;liu.se;liu.se;liu.se",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Danelljan_A_Probabilistic_Framework_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Link\u00f6ping University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.liu.se",
        "aff_unique_abbr": "LiU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "title": "A Robust Multilinear Model Learning Framework for 3D Faces",
        "session": "Face and Gesture",
        "status": "Oral",
        "track": "main",
        "pid": "39",
        "author_site": "Timo Bolkart, Stefanie Wuhrer",
        "author": "Timo Bolkart; Stefanie Wuhrer",
        "abstract": "Multilinear models are widely used to represent the statistical variations of 3D human faces as they decouple shape changes due to identity and expression. Existing methods to learn a multilinear face model degrade if not every person is captured in every expression, if face scans are noisy or partially occluded, if expressions are erroneously labeled, or if the vertex correspondence is inaccurate. These limitations impose requirements on the training data that disqualify large amounts of available 3D face data from being usable to learn a multilinear model. To overcome this, we introduce the first framework to robustly learn a multilinear model from 3D face databases with missing data, corrupt data, wrong semantic correspondence, and inaccurate vertex correspondence. To achieve this robustness to erroneous training data, our framework jointly learns a multilinear model and fixes the data. We evaluate our framework on two publicly available 3D face databases, and show that our framework achieves a data completion accuracy that is comparable to state-of-the-art tensor completion methods. Our method reconstructs corrupt data more accurately than state-of-the-art methods, and improves the quality of the learned model significantly for erroneously labeled expressions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bolkart_A_Robust_Multilinear_CVPR_2016_paper.pdf",
        "aff": "Saarland University, Germany; Inria Grenoble Rh\u00f4ne-Alpes, France",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Bolkart_A_Robust_Multilinear_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2572162,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4651699787206079128&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "mmci.uni-saarland.de;inria.fr",
        "email": "mmci.uni-saarland.de;inria.fr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bolkart_A_Robust_Multilinear_CVPR_2016_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Saarland University;INRIA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-saarland.de;https://www.inria.fr",
        "aff_unique_abbr": "UdS;Inria",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Grenoble Rh\u00f4ne-Alpes",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;France"
    },
    {
        "title": "A Task-Oriented Approach for Cost-Sensitive Recognition",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "76",
        "author_site": "Roozbeh Mottaghi, Hannaneh Hajishirzi, Ali Farhadi",
        "author": "Roozbeh Mottaghi; Hannaneh Hajishirzi; Ali Farhadi",
        "abstract": "With the recent progress in visual recognition, we have already started to see a surge of vision related real-world applications. These applications, unlike general scene understanding, are task oriented and require specific information from visual data. Considering the current growth in new sensory devices, feature designs, feature learning methods, and algorithms, the search in the space of features and models becomes combinatorial. In this paper, we propose a novel cost-sensitive task-oriented recognition method that is based on a combination of linguistic semantics and visual cues. Our task-oriented framework is able to generalize to unseen tasks for which there is no training data and outperforms state-of-the-art cost-based recognition baselines on our new task-based dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mottaghi_A_Task-Oriented_Approach_CVPR_2016_paper.pdf",
        "aff": "Allen Institute for Artificial Intelligence; University of Washington; Allen Institute for Artificial Intelligence + University of Washington",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Mottaghi_A_Task-Oriented_Approach_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 813980,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17517969928756167053&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mottaghi_A_Task-Oriented_Approach_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "Allen Institute for Artificial Intelligence;University of Washington",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://allenai.org;https://www.washington.edu",
        "aff_unique_abbr": "AI2;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Text Detection System for Natural Scenes With Convolutional Feature Learning and Cascaded Classification",
        "session": "Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "67",
        "author_site": "Siyu Zhu, Richard Zanibbi",
        "author": "Siyu Zhu; Richard Zanibbi",
        "abstract": "We propose a system that finds text in natural scenes using a variety of cues. Our novel data-driven method incorporates coarse-to-fine detection of character pixels using convolutional features (Text-Conv), followed by extracting connected components (CCs) from characters using edge and color features, and finally performing a graph-based segmentation of CCs into words (Word-Graph). For Text-Conv, the initial detection is based on convolutional feature maps similar to those used in Convolutional Neural Networks (CNNs), but learned using Convolutional k-means. Convolution masks defined by local and neighboring patch features are used to improve detection accuracy. The Word-Graph algorithm uses contextual information to both improve word segmentation and prune false character/word detections. Different definitions for foreground (text) regions are used to train the detection stages, some based on bounding box intersection, and others on bounding box and pixel intersection. Our system obtains pixel, character, and word detection f-measures of 93.14%, 90.26%, and 86.77% respectively for the ICDAR 2015 Robust Reading Focused Scene Text dataset, out-performing state-of-the-art systems. This approach may work for other detection targets with homogenous color in natural scenes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_A_Text_Detection_CVPR_2016_paper.pdf",
        "aff": "Center for Imaging Science, Rochester Institute of Technology, NY, USA; Department of Computer Science, Rochester Institute of Technology, NY, USA",
        "project": "https://www.cs.rit.edu/~dprl/Software.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1149525,
        "gs_citation": 88,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1150953252539144219&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "hotmail.com;cs.rit.edu",
        "email": "hotmail.com;cs.rit.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_A_Text_Detection_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Rochester Institute of Technology",
        "aff_unique_dep": "Center for Imaging Science",
        "aff_unique_url": "https://www.rit.edu",
        "aff_unique_abbr": "RIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Rochester",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "A Weighted Variational Model for Simultaneous Reflectance and Illumination Estimation",
        "session": "Image Enhancement, Restoration, and Texture",
        "status": "Poster",
        "track": "main",
        "pid": "56",
        "author_site": "Xueyang Fu, Delu Zeng, Yue Huang, Xiao-Ping Zhang, Xinghao Ding",
        "author": "Xueyang Fu; Delu Zeng; Yue Huang; Xiao-Ping Zhang; Xinghao Ding",
        "abstract": "We propose a weighted variational model to estimate both the reflectance and the illumination from an observed image. We show that, though it is widely adopted for ease of modeling, the log-transformed image for this task is not ideal. Based on the previous investigation of the logarithmic transformation, a new weighted variational model is proposed for better prior representation, which is imposed in the regularization terms. Different from conventional variational models, the proposed model can preserve the estimated reflectance with more details. Moreover, the proposed model can suppress noise to some extent. An alternating minimization scheme is adopted to solve the proposed model. Experimental results demonstrate the effectiveness of the proposed model with its algorithm. Compared with other variational methods, the proposed method yields comparable or better results on both subjective and objective assessments.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Fu_A_Weighted_Variational_CVPR_2016_paper.pdf",
        "aff": "Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, China+School of Information Science and Engineering, Xiamen University, China; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, China+School of Information Science and Engineering, Xiamen University, China; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, China+School of Information Science and Engineering, Xiamen University, China; Department of Electrical and Computer Engineering, Ryerson University, Canada+School of Information Science and Engineering, Xiamen University, China; Fujian Key Laboratory of Sensing and Computing for Smart City, Xiamen University, China+School of Information Science and Engineering, Xiamen University, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3826184,
        "gs_citation": 1183,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7888642614113856559&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "stu.xmu.edu.cn;xmu.edu.cn;xmu.edu.cn;ee.ryerson.ca;xmu.edu.cn",
        "email": "stu.xmu.edu.cn;xmu.edu.cn;xmu.edu.cn;ee.ryerson.ca;xmu.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Fu_A_Weighted_Variational_CVPR_2016_paper.html",
        "aff_unique_index": "0+0;0+0;0+0;1+0;0+0",
        "aff_unique_norm": "Xiamen University;Ryerson University",
        "aff_unique_dep": "Fujian Key Laboratory of Sensing and Computing for Smart City;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.xmu.edu.cn;https://www.ryerson.ca",
        "aff_unique_abbr": ";Ryerson",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;1+0;0+0",
        "aff_country_unique": "China;Canada"
    },
    {
        "title": "ASP Vision: Optically Computing the First Layer of Convolutional Neural Networks Using Angle Sensitive Pixels",
        "session": "Vision With Alternative Sensors",
        "status": "Oral",
        "track": "main",
        "pid": "16",
        "author_site": "Huaijin G. Chen, Suren Jayasuriya, Jiyue Yang, Judy Stephen, Sriram Sivaramakrishnan, Ashok Veeraraghavan, Alyosha Molnar",
        "author": "Huaijin G. Chen; Suren Jayasuriya; Jiyue Yang; Judy Stephen; Sriram Sivaramakrishnan; Ashok Veeraraghavan; Alyosha Molnar",
        "abstract": "Deep learning using convolutional neural networks (CNNs) is quickly becoming the state-of-the-art for challenging computer vision applications. However, deep learning's power consumption and bandwidth requirements currently limit its application in embedded and mobile systems with tight energy budgets. In this paper, we explore the energy savings of optically computing the first layer of CNNs. To do so, we utilize bio-inspired Angle Sensitive Pixels (ASPs), custom CMOS diffractive image sensors which act similar to Gabor filter banks in the V1 layer of the human visual cortex. ASPs replace both image sensing and the first layer of a conventional CNN by directly performing optical edge filtering, saving sensing energy, data bandwidth, and CNN FLOPS to compute. Our experimental results (both on synthetic data and a hardware prototype) for a variety of vision tasks such as digit recognition, object recognition, and face identification demonstrate 97% reduction in image sensor power consumption and 90% reduction in data bandwidth from sensor to CPU, while achieving similar performance compared to traditional deep learning pipelines.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_ASP_Vision_Optically_CVPR_2016_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14342182068155810503&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_ASP_Vision_Optically_CVPR_2016_paper.html"
    },
    {
        "title": "Accelerated Generative Models for 3D Point Cloud Data",
        "session": "3D Reconstruction",
        "status": "Spotlight",
        "track": "main",
        "pid": "22",
        "author_site": "Benjamin Eckart, Kihwan Kim, Alejandro Troccoli, Alonzo Kelly, Jan Kautz",
        "author": "Benjamin Eckart; Kihwan Kim; Alejandro Troccoli; Alonzo Kelly; Jan Kautz",
        "abstract": "Finding meaningful, structured representations of 3D point cloud data (PCD) has become a core task for spatial perception applications.    In this paper we introduce a method for constructing compact generative representations of PCD at multiple levels of detail.   As opposed to deterministic structures such as voxel grids or octrees, we propose probabilistic subdivisions of the data through local mixture modeling, and show how these subdivisions can provide a maximum likelihood segmentation of the data. The final representation is hierarchical, compact, parametric, and statistically derived, facilitating run-time occupancy calculations through stochastic sampling. Unlike traditional deterministic spatial subdivision methods, our technique enables dynamic creation of voxel grids according the application's best needs.   In contrast to other generative models for PCD, we explicitly enforce sparsity among points and mixtures, a technique which we call expectation sparsification. This leads to a highly parallel hierarchical Expectation Maximization (EM) algorithm well-suited for the GPU and real-time execution. We explore the trade-offs between model fidelity and model size at various levels of detail, our tests showing favorable performance when compared to octree and NDT-based methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Eckart_Accelerated_Generative_Models_CVPR_2016_paper.pdf",
        "aff": "The Robotics Institute, Carnegie Mellon University+NVIDIA Research; NVIDIA Research; NVIDIA Research; The Robotics Institute, Carnegie Mellon University; NVIDIA Research",
        "project": "https://research.nvidia.com/publication/accelerated-generative-models",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Eckart_Accelerated_Generative_Models_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1661702,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3843861668513929787&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Eckart_Accelerated_Generative_Models_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;1;0;1",
        "aff_unique_norm": "Carnegie Mellon University;NVIDIA",
        "aff_unique_dep": "The Robotics Institute;NVIDIA Research",
        "aff_unique_url": "https://www.cmu.edu;https://www.nvidia.com/research",
        "aff_unique_abbr": "CMU;NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Accumulated Stability Voting: A Robust Descriptor From Descriptors of Multiple Scales",
        "session": "Feature Extraction and Matching",
        "status": "Poster",
        "track": "main",
        "pid": "35",
        "author_site": "Tsun-Yi Yang, Yen-Yu Lin, Yung-Yu Chuang",
        "author": "Tsun-Yi Yang; Yen-Yu Lin; Yung-Yu Chuang",
        "abstract": "This paper proposes a novel local descriptor through accumulated stability voting (ASV). The stability of feature dimensions is measured by their differences across scales. To be more robust to noise, the stability is further quantized by thresholding. The principle of maximum entropy is utilized for determining the best thresholds for maximizing discriminant power of the resultant descriptor. Accumulating stability renders a real-valued descriptor and it can be converted into a binary descriptor by an additional thresholding process. The real-valued descriptor attains high matching accuracy while the binary descriptor makes a good compromise between storage and accuracy. Our descriptors are simple yet effective, and easy to implement. In addition, our descriptors require no training. Experiments on popular benchmarks demonstrate the effectiveness of our descriptors and their superiority to the state-of-the-art descriptors.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_Accumulated_Stability_Voting_CVPR_2016_paper.pdf",
        "aff": "Academia Sinica, Taiwan+National Taiwan University, Taiwan; Academia Sinica, Taiwan; National Taiwan University, Taiwan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1861887,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16089775803550584751&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "citi.sinica.edu.tw;citi.sinica.edu.tw;csie.ntu.edu.tw",
        "email": "citi.sinica.edu.tw;citi.sinica.edu.tw;csie.ntu.edu.tw",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_Accumulated_Stability_Voting_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "Academia Sinica;National Taiwan University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sinica.edu.tw;https://www.ntu.edu.tw",
        "aff_unique_abbr": "Academia Sinica;NTU",
        "aff_campus_unique_index": "0+0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Accurate Image Super-Resolution Using Very Deep Convolutional Networks",
        "session": "Image Processing and Restoration",
        "status": "Oral",
        "track": "main",
        "pid": "16",
        "author_site": "Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee",
        "author": "Jiwon Kim; Jung Kwon Lee; Kyoung Mu Lee",
        "abstract": "We present a highly accurate single image superresolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification [19]. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.pdf",
        "aff": "Department of ECE, ASRI, Seoul National University, Korea; Department of ECE, ASRI, Seoul National University, Korea; Department of ECE, ASRI, Seoul National University, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1392268,
        "gs_citation": 8887,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8124948202418109550&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Action Recognition in Video Using Sparse Coding and Relative Features",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "46",
        "author_site": "Anal\u00ed Alfaro, Domingo Mery, Alvaro Soto",
        "author": "Anali Alfaro; Domingo Mery; Alvaro Soto",
        "abstract": "This work presents an approach to category-based action recognition in video using sparse coding techniques. The proposed approach includes two main contributions: i) A new method to handle intra-class variations by decomposing each video into a reduced set of representative atomic action acts or key-sequences, and ii) A new video descriptor, ITRA: Inter-Temporal Relational Act Descriptor, that exploits the power of comparative reasoning to capture relative similarity relations among key-sequences. In terms of the method to obtain key-sequences, we introduce a loss function that, for each video, leads to the identification of a sparse set of representative key-frames capturing both, relevant particularities arising in the input video, as well as relevant generalities arising in the complete class collection. In terms of the method to obtain the ITRA descriptor, we introduce a novel scheme to quantify relative intra and inter-class similarities among local temporal patterns arising in the videos. The resulting ITRA descriptor demonstrates to be highly effective to discriminate among action categories. As a result, the proposed approach reaches remarkable action recognition performance on several popular benchmark datasets, outperforming alternative state-of-the-art techniques by a  large margin.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Alfaro_Action_Recognition_in_CVPR_2016_paper.pdf",
        "aff": "P. Universidad Catolica de Chile; P. Universidad Catolica de Chile; P. Universidad Catolica de Chile",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1019618,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1345053409561182076&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "uc.cl;ing.puc.cl;ing.uc.cl",
        "email": "uc.cl;ing.puc.cl;ing.uc.cl",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Alfaro_Action_Recognition_in_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Pontificia Universidad Cat\u00f3lica de Chile",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.puc.cl",
        "aff_unique_abbr": "PUC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Chile"
    },
    {
        "title": "Actionness Estimation Using Hybrid Fully Convolutional Networks",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "48",
        "author_site": "Limin Wang, Yu Qiao, Xiaoou Tang, Luc Van Gool",
        "author": "Limin Wang; Yu Qiao; Xiaoou Tang; Luc Van Gool",
        "abstract": "Actionness was introduced to quantify the likelihood of containing a generic action instance at a specific location. Accurate and efficient estimation of actionness is important in video analysis and may benefit other relevant tasks such as action recognition and action detection. This paper presents a new deep architecture for actionness estimation, called hybrid fully convolutional network (H-FCN), which is composed of appearance FCN (A-FCN) and motion FCN (M-FCN). These two FCNs leverage the strong capacity of deep models to estimate actionness maps from the perspectives of static appearance and dynamic motion, respectively. In addition, the fully convolutional nature of H-FCN allows it to efficiently process videos with arbitrary sizes. Experiments are conducted on the challenging datasets of Stanford40, UCF Sports, and JHMDB to verify the effectiveness of H-FCN on actionness estimation, which demonstrate that our method achieves superior performance to previous ones. Moreover, we apply the estimated actionness maps on action proposal generation and action detection. Our actionness maps advance the current state-of-the-art performance of these tasks substantially.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Actionness_Estimation_Using_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1154307,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=543285042061997062&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Actionness_Estimation_Using_CVPR_2016_paper.html"
    },
    {
        "title": "Actions ~ Transformations",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "43",
        "author_site": "Xiaolong Wang, Ali Farhadi, Abhinav Gupta",
        "author": "Xiaolong Wang; Ali Farhadi; Abhinav Gupta",
        "abstract": "What defines an action like \"kicking ball\"? We argue that the true meaning of an action lies in the change or transformation an action brings to the environment. In this paper, we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens (precondition) to the state after the action (effect). Motivated by recent advancements of video representation using deep learning, we design a Siamese network which models the action as a transformation on a high-level feature space. We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51. More importantly, our approach is able to  generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Actions__Transformations_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 347,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=432224764712457055&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Actions__Transformations_CVPR_2016_paper.html"
    },
    {
        "title": "Active Image Segmentation Propagation",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "65",
        "author_site": "Suyog Dutt Jain, Kristen Grauman",
        "author": "Suyog Dutt Jain; Kristen Grauman",
        "abstract": "We propose a semi-automatic method to obtain foreground object masks for a large set of related images.  We develop a stagewise active approach to propagation: in each stage, we actively determine the images that appear most valuable for human annotation, then revise the foreground estimates in all unlabeled images accordingly.  In order to identify images that, once annotated, will propagate well to other examples, we introduce an active selection procedure that operates on the joint segmentation graph over all images.  It prioritizes human intervention for those images that are uncertain and influential in the graph, while also mutually diverse.  We apply our method to obtain foreground masks for over 1 million images.  Our method yields state-of-the-art accuracy on the ImageNet and MIT Object Discovery datasets, and it focuses human attention more effectively than existing propagation strategies.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Jain_Active_Image_Segmentation_CVPR_2016_paper.pdf",
        "aff": "University of Texas at Austin; University of Texas at Austin",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 894939,
        "gs_citation": 293,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5803700921650871808&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Jain_Active_Image_Segmentation_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Active Learning for Delineation of Curvilinear Structures",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "73",
        "author_site": "Agata Mosinska-Domanska, Raphael Sznitman, Przemyslaw Glowacki, Pascal Fua",
        "author": "Agata Mosinska-Domanska; Raphael Sznitman; Przemyslaw Glowacki; Pascal Fua",
        "abstract": "Many recent delineation techniques owe  much of their increased effectiveness to path classification  algorithms that  make it possible to distinguish  promising paths from others. The downside of this  development is that they  require annotated training data, which is tedious to produce.  In this paper,  we propose an Active Learning approach  that considerably speeds up  the annotation  process. Unlike  standard ones,  it takes  advantage of  the specificities of the delineation problem. It  operates on a graph and can reduce the training  set size  by up  to 80%  without compromising  the reconstruction quality.  We  will  show  that  our  approach outperforms  conventional  ones  on  various biomedical  and  natural  image  datasets,  thus  showing  that  it  is  broadly applicable.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mosinska-Domanska_Active_Learning_for_CVPR_2016_paper.pdf",
        "aff": "EPFL; University of Bern; EPFL; EPFL",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4213395,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7368720735131537092&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "epfl.ch;artorg.unibe.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;artorg.unibe.ch;epfl.ch;epfl.ch",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mosinska-Domanska_Active_Learning_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "EPFL;University of Bern",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.epfl.ch;https://www.unibe.ch",
        "aff_unique_abbr": "EPFL;UniBE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Actor-Action Semantic Segmentation With Grouping Process Models",
        "session": "Activity Recognition",
        "status": "Spotlight",
        "track": "main",
        "pid": "6",
        "author_site": "Chenliang Xu, Jason J. Corso",
        "author": "Chenliang Xu; Jason J. Corso",
        "abstract": "Actor-action semantic segmentation made an important step toward advanced video understanding: what action is happening; who is performing the action; and where is the action happening in space-time. Current methods based on layered CRFs for this problem are local and unable to capture the long-ranging interactions of video parts. We propose a new model that combines the labeling CRF with a supervoxel hierarchy, where supervoxels at various scales provide cues for possible groupings of nodes in the CRF to encourage adaptive and long-ranging interactions. The new model defines a dynamic and continuous process of information exchange: the CRF influences what supervoxels in the hierarchy are active, and these active supervoxels, in turn, affect the connectivities in the CRF; we hence call it a grouping process model. By further incorporating the video-level recognition, the proposed method achieves a large margin of 60% relative improvement over the state of the art on the recent A2D large-scale video labeling dataset, which demonstrates the effectiveness of our modeling.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_Actor-Action_Semantic_Segmentation_CVPR_2016_paper.pdf",
        "aff": "Electrical Engineering and Computer Science, University of Michigan, Ann Arbor; Electrical Engineering and Computer Science, University of Michigan, Ann Arbor",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Xu_Actor-Action_Semantic_Segmentation_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1351245,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11687823662229568170&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xu_Actor-Action_Semantic_Segmentation_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Department of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adaptive 3D Face Reconstruction From Unconstrained Photo Collections",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "44",
        "author_site": "Joseph Roth, Yiying Tong, Xiaoming Liu",
        "author": "Joseph Roth; Yiying Tong; Xiaoming Liu",
        "abstract": "Given a collection of \"in-the-wild\" face images captured under a variety of unknown pose, expression, and illumination conditions, this paper presents a method for reconstructing a 3D face surface model of an individual along with albedo information. Motivated by the success of recent face reconstruction techniques on large photo collections, we extend prior work to adapt to low quality photo collections with fewer images. We achieve this by fitting a 3D Morphable Model to form a personalized template and developing a novel photometric stereo formulation, under a coarse-to-fine scheme. Superior experimental results are reported on synthetic and real-world photo collections.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Roth_Adaptive_3D_Face_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science and Engineering, Michigan State University; Department of Computer Science and Engineering, Michigan State University; Department of Computer Science and Engineering, Michigan State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2748278,
        "gs_citation": 227,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1946135532139990472&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "msu.edu;msu.edu;msu.edu",
        "email": "msu.edu;msu.edu;msu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Roth_Adaptive_3D_Face_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Adaptive Decontamination of the Training Set: A Unified Formulation for Discriminative Visual Tracking",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "71",
        "author_site": "Martin Danelljan, Gustav H\u00e4ger, Fahad Shahbaz Khan, Michael Felsberg",
        "author": "Martin Danelljan; Gustav Hager; Fahad Shahbaz Khan; Michael Felsberg",
        "abstract": "Tracking-by-detection methods have demonstrated competitive performance in recent years. In these approaches, the tracking model heavily relies on the quality of the training set. Due to the limited amount of labeled training data, additional samples need to be extracted and labeled by the tracker itself. This often leads to the inclusion of corrupted training samples, due to occlusions, misalignments and other perturbations. Existing tracking-by-detection methods either ignore this problem, or employ a separate component for managing the training set.   We propose a novel generic approach for alleviating the problem of corrupted training samples in tracking-by-detection frameworks. Our approach dynamically manages the training set by estimating the quality of the samples. Contrary to existing approaches, we propose a unified formulation by minimizing a single loss over both the target appearance model and the sample quality weights. The joint formulation enables corrupted samples to be down-weighted while increasing the impact of correct ones. Experiments are performed on three benchmarks: OTB-2015 with 100 videos, VOT-2015 with 60 videos, and Temple-Color with 128 videos. On the OTB-2015, our unified formulation significantly improves the baseline, with a gain of 3.8% in mean overlap precision. Finally, our method achieves state-of-the-art results on all three datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Danelljan_Adaptive_Decontamination_of_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Danelljan_Adaptive_Decontamination_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1473834,
        "gs_citation": 515,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9805574301806712807&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Danelljan_Adaptive_Decontamination_of_CVPR_2016_paper.html"
    },
    {
        "title": "Adaptive Object Detection Using Adjacency and Zoom Prediction",
        "session": "Object Detection 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "10",
        "author_site": "Yongxi Lu, Tara Javidi, Svetlana Lazebnik",
        "author": "Yongxi Lu; Tara Javidi; Svetlana Lazebnik",
        "abstract": "State-of-the-art object detection systems rely on an accurate set of region proposals. Several recent methods use a neural network architecture to hypothesize promising object locations. While these approaches are computationally efficient, they rely on fixed image regions as anchors for predictions. In this paper we propose to use a search strategy that adaptively directs computational resources to sub-regions likely to contain objects. Compared to methods based on fixed anchor locations, our approach naturally adapts to cases where object instances are sparse and small. Our approach is comparable in terms of accuracy to the state-of-the-art Faster R-CNN approach while using two orders of magnitude fewer anchors on average. Code is publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lu_Adaptive_Object_Detection_CVPR_2016_paper.pdf",
        "aff": "University of California, San Diego; University of California, San Diego; University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1242588,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5982959005768977476&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ucsd.edu;ucsd.edu;illinois.edu",
        "email": "ucsd.edu;ucsd.edu;illinois.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lu_Adaptive_Object_Detection_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, San Diego;University of Illinois Urbana-Champaign",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsd.edu;https://illinois.edu",
        "aff_unique_abbr": "UCSD;UIUC",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "San Diego;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Affinity CNN: Learning Pixel-Centric Pairwise Relations for Figure/Ground Embedding",
        "session": "Segmentation and Contour Detection",
        "status": "Spotlight",
        "track": "main",
        "pid": "19",
        "author_site": "Michael Maire, Takuya Narihira, Stella X. Yu",
        "author": "Michael Maire; Takuya Narihira; Stella X. Yu",
        "abstract": "Spectral embedding provides a framework for solving perceptual organization problems, including image segmentation and figure/ground organization.  From an affinity matrix describing pairwise relationships between pixels, it clusters pixels into regions, and, using a complex-valued extension, orders pixels according to layer.  We train a convolutional neural network (CNN) to directly predict the pairwise relationships that define this affinity matrix.  Spectral embedding then resolves these predictions into a globally-consistent segmentation and figure/ground organization of the scene.  Experiments demonstrate significant benefit to this direct coupling compared to prior works which use explicit intermediate stages, such as edge detection, on the pathway from image to affinities.  Our results suggest spectral embedding as a powerful alternative to the conditional random field (CRF)-based globalization schemes typically coupled to deep neural networks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Maire_Affinity_CNN_Learning_CVPR_2016_paper.pdf",
        "aff": "TTI Chicago; Sony Corp.; UC Berkeley / ICSI",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Maire_Affinity_CNN_Learning_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5249526,
        "gs_citation": 76,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13922530338235144222&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": "ttic.edu;jp.sony.com;berkeley.edu",
        "email": "ttic.edu;jp.sony.com;berkeley.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Maire_Affinity_CNN_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;Sony Corporation;University of California, Berkeley",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.sony.com;https://www.berkeley.edu",
        "aff_unique_abbr": "TTI;Sony;UC Berkeley",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Chicago;;Berkeley",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "title": "Aggregating Image and Text Quantized Correlated Components",
        "session": "Image Indexing and Retrieval",
        "status": "Poster",
        "track": "main",
        "pid": "59",
        "author_site": "Thi Quynh Nhi Tran, Herv\u00e9 Le Borgne, Michel Crucianu",
        "author": "Thi Quynh Nhi Tran; Herve Le Borgne; Michel Crucianu",
        "abstract": "Cross-modal tasks occur naturally for multimedia content that can be described along two or more modalities like visual content and text. Such tasks require to \"translate\" information from one modality to another. Methods like kernelized canonical correlation analysis (KCCA) attempt to solve such tasks by finding aligned subspaces in the description spaces of different modalities. Since they favor correlations against modality-specific information, these methods have shown some success in both cross-modal and bi-modal tasks. However, we show that a direct use of the subspace alignment obtained by KCCA only leads to coarse translation abilities. To address this problem, we first put forward here a new representation method that aggregates information provided by the projections of both modalities on their aligned subspaces. We further suggest a method relying on neighborhoods in these subspaces to complete uni-modal information. Our proposal exhibits state-of-the-art results for bi-modal classification on Pascal VOC07 and for cross-modal retrieval on FlickR 8K and FlickR 30K.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tran_Aggregating_Image_and_CVPR_2016_paper.pdf",
        "aff": "CEA, LIST and CEDRIC-CNAM; CEA, LIST; CEDRIC-CNAM",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Tran_Aggregating_Image_and_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 594245,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15925855985894852537&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cea.fr;cea.fr;cnam.fr",
        "email": "cea.fr;cea.fr;cnam.fr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tran_Aggregating_Image_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "CEA;CNAM - Conservatoire National des Arts et M\u00e9tiers",
        "aff_unique_dep": "LIST;CEDRIC",
        "aff_unique_url": "https://www.cea.fr;https://www.cnam.fr",
        "aff_unique_abbr": "CEA;CNAM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Ambiguity Helps: Classification With Disagreements in Crowdsourced Annotations",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "75",
        "author_site": "Viktoriia Sharmanska, Daniel Hern\u00e1ndez-Lobato, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Novi Quadrianto",
        "author": "Viktoriia Sharmanska; Daniel Hernandez-Lobato; Jose Miguel Hernandez-Lobato; Novi Quadrianto",
        "abstract": "Imagine we show an image to a person and ask her/him to decide whether the scene in the image is warm or not warm, and whether it is easy or not to spot a squirrel in the image. For exactly the same image, the answers to those questions are likely to differ from person to person. This is because the task is inherently ambiguous. Such an ambiguous, therefore challenging, task is pushing the boundary of computer vision in showing what can and can not be learned from visual data. Crowdsourcing has been invaluable for collecting annotations. This is particularly so for a task that goes beyond a clear-cut dichotomy as multiple human judgments per image are needed to reach a consensus. This paper makes conceptual and technical contributions. On the conceptual side, we define disagreements among annotators as privileged information about the data instance. On the technical side, we propose a framework to incorporate annotation disagreements into the classifiers. The proposed framework is simple, relatively fast, and outperforms classifiers that do not take into account the disagreements, especially if tested on high confidence annotations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sharmanska_Ambiguity_Helps_Classification_CVPR_2016_paper.pdf",
        "aff": "SMiLe CLiNiC, University of Sussex, Brighton, UK; Universidad Aut\u00f3noma de Madrid, Madrid, Spain; Harvard University, Cambridge, Massachusetts, US; SMiLe CLiNiC, University of Sussex, Brighton, UK",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Sharmanska_Ambiguity_Helps_Classification_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1662973,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13149686940735853387&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sharmanska_Ambiguity_Helps_Classification_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Sussex;Universidad Aut\u00f3noma de Madrid;Harvard University",
        "aff_unique_dep": "SMiLe CLiNiC;;",
        "aff_unique_url": "https://www.sussex.ac.uk;https://www.uam.es;https://www.harvard.edu",
        "aff_unique_abbr": ";UAM;Harvard",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Brighton;Madrid;Cambridge",
        "aff_country_unique_index": "0;1;2;0",
        "aff_country_unique": "United Kingdom;Spain;United States"
    },
    {
        "title": "Amplitude Modulated Video Camera - Light Separation in Dynamic Scenes",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "72",
        "author_site": "Amir Kolaman, Maxim Lvov, Rami Hagege, Hugo Guterman",
        "author": "Amir Kolaman; Maxim Lvov; Rami Hagege; Hugo Guterman",
        "abstract": "Controlled light conditions improve considerably the performance of most computer vision algorithms. Dynamic light conditions create varying spatial changes in color and intensity across the scene. These condition, caused by a moving shadow for example, force developers to create algorithms which are robust to such variations. We suggest a computational camera which produces images that are not influenced by environmental variations in light conditions. The key insight is that many years ago, similar difficulties were already solved in radio communication; As a result each channel is immune to interference from other radio channels. Amplitude Modulated (AM) video camera  separates the influence of a modulated light from other unknown light sources in the scene; Causing the AM video camera frame to appear the same - independent of the light  conditions in which it was taken. We built a prototype of the AM video camera by using off the shelf hardware and tested it. AM video camera was used to demonstrate color constancy, shadow removal and contrast enhancement in real time. We show theoretically and empirically that: 1. the proposed system can produce images with similar noise levels as a standard camera. 2. The images created by such camera are almost completely immune to temporal, spatial and spectral changes in the background light.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kolaman_Amplitude_Modulated_Video_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3534687,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6920815170293712864&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kolaman_Amplitude_Modulated_Video_CVPR_2016_paper.html"
    },
    {
        "title": "An Efficient Exact-PGA Algorithm for Constant Curvature Manifolds",
        "session": "Statistical Methods and Transfer Learning",
        "status": "Spotlight",
        "track": "main",
        "pid": "20",
        "author_site": "Rudrasis Chakraborty, Dohyung Seo, Baba C. Vemuri",
        "author": "Rudrasis Chakraborty; Dohyung Seo; Baba C. Vemuri",
        "abstract": "Manifold-valued datasets are widely encountered in many computer vision tasks. A non-linear analog of the PCA algorithm, called the Principal Geodesic Analysis (PGA) algorithm suited for data lying on Riemannian manifolds was reported in literature a decade ago. Since the objective function in the PGA algorithm is highly non-linear and hard to solve efficiently in general, researchers have proposed a linear approximation. Though this linear approximation is easy to compute, it lacks accuracy especially when the data exhibits a large variance. Recently, an alternative called the exact PGA was proposed which tries to solve the optimization without any linearization. For general Riemannian manifolds, though it yields a better accuracy than the original (linearized) PGA, for data that exhibit large variance, the optimization is not computationally efficient. In this paper, we propose an efficient exact PGA algorithm for constant curvature Riemannian manifolds (CCM-EPGA). The CCM-EPGA algorithm differs significantly from existing PGA algorithms in two aspects, (i) the distance between a given manifold-valued data point and the principal submanifold is computed analytically and thus no optimization is required as in the existing methods. (ii) Unlike the existing PGA algorithms, the descent into codimension-1 submanifolds does not require any optimization but is accomplished through the use of the Rimeannian inverse Exponential map and the parallel transport operations. We present theoretical and experimental results for constant curvature Riemannian manifolds depicting favorable performance of the CCM-EPGA algorithm compared to existing PGA algorithms. We also present data reconstruction from the principal components which has not been reported in literature in this setting.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chakraborty_An_Efficient_Exact-PGA_CVPR_2016_paper.pdf",
        "aff": "Department of CISE, University of Florida, FL 32611, USA; U-Systems, A GE Healthcare Company, CA, USA; Department of CISE, University of Florida, FL 32611, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 605412,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7160445436653590820&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cise.ufl.edu;cise.ufl.edu;gmail.com",
        "email": "cise.ufl.edu;cise.ufl.edu;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chakraborty_An_Efficient_Exact-PGA_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Florida;U-Systems",
        "aff_unique_dep": "Department of CISE;",
        "aff_unique_url": "https://www.ufl.edu;",
        "aff_unique_abbr": "UF;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Gainesville;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "An Egocentric Look at Video Photographer Identity",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "53",
        "author_site": "Yedid Hoshen, Shmuel Peleg",
        "author": "Yedid Hoshen; Shmuel Peleg",
        "abstract": "Egocentric cameras are being worn by an increasing number of users, among them many security forces worldwide. GoPro cameras already penetrated the mass market, reporting substantial increase in sales every year. As head-worn cameras do not capture the photographer, it may seem that the anonymity of the photographer is preserved even when the video is publicly distributed.   We show that camera motion, as can be computed from the egocentric video, provides unique identity information. The photographer can be reliably recognized from a few seconds of video captured when walking. The proposed method achieves more than 90% recognition accuracy in cases where the random success rate is only 3%.  Applications can include theft prevention by locking the camera when not worn by its lawful owner. Searching video sharing services (e.g. YouTube) for egocentric videos shot by a specific photographer may also become possible. An important message in this paper is that photographers should be aware that sharing egocentric video will compromise their anonymity, even when their face is not visible.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hoshen_An_Egocentric_Look_CVPR_2016_paper.pdf",
        "aff": "The Hebrew University of Jerusalem; The Hebrew University of Jerusalem",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1031924,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17856608017875018501&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hoshen_An_Egocentric_Look_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "An Empirical Evaluation of Current Convolutional Architectures' Ability to Manage Nuisance Location and Scale Variability",
        "session": "Statistical Methods and Learning",
        "status": "Poster",
        "track": "main",
        "pid": "70",
        "author_site": "Nikolaos Karianakis, Jingming Dong, Stefano Soatto",
        "author": "Nikolaos Karianakis; Jingming Dong; Stefano Soatto",
        "abstract": "We conduct an empirical study to test the ability of convolutional neural networks (CNNs) to reduce the effects of nuisance transformations of the input data, such as location, scale and aspect ratio. We isolate factors by adopting a common convolutional architecture either deployed globally on the image to compute class posterior distributions, or restricted locally to compute class conditional distributions given location, scale and aspect ratios of bounding boxes determined by proposal heuristics. In theory, averaging the latter should yield inferior performance compared to proper marginalization. Yet empirical evidence suggests the converse, leading us to conclude that - at the current level of complexity of convolutional architectures and scale of the data sets used to train them - CNNs are not very effective at marginalizing nuisance variability. We also quantify the effects of context on the overall classification task and its impact on the performance of CNNs, and propose improved sampling techniques for heuristic proposal schemes that improve end-to-end performance to state-of-the-art levels. We test our hypothesis on a classification task using the ImageNet Challenge benchmark and on a wide-baseline matching task using the Oxford and Fischer's datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Karianakis_An_Empirical_Evaluation_CVPR_2016_paper.pdf",
        "aff": "UCLA Vision Lab, University of California, Los Angeles, CA 90095; UCLA Vision Lab, University of California, Los Angeles, CA 90095; UCLA Vision Lab, University of California, Los Angeles, CA 90095",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 879927,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1558022500553685989&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ucla.edu;cs.ucla.edu;ucla.edu",
        "email": "ucla.edu;cs.ucla.edu;ucla.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Karianakis_An_Empirical_Evaluation_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "UCLA Vision Lab",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Analyzing Classifiers: Fisher Vectors and Deep Neural Networks",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "70",
        "author_site": "Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Klaus-Robert M\u00fcller, Wojciech Samek",
        "author": "Sebastian Lapuschkin; Alexander Binder; Gregoire Montavon; Klaus-Robert Muller; Wojciech Samek",
        "abstract": "Fisher vector (FV) classifiers and Deep Neural Networks (DNNs) are popular and successful algorithms for solving image classification problems. However, both are generally considered `black box' predictors as the non-linear transformations involved have so far prevented transparent and interpretable reasoning. Recently, a principled technique, Layer-wise Relevance Propagation (LRP), has been developed in order to better comprehend the inherent structured reasoning of complex nonlinear classification models such as Bag of Feature models or DNNs. In this paper we (1) extend the LRP framework also for Fisher vector classifiers and then use it as analysis tool to (2) quantify the importance of context for classification, (3) qualitatively compare DNNs against FV classifiers in terms of important image regions and (4) detect potential flaws and biases in data.  All experiments are performed on the PASCAL VOC 2007 and ILSVRC 2012 data sets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bach_Analyzing_Classifiers_Fisher_CVPR_2016_paper.pdf",
        "aff": "Fraunhofer HHI; Singapore University of Technology and Design; TU Berlin; TU Berlin + Korea University; Fraunhofer HHI",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Bach_Analyzing_Classifiers_Fisher_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2199992,
        "gs_citation": 262,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11526767346558625419&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "hhi.fraunhofer.de;sutd.edu.sg;tu-berlin.de;tu-berlin.de;hhi.fraunhofer.de",
        "email": "hhi.fraunhofer.de;sutd.edu.sg;tu-berlin.de;tu-berlin.de;hhi.fraunhofer.de",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bach_Analyzing_Classifiers_Fisher_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;2+3;0",
        "aff_unique_norm": "Fraunhofer Heinrich Hertz Institute;Singapore University of Technology and Design;Technische Universit\u00e4t Berlin;Korea University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.hhi.fraunhofer.de/;https://www.sutd.edu.sg;https://www.tu-berlin.de;https://www.korea.ac.kr",
        "aff_unique_abbr": "HHI;SUTD;TU Berlin;KU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Berlin",
        "aff_country_unique_index": "0;1;0;0+2;0",
        "aff_country_unique": "Germany;Singapore;South Korea"
    },
    {
        "title": "Answer-Type Prediction for Visual Question Answering",
        "session": "Images and Language",
        "status": "Poster",
        "track": "main",
        "pid": "46",
        "author_site": "Kushal Kafle, Christopher Kanan",
        "author": "Kushal Kafle; Christopher Kanan",
        "abstract": "Recently, algorithms for object recognition and related tasks have become sufficiently proficient that new vision tasks can now be pursued. In this paper, we build a system capable of answering open-ended text-based questions about images, which is known as Visual Question Answering (VQA). Our approach's key insight is that we can predict the form of the answer from the question. We formulate our solution in a Bayesian framework. When our approach is combined with a discriminative model, the combined model achieves state-of-the-art results on four benchmark datasets for open-ended VQA: DAQUAR, COCO-QA, The VQA Dataset, and Visual7W.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kafle_Answer-Type_Prediction_for_CVPR_2016_paper.pdf",
        "aff": "Chester F. Carlson Center for Imaging Science; Chester F. Carlson Center for Imaging Science",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 656423,
        "gs_citation": 167,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9180624711728764399&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "rit.edu;rit.edu",
        "email": "rit.edu;rit.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kafle_Answer-Type_Prediction_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Chester F. Carlson Center for Imaging Science",
        "aff_unique_dep": "Center for Imaging Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Anticipating Visual Representations From Unlabeled Video",
        "session": "Language and Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "11",
        "author_site": "Carl Vondrick, Hamed Pirsiavash, Antonio Torralba",
        "author": "Carl Vondrick; Hamed Pirsiavash; Antonio Torralba",
        "abstract": "Anticipating actions and objects before they start or appear is a difficult problem in computer vision with several real-world applications. This task is challenging partly because it requires leveraging extensive knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently learning this knowledge is through readily available unlabeled video. We present a framework that capitalizes on temporal structure in unlabeled video to learn to anticipate human actions and objects. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. Visual representations are a promising prediction target because they encode images at a higher semantic level than pixels yet are automatic to compute. We then apply recognition algorithms on our predicted representation to anticipate objects and actions. We experimentally validate this idea on two datasets, anticipating actions one second in the future and objects five seconds in the future.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Vondrick_Anticipating_Visual_Representations_CVPR_2016_paper.pdf",
        "aff": "Massachusetts Institute of Technology; University of Maryland, Baltimore County; Massachusetts Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 888532,
        "gs_citation": 533,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5316034551790483635&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "mit.edu;umbc.edu;mit.edu",
        "email": "mit.edu;umbc.edu;mit.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Vondrick_Anticipating_Visual_Representations_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Maryland, Baltimore County",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.umbc.edu",
        "aff_unique_abbr": "MIT;UMBC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Baltimore County",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Approximate Log-Hilbert-Schmidt Distances Between Covariance Operators for Image Classification",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "69",
        "author_site": "H\u00e0 Quang Minh, Marco San Biagio, Loris Bazzani, Vittorio Murino",
        "author": "Ha Quang Minh; Marco San Biagio; Loris Bazzani; Vittorio Murino",
        "abstract": "This paper presents a novel framework for visual object recognition using infinite-dimensional covariance operators of input features, in the paradigm of kernel methods on infinite-dimensional Riemannian manifolds. Our formulation provides a rich representation of image features by exploiting their non-linear correlations, using the power of kernel methods and Riemannian geometry. Theoretically, we provide an approximate formulation for the Log-Hilbert-Schmidt distance between covariance operators that is efficient to compute and scalable to large datasets. Empirically, we apply our framework to the task of image classification on eight different, challenging datasets. In almost all cases, the results obtained outperform other state of the art methods, demonstrating the competitiveness and potential of our framework.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Minh_Approximate_Log-Hilbert-Schmidt_Distances_CVPR_2016_paper.pdf",
        "aff": "Pattern Analysis and Computer Vision (PA VIS), Istituto Italiano di Tecnologia (IIT), Italy; Pattern Analysis and Computer Vision (PA VIS), Istituto Italiano di Tecnologia (IIT), Italy; Department of Computer Science, Dartmouth College, USA; Pattern Analysis and Computer Vision (PA VIS), Istituto Italiano di Tecnologia (IIT), Italy",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Minh_Approximate_Log-Hilbert-Schmidt_Distances_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 865418,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5977587158422771269&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "iit.it;iit.it;gmail.com;iit.it",
        "email": "iit.it;iit.it;gmail.com;iit.it",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Minh_Approximate_Log-Hilbert-Schmidt_Distances_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Istituto Italiano di Tecnologia;Dartmouth College",
        "aff_unique_dep": "Pattern Analysis and Computer Vision (PA VIS);Department of Computer Science",
        "aff_unique_url": "https://www.iit.it;https://dartmouth.edu",
        "aff_unique_abbr": "IIT;Dartmouth",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Italy;United States"
    },
    {
        "title": "Ask Me Anything: Free-Form Visual Question Answering Based on Knowledge From External Sources",
        "session": "High Level Semantics",
        "status": "Spotlight",
        "track": "main",
        "pid": "8",
        "author_site": "Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, Anton van den Hengel",
        "author": "Qi Wu; Peng Wang; Chunhua Shen; Anthony Dick; Anton van den Hengel",
        "abstract": "We propose a method for visual question answering which combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. This allows more complex questions to be answered using the predominant neural network-based approach than has previously been possible.  It particularly allows questions to be asked about the contents of an image, even when the image itself does not contain the whole answer. The method constructs a textual representation of the semantic content of an image, and merges it with textual information sourced from a knowledge base, to develop a deeper understanding of the scene viewed. Priming a recurrent neural network with this combined information, and the submitted question, leads to a very flexible visual question answering approach. We are specifically able to answer questions posed in natural language, that refer to information not contained in the image. We demonstrate the effectiveness of our model on two publicly available datasets, Toronto COCO-QA and VQA, and show that it produces the best reported results in both cases.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wu_Ask_Me_Anything_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wu_Ask_Me_Anything_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1407233,
        "gs_citation": 475,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4355791742990129767&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wu_Ask_Me_Anything_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Adelaide",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Attention to Scale: Scale-Aware Semantic Image Segmentation",
        "session": "Semantic Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "66",
        "author_site": "Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, Alan L. Yuille",
        "author": "Liang-Chieh Chen; Yi Yang; Jiang Wang; Wei Xu; Alan L. Yuille",
        "abstract": "Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixel-wise classification. In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. The proposed attention model not only outperforms average- and max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Attention_to_Scale_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Chen_Attention_to_Scale_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1933381,
        "gs_citation": 1726,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14655587414800838242&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Attention_to_Scale_CVPR_2016_paper.html"
    },
    {
        "title": "Augmented Blendshapes for Real-Time Simultaneous 3D Head Modeling and Facial Motion Capture",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "29",
        "author_site": "Diego Thomas, Rin-ichiro Taniguchi",
        "author": "Diego Thomas; Rin-ichiro Taniguchi",
        "abstract": "We propose a method to build in real-time animated 3D head models using a consumer-grade RGB-D camera. Our framework is the first one to provide simultaneously comprehensive facial motion tracking and a detailed 3D model of the user's head. Anyone's head can be instantly reconstructed and his facial motion captured without requiring any training or pre-scanning. The user starts facing the camera with a neutral expression in the first frame, but is free to move, talk and change his face expression as he wills otherwise. The facial motion is tracked using a blendshape representation while the fine geometric details are captured using a Bump image mapped over the template mesh. We propose an efficient algorithm to grow and refine the 3D model of the head on-the-fly and in real-time. We demonstrate robust and high-fidelity simultaneous facial motion tracking and 3D head modeling results on a wide range of subjects with various head poses and facial expressions. Our proposed method offers interesting possibilities for animation production and 3D video telecommunications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Thomas_Augmented_Blendshapes_for_CVPR_2016_paper.pdf",
        "aff": "Kyushu University, Fukuoka, Japan; Kyushu University, Fukuoka, Japan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3454723,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11242426235942404265&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "gmail.com;kyudai.jp",
        "email": "gmail.com;kyudai.jp",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Thomas_Augmented_Blendshapes_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Kyushu University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kyushu-u.ac.jp",
        "aff_unique_abbr": "Kyushu U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Fukuoka",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Automated 3D Face Reconstruction From Multiple Images Using Quality Measures",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "42",
        "author_site": "Marcel Piotraschke, Volker Blanz",
        "author": "Marcel Piotraschke; Volker Blanz",
        "abstract": "Automated 3D reconstruction of faces from images is challenging if the image material is difficult in terms of pose, lighting, occlusions and facial expressions, and if the initial 2D feature positions are inaccurate or unreliable. We propose a method that reconstructs individual 3D shapes from multiple single images of one person, judges their quality and then combines the best of all results. This is done separately for different regions of the face. The core element of this algorithm and the focus of our paper is a quality measure that judges a reconstruction without information about the true shape. We evaluate different quality measures, develop a method for combining results, and present a complete processing pipeline for automated reconstruction.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Piotraschke_Automated_3D_Face_CVPR_2016_paper.pdf",
        "aff": "Institute for Vision and Graphics, University of Siegen, Germany; Institute for Vision and Graphics, University of Siegen, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1532081,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12347225338951316010&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "nt.uni-siegen.de;informatik.uni-siegen.de",
        "email": "nt.uni-siegen.de;informatik.uni-siegen.de",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Piotraschke_Automated_3D_Face_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Siegen",
        "aff_unique_dep": "Institute for Vision and Graphics",
        "aff_unique_url": "https://www.uni-siegen.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Automatic Content-Aware Color and Tone Stylization",
        "session": "Computational Photography and Biomedical Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "23",
        "author_site": "Joon-Young Lee, Kalyan Sunkavalli, Zhe Lin, Xiaohui Shen, In So Kweon",
        "author": "Joon-Young Lee; Kalyan Sunkavalli; Zhe Lin; Xiaohui Shen; In So Kweon",
        "abstract": "We introduce a new technique that automatically generates diverse, visually compelling stylizations for a photograph in an unsupervised manner. We achieve this by learning style ranking for a given input using a large photo collection and selecting a diverse subset of matching styles for final style transfer. We also propose an improved technique that transfers the global color and tone of the chosen exemplars to the input photograph while avoiding the common visual artifacts produced by the existing style transfer methods. Together, our style selection and transfer techniques produce compelling, artifact-free results on a wide range of input photographs, and a user study shows that our results are preferred over other techniques.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lee_Automatic_Content-Aware_Color_CVPR_2016_paper.pdf",
        "aff": "Adobe Research; Adobe Research; Adobe Research; Adobe Research; KAIST",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3558835,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15506411103670976619&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lee_Automatic_Content-Aware_Color_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Adobe;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "Adobe Research;",
        "aff_unique_url": "https://research.adobe.com;https://www.kaist.ac.kr",
        "aff_unique_abbr": "Adobe;KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "title": "Automatic Fence Segmentation in Videos of Dynamic Scenes",
        "session": "Video Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "76",
        "author_site": "Renjiao Yi, Jue Wang, Ping Tan",
        "author": "Renjiao Yi; Jue Wang; Ping Tan",
        "abstract": "We present a fully automatic approach to detect and segment fence-like occluders from a video clip. Unlike previous approaches that usually assume either static scenes or cameras, our method is capable of handling both dynamic scenes and moving cameras. Under a bottom-up framework, it first clusters pixels into coherent groups using color and motion features. These pixel groups are then analyzed in a fully connected graph, and labeled as either fence or non-fence using graph-cut optimization. Finally, we solve a dense Conditional Random Filed (CRF) constructed from multiple frames to enhance both spatial accuracy and temporal coherence of the segmentation. Once segmented, one can use existing hole-filling methods to generate a fence-free output. Extensive evaluation suggests that our method outperforms previous automatic and interactive approaches on complex examples captured by mobile devices.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yi_Automatic_Fence_Segmentation_CVPR_2016_paper.pdf",
        "aff": "Simon Fraser University + National University of Defence Technology; Adobe Research; Simon Fraser University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3672976,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11981344545048773055&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yi_Automatic_Fence_Segmentation_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;2;0",
        "aff_unique_norm": "Simon Fraser University;National University of Defense Technology;Adobe",
        "aff_unique_dep": ";;Adobe Research",
        "aff_unique_url": "https://www.sfu.ca;http://www.nudt.edu.cn/;https://research.adobe.com",
        "aff_unique_abbr": "SFU;NUDT;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;2;0",
        "aff_country_unique": "Canada;China;United States"
    },
    {
        "title": "Automatic Image Cropping : A Computational Complexity Study",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "54",
        "author_site": "Jiansheng Chen, Gaocheng Bai, Shaoheng Liang, Zhengqin Li",
        "author": "Jiansheng Chen; Gaocheng Bai; Shaoheng Liang; Zhengqin Li",
        "abstract": "Attention based automatic image cropping aims at preserving the most visually important region in an image. A common task in this kind of method is to search for the smallest rectangle inside which the summed attention is maximized. We demonstrate that under appropriate formulations, this task can be achieved using efficient algorithms with low computational complexity. In a  practically useful scenario where the aspect ratio of the cropping rectangle is given, the problem can be solved with a computational complexity linear to the number of image pixels. We also study the possibility of multiple rectangle cropping and a new model facilitating fully automated image cropping.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Automatic_Image_Cropping_CVPR_2016_paper.pdf",
        "aff": "Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1819909,
        "gs_citation": 147,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11570414736798582792&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mail.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn",
        "email": "mail.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Automatic_Image_Cropping_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Automating Carotid Intima-Media Thickness Video Interpretation With Convolutional Neural Networks",
        "session": "Biomedical Image Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "29",
        "author_site": "Jae Shin, Nima Tajbakhsh, R. Todd Hurst, Christopher B. Kendall, Jianming Liang",
        "author": "Jae Shin; Nima Tajbakhsh; R. Todd Hurst; Christopher B. Kendall; Jianming Liang",
        "abstract": "Cardiovascular disease (CVD) is the leading cause of mortality yet largely preventable, but the key to prevention is to identify at risk individuals before adverse events. For predicting individual CVD risk, carotid intima-media thickness (CIMT), a noninvasive ultrasound method, has proven to be valuable, offering several advantages over CT coronary artery calcium score. However, each CIMT examination includes several ultrasound videos, and interpreting each of these CIMT videos involves three operations: (1) select three end-diastolic ultrasound frames (EUF) in the video, (2) localize a region of interest (ROI) in each selected frame, and (3) trace the lumen-intima interface and the media-adventitia interface in each ROI to measure CIMT. These operations are tedious, laborious, and time consuming, a serious limitation that hinders the widespread utilization of CIMT in clinical practice. To overcome this limitation, this paper presents a new system to automate CIMT video interpretation. Our extensive experiments demonstrate that the suggested system significantly outperforms the state-of-the-art methods. The superior performance is attributable to our unified framework based on convolutional neural networks (CNNs) coupled with our informative image representation and effective post-processing of the CNN outputs, which are uniquely designed for each of the above three operations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shin_Automating_Carotid_Intima-Media_CVPR_2016_paper.pdf",
        "aff": "Department of Biomedical Informatics, Arizona State University; Department of Biomedical Informatics, Arizona State University; Division of Cardiovascular Diseases, Mayo Clinic Arizona; Division of Cardiovascular Diseases, Mayo Clinic Arizona; Department of Biomedical Informatics, Arizona State University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Shin_Automating_Carotid_Intima-Media_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1589098,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15367336858685955386&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "asu.edu;asu.edu;mayo.edu;mayo.edu;asu.edu",
        "email": "asu.edu;asu.edu;mayo.edu;mayo.edu;asu.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shin_Automating_Carotid_Intima-Media_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Arizona State University;Mayo Clinic",
        "aff_unique_dep": "Department of Biomedical Informatics;Division of Cardiovascular Diseases",
        "aff_unique_url": "https://www.asu.edu;https://www.mayoclinic.org",
        "aff_unique_abbr": "ASU;Mayo Clinic",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Arizona",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "BORDER: An Oriented Rectangles Approach to Texture-Less Object Recognition",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "64",
        "author_site": "Jacob Chan, Jimmy Addison Lee, Qian Kemao",
        "author": "Jacob Chan; Jimmy Addison Lee; Qian Kemao",
        "abstract": "This paper presents an algorithm coined BORDER (Bounding Oriented-Rectangle Descriptors for Enclosed Regions) for texture-less object recognition. By fusing a regional object encompassment concept with descriptor-based pipelines, we extend local-patches into scalable object-sized oriented rectangles for optimal object information encapsulation with minimal outliers. We correspondingly introduce a modified line-segment detection technique termed Linelets to stabilize keypoint repeatability in homogenous conditions. In addition, a unique sampling technique facilitates the incorporation of robust angle primitives to produce discriminative rotation-invariant descriptors. BORDER's high competence in object recognition particularly excels in homogenous conditions obtaining superior detection rates in the presence of high-clutter, occlusion and scale-rotation changes when compared with modern state-of-the-art texture-less object detectors such as BOLD and LINE2D on public texture-less object databases.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chan_BORDER_An_Oriented_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4828560,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=354019106569708295&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chan_BORDER_An_Oriented_CVPR_2016_paper.html"
    },
    {
        "title": "Backtracking ScSPM Image Classifier for Weakly Supervised Top-Down Saliency",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "78",
        "author_site": "Hisham Cholakkal, Jubin Johnson, Deepu Rajan",
        "author": "Hisham Cholakkal; Jubin Johnson; Deepu Rajan",
        "abstract": "Top-down saliency models produce a probability map that peaks at target locations specified by a task/goal such as object detection. They are usually trained in a supervised setting involving annotations of objects. We propose a weakly supervised top-down saliency framework using only binary labels that indicate the presence/absence of an object in an image. First, the probabilistic contribution of each image patch to the confidence of an ScSPM-based classifier produces a Reverse-ScSPM (R-ScSPM) saliency map. Neighborhood information is then incorporated through a contextual saliency map which is estimated using logistic regression learnt on patches having high R-ScSPM saliency. Both the saliency maps are combined to obtain the final saliency map. We evaluate the performance of the proposed weakly supervised top-down saliency and achieves comparable performance with fully supervised approaches. Experiments are carried out on 5 challenging datasets across 3 different applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cholakkal_Backtracking_ScSPM_Image_CVPR_2016_paper.pdf",
        "aff": "Multimedia Lab, School of Computer Science and Engineering, Nanyang Technological University Singapore; Multimedia Lab, School of Computer Science and Engineering, Nanyang Technological University Singapore; Multimedia Lab, School of Computer Science and Engineering, Nanyang Technological University Singapore",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Cholakkal_Backtracking_ScSPM_Image_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1702389,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4008163333573857020&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cholakkal_Backtracking_ScSPM_Image_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Beyond F-Formations: Determining Social Involvement in Free Standing Conversing Groups From Static Images",
        "session": "Events, Activities, and Surveillance",
        "status": "Poster",
        "track": "main",
        "pid": "35",
        "author_site": "Lu Zhang, Hayley Hung",
        "author": "Lu Zhang; Hayley Hung",
        "abstract": "In this paper, we present the first attempt to analyse differing levels of social involvement in free standing conversing groups (or the so-called F-formations) from static images. In addition, we enrich state-of-the-art F-formation modelling by learning a frustum of attention that accounts for the spatial context. That is, F-formation configurations vary with respect to the arrangement of furniture and the non-uniform crowdedness in the space during mingling scenarios. The majority of prior works have considered the labelling of conversing group as an objective task, requiring only a single annotator. However, we show that by embracing the subjectivity of social involvement, we not only generate a richer model of the social interactions in a scene but also significantly improve F-formation detection. We carry out extensive experimental validation of our proposed approach by collecting a novel set of multi-annotator labels of involvement on the publicly available Idiap Poster Data;  the only multi-annotator labelled database of free standing conversing groups that is currently available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Beyond_F-Formations_Determining_CVPR_2016_paper.pdf",
        "aff": "Delft University of Technology; Delft University of Technology + University of Twente",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 903737,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15830869485938233051&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "tudelft.nl;tudelft.nl",
        "email": "tudelft.nl;tudelft.nl",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Beyond_F-Formations_Determining_CVPR_2016_paper.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Delft University of Technology;University of Twente",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tudelft.nl;https://www.utwente.nl",
        "aff_unique_abbr": "TU Delft;UT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Netherlands"
    },
    {
        "title": "Beyond Local Search: Tracking Objects Everywhere With Instance-Specific Proposals",
        "session": "Video Analysis 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "20",
        "author_site": "Gao Zhu, Fatih Porikli, Hongdong Li",
        "author": "Gao Zhu; Fatih Porikli; Hongdong Li",
        "abstract": "Most tracking-by-detection methods employ a local search window around the predicted object location in the current frame assuming the previous location is accurate, the trajectory is smooth, and the computational capacity permits a search radius that can accommodate the maximum speed yet small enough to reduce mismatches. These, however, may not be valid always, in particular for fast and irregularly moving objects. Here, we present an object tracker that is not limited to a local search window and has ability to probe efficiently the entire frame. Our method generates a small number of \"high-quality\" proposals by a novel instance-specific objectness measure and evaluates them against the object model that can be adopted from an existing tracking-by-detection approach as a core tracker. During the tracking process, we update the object model concentrating on hard false-positives supplied by the proposals, which help suppressing distractors caused by difficult background clutters, and learn how to re-rank proposals according to the object model. Since we reduce significantly the number of hypotheses the core tracker evaluates, we can use richer object descriptors and stronger detector.  Our method outperforms most recent state-of-the-art trackers on popular tracking benchmarks, and provides improved robustness for fast moving objects as well as for ultra low-frame-rate videos.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_Beyond_Local_Search_CVPR_2016_paper.pdf",
        "aff": "Australian National University1+NICTA2+ARC Centre of Excellence for Robotic Vision3; Australian National University1+NICTA2+ARC Centre of Excellence for Robotic Vision3; Australian National University1+ARC Centre of Excellence for Robotic Vision3",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1289867,
        "gs_citation": 255,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11331319226269665684&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "anu.edu.au;anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;anu.edu.au",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_Beyond_Local_Search_CVPR_2016_paper.html",
        "aff_unique_index": "0+1+2;0+1+2;0+2",
        "aff_unique_norm": "Australian National University;NICTA;ARC Centre of Excellence for Robotic Vision",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.anu.edu.au;https://www.nicta.com.au;https://roboticvision.org/",
        "aff_unique_abbr": "ANU;NICTA;ARC CoE for Robotic Vision",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0+0;0+0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Bilateral Space Video Segmentation",
        "session": "Video Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "80",
        "author_site": "Nicolas Maerki, Federico Perazzi, Oliver Wang, Alexander Sorkine-Hornung",
        "author": "Nicolas Maerki; Federico Perazzi; Oliver Wang; Alexander Sorkine-Hornung",
        "abstract": "In this work, we propose a novel approach to video segmentation that operates in bilateral space. We design a new energy on the vertices of a regularly sampled spatio-temporal bilateral grid, which can be solved efficiently using a standard graph cut label assignment. Using a bilateral formulation, the energy that we minimize implicitly approximates long-range, spatio-temporal connections between pixels while still containing only a small number of variables and only local graph edges. We compare to a number of recent methods, and show that our approach achieves state-of-the-art results on multiple benchmarks in a fraction of the runtime. Furthermore, our method scales linearly with image size, allowing for interactive feedback on real-world high resolution video.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Maerki_Bilateral_Space_Video_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2049137,
        "gs_citation": 268,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13387448665720685558&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Maerki_Bilateral_Space_Video_CVPR_2016_paper.html"
    },
    {
        "title": "Blind Image Deblurring Using Dark Channel Prior",
        "session": "Image Processing and Restoration",
        "status": "Oral",
        "track": "main",
        "pid": "14",
        "author_site": "Jinshan Pan, Deqing Sun, Hanspeter Pfister, Ming-Hsuan Yang",
        "author": "Jinshan Pan; Deqing Sun; Hanspeter Pfister; Ming-Hsuan Yang",
        "abstract": "We present a simple and effective blind image deblurring method based on the dark channel prior. Our work is inspired by the interesting observation that the dark channel of blurred images is less sparse. While most image patches in the clean image contain some dark pixels, these pixels are not dark when averaged with neighboring high-intensity pixels during the blur process.Our analysis shows that this change in the sparsity of the dark channel is an inherent property of the blur process, both theoretically and empirically. This change in the sparsity of the dark channel is an inherent property of the blur process, which we both prove mathematically and validate using training data. Therefore, enforcing the sparsity of the dark channel helps blind deblurring on various scenarios, including natural, face, text, and low-illumination images. However, sparsity of the dark channel introduces a non-convex non-linear optimization problem. We introduce a linear approximation of the min operator to compute the dark channel. Our look-up-table-based method converges fast in practice and can be directly extended to non-uniform deblurring. Extensive experiments show that our method achieves state-of-the-art results on deblurring natural images and compares favorably methods that are well-engineered for specific scenarios.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Blind_Image_Deblurring_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Pan_Blind_Image_Deblurring_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4256300,
        "gs_citation": 968,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9986114152538067874&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Pan_Blind_Image_Deblurring_CVPR_2016_paper.html"
    },
    {
        "title": "Blind Image Deconvolution by Automatic Gradient Activation",
        "session": "Deblurring and Super-Resolution",
        "status": "Poster",
        "track": "main",
        "pid": "36",
        "author_site": "Dong Gong, Mingkui Tan, Yanning Zhang, Anton van den Hengel, Qinfeng Shi",
        "author": "Dong Gong; Mingkui Tan; Yanning Zhang; Anton van den Hengel; Qinfeng Shi",
        "abstract": "Blind image deconvolution is an ill-posed inverse problem which is often addressed through the application of appropriate prior. Although some priors are informative in general, many images do not strictly conform to this, leading to degraded performance in the kernel estimation. More critically, real images may be contaminated by nonuniform noise such as saturation and outliers.Methods for removing specific image areas based on some priors have been proposed, but they operate either manually or by defining fixed criteria. We show here that a subset of the image gradients are adequate to estimate the blur kernel robustly, no matter the gradient image is sparse or not. We thus introduce a gradient activation method to automatically select a subset of gradients of the latent image in a cutting-plane-based optimization scheme for kernel estimation. No extra assumption is used in our model, which greatly improves the accuracy and flexibility. More importantly, the proposed method affords great convenience for handling noise and outliers. Experiments on both synthetic data and real-world images demonstrate the effectiveness and robustness of the proposed method in comparison with the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gong_Blind_Image_Deconvolution_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science, Northwestern Polytechnical University, Xi\u2019an, China+School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, Northwestern Polytechnical University, Xi\u2019an, China; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1950502,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12684923089670761427&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;nwpu.edu.cn;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;nwpu.edu.cn;adelaide.edu.au;adelaide.edu.au",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gong_Blind_Image_Deconvolution_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;0;1;1",
        "aff_unique_norm": "Northwestern Polytechnical University;University of Adelaide",
        "aff_unique_dep": "School of Computer Science;School of Computer Science",
        "aff_unique_url": "https://www.nwpu.edu.cn;https://www.adelaide.edu.au",
        "aff_unique_abbr": "NPU;Adelaide",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Xi'an;",
        "aff_country_unique_index": "0+1;1;0;1;1",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Blockout: Dynamic Model Selection for Hierarchical Deep Networks",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "35",
        "author_site": "Calvin Murdock, Zhen Li, Howard Zhou, Tom Duerig",
        "author": "Calvin Murdock; Zhen Li; Howard Zhou; Tom Duerig",
        "abstract": "Most deep architectures for image classification--even those that are trained to classify a large number of diverse categories--learn shared image representations with a single model. Intuitively, however, categories that are more similar should share more information than those that are very different. While hierarchical deep networks address this problem by learning separate features for subsets of related categories, current implementations require simplified models using fixed architectures specified via heuristic clustering methods. Instead, we propose Blockout, a method for regularization and model selection that simultaneously learns both the model architecture and parameters. A generalization of Dropout, our approach gives a novel parametrization of hierarchical architectures that allows for structure learning via back-propagation. To demonstrate its utility, we evaluate Blockout on the CIFAR and ImageNet datasets, demonstrating improved classification accuracy, better regularization performance, faster training, and the clear emergence of hierarchical network structures.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Murdock_Blockout_Dynamic_Model_CVPR_2016_paper.pdf",
        "aff": "Carnegie Mellon University; Google Research; Google Research; Google Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4887852,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9545676190156304152&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.cmu.edu;google.com;google.com;google.com",
        "email": "cs.cmu.edu;google.com;google.com;google.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Murdock_Blockout_Dynamic_Model_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Carnegie Mellon University;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.cmu.edu;https://research.google",
        "aff_unique_abbr": "CMU;Google Research",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Bottom-Up and Top-Down Reasoning With Hierarchical Rectified Gaussians",
        "session": "People and Faces",
        "status": "Spotlight",
        "track": "main",
        "pid": "33",
        "author_site": "Peiyun Hu, Deva Ramanan",
        "author": "Peiyun Hu; Deva Ramanan",
        "abstract": "Convolutional neural nets (CNNs) have demonstrated remarkable performance in recent history. Such approaches tend to work in a \"unidirectional\" bottom-up feed-forward fashion. However, practical experience and biological evidence tells us that feedback plays a crucial role, particularly for detailed spatial understanding tasks. This work explores \"bidirectional\" architectures that also reason with top-down feedback: neural units are influenced by both lower and higher-level units.  We do so by treating units as rectified latent variables in a quadratic energy function, which can be seen as a hierarchical Rectified Gaussian model (RGs). We show that RGs can be optimized with a quadratic program (QP), that can in turn be optimized with a recurrent neural network (with rectified linear units). This allows RGs to be trained with GPU-optimized gradient descent. From a theoretical perspective, RGs help establish a connection between CNNs and hierarchical probabilistic models. From a practical perspective, RGs are well suited for detailed spatial tasks that can benefit from top-down reasoning. We illustrate them on the challenging task of keypoint localization under occlusions, where local bottom-up evidence may be misleading. We demonstrate state-of-the-art results on challenging benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hu_Bottom-Up_and_Top-Down_CVPR_2016_paper.pdf",
        "aff": "UC Irvine; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3326198,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2488280427219234841&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "ics.uci.edu;cs.cmu.edu",
        "email": "ics.uci.edu;cs.cmu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hu_Bottom-Up_and_Top-Down_CVPR_2016_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Irvine;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uci.edu;https://www.cmu.edu",
        "aff_unique_abbr": "UCI;CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "BoxCars: 3D Boxes as CNN Input for Improved Fine-Grained Vehicle Recognition",
        "session": "Video Surveilance",
        "status": "Poster",
        "track": "main",
        "pid": "80",
        "author_site": "Jakub Sochor, Adam Herout, Ji\u0159\u00ed Havel",
        "author": "Jakub Sochor; Adam Herout; Jiri Havel",
        "abstract": "We are dealing with the problem of fine-grained vehicle make&model recognition and verification. Our contribution is showing that extracting additional data from the video stream - besides the vehicle image itself - and feeding it into the deep convolutional neural network boosts the recognition performance considerably.  This additional information includes: 3D vehicle bounding box used for \"unpacking\" the vehicle image, its rasterized low-resolution shape, and information about the 3D vehicle orientation. Experiments show that adding such information decreases classification error by 26% (the accuracy is improved from 0.772 to 0.832) and boosts verification average precision by 208% (0.378 to 0.785) compared to baseline pure CNN without any input modifications. Also, the pure baseline CNN outperforms the recent state of the art solution by 0.081. We provide an annotated set \"BoxCars\" of surveillance vehicle images augmented by various automatically extracted auxiliary information.  Our approach and the dataset can considerably improve the performance of traffic surveillance systems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sochor_BoxCars_3D_Boxes_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 243,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5496395157120595155&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sochor_BoxCars_3D_Boxes_CVPR_2016_paper.html"
    },
    {
        "title": "CNN-N-Gram for Handwriting Word Recognition",
        "session": "Recognition and Labeling",
        "status": "Oral",
        "track": "main",
        "pid": "5",
        "author_site": "Arik Poznanski , Lior Wolf",
        "author": "Arik Poznanski; Lior Wolf",
        "abstract": "Given an image of a handwritten word, a CNN is employed to estimate its n-gram frequency profile, which is the set of n-grams contained in the word. Frequencies for unigrams, bigrams and trigrams are estimated for the entire word and for parts of it. Canonical Correlation Analysis is then used to match the estimated profile to the true profiles of all words in a large dictionary. The CNN that is used employs several novelties such as the use of multiple fully connected branches. Applied to all commonly used handwriting recognition benchmarks, our method outperforms, by a very large margin, all existing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Poznanski_CNN-N-Gram_for_Handwriting_CVPR_2016_paper.pdf",
        "aff": "The Blavatnik School of Computer Science, Tel Aviv University; The Blavatnik School of Computer Science, Tel Aviv University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 474399,
        "gs_citation": 226,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4802890910404790054&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;cs.tau.ac.il",
        "email": "gmail.com;cs.tau.ac.il",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Poznanski_CNN-N-Gram_for_Handwriting_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tel Aviv University",
        "aff_unique_dep": "Blavatnik School of Computer Science",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tel Aviv",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "CNN-RNN: A Unified Framework for Multi-Label Image Classification",
        "session": "Recognition and Labeling",
        "status": "Oral",
        "track": "main",
        "pid": "3",
        "author_site": "Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang, Wei Xu",
        "author": "Jiang Wang; Yi Yang; Junhua Mao; Zhiheng Huang; Chang Huang; Wei Xu",
        "abstract": "While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that most real world images contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in an unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_CNN-RNN_A_Unified_CVPR_2016_paper.pdf",
        "aff": "Baidu Research; Baidu Research; University of California at Los Angles; Facebook Speech; Horizon Robotics; Baidu Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1803138,
        "gs_citation": 1717,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11947203399516238933&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_CNN-RNN_A_Unified_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;2;3;0",
        "aff_unique_norm": "Baidu;University of California, Los Angeles;Meta;Horizon Robotics",
        "aff_unique_dep": "Baidu Research;;Facebook Speech;",
        "aff_unique_url": "https://research.baidu.com;https://www.ucla.edu;https://www.facebook.com;https://www.horizon-robotics.com/",
        "aff_unique_abbr": "Baidu;UCLA;FB;Horizon Robotics",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0;1;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "CP-mtML: Coupled Projection Multi-Task Metric Learning for Large Scale Face Retrieval",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "47",
        "author_site": "Binod Bhattarai, Gaurav Sharma, Frederic Jurie",
        "author": "Binod Bhattarai; Gaurav Sharma; Frederic Jurie",
        "abstract": "We propose a novel Coupled Projection multi-task Met- ric Learning (CP-mtML) method for large scale face re- trieval. In contrast to previous works which were limited to low dimensional features and small datasets, the proposed method scales to large datasets with high dimensional face descriptors. It utilises pairwise (dis-)similarity constraints as supervision and hence does not require exhaustive class annotation for every training image. While, traditionally, multi-task learning methods have been validated on same dataset but different tasks, we work on the more chal- lenging setting with heterogeneous datasets and different tasks. We show empirical validation on multiple face im- age datasets of different facial traits, e.g. identity, age and expression. We use classic Local Binary Pattern (LBP) de- scriptors along with the recent Deep Convolutional Neural Network (CNN) features. The experiments clearly demon- strate the scalability and improved performance of the pro- posed method on the tasks of identity and age based face image retrieval compared to competitive existing methods, on the standard datasets and with the presence of a million distractor face images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bhattarai_CP-mtML_Coupled_Projection_CVPR_2016_paper.pdf",
        "aff": "University of Caen, France; MPI for Informatics, Germany+IIT Kanpur, India; University of Caen, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 685071,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3932641171585432189&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "; ; ",
        "email": "; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bhattarai_CP-mtML_Coupled_Projection_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "University of Caen;Max Planck Institute for Informatics;Indian Institute of Technology Kanpur",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unicaen.fr;https://www.mpi-inf.mpg.de;https://www.iitk.ac.in",
        "aff_unique_abbr": "unicaen;MPII;IITK",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+2;0",
        "aff_country_unique": "France;Germany;India"
    },
    {
        "id": "45cff1c152",
        "title": "CRAFT Objects From Images",
        "site": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_CRAFT_Objects_From_CVPR_2016_paper.html",
        "author": "Bin Yang; Junjie Yan; Zhen Lei; Stan Z. Li",
        "abstract": "Object detection is a fundamental problem in image understanding. One popular solution is the R-CNN framework and its fast versions. They decompose the object detection problem into two cascaded easier tasks: 1) generating object proposals from images, 2) classifying proposals into various object categories. Despite that we are handling with two relatively easier tasks, they are not solved perfectly and there's still room for improvement. In this paper, we push the \"divide and conquer\" solution even further by dividing each task into two sub-tasks. We call the proposed method \"CRAFT\" (Cascade Region-proposal-network And FasT-rcnn), which tackles each task with a carefully designed network cascade. We show that the cascade structure helps in both tasks: in proposal generation, it provides more compact and better localized object proposals; in object classification, it reduces false positives (mainly between ambiguous categories) by capturing both inter- and intra-category variances. CRAFT achieves consistent and considerable improvement over the state-of-the-art on object detection benchmarks like PASCAL VOC 07/12 and ILSVRC.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf",
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; Tsinghua University; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 846896,
        "gs_citation": 173,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1660509076243839928&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "nlpr.ia.ac.cn;outlook.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;outlook.com;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;Tsinghua University",
        "aff_unique_dep": "Institute of Automation;",
        "aff_unique_url": "http://www.ia.cas.cn;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "CAS;THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Camera Calibration From Dynamic Silhouettes Using Motion Barcodes",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "33",
        "author_site": "Gil Ben-Artzi, Yoni Kasten, Shmuel Peleg, Michael Werman",
        "author": "Gil Ben-Artzi; Yoni Kasten; Shmuel Peleg; Michael Werman",
        "abstract": "Computing the epipolar geometry between cameras with very different viewpoints is often problematic as matching points are hard to find. In these cases, it has been proposed to use information from dynamic objects in the scene for suggesting point and line correspondences.  We propose a speed up of about two orders of magnitude, as well as an increase in robustness and accuracy, to methods computing epipolar geometry from dynamic silhouettes based on a new temporal signature, motion barcode for lines. This is a binary temporal sequence for lines, indicating for each frame the existence of at least one foreground pixel on that line. The motion barcodes of two corresponding epipolar lines are very similar so the search for corresponding epipolar lines can be limited to lines having similar barcodes leading to increased speed, accuracy, and robustness in computing the epipolar geometry.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ben-Artzi_Camera_Calibration_From_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 856092,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15654055861991956485&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ben-Artzi_Camera_Calibration_From_CVPR_2016_paper.html"
    },
    {
        "title": "Camera Calibration From Periodic Motion of a Pedestrian",
        "session": "Video Surveilance",
        "status": "Poster",
        "track": "main",
        "pid": "82",
        "author_site": "Shiyao Huang, Xianghua Ying, Jiangpeng Rong, Zeyu Shang, Hongbin Zha",
        "author": "Shiyao Huang; Xianghua Ying; Jiangpeng Rong; Zeyu Shang; Hongbin Zha",
        "abstract": "Camera calibration directly from image sequences of a pedestrian without using any calibration object is a really challenging task and should be well solved in computer vision, especially in visual surveillance. In this paper, we propose a novel camera calibration method based on recovering the three orthogonal vanishing points (TOVPs), just using an image sequence of a pedestrian walking in a straight line, without any assumption of scenes or motions, e.g., control points with known 3D coordinates, parallel or perpendicular lines, non-natural or pre-designed special human motions, as often necessary in previous methods. The traces of shoes of a pedestrian carry more rich and easily detectable metric information than all other body parts in the periodic motion of a pedestrian, but such information is usually overlooked by previous work. In this paper, we employ the images of the toes of the shoes on the ground plane to determine the vanishing point corresponding to the walking direction, and then utilize harmonic conjugate properties in projective geometry to recover the vanishing point corresponding to the perpendicular direction of the walking direction in the horizontal plane and the vanishing point corresponding to the vertical direction. After recovering all of the TOVPs, the intrinsic and extrinsic parameters of the camera can be determined. Experiments on various scenes and viewing angles prove the feasibility and accuracy of the proposed method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Camera_Calibration_From_CVPR_2016_paper.pdf",
        "aff": "Key Laboratory of Machine Perception (Ministry of Education); Key Laboratory of Machine Perception (Ministry of Education); Key Laboratory of Machine Perception (Ministry of Education); Key Laboratory of Machine Perception (Ministry of Education); Key Laboratory of Machine Perception (Ministry of Education)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1092532,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6429633978886800352&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cis.pku.edu.cn;cis.pku.edu.cn;cis.pku.edu.cn;cis.pku.edu.cn;cis.pku.edu.cn",
        "email": "cis.pku.edu.cn;cis.pku.edu.cn;cis.pku.edu.cn;cis.pku.edu.cn;cis.pku.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Huang_Camera_Calibration_From_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Key Laboratory of Machine Perception",
        "aff_unique_dep": "Machine Perception",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Canny Text Detector: Fast and Robust Scene Text Localization Algorithm",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "58",
        "author_site": "Hojin Cho, Myungchul Sung, Bongjin Jun",
        "author": "Hojin Cho; Myungchul Sung; Bongjin Jun",
        "abstract": "This paper presents a novel scene text detection algorithm, Canny Text Detector, which takes advantage of the similarity between image edge and text for effective text localization with improved recall rate. As closely related edge pixels construct the structural information of an object, we observe that cohesive characters compose a meaningful word/sentence sharing similar properties such as spatial location, size, color, and stroke width regardless of language. However, prevalent scene text detection approaches have not fully utilized such similarity, but mostly rely on the characters classified with high confidence, leading to low recall rate. By exploiting the similarity, our approach can quickly and robustly localize a variety of texts. Inspired by the original Canny edge detector, our algorithm makes use of double threshold and hysteresis tracking to detect texts of low confidence. Experimental results on public datasets demonstrate that our algorithm outperforms the state-of-the-art scene text detection methods in terms of detection rate.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cho_Canny_Text_Detector_CVPR_2016_paper.pdf",
        "aff": "Stradvision, Inc.; Stradvision, Inc.; Stradvision, Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4567204,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10313182992118229371&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "stradvision.com;stradvision.com;stradvision.com",
        "email": "stradvision.com;stradvision.com;stradvision.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cho_Canny_Text_Detector_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "StradVision",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stradvision.com",
        "aff_unique_abbr": "Stradvision",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Cascaded Interactional Targeting Network for Egocentric Video Analysis",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "44",
        "author_site": "Yang Zhou, Bingbing Ni, Richang Hong, Xiaokang Yang, Qi Tian",
        "author": "Yang Zhou; Bingbing Ni; Richang Hong; Xiaokang Yang; Qi Tian",
        "abstract": "Knowing how hands move and what object is being manipulated are two key sub-tasks for analyzing first-person (egocentric) action. However, lack of fully annotated hand data as well as imprecise foreground segmentation make either sub-task challenging. This work aims to explicitly address these two issues via introducing a cascaded interactional targeting (i.e., infer both hand and active object regions) deep neural network. Firstly, a novel EM-like learning framework is proposed to train the pixel-level deep convolutional neural network (DCNN) by seamlessly integrating weakly supervised data (i.e., massive bounding box annotations) with a small set of strongly supervised data (i.e., fully annotated hand segmentation maps) to achieve state-of-the-art hand segmentation performance. Secondly, the resulting high-quality hand segmentation maps are further paired with the corresponding motion maps and object feature maps, in order to explore the contextual information among object, motion and hand to generate interactional foreground regions (operated objects). The resulting interactional target maps (hand + active object) from our cascaded DCNN are further utilized to form discriminative action representation. Experiments show that our framework has achieved the state-of-the-art egocentric action recognition performance on the benchmark dataset Activities of Daily Living (ADL).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Cascaded_Interactional_Targeting_CVPR_2016_paper.pdf",
        "aff": "University of Texas at San Antonio, US; Shanghai Jiao Tong University, China; HeFei University of Technology, China; Shanghai Jiao Tong University, China; University of Texas at San Antonio, US",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1999740,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14675261385005567531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "my.utsa.edu;sjtu.edu.cn;hfut.edu.cn;sjtu.edu.cn;utsa.edu",
        "email": "my.utsa.edu;sjtu.edu.cn;hfut.edu.cn;sjtu.edu.cn;utsa.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Cascaded_Interactional_Targeting_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;1;0",
        "aff_unique_norm": "University of Texas at San Antonio;Shanghai Jiao Tong University;Hefei University of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utsa.edu;https://www.sjtu.edu.cn;http://www.hfut.edu.cn",
        "aff_unique_abbr": "UTSA;SJTU;HFT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Antonio;",
        "aff_country_unique_index": "0;1;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "16032bd672",
        "title": "Cataloging Public Objects Using Aerial and Street-Level Images - Urban Trees",
        "site": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wegner_Cataloging_Public_Objects_CVPR_2016_paper.html",
        "author": "Jan D. Wegner; Steven Branson; David Hall; Konrad Schindler; Pietro Perona",
        "abstract": "Each corner of the inhabited world is imaged from multiple viewpoints with increasing frequency. Online map services like Google Maps or Here Maps provide direct access to huge amounts of densely sampled, georeferenced images from street view and aerial perspective. There is an opportunity to design computer vision systems that will help us search, catalog and monitor public infrastructure, buildings and artifacts. We explore the architecture and feasibility of such a system. The main technical challenge is combining test time information from multiple views of each geographic location (e.g., aerial and street views). We implement two modules: det2geo, which detects the set of loca- tions of objects belonging to a given category, and geo2cat, which computes the fine-grained category of the object at a given location. We introduce a solution that adapts state-of-the-art CNN-based object detectors and classifiers. We test our method on \"Pasadena Urban Trees\", a new dataset of 80,000 trees with geographic and species annotations, and show that combining multiple views significantly improves both tree detection and tree species classification, rivaling human performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wegner_Cataloging_Public_Objects_CVPR_2016_paper.pdf",
        "aff": "ETH Z\u00fcrich; California Institute of Technology; California Institute of Technology; ETH Z\u00fcrich; California Institute of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wegner_Cataloging_Public_Objects_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2254942,
        "gs_citation": 201,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17236403473716715960&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "ETH Zurich;California Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.caltech.edu",
        "aff_unique_abbr": "ETHZ;Caltech",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Pasadena",
        "aff_country_unique_index": "0;1;1;0;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "Closed-Form Training of Mahalanobis Distance for Supervised Clustering",
        "session": "Grouping and Optimization Methods",
        "status": "Oral",
        "track": "main",
        "pid": "13",
        "author_site": "Marc T. Law, YaoLiang Yu, Matthieu Cord, Eric P. Xing",
        "author": "Marc T. Law; YaoLiang Yu; Matthieu Cord; Eric P. Xing",
        "abstract": "Clustering is the task of grouping a set of objects so that objects in the same cluster are more similar to each other than to those in other clusters. The crucial step in most clustering algorithms is to find an appropriate similarity metric, which is both challenging and problem-dependent. Supervised clustering approaches, which can exploit labeled clustered training data that share a common metric with the test set, have thus been proposed. Unfortunately, current metric learning approaches for supervised clustering do not scale to large or even medium-sized datasets. In this paper, we propose a new structured Mahalanobis Distance Metric Learning method for supervised clustering. We formulate our problem as an instance of large margin structured prediction and prove that it can be solved very efficiently in closed-form. The complexity of our method is (in most cases) linear in the size of the training dataset. We further reveal a striking similarity between our approach and multivariate linear regression. Experiments on both synthetic and real datasets confirm several orders of magnitude speedup while still achieving state-of-the-art performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Law_Closed-Form_Training_of_CVPR_2016_paper.pdf",
        "aff": "Sorbonne Universit \u00b4es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris, France; Machine Learning Department, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, Pennsylvania 15213, USA; Sorbonne Universit \u00b4es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris, France; Machine Learning Department, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, Pennsylvania 15213, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Law_Closed-Form_Training_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2913678,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=286574400735410207&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Law_Closed-Form_Training_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Sorbonne Universit\u00e9s;Carnegie Mellon University",
        "aff_unique_dep": ";Machine Learning Department",
        "aff_unique_url": "https://www.sorbonne-universite.fr;https://www.cmu.edu",
        "aff_unique_abbr": "Sorbonne;CMU",
        "aff_campus_unique_index": "0;1;0;1",
        "aff_campus_unique": "Paris;Pittsburgh",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "France;United States"
    },
    {
        "title": "CoMaL: Good Features to Match on Object Boundaries",
        "session": "Feature Extraction and Matching",
        "status": "Poster",
        "track": "main",
        "pid": "36",
        "author_site": "Swarna K. Ravindran, Anurag Mittal",
        "author": "Swarna K. Ravindran; Anurag Mittal",
        "abstract": "Traditional Feature Detectors and Trackers use information aggregation in 2D patches to detect and match discriminative patches.  However, this information does not remain the same at object boundaries when there is object motion against a significantly varying background.  In this paper, we propose a new approach for feature detection, tracking and re-detection that gives significantly improved results at the object boundaries.  We utilize level lines or iso-intensity curves that often remain stable and can be reliably detected even at the object boundaries, which they often trace. Stable portions of long level lines are detected and points of high curvature  are detected on such curves for corner detection.  Further, this level line is used to separate the portions belonging to the two objects, which is then used for robust matching of such points.  While such CoMaL (Corners on Maximally-stable Level Line Segments) points were found to be much more reliable at the object boundary regions, they perform comparably at the interior regions as well. This is illustrated in exhaustive experiments on real-world datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ravindran_CoMaL_Good_Features_CVPR_2016_paper.pdf",
        "aff": "Indian Institute of Technology Madras+Duke University; Indian Institute of Technology Madras",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1971240,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3510382642930671821&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.duke.edu;cse.iitm.ac.in",
        "email": "cs.duke.edu;cse.iitm.ac.in",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ravindran_CoMaL_Good_Features_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Indian Institute of Technology Madras;Duke University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitm.ac.in;https://www.duke.edu",
        "aff_unique_abbr": "IIT Madras;Duke",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Madras;",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "India;United States"
    },
    {
        "title": "Coherent Parametric Contours for Interactive Video Object Segmentation",
        "session": "Segmentation and Saliency",
        "status": "Poster",
        "track": "main",
        "pid": "69",
        "author_site": "Yao Lu, Xue Bai, Linda Shapiro, Jue Wang",
        "author": "Yao Lu; Xue Bai; Linda Shapiro; Jue Wang",
        "abstract": "Interactive video segmentation systems aim at producing sub-pixel-level object boundaries for visual effect applications. Recent approaches mainly focus on using sparse user input (i.e. scribbles) for efficient segmentation; however, the quality of the final object boundaries is not satisfactory for the following reasons: (1) the boundary on each frame is often not accurate; (2) boundaries across adjacent frames wiggle around inconsistently, causing temporal flickering; and (3) there is a lack of direct user control for fine tuning.  We propose Coherent Parametric Contours, a novel video segmentation propagation framework that addresses all the above issues. Our approach directly models the object boundary using a set of parametric curves,  providing direct user controls for manual adjustment. A spatio-temporal optimization algorithm is employed to produce object boundaries that are spatially accurate and temporally stable. We show that existing evaluation datasets are limited and demonstrate a new set to cover the common cases in professional rotoscoping. A new metric for evaluating temporal consistency is proposed. Results show that our approach generates higher quality, more coherent segmentation results than previous methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lu_Coherent_Parametric_Contours_CVPR_2016_paper.pdf",
        "aff": "University of Washington; Adobe; University of Washington; Adobe",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1764350,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12890839373495716665&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cs.washington.edu;adobe.com;cs.washington.edu;adobe.com",
        "email": "cs.washington.edu;adobe.com;cs.washington.edu;adobe.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lu_Coherent_Parametric_Contours_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Washington;Adobe",
        "aff_unique_dep": ";Adobe Inc.",
        "aff_unique_url": "https://www.washington.edu;https://www.adobe.com",
        "aff_unique_abbr": "UW;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Collaborative Quantization for Cross-Modal Similarity Search",
        "session": "Image Indexing and Retrieval",
        "status": "Poster",
        "track": "main",
        "pid": "58",
        "author_site": "Ting Zhang, Jingdong Wang",
        "author": "Ting Zhang; Jingdong Wang",
        "abstract": "Cross-modal similarity search is a problem about designing a search  system supporting querying across content modalities, e.g., using an image to search for texts or using a text to search for images. This  paper presents a compact coding solution for efficient search, with a  focus on the quantization approach which has already shown the superior  performance over the hashing solutions in the single-modal similarity  search. We propose a cross modal quantization approach, which is among  the early attempts to introduce quantization into cross-modal search.  The major contribution lies in jointly learning the quantizers for both  modalities through aligning the quantized representations for each  pair of image and text belonging to a document. In addition, our  approach simultaneously learns the common space for both modalities in  which quantization is conducted to enable efficient and effective search  using the Euclidean distance computed in the common space with fast  distance table lookup. Experimental results compared with several  competitive algorithms over three benchmark datasets demonstrate that  the proposed approach achieves the state-of-the-art performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Collaborative_Quantization_for_CVPR_2016_paper.pdf",
        "aff": "University of Science and Technology of China, China; Microsoft Research, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1179943,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18116718518472002138&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mail.ustc.edu.cn;microsoft.com",
        "email": "mail.ustc.edu.cn;microsoft.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Collaborative_Quantization_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Science and Technology of China;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "USTC;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis",
        "session": "Computational Photography and Biomedical Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "24",
        "author_site": "Chuan Li, Michael Wand",
        "author": "Chuan Li; Michael Wand",
        "abstract": "This paper studies a combination of generative Markov random field (MRF) models and discriminatively trained deep convolutional neural networks (dCNNs) for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN feature pyramid, controling the image layout at an abstract level. We apply the method to both photographic and non-photo-realistic (artwork) synthesis tasks. The MRF regularizer prevents over-excitation artifacts and reduces implausible feature mixtures common to previous dCNN inversion approaches, permitting synthezing photographic content with increased visual plausibility. Unlike standard MRF-based texture synthesis, the combined system can both match and adapt local features with considerable variability, yielding results far out of reach of classic generative MRF methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Combining_Markov_Random_CVPR_2016_paper.pdf",
        "aff": "Mainz University; Mainz University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 817379,
        "gs_citation": 942,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14693487110597042194&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "uni-mainz.de;uni-mainz.de",
        "email": "uni-mainz.de;uni-mainz.de",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Combining_Markov_Random_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Johannes Gutenberg University Mainz",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-mainz.de/",
        "aff_unique_abbr": "JGU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Compact Bilinear Pooling",
        "session": "Feature Extraction and Description",
        "status": "Poster",
        "track": "main",
        "pid": "34",
        "author_site": "Yang Gao, Oscar Beijbom, Ning Zhang, Trevor Darrell",
        "author": "Yang Gao; Oscar Beijbom; Ning Zhang; Trevor Darrell",
        "abstract": "Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition. However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis. We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions. Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system. The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods. Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gao_Compact_Bilinear_Pooling_CVPR_2016_paper.pdf",
        "aff": "EECS, UC Berkeley; EECS, UC Berkeley; Snapchat Inc. + EECS, UC Berkeley; EECS, UC Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 912486,
        "gs_citation": 1102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9277709843201647782&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;snapchat.com;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;snapchat.com;eecs.berkeley.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gao_Compact_Bilinear_Pooling_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1+0;0",
        "aff_unique_norm": "University of California, Berkeley;Snapchat",
        "aff_unique_dep": "Electrical Engineering and Computer Sciences;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.snapchat.com",
        "aff_unique_abbr": "UC Berkeley;Snapchat",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Comparative Deep Learning of Hybrid Representations for Image Recommendations",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "31",
        "author_site": "Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, Houqiang Li",
        "author": "Chenyi Lei; Dong Liu; Weiping Li; Zheng-Jun Zha; Houqiang Li",
        "abstract": "In many image-related tasks, learning expressive and discriminative representations of images is essential, and deep learning has been studied for automating the learning of such representations. Some user-centric tasks, such as image recommendations, call for effective representations of not only images but also preferences and intents of users over images. Such representations are termed hybrid and addressed via a deep learning approach in this paper. We design a dual-net deep network, in which the two sub-networks map input images and preferences of users into a same latent semantic space, and then the distances between images and users in the latent space are calculated to make decisions. We further propose a comparative deep learning (CDL) method to train the deep network, using a pair of images compared against one user to learn the pattern of their relative distances. The CDL embraces much more training data than naive deep learning, and thus achieves superior performance than the latter, with no cost of increasing network complexity. Experimental results with real-world data sets for image recommendations have shown the proposed dual-net network and CDL greatly outperform other state-of-the-art image recommendation solutions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lei_Comparative_Deep_Learning_CVPR_2016_paper.pdf",
        "aff": "CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei 230027, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei 230027, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei 230027, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei 230027, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei 230027, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 10974030,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9549832290163637065&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mail.ustc.edu.cn;ustc.edu.cn;ustc.edu.cn;ustc.edu.cn;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;ustc.edu.cn;ustc.edu.cn;ustc.edu.cn;ustc.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lei_Comparative_Deep_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hefei",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Composition-Preserving Deep Photo Aesthetics Assessment",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "53",
        "author_site": "Long Mai, Hailin Jin, Feng Liu",
        "author": "Long Mai; Hailin Jin; Feng Liu",
        "abstract": "Photo aesthetics assessment is challenging. Deep convolutional neural network (ConvNet) methods have recently shown promising results for aesthetics assessment. The performance of these deep ConvNet methods, however, is often compromised by the constraint that the neural network only takes the fixed-size input. To accommodate this requirement, input images need to be transformed via cropping, scaling, or padding, which often damages image composition, reduces image resolution, or causes image distortion, thus compromising the aesthetics of the original images. In this paper, we present a composition-preserving deep ConvNet method that directly learns aesthetics features from the original input images without any image transformations. Specifically, our method adds an adaptive spatial pooling layer upon the regular convolution and pooling layers to directly handle input images with original sizes and aspect ratios. To allow for multi-scale feature extraction, we develop the Multi-Net Adaptive Spatial Pooling ConvNet architecture which consists of multiple sub-networks with different adaptive spatial pooling sizes and leverage a scene-based aggregation layer to effectively combine the predictions from multiple sub-networks. Our experiments on the large-scale aesthetics assessment benchmark (AVA) demonstrate that our method can significantly improve the state-of-the-art results in photo aesthetics assessment.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mai_Composition-Preserving_Deep_Photo_CVPR_2016_paper.pdf",
        "aff": "Portland State University; Adobe Research; Portland State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1627503,
        "gs_citation": 340,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9462814290043870105&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.pdx.edu;adobe.com;cs.pdx.edu",
        "email": "cs.pdx.edu;adobe.com;cs.pdx.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mai_Composition-Preserving_Deep_Photo_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Portland State University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.pdx.edu;https://research.adobe.com",
        "aff_unique_abbr": "PSU;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Computational Imaging for VLBI Image Reconstruction",
        "session": "Vision With Alternative Sensors",
        "status": "Oral",
        "track": "main",
        "pid": "17",
        "author_site": "Katherine L. Bouman, Michael D. Johnson, Daniel Zoran, Vincent L. Fish, Sheperd S. Doeleman, William T. Freeman",
        "author": "Katherine L. Bouman; Michael D. Johnson; Daniel Zoran; Vincent L. Fish; Sheperd S. Doeleman; William T. Freeman",
        "abstract": "Very long baseline interferometry (VLBI) is a technique for imaging celestial radio emissions by simultaneously observing a source from telescopes distributed across Earth. The challenges in reconstructing images from fine angular resolution VLBI data are immense. The data is extremely sparse and noisy, thus requiring statistical image models such as those designed in the computer vision community. In this paper we present a novel Bayesian approach for VLBI image reconstruction. While other methods often require careful tuning and parameter selection for different types of data, our method (CHIRP) produces good results under different settings such as low SNR or extended emission. The success of our method is demonstrated on realistic synthetic experiments as well as publicly available real data. We present this problem in a way that is accessible to members of the community, and provide a dataset website (vlbiimaging.csail.mit.edu) that facilitates controlled comparisons across algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bouman_Computational_Imaging_for_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 883557,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11370937283059946781&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bouman_Computational_Imaging_for_CVPR_2016_paper.html"
    },
    {
        "title": "Conditional Graphical Lasso for Multi-Label Image Classification",
        "session": "Scene and Image Classification",
        "status": "Poster",
        "track": "main",
        "pid": "77",
        "author_site": "Qiang Li, Maoying Qiao, Wei Bian, Dacheng Tao",
        "author": "Qiang Li; Maoying Qiao; Wei Bian; Dacheng Tao",
        "abstract": "Multi-label image classification aims to predict multiple labels for a single image which contains diverse content. By utilizing label correlations, various techniques have been developed to improve classification performance. However, current existing methods either neglect image features when exploiting label correlations or lack the ability to learn image-dependent conditional label structures. In this paper, we develop conditional graphical Lasso (CGL) to handle these challenges. CGL provides a unified Bayesian framework for structure and parameter learning conditioned on image features. We formulate the multi-label prediction as CGL inference problem, which is solved by a mean field variational approach. Meanwhile, CGL learning is efficient due to a tailored proximal gradient procedure by applying the maximum a posterior (MAP) methodology. CGL performs competitively for multi-label image classification on benchmark datasets MULAN scene, PASCAL VOC 2007 and PASCAL VOC 2012, compared with the state-of-the-art multi-label classification algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Conditional_Graphical_Lasso_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1681512578006646118&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Conditional_Graphical_Lasso_CVPR_2016_paper.html"
    },
    {
        "title": "Conformal Surface Alignment With Optimal Mobius Search",
        "session": "Biomedical Image Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "27",
        "author_site": "Huu Le, Tat-Jun Chin, David Suter",
        "author": "Huu Le; Tat-Jun Chin; David Suter",
        "abstract": "Deformations of surfaces with the same intrinsic shape can often be described accurately by a conformal model. A major focus of computational conformal geometry is the estimation of the conformal mapping that aligns a given pair of object surfaces. The uniformization theorem en- ables this task to be acccomplished in a canonical 2D do- main, wherein the surfaces can be aligned using a M obius transformation. Current algorithms for estimating M obius transformations, however, often cannot provide satisfactory alignment or are computationally too costly. This paper in- troduces a novel globally optimal algorithm for estimating Mobius transformations to align surfaces that are topologi- cal discs. Unlike previous methods, the proposed algorithm deterministically calculates the best transformation, with- out requiring good initializations. Further, our algorithm is also much faster than previous techniques in practice. We demonstrate the efficacy of our algorithm on data commonly used in computational conformal geometry.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Le_Conformal_Surface_Alignment_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science, The University of Adelaide; School of Computer Science, The University of Adelaide; School of Computer Science, The University of Adelaide",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Le_Conformal_Surface_Alignment_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1670749,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17353340663299101654&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Le_Conformal_Surface_Alignment_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Adelaide",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Consensus of Non-Rigid Reconstructions",
        "session": "Non-Rigid Reconstruction and Motion Analysis",
        "status": "Oral",
        "track": "main",
        "pid": "13",
        "author_site": "Minsik Lee, Jungchan Cho, Songhwai Oh",
        "author": "Minsik Lee; Jungchan Cho; Songhwai Oh",
        "abstract": "Recently, there have been many progresses for the problem of non-rigid structure reconstruction based on 2D trajectories, but it is still challenging to deal with complex deformations or restricted view ranges. Promising alternatives are the piecewise reconstruction approaches, which divide trajectories into several local parts and stitch their individual reconstructions to produce an entire 3D structure. These methods show the state-of-the-art performance, however, most of them are specialized for relatively smooth surfaces and some are quite complicated. Meanwhile, it has been reported numerously in the field of pattern recognition that obtaining consensus from many weak hypotheses can give a strong, powerful result. Inspired by these reports, in this paper, we push the concept of part-based reconstruction to the limit: Instead of considering the parts as explicitly-divided local patches, we draw a large number of small random trajectory sets. From their individual reconstructions, we pull out a statistic of each 3D point to retrieve a strong reconstruction, of which the procedure can be expressed as a sparse l_1-norm minimization problem. In order to resolve the reflection ambiguity between weak (and possibly bad) reconstructions, we propose a novel optimization framework which only involves a single eigenvalue decomposition. The proposed method can be applied to any type of data and outperforms the existing methods for the benchmark sequences, even though it is composed of a few, simple steps. Furthermore, it is easily parallelizable, which is another advantage.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lee_Consensus_of_Non-Rigid_CVPR_2016_paper.pdf",
        "aff": "Division of EE, Hanyang University, Korea\u2020; Department of ECE, ASRI, Seoul National University, Korea\u2021; Department of ECE, ASRI, Seoul National University, Korea\u2021",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Lee_Consensus_of_Non-Rigid_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 4943570,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5222607952476777933&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "hanyang.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "hanyang.ac.kr;snu.ac.kr;snu.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lee_Consensus_of_Non-Rigid_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Hanyang University;Seoul National University",
        "aff_unique_dep": "Division of EE;Department of Electrical and Computer Engineering",
        "aff_unique_url": "http://www.hanyang.ac.kr;https://www.snu.ac.kr",
        "aff_unique_abbr": "HYU;SNU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Seoul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Consistency of Silhouettes and Their Duals",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "34",
        "author_site": "Matthew Trager, Martial Hebert, Jean Ponce",
        "author": "Matthew Trager; Martial Hebert; Jean Ponce",
        "abstract": "Silhouettes provide rich information on three-dimensional shape, since the intersection of the associated visual cones generates the \"visual hull\", which encloses and approximates the original shape. However, not all silhouettes can actually be projections of the same object in space: this simple observation has implications in object recognition and multi-view segmentation, and has been (often implicitly) used as a basis for camera calibration. In this paper, we investigate the conditions for multiple silhouettes, or more generally arbitrary closed image sets, to be geometrically \"consistent\". We present this notion as a natural generalization of traditional multi-view geometry, which deals with consistency for points. After discussing some general results, we present a \"dual\" formulation for consistency, that gives conditions for a family of planar sets to be sections of the same object. Finally, we introduce a more general notion of silhouette \"compatibility\" under partial knowledge of the camera projections, and point out some possible directions for future research.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Trager_Consistency_of_Silhouettes_CVPR_2016_paper.pdf",
        "aff": "Inria; Carnegie Mellon University; \u00b4Ecole Normale Sup\u00e9rieure / PSL Research University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Trager_Consistency_of_Silhouettes_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1323656,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6719001906554951738&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "inria.fr;cmu.edu;ens.fr",
        "email": "inria.fr;cmu.edu;ens.fr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Trager_Consistency_of_Silhouettes_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "INRIA;Carnegie Mellon University;Ecole Normale Sup\u00e9rieure",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.inria.fr;https://www.cmu.edu;https://www.ens.fr",
        "aff_unique_abbr": "Inria;CMU;ENS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "France;United States"
    },
    {
        "title": "Constrained Deep Transfer Feature Learning and Its Applications",
        "session": "Transfer Learning",
        "status": "Poster",
        "track": "main",
        "pid": "59",
        "author_site": "Yue Wu, Qiang Ji",
        "author": "Yue Wu; Qiang Ji",
        "abstract": "Feature learning with deep models has achieved impressive results for both data representation and classification for various vision tasks. Deep feature learning, however,typically requires a large amount of training data, which may not be feasible for some application domains. Transfer learning can be one of the approaches to alleviate this problem by transferring data from data-rich source domain to data-scarce target domain. Existing transfer learning methods typically perform one-shot transfer learning and often ignore the specific properties that the transferred data must satisfy. To address these issues, we introduce a constrained deep transfer feature learning method to perform simultaneous transfer learning and feature learning by performing transfer learning in a progressively improving feature space iteratively in order to better narrow the gap between the target domain and the source domain for effective transfer of the data from source domain to target domain. Furthermore, we propose to exploit the target domain knowledge and incorporate such prior knowledge as constraint during transfer learning to ensure that the transferred data satisfies certain properties of the target domain.  To demonstrate the effectiveness of the proposed constrained deep transfer feature learning method, we apply it to thermal feature learning for eye detection by transferring from the visible domain. We also applied the proposed method for cross-view facial expression recognition as a second application. The experimental results demonstrate the effectiveness of the proposed method for both applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wu_Constrained_Deep_Transfer_CVPR_2016_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 742119,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13641085748241317363&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wu_Constrained_Deep_Transfer_CVPR_2016_paper.html"
    },
    {
        "title": "Constrained Joint Cascade Regression Framework for Simultaneous Facial Action Unit Recognition and Facial Landmark Detection",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "40",
        "author_site": "Yue Wu, Qiang Ji",
        "author": "Yue Wu; Qiang Ji",
        "abstract": "Cascade regression framework has been shown to be effective for facial landmark detection. It starts from an initial face shape and gradually predicts the face shape update from the local appearance features to generate the facial landmark locations in the next iteration until convergence. In this paper, we improve upon the cascade regression framework and propose the Constrained Joint Cascade Regression Framework (CJCRF) for simultaneous facial action unit recognition and facial landmark detection, which are two related face analysis tasks, but are seldomly exploited together. In particular, we first learn the relationships among facial action units and face shapes as a constraint. Then, in the proposed constrained joint cascade regression framework, with the help from the constraint, we iteratively update the facial landmark locations and the action unit activation probabilities until convergence. Experimental results demonstrate that the intertwined relationships of facial action units and face shapes boost the performances of both facial action unit recognition and facial landmark detection. The experimental results also demonstrate the effectiveness of the proposed method comparing to the state-of-the-art works.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wu_Constrained_Joint_Cascade_CVPR_2016_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2056382,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9310081334008291092&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wu_Constrained_Joint_Cascade_CVPR_2016_paper.html"
    },
    {
        "title": "Constructing Canonical Regions for Fast and Effective View Selection",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "35",
        "author_site": "Wencheng Wang, Tianhao Gao",
        "author": "Wencheng Wang; Tianhao Gao",
        "abstract": "In view selection, little work has been done for optimizing the search process; views must be densely distributed and checked individually. Thus, evaluating poor views wastes much time, and a poor view may even be misidentified as a best one. In this paper, we propose a search strategy by identifying the regions that are very likely to contain best views, referred to as canonical regions. It is by decomposing the model under investigation into meaningful parts, and using the canonical views of these parts to generate canonical regions. Applying existing view selection methods in the canonical regions can not only accelerate the search process but also guarantee the quality of obtained views. As a result, when our canonical regions are used for searching N-best views during comprehensive model analysis, we can attain greater search speed and reduce the number of views required. Experimental results show the effectiveness of our method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Constructing_Canonical_Regions_CVPR_2016_paper.pdf",
        "aff": "State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences + University of Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences + University of Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wang_Constructing_Canonical_Regions_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1090996,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11740712507976081427&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "ios.ac.cn;ios.ac.cn",
        "email": "ios.ac.cn;ios.ac.cn",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Constructing_Canonical_Regions_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Software;",
        "aff_unique_url": "http://www.ios.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Context Encoders: Feature Learning by Inpainting",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "30",
        "author_site": "Deepak Pathak, Philipp Kr\u00e4henbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros",
        "author": "Deepak Pathak; Philipp Krahenbuhl; Jeff Donahue; Trevor Darrell; Alexei A. Efros",
        "abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss.  The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "project": "the author\u2019s website",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1382952,
        "gs_citation": 7142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11404163095581754770&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "email": "cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu;cs.berkeley.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Pathak_Context_Encoders_Feature_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Context-Aware Gaussian Fields for Non-Rigid Point Set Registration",
        "session": "Image Alignment and Registration",
        "status": "Poster",
        "track": "main",
        "pid": "55",
        "author_site": "Gang Wang, Zhicheng Wang, Yufei Chen, Qiangqiang Zhou, Weidong Zhao",
        "author": "Gang Wang; Zhicheng Wang; Yufei Chen; Qiangqiang Zhou; Weidong Zhao",
        "abstract": "Point set registration (PSR) is a fundamental problem in computer vision and pattern recognition, and it has been successfully applied to many applications. Although widely used, existing PSR methods cannot align point sets robustly under degradations, such as deformation, noise, occlusion, outlier, rotation, and multi-view changes. This paper proposes context-aware Gaussian fields (CA-LapGF) for non-rigid PSR subject to global rigid and local non-rigid geometric constraints, where a laplacian regularized term is added to preserve the intrinsic geometry of the transformed set. CA-LapGF uses a robust objective function and the quasi-Newton algorithm to estimate the likely correspondences, and the non-rigid transformation parameters between two point sets iteratively. The CA-LapGF can estimate non-rigid transformations, which are mapped to reproducing kernel Hilbert spaces, accurately and robustly in the presence of degradations. Experimental results on synthetic and real images reveal that how CA-LapGF outperforms state-of-the-art algorithms for non-rigid PSR.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Context-Aware_Gaussian_Fields_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7712257,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8936818209151847446&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Context-Aware_Gaussian_Fields_CVPR_2016_paper.html"
    },
    {
        "title": "Contour Detection in Unstructured 3D Point Clouds",
        "session": "Image Processing and Restoration",
        "status": "Oral",
        "track": "main",
        "pid": "12",
        "author_site": "Timo Hackel, Jan D. Wegner, Konrad Schindler",
        "author": "Timo Hackel; Jan D. Wegner; Konrad Schindler",
        "abstract": "We describe a method to automatically detect contours, i.e. lines along which the surface orientation sharply changes, in large-scale outdoor point clouds. Contours are important intermediate features for structuring point clouds and converting them into high-quality surface or solid models, and are extensively used in graphics and mapping applications. Yet, detecting them in unstructured, inhomogeneous point clouds turns out to be surprisingly difficult, and existing line detection algorithms largely fail. We approach contour extraction as a two-stage discriminative learning problem. In the first stage, a contour score for each individual point is predicted with a binary classifier, using a set of features extracted from the point's neighborhood. The contour scores serve as a basis to construct an overcomplete graph of candidate contours. The second stage selects an optimal set of contours from the candidates. This amounts to a further binary classification in a higher-order MRF, whose cliques encode a preference for connected contours and penalize loose ends. The method can handle point clouds >10^7 points in a couple of minutes, and vastly outperforms a baseline that performs Canny-style edge detection on a range image representation of the point cloud.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hackel_Contour_Detection_in_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1416880,
        "gs_citation": 267,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16287403422208044581&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hackel_Contour_Detection_in_CVPR_2016_paper.html"
    },
    {
        "title": "Convexity Shape Constraints for Image Segmentation",
        "session": "Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "43",
        "author_site": "Loic A. Royer, David L. Richmond, Carsten Rother, Bjoern Andres, Dagmar Kainmueller",
        "author": "Loic A. Royer; David L. Richmond; Carsten Rother; Bjoern Andres; Dagmar Kainmueller",
        "abstract": "Segmenting an image into multiple components is a central task in computer vision. In many practical scenarios, prior knowledge about plausible components is available. Incorporating such prior knowledge into models and algorithms for image segmentation is highly desirable, yet can be non-trivial. In this work, we introduce a new approach that allows, for the first time, to constrain some or all components of a segmentation to have convex shapes. Specifically, we extend the Minimum Cost Multicut Problem by a class of constraints that enforce convexity. To solve instances of this NP-hard integer linear program to optimality, we separate the proposed constraints in the branch-and-cut loop of a state-of-the-art ILP solver. Results on photographs and micrographs demonstrate the effectiveness of the approach as well as its advantages over the state-of-the-art heuristic.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Royer_Convexity_Shape_Constraints_CVPR_2016_paper.pdf",
        "aff": "MPI-CBG; MPI-CBG; TU Dresden; MPI for Informatics; MPI-CBG",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1018553,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15832129550267721614&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 12,
        "aff_domain": "mpi-cbg.de; ; ; ;mpi-cbg.de",
        "email": "mpi-cbg.de; ; ; ;mpi-cbg.de",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Royer_Convexity_Shape_Constraints_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Max Planck Institute of Molecular Cell Biology and Genetics;Technische Universit\u00e4t Dresden;Max Planck Institute for Informatics",
        "aff_unique_dep": ";;Informatics",
        "aff_unique_url": "https://www.mpi-cbg.de;https://www.tu-dresden.de;https://www.mpi-inf.mpg.de",
        "aff_unique_abbr": "MPI-CBG;TUD;MPII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Convolutional Networks for Shape From Light Field",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "77",
        "author_site": "Stefan Heber, Thomas Pock",
        "author": "Stefan Heber; Thomas Pock",
        "abstract": "Convolutional Neural Networks (CNNs) have recently been successfully applied to various Computer Vision (CV) applications. In this paper we utilize CNNs to predict depth information for given Light Field (LF) data. The proposed method learns an end-to-end mapping between the 4D light field and a representation of the corresponding 4D depth field in terms of 2D hyperplane orientations. The obtained prediction is then further refined in a post processing step by applying a higher-order regularization. Existing LF datasets are not sufficient for the purpose of the training scheme tackled in this paper. This is mainly due to the fact that the ground truth depth of existing datasets is inaccurate and/or the datasets are limited to a small number of LFs. This made it necessary to generate a new synthetic LF dataset, which is based on the raytracing software POV-Ray. This new dataset provides floating point accurate ground truth depth fields, and due to a random scene generator the dataset can be scaled as required.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Heber_Convolutional_Networks_for_CVPR_2016_paper.pdf",
        "aff": "Graz University of Technology; Graz University of Technology + Austrian Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3647598,
        "gs_citation": 151,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10959150280657382770&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at",
        "email": "icg.tugraz.at;icg.tugraz.at",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Heber_Convolutional_Networks_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Graz University of Technology;Austrian Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tugraz.at;https://www.ait.ac.at",
        "aff_unique_abbr": "TUGraz;AIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Austria"
    },
    {
        "title": "Convolutional Pose Machines",
        "session": "Human Pose Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "19",
        "author_site": "Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh",
        "author": "Shih-En Wei; Varun Ramakrishna; Takeo Kanade; Yaser Sheikh",
        "abstract": "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wei_Convolutional_Pose_Machines_CVPR_2016_paper.pdf",
        "aff": "The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wei_Convolutional_Pose_Machines_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2714417,
        "gs_citation": 3968,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16225467191686938474&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wei_Convolutional_Pose_Machines_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "The Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Convolutional Two-Stream Network Fusion for Video Action Recognition",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "47",
        "author_site": "Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman",
        "author": "Christoph Feichtenhofer; Axel Pinz; Andrew Zisserman",
        "abstract": "Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters; (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy; finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.pdf",
        "aff": "Graz University of Technology; Graz University of Technology; University of Oxford",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 961421,
        "gs_citation": 3662,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12361422087232908082&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "tugraz.at;tugraz.at;robots.ox.ac.uk",
        "email": "tugraz.at;tugraz.at;robots.ox.ac.uk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Graz University of Technology;University of Oxford",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tugraz.at;https://www.ox.ac.uk",
        "aff_unique_abbr": "TUGraz;Oxford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Austria;United Kingdom"
    },
    {
        "title": "Coordinating Multiple Disparity Proposals for Stereo Computation",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "25",
        "author_site": "Ang Li, Dapeng Chen, Yuanliu Liu, Zejian Yuan",
        "author": "Ang Li; Dapeng Chen; Yuanliu Liu; Zejian Yuan",
        "abstract": "While great progress has been made in stereo computation over the last decades, large textureless regions remain challenging. Segment-based methods can tackle this problem properly, but their performances are sensitive to the segmentation results. In this paper, we alleviate the sensitivity by generating multiple proposals on absolute and relative disparities from multi-segmentations. These proposals supply rich descriptions of surface structures. Especially, the relative disparity between distant pixels can encode the large structure, which is critical to handle the large texture-less regions. The proposals are coordinated by point-wise competition and pairwise collaboration within a MRF model. During inference, a dynamic programming is performed in different directions with various step sizes, so the long-range connections are better preserved. In the experiments, we carefully analyzed the effectiveness of the major components. Results on the 2014 Middlebury and KITTI 2015 stereo benchmark show that our method is comparable to state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Coordinating_Multiple_Disparity_CVPR_2016_paper.pdf",
        "aff": "Xi\u2019an Jiaotong University, China; Xi\u2019an Jiaotong University, China; Xi\u2019an Jiaotong University, China; Xi\u2019an Jiaotong University, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4901768,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16431535189463541044&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "stu.xjtu.edu.cn;stu.xjtu.edu.cn;gmail.com;gmail.com",
        "email": "stu.xjtu.edu.cn;stu.xjtu.edu.cn;gmail.com;gmail.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Coordinating_Multiple_Disparity_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Xi'an Jiao Tong University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://en.xjtu.edu.cn/",
        "aff_unique_abbr": "XJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Copula Ordinal Regression for Joint Estimation of Facial Action Unit Intensity",
        "session": "Face Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "38",
        "author_site": "Robert Walecki, Ognjen Rudovic, Vladimir Pavlovic, Maja Pantic",
        "author": "Robert Walecki; Ognjen Rudovic; Vladimir Pavlovic; Maja Pantic",
        "abstract": "Joint modeling of the intensity of facial action units (AUs) from face images is challenging due to the large number of AUs (30+) and their intensity levels (6). This is  in part due to the lack of suitable models that can efficiently handle such a large number of outputs/classes simultaneously, but also due to the lack of target data. For this reason, majority of the methods proposed resort to independent classifiers for the AU intensity. This is suboptimal for at least two reasons: the facial appearance of some AUs changes depending on the intensity of other AUs, and some AUs co-occur more often than others. Encoding this is expected to improve the estimation of target AU intensities, especially in the case of noisy image features, head-pose variations and imbalanced training data. To this end, we introduce a novel modeling framework, Copula Ordinal Regression (COR), that leverages the power of copula functions and CRFs, to detangle the probabilistic modeling of AU dependencies from the marginal modeling of the AU intensity. Consequently, the COR model achieves the joint learning and inference of intensities of multiple AUs, while being computationally tractable. We show on two challenging datasets of naturalistic facial expressions that the proposed approach consistently outperforms (i) independent modeling of AU intensities, and (ii) the state-of-the-art approach for the target task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Walecki_Copula_Ordinal_Regression_CVPR_2016_paper.pdf",
        "aff": "Department of Computing, Imperial College London, UK; Department of Computing, Imperial College London, UK; Department of Computer Science, Rutgers University, USA; EEMCS, University of Twente, The Netherlands",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2065718,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6211429006231621793&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;cs.rutgers.edu;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;cs.rutgers.edu;imperial.ac.uk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Walecki_Copula_Ordinal_Regression_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Imperial College London;Rutgers University;University of Twente",
        "aff_unique_dep": "Department of Computing;Department of Computer Science;EEMCS",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.rutgers.edu;https://www.utwente.nl",
        "aff_unique_abbr": "Imperial;Rutgers;UT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0;1;2",
        "aff_country_unique": "United Kingdom;United States;Netherlands"
    },
    {
        "title": "Coupled Harmonic Bases for Longitudinal Characterization of Brain Networks",
        "session": "Biomedical Image Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "28",
        "author_site": "Seong Jae Hwang, Nagesh Adluru, Maxwell D. Collins, Sathya N. Ravi, Barbara B. Bendlin, Sterling C. Johnson, Vikas Singh",
        "author": "Seong Jae Hwang; Nagesh Adluru; Maxwell D. Collins; Sathya N. Ravi; Barbara B. Bendlin; Sterling C. Johnson; Vikas Singh",
        "abstract": "There is a great deal of interest in using large scale brain imaging studies to understand how brain connectivity evolves over time for an individual and how it varies over different levels/quantiles of cognitive function. To do so, one typically performs so-called tractography procedures on diffusion MR brain images and derives measures of brain connectivity expressed as graphs. The nodes correspond to distinct brain regions and the edges encode the strength of the connection. The scientific interest is in characterizing the evolution of these graphs over time or from healthy individuals to diseased. We pose this important question in terms of the Laplacian of the connectivity graphs derived from various longitudinal or disease time points - quantifying its progression is then expressed in terms of coupling the harmonic bases of a full set of Laplacians. We derive a coupled system of generalized eigenvalue problems (and corresponding numerical optimization schemes) whose solution helps characterize the full life cycle of brain connectivity evolution in a given dataset. Finally, we show a set of results on a diffusion MR imaging dataset of middle aged people at risk for Alzheimer's disease (AD), who are cognitively healthy. In such asymptomatic adults, we find that a framework for characterizing brain connectivity evolution provides the ability to predict cognitive scores for individual subjects, and for estimating the progression of participant's brain connectivity into the future.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hwang_Coupled_Harmonic_Bases_CVPR_2016_paper.pdf",
        "aff": "Dept. of Computer Sciences, University of Wisconsin \u2013 Madison; Waisman Center, University of Wisconsin \u2013 Madison; Dept. of Computer Sciences, University of Wisconsin \u2013 Madison; Dept. of Computer Sciences, University of Wisconsin \u2013 Madison; William S. Middleton V A Hospital, Madison, WI; William S. Middleton V A Hospital, Madison, WI; Dept. of Biostatistics and Med. Informatics, University of Wisconsin \u2013 Madison+Dept. of Computer Sciences, University of Wisconsin \u2013 Madison",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2508789,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13214512227139816172&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hwang_Coupled_Harmonic_Bases_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;0;2;2;0+0",
        "aff_unique_norm": "University of Wisconsin\u2013Madison;University of Wisconsin \u2013 Madison;William S. Middleton Memorial Veterans Hospital",
        "aff_unique_dep": "Department of Computer Sciences;Waisman Center;",
        "aff_unique_url": "https://www.wisc.edu;https://www.waisman.wisc.edu;https://www.wisconsinva.gov/Madison/",
        "aff_unique_abbr": "UW\u2013Madison;;",
        "aff_campus_unique_index": "0;0;0;0;0;0;0+0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Cross Modal Distillation for Supervision Transfer",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "61",
        "author_site": "Saurabh Gupta, Judy Hoffman, Jitendra Malik",
        "author": "Saurabh Gupta; Judy Hoffman; Jitendra Malik",
        "abstract": "In this work we propose a technique that transfers supervision between images from different modalities. We use learned representations from a large labeled modality as supervisory signal for training representations for a new unlabeled paired modality. Our method enables learning of rich representations for unlabeled modalities and can be used as a pre-training procedure for new modalities with limited labeled data. We transfer supervision from labeled RGB images to unlabeled depth and optical flow images and demonstrate large improvements for both these cross modal supervision transfers.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gupta_Cross_Modal_Distillation_CVPR_2016_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "project": "",
        "github": "https://github.com/s-gupta/fast-rcnn/tree/distillation",
        "supp": "",
        "arxiv": "",
        "pdf_size": 868288,
        "gs_citation": 668,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5229292558359831784&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gupta_Cross_Modal_Distillation_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Cross-Stitch Networks for Multi-Task Learning",
        "session": "Statistical Methods and Transfer Learning",
        "status": "Spotlight",
        "track": "main",
        "pid": "22",
        "author_site": "Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, Martial Hebert",
        "author": "Ishan Misra; Abhinav Shrivastava; Abhinav Gupta; Martial Hebert",
        "abstract": "Multi-task learning in Convolutional Networks has displayed remarkable success in the field of recognition. This success can be largely attributed to learning shared representations from multiple supervisory tasks. However, existing multi-task approaches rely on enumerating multiple network architectures specific to the tasks at hand, that do not generalize. In this paper, we propose a principled approach to learn shared representations in ConvNets using multi-task learning. Specifically, we propose a new sharing unit: \"cross-stitch\" unit. These units combine the activations from multiple networks and can be trained end-to-end. A network with cross-stitch units can learn an optimal combination of shared and task-specific representations. Our proposed method generalizes across multiple tasks and shows dramatically improved performance over baseline methods for categories with few training examples.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Misra_Cross-Stitch_Networks_for_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3198911,
        "gs_citation": 1784,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9201269953233977173&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Misra_Cross-Stitch_Networks_for_CVPR_2016_paper.html"
    },
    {
        "title": "D3: Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images",
        "session": "Image Enhancement, Restoration, and Texture",
        "status": "Poster",
        "track": "main",
        "pid": "54",
        "author_site": "Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, Thomas S. Huang",
        "author": "Zhangyang Wang; Ding Liu; Shiyu Chang; Qing Ling; Yingzhen Yang; Thomas S. Huang",
        "abstract": "In this paper, we design a Deep Dual-Domain (D3) based fast restoration model to remove artifacts of JPEG compressed images. It leverages the large learning capacity of deep networks, as well as the problem-specific expertise that was hardly incorporated in the past design of deep architectures. For the latter, we take into consideration both the prior knowledge of the JPEG compression scheme, and the successful practice of the sparsity-based dual-domain approach. We further design the One-Step Sparse Inference (1-SI) module, as an efficient and light-weighted feed-forward approximation of sparse coding. Extensive experiments verify the superiority of the proposed D3 model over several state-of-the-art methods. Specifically, our best model is capable of outperforming the latest deep model for around 1 dB in PSNR, and is 30 times faster.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_D3_Deep_Dual-Domain_CVPR_2016_paper.pdf",
        "aff": "Beckman Institute, University of Illinois at Urbana-Champaign; Beckman Institute, University of Illinois at Urbana-Champaign; Beckman Institute, University of Illinois at Urbana-Champaign; Department of Automation, University of Science and Technology of China; Beckman Institute, University of Illinois at Urbana-Champaign; Beckman Institute, University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 741138,
        "gs_citation": 249,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5460802687666357032&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;mail.ustc.edu.cn;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;mail.ustc.edu.cn;illinois.edu;illinois.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_D3_Deep_Dual-Domain_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;University of Science and Technology of China",
        "aff_unique_dep": "Beckman Institute;Department of Automation",
        "aff_unique_url": "https://www.illinois.edu;http://www.ustc.edu.cn",
        "aff_unique_abbr": "UIUC;USTC",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "DAG-Recurrent Neural Networks For Scene Labeling",
        "session": "Semantic Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "64",
        "author_site": "Bing Shuai, Zhen Zuo, Bing Wang, Gang Wang",
        "author": "Bing Shuai; Zhen Zuo; Bing Wang; Gang Wang",
        "abstract": "In image labeling, local representations for image units (pixels, patches or superpixels) are usually generated from their surrounding image patches, thus long-range contextual information is not effectively encoded. In this paper, we introduce recurrent neural networks (RNNs) to address this issue. Specifically, directed acyclic graph RNNs (DAG-RNNs) are proposed to process DAG-structured images, which enables the network to model long-range semantic dependencies among image units. Our DAG-RNNs are capable of tremendously enhancing the discriminative power of local representations, which significantly benefits the local classification. Meanwhile, we propose a novel class weighting function that attends to rare classes, which phenomenally boosts the recognition accuracy for non-frequent classes. Integrating with convolution and deconvolution layers, our DAG-RNNs achieve new state-of-the-art results on the challenging SiftFlow, CamVid and Barcelona benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shuai_DAG-Recurrent_Neural_Networks_CVPR_2016_paper.pdf",
        "aff": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1009562,
        "gs_citation": 201,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11146704453202615802&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shuai_DAG-Recurrent_Neural_Networks_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "DCAN: Deep Contour-Aware Networks for Accurate Gland Segmentation",
        "session": "Biomedical Image Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "25",
        "author_site": "Hao Chen, Xiaojuan Qi, Lequan Yu, Pheng-Ann Heng",
        "author": "Hao Chen; Xiaojuan Qi; Lequan Yu; Pheng-Ann Heng",
        "abstract": "The morphology of glands has been used routinely by pathologists to assess the malignancy degree of adenocarcinomas. Accurate segmentation of glands from histology images is a crucial step to obtain reliable morphological statistics for quantitative diagnosis. In this paper, we proposed an efficient deep contour-aware network (DCAN) to solve this challenging problem under a unified multi-task learning framework. In the proposed network, multi-level contextual features from the hierarchical architecture are explored with auxiliary supervision for accurate gland segmentation. When incorporated with multi-task regularization during the training, the discriminative capability of intermediate features can be further improved. Moreover, our network can not only output accurate probability maps of glands, but also depict clear contours simultaneously for separating clustered objects, which further boosts the gland segmentation performance. This unified framework can be efficient when applied to large-scale histopathological data without resorting to additional steps to generate contours based on low-level cues for post-separating. Our method won the 2015 MICCAI Gland Segmentation Challenge out of 13 competitive teams, surpassing all the other methods by a significant margin.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_DCAN_Deep_Contour-Aware_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1541301,
        "gs_citation": 803,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6985234319667339053&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk;cse.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_DCAN_Deep_Contour-Aware_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection",
        "session": "Segmentation and Saliency",
        "status": "Poster",
        "track": "main",
        "pid": "73",
        "author_site": "Nian Liu, Junwei Han",
        "author": "Nian Liu; Junwei Han",
        "abstract": "Traditional1 salient object detection models often use hand-crafted features to formulate contrast and various prior knowledge, and then combine them artificially. In this work, we propose a novel end-to-end deep hierarchical saliency network (DHSNet) based on convolutional neural networks for detecting salient objects. DHSNet first makes a coarse global prediction by automatically learning various global structured saliency cues, including global contrast, objectness, compactness, and their optimal combination. Then a novel hierarchical recurrent convolutional neural network (HRCNN) is adopted to further hierarchically and progressively refine the details of saliency maps step by step via integrating local context information. The whole architecture works in a global to local and coarse to fine manner. DHSNet is directly trained using whole images and corresponding ground truth saliency masks. When testing, saliency maps can be generated by directly and efficiently feedforwarding testing images through the network, without relying on any other techniques. Evaluations on four benchmark datasets and comparisons with other 11 state-of-the-art algorithms demonstrate that DHSNet not only shows its significant superiority in terms of performance, but also achieves a real-time speed of 23 FPS on modern GPUs.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1208938,
        "gs_citation": 986,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=334541374814635192&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.html"
    },
    {
        "title": "DeLay: Robust Spatial Layout Estimation for Cluttered Indoor Scenes",
        "session": "Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "66",
        "author_site": "Saumitro Dasgupta, Kuan Fang, Kevin Chen, Silvio Savarese",
        "author": "Saumitro Dasgupta; Kuan Fang; Kevin Chen; Silvio Savarese",
        "abstract": "We consider the problem of estimating the spatial layout of an indoor scene from a monocular RGB image, modeled as the projection of a 3D cuboid. Existing solutions to this problem often rely strongly on hand-engineered features and vanishing point detection, which are prone to failure in the presence of clutter. In this paper, we present a method that uses a fully convolutional neural network (FCNN) in conjunction with a novel optimization framework for generating layout estimates. We demonstrate that our method is robust in the presence of clutter and handles a wide range of highly challenging scenes. We evaluate our method on two standard benchmarks and show that it achieves state of the art results, outperforming previous methods by a wide margin.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Dasgupta_DeLay_Robust_Spatial_CVPR_2016_paper.pdf",
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1194466,
        "gs_citation": 189,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8605656638940436196&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;stanford.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Dasgupta_DeLay_Robust_Spatial_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Canonical Time Warping",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "60",
        "author_site": "George Trigeorgis, Mihalis A. Nicolaou, Stefanos Zafeiriou, Bj\u00f6rn W. Schuller",
        "author": "George Trigeorgis; Mihalis A. Nicolaou; Stefanos Zafeiriou; Bjorn W. Schuller",
        "abstract": "Machine learning algorithms for the analysis of time-series often depend on the assumption that the utilised data are temporally aligned. Any temporal discrepancies arising in the data is certain to lead to ill-generalisable models, which in turn fail to correctly capture the properties of the task at hand. The temporal alignment of time-series is thus a crucial challenge manifesting in a multitude of applications. Nevertheless, the vast majority of algorithms oriented towards the temporal alignment of time-series are applied directly on the observation space, or utilise simple linear projections.  Thus, they fail to capture complex, hierarchical non-linear representations which may prove to be beneficial towards the task of temporal alignment, particularly when dealing with multi-modal data (e.g., aligning visual and acoustic information). To this end, we present the Deep Canonical Time Warping (DCTW), a method which automatically learns complex non-linear representations of multiple time-series, generated such that (i) they are highly correlated, and (ii) temporally in alignment. By means of experiments on four real datasets, we show that the representations learnt via the proposed DCTW significantly outperform state-of-the-art methods in temporal alignment, elegantly handling scenarios with highly heterogeneous features, such as the temporal alignment of acoustic and visual features.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Trigeorgis_Deep_Canonical_Time_CVPR_2016_paper.pdf",
        "aff": "Imperial College London, UK+Center for Machine Vision and Signal Analysis, University of Oulu, Finland; Goldsmiths, University of London, UK; Imperial College London, UK+Center for Machine Vision and Signal Analysis, University of Oulu, Finland; Imperial College London, UK",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Trigeorgis_Deep_Canonical_Time_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 790227,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15585827742968396160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "imperial.ac.uk;gold.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;gold.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Trigeorgis_Deep_Canonical_Time_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;2;0+1;0",
        "aff_unique_norm": "Imperial College London;University of Oulu;University of London",
        "aff_unique_dep": ";Center for Machine Vision and Signal Analysis;Goldsmiths",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.oulu.fi;https://www.gold.ac.uk",
        "aff_unique_abbr": "ICL;;Goldsmiths",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";London",
        "aff_country_unique_index": "0+1;0;0+1;0",
        "aff_country_unique": "United Kingdom;Finland"
    },
    {
        "title": "Deep Compositional Captioning: Describing Novel Object Categories Without Paired Training Data",
        "session": "Image Captioning and Question Answering",
        "status": "Oral",
        "track": "main",
        "pid": "1",
        "author_site": "Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond Mooney, Kate Saenko, Trevor Darrell",
        "author": "Lisa Anne Hendricks; Subhashini Venugopalan; Marcus Rohrbach; Raymond Mooney; Kate Saenko; Trevor Darrell",
        "abstract": "While recent deep neural network models have achieved promising results on the image captioning task, they rely largely on the availability of corpora with paired image and sentence captions to describe objects in context. In this work, we propose the Deep Compositional Captioner (DCC) to address the task of generating descriptions of novel objects which are not present in paired image-sentence datasets.  Our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically similar concepts.  Current deep caption models can only describe objects contained in paired image-sentence corpora, despite the fact that they are pre-trained with large object recognition datasets, namely ImageNet.  In contrast, our model can compose sentences that describe novel objects and their interactions with other objects. We demonstrate our model's ability to describe novel concepts by empirically evaluating its performance on MSCOCO and show qualitative results on ImageNet images of objects for which no paired image-caption data exist. Further, we extend our approach to generate descriptions of objects in video clips. Our results show that DCC has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hendricks_Deep_Compositional_Captioning_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Hendricks_Deep_Compositional_Captioning_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3766719,
        "gs_citation": 346,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16795690772151880119&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hendricks_Deep_Compositional_Captioning_CVPR_2016_paper.html"
    },
    {
        "title": "Deep Contrast Learning for Salient Object Detection",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "51",
        "author_site": "Guanbin Li, Yizhou Yu",
        "author": "Guanbin Li; Yizhou Yu",
        "abstract": "Salient object detection has recently witnessed substantial progress due to powerful features extracted using deep convolutional neural networks (CNNs). However, existing CNN-based methods operate at the patch level instead of the pixel level. Resulting saliency maps are typically blurry, especially near the boundary of salient objects. Furthermore, image patches are treated as independent samples even when they are overlapping, giving rise to significant redundancy in computation and storage. In this paper, we propose an end-to-end deep contrast network to overcome the aforementioned limitations. Our deep network consists of two complementary components, a pixel-level fully convolutional stream and a segment-wise spatial pooling stream. The first stream directly produces a saliency map with pixel-level accuracy from an input image. The second stream extracts segment-wise features very efficiently, and better models saliency discontinuities along object boundaries. Finally, a fully connected CRF model can be optionally incorporated to improve spatial coherence and contour localization in the fused result from these two streams. Experimental results demonstrate that our deep model significantly improves the state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Deep_Contrast_Learning_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, The University of Hong Kong; Department of Computer Science, The University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Li_Deep_Contrast_Learning_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2749945,
        "gs_citation": 947,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9364796585299081371&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "cs.hku.hk;cs.hku.hk",
        "email": "cs.hku.hk;cs.hku.hk",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Deep_Contrast_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Hong Kong",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.hku.hk",
        "aff_unique_abbr": "HKU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Decision Network for Multi-Class Image Classification",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "80",
        "author_site": "Venkatesh N. Murthy, Vivek Singh, Terrence Chen, R. Manmatha, Dorin Comaniciu",
        "author": "Venkatesh N. Murthy; Vivek Singh; Terrence Chen; R. Manmatha; Dorin Comaniciu",
        "abstract": "In this paper, we present a novel Deep Decision Network (DDN) that provides an alternative approach towards building an efficient deep learning network. During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. This results in a tree-like structured network driven by the data. The proposed method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others. DDN also has the ability to make early decisions thus making it suitable for time-sensitive applications.  We validate DDN on two publicly available benchmark datasets: CIFAR-10 and CIFAR-100 and it yields state-of-the-art classification performance on both the datasets. The proposed algorithm has no limitations to be applied to any generic classification problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Murthy_Deep_Decision_Network_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science, University of Massachusetts, Amherst, MA, USA; School of Computer Science, University of Massachusetts, Amherst, MA, USA + Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA; Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA; School of Computer Science, University of Massachusetts, Amherst, MA, USA; Medical Imaging Technologies, Siemens Healthcare, Princeton, NJ, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 719946,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17217156280010685089&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.umass.edu;siemens.com;siemens.com;cs.umass.edu;siemens.com",
        "email": "cs.umass.edu;siemens.com;siemens.com;cs.umass.edu;siemens.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Murthy_Deep_Decision_Network_CVPR_2016_paper.html",
        "aff_unique_index": "0;0+1;1;0;1",
        "aff_unique_norm": "University of Massachusetts Amherst;Siemens Healthcare",
        "aff_unique_dep": "School of Computer Science;Medical Imaging Technologies",
        "aff_unique_url": "https://www.umass.edu;https://www.siemens-healthineers.com",
        "aff_unique_abbr": "UMass Amherst;Siemens",
        "aff_campus_unique_index": "0;0+1;1;0;1",
        "aff_campus_unique": "Amherst;Princeton",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1d7e17299a",
        "title": "Deep Exemplar 2D-3D Detection by Adapting From Real to Rendered Views",
        "site": "https://openaccess.thecvf.com/content_cvpr_2016/html/Massa_Deep_Exemplar_2D-3D_CVPR_2016_paper.html",
        "author": "Francisco Massa; Bryan C. Russell; Mathieu Aubry",
        "abstract": "This paper presents an end-to-end convolutional neural network (CNN) for 2D-3D exemplar detection. We demonstrate that the ability to adapt the features of natural images to better align with those of CAD rendered views is critical to the success of our technique. We show that the adaptation can be learned by compositing rendered views of textured object models on natural images. Our approach can be naturally incorporated into a CNN detection pipeline and extends the accuracy and speed benefits from recent advances in deep learning to 2D-3D exemplar detection. We applied our method to two tasks: instance detection, where we evaluated on the IKEA dataset, and object category detection, where we out-perform Aubry et al. for \"chair\" detection on a subset of the Pascal VOC dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Massa_Deep_Exemplar_2D-3D_CVPR_2016_paper.pdf",
        "aff": "\u00b4Ecole des Ponts ParisTech\u2217; Adobe Research; \u00b4Ecole des Ponts ParisTech\u2217+UC Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1793249,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5090145128798365558&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "; ; ",
        "email": "; ; ",
        "author_num": 3,
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "Ecole des Ponts ParisTech;Adobe;University of California, Berkeley",
        "aff_unique_dep": ";Adobe Research;",
        "aff_unique_url": "https://www.ponts.org;https://research.adobe.com;https://www.berkeley.edu",
        "aff_unique_abbr": "ENPC;Adobe;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;1;0+1",
        "aff_country_unique": "France;United States"
    },
    {
        "title": "Deep Gaussian Conditional Random Field Network: A Model-Based Deep Network for Discriminative Denoising",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "27",
        "author_site": "Raviteja Vemulapalli, Oncel Tuzel, Ming-Yu Liu",
        "author": "Raviteja Vemulapalli; Oncel Tuzel; Ming-Yu Liu",
        "abstract": "We propose a novel end-to-end trainable deep network architecture for image denoising based on a Gaussian Conditional Random Field (GCRF) model. In contrast to the existing discriminative denoising methods that train a separate model for each individual noise level, the proposed deep network explicitly models the input noise variance and hence is capable of handling a range of noise levels. Our deep network, which we refer to as deep GCRF network, consists of two sub-networks: (i) a parameter generation network that generates the pairwise potential parameters based on the noisy input image, and  ii) an inference network whose layers perform the computations involved in an iterative GCRF inference procedure. We train two deep GCRF networks (each network operates over a range of noise levels: one for low input noise levels and one for high input noise  levels) discriminatively by maximizing the peak signal-to-noise ratio measure. Experiments on Berkeley segmentation and PASCALVOC datasets show that the proposed approach produces results on par with the state-of-the-art without training a separate network for each individual noise level.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Vemulapalli_Deep_Gaussian_Conditional_CVPR_2016_paper.pdf",
        "aff": "Center for Automation Research, UMIACS, University of Maryland, College Park; Mitsubishi Electric Research Laboratories, Cambridge, MA; Mitsubishi Electric Research Laboratories, Cambridge, MA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Vemulapalli_Deep_Gaussian_Conditional_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 541550,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1445175183565345251&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "umd.edu;merl.com;merl.com",
        "email": "umd.edu;merl.com;merl.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Vemulapalli_Deep_Gaussian_Conditional_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Maryland, College Park;Mitsubishi Electric Research Laboratories",
        "aff_unique_dep": "Center for Automation Research, UMIACS;",
        "aff_unique_url": "https://www.umd.edu;https://www.merl.com",
        "aff_unique_abbr": "UMD;MERL",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "College Park;Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled",
        "session": "Video Understanding",
        "status": "Oral",
        "track": "main",
        "pid": "1",
        "author_site": "Oscar Koller, Hermann Ney, Richard Bowden",
        "author": "Oscar Koller; Hermann Ney; Richard Bowden",
        "abstract": "This work presents a new approach to learning a frame-based classifier on weakly labelled sequence data by embedding a CNN within an iterative EM algorithm. This allows the CNN to be trained on a vast number of example images when only loose sequence level information is available for the source videos. Although we demonstrate this in the context of hand shape recognition, the approach has wider application to any video recognition task where frame level labelling is not available. The iterative EM algorithm leverages the discriminative ability of the CNN to iteratively refine the frame level annotation and subsequent training of the CNN. By embedding the classifier within an EM framework the CNN can easily be trained on 1 million hand images. We demonstrate that the final classifier generalises over both individuals and data sets. The algorithm is evaluated on over 3000 manually labelled hand shape images of 60 different classes which will be released to the community. Furthermore, we demonstrate its use in continuous sign language recognition on two publicly available large sign language data sets, where it outperforms the current state-of-the-art by a large margin. To our knowledge no previous work has explored expectation maximization without Gaussian mixture models to exploit weak sequence labels for sign language recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Koller_Deep_Hand_How_CVPR_2016_paper.pdf",
        "aff": "Human Language Technology & Pattern Recog., RWTH Aachen University, Germany; Human Language Technology & Pattern Recog., RWTH Aachen University, Germany; Centre for Vision Speech & Signal Processing, University of Surrey, UK",
        "project": "http://www.hltpr.rwth-aachen.de/~koller/1miohands",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 911939,
        "gs_citation": 370,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9447690369747858770&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "cs.rwth-aachen.de;cs.rwth-aachen.de;surrey.ac.uk",
        "email": "cs.rwth-aachen.de;cs.rwth-aachen.de;surrey.ac.uk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Koller_Deep_Hand_How_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "RWTH Aachen University;University of Surrey",
        "aff_unique_dep": "Human Language Technology & Pattern Recognition;Centre for Vision Speech & Signal Processing",
        "aff_unique_url": "https://www.rwth-aachen.de;https://www.surrey.ac.uk",
        "aff_unique_abbr": "RWTH;Surrey",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Aachen;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "title": "Deep Interactive Object Selection",
        "session": "Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "40",
        "author_site": "Ning Xu, Brian Price, Scott Cohen, Jimei Yang, Thomas S. Huang",
        "author": "Ning Xu; Brian Price; Scott Cohen; Jimei Yang; Thomas S. Huang",
        "abstract": "Interactive object selection is a very important research problem and has many applications. Previous algorithms require substantial user interactions to estimate the foreground and background distributions. In this paper, we present a novel deep-learning-based algorithm which has much better understanding of objectness and can reduce user interactions to just a few clicks. Our algorithm transforms user-provided positive and negative clicks into two Euclidean distance maps which are then concatenated with the RBG channels of images to compose (image, user interactions) pairs. We generate many of such pairs by combining several random sampling strategies to model users' click patterns and use them to finetune deep Fully Convolutional Networks (FCNs). Finally the output probability maps of our FCN-8s model is integrated with graph cut optimization to refine the boundary segments. Our model is trained on the PASCAL segmentation dataset and evaluated on other datasets with different object classes. Experimental results on both seen and unseen objects clearly demonstrate that our algorithm has a good generalization ability and is superior to all existing interactive object selection approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_Deep_Interactive_Object_CVPR_2016_paper.pdf",
        "aff": "University of Illinois at Urbana-Champaign; Adobe Research; Adobe Research; Adobe Research; University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6630521,
        "gs_citation": 563,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4681740942573351378&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "illinois.edu;adobe.com;adobe.com;adobe.com;illinois.edu",
        "email": "illinois.edu;adobe.com;adobe.com;adobe.com;illinois.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xu_Deep_Interactive_Object_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://illinois.edu;https://research.adobe.com",
        "aff_unique_abbr": "UIUC;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Metric Learning via Lifted Structured Feature Embedding",
        "session": "Statistical Methods and Transfer Learning",
        "status": "Spotlight",
        "track": "main",
        "pid": "23",
        "author_site": "Hyun Oh Song, Yu Xiang, Stefanie Jegelka, Silvio Savarese",
        "author": "Hyun Oh Song; Yu Xiang; Stefanie Jegelka; Silvio Savarese",
        "abstract": "Learning the distance metric between pairs of examples is of great importance for learning and visual recognition. With the remarkable success from the state of the art convolutional neural networks, recent works have shown promising results on discriminatively training the networks to learn semantic feature embeddings where similar examples are mapped close to each other and dissimilar examples are mapped farther apart. In this paper, we describe an algorithm for taking full advantage of the training batches in the neural network training by lifting the vector of pairwise distances within the batch to the matrix of pairwise distances. This step enables the algorithm to learn the state of the art feature embedding by optimizing a novel structured prediction objective for active hard negative mining on the lifted problem. Additionally, we collected Online Products dataset: 120k images of 23k classes of online products for metric learning. Our experiments on the CUB-200-2011, CARS196, and Online Products datasets demonstrate significant improvement over existing deep feature embedding methods on all experimented embedding sizes with the GoogLeNet network. The source code and the dataset are available at:  https://github.com/rksltnl/Deep-Metric-Learning-CVPR16",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Song_Deep_Metric_Learning_CVPR_2016_paper.pdf",
        "aff": "Stanford University; Stanford University; MIT; Stanford University",
        "project": "",
        "github": "https://github.com/rksltnl/Deep-Metric-Learning-CVPR16",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Song_Deep_Metric_Learning_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5363743,
        "gs_citation": 2134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2086461462991139373&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;csail.mit.edu;stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;csail.mit.edu;stanford.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Song_Deep_Metric_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Stanford University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://web.mit.edu",
        "aff_unique_abbr": "Stanford;MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Reflectance Maps",
        "session": "Vision For Graphics",
        "status": "Poster",
        "track": "main",
        "pid": "77",
        "author_site": "Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Efstratios Gavves, Tinne Tuytelaars",
        "author": "Konstantinos Rematas; Tobias Ritschel; Mario Fritz; Efstratios Gavves; Tinne Tuytelaars",
        "abstract": "Undoing the image formation process and therefore decomposing appearance into its intrinsic properties is a challenging task due to the under-constraint nature of this inverse problem. While significant progress has been made on inferring shape, materials and illumination from images only, progress in unconstrained setting is still limited. We propose a fully convolutional neural architecture to estimate reflectance maps of specular materials in natural lighting conditions. We achieve this in an end-to-end learning formulation that directly predicts a reflectance map from the image itself. We show how to improve estimates by facilitating additional supervision in an indirect scheme that first predicts surface orientation and afterwards predicts the reflectance map by a learning-based sparse data interpolation. In order to analyze performance on this difficult task, we propose a new challenge of Specular MAterials on SHapes with complex IllumiNation (SMASHINg) using both synthetic and real images. Furthermore, we show the application our method to a range of image-based editing tasks on real images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Rematas_Deep_Reflectance_Maps_CVPR_2016_paper.pdf",
        "aff": "KU Leuven; University College London; MPI Informatics; University of Amsterdam; KU Leuven",
        "project": "http://homes.cs.washington.edu/~krematas/DRM/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2489160,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10145400263176017027&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": "esat.kuleuven.be;ucl.ac.uk;mpi-inf.mpg.de;uva.nl;esat.kuleuven.be",
        "email": "esat.kuleuven.be;ucl.ac.uk;mpi-inf.mpg.de;uva.nl;esat.kuleuven.be",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Rematas_Deep_Reflectance_Maps_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "Katholieke Universiteit Leuven;University College London;Max Planck Institute for Informatics;University of Amsterdam",
        "aff_unique_dep": ";;Informatics;",
        "aff_unique_url": "https://www.kuleuven.be;https://www.ucl.ac.uk;https://www.mpi-inf.mpg.de;https://www.uva.nl",
        "aff_unique_abbr": "KU Leuven;UCL;MPII;UvA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;3;0",
        "aff_country_unique": "Belgium;United Kingdom;Germany;Netherlands"
    },
    {
        "title": "Deep Region and Multi-Label Learning for Facial Action Unit Detection",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "39",
        "author_site": "Kaili Zhao, Wen-Sheng Chu, Honggang Zhang",
        "author": "Kaili Zhao; Wen-Sheng Chu; Honggang Zhang",
        "abstract": "Region learning (RL) and multi-label learning (ML) have recently attracted increasing attentions in the field of facial Action Unit (AU) detection. Knowing that AUs are active on sparse facial regions, RL aims to identify these regions for a better specificity. On the other hand, a strong statistical evidence of AU correlations suggests that ML is a natural way to model the detection task. In this paper, we propose Deep Region and Multi-label Learning (DRML), a unified deep network that simultaneously addresses these two problems. One crucial aspect in DRML is a novel region layer that uses feed-forward functions to induce important facial regions, forcing the learned weights to capture structural information of the face. Our region layer serves as an alternative design between locally connected layers (i.e., confined kernels to individual pixels) and conventional convolution layers (i.e., shared kernels across an entire image). Unlike previous studies that solve RL and ML alternately, DRML by construction addresses both problems, allowing the two seemingly irrelevant problems to interact more directly. The complete network is end-to-end trainable, and automatically learns representations robust to variations inherent within a local region. Experiments on BP4D and DISFA benchmarks show that DRML performs the highest average F1-score and AUC within and across datasets in comparison with alternative methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhao_Deep_Region_and_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1537156,
        "gs_citation": 414,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17616558309133202899&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhao_Deep_Region_and_CVPR_2016_paper.html"
    },
    {
        "title": "Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "72",
        "author_site": "Hongye Liu, Yonghong Tian, Yaowei Yang, Lu Pang, Tiejun Huang",
        "author": "Hongye Liu; Yonghong Tian; Yaowei Yang; Lu Pang; Tiejun Huang",
        "abstract": "The growing explosion in the use of surveillance cameras in public security highlights the importance of vehicle search from a large-scale image or video database. However, compared with person re-identification or face recognition, vehicle search problem has long been neglected by researchers in vision community. This paper focuses on an interesting but challenging problem, vehicle re-identification (a.k.a precise vehicle search). We propose a Deep Relative Distance Learning (DRDL) method which exploits a two-branch deep convolutional network to project raw vehicle images into an Euclidean space where distance can be directly used to measure the similarity of arbitrary two vehicles. To further facilitate the future research on this problem, we also present a carefully-organized large-scale image database \"VehicleID\", which includes multiple images of the same vehicle captured by different real-world cameras in a city. We evaluate our DRDL method on our VehicleID dataset and another recently-released vehicle model classification dataset \"CompCars\" in three sets of experiments: vehicle re-identification, vehicle model verification and vehicle retrieval. Experimental results show that our method can achieve promising results and outperforms several state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Deep_Relative_Distance_CVPR_2016_paper.pdf",
        "aff": "National Engineering Laboratory for Video Technology, Peking University, Beijing + Cooperative Medianet Innovation Center, China; National Engineering Laboratory for Video Technology, Peking University, Beijing + Cooperative Medianet Innovation Center, China; School of Information and Electronics, Beijing Institute of Technology, Beijing; National Engineering Laboratory for Video Technology, Peking University, Beijing + Cooperative Medianet Innovation Center, China; National Engineering Laboratory for Video Technology, Peking University, Beijing + Cooperative Medianet Innovation Center, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1272240,
        "gs_citation": 900,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18165488441205385819&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "pku.edu.cn;pku.edu.cn;bit.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn;bit.edu.cn;pku.edu.cn;pku.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liu_Deep_Relative_Distance_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1;2;0+1;0+1",
        "aff_unique_norm": "Peking University;Cooperative Medianet Innovation Center;Beijing Institute of Technology",
        "aff_unique_dep": "National Engineering Laboratory for Video Technology;;School of Information and Electronics",
        "aff_unique_url": "http://www.pku.edu.cn;;http://www.bit.edu.cn",
        "aff_unique_abbr": "PKU;;BIT",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Residual Learning for Image Recognition",
        "session": "Object Recognition and Detection",
        "status": "Oral",
        "track": "main",
        "pid": "2",
        "author_site": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
        "author": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun",
        "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf",
        "aff": "Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research",
        "project": "http://image-net.org/challenges/LSVRC/2015/; http://mscoco.org/dataset/#detections-challenge2015",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/He_Deep_Residual_Learning_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 603123,
        "gs_citation": 269257,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9281510746729853742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 53,
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Saliency With Encoded Low Level Distance Map and High Level Features",
        "session": "Segmentation and Saliency",
        "status": "Poster",
        "track": "main",
        "pid": "71",
        "author_site": "Gayoung Lee, Yu-Wing Tai, Junmo Kim",
        "author": "Gayoung Lee; Yu-Wing Tai; Junmo Kim",
        "abstract": "Recent advances in saliency detection have utilized deep learning to obtain high level features to detect salient regions in a scene. They have demonstrated superior results over previous works that utilize hand-crafted low level features for saliency detection. In this paper, we demonstrate that the hand-crafted features can provide complementary effects to enhance performance of saliency detection that utilizes only high level features. Our method utilizes both high level and low level features for saliency detection under a unified deep learning framework. The high level features are extracted using the VGG-net, and the low level features are compared with other parts of an image to form a low level distance map. The low level distance map is then encoded using a CNN with multiple 1*1 convolutional and ReLU layers. We concatenate the encoded low level distance map and the high level features, and connect them to a fully connected neural network classifier to evaluate the saliency of a query region. Our experiments show that our method can further improve performance of the state-of-the-art deep learning based saliency detection methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lee_Deep_Saliency_With_CVPR_2016_paper.pdf",
        "aff": "KAIST; SenseTime Group Limited; KAIST",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4217128,
        "gs_citation": 570,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=572273089181625491&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "gmail.com;gmail.com;kaist.ac.kr",
        "email": "gmail.com;gmail.com;kaist.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lee_Deep_Saliency_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;SenseTime Group Limited",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.sensetime.com",
        "aff_unique_abbr": "KAIST;SenseTime",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "South Korea;China"
    },
    {
        "title": "Deep SimNets",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "25",
        "author_site": "Nadav Cohen, Or Sharir, Amnon Shashua",
        "author": "Nadav Cohen; Or Sharir; Amnon Shashua",
        "abstract": "We present a deep layered architecture that generalizes convolutional neural networks (ConvNets).  The architecture, called SimNets, is driven by two operators: (i) a similarity function that generalizes inner-product, and (ii) a log-mean-exp function called MEX that generalizes maximum and average.  The two operators applied in succession give rise to a standard neuron but in \"feature space\".  The feature spaces realized by SimNets depend on the choice of the similarity operator.  The simplest setting, which corresponds to a convolution, realizes the feature space of the Exponential kernel, while other settings realize feature spaces of more powerful kernels (Generalized Gaussian, which includes as special cases RBF and Laplacian), or even dynamically learned feature spaces (Generalized Multiple Kernel Learning).  As a result, the SimNet contains a higher abstraction level compared to a traditional ConvNet.  We argue that enhanced expressiveness is important when the networks are small due to run-time constraints (such as those imposed by mobile applications).  Empirical evaluation validates the superior expressiveness of SimNets, showing a significant gain in accuracy over ConvNets when computational resources at run-time are limited.  We also show that in large-scale settings, where computational complexity is less of a concern, the additional capacity of SimNets can be controlled with proper regularization, yielding accuracies comparable to state of the art ConvNets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cohen_Deep_SimNets_CVPR_2016_paper.pdf",
        "aff": "The Hebrew University of Jerusalem; The Hebrew University of Jerusalem; The Hebrew University of Jerusalem",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1498780,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12429527784042599833&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cohen_Deep_SimNets_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images",
        "session": "Object Detection 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "6",
        "author_site": "Shuran Song, Jianxiong Xiao",
        "author": "Shuran Song; Jianxiong Xiao",
        "abstract": "We focus on the task of amodal 3D object detection in RGB-D images, which aims to produce a 3D bounding box of an object in metric form at its full extent. We introduce Deep Sliding Shapes, a 3D ConvNet formulation that takes a 3D volumetric scene from a RGB-D image as input and outputs 3D object bounding boxes. In our approach, we propose the first 3D Region Proposal Network (RPN) to learn objectness from geometric shapes and the first joint Object Recognition Network (ORN) to extract geometric features in 3D and color features in 2D. In particular, we handle objects of various sizes by training an amodal RPN at two different scales and an ORN to regress 3D bounding boxes. Experiments show that our algorithm outperforms the state-of-the-art by 13.8 in mAP and is 200x faster than the original Sliding Shapes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Song_Deep_Sliding_Shapes_CVPR_2016_paper.pdf",
        "aff": "Princeton University; Princeton University",
        "project": "http://dss.cs.princeton.edu",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1852524,
        "gs_citation": 887,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9144348630084657649&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Song_Deep_Sliding_Shapes_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deep Structured Scene Parsing by Learning With Image Descriptions",
        "session": "Recognition and Labeling",
        "status": "Oral",
        "track": "main",
        "pid": "2",
        "author_site": "Liang Lin, Guangrun Wang, Rui Zhang, Ruimao Zhang, Xiaodan Liang, Wangmeng Zuo",
        "author": "Liang Lin; Guangrun Wang; Rui Zhang; Ruimao Zhang; Xiaodan Liang; Wangmeng Zuo",
        "abstract": "This paper addresses the problem of structured scene parsing, i.e., parsing the input scene into a configuration including hierarchical semantic objects with their interaction relations. We propose a deep architecture consisting of two networks: i) a convolutional neural network (CNN) extracting the image representation for pixelwise object labeling and ii) a recursive neural network (RNN) discovering the hierarchical object structure and the inter-object relations. Rather than relying on elaborative annotations (e.g., manually labeled semantic maps and relations), we train our deep model in a weakly-supervised manner by leveraging the descriptive sentences of the training images. Specifically, we decompose each sentence into a semantic tree consisting of nouns and verb phrases, and facilitate these trees discovering the configurations of the training images. Once these scene configurations are determined, then the parameters of both the CNN and RNN are updated accordingly by back propagation. The entire model training is accomplished through an Expectation-Maximization method. Extensive experiments suggest that our model is capable of producing meaningful and structured scene configurations and achieving more favorable scene labeling performance on PASCAL VOC 2012 over other state-of-the-art weakly-supervised methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lin_Deep_Structured_Scene_CVPR_2016_paper.pdf",
        "aff": "School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Technology, Harbin Institute of Technology, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 932581,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1611003948026832545&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "ieee.org; ;gmail.com; ;gmail.com; ",
        "email": "ieee.org; ;gmail.com; ;gmail.com; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lin_Deep_Structured_Scene_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0;1",
        "aff_unique_norm": "Sun Yat-sen University;Harbin Institute of Technology",
        "aff_unique_dep": "School of Data and Computer Science;School of Computer Science and Technology",
        "aff_unique_url": "http://www.sysu.edu.cn;http://www.hit.edu.cn/",
        "aff_unique_abbr": "SYSU;HIT",
        "aff_campus_unique_index": "0;0;0;0;0;1",
        "aff_campus_unique": "Guangzhou;Harbin",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Deep Supervised Hashing for Fast Image Retrieval",
        "session": "Image Indexing and Retrieval",
        "status": "Poster",
        "track": "main",
        "pid": "61",
        "author_site": "Haomiao Liu, Ruiping Wang, Shiguang Shan, Xilin Chen",
        "author": "Haomiao Liu; Ruiping Wang; Shiguang Shan; Xilin Chen",
        "abstract": "In this paper, we present a new hashing method to learn compact binary codes for highly efficient image retrieval on large-scale datasets. While the complex image appearance variations still pose a great challenge to reliable retrieval, in light of the recent progress of Convolutional Neural Networks (CNNs) in learning robust image representation on various vision tasks, this paper proposes a novel Deep Supervised Hashing (DSH) method to learn compact similarity-preserving binary code for the huge body of image data. Specifically, we devise a CNN architecture that takes pairs of images (similar/dissimilar) as training inputs and encourages the output of each image to approximate discrete values (e.g. +1/-1). To this end, a loss function is elaborately designed to maximize the discriminability of the output space by encoding the supervised information from the input image pairs, and simultaneously imposing regularization on the real-valued outputs to approximate the desired discrete values. For image retrieval, new-coming query images can be easily encoded by propagating through the network and then quantizing the network outputs to binary codes representation. Extensive experiments on two large scale datasets CIFAR-10 and NUS-WIDE show the promising performance of our method compared with the state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Deep_Supervised_Hashing_CVPR_2016_paper.pdf",
        "aff": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + University of Chinese Academy of Sciences, Beijing, 100049, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Liu_Deep_Supervised_Hashing_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1316105,
        "gs_citation": 1096,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17775547206178862839&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": "vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "vipl.ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liu_Deep_Supervised_Hashing_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "DeepCAMP: Deep Convolutional Action & Attribute Mid-Level Patterns",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "57",
        "author_site": "Ali Diba, Ali Mohammad Pazandeh, Hamed Pirsiavash, Luc Van Gool",
        "author": "Ali Diba; Ali Mohammad Pazandeh; Hamed Pirsiavash; Luc Van Gool",
        "abstract": "The recognition of human actions and the determination of human attributes are two tasks that call for fine-grained classification. Indeed, often rather small and inconspicuous objects and features  have to be detected to tell their classes apart. In order to deal with this challenge, we propose a  novel convolutional neural network that mines mid-level image patches that are sufficiently  dedicated to resolve the corresponding subtleties. In particular, we train a newly designed CNN (DeepPattern) that learns discriminative patch groups. There are two innovative aspects to this. On  the one hand we pay attention to contextual information in an original fashion. On the other hand,  we let an iteration of feature learning and patch clustering purify the set of dedicated patches that we use. We validate  our method for action classification on two challenging datasets: PASCAL VOC 2012 Action and Stanford  40 Actions, and for attribute recognition we use the Berkeley Attributes of People dataset. Our  discriminative mid-level mining CNN obtains state-of-the-art results on these datasets, without a  need for annotations about parts and poses.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Diba_DeepCAMP_Deep_Convolutional_CVPR_2016_paper.pdf",
        "aff": "ESAT-PSI, KU Leuven; SUT; University of Maryland Baltimore County; CVL, ETH Zurich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1480289,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11656382074763608783&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "esat.kuleuven.be;ee.sharif.edu;umbc.edu; ",
        "email": "esat.kuleuven.be;ee.sharif.edu;umbc.edu; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Diba_DeepCAMP_Deep_Convolutional_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "KU Leuven;SUT;University of Maryland, Baltimore County;ETH Zurich",
        "aff_unique_dep": "ESAT-PSI;;;Computer Vision Laboratory",
        "aff_unique_url": "https://www.kuleuven.be;;https://www.umbc.edu;https://www.ethz.ch",
        "aff_unique_abbr": "KU Leuven;;UMBC;ETHZ",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Baltimore County",
        "aff_country_unique_index": "0;2;3",
        "aff_country_unique": "Belgium;;United States;Switzerland"
    },
    {
        "title": "DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation",
        "session": "Human Pose Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "41",
        "author_site": "Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter V. Gehler, Bernt Schiele",
        "author": "Leonid Pishchulin; Eldar Insafutdinov; Siyu Tang; Bjoern Andres; Mykhaylo Andriluka; Peter V. Gehler; Bernt Schiele",
        "abstract": "This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Pishchulin_DeepCut_Joint_Subset_CVPR_2016_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Pishchulin_DeepCut_Joint_Subset_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2013355,
        "gs_citation": 1446,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2566321101337466182&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Pishchulin_DeepCut_Joint_Subset_CVPR_2016_paper.html"
    },
    {
        "title": "DeepFashion: Powering Robust Clothes Recognition and Retrieval With Rich Annotations",
        "session": "Fine Grained Categorization",
        "status": "Poster",
        "track": "main",
        "pid": "36",
        "author_site": "Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, Xiaoou Tang",
        "author": "Ziwei Liu; Ping Luo; Shi Qiu; Xiaogang Wang; Xiaoou Tang",
        "abstract": "Recent advances in clothes recognition have been driven by the construction of clothes datasets. Existing datasets are limited in the amount of annotations and are difficult to cope with the various challenges in real-world applications. In this work, we introduce DeepFashion, a large-scale clothes dataset with comprehensive annotations. It contains over 800,000 images, which are richly annotated with massive attributes, clothing landmarks, and correspondence of images taken under different scenarios including store, street snapshot, and consumer. Such rich annotations enable the development of powerful algorithms in clothes recognition and facilitating future researches. To demonstrate the advantages of DeepFashion, we propose a new deep model, namely FashionNet, which learns clothing features by jointly predicting clothing attributes and landmarks. The estimated landmarks are then employed to pool or gate the learned features. It is optimized in an iterative manner. Extensive experiments demonstrate the effectiveness of FashionNet and the usefulness of DeepFashion.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 2303,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9615172549462837472&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.html"
    },
    {
        "title": "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "34",
        "author_site": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard",
        "author": "Seyed-Mohsen Moosavi-Dezfooli; Alhussein Fawzi; Pascal Frossard",
        "abstract": "State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.pdf",
        "aff": "\u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne; \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne; \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne",
        "project": "",
        "github": "http://github.com/lts4/deepfool",
        "supp": "",
        "arxiv": "",
        "pdf_size": 502500,
        "gs_citation": 6723,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14711824744254857883&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lausanne",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "DeepHand: Robust Hand Pose Estimation by Completing a Matrix Imputed With Deep Features",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "39",
        "author_site": "Ayan Sinha, Chiho Choi, Karthik Ramani",
        "author": "Ayan Sinha; Chiho Choi; Karthik Ramani",
        "abstract": "We propose DeepHand to estimate the 3D pose of a hand using depth data from commercial 3D sensors. We discriminatively train convolutional neural networks to output a low dimensional activation feature given a depth map. This activation feature vector is representative of the global or local joint angle parameters of a hand pose. We efficiently identify 'spatial' nearest neighbors to the activation feature, from a database of features corresponding to synthetic depth maps, and store some 'temporal' neighbors from previous frames. Our matrix completion algorithm uses these 'spatio-temporal' activation features and the corresponding known pose parameter values to to estimate the unknown pose parameters of the input feature vector. Our database of activation features supplements large viewpoint coverage and our hierarchical estimation of pose parameters is robust to occlusions. We show that our approach  compares favorably to state-of-the-art methods while achieving real time performance (32 FPS) on a standard computer.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sinha_DeepHand_Robust_Hand_CVPR_2016_paper.pdf",
        "aff": "Purdue University; Purdue University; Purdue University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Sinha_DeepHand_Robust_Hand_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 880413,
        "gs_citation": 228,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12314931544263264267&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "purdue.edu;purdue.edu;purdue.edu",
        "email": "purdue.edu;purdue.edu;purdue.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sinha_DeepHand_Robust_Hand_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "DeepStereo: Learning to Predict New Views From the World's Imagery",
        "session": "3D Reconstruction",
        "status": "Spotlight",
        "track": "main",
        "pid": "24",
        "author_site": "John Flynn, Ivan Neulander, James Philbin, Noah Snavely",
        "author": "John Flynn; Ivan Neulander; James Philbin; Noah Snavely",
        "abstract": "Deep networks have recently enjoyed enormous success when applied to recognition and classification problems in computer vision [22, 32], but their use in graphics problems has been limited ([23, 7] are notable recent exceptions). In this work, we present a novel deep architecture that per- forms new view synthesis directly from pixels, trained from a large number of posed image sets. In contrast to tradi- tional approaches which consist of multiple complex stages of processing, each of which require careful tuning and can fail in unexpected ways, our system is trained end-to-end. The pixels from neighboring views of a scene are presented to the network which then directly produces the pixels of the unseen view. The benefits of our approach include gen- erality (we only require posed image sets and can easily apply our method to different domains), and high quality results on traditionally difficult scenes. We believe this is due to the end-to-end nature of our system which is able to plausibly generate pixels according to color, depth, and tex- ture priors learnt automatically from the training data. We show view interpolation results on imagery from the KITTI dataset [12], from data from [1] as well as on StreetView images. To our knowledge, our work is the first to apply deep learning to the problem of new view synthesis from sets of real-world, natural imagery.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Flynn_DeepStereo_Learning_to_CVPR_2016_paper.pdf",
        "aff": "Zoox\u2217; Google Inc.; Zoox\u2217; Google Inc.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2467823,
        "gs_citation": 798,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17671464158325028599&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "zoox.com;google.com;zoox.com;google.com",
        "email": "zoox.com;google.com;zoox.com;google.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Flynn_DeepStereo_Learning_to_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Zoox;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.zoox.com;https://www.google.com",
        "aff_unique_abbr": "Zoox;Google",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Deeply-Recursive Convolutional Network for Image Super-Resolution",
        "session": "Image Processing and Restoration",
        "status": "Oral",
        "track": "main",
        "pid": "15",
        "author_site": "Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee",
        "author": "Jiwon Kim; Jung Kwon Lee; Kyoung Mu Lee",
        "abstract": "We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/ vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive supervision and skip-connection. Our method outperforms previous methods by a large margin.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper.pdf",
        "aff": "Department of ECE, ASRI, Seoul National University, Korea; Department of ECE, ASRI, Seoul National University, Korea; Department of ECE, ASRI, Seoul National University, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1160865,
        "gs_citation": 3548,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=60804554760851082&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Dense Human Body Correspondences Using Convolutional Networks",
        "session": "Recognition and Parsing In 3D",
        "status": "Oral",
        "track": "main",
        "pid": "5",
        "author_site": "Lingyu Wei, Qixing Huang, Duygu Ceylan, Etienne Vouga, Hao Li",
        "author": "Lingyu Wei; Qixing Huang; Duygu Ceylan; Etienne Vouga; Hao Li",
        "abstract": "We propose a deep learning approach for finding dense correspondences between 3D scans of people. Our method requires only partial geometric information in the form of two depth maps or partial reconstructed surfaces, works for humans in arbitrary poses and wearing any clothing, does not require the two people to be scanned from similar viewpoints, and runs in real time. We use a deep convolutional neural network to train a feature descriptor on depth map pixels, but crucially, rather than training the network to solve the shape correspondence problem directly, we train it to solve a body region classification problem, modified to increase the smoothness of the learned descriptors near region boundaries. This approach ensures that nearby points on the human body are nearby in feature space, and vice versa, rendering the feature descriptor suitable for computing dense correspondences between the scans. We validate our method on real and synthetic data for both clothed and unclothed humans, and show that our correspondences are more robust than is possible with state-of-the-art unsupervised methods, and more accurate to those found using methods that require full watertight 3D geometry.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wei_Dense_Human_Body_CVPR_2016_paper.pdf",
        "aff": "University of Southern California; Toyota Technological Institute at Chicago; Adobe Research; University of Texas at Austin; University of Southern California",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wei_Dense_Human_Body_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1754880,
        "gs_citation": 242,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4305356127672700379&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "gmail.com;ttic.edu;adobe.com;cs.utexas.edu;hao-li.com",
        "email": "gmail.com;ttic.edu;adobe.com;cs.utexas.edu;hao-li.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wei_Dense_Human_Body_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;3;0",
        "aff_unique_norm": "University of Southern California;Toyota Technological Institute at Chicago;Adobe;University of Texas at Austin",
        "aff_unique_dep": ";;Adobe Research;",
        "aff_unique_url": "https://www.usc.edu;https://www.tti-chicago.org;https://research.adobe.com;https://www.utexas.edu",
        "aff_unique_abbr": "USC;TTI Chicago;Adobe;UT Austin",
        "aff_campus_unique_index": "0;1;3;0",
        "aff_campus_unique": "Los Angeles;Chicago;;Austin",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Dense Monocular Depth Estimation in Complex Dynamic Scenes",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "29",
        "author_site": "Ren\u00e9 Ranftl, Vibhav Vineet, Qifeng Chen, Vladlen Koltun",
        "author": "Rene Ranftl; Vibhav Vineet; Qifeng Chen; Vladlen Koltun",
        "abstract": "We present an approach to dense depth estimation from a single monocular camera that is moving through a dynamic scene. The approach produces a dense depth map from two consecutive frames. Moving objects are reconstructed along with the surrounding environment. We provide a novel motion segmentation algorithm that segments the optical flow field into a set of motion models, each with its own epipolar geometry. We then show that the scene can be reconstructed based on these motion models by optimizing a convex program. The optimization jointly reasons about the scales of different objects and assembles the scene in a common coordinate frame, determined up to a global scale. Experimental results demonstrate that the presented approach outperforms prior methods for monocular depth estimation in dynamic scenes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ranftl_Dense_Monocular_Depth_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2307998,
        "gs_citation": 235,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17712125170560579662&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ranftl_Dense_Monocular_Depth_CVPR_2016_paper.html"
    },
    {
        "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning",
        "session": "Image & Video Captioning and Descriptions",
        "status": "Oral",
        "track": "main",
        "pid": "2",
        "author_site": "Justin Johnson, Andrej Karpathy, Li Fei-Fei",
        "author": "Justin Johnson; Andrej Karpathy; Li Fei-Fei",
        "abstract": "We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, Stanford University; Department of Computer Science, Stanford University; Department of Computer Science, Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1762399,
        "gs_citation": 1539,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7906290079702357124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Depth From Semi-Calibrated Stereo and Defocus",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "74",
        "author_site": "Ting-Chun Wang, Manohar Srikanth, Ravi Ramamoorthi",
        "author": "Ting-Chun Wang; Manohar Srikanth; Ravi Ramamoorthi",
        "abstract": "In this work, we propose a multi-camera system where we combine a main high-quality camera with two low-res auxiliary cameras. The auxiliary cameras are well calibrated and act as a passive depth sensor by generating disparity maps. The main camera has an interchangeable lens and can produce good quality images at high resolution. Our goal is, given the low-res depth map from the auxiliary cameras, generate a depth map from the viewpoint of the main camera. The advantage of our system, compared to other systems such as light-field cameras or RGBD sensors, is the ability to generate a high-resolution color image with a complete depth map, without sacrificing resolution and with minimal auxiliary hardware.  Since the main camera has an interchangeable lens, it cannot be calibrated beforehand, and directly applying stereo matching on it and either of the auxiliary cameras often leads to unsatisfactory results. Utilizing both the calibrated cameras at once, we propose a novel approach to better estimate the disparity map of the main camera. Then by combining the defocus cue of the main camera, the disparity map can be further improved. We demonstrate the performance of our algorithm on various scenes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Depth_From_Semi-Calibrated_CVPR_2016_paper.pdf",
        "aff": "UC Berkeley; Nokia Technologies; UC San Diego",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3827759,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3871167659559672655&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "berkeley.edu;alum.mit.edu;cs.ucsd.edu",
        "email": "berkeley.edu;alum.mit.edu;cs.ucsd.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Depth_From_Semi-Calibrated_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of California, Berkeley;Nokia;University of California, San Diego",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.nokia.com;https://www.ucsd.edu",
        "aff_unique_abbr": "UC Berkeley;Nokia;UCSD",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Berkeley;;San Diego",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Finland"
    },
    {
        "title": "Detecting Events and Key Actors in Multi-Person Videos",
        "session": "Actions and Human Pose",
        "status": "Oral",
        "track": "main",
        "pid": "2",
        "author_site": "Vignesh Ramanathan, Jonathan Huang, Sami Abu-El-Haija, Alexander Gorban, Kevin Murphy, Li Fei-Fei",
        "author": "Vignesh Ramanathan; Jonathan Huang; Sami Abu-El-Haija; Alexander Gorban; Kevin Murphy; Li Fei-Fei",
        "abstract": "Multi-person event recognition is a challenging task, often with many people active in the scene but only a small subset contributing to an actual event. In this paper, we propose a model which learns to detect events in such videos while automatically \"attending\" to the people responsible for the event.  Our model does not use explicit annotations regarding who or where those people are during training and testing. In particular, we track people in videos and use a recurrent neural network (RNN) to represent the track features.  We learn time-varying attention weights to combine these features at each time-instant. The attended features are then processed using another RNN for event detection/classification.  Since most video datasets with multiple people are restricted to a small number of videos, we also collected a new basketball dataset comprising 257 basketball games with 14K event annotations corresponding to 11 event classes.  Our model outperforms state-of-the-art methods for both event classification  and  detection on this new dataset. Additionally, we show that the attention mechanism is able to consistently localize the relevant players.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ramanathan_Detecting_Events_and_CVPR_2016_paper.pdf",
        "aff": "Stanford University+Google; Google; Google; Google; Google; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2030956,
        "gs_citation": 284,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2028847705463907461&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "cs.stanford.edu;google.com;google.com;google.com;google.com;cs.stanford.edu",
        "email": "cs.stanford.edu;google.com;google.com;google.com;google.com;cs.stanford.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ramanathan_Detecting_Events_and_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;1;1;1;0",
        "aff_unique_norm": "Stanford University;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.stanford.edu;https://www.google.com",
        "aff_unique_abbr": "Stanford;Google",
        "aff_campus_unique_index": "0+1;1;1;1;1;0",
        "aff_campus_unique": "Stanford;Mountain View",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Detecting Migrating Birds at Night",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "64",
        "author_site": "Jia-Bin Huang, Rich Caruana, Andrew Farnsworth, Steve Kelling, Narendra Ahuja",
        "author": "Jia-Bin Huang; Rich Caruana; Andrew Farnsworth; Steve Kelling; Narendra Ahuja",
        "abstract": "Bird migration is a critical indicator of environmental health, biodiversity, and climate change. Existing techniques for monitoring bird migration are either expensive (e.g., satellite tracking), labor-intensive (e.g., moon watching), indirect and thus less accurate (e.g., weather radar), or intrusive (e.g., attaching geolocators on captured birds). In this paper, we present a vision-based system for detecting migrating birds in flight at night. Our system takes stereo videos of the night sky as inputs, detects multiple flying birds and estimates their orientations, speeds, and altitudes. The main challenge lies in detecting flying birds of unknown trajectories under high noise level due to the low-light environment. We address this problem by incorporating stereo constraints for rejecting physically implausible configurations and gathering evidence from two (or more) views. Specifically, we develop a robust stereo-based 3D line fitting algorithm for geometric verification and a deformable part response accumulation strategy for trajectory verification. We demonstrate the effectiveness of the proposed approach through quantitative evaluation of  real videos of birds migrating at night collected with near-infrared cameras.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Detecting_Migrating_Birds_CVPR_2016_paper.pdf",
        "aff": "University of Illinois Urbana-Champaign; Microsoft Research; Cornell Lab of Ornithology; Cornell Lab of Ornithology; University of Illinois Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1204584,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12654324277551570321&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "illinois.edu;microsoft.com;cornell.edu;cornell.edu;illinois.edu",
        "email": "illinois.edu;microsoft.com;cornell.edu;cornell.edu;illinois.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Huang_Detecting_Migrating_Birds_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;2;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Microsoft;Cornell University",
        "aff_unique_dep": ";Microsoft Research;Cornell Lab of Ornithology",
        "aff_unique_url": "https://illinois.edu;https://www.microsoft.com/en-us/research;https://www.birds.cornell.edu",
        "aff_unique_abbr": "UIUC;MSR;Cornell",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Detecting Repeating Objects Using Patch Correlation Analysis",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "69",
        "author_site": "Inbar Huberman, Raanan Fattal",
        "author": "Inbar Huberman; Raanan Fattal",
        "abstract": "In this paper we describe a new method for detecting and counting a repeating object in an image. While the method relies on a fairly sophisticated deformable part model, unlike existing techniques it estimates the model parameters in an unsupervised fashion thus alleviating the need for a user-annotated training data and avoiding the associated specificity. This automatic fitting process is carried out by exploiting the recurrence of small image patches associated with the repeating object and analyzing their spatial correlation. The analysis allows us to reject outlier patches, recover the visual and shape parameters of the part model, and detect the object instances efficiently.  In order to achieve a practical system which is able to cope with diverse images, we describe a simple and intuitive active-learning procedure that updates the object classification by querying the user on very few carefully chosen marginal classifications. Evaluation of the new method against the state-of-the-art techniques demonstrates its ability to achieve higher accuracy through a better user experience.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Huberman_Detecting_Repeating_Objects_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel; School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Huberman_Detecting_Repeating_Objects_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1049746,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11868719132361723069&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Huberman_Detecting_Repeating_Objects_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Detecting Vanishing Points Using Global Image Context in a Non-Manhattan World",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "39",
        "author_site": "Menghua Zhai, Scott Workman, Nathan Jacobs",
        "author": "Menghua Zhai; Scott Workman; Nathan Jacobs",
        "abstract": "We propose a novel method for detecting horizontal vanishing points and the zenith vanishing point in man-made environments. The dominant trend in existing methods is to first find candidate vanishing points, then remove outliers by enforcing mutual orthogonality. Our method reverses this process: we propose a set of horizon line candidates and score each based on the vanishing points it contains. A key element of our approach is the use of global image context, extracted with a deep convolutional network, to constrain the set of candidates under consideration. Our method does not make a Manhattan-world assumption and can operate effectively on scenes with only a single horizontal vanishing point.  We evaluate our approach on three benchmark datasets and achieve state-of-the-art performance on each. In addition, our approach is significantly faster than the previous best method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhai_Detecting_Vanishing_Points_CVPR_2016_paper.pdf",
        "aff": "Computer Science, University of Kentucky; Computer Science, University of Kentucky; Computer Science, University of Kentucky",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhai_Detecting_Vanishing_Points_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3367171,
        "gs_citation": 138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13333532781370016030&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.uky.edu;cs.uky.edu;cs.uky.edu",
        "email": "cs.uky.edu;cs.uky.edu;cs.uky.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhai_Detecting_Vanishing_Points_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Kentucky",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "https://www.uky.edu",
        "aff_unique_abbr": "UK",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Detection and Accurate Localization of Circular Fiducials Under Highly Challenging Conditions",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "60",
        "author_site": "Lilian Calvet, Pierre Gurdjos, Carsten Griwodz, Simone Gasparini",
        "author": "Lilian Calvet; Pierre Gurdjos; Carsten Griwodz; Simone Gasparini",
        "abstract": "Using fiducial markers ensures reliable detection and identification of planar features in images. Fiducials are used in a wide range of applications, especially when a reliable visual reference is needed, e.g., to track the camera in cluttered or textureless environments. A marker designed for such applications must be robust to partial occlusions, varying distances and angles of view, and fast camera motions. In this paper, we present a robust, highly accurate fiducial system, whose markers consist of concentric rings, along with its theoretical foundations. Relying on projective properties, it allows to robustly localize the imaged marker and to accurately detect the position of the image of the (common) circle center. We demonstrate that our system can detect and accurately localize these circular fiducials under very challenging conditions and the experimental results reveal that it outperforms other recent fiducial systems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Calvet_Detection_and_Accurate_CVPR_2016_paper.pdf",
        "aff": "Simula Research Laboratory, Oslo, Norway; University of Toulouse, France; Simula Research Laboratory, Oslo, Norway; University of Toulouse, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2213316,
        "gs_citation": 116,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16652147468408255557&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "simula.no;enseeiht.fr;simula.no;enseeiht.fr",
        "email": "simula.no;enseeiht.fr;simula.no;enseeiht.fr",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Calvet_Detection_and_Accurate_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Simula Research Laboratory;University of Toulouse",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.simula.no;https://www.univ-toulouse.fr",
        "aff_unique_abbr": "Simula;UT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Oslo;",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "Norway;France"
    },
    {
        "title": "Determining Occlusions From Space and Time Image Reconstructions",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "66",
        "author_site": "Juan-Manuel P\u00e9rez-R\u00faa, Tomas Crivelli, Patrick Bouthemy, Patrick P\u00e9rez",
        "author": "Juan-Manuel Perez-Rua; Tomas Crivelli; Patrick Bouthemy; Patrick Perez",
        "abstract": "The problem of localizing occlusions between consecutive frames of a video is important but rarely tackled on its own. In most works, it is tightly interleaved with the computation of accurate optical flows, which leads to a delicate chicken-and-egg problem. With this in mind, we propose a novel approach to occlusion detection where visibility or not of a point in next frame is formulated in terms of visual reconstruction. The key issue is now to determine how well a pixel in the first image can be \"recon- structed\" from co-located colors in the next image. We first exploit this reasoning at the pixel level with a new detection criterion. Contrary to the ubiquitous displaced-frame-difference and forward-backward flow vector matching, the proposed alternative does not critically depend on a precomputed, dense displacement field, while being shown to be more effective. We then leverage this local modeling within an energy-minimization framework that delivers occlusion maps. An easy-to-obtain collection of parametric motion models is exploited within the energy to provide the required level of motion information. Our approach outperforms state-of-the-art detection methods on the challenging MPI Sintel dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Perez-Rua_Determining_Occlusions_From_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2154504,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4829190701333879441&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Perez-Rua_Determining_Occlusions_From_CVPR_2016_paper.html"
    },
    {
        "title": "Dictionary Pair Classifier Driven Convolutional Neural Networks for Object Detection",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "69",
        "author_site": "Keze Wang, Liang Lin, Wangmeng Zuo, Shuhang Gu, Lei Zhang",
        "author": "Keze Wang; Liang Lin; Wangmeng Zuo; Shuhang Gu; Lei Zhang",
        "abstract": "Feature representation and object category classification are two key components of most object detection methods. While significant improvements have been achieved for deep feature representation learning, traditional SVM/softmax classifiers remain the dominant methods for final object category classification. However, SVM/softmax classifiers lack the capacity of explicitly exploiting the complex structure of deep features, as they are purely discriminative methods. The recently proposed discriminative dictionary pair learning (DPL) model involves a fidelity term to minimize the reconstruction loss and a discrimination term to enhance the discriminative capability of the learned dictionary pair, and thus is appropriate for balancing the representation and discrimination to boost object detection performance. In this paper, we propose a novel object detection system by unifying DPL with the convolutional feature learning. Specifically, we incorporate DPL as a Dictionary Pair Classifier Layer (DPCL) into the deep architecture, and develop an end-to-end learning algorithm for optimizing the dictionary pairs and the neural networks simultaneously. Moreover, we design a multi-task loss for guiding our model to accomplish the three correlated tasks: objectness estimation, categoryness computation, and bounding box regression. From the extensive experiments on PASCAL VOC 2007/2012 benchmarks, our approach demonstrates the effectiveness to substantially improve the performances over the popular existing object detection frameworks (e.g., R-CNN [13] and FRCN [12]), and achieves new state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Dictionary_Pair_Classifier_CVPR_2016_paper.pdf",
        "aff": "School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China+Department of Computing, The Hong Kong Polytechnic University; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Technology, Harbin Institute of Technology, China; Department of Computing, The Hong Kong Polytechnic University; Department of Computing, The Hong Kong Polytechnic University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1130928,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12331202305500582498&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;ieee.org;gmail.com;comp.polyu.edu.hk;comp.polyu.edu.hk",
        "email": "gmail.com;ieee.org;gmail.com;comp.polyu.edu.hk;comp.polyu.edu.hk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Dictionary_Pair_Classifier_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;2;1;1",
        "aff_unique_norm": "Sun Yat-sen University;Hong Kong Polytechnic University;Harbin Institute of Technology",
        "aff_unique_dep": "School of Data and Computer Science;Department of Computing;School of Computer Science and Technology",
        "aff_unique_url": "http://www.sysu.edu.cn;https://www.polyu.edu.hk;http://www.hit.edu.cn/",
        "aff_unique_abbr": "SYSU;PolyU;HIT",
        "aff_campus_unique_index": "0+1;0;2;1;1",
        "aff_campus_unique": "Guangzhou;Hong Kong SAR;Harbin",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Direct Prediction of 3D Body Poses From Motion Compensated Sequences",
        "session": "Events, Activities, and Surveillance",
        "status": "Poster",
        "track": "main",
        "pid": "25",
        "author_site": "Bugra Tekin, Artem Rozantsev, Vincent Lepetit, Pascal Fua",
        "author": "Bugra Tekin; Artem Rozantsev; Vincent Lepetit; Pascal Fua",
        "abstract": "We propose an efficient approach to exploiting motion information from consecutive frames of a video sequence to recover the 3D pose of people.  Previous approaches typically compute candidate poses in individual frames and then link them in a post-processing step to resolve ambiguities. By contrast, we directly regress from a spatio-temporal volume of bounding boxes to a 3D pose in the central frame.   We further show that, for this approach to  achieve its full potential, it is essential to compensate for the motion in consecutive frames so that the subject remains centered. This then allows us to effectively overcome  ambiguities and improve upon the state-of-the-art by a large margin on the Human3.6m, HumanEva, and KTH Multiview Football 3D human pose  estimation benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tekin_Direct_Prediction_of_CVPR_2016_paper.pdf",
        "aff": "CVLab, EPFL, Lausanne, Switzerland; CVLab, EPFL, Lausanne, Switzerland; CVLab, EPFL, Lausanne, Switzerland + TU Graz, Graz, Austria; CVLab, EPFL, Lausanne, Switzerland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Tekin_Direct_Prediction_of_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1789391,
        "gs_citation": 273,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16051539174727720794&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch;icg.tugraz.at",
        "email": "epfl.ch;epfl.ch;epfl.ch;icg.tugraz.at",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tekin_Direct_Prediction_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "EPFL;Graz University of Technology",
        "aff_unique_dep": "CVLab;",
        "aff_unique_url": "https://www.epfl.ch;https://www.tugraz.at",
        "aff_unique_abbr": "EPFL;TU Graz",
        "aff_campus_unique_index": "0;0;0+1;0",
        "aff_campus_unique": "Lausanne;Graz",
        "aff_country_unique_index": "0;0;0+1;0",
        "aff_country_unique": "Switzerland;Austria"
    },
    {
        "title": "Discovering the Physical Parts of an Articulated Object Class From Multiple Videos",
        "session": "Video Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "77",
        "author_site": "Luca Del Pero, Susanna Ricco, Rahul Sukthankar, Vittorio Ferrari",
        "author": "Luca Del Pero; Susanna Ricco; Rahul Sukthankar; Vittorio Ferrari",
        "abstract": "We propose a motion-based method to discover the physical parts of an articulated object class (e.g. head/torso/leg of a horse) from multiple videos. The key is to find object regions that exhibit consistent motion relative to the rest of the object, across multiple videos. We can then learn a location model for the parts and segment them accurately in the individual videos using an energy function that also enforces temporal and spatial consistency in part motion. Unlike our approach, traditional methods for motion segmentation or non-rigid structure from motion operate on one video at a time. Hence they cannot discover a part unless it displays independent motion in that particular video. We evaluate our method on a new dataset of 32 videos of tigers and horses, where we significantly outperform a recent motion segmentation method on the task of part discovery (obtaining roughly twice the accuracy).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Del_Pero_Discovering_the_Physical_CVPR_2016_paper.pdf",
        "aff": "University of Edinburgh+Blippar; Google Research; Google Research; University of Edinburgh",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 860882,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11147450354475899249&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "blippar.com;google.com;google.com;inf.ed.ac.uk",
        "email": "blippar.com;google.com;google.com;inf.ed.ac.uk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Del_Pero_Discovering_the_Physical_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;2;2;0",
        "aff_unique_norm": "University of Edinburgh;Blippar;Google",
        "aff_unique_dep": ";;Google Research",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.blippar.com;https://research.google",
        "aff_unique_abbr": "Edinburgh;Blippar;Google Research",
        "aff_campus_unique_index": ";1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0+0;1;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "title": "Discriminative Hierarchical Rank Pooling for Activity Recognition",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "46",
        "author_site": "Basura Fernando, Peter Anderson, Marcus Hutter, Stephen Gould",
        "author": "Basura Fernando; Peter Anderson; Marcus Hutter; Stephen Gould",
        "abstract": "We present hierarchical rank pooling, a video sequence encoding method for activity recognition. It consists of a network of rank pooling functions which captures the dynamics of rich convolutional neural network features within a video sequence. By stacking non-linear feature functions and rank pooling over one another, we obtain a high capacity dynamic encoding mechanism, which is used for action recognition. We present a method for jointly learning the video representation and activity classifier parameters. Our method obtains state-of-the art results on three important activity recognition benchmarks: 76.7% on Hollywood2, 66.9% on HMDB51 and, 91.4% on UCF101.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Fernando_Discriminative_Hierarchical_Rank_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Fernando_Discriminative_Hierarchical_Rank_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=684080648808305036&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Fernando_Discriminative_Hierarchical_Rank_CVPR_2016_paper.html"
    },
    {
        "title": "Discriminative Invariant Kernel Features: A Bells-and-Whistles-Free Approach to Unsupervised Face Recognition and Pose Estimation",
        "session": "People and Faces",
        "status": "Spotlight",
        "track": "main",
        "pid": "32",
        "author_site": "Dipan K. Pal, Felix Juefei-Xu, Marios Savvides",
        "author": "Dipan K. Pal; Felix Juefei-Xu; Marios Savvides",
        "abstract": "We propose an explicitly discriminative and `simple' approach to generate invariance to nuisance transformations modeled as unitary. In practice, the approach works well to handle non-unitary transformations as well. Our theoretical results extend the reach of a recent theory of invariance to discriminative and kernelized features based on unitary kernels. As a special case, a single common framework can be used to generate subject-specific pose-invariant features for face recognition and vice-versa for pose estimation. We show that our main proposed method (DIKF) can perform well under very challenging large-scale semi-synthetic face matching and pose estimation protocols with unaligned faces using no land-marking whatsoever. We additionally benchmark on CMU MPIE and outperform previous work in almost all cases on off-angle face matching while we are on par with the previous state-of-the-art on the LFW unsupervised and image-restricted protocols, without any low-level image descriptors other than raw-pixels.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Pal_Discriminative_Invariant_Kernel_CVPR_2016_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2442380,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5888095304529474317&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "andrew.cmu.edu;cmu.edu;ri.cmu.edu",
        "email": "andrew.cmu.edu;cmu.edu;ri.cmu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Pal_Discriminative_Invariant_Kernel_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Discriminative Multi-Modal Feature Fusion for RGBD Indoor Scene Recognition",
        "session": "Scene and Image Classification",
        "status": "Poster",
        "track": "main",
        "pid": "76",
        "author_site": "Hongyuan Zhu, Jean-Baptiste Weibel, Shijian Lu",
        "author": "Hongyuan Zhu; Jean-Baptiste Weibel; Shijian Lu",
        "abstract": "RGBD scene recognition has attracted increasingly attention due to the rapid development of depth sensors and their wide application scenarios. While many research has been conducted, most work used hand-crafted features which are difficult to capture high-level semantic structures. Recently, the feature extracted from deep convolutional neural network has produced state-of-the-art results for various computer vision tasks, which inspire researchers to explore incorporating CNN learned features for RGBD scene understanding. On the other hand, most existing work combines rgb and depth features without adequately exploiting the consistency and complementary information between them. Inspired by some recent work on RGBD object recognition using multi-modal feature fusion, we introduce a novel discriminative multi-modal fusion framework for rgbd scene recognition for the first time which simultaneously considers the inter- and intra-modality correlation for all samples and meanwhile regularizing the learned features to be discriminative and compact. The results from the multimodal layer can be back-propagated to the lower CNN layers, hence the parameters of the CNN layers and multimodal layers are updated iteratively until convergence. Experiments on the recently proposed large scale SUN RGB-D datasets show that our method achieved the state-of-the-art without any image segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_Discriminative_Multi-Modal_Feature_CVPR_2016_paper.pdf",
        "aff": "I2R, A\u2217Star, Singapore; Georgia Tech, USA; I2R, A\u2217Star, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 571940,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5697056405734837271&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "i2r.a-star.edu.sg;gatech.edu;i2r.a-star.edu.sg",
        "email": "i2r.a-star.edu.sg;gatech.edu;i2r.a-star.edu.sg",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_Discriminative_Multi-Modal_Feature_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Institute for Infocomm Research;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg;https://www.gatech.edu",
        "aff_unique_abbr": "I2R;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "title": "Discriminatively Embedded K-Means for Multi-View Clustering",
        "session": "Learning and Optimization",
        "status": "Spotlight",
        "track": "main",
        "pid": "7",
        "author_site": "Jinglin Xu, Junwei Han, Feiping Nie",
        "author": "Jinglin Xu; Junwei Han; Feiping Nie",
        "abstract": "In real world applications, more and more data, for example, image/video data, are high dimensional and represented by multiple views which describe different perspectives of the data. Efficiently clustering such data is a challenge. To address this problem, this paper proposes a novel multi-view clustering method called Discriminatively Embedded K-Means (DEKM), which embeds the synchronous learning of multiple discriminative subspaces into multi-view K-Means clustering to construct a unified framework, and adaptively control the intercoordinations between these subspaces simultaneously. In this framework, we firstly design a weighted multi-view Linear Discriminant Analysis (LDA), and then develop an unsupervised optimization scheme to alternatively learn the common clustering indicator, multiple discriminative subspaces and weights for heterogeneous features with convergence. Comprehensive evaluations on three benchmark datasets and comparisons with several state-of-the-art multi-view clustering algorithms demonstrate the superiority of the proposed work.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_Discriminatively_Embedded_K-Means_CVPR_2016_paper.pdf",
        "aff": "School of Automation; School of Computer Science and Center for OPTIMAL; School of Computer Science and Center for OPTIMAL",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4674297,
        "gs_citation": 169,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6597646594235342184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;gmail.com",
        "email": ";;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xu_Discriminatively_Embedded_K-Means_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "School of Automation;School of Computer Science",
        "aff_unique_dep": "Automation;Computer Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "DisturbLabel: Regularizing CNN on the Loss Layer",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "22",
        "author_site": "Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, Qi Tian",
        "author": "Lingxi Xie; Jingdong Wang; Zhen Wei; Meng Wang; Qi Tian",
        "abstract": "During a long period of time we are combating over-fitting in the CNN training process with model regularization, including weight decay, model averaging, data augmentation, etc. In this paper, we present DisturbLabel, an extremely simple algorithm which randomly replaces a part of labels as incorrect values in each iteration. Although it seems weird to intentionally generate incorrect training labels, we show that DisturbLabel prevents the network training from over-fitting by implicitly averaging over exponentially many networks which are trained with different label sets. To the best of our knowledge, DisturbLabel serves as the first work which adds noises on the loss layer. Meanwhile, DisturbLabel cooperates well with Dropout to provide complementary regularization functions. Experiments demonstrate competitive recognition results on several popular image recognition datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_DisturbLabel_Regularizing_CNN_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 314,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11976349396086878628&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xie_DisturbLabel_Regularizing_CNN_CVPR_2016_paper.html"
    },
    {
        "title": "Do Computational Models Differ Systematically From Human Object Perception?",
        "session": "Recognition Beyond Objects",
        "status": "Spotlight",
        "track": "main",
        "pid": "11",
        "author_site": "R. T. Pramod, S. P. Arun",
        "author": "R. T. Pramod; S. P. Arun",
        "abstract": "Recent advances in neural networks have revolutionized computer vision, but these algorithms are still outperformed by humans. Could this performance gap be due to systematic differences between object representations in humans and machines? To answer this question we collected a large dataset of 26,675 perceived dissimilarity measurements from 2,801 visual objects across 269 human subjects, and used this dataset to train and test leading computational models. The best model (a combination of all models) accounted for 68% of the explainable variance. Importantly, all computational models showed systematic deviations from perception: (1) They underestimated perceptual distances between objects with symmetry or large area differences; (2) They overestimated perceptual distances between objects with shared features. Our results reveal critical elements missing in computer vision algorithms and point to explicit encoding of these properties in higher visual areas in the brain.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Pramod_Do_Computational_Models_CVPR_2016_paper.pdf",
        "aff": "Department of Electrical Communication Engineering & Centre for Neuroscience, Indian Institute of Science, Bangalore, India; Centre for Neuroscience, Indian Institute of Science, Bangalore, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Pramod_Do_Computational_Models_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2738473,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3234669921752025215&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ece.iisc.ernet.in;cns.iisc.ernet.in",
        "email": "ece.iisc.ernet.in;cns.iisc.ernet.in",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Pramod_Do_Computational_Models_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Science",
        "aff_unique_dep": "Department of Electrical Communication Engineering & Centre for Neuroscience",
        "aff_unique_url": "https://www.iisc.ac.in",
        "aff_unique_abbr": "IISc",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Bangalore",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Do It Yourself Hyperspectral Imaging With Everyday Digital Cameras",
        "session": "Computational Photography and Biomedical Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "22",
        "author_site": "Seoung Wug Oh, Michael S. Brown, Marc Pollefeys, Seon Joo Kim",
        "author": "Seoung Wug Oh; Michael S. Brown; Marc Pollefeys; Seon Joo Kim",
        "abstract": "Capturing hyperspectral images requires expensive and specialized hardware that is not readily accessible to most users. Digital cameras, on the other hand, are significantly cheaper in comparison and can be easily purchased and used. In this paper, we present a framework for reconstructing hyperspectral images by using multiple consumer-level digital cameras. Our approach works by exploiting the different spectral sensitivities of different camera sensors. In particular, due to the differences in spectral sensitivities of the cameras, different cameras yield different RGB measurements for the same spectral signal. We  introduce an algorithm that is able to combine and convert these different RGB measurements into a single hyperspectral image for both indoor and outdoor scenes. This camera-based approach allows hyperspectral imaging at a fraction of the cost of most existing hyperspectral hardware. We validate the accuracy of our reconstruction against ground truth hyperspectral images (using both synthetic and real cases) and show its usage on relighting applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Oh_Do_It_Yourself_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Oh_Do_It_Yourself_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2377040,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12373607136224690782&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Oh_Do_It_Yourself_CVPR_2016_paper.html"
    },
    {
        "title": "Dynamic Image Networks for Action Recognition",
        "session": "Actions and Human Pose",
        "status": "Oral",
        "track": "main",
        "pid": "1",
        "author_site": "Hakan Bilen, Basura Fernando, Efstratios Gavves, Andrea Vedaldi, Stephen Gould",
        "author": "Hakan Bilen; Basura Fernando; Efstratios Gavves; Andrea Vedaldi; Stephen Gould",
        "abstract": "We introduce the concept of dynamic image, a novel compact representation of videos useful for video analysis especially when convolutional neural networks (CNNs) are used. The dynamic image is based on the rank pooling concept and is obtained through the parameters of a ranking machine that encodes the temporal evolution of the frames of the video. Dynamic images are obtained by directly applying rank pooling on the raw image pixels of a video producing a single RGB image per video. This idea is simple but powerful as it enables the use of existing CNN models directly on video data with fine-tuning. We present an efficient and effective approximate rank pooling operator, speeding it up orders of magnitude compared to rank pooling. Our new approximate rank pooling CNN layer allows us to generalize dynamic images to dynamic feature maps and we demonstrate the power of our new representations on standard benchmarks in action recognition achieving state-of-the-art performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bilen_Dynamic_Image_Networks_CVPR_2016_paper.pdf",
        "aff": "University of Oxford; The Australian National University; QUV A Lab, University of Amsterdam; University of Oxford; The Australian National University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 630839,
        "gs_citation": 727,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10222509107535750356&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bilen_Dynamic_Image_Networks_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;0;1",
        "aff_unique_norm": "University of Oxford;Australian National University;University of Amsterdam",
        "aff_unique_dep": ";;QUV A Lab",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.anu.edu.au;https://www.uva.nl",
        "aff_unique_abbr": "Oxford;ANU;UvA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0;1",
        "aff_country_unique": "United Kingdom;Australia;Netherlands"
    },
    {
        "title": "Efficient 3D Room Shape Recovery From a Single Panorama",
        "session": "3D Shape Reconstruction",
        "status": "Oral",
        "track": "main",
        "pid": "14",
        "author_site": "Hao Yang, Hui Zhang",
        "author": "Hao Yang; Hui Zhang",
        "abstract": "We propose a method to recover the shape of a 3D room from a full-view indoor panorama. Our algorithm can automatically infer a 3D shape from a collection of partially oriented superpixel facets and line segments. The core part of the algorithm is a constraint graph, which includes lines and superpixels as vertices, and encodes their geometric relations as edges. A novel approach is proposed to perform 3D reconstruction based on the constraint graph by solving all the geometric constraints as constrained linear least-squares. The selected constraints used for reconstruction are identified using an occlusion detection method with a Markov random field. Experiments show that our method can recover room shapes that can not be addressed by previous approaches. Our method is also efficient, that is, the inference time for each panorama is less than 1 minute.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_Efficient_3D_Room_CVPR_2016_paper.pdf",
        "aff": "School of Software, Tsinghua University, Beijing, China+Tsinghua National Laboratory for Information Science and Technology (TNList); School of Software, Tsinghua University, Beijing, China+Tsinghua National Laboratory for Information Science and Technology (TNList)",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Yang_Efficient_3D_Room_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2537782,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12245044432781869777&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;tsinghua.edu.cn",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_Efficient_3D_Room_CVPR_2016_paper.html",
        "aff_unique_index": "0+0;0+0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "School of Software",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Efficient Coarse-To-Fine PatchMatch for Large Displacement Optical Flow",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "44",
        "author_site": "Yinlin Hu, Rui Song, Yunsong Li",
        "author": "Yinlin Hu; Rui Song; Yunsong Li",
        "abstract": "As a key component in many computer vision systems, optical flow estimation, especially with large displacements, remains an open problem. In this paper we present a simple but powerful matching method works in a coarse-to-fine scheme for optical flow estimation. Inspired by the nearest neighbor field (NNF) algorithms, our approach, called CPM (Coarse-to-fine PatchMatch), blends an efficient random search strategy with the coarse-to-fine scheme for optical flow problem. Unlike existing NNF techniques, which is efficient but the results is often too noisy for optical flow caused by the lack of global regularization, we propose a propagation step with constrained random search radius between adjacent levels on the hierarchical architecture. The resulting correspondences enjoys a built-in smoothing effect, which is more suited for optical flow estimation than NNF techniques. Furthermore, our approach can also capture the tiny structures with large motions which is a problem for traditional coarse-to-fine optical flow algorithms. Interpolated by an edge-preserving interpolation method (EpicFlow), our method outperforms the state of the art on MPI-Sintel and KITTI, and runs much faster than the competing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hu_Efficient_Coarse-To-Fine_PatchMatch_CVPR_2016_paper.pdf",
        "aff": "Xidian University, China; Xidian University, China + Shanghai Institute of Technical Physics, China; Xidian University, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Hu_Efficient_Coarse-To-Fine_PatchMatch_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1009054,
        "gs_citation": 273,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9712737645239075038&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com;xidian.edu.cn;mail.xidian.edu.cn",
        "email": "gmail.com;xidian.edu.cn;mail.xidian.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hu_Efficient_Coarse-To-Fine_PatchMatch_CVPR_2016_paper.html",
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Xidian University;Shanghai Institute of Technical Physics",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.xidian.edu.cn/;",
        "aff_unique_abbr": "Xidian;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Efficient Deep Learning for Stereo Matching",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "43",
        "author_site": "Wenjie Luo, Alexander G. Schwing, Raquel Urtasun",
        "author": "Wenjie Luo; Alexander G. Schwing; Raquel Urtasun",
        "abstract": "In the past year, convolutional neural networks have been shown to perform extremely well for stereo estimation. However, current architectures rely on siamese networks which exploit concatenation followed by further processing layers, requiring a minute of GPU computation per image pair. In contrast, in this paper we propose a matching network which is able to produce very accurate results in less than a second of GPU computation. Towards this goal, we exploit a product layer which simply computes the inner product between the two representations of a siamese architecture. We train our network by treating the problem as multi-class classification, where the classes are all possible disparities. This allows us to get calibrated scores, which result in much better matching performance when compared to existing approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Luo_Efficient_Deep_Learning_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2253641,
        "gs_citation": 966,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17438913042615584120&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Luo_Efficient_Deep_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Efficient Globally Optimal 2D-To-3D Deformable Shape Matching",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "74",
        "author_site": "Zorah L\u00e4hner, Emanuele Rodol\u00e0, Frank R. Schmidt, Michael M. Bronstein, Daniel Cremers",
        "author": "Zorah Lahner; Emanuele Rodola; Frank R. Schmidt; Michael M. Bronstein; Daniel Cremers",
        "abstract": "We propose the first algorithm for non-rigid 2D-to-3D shape matching, where the input is a 2D query shape as well as a 3D target shape and the output is a continuous matching curve represented as a closed contour on the 3D shape. We cast the problem as finding the shortest circular path on the product 3-manifold of the two shapes. We prove that the optimal matching can be computed in polynomial time with a (worst-case) complexity of O(m*n^2*log(n)), where m and n denote the number of vertices on the 2D and the 3D shape respectively. Quantitative evaluation confirms that the method provides excellent results for sketch-based deformable 3D shape retrieval.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lahner_Efficient_Globally_Optimal_CVPR_2016_paper.pdf",
        "aff": "TU M \u00a8unchen; TU M \u00a8unchen+Universit `a della Svizzera Italiana; TU M \u00a8unchen; Universit `a della Svizzera Italiana; TU M \u00a8unchen",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1343264,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16485417269886684756&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 17,
        "aff_domain": "in.tum.de;usi.ch;cs.tum.edu;usi.ch;tum.de",
        "email": "in.tum.de;usi.ch;cs.tum.edu;usi.ch;tum.de",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lahner_Efficient_Globally_Optimal_CVPR_2016_paper.html",
        "aff_unique_index": "0;0+1;0;1;0",
        "aff_unique_norm": "Technical University of Munich;Universit\u00e0 della Svizzera italiana",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tum.de;https://www.usi.ch",
        "aff_unique_abbr": "TUM;USI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Munich;",
        "aff_country_unique_index": "0;0+1;0;1;0",
        "aff_country_unique": "Germany;Switzerland"
    },
    {
        "title": "Efficient Indexing of Billion-Scale Datasets of Deep Descriptors",
        "session": "Image Indexing and Retrieval",
        "status": "Poster",
        "track": "main",
        "pid": "60",
        "author_site": "Artem Babenko, Victor Lempitsky",
        "author": "Artem Babenko; Victor Lempitsky",
        "abstract": "Existing billion-scale nearest neighbor search systems have mostly been compared on a single dataset of a billion of SIFT vectors, where systems based on the Inverted Multi-Index (IMI) have been performing very well, achieving state-of-the-art recall in several milliseconds. SIFT-like descriptors, however, are quickly being replaced with descriptors based on deep neural networks (DNN) that provide better performance for many computer vision tasks.     In this paper, we introduce a new dataset of one billion descriptors based on DNNs and reveal the relative inefficiency of IMI-based indexing for such descriptors compared to SIFT data. We then introduce two new indexing structures, the Non-Orthogonal Inverted Multi-Index (NO-IMI) and the Generalized Non-Orthogonal Inverted Multi-Index (GNO-IMI). We show that due to additional flexibility, the new structures are  able to adapt to DNN descriptor distribution in a better way. In particular, extensive experiments on the new dataset demonstrate that these data structures provide considerably better trade-off between the speed of retrieval and recall, given similar amount of memory, as compared to the standard Inverted Multi-Index.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Babenko_Efficient_Indexing_of_CVPR_2016_paper.pdf",
        "aff": "Yandex + Moscow Institute of Physics and Technology; Skolkovo Institute of Science and Technology (Skoltech)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1306258,
        "gs_citation": 285,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4113140298092468809&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "phystech.edu;skoltech.ru",
        "email": "phystech.edu;skoltech.ru",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Babenko_Efficient_Indexing_of_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Yandex;Moscow Institute of Physics and Technology;Skolkovo Institute of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://yandex.com;https://www.mipt.ru/en;https://www.skoltech.ru",
        "aff_unique_abbr": "Yandex;MIPT;Skoltech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Russian Federation"
    },
    {
        "title": "Efficient Intersection of Three Quadrics and Applications in Computer Vision",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "33",
        "author_site": "Zuzana Kukelova, Jan Heller, Andrew Fitzgibbon",
        "author": "Zuzana Kukelova; Jan Heller; Andrew Fitzgibbon",
        "abstract": "In this paper, we present a new algorithm for finding all intersections of three quadrics. The proposed method is algebraic in nature and it is considerably more efficient than the Groebner basis and resultant-based solutions previously used in computer vision applications. We identify several computer vision problems that are formulated and solved as systems of three quadratic equations and for which our algorithm readily delivers considerably faster results. Also, we propose new formulations of three important vision problems: absolute camera pose with unknown focal length, generalized pose-and-scale, and hand-eye calibration with known translation. These new formulations allow our algorithm to significantly outperform the state-of-the-art in speed.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kukelova_Efficient_Intersection_of_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Kukelova_Efficient_Intersection_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 783022,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1046869068158629516&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kukelova_Efficient_Intersection_of_CVPR_2016_paper.html"
    },
    {
        "title": "Efficient Large-Scale Approximate Nearest Neighbor Search on the GPU",
        "session": "Image Indexing and Retrieval",
        "status": "Poster",
        "track": "main",
        "pid": "57",
        "author_site": "Patrick Wieschollek, Oliver Wang, Alexander Sorkine-Hornung, Hendrik P. A. Lensch",
        "author": "Patrick Wieschollek; Oliver Wang; Alexander Sorkine-Hornung; Hendrik P. A. Lensch",
        "abstract": "We present a new approach for efficient approximate nearest neighbor (ANN) search in high dimensional spaces, extending the idea of Product Quantization. We propose a two level product and vector quantization tree that reduces the number of vector comparisons required during tree traversal. Our approach also includes a novel highly parallelizable re-ranking method for candidate vectors by efficiently reusing already computed intermediate values. Due to its small memory footprint during traversal the method lends itself to an efficient, parallel GPU implementation. This Product Quantization Tree approach significantly outperforms recent state of the art methods for high dimensional nearest neighbor queries on standard reference datasets.  Ours is the first work that demonstrates GPU performance superior to CPU performance on high dimensional, large scale ANN problems in time-critical real-world applications, like loop-closing in videos.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wieschollek_Efficient_Large-Scale_Approximate_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wieschollek_Efficient_Large-Scale_Approximate_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 919380,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=809551570572889890&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wieschollek_Efficient_Large-Scale_Approximate_CVPR_2016_paper.html"
    },
    {
        "title": "Efficient Large-Scale Similarity Search Using Matrix Factorization",
        "session": "Image Indexing and Retrieval",
        "status": "Poster",
        "track": "main",
        "pid": "62",
        "author_site": "Ahmet Iscen, Michael Rabbat, Teddy Furon",
        "author": "Ahmet Iscen; Michael Rabbat; Teddy Furon",
        "abstract": "We consider the image retrieval problem of finding the images in a dataset that are most similar to a query image. Our goal is to reduce the number of vector operations and memory for performing a search without sacrificing accuracy of the returned images. We adopt a group testing formulation and design the decoding architecture using either dictionary learning or eigendecomposition. The latter is a plausible option for small-to-medium sized problems with high-dimensional global image descriptors, whereas dictionary learning is applicable in large-scale scenarios. We evaluate our approach for global descriptors obtained from both SIFT and CNN features. Experiments with standard image search benchmarks, including the Yahoo100M dataset comprising 100 million images, show that our method gives comparable (and sometimes superior) accuracy compared to exhaustive search while requiring only 10% of the vector operations and memory. Moreover, for the same search complexity, our method gives significantly better accuracy compared to approaches based on dimensionality reduction or locality sensitive hashing.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Iscen_Efficient_Large-Scale_Similarity_CVPR_2016_paper.pdf",
        "aff": "Inria, Rennes, France; McGill University, Montr \u00b4eal, Canada; Inria, Rennes, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 728855,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17443722390257366432&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "inria.fr;mcgill.ca;inria.fr",
        "email": "inria.fr;mcgill.ca;inria.fr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Iscen_Efficient_Large-Scale_Similarity_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "INRIA;McGill University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.inria.fr;https://www.mcgill.ca",
        "aff_unique_abbr": "Inria;McGill",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Rennes;Montr\u00e9al",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "France;Canada"
    },
    {
        "title": "Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation",
        "session": "Semantic Parsing and Segmentation",
        "status": "Spotlight",
        "track": "main",
        "pid": "18",
        "author_site": "Guosheng Lin, Chunhua Shen, Anton van den Hengel, Ian Reid",
        "author": "Guosheng Lin; Chunhua Shen; Anton van den Hengel; Ian Reid",
        "abstract": "Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks(CNNs). We show how to improve semantic segmentation through the use of contextual information; specifically, we explore 'patch-patch' context between image regions, and 'patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-over-union score of 78.0 on the challenging PASCAL VOC 2012 dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1110857,
        "gs_citation": 1221,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14079239548452791512&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.html"
    },
    {
        "title": "Efficient Point Process Inference for Large-Scale Object Detection",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "62",
        "author_site": "Trung T. Pham, Seyed Hamid Rezatofighi, Ian Reid, Tat-Jun Chin",
        "author": "Trung T. Pham; Seyed Hamid Rezatofighi; Ian Reid; Tat-Jun Chin",
        "abstract": "We tackle the problem of large-scale object detection in images, where the number of objects can be arbitrarily large, and can exhibit significant overlap/occlusion.  A successful approach to modelling the large-scale nature of this problem has been via point process density functions which jointly encode object qualities and spatial interactions.  But the corresponding optimisation problem is typically difficult or intractable, and many of the best current methods rely on Monte Carlo Markov Chain (MCMC) simulation, which converges slowly in a large solution space.   We propose an efficient point process inference for large-scale object detection using discrete energy minimization. In particular, we approximate the solution space by a finite set of object proposals and cast the point process density function to a corresponding energy function of binary variables whose values indicate which object proposals are accepted. We resort to the local submodular approximation (LSA) based trust-region optimisation to find the optimal solution. Furthermore we analyse the error of LSA approximation, and show how to adjust the point process energy to dramatically speed up the convergence without harms in the optimality. We demonstrate the superior efficiency and accuracy of our method using a variety of large-scale object detection applications such as crowd human detection, birds, cells counting/localization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Pham_Efficient_Point_Process_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science; School of Computer Science; School of Computer Science; School of Computer Science",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Pham_Efficient_Point_Process_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1700784,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14426710361497359117&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Pham_Efficient_Point_Process_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "School of Computer Science",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Efficient Temporal Sequence Comparison and Classification Using Gram Matrix Embeddings on a Riemannian Manifold",
        "session": "Statistical Methods and Learning",
        "status": "Poster",
        "track": "main",
        "pid": "76",
        "author_site": "Xikang Zhang, Yin Wang, Mengran Gou, Mario Sznaier, Octavia Camps",
        "author": "Xikang Zhang; Yin Wang; Mengran Gou; Mario Sznaier; Octavia Camps",
        "abstract": "In this paper we propose a new framework to compare and classify temporal sequences. The proposed approach captures the underlying dynamics of the data while avoiding expensive estimation procedures, making it suitable to process large numbers of sequences. The main idea is to first embed the  sequences into  a Riemannian manifold by  using positive definite regularized Gram matrices of  their Hankelets.  The  advantages of the this approach are: 1) it allows for using non-Euclidean similarity functions  on the Positive Definite matrix manifold, which capture better the underlying geometry than directly comparing the sequences or their  Hankel matrices; and 2)  Gram matrices inherit desirable properties from the underlying Hankel matrices: their rank  measure the complexity of the underlying dynamics, and the rank and the coefficients of the associated regressive models are  invariant to affine transformations and varying initial conditions. The benefits of this approach are illustrated with extensive experiments in 3D action recognition using 3D joints sequences. In spite of its simplicity,  the  performance of this approach is competitive or  better  than  using state-of-art approaches for this problem.  Further, these results hold across a variety of  metrics, supporting the idea that the improvement stems from the embedding itself, rather than from  using  one of these metrics.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Efficient_Temporal_Sequence_CVPR_2016_paper.pdf",
        "aff": "Northeastern University, Boston, MA 02115, US; Northeastern University, Boston, MA 02115, US; Northeastern University, Boston, MA 02115, US; Northeastern University, Boston, MA 02115, US; Northeastern University, Boston, MA 02115, US",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 704202,
        "gs_citation": 127,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10584768279821917323&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ece.neu.edu;husky.neu.edu;coe.neu.edu;coe.neu.edu;coe.neu.edu",
        "email": "ece.neu.edu;husky.neu.edu;coe.neu.edu;coe.neu.edu;coe.neu.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Efficient_Temporal_Sequence_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.northeastern.edu",
        "aff_unique_abbr": "NEU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Boston",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Efficient Training of Very Deep Neural Networks for Supervised Hashing",
        "session": "Supervised Learning",
        "status": "Poster",
        "track": "main",
        "pid": "77",
        "author_site": "Ziming Zhang, Yuting Chen, Venkatesh Saligrama",
        "author": "Ziming Zhang; Yuting Chen; Venkatesh Saligrama",
        "abstract": "In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively \"shallow\" networks limited by the issues arising in back propagation (e.g. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Efficient_Training_of_CVPR_2016_paper.pdf",
        "aff": "Center for Information & Systems Engineering, Boston University; Center for Information & Systems Engineering, Boston University; Center for Information & Systems Engineering, Boston University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7389924,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5218623187270418652&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "bu.edu;bu.edu;bu.edu",
        "email": "bu.edu;bu.edu;bu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Efficient_Training_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Center for Information & Systems Engineering",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Boston",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Efficient and Robust Color Consistency for Community Photo Collections",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "46",
        "author_site": "Jaesik Park, Yu-Wing Tai, Sudipta N. Sinha, In So Kweon",
        "author": "Jaesik Park; Yu-Wing Tai; Sudipta N. Sinha; In So Kweon",
        "abstract": "We present an efficient technique to optimize color consistency of a collection of images depicting a common scene. Our method first recovers sparse pixel correspondences in the input images and stacks them into a matrix with many missing entries. We show that this matrix satisfies a rank two constraint under a simple color correction model. These parameters can be viewed as pseudo white balance and gamma correction parameters for each input image. We present a robust low-rank matrix factorization method to estimate the unknown parameters of this model. Using them, we improve color consistency of the input images or perform color transfer with any input image as the source. Our approach is insensitive to outliers in the pixel correspondences thereby precluding the need for complex pre-processing steps. We demonstrate high-quality color consistency results on large photo collections of popular tourist landmarks and personal photo collections containing images of people.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Park_Efficient_and_Robust_CVPR_2016_paper.pdf",
        "aff": "Intel Labs; SenseTime; Microsoft Research; KAIST",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Park_Efficient_and_Robust_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2126455,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1454151019693620653&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "intel.com;sensetime.com;microsoft.com;kaist.ac.kr",
        "email": "intel.com;sensetime.com;microsoft.com;kaist.ac.kr",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Park_Efficient_and_Robust_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Intel;SenseTime;Microsoft;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "Intel Labs;;Microsoft Research;",
        "aff_unique_url": "https://www.intel.com;https://www.sensetime.com;https://www.microsoft.com/en-us/research;https://www.kaist.ac.kr",
        "aff_unique_abbr": "Intel;SenseTime;MSR;KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2",
        "aff_country_unique": "United States;China;South Korea"
    },
    {
        "title": "Efficiently Creating 3D Training Data for Fine Hand Pose Estimation",
        "session": "Human Pose Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "44",
        "author_site": "Markus Oberweger, Gernot Riegler, Paul Wohlhart, Vincent Lepetit",
        "author": "Markus Oberweger; Gernot Riegler; Paul Wohlhart; Vincent Lepetit",
        "abstract": "While many recent hand pose estimation methods critically rely on a training set of labelled frames, the creation of such a dataset is a challenging task that has been overlooked so far. As a result, existing datasets are limited to a few sequences and individuals, with limited accuracy, and this prevents these methods from delivering their full potential. We propose a semi-automated method for efficiently and accurately labeling each frame of a hand depth video with the corresponding 3D locations of the joints: The user is asked to provide only an estimate of the 2D reprojections of the visible joints in some reference frames, which are automatically selected to minimize the labeling work by efficiently optimizing a sub-modular loss function. We then exploit spatial, temporal, and appearance constraints to retrieve the full 3D poses of the hand over the complete sequence. We show that this data can be used to train a recent state-of-the-art hand pose estimation method, leading to increased accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Oberweger_Efficiently_Creating_3D_CVPR_2016_paper.pdf",
        "aff": "Institute for Computer Graphics and Vision, Graz University of Technology, Austria; Institute for Computer Graphics and Vision, Graz University of Technology, Austria; Institute for Computer Graphics and Vision, Graz University of Technology, Austria; Institute for Computer Graphics and Vision, Graz University of Technology, Austria",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Oberweger_Efficiently_Creating_3D_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1071462,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9924541753788288267&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "email": "icg.tugraz.at;icg.tugraz.at;icg.tugraz.at;icg.tugraz.at",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Oberweger_Efficiently_Creating_3D_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Graz University of Technology",
        "aff_unique_dep": "Institute for Computer Graphics and Vision",
        "aff_unique_url": "https://www.tugraz.at",
        "aff_unique_abbr": "TU Graz",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Graz",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Austria"
    },
    {
        "title": "Egocentric Future Localization",
        "session": "Non-Rigid Reconstruction and Motion Analysis",
        "status": "Oral",
        "track": "main",
        "pid": "16",
        "author_site": "Hyun Soo Park, Jyh-Jing Hwang, Yedong Niu, Jianbo Shi",
        "author": "Hyun Soo Park; Jyh-Jing Hwang; Yedong Niu; Jianbo Shi",
        "abstract": "We presents a method for future localization: to predict plausible future trajectories of ego-motion in egocentric stereo images.  Our paths avoid obstacles, move between objects, even turn around a corner into space behind objects. As a byproduct of the predicted trajectories, we discover the empty space occluded by foreground objects.   One key innovation is the creation of an EgoRetinal map, akin to an illustrated tourist map, that `rearranges' pixels taking into accounts depth information, the ground plane, and body motion direction, so that it allows motion planning and perception of objects on one image space. We learn to plan trajectories  directly on this EgoRetinal map using first person experience of walking around in a variety of scenes.  In a testing phase, given an novel scene, we find multiple hypotheses of future trajectories from the learned experience.  We refine them by minimizing a cost function that describes compatibility between the obstacles in the EgoRetinal map and trajectories.  We quantitatively evaluate our method to show predictive validity and apply to various real world daily activities including walking, shopping, and social interactions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Park_Egocentric_Future_Localization_CVPR_2016_paper.pdf",
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7069359,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11141993996910485743&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Park_Egocentric_Future_Localization_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Embedding Label Structures for Fine-Grained Feature Representation",
        "session": "Fine Grained Categorization",
        "status": "Poster",
        "track": "main",
        "pid": "38",
        "author_site": "Xiaofan Zhang, Feng Zhou, Yuanqing Lin, Shaoting Zhang",
        "author": "Xiaofan Zhang; Feng Zhou; Yuanqing Lin; Shaoting Zhang",
        "abstract": "Recent algorithms in convolutional neural networks (CNN) considerably advance the fine-grained image classification, which aims to differentiate the subtle differences among subordinate classes. However, previous studies have rarely focused on learning a fined-grained and structured feature representation that is able to locate relevant images at different levels of relevance, e.g., discovering cars from the same make or the same model, both of which require high precision. In this paper, we propose two main contributions to tackle this problem. 1) A multi-task learning framework is designed to effectively learn fine-grained feature representations by jointly optimizing both classification and similarity constraints. 2) To model the multi-level relevance, label structures such as hierarchy or shared attributes are seamlessly embedded into the framework by generalizing the triplet loss. Extensive and thorough experiments have been conducted on three fine-grained datasets, i.e., the Stanford car, the car-333, and the food datasets, which contain either hierarchical labels or shared attributes. Our proposed method has achieved very competitive performance, i.e., among state-of-the-art classification accuracy. More importantly, it significantly outperforms previous fine-grained feature representations for image retrieval at different levels of relevance",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Embedding_Label_Structures_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 9712059,
        "gs_citation": 259,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9028196953255186124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Embedding_Label_Structures_CVPR_2016_paper.html"
    },
    {
        "title": "EmotioNet: An Accurate, Real-Time Algorithm for the Automatic Annotation of a Million Facial Expressions in the Wild",
        "session": "People and Faces",
        "status": "Spotlight",
        "track": "main",
        "pid": "29",
        "author_site": "C. Fabian Benitez-Quiroz, Ramprakash Srinivasan, Aleix M. Martinez",
        "author": "C. Fabian Benitez-Quiroz; Ramprakash Srinivasan; Aleix M. Martinez",
        "abstract": "Research in face perception and emotion theory requires very large annotated databases of images of facial expressions of emotion. Annotations should include Action Units (AUs) and their intensities as well as emotion category. This goal cannot be readily achieved manually. Herein, we present a novel computer vision algorithm to annotate a large database of one million images of facial expressions of emotion in the wild (i.e., face images downloaded from the Internet). First, we show that this newly proposed algorithm can recognize AUs and their intensities reliably across databases. To our knowledge, this is the first published algorithm to achieve highly-accurate results in the recognition of AUs and their intensities across multiple databases. Our algorithm also runs in real-time (>30 images/second), allowing it to work with large numbers of images and video sequences. Second, we use WordNet to download 1,000,000 images of facial expressions with associated emotion keywords from the Internet. These images are then automatically annotated with AUs, AU intensities and emotion categories by our algorithm. The result is a highly useful database that can be readily queried using semantic descriptions for applications in computer vision, affective computing, social and cognitive psychology and neuroscience; e.g., \"show me all the images with happy faces\" or \"all images with AU 1 at intensity c.\"",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Benitez-Quiroz_EmotioNet_An_Accurate_CVPR_2016_paper.pdf",
        "aff": "Dept. Electrical and Computer Engineering, The Ohio State University; Dept. Electrical and Computer Engineering, The Ohio State University; Dept. Electrical and Computer Engineering, The Ohio State University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Benitez-Quiroz_EmotioNet_An_Accurate_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2677624,
        "gs_citation": 711,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15358910599513919895&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Benitez-Quiroz_EmotioNet_An_Accurate_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Ohio State University",
        "aff_unique_dep": "Dept. of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "End-To-End Learning of Action Detection From Frame Glimpses in Videos",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "45",
        "author_site": "Serena Yeung, Olga Russakovsky, Greg Mori, Li Fei-Fei",
        "author": "Serena Yeung; Olga Russakovsky; Greg Mori; Li Fei-Fei",
        "abstract": "In this work we introduce a fully end-to-end approach for action detection in videos that learns to directly predict the temporal bounds of actions. Our intuition is that the process of detecting actions is naturally one of observation and refinement: observing moments in video, and refining hypotheses about when an action is occurring. Based on this insight, we formulate our model as a recurrent neural network-based agent that interacts with a video over time. The agent observes video frames and decides both where to look next and whether to emit a prediction. Since backpropagation is not adequate in this non-differentiable setting, we use REINFORCE to learn the agent's task-specific decision policy. Our model achieves state-of-the-art results on the THUMOS'14 and ActivityNet datasets while observing only a fraction (2% or less) of the video frames.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yeung_End-To-End_Learning_of_CVPR_2016_paper.pdf",
        "aff": "Stanford University; Carnegie Mellon University+Stanford University; Simon Fraser University; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1075088,
        "gs_citation": 763,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2818271568652020089&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "cs.stanford.edu;cmu.edu;cs.sfu.ca;cs.stanford.edu",
        "email": "cs.stanford.edu;cmu.edu;cs.sfu.ca;cs.stanford.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yeung_End-To-End_Learning_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+0;2;0",
        "aff_unique_norm": "Stanford University;Carnegie Mellon University;Simon Fraser University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.cmu.edu;https://www.sfu.ca",
        "aff_unique_abbr": "Stanford;CMU;SFU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0+0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "End-To-End Learning of Deformable Mixture of Parts and Deep Convolutional Neural Networks for Human Pose Estimation",
        "session": "Actions and Human Pose",
        "status": "Oral",
        "track": "main",
        "pid": "5",
        "author_site": "Wei Yang, Wanli Ouyang, Hongsheng Li, Xiaogang Wang",
        "author": "Wei Yang; Wanli Ouyang; Hongsheng Li; Xiaogang Wang",
        "abstract": "Recently, Deep Convolutional Neural Networks (DCNNs) have been applied to the task of human pose estimation, and have shown its potential of learning better feature representations and capturing contextual relationships. However, it is difficult to incorporate domain prior knowledge such as geometric relationships among body parts into DCNNs. In addition, training DCNN-based body part detectors without consideration of global body joint consistency introduces ambiguities, which increases the complexity of training. In this paper, we propose a novel end-to-end framework for human pose estimation that combines DCNNs with the expressive deformable mixture of parts. We explicitly incorporate domain prior knowledge into the framework, which greatly regularizes the learning process and enables the flexibility of our framework for  loopy models or tree-structured models. The effectiveness of jointly learning a DCNN with a deformable mixture of parts model is evaluated through intensive experiments on several widely used benchmarks. The proposed approach significantly improves the performance compared with state-of-the-art approaches, especially on benchmarks with challenging articulations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_End-To-End_Learning_of_CVPR_2016_paper.pdf",
        "aff": "Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Yang_End-To-End_Learning_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3109843,
        "gs_citation": 346,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15832375094889422316&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_End-To-End_Learning_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "End-To-End People Detection in Crowded Scenes",
        "session": "Object Detection 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "7",
        "author_site": "Russell Stewart, Mykhaylo Andriluka, Andrew Y. Ng",
        "author": "Russell Stewart; Mykhaylo Andriluka; Andrew Y. Ng",
        "abstract": "Current people detectors operate either by scanning an image in a sliding window fashion or by classifying a discrete set of proposals. We propose a model that is based on decoding an image into a set of people detections. Our system takes an image as input and directly outputs a set of distinct detection hypotheses. Because we generate predictions jointly, common post-processing steps such as non-maximum suppression are unnecessary. We use a recurrent LSTM layer for sequence generation and train our model end-to-end with a new loss function that operates on sets of detections.  We demonstrate the effectiveness of our approach on the challenging task of detecting people in crowded scenes",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Stewart_End-To-End_People_Detection_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1142966,
        "gs_citation": 684,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10356613063738074019&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Stewart_End-To-End_People_Detection_CVPR_2016_paper.html"
    },
    {
        "title": "End-To-End Saliency Mapping via Probability Distribution Prediction",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "49",
        "author_site": "Saumya Jetley, Naila Murray, Eleonora Vig",
        "author": "Saumya Jetley; Naila Murray; Eleonora Vig",
        "abstract": "Most saliency estimation methods aim to explicitly model low-level conspicuity cues such as edges or blobs and may additionally incorporate top-down cues using face or text detection. Data-driven methods for training saliency models using eye-fixation data are increasingly popular, particularly with the introduction of large-scale datasets and deep architectures. However, current methods in this latter paradigm use loss functions designed for classification or regression tasks whereas saliency estimation is evaluated on topographical maps. In this work, we introduce a new saliency map model which formulates a map as a generalized Bernoulli distribution. We then train a deep architecture to predict such maps using novel loss functions which pair the softmax activation function with measures designed to compute distances between probability distributions. We show in extensive experiments the effectiveness of such loss functions over standard ones on four public benchmark datasets, and demonstrate improved performance over state-of-the-art saliency methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Jetley_End-To-End_Saliency_Mapping_CVPR_2016_paper.pdf",
        "aff": "University of Oxford; XRCE; XRCE+German Aerospace Center",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1582047,
        "gs_citation": 186,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4684264694230695771&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "robots.ox.ac.uk;xrce.xerox.com;dlr.de",
        "email": "robots.ox.ac.uk;xrce.xerox.com;dlr.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Jetley_End-To-End_Saliency_Mapping_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1+2",
        "aff_unique_norm": "University of Oxford;XRCE;German Aerospace Center",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.xrce.xerox.com;https://www.dlr.de",
        "aff_unique_abbr": "Oxford;XRCE;DLR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1+2",
        "aff_country_unique": "United Kingdom;France;Germany"
    },
    {
        "title": "Equiangular Kernel Dictionary Learning With Applications to Dynamic Texture Analysis",
        "session": "Feature Extraction and Description",
        "status": "Poster",
        "track": "main",
        "pid": "33",
        "author_site": "Yuhui Quan, Chenglong Bao, Hui Ji",
        "author": "Yuhui Quan; Chenglong Bao; Hui Ji",
        "abstract": "Most existing dictionary learning algorithms consider a linear sparse model, which often  cannot effectively characterize the nonlinear properties present in many types of visual data, e.g. dynamic texture (DT). Such nonlinear properties can be exploited by the so-called  kernel sparse coding. This paper proposed an equiangular kernel dictionary learning method with optimal mutual coherence to exploit  the nonlinear sparsity of  high-dimensional visual data. Two main issues are addressed in the proposed method: (1) coding stability for redundant dictionary of infinite-dimensional space; and (2) computational efficiency for computing kernel matrix of training samples of high-dimensional data. The proposed kernel sparse coding method is applied to dynamic texture analysis with both local DT pattern extraction and global DT pattern characterization. The experimental results showed its performance gain over existing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Quan_Equiangular_Kernel_Dictionary_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science &Engineering, South China Univ. of Tech., Guangzhou 510006, China+Department of Mathematics, National University of Singapore, Singapore 117542; Department of Mathematics, National University of Singapore, Singapore 117542; Department of Mathematics, National University of Singapore, Singapore 117542",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Quan_Equiangular_Kernel_Dictionary_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 865777,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5837249934095783046&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "scut.edu.cn;nus.edu.sg;nus.edu.sg",
        "email": "scut.edu.cn;nus.edu.sg;nus.edu.sg",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Quan_Equiangular_Kernel_Dictionary_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "South China University of Technology;National University of Singapore",
        "aff_unique_dep": "School of Computer Science & Engineering;Department of Mathematics",
        "aff_unique_url": "https://www.scut.edu.cn;https://www.nus.edu.sg",
        "aff_unique_abbr": "SCUT;NUS",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Guangzhou;",
        "aff_country_unique_index": "0+1;1;1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "title": "Estimating Correspondences of Deformable Objects \"In-The-Wild\"",
        "session": "Image Alignment and Registration",
        "status": "Poster",
        "track": "main",
        "pid": "53",
        "author_site": "Yuxiang Zhou, Epameinondas Antonakos, Joan Alabort-i-Medina, Anastasios Roussos, Stefanos Zafeiriou",
        "author": "Yuxiang Zhou; Epameinondas Antonakos; Joan Alabort-i-Medina; Anastasios Roussos; Stefanos Zafeiriou",
        "abstract": "During the past few years we have witnessed the development of many methodologies for building and fitting Statistical Deformable Models (SDMs). The construction of accurate SDMs requires careful annotation of images with regards to a consistent set of landmarks. However, the manual annotation of a large amount of images is a tedious, laborious and expensive procedure. Furthermore, for several deformable objects, e.g. human body, it is difficult to define a consistent set of landmarks, and, thus, it becomes impossible to train humans in order to accurately annotate a collection of images. Nevertheless, for the majority of objects, it is possible to extract the shape by object segmentation or even by shape drawing. In this paper, we show for the first time, to the best of our knowledge, that it is possible to construct SDMs by putting object shapes in dense correspondence. Such SDMs can be built with much less effort for a large battery of objects. Additionally, we show that, by sampling the dense model, a part-based SDM can be learned with its parts being in correspondence. We employ our framework to develop SDMs of human arms and legs, which can be used for the segmentation of the outline of the human body, as well as to provide better and more consistent annotations for body joints.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Estimating_Correspondences_of_CVPR_2016_paper.pdf",
        "aff": "Department of Computing, Imperial College London, U.K.; Department of Computing, Imperial College London, U.K.; Department of Computing, Imperial College London, U.K.; Department of Computing, Imperial College London, U.K.; Department of Computing, Imperial College London, U.K. + Center for Machine Vision and Signal Analysis, University of Oulu, Finland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhou_Estimating_Correspondences_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5419987,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3891252600622231826&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Estimating_Correspondences_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "Imperial College London;University of Oulu",
        "aff_unique_dep": "Department of Computing;Center for Machine Vision and Signal Analysis",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.oulu.fi",
        "aff_unique_abbr": "Imperial;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0;0;0;0+1",
        "aff_country_unique": "United Kingdom;Finland"
    },
    {
        "title": "Estimating Sparse Signals With Smooth Support via Convex Programming and Block Sparsity",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "65",
        "author_site": "Sohil Shah, Tom Goldstein, Christoph Studer",
        "author": "Sohil Shah; Tom Goldstein; Christoph Studer",
        "abstract": "Conventional algorithms for sparse signal recovery and sparse representation rely on l1-norm regularized variational methods. However, when applied to the reconstruction of sparse images, i.e., images where only a few pixels are non-zero, simple l1-norm-based methods ignore poten- tial correlations in the support between adjacent pixels. In a number of applications, one is interested in images that are not only sparse, but also have a support with smooth (or contiguous) boundaries. Existing algorithms that take into account such a support structure mostly rely on non-convex methods and--as a consequence--do not scale well to high-dimensional problems and/or do not converge to global optima. In this paper, we explore the use of new block l1-norm regularizers, which enforce image sparsity while simultaneously promoting smooth support structure. By exploiting the convexity of our regularizers, we develop new computationally-efficient recovery algorithms that guarantee global optimality. We demonstrate the efficacy of our regularizers on a variety of imaging tasks including compressive image recovery, image restoration, and robust PCA.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shah_Estimating_Sparse_Signals_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1143115,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17990490962520609493&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shah_Estimating_Sparse_Signals_CVPR_2016_paper.html"
    },
    {
        "title": "Event-Specific Image Importance",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "28",
        "author_site": "Yufei Wang, Zhe Lin, Xiaohui Shen, Radom\u00edr M\u0115ch, Gavin Miller, Garrison W. Cottrell",
        "author": "Yufei Wang; Zhe Lin; Xiaohui Shen; Radomir Mech; Gavin Miller; Garrison W. Cottrell",
        "abstract": "When creating a photo album of an event, people typically select a few important images to keep or share. There is some consistency in the process of choosing the important images, and discarding the unimportant ones. Modeling this selection process will assist automatic photo selection and album summarization. In this paper, we show that the selection of important images is consistent among different viewers, and that this selection process is related to the event type of the album. We introduce the concept of event-specific image importance. We collected a new event album dataset with human annotation of the relative image importance with each event album. We also propose a Convolutional Neural Network (CNN) based method to predict the image importance score of a given event album, using a novel rank loss function and a progressive training scheme. Results demonstrate that our method significantly outperforms various baseline methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Event-Specific_Image_Importance_CVPR_2016_paper.pdf",
        "aff": "University of California, San Diego; Adobe Research; Adobe Research; Adobe Research; Adobe Research; University of California, San Diego",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wang_Event-Specific_Image_Importance_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1195926,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=631592716176278540&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ucsd.edu;adobe.com;adobe.com;adobe.com;adobe.com;ucsd.edu",
        "email": "ucsd.edu;adobe.com;adobe.com;adobe.com;adobe.com;ucsd.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Event-Specific_Image_Importance_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "University of California, San Diego;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.ucsd.edu;https://research.adobe.com",
        "aff_unique_abbr": "UCSD;Adobe",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Exemplar-Driven Top-Down Saliency Detection via Deep Association",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "46",
        "author_site": "Shengfeng He, Rynson W.H. Lau, Qingxiong Yang",
        "author": "Shengfeng He; Rynson W.H. Lau; Qingxiong Yang",
        "abstract": "Top-down saliency detection is a knowledge-driven search task. While some previous methods aim to learn this \"knowledge\" from category-specific data, others transfer existing annotations in a large dataset through appearance matching. In contrast, we propose in this paper a locate-by-exemplar strategy. This approach is challenging, as we only use a few exemplars (up to 4) and the appearances among the query object and the exemplars can be very different. To address it, we design a two-stage deep model to learn the intra-class association between the exemplars and query objects. The first stage is for learning object-to-object association, and the second stage is to learn background discrimination. Extensive experimental evaluations show that the proposed method outperforms different baselines and the category-specific models. In addition, we explore the influence of exemplar properties, in terms of exemplar number and quality. Furthermore, we show that the learned model is a universal model and offers great generalization to unseen objects.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Exemplar-Driven_Top-Down_Saliency_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, City University of Hong Kong; Department of Computer Science, City University of Hong Kong; School of Information Science and Technology, University of Science and Technology of China",
        "project": "http://www.shengfenghe.com/exemplarsaliency.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2216625,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6111126653585254700&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/He_Exemplar-Driven_Top-Down_Saliency_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "City University of Hong Kong;University of Science and Technology of China",
        "aff_unique_dep": "Department of Computer Science;School of Information Science and Technology",
        "aff_unique_url": "https://www.cityu.edu.hk;http://www.ustc.edu.cn",
        "aff_unique_abbr": "CityU;USTC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Exploit All the Layers: Fast and Accurate CNN Object Detector With Scale Dependent Pooling and Cascaded Rejection Classifiers",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "68",
        "author_site": "Fan Yang, Wongun Choi, Yuanqing Lin",
        "author": "Fan Yang; Wongun Choi; Yuanqing Lin",
        "abstract": "In this paper, we investigate two new strategies to detect objects accurately and efficiently using deep convolutional neural network: 1) scale-dependent pooling and 2) layer-wise cascaded rejection classifiers. The scale-dependent pooling (SDP) improves detection accuracy by exploiting appropriate convolutional features depending on the scale of candidate object proposals. The cascaded rejection classifiers (CRC) effectively utilize convolutional features and eliminate negative object proposals in a cascaded manner, which greatly speeds up the detection while maintaining high accuracy. In combination of the two, our method achieves significantly better accuracy compared to other state-of-the-arts in three challenging datasets, PASCAL object detection challenge, KITTI object detection benchmark and newly collected Inner-city dataset, while being more efficient.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_Exploit_All_the_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, University of Maryland College Park + NEC Laboratories America; NEC Laboratories America; NEC Laboratories America",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1980330,
        "gs_citation": 751,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4792353051001537191&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.umd.edu;nec-labs.com;nec-labs.com",
        "email": "cs.umd.edu;nec-labs.com;nec-labs.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_Exploit_All_the_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "University of Maryland, College Park;NEC Laboratories America",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www/umd.edu;https://www.nec-labs.com",
        "aff_unique_abbr": "UMD;NEC Labs America",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "College Park;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Exploit Bounding Box Annotations for Multi-Label Object Recognition",
        "session": "Feature Extraction and Description",
        "status": "Poster",
        "track": "main",
        "pid": "30",
        "author_site": "Hao Yang, Joey Tianyi Zhou, Yu Zhang, Bin-Bin Gao, Jianxin Wu, Jianfei Cai",
        "author": "Hao Yang; Joey Tianyi Zhou; Yu Zhang; Bin-Bin Gao; Jianxin Wu; Jianfei Cai",
        "abstract": "Convolutional neural networks (CNNs) have shown great performance as general feature representations for object recognition applications. However, for multi-label images that contain multiple objects from different categories, scales and locations, global CNN features are not optimal. In this paper, we incorporate local information to enhance the feature discriminative power. In particular, we first extract object proposals from each image. With each image treated as a bag and object proposals extracted from it treated as instances, we transform the multi-label recognition problem into a multi-class multi-instance learning problem. Then, in addition to extracting the typical CNN feature representation from each proposal, we propose to make use of ground-truth bounding box annotations (strong labels) to add another level of local information by using nearest-neighbor relationships of local regions to form a multi-view pipeline. The proposed multi-view multi-instance framework utilizes both weak and strong labels effectively, and more importantly it has the generalization ability to even boost the performance of unseen categories by partial strong labels from other categories. Our framework is extensively compared with state-of-the-art hand-crafted feature based methods and CNN based methods on two multi-label benchmark datasets. The experimental results validate the discriminative power and the generalization ability of the proposed framework. With strong labels, our framework is able to achieve state-of-the-art results in both datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_Exploit_Bounding_Box_CVPR_2016_paper.pdf",
        "aff": "SCE, Nanyang Technological University; IHPC, A*STAR; Bioinformatics Institute, A*STAR; National Key Laboratory for Novel Software Technology, Nanjing University; National Key Laboratory for Novel Software Technology, Nanjing University; SCE, Nanyang Technological University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 590555,
        "gs_citation": 210,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15186700548485033964&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "gmail.com;ihpc.a-star.edu.sg;bii.a-star.edu.sg;lamda.nju.edu.cn;nju.edu.cn;ntu.edu.sg",
        "email": "gmail.com;ihpc.a-star.edu.sg;bii.a-star.edu.sg;lamda.nju.edu.cn;nju.edu.cn;ntu.edu.sg",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_Exploit_Bounding_Box_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;3;3;0",
        "aff_unique_norm": "Nanyang Technological University;A*STAR;Agency for Science, Technology and Research;Nanjing University",
        "aff_unique_dep": "School of Computer Engineering;Institute of High Performance Computing;Bioinformatics Institute;National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.a-star.edu.sg;https://www.a-star.edu.sg;http://www.nju.edu.cn",
        "aff_unique_abbr": "NTU;A*STAR;A*STAR;Nanjing University",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;1;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "title": "Exploiting Spectral-Spatial Correlation for Coded Hyperspectral Image Restoration",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "75",
        "author_site": "Ying Fu, Yinqiang Zheng, Imari Sato, Yoichi Sato",
        "author": "Ying Fu; Yinqiang Zheng; Imari Sato; Yoichi Sato",
        "abstract": "Conventional scanning and multiplexing techniques for hyperspectral imaging suffer from limited temporal and/or spatial resolution. To resolve this issue, coding techniques are becoming increasingly popular in developing   snapshot systems for high-resolution hyperspectral imaging. For such systems, it is    a critical task to accurately restore the 3D hyperspectral image    from its corresponding coded 2D image. In this paper, we propose an    effective method for coded hyperspectral image restoration, which exploits extensive structure sparsity in the hyperspectral image. Specifically, we simultaneously explore spectral and spatial correlation via low-rank regularizations, and formulate the restoration problem into a variational optimization model, which can be solved via an iterative numerical algorithm. Experimental   results using both synthetic data and real images show that the proposed method can significantly outperform the    state-of-the-art methods on several popular coding-based hyperspectral imaging systems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Fu_Exploiting_Spectral-Spatial_Correlation_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2782469,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3466761348508793767&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Fu_Exploiting_Spectral-Spatial_Correlation_CVPR_2016_paper.html"
    },
    {
        "title": "Eye Tracking for Everyone",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "73",
        "author_site": "Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Suchendra Bhandarkar, Wojciech Matusik, Antonio Torralba",
        "author": "Kyle Krafka; Aditya Khosla; Petr Kellnhofer; Harini Kannan; Suchendra Bhandarkar; Wojciech Matusik; Antonio Torralba",
        "abstract": "From scientific research to commercial applications, eye tracking is an important tool across many domains. Despite its range of applications, eye tracking has yet to become a pervasive technology. We believe that we can put the power of eye tracking in everyone's palm by building eye tracking software that works on commodity hardware such as mobile phones and tablets, without the need for additional sensors or devices. We tackle this problem by introducing GazeCapture, the first large-scale dataset for eye tracking, containing data from over 1450 people consisting of almost 2:5M frames. Using GazeCapture, we train iTracker, a convolutional neural network for eye tracking, which achieves a significant reduction in error over previous approaches while running in real time (10-15fps) on a modern mobile device. Our model achieves a prediction error of 1.71cm and 2.53cm without calibration on mobile phones and tablets respectively. With calibration, this is reduced to 1.34cm and 2.12cm. Further, we demonstrate that the features learned by iTracker generalize well to other datasets, achieving state-of-the-art results. The code, data, and models are available at http://gazecapture.csail.mit.edu.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Krafka_Eye_Tracking_for_CVPR_2016_paper.pdf",
        "aff": "University of Georgia; Massachusetts Institute of Technology+MPI Informatik; Massachusetts Institute of Technology; Massachusetts Institute of Technology; University of Georgia; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "project": "http://gazecapture.csail.mit.edu",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2403373,
        "gs_citation": 1274,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9163144060397946813&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.uga.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu;cs.uga.edu;csail.mit.edu;csail.mit.edu",
        "email": "cs.uga.edu;csail.mit.edu;csail.mit.edu;csail.mit.edu;cs.uga.edu;csail.mit.edu;csail.mit.edu",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Krafka_Eye_Tracking_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+2;1;1;0;1;1",
        "aff_unique_norm": "University of Georgia;Massachusetts Institute of Technology;Max Planck Institute for Informatics",
        "aff_unique_dep": ";;Informatik",
        "aff_unique_url": "https://www.uga.edu;https://web.mit.edu;https://www.mpi-inf.mpg.de",
        "aff_unique_abbr": "UGA;MIT;MPII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0;0;0;0;0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "FANNG: Fast Approximate Nearest Neighbour Graphs",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "45",
        "author_site": "Ben Harwood, Tom Drummond",
        "author": "Ben Harwood; Tom Drummond",
        "abstract": "We present a new method for approximate nearest neighbour search on large datasets of high dimensional feature vectors, such as SIFT or GIST descriptors.  Our approach constructs a directed graph that can be efficiently explored for nearest neighbour queries. Each vertex in this graph represents a feature vector from the dataset being searched.  The directed edges are computed by exploiting the fact that, for these datasets, the intrinsic dimensionality of the local manifold-like structure formed by the elements of the dataset is significantly lower than the embedding space.  We also provide an efficient search algorithm that uses this graph to rapidly find the nearest neighbour to a query with high probability.   We show how the method can be adapted to give a strong guarantee of 100% recall where the query is within a threshold distance of its nearest neighbour. We demonstrate that our method is significantly more efficient than existing state of the art methods.  In particular, our GPU implementation can deliver 90% recall for queries on a data set of 1 million SIFT descriptors at a rate of over 1.2 million queries per second on a Titan X. Finally we also demonstrate how our method scales to datasets of 5M and 20M entries.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Harwood_FANNG_Fast_Approximate_CVPR_2016_paper.pdf",
        "aff": "Department of Electrical and Computer Systems Engineering, Monash University, Victoria, Australia; Department of Electrical and Computer Systems Engineering, Monash University, Victoria, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 523873,
        "gs_citation": 172,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15267137493898642180&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "monash.edu;monash.edu",
        "email": "monash.edu;monash.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Harwood_FANNG_Fast_Approximate_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Monash University",
        "aff_unique_dep": "Department of Electrical and Computer Systems Engineering",
        "aff_unique_url": "https://www.monash.edu",
        "aff_unique_abbr": "Monash",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Victoria",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Face Alignment Across Large Poses: A 3D Solution",
        "session": "Matching and Alignment",
        "status": "Oral",
        "track": "main",
        "pid": "16",
        "author_site": "Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, Stan Z. Li",
        "author": "Xiangyu Zhu; Zhen Lei; Xiaoming Liu; Hailin Shi; Stan Z. Li",
        "abstract": "Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in CV community. However, most algorithms are designed for faces in small to medium poses (below 45 degree), lacking the ability to align faces in large-pose up to 90 degree. The challenges are three-fold: Firstly, the commonly used landmark-based face model assumes that all the landmarks are visible and is therefore not suitable for profile views. Secondly, the face appearance varies more dramatically in large poses, ranging from frontal view to profile view. Thirdly, labelling landmarks in large poses is an extremely challenging work since the invisible landmarks have to be guessed. In this paper, we propose a solution to the three problems in an new alignment framework, called 3D Dense Face Alignment (3DDFA), in which a dense 3D face model is fitted to the image via convolutional neutral network (CNN). We also propose a method to synthesize large-scale training samples in profile views to solve the third problem of data labelling. Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_Face_Alignment_Across_CVPR_2016_paper.pdf",
        "aff": "Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; Department of Computer Science and Engineering, Michigan State University; Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; Center for Biometrics and Security Research & National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhu_Face_Alignment_Across_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4885145,
        "gs_citation": 1424,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14925470721697460715&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;msu.edu;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;msu.edu;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_Face_Alignment_Across_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;Michigan State University",
        "aff_unique_dep": "Institute of Automation;Department of Computer Science and Engineering",
        "aff_unique_url": "http://www.ia.cas.cn;https://www.msu.edu",
        "aff_unique_abbr": "CAS;MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Face2Face: Real-Time Face Capture and Reenactment of RGB Videos",
        "session": "Computational Photography and Faces",
        "status": "Oral",
        "track": "main",
        "pid": "14",
        "author_site": "Justus Thies, Michael Zollh\u00f6fer, Marc Stamminger, Christian Theobalt, Matthias Nie\u00dfner",
        "author": "Justus Thies; Michael Zollhofer; Marc Stamminger; Christian Theobalt; Matthias Niessner",
        "abstract": "We present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. Reenactment is then achieved by fast and efficient deformation transfer between source and target. The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Thies_Face2Face_Real-Time_Face_CVPR_2016_paper.pdf",
        "aff": "University of Erlangen-Nuremberg; Max-Planck-Institute for Informatics; University of Erlangen-Nuremberg; Max-Planck-Institute for Informatics; Stanford University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Thies_Face2Face_Real-Time_Face_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3191960,
        "gs_citation": 2654,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2535222170250033054&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 31,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Thies_Face2Face_Real-Time_Face_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1;2",
        "aff_unique_norm": "Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg;Max-Planck-Institute for Informatics;Stanford University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www fau.de;https://mpi-inf.mpg.de;https://www.stanford.edu",
        "aff_unique_abbr": "FAU;MPII;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "Germany;United States"
    },
    {
        "title": "Facial Expression Intensity Estimation Using Ordinal Information",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "47",
        "author_site": "Rui Zhao, Quan Gan, Shangfei Wang, Qiang Ji",
        "author": "Rui Zhao; Quan Gan; Shangfei Wang; Qiang Ji",
        "abstract": "Previous studies on facial expression analysis have been focused on recognizing basic expression categories. There is limited amount of work on the continuous expression intensity estimation, which is important for detecting and tracking emotion change. Part of the reason is the lack of labeled data with annotated expression intensity since expression intensity annotation requires expertise and is time consuming. In this work, we treat the expression intensity estimation as a regression problem. By taking advantage of the natural onset-apex-offset evolution pattern of facial expression, the proposed method can handle different amounts of annotations to perform frame-level expression intensity estimation. In fully supervised case, all the frames are provided with intensity annotations. In weakly supervised case, only the annotations of selected key frames are used. While in unsupervised case, expression intensity can be estimated without any annotations. An efficient optimization algorithm based on Alternating Direction Method of Multipliers (ADMM) is developed for solving the optimization problem associated with parameter learning. We demonstrate the effectiveness of proposed method by comparing it against both fully supervised and unsupervised approaches on benchmark facial expression datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhao_Facial_Expression_Intensity_CVPR_2016_paper.pdf",
        "aff": "Department of Electrical, Computer & Systems Engineering, Rensselaer Polytechnic Institute; School of Computer Science and Technology, University of Science and Technology of China; School of Computer Science and Technology, University of Science and Technology of China; Department of Electrical, Computer & Systems Engineering, Rensselaer Polytechnic Institute",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhao_Facial_Expression_Intensity_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 795506,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1088408797870659710&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "rpi.edu;mail.ustc.edu.cn;ustc.edu.cn;rpi.edu",
        "email": "rpi.edu;mail.ustc.edu.cn;ustc.edu.cn;rpi.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhao_Facial_Expression_Intensity_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Rensselaer Polytechnic Institute;University of Science and Technology of China",
        "aff_unique_dep": "Department of Electrical, Computer & Systems Engineering;School of Computer Science and Technology",
        "aff_unique_url": "https://www.rpi.edu;http://www.ustc.edu.cn",
        "aff_unique_abbr": "RPI;USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Factors in Finetuning Deep Model for Object Detection With Long-Tail Distribution",
        "session": "Object Detection 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "12",
        "author_site": "Wanli Ouyang, Xiaogang Wang, Cong Zhang, Xiaokang Yang",
        "author": "Wanli Ouyang; Xiaogang Wang; Cong Zhang; Xiaokang Yang",
        "abstract": "Finetuning from a pretrained deep model is found to yield state-of-the-art performance for many vision tasks. This paper investigates many factors that influence the performance in finetuning for object detection.   There is a long-tailed distribution of sample numbers for classes in object detection. Our analysis and empirical results show that classes with more samples have higher impact on the feature learning. And it is better to make the sample number more uniform across classes. Generic object detection can be considered as multiple equally important tasks. Detection of each class is a task. These classes/tasks have their individuality in discriminative visual appearance representation. Taking this individuality into account, we cluster objects into visually similar class groups and learn deep representations for these groups separately. A hierarchical feature learning  scheme is proposed. In this scheme, the knowledge from the group with large number of classes is transferred for learning features in its sub-groups. Finetuned on the GoogLeNet model, experimental results show 4.7% absolute mAP improvement of our approach on the ImageNet object detection dataset without increasing much computational cost at the testing stage.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ouyang_Factors_in_Finetuning_CVPR_2016_paper.pdf",
        "aff": "The Chinese University of Hong Kong; The Chinese University of Hong Kong; Shanghai Jiaotong University; Shanghai Jiaotong University",
        "project": "www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 992599,
        "gs_citation": 256,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16601034870430193568&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;sjtu.edu.cn;sjtu.edu.cn",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;sjtu.edu.cn;sjtu.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ouyang_Factors_in_Finetuning_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "Chinese University of Hong Kong;Shanghai Jiao Tong University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "CUHK;SJTU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Fashion Style in 128 Floats: Joint Ranking and Classification Using Weak Data for Feature Extraction",
        "session": "Feature Extraction and Description",
        "status": "Poster",
        "track": "main",
        "pid": "32",
        "author_site": "Edgar Simo-Serra, Hiroshi Ishikawa",
        "author": "Edgar Simo-Serra; Hiroshi Ishikawa",
        "abstract": "We propose a novel approach for learning features from weakly-supervised data by joint ranking and classification. In order to exploit data with weak labels, we jointly train a feature extraction network with a ranking loss and a classification network with a cross-entropy loss. We obtain high-quality compact discriminative features with few parameters, learned on relatively small datasets without additional annotations. This enables us to tackle tasks with specialized images not very similar to the more generic ones in existing fully-supervised datasets. We show that the resulting features in combination with a linear classifier surpass the state-of-the-art on the Hipster Wars dataset despite using features only 0.3% of the size. Our proposed features significantly outperform those obtained from networks trained on ImageNet, despite being 32 times smaller (128 single-precision floats), trained on noisy and weakly-labeled data, and using only 1.5% of the number of parameters.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Simo-Serra_Fashion_Style_in_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science and Engineering, Waseda University, Tokyo, Japan; Department of Computer Science and Engineering, Waseda University, Tokyo, Japan",
        "project": "http://hi.cs.waseda.ac.jp/ esimo/research/stylenet/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Simo-Serra_Fashion_Style_in_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1700394,
        "gs_citation": 194,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=24076339492166002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "aoni.waseda.jp;waseda.jp",
        "email": "aoni.waseda.jp;waseda.jp",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Simo-Serra_Fashion_Style_in_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Waseda University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.waseda.jp/top",
        "aff_unique_abbr": "Waseda",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Fast Algorithms for Convolutional Neural Networks",
        "session": "Statistical Methods and Transfer Learning",
        "status": "Spotlight",
        "track": "main",
        "pid": "24",
        "author_site": "Andrew Lavin, Scott Gray",
        "author": "Andrew Lavin; Scott Gray",
        "abstract": "Deep convolutional neural networks take GPU-days of computation to train on large data sets. Pedestrian detection for self driving cars requires very low latency. Image recognition for mobile phones is constrained by limited processing resources. The success of convolutional neural networks in these situations is limited by how fast we can compute them. Conventional FFT based convolution is fast for large filters, but state of the art convolutional neural networks use small, 3x3 filters. We introduce a new class of fast algorithms for convolutional neural networks using Winograd's minimal filtering algorithms. The algorithms compute minimal complexity convolution over small tiles, which makes them fast with small filters and small batch sizes. We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lavin_Fast_Algorithms_for_CVPR_2016_paper.pdf",
        "aff": "ACM; Nervana Systems",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Lavin_Fast_Algorithms_for_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 520380,
        "gs_citation": 1287,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4107269088164149002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "acm.org;nervanasys.com",
        "email": "acm.org;nervanasys.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lavin_Fast_Algorithms_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Association for Computing Machinery;Nervana Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.acm.org;https://www.nervanasystems.com",
        "aff_unique_abbr": "ACM;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fast Algorithms for Linear and Kernel SVM+",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "82",
        "author_site": "Wen Li, Dengxin Dai, Mingkui Tan, Dong Xu, Luc Van Gool",
        "author": "Wen Li; Dengxin Dai; Mingkui Tan; Dong Xu; Luc Van Gool",
        "abstract": "The SVM+ approach has shown excellent performance in visual recognition tasks for exploiting privileged information in the   training data. In this paper, we propose two efficient algorithms for solving the linear and kernel SVM+, respectively. For linear SVM+, we absorb the bias term into the weight vector, and formulate a new optimization problem with simpler constraints in the dual form. Then, we develop an efficient dual coordinate descent algorithm to solve the new optimization problem. For kernel SVM+, we further apply the l2-loss, which leads to a simpler optimization problem in the dual form with only half of dual variables when compared with the dual form of the original SVM+ method. More interestingly, we show that our new dual problem can be efficiently solved by using the SMO algorithm of the one-class SVM problem. Comprehensive experiments on three datasets clearly demonstrate that our proposed algorithms achieve significant speed-up than the state-of-the-art solvers for linear and kernel SVM+.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Fast_Algorithms_for_CVPR_2016_paper.pdf",
        "aff": "Computer Vision Laboratory, ETH Z\u00fcrich, Switzerland; Computer Vision Laboratory, ETH Z\u00fcrich, Switzerland; School of Computer Science, University of Adelaide, Australia; School of Electrical and Information Engineering, University of Sydney, Australia; Computer Vision Laboratory, ETH Z\u00fcrich, Switzerland+VISICS, ESAT/PSI, KU Leuven, Belgium",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 515761,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14332054601668616025&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Fast_Algorithms_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;2;0+3",
        "aff_unique_norm": "ETH Zurich;University of Adelaide;University of Sydney;KU Leuven",
        "aff_unique_dep": "Computer Vision Laboratory;School of Computer Science;School of Electrical and Information Engineering;VISICS, ESAT/PSI",
        "aff_unique_url": "https://www.ethz.ch;https://www.adelaide.edu.au;https://www.sydney.edu.au;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;Adelaide;USYD;KU Leuven",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Z\u00fcrich;",
        "aff_country_unique_index": "0;0;1;1;0+2",
        "aff_country_unique": "Switzerland;Australia;Belgium"
    },
    {
        "title": "Fast ConvNets Using Group-Wise Brain Damage",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "32",
        "author_site": "Vadim Lebedev, Victor Lempitsky",
        "author": "Vadim Lebedev; Victor Lempitsky",
        "abstract": "We revisit the idea of brain damage, i.e. the pruning of the coefficients of a neural network, and suggest how brain damage can be modified and used to speedup convolutional layers in ConvNets. The approach uses the fact that many efficient implementations reduce generalized convolutions to matrix multiplications. The suggested brain damage process prunes the convolutional kernel tensor in a group-wise fashion. After such pruning, convolutions can be reduced to multiplications of thinned dense matrices, which leads to speedup. We investigate different ways to add group-wise prunning to the learning process, and show that several-fold speedups of convolutional layers can be attained using group-sparsity regularizers. Our approach can adjust the shapes of the receptive fields in the convolutional layers, and even prune excessive feature maps from ConvNets, all in a data-driven way.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lebedev_Fast_ConvNets_Using_CVPR_2016_paper.pdf",
        "aff": "Skolkovo Institute of Science and Technology (Skoltech) + Yandex; Skolkovo Institute of Science and Technology (Skoltech)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 563367,
        "gs_citation": 579,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13493663947999671584&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "skoltech.ru;skoltech.ru",
        "email": "skoltech.ru;skoltech.ru",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lebedev_Fast_ConvNets_Using_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Skolkovo Institute of Science and Technology;Yandex",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.skoltech.ru;https://yandex.com",
        "aff_unique_abbr": "Skoltech;Yandex",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Russian Federation"
    },
    {
        "title": "Fast Detection of Curved Edges at Low SNR",
        "session": "Edge Contour Detection",
        "status": "Poster",
        "track": "main",
        "pid": "23",
        "author_site": "Nati Ofir, Meirav Galun, Boaz Nadler, Ronen Basri",
        "author": "Nati Ofir; Meirav Galun; Boaz Nadler; Ronen Basri",
        "abstract": "Detecting edges is a fundamental problem in computer vision with many applications, some involving very noisy images. While most edge detection methods are fast, they perform well only on relatively clean images. Unfortunately, sophisticated methods that are robust to high levels of noise are quite slow. In this paper we develop a novel multiscale method to detect curved edges in noisy images. Even though our algorithm searches for edges over an exponentially large set of candidate curves, its runtime is nearly linear in the total number of image pixels. As we demonstrate experimentally, our algorithm is orders of magnitude faster than previous methods designed to deal with high noise levels.  At the same time it obtains comparable and often superior results to existing methods on a variety of challenging noisy images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ofir_Fast_Detection_of_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1586889,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17903750804562089900&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ofir_Fast_Detection_of_CVPR_2016_paper.html"
    },
    {
        "title": "Fast Temporal Activity Proposals for Efficient Detection of Human Actions in Untrimmed Videos",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "45",
        "author_site": "Fabian Caba Heilbron, Juan Carlos Niebles, Bernard Ghanem",
        "author": "Fabian Caba Heilbron; Juan Carlos Niebles; Bernard Ghanem",
        "abstract": "In many large-scale video analysis scenarios, one is interested in localizing and recognizing human activities that occur in short temporal intervals within long untrimmed videos. Current approaches for activity detection still struggle to handle large-scale video collections and the task remains relatively unexplored. This is in part due to the computational complexity of current action recognition approaches and the lack of a method that proposes fewer intervals in the video, where activity processing can be focused. In this paper, we introduce a proposal method that aims to recover temporal segments containing actions in untrimmed videos. Building on techniques for learning sparse dictionaries, we introduce a learning framework to represent and retrieve activity proposals. We demonstrate the capabilities of our method in not only producing high quality proposals but also in its efficiency. Finally, we show the positive impact our method has on recognition performance when it is used for action detection, while running at 10FPS.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Heilbron_Fast_Temporal_Activity_CVPR_2016_paper.pdf",
        "aff": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia; Department of Computer Science, Stanford University+Universidad del Norte, Colombia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "project": "http://www.cabaf.net/temporalproposals",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Heilbron_Fast_Temporal_Activity_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2984832,
        "gs_citation": 355,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=431499843875644381&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "kaust.edu.sa;cs.stanford.edu;kaust.edu.sa",
        "email": "kaust.edu.sa;cs.stanford.edu;kaust.edu.sa",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Heilbron_Fast_Temporal_Activity_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology;Stanford University;Universidad del Norte",
        "aff_unique_dep": ";Department of Computer Science;",
        "aff_unique_url": "https://www.kaust.edu.sa;https://www.stanford.edu;https://www.uninorte.edu.co",
        "aff_unique_abbr": "KAUST;Stanford;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;1+2;0",
        "aff_country_unique": "Saudi Arabia;United States;Colombia"
    },
    {
        "id": "677b5533a6",
        "title": "Fast Training of Triplet-Based Deep Binary Embedding Networks",
        "site": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhuang_Fast_Training_of_CVPR_2016_paper.html",
        "author": "Bohan Zhuang; Guosheng Lin; Chunhua Shen; Ian Reid",
        "abstract": "In this paper, we aim to learn a mapping (or embedding) from images to a compact binary space in which Hamming distances correspond to a ranking measure for the image retrieval task. We make use of a triplet loss because this has been shown to be most effective for ranking problems. How- ever, training in previous works can be prohibitively expensive due to the fact that optimization is directly performed on the triplet space, where the number of possible triplets for training is cubic in the number of training examples. To address this issue, we propose to formulate high-order binary codes learning as a multi-label classification problem by explicitly separating learning into two interleaved stages. To solve the first stage, we design a large-scale high-order binary codes inference algorithm to reduce the high-order objective to a standard binary quadratic problem such that graph cuts can be used to efficiently infer the binary codes which serve as the labels of each training datum. In the second stage we propose to map the original image to compact binary codes via carefully designed deep convolutional neural networks (CNNs) and the hash- ing function fitting can be solved by training binary CNN classifiers. An incremental/interleaved optimization strategy is proffered to ensure that these two steps are interactive with each other during training for better accuracy. We conduct experiments on several benchmark datasets, which demonstrate both improved training time (by as much as two orders of magnitude) as well as producing state-of-the- art hashing for various retrieval tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhuang_Fast_Training_of_CVPR_2016_paper.pdf",
        "aff": "The University of Adelaide; The University of Adelaide; The University of Adelaide; The University of Adelaide + Australian Centre for Robotic Vision",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 705354,
        "gs_citation": 146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6763639389637362389&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "University of Adelaide;Australian Centre for Robotic Vision",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.adelaide.edu.au;https://roboticvision.org/",
        "aff_unique_abbr": "Adelaide;ACRV",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "e1b7ad1797",
        "title": "Fast Zero-Shot Image Tagging",
        "site": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Fast_Zero-Shot_Image_CVPR_2016_paper.html",
        "author": "Yang Zhang; Boqing Gong; Mubarak Shah",
        "abstract": "The well-known word analogy experiments show that the recent word vectors capture fine-grained linguistic regularities in words by linear vector offsets, but it is unclear how well the simple vector offsets can encode visual regularities over words. We study a particular image-word relevance relation in this paper. Our results tell that, given an image, its relevant tags' word vectors rank ahead of the irrelevant tags' along a principal direction in the word vector space. Inspired by this observation, we propose to solve image tagging by estimating the principal direction for an image. Particularly, we exploit linear mappings and nonlinear deep neural networks to approximate the principal direction from an input image. We arrive at a quite versatile tagging model. It runs fast given a test image, in constant time w.r.t. the training set size. It not only gives rise to superior performance for the conventional tagging task on the NUS-WIDE dataset, but also outperforms competitive baselines on annotating images with previously unseen tags. To this end, we name our approach fast zero-shot image tagging (Fast0Tag) to recognize that it possesses the advantages of both FastTag (Chen et al. 2013) and zero-shot learning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Fast_Zero-Shot_Image_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhang_Fast_Zero-Shot_Image_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15036308763868810850&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3
    },
    {
        "title": "Feature Space Optimization for Semantic Video Segmentation",
        "session": "Semantic Segmentation",
        "status": "Oral",
        "track": "main",
        "pid": "15",
        "author_site": "Abhijit Kundu, Vibhav Vineet, Vladlen Koltun",
        "author": "Abhijit Kundu; Vibhav Vineet; Vladlen Koltun",
        "abstract": "We present an approach to long-range spatio-temporal regularization in semantic video segmentation. Temporal regularization in video is challenging because both the camera and the scene may be in motion. Thus Euclidean distance in the space-time volume is not a good proxy for correspondence. We optimize the mapping of pixels to a Euclidean feature space so as to minimize distances between corresponding points. Structured prediction is performed by a dense CRF that operates on the optimized features. Experimental results demonstrate that the presented approach increases the accuracy and temporal consistency of semantic video segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kundu_Feature_Space_Optimization_CVPR_2016_paper.pdf",
        "aff": "Georgia Tech; Intel Labs; Intel Labs",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2290796,
        "gs_citation": 257,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14460526664834954900&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "; ; ",
        "email": "; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kundu_Feature_Space_Optimization_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Georgia Institute of Technology;Intel",
        "aff_unique_dep": ";Intel Labs",
        "aff_unique_url": "https://www.gatech.edu;https://www.intel.com",
        "aff_unique_abbr": "Georgia Tech;Intel",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fine-Grained Categorization and Dataset Bootstrapping Using Deep Metric Learning With Humans in the Loop",
        "session": "Fine Grained Categorization",
        "status": "Poster",
        "track": "main",
        "pid": "42",
        "author_site": "Yin Cui, Feng Zhou, Yuanqing Lin, Serge Belongie",
        "author": "Yin Cui; Feng Zhou; Yuanqing Lin; Serge Belongie",
        "abstract": "Existing fine-grained visual categorization methods often suffer from three challenges: lack of training data, large number of fine-grained categories, and high intra-class vs. low inter-class variance. In this work we propose a generic iterative framework for fine-grained categorization and dataset bootstrapping that handles these three challenges. Using deep metric learning with humans in the loop, we learn a low dimensional feature embedding with anchor points on manifolds for each category. These anchor points capture intra-class variances and remain discriminative between classes. In each round, images with high confidence scores from our model are sent to humans for labeling. By comparing with exemplar images, labelers mark each candidate image as either a \"true positive\" or a \"false positive.\" True positives are added into our current dataset and false positives are regarded as \"hard negatives\" for our metric learning model. Then the model is re-trained with an expanded dataset and hard negatives for the next round. To demonstrate the effectiveness of the proposed framework, we bootstrap a fine-grained flower dataset with 620 categories from Instagram images. The proposed deep metric learning scheme is evaluated on both our dataset and the CUB-200-2001 Birds dataset. Experimental evaluations show significant performance gain using dataset bootstrapping and demonstrate state-of-the-art results achieved by the proposed deep metric learning methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cui_Fine-Grained_Categorization_and_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, Cornell University + Cornell Tech; NEC Labs America; NEC Labs America; Department of Computer Science, Cornell University + Cornell Tech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1323617,
        "gs_citation": 293,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3223794826182769663&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.cornell.edu;cs.cornell.edu;nec-labs.com;nec-labs.com",
        "email": "cs.cornell.edu;cs.cornell.edu;nec-labs.com;nec-labs.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cui_Fine-Grained_Categorization_and_CVPR_2016_paper.html",
        "aff_unique_index": "0+0;1;1;0+0",
        "aff_unique_norm": "Cornell University;NEC Labs America",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.cornell.edu;https://www.nec-labs.com",
        "aff_unique_abbr": "Cornell;NEC LA",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";New York City",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Fine-Grained Image Classification by Exploring Bipartite-Graph Labels",
        "session": "Fine Grained Categorization",
        "status": "Poster",
        "track": "main",
        "pid": "39",
        "author_site": "Feng Zhou, Yuanqing Lin",
        "author": "Feng Zhou; Yuanqing Lin",
        "abstract": "Given a food image, can a fine-grained object recognition engine tell \"which restaurant which dish\" the food belongs to? Such ultra-fine grained image recognition is the key for many applications like search by images, but it is very challenging because it needs to discern subtle difference between classes while dealing with the scarcity of training data. Fortunately, the ultra-fine granularity naturally brings rich relationships among object classes. This paper proposes a novel approach to exploit the rich relationships through bipartite-graph labels (BGL). We show how to model BGL in an overall convolutional neural networks and the resulting system can be optimized through back-propagation. We also show that it is computationally efficient in inference thanks to the bipartite structure. To facilitate the study, we construct a new food benchmark dataset, which consists of 37,885 food images collected from 6 restaurants and totally 975 menus. Experimental results on this new food and three other datasets demonstrate BGL advances previous works in fine-grained object recognition. An online demo is available at http://www.f-zhou.com/fg_demo/.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Fine-Grained_Image_Classification_CVPR_2016_paper.pdf",
        "aff": "NEC Labs; NEC Labs",
        "project": "http://www.f-zhou.com/fg_demo/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1369373,
        "gs_citation": 168,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15696714991438377085&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "nec-labs.com;nec-labs.com",
        "email": "nec-labs.com;nec-labs.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Fine-Grained_Image_Classification_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "NEC Laboratories",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nec-labs.com",
        "aff_unique_abbr": "NEC Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "FireCaffe: Near-Linear Acceleration of Deep Neural Network Training on Compute Clusters",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "36",
        "author_site": "Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Kurt Keutzer",
        "author": "Forrest N. Iandola; Matthew W. Moskewicz; Khalid Ashraf; Kurt Keutzer",
        "abstract": "Long training times for high-accuracy deep neural networks (DNNs) impede research into new DNN architectures and slow the development of high-accuracy DNNs. In this paper we present FireCaffe, which successfully scales deep neural network training across a cluster of GPUs. We also present a number of best practices to aid in comparing advancements in methods for scaling and accelerating the training of deep neural networks. The speed and scalability of distributed algorithms is almost always limited by the overhead of communicating between servers; DNN training is not an exception to this rule. Therefore, the key consideration here is to reduce communication overhead wherever possible, while not degrading the accuracy of the DNN models that we train. Our approach has three key pillars. First, we select network hardware that achieves high bandwidth between GPU servers -- Infiniband or Cray interconnects are ideal for this. Second, we consider a number of communication algorithms, and we find that reduction trees are more efficient and scalable than the traditional parameter server approach. Third, we optionally increase the batch size to reduce the total quantity of communication during DNN training, and we identify hyperparameters that allow us to reproduce the small-batch accuracy while training with large batch sizes. When training GoogLeNet and Network-in-Network on ImageNet, we achieve a 47x and 39x speedup, respectively, when training on a cluster of 128 GPUs.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Iandola_FireCaffe_Near-Linear_Acceleration_CVPR_2016_paper.pdf",
        "aff": "DeepScale\u2217; UC Berkeley; DeepScale\u2217; UC Berkeley",
        "project": "http://deepscale.ai",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Iandola_FireCaffe_Near-Linear_Acceleration_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 754979,
        "gs_citation": 404,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=609736029335684625&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Iandola_FireCaffe_Near-Linear_Acceleration_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "DeepScale;University of California, Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://deepscale.ai;https://www.berkeley.edu",
        "aff_unique_abbr": "DeepScale;UC Berkeley",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "First Person Action Recognition Using Deep Learned Descriptors",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "39",
        "author_site": "Suriya Singh, Chetan Arora, C. V. Jawahar",
        "author": "Suriya Singh; Chetan Arora; C. V. Jawahar",
        "abstract": "We focus on the problem of wearer's action recognition in first person a.k.a. egocentric videos. This problem is more challenging than third person activity recognition due to unavailability of wearer's pose and sharp movements in the videos caused by the natural head motion of the wearer. Carefully crafted features based on hands and objects cues for the problem have been shown to be successful for limited targeted datasets. We propose convolutional neural networks (CNNs) for end to end learning and classification of wearer's actions. The proposed network makes use of egocentric cues by capturing hand pose, head motion and saliency map. It is compact. It can also be trained from relatively small number of labeled egocentric videos that are available. We show that the proposed network can generalize and give state of the art performance on various disparate egocentric action datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Singh_First_Person_Action_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1309353,
        "gs_citation": 264,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12101736985359932401&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Singh_First_Person_Action_CVPR_2016_paper.html"
    },
    {
        "title": "Fits Like a Glove: Rapid and Reliable Hand Shape Personalization",
        "session": "People and Faces",
        "status": "Spotlight",
        "track": "main",
        "pid": "34",
        "author_site": "David Joseph Tan, Thomas Cashman, Jonathan Taylor, Andrew Fitzgibbon, Daniel Tarlow, Sameh Khamis, Shahram Izadi, Jamie Shotton",
        "author": "David Joseph Tan; Thomas Cashman; Jonathan Taylor; Andrew Fitzgibbon; Daniel Tarlow; Sameh Khamis; Shahram Izadi; Jamie Shotton",
        "abstract": "We present a fast, practical method for personalizing a hand shape basis to an individual user's detailed hand shape using only a small set of depth images. To achieve this, we minimize an energy based on a sum of render-and-compare cost functions called the golden energy. However, this energy is only piecewise continuous, due to pixels crossing occlusion boundaries, and is therefore not obviously amenable to efficient gradient-based optimization. A key insight is that the energy is the combination of a smooth low-frequency function with a high-frequency, low-amplitude, piecewise continuous function. A central finite difference approximation with a suitable step size can therefore jump over the discontinuities to obtain a good approximation to the energy's low-frequency behavior, allowing efficient gradient-based optimization. Experimental results quantitatively demonstrate for the first time that detailed personalized models improve the accuracy of hand tracking and achieve competitive results in both tracking and model registration.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tan_Fits_Like_a_CVPR_2016_paper.pdf",
        "aff": ";;;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Tan_Fits_Like_a_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "gs_citation": 154,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12332145107136314841&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tan_Fits_Like_a_CVPR_2016_paper.html"
    },
    {
        "title": "Force From Motion: Decoding Physical Sensation in a First Person Video",
        "session": "Video Understanding",
        "status": "Oral",
        "track": "main",
        "pid": "5",
        "author_site": "Hyun Soo Park, jyh-Jing Hwang, Jianbo Shi",
        "author": "Hyun Soo Park; jyh-Jing Hwang; Jianbo Shi",
        "abstract": "A first-person video can generate powerful physical sensations of action in an observer. In this paper, we focus on a problem of Force from Motion---decoding the sensation of 1) passive forces such as the gravity, 2) the physical scale of the motion (speed) and space, and 3) active forces exerted by the observer such as pedaling a bike or banking on a ski turn.    The sensation of gravity can be observed in a natural image. We learn this image cue for predicting a gravity direction in a 2D image and integrate the prediction across images to estimate the 3D gravity direction using structure from motion. The sense of physical scale is revealed to us when the body is in a dynamically balanced state. We compute the unknown physical scale of 3D reconstructed camera motion by leveraging the torque equilibrium at a banked turn that relates the centripetal force, gravity, and the body leaning angle. The active force and torque governs 3D egomotion through the physics of rigid body dynamics. Using an inverse dynamics optimization, we directly minimize 2D reprojection error (in video) with respect to 3D world structure, active forces, and additional passive forces such as air drag and friction force. We use structure from motion with the physical scale and gravity direction as an initialization of our bundle adjustment for force estimation. Our method shows quantitatively equivalent reconstruction comparing to IMU measurements in terms of gravity and scale recovery and outperforms method based on 2D optical flow for an active action recognition task. We apply our method to first person videos of mountain biking, urban bike racing, skiing, speedflying with parachute, and wingsuit flying where inertial measurements are not accessible.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Park_Force_From_Motion_CVPR_2016_paper.pdf",
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5140571,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8417503619615399496&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;seas.upenn.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Park_Force_From_Motion_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ForgetMeNot: Memory-Aware Forensic Facial Sketch Matching",
        "session": "People and Faces",
        "status": "Spotlight",
        "track": "main",
        "pid": "30",
        "author_site": "Shuxin Ouyang, Timothy M. Hospedales, Yi-Zhe Song, Xueming Li",
        "author": "Shuxin Ouyang; Timothy M. Hospedales; Yi-Zhe Song; Xueming Li",
        "abstract": "We investigate whether it is possible to improve the performance of automated facial forensic sketch matching by  learning from examples of facial forgetting over time. Forensic facial sketch recognition is a key capability for law enforcement, but remains an unsolved problem. It is extremely challenging because there are three distinct contributors to the domain gap between forensic sketches and photos: The well studied sketch-photo modality gap, and the less studied gaps due to (i) the forgetting process of the eye-witness and (ii) their inability to elucidate their memory. In this paper we address the memory problem head on by introducing a database of 400 forensic sketches created at different time-delays. Based on this database we build a model to reverse the forgetting process. Surprisingly, we show that it is possible to systematically \"un-forget\" facial details. Moreover, it is possible to apply this model to dramatically improve forensic sketch recognition in practice: we achieve state of the art results when matching 195 benchmark forensic sketches against corresponding photos and a 10,030 mugshot database.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ouyang_ForgetMeNot_Memory-Aware_Forensic_CVPR_2016_paper.pdf",
        "aff": "Beijing University of Posts and Telecommunications+Queen Mary University of London, UK; Queen Mary University of London, UK; Queen Mary University of London, UK; Beijing University of Posts and Telecommunications",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Ouyang_ForgetMeNot_Memory-Aware_Forensic_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1968947,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16993486499756034079&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff_domain": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk;bupt.edu.cn",
        "email": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk;bupt.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ouyang_ForgetMeNot_Memory-Aware_Forensic_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;1;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Queen Mary University of London",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.qmul.ac.uk",
        "aff_unique_abbr": "BUPT;QMUL",
        "aff_campus_unique_index": "0+1;1;1;0",
        "aff_campus_unique": "Beijing;London",
        "aff_country_unique_index": "0+1;1;1;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "title": "From Bows to Arrows: Rolling Shutter Rectification of Urban Scenes",
        "session": "Image Enhancement, Restoration, and Texture",
        "status": "Poster",
        "track": "main",
        "pid": "55",
        "author_site": "Vijay Rengarajan, Ambasamudram N. Rajagopalan, Rangarajan Aravind",
        "author": "Vijay Rengarajan; Ambasamudram N. Rajagopalan; Rangarajan Aravind",
        "abstract": "The rule of perspectivity that 'straight-lines-must-remain-straight' is easily inflected in CMOS cameras by distortions introduced by motion. Lines can be rendered as curves due to the row-wise exposure mechanism known as rolling shutter (RS). We solve the problem of correcting distortions arising from handheld cameras due to RS effect from a single image free from motion blur with special relevance to urban scenes. We develop a procedure to extract prominent curves from the RS image since this is essential for deciphering the varying row-wise motion. We pose an optimization problem with line desirability costs based on straightness, angle, and length, to resolve the geometric ambiguities while estimating the camera motion based on a rotation-only model assuming known camera intrinsic matrix. Finally, we rectify the RS image based on the estimated camera trajectory using inverse mapping. We show rectification results for RS images captured using mobile phone cameras. We also compare our single image method against existing video and nonblind RS rectification methods that typically require multiple images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Rengarajan_From_Bows_to_CVPR_2016_paper.pdf",
        "aff": "Indian Institute of Technology Madras; Indian Institute of Technology Madras; Indian Institute of Technology Madras",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Rengarajan_From_Bows_to_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2167262,
        "gs_citation": 70,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5689621283430920954&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ee.iitm.ac.in;ee.iitm.ac.in;ee.iitm.ac.in",
        "email": "ee.iitm.ac.in;ee.iitm.ac.in;ee.iitm.ac.in",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Rengarajan_From_Bows_to_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Madras",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitm.ac.in",
        "aff_unique_abbr": "IIT Madras",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Madras",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "From Dusk Till Dawn: Modeling in the Dark",
        "session": "3D Reconstruction",
        "status": "Spotlight",
        "track": "main",
        "pid": "21",
        "author_site": "Filip Radenovi\u0107, Johannes L. Sch\u00f6nberger, Dinghuang Ji, Jan-Michael Frahm, Ond\u0159ej Chum, Ji\u0159\u00ed Matas",
        "author": "Filip Radenovic; Johannes L. Schonberger; Dinghuang Ji; Jan-Michael Frahm; Ondrej Chum; Jiri Matas",
        "abstract": "Internet photo collections naturally contain a large variety of illumination conditions, with the largest difference between day and night images. Current modeling techniques do not embrace the broad illumination range often leading to reconstruction failure or severe artifacts. We present an algorithm that leverages the appearance variety to obtain more complete and accurate scene geometry along with consistent multi-illumination appearance information. The proposed method relies on automatic scene appearance grouping, which is used to obtain separate dense 3D models. Subsequent model fusion combines the separate models into a complete and accurate reconstruction of the scene. In addition, we propose a method to derive the appearance information for the model under the different illumination conditions, even for scene parts that are not observed under one illumination condition. To achieve this, we develop a cross-illumination color transfer technique. We evaluate our method on a large variety of landmarks from across Europe reconstructed from a database of 7.4M images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Radenovic_From_Dusk_Till_CVPR_2016_paper.pdf",
        "aff": "CMP, Faculty of Electrical Engineering, Czech Technical University in Prague; Department of Computer Science, The University of North Carolina at Chapel Hill + Department of Computer Science, Eidgen \u00a8ossisch Technische Hochschule Z \u00a8urich; Department of Computer Science, The University of North Carolina at Chapel Hill; Department of Computer Science, The University of North Carolina at Chapel Hill; CMP, Faculty of Electrical Engineering, Czech Technical University in Prague; CMP, Faculty of Electrical Engineering, Czech Technical University in Prague",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Radenovic_From_Dusk_Till_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 3754512,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6793013771753368651&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Radenovic_From_Dusk_Till_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+2;1;1;0;0",
        "aff_unique_norm": "Czech Technical University in Prague;University of North Carolina at Chapel Hill;Eidgenossische Technische Hochschule Zurich",
        "aff_unique_dep": "Faculty of Electrical Engineering;Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.cvut.cz;https://www.unc.edu;https://www.ethz.ch",
        "aff_unique_abbr": "CTU;UNC Chapel Hill;ETH Zurich",
        "aff_campus_unique_index": "0;1+2;1;1;0;0",
        "aff_campus_unique": "Prague;Chapel Hill;Zurich",
        "aff_country_unique_index": "0;1+2;1;1;0;0",
        "aff_country_unique": "Czech Republic;United States;Switzerland"
    },
    {
        "title": "From Keyframes to Key Objects: Video Summarization by Representative Object Proposal Selection",
        "session": "Events, Activities, and Surveillance",
        "status": "Poster",
        "track": "main",
        "pid": "30",
        "author_site": "Jingjing Meng, Hongxing Wang, Junsong Yuan, Yap-Peng Tan",
        "author": "Jingjing Meng; Hongxing Wang; Junsong Yuan; Yap-Peng Tan",
        "abstract": "We propose to summarize a video into a few key objects by selecting representative object proposals generated from video frames. This representative selection problem is formulated as a sparse dictionary selection problem, i.e., choosing a few representatives object proposals to reconstruct the whole proposal pool. Compared with existing sparse dictionary selection based representative selection methods, our new formulation can incorporate object proposal priors and locality prior in the feature space when selecting representatives. Consequently it can better locate key objects and suppress outlier proposals. We convert the optimization problem into a proximal gradient problem and solve it by the fast iterative shrinkage thresholding algorithm (FISTA). Experiments on synthetic data and real benchmark datasets show promising results of our key object summarization apporach in video content mining and search. Comparisons with existing representative selection approaches such as K-mediod, sparse dictionary selection and density based selection validate that our formulation can better capture the key video objects despite appearance variations, cluttered backgrounds and camera motions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Meng_From_Keyframes_to_CVPR_2016_paper.pdf",
        "aff": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore + School of Software Engineering, Chongqing University, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4006354,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14716165643022125954&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ntu.edu.sg;cqu.edu.cn;ntu.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;cqu.edu.cn;ntu.edu.sg;ntu.edu.sg",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Meng_From_Keyframes_to_CVPR_2016_paper.html",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "Nanyang Technological University;Chongqing University",
        "aff_unique_dep": "School of Electrical and Electronic Engineering;School of Software Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg;http://www.cqu.edu.cn/",
        "aff_unique_abbr": "NTU;CQU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Singapore;",
        "aff_country_unique_index": "0;0+1;0;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "title": "From Noise Modeling to Blind Image Denoising",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "45",
        "author_site": "Fengyuan Zhu, Guangyong Chen, Pheng-Ann Heng",
        "author": "Fengyuan Zhu; Guangyong Chen; Pheng-Ann Heng",
        "abstract": "Traditional image denoising algorithms always assume the noise to be homogeneous white Gaussian distributed. However, the noise on real images can be much more complex empirically. This paper addresses this problem and proposes a novel blind image denoising algorithm which can cope with real-world noisy images even when the noise model is not provided. It is realized by modeling image noise with mixture of Gaussian distribution (MoG) which can approximate large varieties of continuous distributions. As the number of components for MoG is unknown practically, this work adopts Bayesian nonparametric technique and proposes a novel Low-rank MoG filter (LR-MoG) to recover clean signals (patches) from noisy ones contaminated by MoG noise. Based on LR-MoG, a novel blind image denoising approach is developed. To test the proposed method, this study conducts extensive experiments on synthesis and real images. Our method achieves the state-of-the-art performance consistently.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_From_Noise_Modeling_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1741822,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15264197396321749677&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_From_Noise_Modeling_CVPR_2016_paper.html"
    },
    {
        "title": "Full Flow: Optical Flow Estimation By Global Optimization Over Regular Grids",
        "session": "Non-Rigid Reconstruction and Motion Analysis",
        "status": "Oral",
        "track": "main",
        "pid": "17",
        "author_site": "Qifeng Chen, Vladlen Koltun",
        "author": "Qifeng Chen; Vladlen Koltun",
        "abstract": "We present a global optimization approach to optical flow estimation. The approach optimizes a classical optical flow objective over the full space of mappings between discrete grids. No descriptor matching is used. The highly regular structure of the space of mappings enables optimizations that reduce the computational complexity of the algorithm's inner loop from quadratic to linear and support efficient matching of tens of thousands of nodes to tens of thousands of displacements. We show that one-shot global optimization of a classical Horn-Schunck-type objective over regular grids at a single resolution is sufficient to initialize continuous interpolation and achieve state-of-the-art performance on challenging modern benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Full_Flow_Optical_CVPR_2016_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 227,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10385493147208433114&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Full_Flow_Optical_CVPR_2016_paper.html"
    },
    {
        "title": "Functional Faces: Groupwise Dense Correspondence Using Functional Maps",
        "session": "Shape Representations and Matching",
        "status": "Poster",
        "track": "main",
        "pid": "52",
        "author_site": "Chao Zhang, William A. P. Smith, Arnaud Dessein, Nick Pears, Hang Dai",
        "author": "Chao Zhang; William A. P. Smith; Arnaud Dessein; Nick Pears; Hang Dai",
        "abstract": "In this paper we present a method for computing dense correspondence between a set of 3D face meshes using functional maps. The functional maps paradigm brings with it a number of advantages for face correspondence. First, it allows us to combine various notions of correspondence. We do so by proposing a number of face-specific functions, suited to either within- or between-subject correspondence. Second, we propose a groupwise variant of the method allowing us to compute cycle-consistent functional maps between all faces in a training set. Since functional maps are of much lower dimension than point-to-point correspondences, this is feasible even when the input meshes are very high resolution. Finally, we show how a functional map provides a geometric constraint that can be used to filter feature matches between non-rigidly deforming surfaces.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Functional_Faces_Groupwise_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, The University of York, UK; Department of Computer Science, The University of York, UK; IMB/LaBRI, Universit\u00e9 de Bordeaux, France; Department of Computer Science, The University of York, UK; Department of Computer Science, The University of York, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1996063,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4245418643257107676&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "york.ac.uk;york.ac.uk;u-bordeaux.fr;york.ac.uk;york.ac.uk",
        "email": "york.ac.uk;york.ac.uk;u-bordeaux.fr;york.ac.uk;york.ac.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Functional_Faces_Groupwise_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of York;Universit\u00e9 de Bordeaux",
        "aff_unique_dep": "Department of Computer Science;IMB/LaBRI",
        "aff_unique_url": "https://www.york.ac.uk;https://www.u-bordeaux.fr",
        "aff_unique_abbr": "York;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United Kingdom;France"
    },
    {
        "title": "G-CNN: An Iterative Grid Based Object Detector",
        "session": "Object Detection 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "12",
        "author_site": "Mahyar Najibi, Mohammad Rastegari, Larry S. Davis",
        "author": "Mahyar Najibi; Mohammad Rastegari; Larry S. Davis",
        "abstract": "We introduce G-CNN, an object detection technique based on CNNs which works without proposal algorithms. G-CNN starts with a multi-scale grid of fixed bounding boxes. We train a regressor to move and scale elements of the grid towards objects iteratively. G-CNN models the problem of object detection as finding a path from a fixed grid to boxes tightly surrounding the objects. G-CNN with around 180 boxes in a multi-scale grid performs comparably to Fast R-CNN which uses around 2K bounding boxes generated with a proposal technique. This strategy makes detection faster by removing the object proposal stage as well as reducing the number of boxes to be processed.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Najibi_G-CNN_An_Iterative_CVPR_2016_paper.pdf",
        "aff": "University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1968151,
        "gs_citation": 267,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15494099097891760167&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.umd.edu;cs.umd.edu;umiacs.umd.edu",
        "email": "cs.umd.edu;cs.umd.edu;umiacs.umd.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Najibi_G-CNN_An_Iterative_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "GIFT: A Real-Time and Scalable 3D Shape Search Engine",
        "session": "Shape Representations and Matching",
        "status": "Poster",
        "track": "main",
        "pid": "51",
        "author_site": "Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, Longin Jan Latecki",
        "author": "Song Bai; Xiang Bai; Zhichao Zhou; Zhaoxiang Zhang; Longin Jan Latecki",
        "abstract": "Projective analysis is an important solution for 3D shape retrieval, since human visual perceptions of 3D shapes rely on various 2D observations from different view points. Although multiple informative and discriminative views are utilized, most projection-based retrieval systems suffer from heavy computational cost, thus cannot satisfy the basic requirement of scalability for search engines.  In this paper, we present a real-time 3D shape search engine based on the projective images of 3D shapes. The real-time property of our search engine results from the following aspects: (1) efficient projection and view feature extraction using GPU acceleration; (2) the first inverted file, referred as F-IF, is utilized to speed up the procedure of multi-view matching; (3) the second inverted file (S-IF), which captures a local distribution of 3D shapes in the feature manifold, is adopted for efficient context-based reranking. As a result, for each query the retrieval task can be finished within one second despite the necessary cost of IO overhead. We name the proposed 3D shape search engine, which combines GPU acceleration and Inverted File (Twice), as GIFT. Besides its high efficiency, GIFT also outperforms the state-of-the-art methods significantly in retrieval accuracy on various shape benchmarks and competitions",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bai_GIFT_A_Real-Time_CVPR_2016_paper.pdf",
        "aff": "Huazhong University of Science and Technology; Huazhong University of Science and Technology; Huazhong University of Science and Technology; CAS Center for Excellence in Brain Science and Intelligence Technology, CASIA; Temple University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Bai_GIFT_A_Real-Time_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1082276,
        "gs_citation": 351,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9700738810804879771&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "hust.edu.cn;hust.edu.cn;hust.edu.cn;ia.ac.cn;temple.edu",
        "email": "hust.edu.cn;hust.edu.cn;hust.edu.cn;ia.ac.cn;temple.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bai_GIFT_A_Real-Time_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Huazhong University of Science and Technology;Chinese Academy of Sciences;Temple University",
        "aff_unique_dep": ";Center for Excellence in Brain Science and Intelligence Technology;",
        "aff_unique_url": "http://www.hust.edu.cn;http://www.cas.cn;https://www.temple.edu",
        "aff_unique_abbr": "HUST;CAS;Temple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "GOGMA: Globally-Optimal Gaussian Mixture Alignment",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "42",
        "author_site": "Dylan Campbell, Lars Petersson",
        "author": "Dylan Campbell; Lars Petersson",
        "abstract": "Gaussian mixture alignment is a family of approaches that are frequently used for robustly solving the point-set registration problem. However, since they use local optimisation, they are susceptible to local minima and can only guarantee local optimality. Consequently, their accuracy is strongly dependent on the quality of the initialisation. This paper presents the first globally-optimal solution to the 3D rigid Gaussian mixture alignment problem under the L2 distance between mixtures. The algorithm, named GOGMA, employs a branch-and-bound approach to search the space of 3D rigid motions SE(3), guaranteeing global optimality regardless of the initialisation. The geometry of SE(3) was used to find novel upper and lower bounds for the objective function and local optimisation was integrated into the scheme to accelerate convergence without voiding the optimality guarantee. The evaluation empirically supported the optimality proof and showed that the method performed much more robustly on two challenging datasets than an existing globally-optimal registration solution.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Campbell_GOGMA_Globally-Optimal_Gaussian_CVPR_2016_paper.pdf",
        "aff": "Australian National University; National ICT Australia (NICTA)",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Campbell_GOGMA_Globally-Optimal_Gaussian_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2320718,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17623309069426060473&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff_domain": "anu.edu.au;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Campbell_GOGMA_Globally-Optimal_Gaussian_CVPR_2016_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Australian National University;National ICT Australia",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.anu.edu.au;https://www.nicta.com.au",
        "aff_unique_abbr": "ANU;NICTA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Gaussian Conditional Random Field Network for Semantic Segmentation",
        "session": "Semantic Parsing and Segmentation",
        "status": "Spotlight",
        "track": "main",
        "pid": "21",
        "author_site": "Raviteja Vemulapalli, Oncel Tuzel, Ming-Yu Liu, Rama Chellapa",
        "author": "Raviteja Vemulapalli; Oncel Tuzel; Ming-Yu Liu; Rama Chellapa",
        "abstract": "In contrast to the existing approaches that use discrete Conditional Random Field (CRF) models, we propose to use a Gaussian CRF model for the task of semantic segmentation. We propose a novel deep network, which we refer to as Gaussian Mean Field (GMF) network, whose layers perform mean field inference over a Gaussian CRF. The proposed GMF network has the desired property that each of its layers produces an output that is closer to the maximum a posteriori solution of the Gaussian CRF compared to its input. By combining the proposed GMF network with deep Convolutional Neural Networks (CNNs), we propose a new end-to-end trainable Gaussian conditional random field network. The proposed Gaussian CRF network is composed of three sub-networks: (i) a CNN-based unary network for generating unary potentials, (ii) a CNN-based pairwise network for generating pairwise potentials, and (iii) a GMF network for performing Gaussian CRF inference. When trained end-to-end in a discriminative fashion, and evaluated on the challenging PASCALVOC 2012 segmentation dataset, the proposed Gaussian CRF network outperforms various recent semantic segmentation approaches that combine CNNs with discrete CRF models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Vemulapalli_Gaussian_Conditional_Random_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Vemulapalli_Gaussian_Conditional_Random_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 750137,
        "gs_citation": 200,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1235607318989426802&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Vemulapalli_Gaussian_Conditional_Random_CVPR_2016_paper.html"
    },
    {
        "title": "Generation and Comprehension of Unambiguous Object Descriptions",
        "session": "Image Captioning and Question Answering",
        "status": "Oral",
        "track": "main",
        "pid": "2",
        "author_site": "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L. Yuille, Kevin Murphy",
        "author": "Junhua Mao; Jonathan Huang; Alexander Toshev; Oana Camburu; Alan L. Yuille; Kevin Murphy",
        "abstract": "We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MS-COCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github.com/mjhucla/Google_Refexp_toolbox.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mao_Generation_and_Comprehension_CVPR_2016_paper.pdf",
        "aff": "Google Inc. + University of California, Los Angeles; Google Inc.; Google Inc.; University of Oxford; University of California, Los Angeles + Johns Hopkins University; Google Inc.",
        "project": "",
        "github": "https://github.com/mjhucla/Google_Refexp_toolbox",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1815679,
        "gs_citation": 1580,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10404264718240229213&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "ucla.edu;stat.ucla.edu;cs.ox.ac.uk;google.com;google.com;google.com",
        "email": "ucla.edu;stat.ucla.edu;cs.ox.ac.uk;google.com;google.com;google.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mao_Generation_and_Comprehension_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;0;2;1+3;0",
        "aff_unique_norm": "Google;University of California, Los Angeles;University of Oxford;Johns Hopkins University",
        "aff_unique_dep": "Google;;;",
        "aff_unique_url": "https://www.google.com;https://www.ucla.edu;https://www.ox.ac.uk;https://www.jhu.edu",
        "aff_unique_abbr": "Google;UCLA;Oxford;JHU",
        "aff_campus_unique_index": "0+1;0;0;1;0",
        "aff_campus_unique": "Mountain View;Los Angeles;",
        "aff_country_unique_index": "0+0;0;0;1;0+0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Geometry-Informed Material Recognition",
        "session": "Recognition Beyond Objects",
        "status": "Spotlight",
        "track": "main",
        "pid": "6",
        "author_site": "Joseph DeGol, Mani Golparvar-Fard, Derek Hoiem",
        "author": "Joseph DeGol; Mani Golparvar-Fard; Derek Hoiem",
        "abstract": "Our goal is to recognize material categories using images and geometry information. In many applications, such as construction management, coarse geometry information is available. We investigate how 3D geometry (surface normals, camera intrinsic and extrinsic parameters) can be used with 2D features (texture and color) to improve material classification. We introduce a new dataset, GeoMat, which is the first to provide both image and geometry data in the form of: (i) training and testing patches that were extracted at different scales and perspectives from real world examples of each material category, and (ii) a large scale construction site scene that includes 160 images and over 800,000 hand labeled 3D points. Our results show that using 2D and 3D features both jointly and independently to model materials improves classification accuracy across multiple scales and viewing directions for both material patches and images of a large scale construction site scene.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/DeGol_Geometry-Informed_Material_Recognition_CVPR_2016_paper.pdf",
        "aff": "University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/DeGol_Geometry-Informed_Material_Recognition_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1151996,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1665090333733680123&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/DeGol_Geometry-Informed_Material_Recognition_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Geospatial Correspondences for Multimodal Registration",
        "session": "Transfer Learning",
        "status": "Poster",
        "track": "main",
        "pid": "58",
        "author_site": "Diego Marcos, Raffay Hamid, Devis Tuia",
        "author": "Diego Marcos; Raffay Hamid; Devis Tuia",
        "abstract": "The growing availability of very high resolution (<1 m/pixel) satellite and aerial images has opened up unprecedented opportunities to monitor and analyze the evolution of land-cover and land-use across the world. To do so, images of the same geographical areas acquired at different times and, potentially, with different sensors must be efficiently parsed to update maps and detect land-cover changes. However, a naive transfer of ground truth labels from one location in the source image to the corresponding location in the target image is not generally feasible, as these images are often only loosely registered (with up to +- 50m of non-uniform errors). Furthermore, land-cover changes in an area over time must be taken into account for an accurate ground truth transfer. To tackle these challenges, we propose a mid-level sensor-invariant representation that encodes image regions in terms of the spatial distribution of their spectral neighbors. We incorporate this representation in a Markov Random Field to simultaneously account for nonlinear mis-registrations and enforce locality priors to find matches between multi-sensor images. We show how our approach can be used to assist in several multimodal land-cover update and change detection problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Marcos_Geospatial_Correspondences_for_CVPR_2016_paper.pdf",
        "aff": "University of Zurich; DigitalGlobe Inc.; University of Zurich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4027281,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17430382208161442971&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "geo.uzh.ch;cc.gatech.edu;geo.uzh.ch",
        "email": "geo.uzh.ch;cc.gatech.edu;geo.uzh.ch",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Marcos_Geospatial_Correspondences_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Zurich;DigitalGlobe",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unizh.ch;https://www.digitalglobe.com",
        "aff_unique_abbr": "UZH;DigitalGlobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "Globally Optimal Manhattan Frame Estimation in Real-Time",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "29",
        "author_site": "Kyungdon Joo, Tae-Hyun Oh, Junsik Kim, In So Kweon",
        "author": "Kyungdon Joo; Tae-Hyun Oh; Junsik Kim; In So Kweon",
        "abstract": "Given a set of surface normals, we pose a Manhattan Frame (MF) estimation problem as a consensus set maximization that maximizes the number of inliers over the rotation search space. We solve this problem through a branch-and-bound framework, which mathematically guarantees a globally optimal solution. However, the computational time of conventional branch-and-bound algorithms are intractable for real-time performance. In this paper, we propose a novel bound computation method within an efficient measurement domain for MF estimation, i.e., the extended Gaussian image (EGI). By relaxing the original problem, we can compute the bounds in real-time, while preserving global optimality. Furthermore, we quantitatively and qualitatively demonstrate the performance of the proposed method for synthetic and real-world data. We also show the versatility of our approach through two applications: extension to multiple MF estimation and video stabilization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Joo_Globally_Optimal_Manhattan_CVPR_2016_paper.pdf",
        "aff": "Robotics and Computer Vision Lab., KAIST, South Korea; Robotics and Computer Vision Lab., KAIST, South Korea; Robotics and Computer Vision Lab., KAIST, South Korea; Robotics and Computer Vision Lab., KAIST, South Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Joo_Globally_Optimal_Manhattan_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2051403,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14608697059522575766&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;gmail.com;gmail.com;kaist.ac.kr",
        "email": "gmail.com;gmail.com;gmail.com;kaist.ac.kr",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Joo_Globally_Optimal_Manhattan_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "KAIST",
        "aff_unique_dep": "Robotics and Computer Vision Lab.",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Globally Optimal Rigid Intensity Based Registration: A Fast Fourier Domain Approach",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "68",
        "author_site": "Behrooz Nasihatkon, Frida Fejne, Fredrik Kahl",
        "author": "Behrooz Nasihatkon; Frida Fejne; Fredrik Kahl",
        "abstract": "High computational cost is the main obstacle to adapting globally optimal branch-and-bound algorithms to intensity-based registration. Existing techniques to speed up such algorithms use a multiresolution pyramid of images and bounds on the target function among different resolutions for rigidly aligning two images. In this paper, we propose a dual algorithm in which the optimization is done in the Fourier domain, and multiple resolution levels are replaced by multiple frequency bands. The algorithm starts by computing the target function in lower frequency bands and keeps adding higher frequency bands until the current subregion is either rejected or divided into smaller areas in a branch and bound manner. Unlike spatial multiresolution approaches, to compute the target function for a wider frequency area, one just needs to compute the target in the residual bands. Therefore, if an area is to be discarded, it performs just enough computations required for the rejection. This property also enables us to use a rather large number of frequency bands compared to the limited number of resolution levels used in the space domain algorithm. Experimental results on real images demonstrate considerable speed gains over the space domain method in most cases.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Nasihatkon_Globally_Optimal_Rigid_CVPR_2016_paper.pdf",
        "aff": "\u2746\u275cstr\u275b\u275dt; \u2746\u275cstr\u275b\u275dt; \u2746\u275cstr\u275b\u275dt",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 928260,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5299882391857365722&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "\u2746\u275cstr\u275b\u275dt; \u2746\u275cstr\u275b\u275dt; \u2746\u275cstr\u275b\u275dt",
        "email": "\u2746\u275cstr\u275b\u275dt; \u2746\u275cstr\u275b\u275dt; \u2746\u275cstr\u275b\u275dt",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Nasihatkon_Globally_Optimal_Rigid_CVPR_2016_paper.html",
        "aff_unique_index": "",
        "aff_unique_norm": "",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Going Deeper into First-Person Activity Recognition",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "43",
        "author_site": "Minghuang Ma, Haoqi Fan, Kris M. Kitani",
        "author": "Minghuang Ma; Haoqi Fan; Kris M. Kitani",
        "abstract": "We bring together ideas from recent work on feature design for egocentric action recognition under one framework by exploring the use of deep convolutional neural networks (CNN). Recent work has shown that features such as hand appearance, object attributes, local hand motion and camera ego-motion are important for characterizing first-person actions. To integrate these ideas under one framework, we propose a twin stream network architecture, where one stream analyzes appearance information and the other stream analyzes motion information. Our appearance stream encodes prior knowledge of the egocentric paradigm by explicitly training the network to segment hands and localize objects. By visualizing certain neuron activation of our network, we show that our proposed architecture naturally learns features that capture object attributes and hand-object configurations. Our extensive experiments on benchmark egocentric action datasets show that our deep architecture enables recognition rates that significantly outperform state-of-the-art techniques - an average 6.6% increase in accuracy over all datasets. Furthermore, by learning to recognize objects, actions and activities jointly, the performance of individual recognition tasks also increase by 30% (actions) and 14% (objects). We also include the results of extensive ablative analysis to highlight the importance of network design decisions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ma_Going_Deeper_into_CVPR_2016_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1623616,
        "gs_citation": 400,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6445621272183411731&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;cs.cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;cs.cmu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ma_Going_Deeper_into_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "GraB: Visual Saliency via Novel Graph Model and Background Priors",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "57",
        "author_site": "Qiaosong Wang, Wen Zheng, Robinson Piramuthu",
        "author": "Qiaosong Wang; Wen Zheng; Robinson Piramuthu",
        "abstract": "We propose an unsupervised bottom-up saliency detection approach by exploiting novel graph structure and background priors. The input image is represented as an undirected graph with superpixels as nodes. Feature vectors are extracted from each node to cover regional color, contrast and texture information. A novel graph model is proposed to effectively capture local and global saliency cues. To obtain more accurate saliency estimations, we optimize the saliency map by using a robust background measure. Comprehensive evaluations on benchmark datasets indicate that our algorithm universally surpasses state-of-the-art unsupervised solutions and performs favorably against supervised approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_GraB_Visual_Saliency_CVPR_2016_paper.pdf",
        "aff": "University of Delaware; eBay Research; eBay Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4453415,
        "gs_citation": 114,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7992616912425811114&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_GraB_Visual_Saliency_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Delaware;eBay",
        "aff_unique_dep": ";eBay Research",
        "aff_unique_url": "https://www.udel.edu;https://research.ebay.com",
        "aff_unique_abbr": "UD;eBay",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Gradient-Domain Image Reconstruction Framework With Intensity-Range and Base-Structure Constraints",
        "session": "Image Enhancement, Restoration, and Texture",
        "status": "Poster",
        "track": "main",
        "pid": "52",
        "author_site": "Takashi Shibata, Masayuki Tanaka, Masatoshi Okutomi",
        "author": "Takashi Shibata; Masayuki Tanaka; Masatoshi Okutomi",
        "abstract": "This paper presents a novel unified gradient-domain image reconstruction framework with intensity-range constraint and base-structure constraint. The existing method for manipulating base structures and detailed textures are classifiable into two major approaches: i) gradient-domain and ii) layer-decomposition. To generate detail-preserving and artifact-free output images, we combine the benefits of the two approaches into the proposed framework by introducing the intensity-range constraint and the base-structure constraint. To preserve details of the input image, the proposed method takes advantage of reconstructing the output image in the gradient domain, while the output intensity is guaranteed to lie within the specified intensity range, e.g. 0-to-255, by the intensity-range constraint. In addition, the reconstructed image lies close to the base structure by the base-structure constraint, which is effective for restraining artifacts. Experimental results show that the proposed framework is effective for various applications such as tone mapping, seamless image cloning, detail enhancement, and image restoration.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shibata_Gradient-Domain_Image_Reconstruction_CVPR_2016_paper.pdf",
        "aff": "Tokyo Institute of Technology+NEC corporation; Tokyo Institute of Technology; Tokyo Institute of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Shibata_Gradient-Domain_Image_Reconstruction_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1415371,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9921768876134398159&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ok.ctrl.titech.ac.jp;ctrl.titech.ac.jp;ctrl.titech.ac.jp",
        "email": "ok.ctrl.titech.ac.jp;ctrl.titech.ac.jp;ctrl.titech.ac.jp",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shibata_Gradient-Domain_Image_Reconstruction_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology;NEC Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.titech.ac.jp;https://www.nec.com",
        "aff_unique_abbr": "Titech;NEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Gradual DropIn of Layers to Train Very Deep Neural Networks",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "23",
        "author_site": "Leslie N. Smith, Emily M. Hand, Timothy Doster",
        "author": "Leslie N. Smith; Emily M. Hand; Timothy Doster",
        "abstract": "We introduce the concept of dynamically growing a neural network during training. In particular, an untrainable deep network starts as a trainable shallow network and newly added layers are slowly, organically added during training, thereby increasing the network's depth. This is accomplished by a new layer, which we call DropIn. The DropIn layer starts by passing the output from a previous layer (effectively skipping over the newly added layers), then increasingly including units from the new layers for both feedforward and backpropagation. We show that deep networks, which are untrainable with conventional methods, will converge with DropIn layers interspersed in the architecture. In addition, we demonstrate that DropIn provides regularization during training in an analogous way as dropout. Experiments are described with the MNIST dataset and various expanded LeNet architectures, CIFAR-10 dataset with its architecture expanded from 3 to 11 layers, and on the ImageNet dataset with the AlexNet architecture expanded to 13 layers and the VGG 16-layer architecture.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Smith_Gradual_DropIn_of_CVPR_2016_paper.pdf",
        "aff": "Naval Research Laboratory; University of Maryland; Naval Research Laboratory",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 700367,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14246894630273856413&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "nrl.navy.mil;cs.umd.edu;nrl.navy.mil",
        "email": "nrl.navy.mil;cs.umd.edu;nrl.navy.mil",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Smith_Gradual_DropIn_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Naval Research Laboratory;University of Maryland",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nrl.navy.mil;https://www/umd.edu",
        "aff_unique_abbr": "NRL;UMD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Gravitational Approach for Point Set Registration",
        "session": "Image Alignment and Registration",
        "status": "Poster",
        "track": "main",
        "pid": "54",
        "author_site": "Vladislav Golyanik, Sk Aziz Ali, Didier Stricker",
        "author": "Vladislav Golyanik; Sk Aziz Ali; Didier Stricker",
        "abstract": "In this paper a new astrodynamics inspired rigid point set registration algorithm is introduced -- the Gravitational Approach (GA). We formulate point set registration as a modified N-body problem with additional constraints and obtain an algorithm with unique properties which is fully scalable with the number of processing cores. In GA, a template point set moves in a viscous medium under gravitational forces induced by a reference point set. Pose updates are completed by numerically solving the differential equations of Newtonian mechanics. We discuss techniques for efficient implementation of the new algorithm and evaluate it on several synthetic and real-world scenarios. GA is compared with the widely used Iterative Closest Point and the state of the art rigid Coherent Point Drift algorithms. Experiments evidence that the new approach is robust against noise and can handle challenging scenarios with structured outliers.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Golyanik_Gravitational_Approach_for_CVPR_2016_paper.pdf",
        "aff": "University of Kaiserslautern, Germany+German Research Center for Arti\ufb01cial Intelligence (DFKI), Germany; University of Kaiserslautern, Germany; University of Kaiserslautern, Germany+German Research Center for Arti\ufb01cial Intelligence (DFKI), Germany",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Golyanik_Gravitational_Approach_for_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 853127,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3964995610964313771&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "dfki.de;rhrk.uni-kl.de;dfki.de",
        "email": "dfki.de;rhrk.uni-kl.de;dfki.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Golyanik_Gravitational_Approach_for_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;0+1",
        "aff_unique_norm": "University of Kaiserslautern;German Research Center for Artificial Intelligence",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-kl.de;https://www.dFKI.de",
        "aff_unique_abbr": "Uni KL;DFKI",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Group MAD Competition - A New Methodology to Compare Objective Image Quality Models",
        "session": "Image Processing and Restoration",
        "status": "Spotlight",
        "track": "main",
        "pid": "18",
        "author_site": "Kede Ma, Qingbo Wu, Zhou Wang, Zhengfang Duanmu, Hongwei Yong, Hongliang Li, Lei Zhang",
        "author": "Kede Ma; Qingbo Wu; Zhou Wang; Zhengfang Duanmu; Hongwei Yong; Hongliang Li; Lei Zhang",
        "abstract": "Objective image quality assessment (IQA) models aim to automatically predict human visual perception of image quality and are of fundamental importance in the field of image processing and computer vision. With an increasing number of IQA models proposed, how to fairly compare their performance becomes a major challenge due to the enormous size of the image space and the limited resource for subjective testing. The standard approach in the literature is to compute several correlation metrics between subjective mean opinion scores (MOSs) and objective model predictions on several well-known subject-rated databases that contain distorted images generated from a few dozens of source images, which provide an extremely limited representation of real-world images. Moreover, most IQA models were developed after these databases became publicly available and often involve machine learning or manual parameter tuning steps to boost their performance on these databases, and thus their generalization capabilities are questionable. Here we propose a substantially different methodology to compare IQA models. We first build a database that contains 4,744 source natural images, together with 94,880 distorted images created from them. We then propose a novel mechanism, namely group MAximum Differentiation (gMAD) competition, that helps automatically select subsets of image pairs from the database that provide the strongest test to let the IQA models compete with each other. Subjective testing on the selected subsets reveals the relative performance of the IQA models and provides useful insights on potential ways to improve them. We report the gMAD competition results between 16 well-known IQA models, but the framework is extendable, allowing future IQA models to be added into the competition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ma_Group_MAD_Competition_CVPR_2016_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 574810,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1663881728357018066&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ma_Group_MAD_Competition_CVPR_2016_paper.html"
    },
    {
        "title": "Groupwise Tracking of Crowded Similar-Appearance Targets From Low-Continuity Image Sequences",
        "session": "Video Analysis 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "21",
        "author_site": "Hongkai Yu, Youjie Zhou, Jeff Simmons, Craig P. Przybyla, Yuewei Lin, Xiaochuan Fan, Yang Mi, Song Wang",
        "author": "Hongkai Yu; Youjie Zhou; Jeff Simmons; Craig P. Przybyla; Yuewei Lin; Xiaochuan Fan; Yang Mi; Song Wang",
        "abstract": "Automatic tracking of large-scale crowded targets are of particular importance in many applications, such as crowded people/vehicle tracking in video surveillance, fiber tracking in materials science, and cell tracking in biomedical imaging. This problem becomes very challenging when the targets show similar appearance and the inter-slice/inter-frame continuity is low due to sparse sampling, camera motion and target occlusion. The main challenge comes from the step of association which aims at matching the predictions and the observations of the multiple targets. In this paper we propose a new groupwise method to explore the target group information and employ the within-group correlations for association and tracking. In particular, the within-group association is modeled by a nonrigid 2D Thin-Plate transform and a sequence of group shrinking, group growing and group merging operations are then developed to refine the composition of each group. We apply the propose method to track large-scale fibers from the microscopy material images and compare its performance against several other multi-target tracking methods. We also apply the proposed method to track crowded people from videos with poor inter-frame continuity.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yu_Groupwise_Tracking_of_CVPR_2016_paper.pdf",
        "aff": "University of South Carolina; University of South Carolina; Air Force Research Lab; Air Force Research Lab; University of South Carolina; University of South Carolina; University of South Carolina; University of South Carolina",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5143838,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13592576343891120169&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cec.sc.edu;cec.sc.edu;afresearchlab.com;afresearchlab.com;cec.sc.edu;cec.sc.edu;cec.sc.edu;cec.sc.edu",
        "email": "cec.sc.edu;cec.sc.edu;afresearchlab.com;afresearchlab.com;cec.sc.edu;cec.sc.edu;cec.sc.edu;cec.sc.edu",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yu_Groupwise_Tracking_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;1;0;0;0;0",
        "aff_unique_norm": "University of South Carolina;Air Force Research Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sc.edu;https://www.afrl.af.mil/",
        "aff_unique_abbr": "USC;AFRL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Guaranteed Outlier Removal With Mixed Integer Linear Programs",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "60",
        "author_site": "Tat-Jun Chin, Yang Heng Kee, Anders Eriksson, Frank Neumann",
        "author": "Tat-Jun Chin; Yang Heng Kee; Anders Eriksson; Frank Neumann",
        "abstract": "The maximum consensus problem is fundamentally important to robust geometric fitting in computer vision. Solving the problem exactly is computationally demanding, and the effort required increases rapidly with the problem size. Although randomized algorithms are much more efficient, the optimality of the solution is not guaranteed. Towards the goal of solving maximum consensus exactly, we present guaranteed outlier removal as a technique to reduce the runtime of exact algorithms. Specifically, before conducting global optimization, we attempt to remove data that are provably true outliers, i.e., those that do not exist in the maximum consensus set. We propose an algorithm based on mixed integer linear programming to perform the removal. The result of our algorithm is a smaller data instance that admits much faster solution by a subsequent exact algorithm, while yielding the same globally optimal result as the original problem. We demonstrate that overall speedups of up to 80% can be achieved on common vision problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chin_Guaranteed_Outlier_Removal_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Chin_Guaranteed_Outlier_Removal_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 4109984,
        "gs_citation": 48,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14076458865788241689&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chin_Guaranteed_Outlier_Removal_CVPR_2016_paper.html"
    },
    {
        "title": "HD Maps: Fine-Grained Road Segmentation by Parsing Ground and Aerial Images",
        "session": "Semantic Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "63",
        "author_site": "Gell\u00e9rt M\u00e1ttyus, Shenlong Wang, Sanja Fidler, Raquel Urtasun",
        "author": "Gellert Mattyus; Shenlong Wang; Sanja Fidler; Raquel Urtasun",
        "abstract": "In this paper we present an approach to enhance existing maps with fine grained segmentation categories such as parking spots and sidewalk, as well as the number and location of road lanes. Towards this goal, we propose an efficient approach that is able to estimate these fine grained categories by doing joint inference over both, monocular aerial imagery, as well as ground images taken from a stereo camera pair mounted on top of a car. Important to this is reasoning about the alignment between the two types of imagery, as even when the measurements are taken with sophisticated GPS+IMU systems, this alignment is not sufficiently accurate. We demonstrate the effectiveness of our approach on a new dataset which enhances KITTI [8] with aerial images taken with a camera mounted on an airplane and flying around the city of Karlsruhe, Germany.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mattyus_HD_Maps_Fine-Grained_CVPR_2016_paper.pdf",
        "aff": "Remote Sensing Technology Institute, German Aerospace Center; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3424448,
        "gs_citation": 181,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6799210352967181354&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 8,
        "aff_domain": "dlr.de;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "dlr.de;cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mattyus_HD_Maps_Fine-Grained_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "German Aerospace Center;University of Toronto",
        "aff_unique_dep": "Remote Sensing Technology Institute;Department of Computer Science",
        "aff_unique_url": "https://www.dlr.de;https://www.utoronto.ca",
        "aff_unique_abbr": "DLR;U of T",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Toronto",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Germany;Canada"
    },
    {
        "title": "Harnessing Object and Scene Semantics for Large-Scale Video Understanding",
        "session": "Activity Recognition",
        "status": "Spotlight",
        "track": "main",
        "pid": "9",
        "author_site": "Zuxuan Wu, Yanwei Fu, Yu-Gang Jiang, Leonid Sigal",
        "author": "Zuxuan Wu; Yanwei Fu; Yu-Gang Jiang; Leonid Sigal",
        "abstract": "Large-scale action recognition and video categorization are important problems in computer vision. To address these problems, we propose a novel object- and scene-based semantic fusion network and representation. Our semantic fusion network combines three streams of information using a three-layer neural network: (i) frame-based low-level CNN features, (ii) object features from a state-of-the-art large-scale CNN object-detector trained to recognize 20K classes, and (iii) scene features from a state-of-the-art CNN scene-detector trained to recognize 205 scenes. The trained network achieves improvements in supervised activity and video categorization in two complex large-scale datasets -  ActivityNet and FCVID, respectively. Further, by examining and back propagating information through the fusion network, semantic relationships (correlations) between video classes and objects/scenes can be discovered. These video class-object/video class-scene relationships can in turn be used as semantic representation for the video classes themselves. We illustrate effectiveness of this semantic representation through experiments on zero-shot action/video classification and clustering.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wu_Harnessing_Object_and_CVPR_2016_paper.pdf",
        "aff": "Shanghai Key Lab of Intel. Info. Processing, School of Computer Science, Fudan University; Disney Research; Shanghai Key Lab of Intel. Info. Processing, School of Computer Science, Fudan University; Disney Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wu_Harnessing_Object_and_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 907443,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13352471531011674426&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "fudan.edu.cn;fudan.edu.cn;disneyresearch.com;disneyresearch.com",
        "email": "fudan.edu.cn;fudan.edu.cn;disneyresearch.com;disneyresearch.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wu_Harnessing_Object_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Fudan University;Disney Research",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.fudan.edu.cn;https://research.disney.com",
        "aff_unique_abbr": "Fudan;Disney Research",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Hedged Deep Tracking",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "55",
        "author_site": "Yuankai Qi, Shengping Zhang, Lei Qin, Hongxun Yao, Qingming Huang, Jongwoo Lim, Ming-Hsuan Yang",
        "author": "Yuankai Qi; Shengping Zhang; Lei Qin; Hongxun Yao; Qingming Huang; Jongwoo Lim; Ming-Hsuan Yang",
        "abstract": "In recent years, several methods have been developed to utilize hierarchical features learned from a deep convolutional neural network (CNN) for visual tracking. However, as the features from a certain CNN layer characterize an object of interest from only one aspect or one level, the performance of such trackers trained with features from one layer (usually the last second layer) can be further improved. In this paper, we propose a novel CNN based tracking framework, which takes full advantage of features from different CNN layers and uses an adaptive Hedge method to hedge several CNN trackers into a stronger one. Extensive experiments on a benchmark dataset of 100 challenging image sequences demonstrate the effectiveness of the proposed algorithm compared with several state-of-the-art trackers.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Qi_Hedged_Deep_Tracking_CVPR_2016_paper.pdf",
        "aff": "Harbin Institute of Technology; Harbin Institute of Technology; Institute of Computing Technology, Chinese Academy of Sciences; Harbin Institute of Technology; University of Chinese Academy of Sciences+Harbin Institute of Technology; Hanyang University; University of California at Merced",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Qi_Hedged_Deep_Tracking_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1718581,
        "gs_citation": 890,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13751537684444264388&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "gmail.com;hit.edu.cn;ict.ac.cn;hit.edu.cn;jdl.ac.cn;hanyang.ac.kr;ucmerced.edu",
        "email": "gmail.com;hit.edu.cn;ict.ac.cn;hit.edu.cn;jdl.ac.cn;hanyang.ac.kr;ucmerced.edu",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Qi_Hedged_Deep_Tracking_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0;2+0;3;4",
        "aff_unique_norm": "Harbin Institute of Technology;Chinese Academy of Sciences;University of Chinese Academy of Sciences;Hanyang University;University of California, Merced",
        "aff_unique_dep": ";Institute of Computing Technology;;;",
        "aff_unique_url": "http://www.hit.edu.cn/;http://www.ict.ac.cn;http://www.ucas.ac.cn;https://www.hanyang.ac.kr;https://www.ucmerced.edu",
        "aff_unique_abbr": "HIT;CAS;UCAS;HYU;UC Merced",
        "aff_campus_unique_index": "0;0;0;0;2",
        "aff_campus_unique": "Harbin;;Merced",
        "aff_country_unique_index": "0;0;0;0;0+0;1;2",
        "aff_country_unique": "China;South Korea;United States"
    },
    {
        "title": "Hedgehog Shape Priors for Multi-Object Segmentation",
        "session": "Computational Photography and Biomedical Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "19",
        "author_site": "Hossam Isack, Olga Veksler, Milan Sonka, Yuri Boykov",
        "author": "Hossam Isack; Olga Veksler; Milan Sonka; Yuri Boykov",
        "abstract": "Star-convexity prior is popular for interactive single object segmentation due to its simplicity and amenability to binary graph cut optimization. We propose a more general multi-object segmentation approach. Moreover, each object can be constrained by a more descriptive shape prior, \"hedgehog\". Each hedgehog shape has its surface normals locally constrained by an arbitrary given vector field, e.g. gradient of the user-scribble distance transform. In contrast to star-convexity, the tightness of our normal constraint can be changed giving better control over allowed shapes. For example, looser constraints, i.e. wider cones of allowed normals, give more relaxed hedgehog shapes. On the other hand, the tightest constraint enforces skeleton consistency with the scribbles. In general, hedgehog shapes are more descriptive than a star, which is only a special case corresponding to a radial vector field and weakest tightness. Our approach has significantly more applications than standard single star-convex segmentation, e.g. in medical data we can separate multiple non-star organs with similar appearances and weak edges. Optimization is done by our modified a-expansion moves shown to be submodular for multi-hedgehog shapes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Isack_Hedgehog_Shape_Priors_CVPR_2016_paper.pdf",
        "aff": "Computer Science, University of Western Ontario, Canada; Computer Science, University of Western Ontario, Canada; Electrical and Computer Engineering, University of Iowa, USA; Computer Science, University of Western Ontario, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4082338,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16672067445417269942&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "csd.uwo.ca;csd.uwo.ca;uiowa.edu;csd.uwo.ca",
        "email": "csd.uwo.ca;csd.uwo.ca;uiowa.edu;csd.uwo.ca",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Isack_Hedgehog_Shape_Priors_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Western Ontario;University of Iowa",
        "aff_unique_dep": "Computer Science;Electrical and Computer Engineering",
        "aff_unique_url": "https://www.uwo.ca;https://www.uiowa.edu",
        "aff_unique_abbr": "UWO;UIowa",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Heterogeneous Light Fields",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "27",
        "author_site": "Maximilian Diebold, Bernd J\u00e4hne, Alexander Gatto",
        "author": "Maximilian Diebold; Bernd Jahne; Alexander Gatto",
        "abstract": "In contrast to traditional binocular or multi-view stereo approaches, the adequately sampled space of observations in light-field imaging allows, to obtain dense and high quality depth maps. It also extends capabilities beyond those of traditional methods. Previously, constant intensity has been assumed for estimating disparity of orientation in most approaches to analyze epipolar plane images (EPIs). Here, we introduce a modified structure tensor approach which improves depth estimation. This extension also includes a model of non-constant intensity on EPI manifolds. We derive an approach to estimate high quality depth maps in luminance-gradient light fields, as well as in color-filtered light fields. Color-filtered light fields pose particular challenges due to the fact that structures can change significantly in appearance with wavelength and can completely vanish at some wavelength. We demonstrate solutions to this challenge and obtain a dense sRGB image reconstruction in addition to dense depth maps.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Diebold_Heterogeneous_Light_Fields_CVPR_2016_paper.pdf",
        "aff": "Heidelberg Collaboratory for Image Processing; Heidelberg Collaboratory for Image Processing; Sony Deutschland GmbH",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 48652858,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14727155960558881013&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "iwr.uni-heidelberg.de; ; ",
        "email": "iwr.uni-heidelberg.de; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Diebold_Heterogeneous_Light_Fields_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Heidelberg University;Sony Deutschland GmbH",
        "aff_unique_dep": "Heidelberg Collaboratory for Image Processing;",
        "aff_unique_url": "https://www.kip.uni-heidelberg.de;https://www.sony.de",
        "aff_unique_abbr": "KIP;Sony D",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Heidelberg;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Hierarchical Gaussian Descriptor for Person Re-Identification",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "64",
        "author_site": "Tetsu Matsukawa, Takahiro Okabe, Einoshin Suzuki, Yoichi Sato",
        "author": "Tetsu Matsukawa; Takahiro Okabe; Einoshin Suzuki; Yoichi Sato",
        "abstract": "Describing the color and textural information of a person image is one of the most crucial aspects of person re-identification. In this paper, we present a novel descriptor based on a hierarchical distribution of pixel features. A hierarchical covariance descriptor has been successfully applied for image classification. However, the mean information of pixel features, which is absent in covariance, tends to be major discriminative information of person images. To solve this problem, we describe a local region in an image via hierarchical Gaussian distribution in which both means and covariances are included in their parameters. More specifically, we model the region as a set of multiple Gaussian distributions in which each Gaussian represents the appearance of a local patch. The characteristics of the set of Gaussians are again described by another Gaussian distribution. In both steps, unlike the hierarchical covariance descriptor, the proposed descriptor can model both the mean and the covariance information of pixel features properly. The results of experiments conducted on five databases indicate that the proposed descriptor exhibits remarkably high performance which outperforms the state-of-the-art descriptors for person re-identification.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Matsukawa_Hierarchical_Gaussian_Descriptor_CVPR_2016_paper.pdf",
        "aff": "Kyushu University; Kyushu Institute of Technology; Kyushu University; The University of Tokyo",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Matsukawa_Hierarchical_Gaussian_Descriptor_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 827406,
        "gs_citation": 744,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16555846182825622927&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": "kyushu-u.ac.jp;ai.kyutech.ac.jp;kyushu-u.ac.jp;iis.u-tokyo.ac.jp",
        "email": "kyushu-u.ac.jp;ai.kyutech.ac.jp;kyushu-u.ac.jp;iis.u-tokyo.ac.jp",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Matsukawa_Hierarchical_Gaussian_Descriptor_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Kyushu University;Kyushu Institute of Technology;University of Tokyo",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.kyushu-u.ac.jp;https://www.kyutech.ac.jp;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "Kyushu U;Kyutech;UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Hierarchical Recurrent Neural Encoder for Video Representation With Application to Captioning",
        "session": "Events, Activities, and Surveillance",
        "status": "Poster",
        "track": "main",
        "pid": "29",
        "author_site": "Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, Yueting Zhuang",
        "author": "Pingbo Pan; Zhongwen Xu; Yi Yang; Fei Wu; Yueting Zhuang",
        "abstract": "Recently, deep learning approach, especially deep Convolutional Neural Networks (ConvNets), have achieved overwhelming accuracy with fast processing speed for image classification. Incorporating temporal structure with deep ConvNets for video representation becomes a fundamental problem for video content analysis.  In this paper, we propose a new approach, namely Hierarchical Recurrent Neural Encoder (HRNE), to exploit temporal information of videos. Compared to recent video representation inference approaches, this paper makes the following three contributions.  First, our HRNE is able to efficiently exploit video temporal structure in a longer range by reducing the length of input information flow, and compositing multiple consecutive inputs at a higher level. Second, computation operations are significantly lessened while attaining more non-linearity. Third, HRNE is able to uncover temporal transitions between frame chunks with different granularities, i.e. it can model the temporal transitions between frames as well as the transitions between segments. We apply the new method to video captioning where temporal information plays a crucial role. Experiments demonstrate that our method outperforms the state-of-the-art on video captioning benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Hierarchical_Recurrent_Neural_CVPR_2016_paper.pdf",
        "aff": "Zhejiang University; University of Technology Sydney; University of Technology Sydney; Zhejiang University; Zhejiang University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Pan_Hierarchical_Recurrent_Neural_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 798375,
        "gs_citation": 506,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=976441969976606731&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "gmail.com;gmail.com;uts.edu.au;cs.zju.edu.cn;cs.zju.edu.cn",
        "email": "gmail.com;gmail.com;uts.edu.au;cs.zju.edu.cn;cs.zju.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Pan_Hierarchical_Recurrent_Neural_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;0;0",
        "aff_unique_norm": "Zhejiang University;University of Technology Sydney",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.uts.edu.au",
        "aff_unique_abbr": "ZJU;UTS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Hierarchically Gated Deep Networks for Semantic Segmentation",
        "session": "Recognition and Labeling",
        "status": "Oral",
        "track": "main",
        "pid": "1",
        "author": "Guo-Jun Qi",
        "abstract": "Semantic segmentation aims to parse the scene structure of images by annotating the labels to each pixel so that images can be segmented into different regions.   While image structures usually have various scales, it is difficult to use a single scale to model the spatial contexts for all individual pixels.  Multi-scale Convolutional Neural Networks (CNNs) and their variants have made striking success for modeling the global scene structure for an image. However, they are limited in labeling fine-grained local structures like pixels and patches, since spatial contexts might be  blindly mixed up without appropriately customizing their scales.  To address this challenge, we develop a novel paradigm of multi-scale deep network to model spatial contexts surrounding different pixels at various scales.  It builds multiple layers of memory cells, learning feature representations for individual pixels at their customized scales by hierarchically absorbing relevant spatial contexts via memory gates between layers.Such Hierarchically Gated Deep Networks (HGDNs) can customize a suitable scale for each pixel, thereby delivering better performance on labeling scene structures of various scales.  We conduct the experiments on two datasets, and show competitive results compared with the other multi-scale deep networks on the semantic segmentation task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Qi_Hierarchically_Gated_Deep_CVPR_2016_paper.pdf",
        "aff": "",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 823330,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2024847456999548042&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "",
        "email": "",
        "author_num": 1,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Qi_Hierarchically_Gated_Deep_CVPR_2016_paper.html"
    },
    {
        "title": "High-Quality Depth From Uncalibrated Small Motion Clip",
        "session": "3D Shape Reconstruction",
        "status": "Oral",
        "track": "main",
        "pid": "13",
        "author_site": "Hyowon Ha, Sunghoon Im, Jaesik Park, Hae-Gon Jeon, In So Kweon",
        "author": "Hyowon Ha; Sunghoon Im; Jaesik Park; Hae-Gon Jeon; In So Kweon",
        "abstract": "We propose a novel approach that generates a high-quality depth map from a set of images captured with a small viewpoint variation, namely small motion clip. As opposed to prior methods that recover scene geometry and camera motions using pre-calibrated cameras, we introduce a self-calibrating bundle adjustment tailored for small motion. This allows our dense stereo algorithm to produce a high-quality depth map for the user without the need for camera calibration. In the dense matching, the distributions of intensity profiles are analyzed to leverage the benefit of having negligible intensity changes within the scene due to the minuscule variation in viewpoint. The depth maps obtained by the proposed framework show accurate and extremely fine structures that are unmatched by previous literature under the same small motion configuration.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ha_High-Quality_Depth_From_CVPR_2016_paper.pdf",
        "aff": "Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology; Intel Labs; Korea Advanced Institute of Science and Technology; Korea Advanced Institute of Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6393642,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5993251170108609890&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ha_High-Quality_Depth_From_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Intel",
        "aff_unique_dep": ";Intel Labs",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.intel.com",
        "aff_unique_abbr": "KAIST;Intel",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "title": "Highlight Detection With Pairwise Deep Ranking for First-Person Video Summarization",
        "session": "Video Analysis 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "24",
        "author_site": "Ting Yao, Tao Mei, Yong Rui",
        "author": "Ting Yao; Tao Mei; Yong Rui",
        "abstract": "The emergence of wearable devices such as portable cameras and smart glasses makes it possible to record life logging first-person videos. Browsing such long unstructured videos is time-consuming and tedious. This paper studies the discovery of moments of user's major or special interest (i.e., highlights) in a video, for generating the summarization of first-person videos. Specifically, we propose a novel pairwise deep ranking model that employs deep learning techniques to learn the relationship between highlight and non-highlight video segments. A two-stream network structure by representing video segments from complementary information on appearance of video frames and temporal dynamics across frames is developed for video highlight detection. Given a long personal video, equipped with the highlight detection model, a highlight score is assigned to each segment. The obtained highlight segments are applied for summarization in two ways: video timelapse and video skimming. The former plays the highlight (non-highlight) segments at low (high) speed rates, while the latter assembles the sequence of segments with the highest scores. On 100 hours of first-person videos for 15 unique sports categories, our highlight detection achieves the improvement over the state-of-the-art RankSVM method by 10.5% in terms of accuracy. Moreover, our approaches produce video summary with better quality by a user study from 35 human subjects.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yao_Highlight_Detection_With_CVPR_2016_paper.pdf",
        "aff": "Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2725141,
        "gs_citation": 345,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=307156259137472303&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yao_Highlight_Detection_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-asia",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Highway Vehicle Counting in Compressed Domain",
        "session": "Video Surveilance",
        "status": "Poster",
        "track": "main",
        "pid": "81",
        "author_site": "Xu Liu, Zilei Wang, Jiashi Feng, Hongsheng Xi",
        "author": "Xu Liu; Zilei Wang; Jiashi Feng; Hongsheng Xi",
        "abstract": "This paper presents a highway vehicle counting method in compressed domain, aiming at achieving acceptable estimation performance approaching the pixel-domain methods. Such a task essentially is challenging because the available information (e.g. motion vector) to describe vehicles in videos is quite limited and inaccurate, and the vehicle count in realistic traffic scenes always varies greatly. To tackle this issue, we first develop a batch of low-level features, which can be extracted from the encoding metadata of videos, to mitigate the informational insufficiency of compressed videos. Then we propose a Hierarchical Classification based Regression (HCR) model to estimate the vehicle count from features. HCR hierarchically divides the traffic scenes into different cases according to vehicle density, such that the broad-variation characteristics of traffic scenes can be better approximated. Finally, we evaluated the proposed method on the real highway surveillance videos. The results show that our method is very competitive to the pixel-domain methods, which can reach similar performance along with its lower complexity.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Highway_Vehicle_Counting_CVPR_2016_paper.pdf",
        "aff": "Department of Automation, University of Science and Technology of China; Department of Automation, University of Science and Technology of China + Department of ECE, National University of Singapore; Department of ECE, National University of Singapore; Department of Automation, University of Science and Technology of China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 677043,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2402905628962495825&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "mail.ustc.edu.cn;ustc.edu.cn;ustc.edu.cn;nus.edu.sg",
        "email": "mail.ustc.edu.cn;ustc.edu.cn;ustc.edu.cn;nus.edu.sg",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liu_Highway_Vehicle_Counting_CVPR_2016_paper.html",
        "aff_unique_index": "0;0+1;1;0",
        "aff_unique_norm": "University of Science and Technology of China;National University of Singapore",
        "aff_unique_dep": "Department of Automation;Department of Electrical and Computer Engineering",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.nus.edu.sg",
        "aff_unique_abbr": "USTC;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;1;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "title": "Homography Estimation From the Common Self-Polar Triangle of Separate Ellipses",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "26",
        "author_site": "Haifei Huang, Hui Zhang, Yiu-ming Cheung",
        "author": "Haifei Huang; Hui Zhang; Yiu-ming Cheung",
        "abstract": "How to avoid ambiguity is a challenging problem for conic-based homography estimation. In this paper, we address the problem of homography estimation from two separate ellipses. We find that any two ellipses have a unique common self-polar triangle, which can provide three line correspondences. Furthermore, by investigating the location features of the common self-polar triangle, we show that one vertex of the triangle lies outside of both ellipses, while the other two vertices lies inside the ellipses separately. Accordingly, one more line correspondence can be obtained from the intersections of the conics and the common self-polar triangle. Therefore, four line correspondences can be obtained based on the common self-polar triangle, which can provide enough constraints for the homography estimation. The main contributions in this paper include: (1) A new discovery on the location features of the common self-polar triangle of separate ellipses. (2) A novel approach for homography estimation.  Simulate experiments and real experiments are conducted to demonstrate the feasibility and accuracy of our approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Homography_Estimation_From_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, Hong Kong Baptist University + United International College, BNU-HKBU; United International College, BNU-HKBU; Department of Computer Science, Hong Kong Baptist University + United International College, BNU-HKBU",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1950464,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4705683143686468926&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 7,
        "aff_domain": "uic.edu.hk;uic.edu.hk;comp.hkbu.edu.hk",
        "email": "uic.edu.hk;uic.edu.hk;comp.hkbu.edu.hk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Huang_Homography_Estimation_From_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;0+1",
        "aff_unique_norm": "Hong Kong Baptist University;United International College",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.hkbu.edu.hk;https://www.uic.edu.hk",
        "aff_unique_abbr": "HKBU;UIC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "How Far Are We From Solving Pedestrian Detection?",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "53",
        "author_site": "Shanshan Zhang, Rodrigo Benenson, Mohamed Omran, Jan Hosang, Bernt Schiele",
        "author": "Shanshan Zhang; Rodrigo Benenson; Mohamed Omran; Jan Hosang; Bernt Schiele",
        "abstract": "Encouraged by the recent progress in pedestrian detection, we investigate the gap between current state-of-the-art methods and the \"perfect single frame detector\". We enable our analysis by creating a human baseline for pedestrian detection (over the Caltech dataset), and by manually clustering the recurrent errors of a top detector. Our results characterise both localisation and background-versus-foreground errors.  To address localisation errors we study the impact of training annotation noise on the detector performance, and show that we can improve even with a small portion of sanitised training data. To address background/foreground discrimination, we study convnets for pedestrian detection, and discuss which factors affect their performance.  Other than our in-depth analysis, we report top performance on the Caltech dataset, and provide a new sanitised set of training and test annotations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_How_Far_Are_CVPR_2016_paper.pdf",
        "aff": "Max Planck Institute for Informatics; Max Planck Institute for Informatics; Max Planck Institute for Informatics; Max Planck Institute for Informatics; Max Planck Institute for Informatics",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhang_How_Far_Are_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2712205,
        "gs_citation": 597,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8448032675851172312&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "email": "mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de;mpi-inf.mpg.de",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_How_Far_Are_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Max Planck Institute for Informatics",
        "aff_unique_dep": "",
        "aff_unique_url": "https://mpi-inf.mpg.de",
        "aff_unique_abbr": "MPII",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "How Hard Can It Be? Estimating the Difficulty of Visual Search in an Image",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "71",
        "author_site": "Radu Tudor Ionescu, Bogdan Alexe, Marius Leordeanu, Marius Popescu, Dim P. Papadopoulos, Vittorio Ferrari",
        "author": "Radu Tudor Ionescu; Bogdan Alexe; Marius Leordeanu; Marius Popescu; Dim P. Papadopoulos; Vittorio Ferrari",
        "abstract": "We address the problem of estimating image difficulty defined as the human response time for solving a visual search task. We collect human annotations of image difficulty for the PASCAL VOC 2012 data set through a crowd-sourcing platform. We then analyze what human interpretable image properties can have an impact on visual search difficulty, and how accurate are those properties for predicting difficulty. Next, we build a regression model based on deep features learned with state of the art convolutional neural networks and show better results for predicting the ground-truth visual search difficulty scores produced by human annotators. Our model is able to correctly rank about 75% image pairs according to their difficulty score. We also show that our difficulty predictor generalizes well to new classes not seen during training. Finally, we demonstrate that our predicted difficulty scores are useful for weakly supervised object localization (8% improvement) and semi-supervised object classification (1% improvement).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ionescu_How_Hard_Can_CVPR_2016_paper.pdf",
        "aff": "University of Bucharest; University of Bucharest+Institute of Mathematical Statistics and Applied Mathematics of the Romanian Academy; Institute of Mathematics of the Romanian Academy; University of Bucharest; University of Edinburgh; University of Edinburgh",
        "project": "http://image-difficulty.herokuapp.com",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1584993,
        "gs_citation": 164,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17993022420884973505&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ionescu_How_Hard_Can_CVPR_2016_paper.html",
        "aff_unique_index": "0;0+1;1;0;2;2",
        "aff_unique_norm": "University of Bucharest;Romanian Academy;University of Edinburgh",
        "aff_unique_dep": ";Institute of Mathematical Statistics and Applied Mathematics;",
        "aff_unique_url": "https://www.unibuc.ro;https://www.racai.ro;https://www.ed.ac.uk",
        "aff_unique_abbr": "Unibuc;;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;1;1",
        "aff_country_unique": "Romania;United Kingdom"
    },
    {
        "title": "Human Pose Estimation With Iterative Error Feedback",
        "session": "Human Pose Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "20",
        "author_site": "Jo\u00e3o Carreira, Pulkit Agrawal, Katerina Fragkiadaki, Jitendra Malik",
        "author": "Joao Carreira; Pulkit Agrawal; Katerina Fragkiadaki; Jitendra Malik",
        "abstract": "Hierarchical feature extractors such as Convolutional Networks (ConvNets) have achieved impressive performance on a variety of classification tasks using purely feedforward processing. Feedforward architectures can learn rich representations of the input space but do not explicitly model dependencies in the output spaces, that are quite structured for tasks such as articulated human pose estimation or object segmentation. Here we propose a framework that expands the expressive power of hierarchical feature extractors to encompass both input and output spaces, by introducing top-down feedback. Instead of directly predicting the outputs in one go, we use a self-correcting model that progressively changes an initial solution by feeding back error predictions, in a process we call Iterative Error Feedback (IEF). IEF shows excellent performance on the task of articulated pose estimation in the challenging MPII and LSP benchmarks, matching the state-of-the-art without requiring ground truth scale annotation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Carreira_Human_Pose_Estimation_CVPR_2016_paper.pdf",
        "aff": "UC Berkeley + Google DeepMind; UC Berkeley; UC Berkeley + Google; UC Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5665207,
        "gs_citation": 1074,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8678128637314457598&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Carreira_Human_Pose_Estimation_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;0+1;0",
        "aff_unique_norm": "University of California, Berkeley;Google",
        "aff_unique_dep": ";Google DeepMind",
        "aff_unique_url": "https://www.berkeley.edu;https://deepmind.com",
        "aff_unique_abbr": "UC Berkeley;DeepMind",
        "aff_campus_unique_index": "0;0;0+2;0",
        "aff_campus_unique": "Berkeley;;Mountain View",
        "aff_country_unique_index": "0+1;0;0+0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "HyperDepth: Learning Depth From Structured Light Without Matching",
        "session": "3D Shape Reconstruction",
        "status": "Oral",
        "track": "main",
        "pid": "16",
        "author_site": "Sean Ryan Fanello, Christoph Rhemann, Vladimir Tankovich, Adarsh Kowdle , Sergio Orts Escolano, David Kim, Shahram Izadi",
        "author": "Sean Ryan Fanello; Christoph Rhemann; Vladimir Tankovich; Adarsh Kowdle; Sergio Orts Escolano; David Kim; Shahram Izadi",
        "abstract": "Structured light sensors are popular due to their robustness to untextured scenes and multipath. These systems triangulate depth by solving a correspondence problem between each camera and projector pixel. This is often framed as a local stereo matching task, correlating patches of pixels in the observed and reference image. However, this is computationally intensive, leading to reduced depth accuracy and framerate. We contribute an algorithm for solving this correspondence problem efficiently, without compromising depth accuracy. For the first time, this problem is cast as a classification-regression task, which we solve extremely efficiently using an ensemble of cascaded random forests. Our algorithm scales in number of disparities, and each pixel can be processed independently, and in parallel. No matching or even access to the corresponding reference pattern is required at runtime, and regressed labels are directly mapped to depth. Our GPU-based algorithm runs at a 1KHz for 1.3MP input/output images, with disparity error of 0.1 subpixels. We show a prototype high framerate depth camera running at 375Hz, useful for solving tracking-related problems. We demonstrate our algorithmic performance, creating high resolution real-time depth maps that surpass the quality of current state of the art depth technologies, highlighting quantization-free results with reduced holes, edge fattening and other stereo-based depth artifacts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Fanello_HyperDepth_Learning_Depth_CVPR_2016_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Fanello_HyperDepth_Learning_Depth_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1832702,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16037842177178030072&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Fanello_HyperDepth_Learning_Depth_CVPR_2016_paper.html"
    },
    {
        "title": "HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection",
        "session": "Object Detection 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "10",
        "author_site": "Tao Kong, Anbang Yao, Yurong Chen, Fuchun Sun",
        "author": "Tao Kong; Anbang Yao; Yurong Chen; Fuchun Sun",
        "abstract": "Almost all of the current top-performing object detection networks employ region proposals to guide the search for object instances. State-of-the-art region proposal methods usually need several thousand proposals to get high recall, thus hurting the detection efficiency. Although the latest Region Proposal Network method gets promising detection accuracy with several hundred proposals, it still struggles in small-size object detection and precise localization (e.g., large IoU thresholds), mainly due to the coarseness of its feature maps. In this paper, we present a deep hierarchical network, namely HyperNet, for handling region proposal generation and object detection jointly. Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space. The Hyper Features well incorporate deep but highly semantic, intermediate but really complementary, and shallow but naturally high-resolution features of the image, thus enabling us to construct HyperNet by sharing them both in generating proposals and detecting objects via an end-to-end joint training strategy. For the deep VGG16 model, our method achieves completely leading recall and state-of-the-art object detection accuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It runs with a speed of 5 fps (including all steps) on a GPU, thus having the potential for real-time processing.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kong_HyperNet_Towards_Accurate_CVPR_2016_paper.pdf",
        "aff": "State Key Lab. of Intelligent Technology and Systems+Tsinghua National Laboratory for Information Science and Technology (TNList)+Department of Computer Science and Technology, Tsinghua University; Intel Labs China; Intel Labs China; State Key Lab. of Intelligent Technology and Systems+Tsinghua National Laboratory for Information Science and Technology (TNList)+Department of Computer Science and Technology, Tsinghua University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 768999,
        "gs_citation": 1150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=417595952743722908&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "mails.tsinghua.edu.cn;intel.com;intel.com;mail.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;intel.com;intel.com;mail.tsinghua.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kong_HyperNet_Towards_Accurate_CVPR_2016_paper.html",
        "aff_unique_index": "0+1+1;2;2;0+1+1",
        "aff_unique_norm": "State Key Laboratory of Intelligent Technology and Systems;Tsinghua University;Intel",
        "aff_unique_dep": "State Key Lab;National Laboratory for Information Science and Technology;Intel Labs",
        "aff_unique_url": ";http://www.tnlist.org/;https://www.intel.cn",
        "aff_unique_abbr": ";TNList;Intel",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Identifying Good Training Data for Self-Supervised Free Space Estimation",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "54",
        "author_site": "Ali Harakeh, Daniel Asmar, Elie Shammas",
        "author": "Ali Harakeh; Daniel Asmar; Elie Shammas",
        "abstract": "This paper proposes a novel technique to extract training data from free space in a scene using a stereo camera. The proposed technique exploits the projection of planes in the v-disparity image paired with Bayesian linear regression to reliably identify training image pixels belonging to free space in a scene. Unlike other methods in the literature, the algorithm does not require any prior training, has only one free parameter, and is shown to provide consistent results over a variety of terrains without the need for any manual tuning. The proposed method is compared to two other data extraction methods from the literature. Results of Support Vector classifiers using training data extracted by the proposed technique are superior in terms of quality and consistency of free space estimation. Furthermore, the computation time required by the proposed technique is shown to be smaller and more consistent than that of other training data extraction methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Harakeh_Identifying_Good_Training_CVPR_2016_paper.pdf",
        "aff": "Department of Mechanical Engineering, VRL Lab, American University of Beirut, Beirut, Lebanon; Department of Mechanical Engineering, VRL Lab, American University of Beirut, Beirut, Lebanon; Department of Mechanical Engineering, VRL Lab, American University of Beirut, Beirut, Lebanon",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2501701,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9282907042050568044&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "mail.aub.edu;aub.edu.lb;aub.edu.lb",
        "email": "mail.aub.edu;aub.edu.lb;aub.edu.lb",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Harakeh_Identifying_Good_Training_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "American University of Beirut",
        "aff_unique_dep": "Department of Mechanical Engineering",
        "aff_unique_url": "https://www.aub.edu.lb",
        "aff_unique_abbr": "AUB",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beirut",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Lebanon"
    },
    {
        "title": "Image Captioning With Semantic Attention",
        "session": "High Level Semantics",
        "status": "Spotlight",
        "track": "main",
        "pid": "11",
        "author_site": "Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo",
        "author": "Quanzeng You; Hailin Jin; Zhaowen Wang; Chen Fang; Jiebo Luo",
        "abstract": "Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation.  We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/You_Image_Captioning_With_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, University of Rochester, Rochester NY 14627, USA; Adobe Research, 345 Park Ave, San Jose CA 95110, USA; Adobe Research, 345 Park Ave, San Jose CA 95110, USA; Adobe Research, 345 Park Ave, San Jose CA 95110, USA; Department of Computer Science, University of Rochester, Rochester NY 14627, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1676646,
        "gs_citation": 2268,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1207835255393155860&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.rochester.edu;adobe.com;adobe.com;adobe.com;cs.rochester.edu",
        "email": "cs.rochester.edu;adobe.com;adobe.com;adobe.com;cs.rochester.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/You_Image_Captioning_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "University of Rochester;Adobe",
        "aff_unique_dep": "Department of Computer Science;Adobe Research",
        "aff_unique_url": "https://www.rochester.edu;https://research.adobe.com",
        "aff_unique_abbr": "U of R;Adobe",
        "aff_campus_unique_index": "0;1;1;1;0",
        "aff_campus_unique": "Rochester;San Jose",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Image Deblurring Using Smartphone Inertial Sensors",
        "session": "Deblurring and Super-Resolution",
        "status": "Poster",
        "track": "main",
        "pid": "39",
        "author_site": "Zhe Hu, Lu Yuan, Stephen Lin, Ming-Hsuan Yang",
        "author": "Zhe Hu; Lu Yuan; Stephen Lin; Ming-Hsuan Yang",
        "abstract": "Removing image blur caused by camera shake is an ill-posed problem, as both the latent image and the point spread function (PSF) are unknown. A recent approach to address this problem is to record camera motion through inertial sensors, i.e., gyroscopes and accelerometers, and then reconstruct spatially-variant PSFs from these readings. While this approach has been effective for high-quality inertial sensors, it has been infeasible for the inertial sensors in smartphones, which are of relatively low quality and present a number of challenging issues, including varying sensor parameters, high sensor noise, and calibration error. In this paper, we identify the issues that plague smartphone inertial sensors and propose a solution that successfully utilizes the sensor readings for image deblurring. With both the sensor data and the image itself, the proposed method is able to accurately estimate the sensor parameters online and also the spatially-variant PSFs for enhanced deblurring performance. The effectiveness of this technique is demonstrated in experiments on a popular mobile phone. With this approach, the quality of image deblurring can be appreciably raised on the most common of imaging devices.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hu_Image_Deblurring_Using_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1112319,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16012128848162456199&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hu_Image_Deblurring_Using_CVPR_2016_paper.html"
    },
    {
        "title": "Image Question Answering Using Convolutional Neural Network With Dynamic Parameter Prediction",
        "session": "Image Captioning and Question Answering",
        "status": "Oral",
        "track": "main",
        "pid": "4",
        "author_site": "Hyeonwoo Noh, Paul Hongsuck Seo, Bohyung Han",
        "author": "Hyeonwoo Noh; Paul Hongsuck Seo; Bohyung Han",
        "abstract": "We tackle image question answering (ImageQA) problem by learning a convolutional neural network (CNN) with a dynamic parameter layer whose weights are determined adaptively based on questions. For the adaptive parameter prediction, we employ a separate parameter prediction network, which consists of gated recurrent unit (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights as its output. However, it is challenging to construct a parameter prediction network for a large number of parameters in the fully-connected dynamic parameter layer of the CNN. We reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer. The proposed network---joint network with the CNN for ImageQA and the parameter prediction network---is trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU. The proposed algorithm illustrates the state-of-the-art performance on all available public ImageQA benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Noh_Image_Question_Answering_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science and Engineering, POSTECH, Korea; Department of Computer Science and Engineering, POSTECH, Korea; Department of Computer Science and Engineering, POSTECH, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1133121,
        "gs_citation": 435,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11570696530124888892&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Noh_Image_Question_Answering_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "POSTECH",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Image Style Transfer Using Convolutional Neural Networks",
        "session": "Computational Photography and Faces",
        "status": "Oral",
        "track": "main",
        "pid": "17",
        "author_site": "Leon A. Gatys, Alexander S. Ecker, Matthias Bethge",
        "author": "Leon A. Gatys; Alexander S. Ecker; Matthias Bethge",
        "abstract": "Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous well-known artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf",
        "aff": "Centre for Integrative Neuroscience, University of T\u00fcbingen, Germany+Bernstein Center for Computational Neuroscience, T\u00fcbingen, Germany+Graduate School of Neural Information Processing, University of T\u00fcbingen, Germany; Centre for Integrative Neuroscience, University of T\u00fcbingen, Germany+Bernstein Center for Computational Neuroscience, T\u00fcbingen, Germany+Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany+Baylor College of Medicine, Houston, TX, USA; Centre for Integrative Neuroscience, University of T\u00fcbingen, Germany+Bernstein Center for Computational Neuroscience, T\u00fcbingen, Germany+Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3046055,
        "gs_citation": 7219,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15430064963552939126&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 17,
        "aff_domain": "bethgelab.org; ; ",
        "email": "bethgelab.org; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gatys_Image_Style_Transfer_CVPR_2016_paper.html",
        "aff_unique_index": "0+1+0;0+1+2+3;0+1+2",
        "aff_unique_norm": "University of T\u00fcbingen;Bernstein Center for Computational Neuroscience;Max Planck Institute for Biological Cybernetics;Baylor College of Medicine",
        "aff_unique_dep": "Centre for Integrative Neuroscience;Computational Neuroscience;;",
        "aff_unique_url": "https://www.uni-tuebingen.de;;https://www.biocybernetics.mpg.de;https://www.bcm.edu",
        "aff_unique_abbr": ";;MPIBC;BCM",
        "aff_campus_unique_index": "1;1+1+2;1+1",
        "aff_campus_unique": ";T\u00fcbingen;Houston",
        "aff_country_unique_index": "0+0+0;0+0+0+1;0+0+0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "title": "Improved Hamming Distance Search Using Variable Length Substrings",
        "session": "Image Indexing and Retrieval",
        "status": "Poster",
        "track": "main",
        "pid": "54",
        "author_site": "Eng-Jon Ong, Miroslaw Bober",
        "author": "Eng-Jon Ong; Miroslaw Bober",
        "abstract": "This paper addresses the problem of ultra-large-scale search in Hamming spaces. There has been considerable research on generating compact binary codes in vision, for example for visual search tasks. However the issue of efficient searching through huge sets of binary codes remains largely unsolved. To this end, we propose a novel, unsupervised approach to thresholded search in Hamming space, supporting long codes (e.g. 512-bits) with a wide-range of Hamming distance radii. Our method is capable of working efficiently with billions of codes delivering between one to three orders of magnitude acceleration, as compared to prior art. This is achieved by relaxing the equal-size constraint in the Multi-Index Hashing approach, leading to multiple hash-tables with variable length hash-keys. Based on the theoretical analysis of the retrieval probabilities of multiple hash-tables we propose a novel search algorithm for obtaining a suitable set of hash-key lengths. The resulting retrieval mechanism is shown empirically to improve the efficiency over the state-of-the-art, across a range of datasets, bit-depths and retrieval thresholds.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ong_Improved_Hamming_Distance_CVPR_2016_paper.pdf",
        "aff": "Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6830030,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2088199883989195999&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "surrey.ac.uk;surrey.ac.uk",
        "email": "surrey.ac.uk;surrey.ac.uk",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ong_Improved_Hamming_Distance_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Surrey",
        "aff_unique_dep": "Centre for Vision, Speech and Signal Processing",
        "aff_unique_url": "https://www.surrey.ac.uk",
        "aff_unique_abbr": "Surrey",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Guildford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Improving Human Action Recognition by Non-Action Classification",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "47",
        "author_site": "Yang Wang, Minh Hoai",
        "author": "Yang Wang; Minh Hoai",
        "abstract": "In this paper we consider the task of recognizing human actions in realistic video where human actions are dominated by irrelevant factors. We first study the benefits of removing non-action video segments, which are the ones that do not portray any human action. We then learn a non-action classifier and use it to down-weight irrelevant video segments. The non-action classifier is trained using ActionThread, a dataset with shot-level annotation for the occurrence or absence of a human action. The non-action classifier can be used to identify non-action shots with high precision and subsequently used to improve the performance of action recognition systems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Improving_Human_Action_CVPR_2016_paper.pdf",
        "aff": "Stony Brook University, Stony Brook, NY 11794, USA; Stony Brook University, Stony Brook, NY 11794, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1294328,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15819992011021678859&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Improving_Human_Action_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stony Brook University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stonybrook.edu",
        "aff_unique_abbr": "SBU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stony Brook",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Improving Person Re-Identification via Pose-Aware Multi-Shot Matching",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "63",
        "author_site": "Yeong-Jun Cho, Kuk-Jin Yoon",
        "author": "Yeong-Jun Cho; Kuk-Jin Yoon",
        "abstract": "Person re-identification is the problem of recognizing people across images or videos from non-overlapping views. Although there has been much progress in person re-identification for the last decade, it still remains a challenging task because of severe appearance changes of a person due to diverse camera viewpoints and person poses. In this paper, we propose a novel framework for person re-identification by analyzing camera viewpoints and person poses, so-called Pose-aware Multi-shot Matching (PaMM), which robustly estimates target poses and efficiently conducts multi-shot matching based on the target pose information. Experimental results using public person re-identification dataset show that the proposed methods are promising for person re-identification under diverse viewpoints and pose variances.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cho_Improving_Person_Re-Identification_CVPR_2016_paper.pdf",
        "aff": "Computer Vision Laboratory, GIST, South Korea; Computer Vision Laboratory, GIST, South Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Cho_Improving_Person_Re-Identification_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2048968,
        "gs_citation": 153,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11512588608170779913&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "gist.ac.kr;gist.ac.kr",
        "email": "gist.ac.kr;gist.ac.kr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cho_Improving_Person_Re-Identification_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "GIST",
        "aff_unique_dep": "Computer Vision Laboratory",
        "aff_unique_url": "https://www.gist.ac.kr",
        "aff_unique_abbr": "GIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Improving the Robustness of Deep Neural Networks via Stability Training",
        "session": "Statistical Methods and Learning",
        "status": "Poster",
        "track": "main",
        "pid": "74",
        "author_site": "Stephan Zheng, Yang Song, Thomas Leung, Ian Goodfellow",
        "author": "Stephan Zheng; Yang Song; Thomas Leung; Ian Goodfellow",
        "abstract": "In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the state-of-the-art Inception architecture against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on large-scale near-duplicate detection, similar-image ranking, and classification on noisy datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zheng_Improving_the_Robustness_CVPR_2016_paper.pdf",
        "aff": "Google+Caltech; Google; Google; Google",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2017586,
        "gs_citation": 823,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17751529404503610138&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "caltech.edu;google.com;google.com;google.com",
        "email": "caltech.edu;google.com;google.com;google.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zheng_Improving_the_Robustness_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;0;0",
        "aff_unique_norm": "Google;California Institute of Technology",
        "aff_unique_dep": "Google;",
        "aff_unique_url": "https://www.google.com;https://www.caltech.edu",
        "aff_unique_abbr": "Google;Caltech",
        "aff_campus_unique_index": "0+1;0;0;0",
        "aff_campus_unique": "Mountain View;Pasadena",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "In Defense of Sparse Tracking: Circulant Sparse Tracker",
        "session": "Video Analysis 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "10",
        "author_site": "Tianzhu Zhang, Adel Bibi, Bernard Ghanem",
        "author": "Tianzhu Zhang; Adel Bibi; Bernard Ghanem",
        "abstract": "Sparse representation has been introduced to visual tracking by finding the best target candidate with minimal reconstruction error within the particle filter framework. However, most sparse representation based trackers have high computational cost, less than promising tracking performance, and limited feature representation. To deal with the above issues, we propose a novel circulant sparse tracker (CST), which exploits circulant target templates. Because of the circulant structure property, CST has the following advantages: (1) It can refine and reduce particles using circular shifts of target templates. (2) The optimization can be efficiently solved entirely in the Fourier domain. (3) High dimensional features can be embedded into CST to significantly improve tracking performance without sacrificing much computation time. Both qualitative and quantitative evaluations on challenging benchmark sequences demonstrate that CST performs better than all other sparse trackers and favorably against state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_In_Defense_of_CVPR_2016_paper.pdf",
        "aff": "King Abdullah University of Science and Technology (KAUST), Saudi Arabia+Institute of Automation, Chinese Academy of Sciences (CASIA), China; King Abdullah University of Science and Technology (KAUST), Saudi Arabia; King Abdullah University of Science and Technology (KAUST), Saudi Arabia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhang_In_Defense_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1478959,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2420185711594047657&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "kaust.edu.sa;kaust.edu.sa;kaust.edu.sa",
        "email": "kaust.edu.sa;kaust.edu.sa;kaust.edu.sa",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_In_Defense_of_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "King Abdullah University of Science and Technology;Chinese Academy of Sciences",
        "aff_unique_dep": ";Institute of Automation",
        "aff_unique_url": "https://www.kaust.edu.sa;http://www.ia.cas.cn",
        "aff_unique_abbr": "KAUST;CASIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "Saudi Arabia;China"
    },
    {
        "title": "In the Shadows, Shape Priors Shine: Using Occlusion to Improve Multi-Region Segmentation",
        "session": "Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "42",
        "author_site": "Yuka Kihara, Matvey Soloviev, Tsuhan Chen",
        "author": "Yuka Kihara; Matvey Soloviev; Tsuhan Chen",
        "abstract": "We present a new algorithm for multi-region segmentation of 2D images with objects that may partially occlude each other. Our algorithm is based on the observation that human performance on this task is based both on prior knowledge about plausible shapes and taking into account the presence of occluding objects whose shape is already known - once an occluded region is identified, the shape prior can be used to guess the shape of the missing part. We capture the former aspect using a deep learning model of shape; for the latter, we simultaneously minimize the energy of all regions and consider only unoccluded pixels for data agreement.   Existing algorithms incorporating object shape priors consider every object separately in turn and can't distinguish genuine deviation from the expected shape from parts missing due to occlusion. We show that our method significantly improves on the performance of a representative algorithm, as evaluated on both preprocessed natural and synthetic images. Furthermore, on the synthetic images, we recover the ground truth segmentation with good accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kihara_In_the_Shadows_CVPR_2016_paper.pdf",
        "aff": "Ricoh Co., Ltd.*; Cornell University; Cornell University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1043431,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12863405380574503283&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "nts.ricoh.co.jp;cs.cornell.edu;cornell.edu",
        "email": "nts.ricoh.co.jp;cs.cornell.edu;cornell.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kihara_In_the_Shadows_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Ricoh Co., Ltd.;Cornell University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ricoh.com/;https://www.cornell.edu",
        "aff_unique_abbr": "Ricoh;Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Japan;United States"
    },
    {
        "title": "Incremental Object Discovery in Time-Varying Image Collections",
        "session": "Image Indexing and Retrieval",
        "status": "Poster",
        "track": "main",
        "pid": "63",
        "author_site": "Theodora Kontogianni, Markus Mathias, Bastian Leibe",
        "author": "Theodora Kontogianni; Markus Mathias; Bastian Leibe",
        "abstract": "Abstract In this paper, we address the problem of object discovery in time-varying, large-scale image collections. A core part of our approach is a novel Limited Horizon Minimum Spanning Tree (LH-MST) structure that closely approximates the Minimum Spanning Tree at a small fraction of the latter's computational cost. Our proposed tree structure can be created in a local neighborhood of the matching graph during image retrieval and can be efficiently updated whenever the image database is extended. We show how the LH-MST can be used within both single-link hierarchical agglomer- ative clustering and the Iconoid Shift framework for object discovery in image collections, resulting in significant efficiency gains and making both approaches capable of incremental clustering with online updates. We evaluate our approach on a dataset of 500k images from the city of Paris and compare its results to the batch version of both clustering algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kontogianni_Incremental_Object_Discovery_CVPR_2016_paper.pdf",
        "aff": "Visual Computing Institute, Computer Vision Group, RWTH Aachen University; Visual Computing Institute, Computer Vision Group, RWTH Aachen University; Visual Computing Institute, Computer Vision Group, RWTH Aachen University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Kontogianni_Incremental_Object_Discovery_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1698003,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15531609734530531264&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "vision.rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de",
        "email": "vision.rwth-aachen.de;vision.rwth-aachen.de;vision.rwth-aachen.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kontogianni_Incremental_Object_Discovery_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "RWTH Aachen University",
        "aff_unique_dep": "Visual Computing Institute, Computer Vision Group",
        "aff_unique_url": "https://www.rwth-aachen.de",
        "aff_unique_abbr": "RWTH",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Aachen",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Inextensible Non-Rigid Shape-From-Motion by Second-Order Cone Programming",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "24",
        "author_site": "Ajad Chhatkuli, Daniel Pizarro, Toby Collins, Adrien Bartoli",
        "author": "Ajad Chhatkuli; Daniel Pizarro; Toby Collins; Adrien Bartoli",
        "abstract": "We present a global and convex formulation for template-less 3D reconstruction of a deforming object with the perspective camera.    We show for the first time how to construct a Second-Order Cone Programming (SOCP) problem for Non-Rigid Shape-from-Motion (NRSfM) using the Maximum-Depth Heuristic (MDH).    In this regard, we deviate strongly from the general trend of using affine cameras and factorization-based methods to solve NRSfM.    In MDH, the points' depths are maximized so that the distance between neighbouring points in camera space are upper bounded by the geodesic distance.    In NRSfM both geodesic and camera space distances are unknown.    We show that, nonetheless, given point correspondences and the camera's intrinsics the whole problem is convex and solvable with SOCP. We show with extensive experiments that our method accurately reconstructs quasi-isometric surfaces from partial views under articulated and strong deformations. It naturally handles missing correspondences, non-smooth objects and is very simple to implement compared to previous methods, with only one free parameter (the neighbourhood size).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chhatkuli_Inextensible_Non-Rigid_Shape-From-Motion_CVPR_2016_paper.pdf",
        "aff": "ISIT - CNRS/Universit \u00b4e d\u2019Auvergne, Clermont-Ferrand, France; ISIT - CNRS/Universit \u00b4e d\u2019Auvergne, Clermont-Ferrand, France; ISIT - CNRS/Universit \u00b4e d\u2019Auvergne, Clermont-Ferrand, France; GEINTRA, Universidad de Alcal \u00b4a, Alcal \u00b4a de Henares, Spain",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1616947,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6092498561869096298&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "gmail.com; ; ; ",
        "email": "gmail.com; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chhatkuli_Inextensible_Non-Rigid_Shape-From-Motion_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Universit\u00e9 d\u2019Auvergne;Universidad de Alcal\u00e1",
        "aff_unique_dep": "ISIT - CNRS;GEINTRA",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Clermont-Ferrand;Alcal\u00e1 de Henares",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "France;Spain"
    },
    {
        "title": "Inferring Forces and Learning Human Utilities From Videos",
        "session": "Video Understanding",
        "status": "Oral",
        "track": "main",
        "pid": "4",
        "author_site": "Yixin Zhu, Chenfanfu Jiang, Yibiao Zhao, Demetri Terzopoulos, Song-Chun Zhu",
        "author": "Yixin Zhu; Chenfanfu Jiang; Yibiao Zhao; Demetri Terzopoulos; Song-Chun Zhu",
        "abstract": "We propose a notion of affordance that takes into account physical quantities generated when the human body interacts with real-world objects, and introduce a learning framework that incorporates the concept of human utilities, which in our opinion provides a deeper and finer-grained account not only of object affordance but also of people's interaction with objects. Rather than defining affordance in terms of the geometric compatibility between body poses and 3D objects, we devise algorithms that employ physics-based simulation to infer the relevant forces/pressures acting on body parts. By observing the choices people make in videos (particularly in selecting a chair in which to sit) our system learns the comfort intervals of the forces exerted on body parts (while sitting). We account for people's preferences in terms of human utilities, which transcend comfort intervals to account also for meaningful tasks within scenes and spatiotemporal constraints in motion planning, such as for the purposes of robot task planning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_Inferring_Forces_and_CVPR_2016_paper.pdf",
        "aff": "UCLA Center for Vision, Cognition, Learning and Autonomy; UCLA Computer Graphics & Vision Laboratory; UCLA Center for Vision, Cognition, Learning and Autonomy; UCLA Computer Graphics & Vision Laboratory; UCLA Center for Vision, Cognition, Learning and Autonomy",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1648451,
        "gs_citation": 113,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8828692287589327249&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ucla.edu;cs.ucla.edu;mit.edu;cs.ucla.edu;stat.ucla.edu",
        "email": "ucla.edu;cs.ucla.edu;mit.edu;cs.ucla.edu;stat.ucla.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_Inferring_Forces_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, Los Angeles",
        "aff_unique_dep": "Center for Vision, Cognition, Learning and Autonomy",
        "aff_unique_url": "https://www.ucla.edu",
        "aff_unique_abbr": "UCLA",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Information Bottleneck Learning Using Privileged Information for Visual Recognition",
        "session": "Supervised Learning",
        "status": "Poster",
        "track": "main",
        "pid": "78",
        "author_site": "Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, Gianfranco Doretto",
        "author": "Saeid Motiian; Marco Piccirilli; Donald A. Adjeroh; Gianfranco Doretto",
        "abstract": "We explore the visual recognition problem from a main data view when an auxiliary data view is available during training. This is important because it allows improving the training of visual classifiers when paired additional data is cheaply available, and it improves the recognition from multi-view data when there is a missing view at testing time. The problem is challenging because of the intrinsic asymmetry caused by the missing auxiliary view during testing. We account for such view during training by extending the information bottleneck method, and by combining it with risk minimization. In this way, we establish an information theoretic principle for leaning any type of visual classifier under this particular setting. We use this principle to design a large-margin classifier with an efficient optimization in the primal space. We extensively compare our method with the state-of-the-art on different visual recognition datasets, and with different types of auxiliary data, and show that the proposed framework has a very promising potential.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Motiian_Information_Bottleneck_Learning_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1133211,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6304832304163384562&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Motiian_Information_Bottleneck_Learning_CVPR_2016_paper.html"
    },
    {
        "title": "Information-Driven Adaptive Structured-Light Scanners",
        "session": "Vision With Alternative Sensors",
        "status": "Oral",
        "track": "main",
        "pid": "13",
        "author_site": "Guy Rosman, Daniela Rus, John W. Fisher III",
        "author": "Guy Rosman; Daniela Rus; John W. Fisher III",
        "abstract": "Sensor planning and active sensing, long studied in robotics, adapt sensor positioning and operation mode in order to maximize information gain. While these concepts are often used to reason about 3D sensors, these are usually treated as a predefined, black-box, component. In this paper we show how the same principles can be used as part of the 3D sensor.  We describe the relevant generative model for structured-light 3D scanning and show how adaptive pattern selection can maximize information gain in an open-loop with-feedback manner. We then demonstrate how different choices of relevant variable sets (corresponding to the subproblems of locatization and mapping) lead to different criteria for pattern selection and can be computed in an online fashion. We show results for both subproblems with several pattern dictionary choices and demonstrate their usefulness for pose estimation and depth acquisition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Rosman_Information-Driven_Adaptive_Structured-Light_CVPR_2016_paper.pdf",
        "aff": "CSAIL, Massachusetts Institute of Technology; CSAIL, Massachusetts Institute of Technology; CSAIL, Massachusetts Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3175294,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3927882006778202996&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "email": "csail.mit.edu;csail.mit.edu;csail.mit.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Rosman_Information-Driven_Adaptive_Structured-Light_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Inside-Outside Net: Detecting Objects in Context With Skip Pooling and Recurrent Neural Networks",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "66",
        "author_site": "Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick",
        "author": "Sean Bell; C. Lawrence Zitnick; Kavita Bala; Ross Girshick",
        "abstract": "It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 77.9% mAP. On the new and more challenging MS COCO dataset, we improve state-of-the-art from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won \"Best Student Entry\" and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf",
        "aff": "Cornell University; Microsoft Research; Cornell University; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Bell_Inside-Outside_Net_Detecting_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 696714,
        "gs_citation": 1678,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17824096759436033262&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.cornell.edu;fb.com;cs.cornell.edu;fb.com",
        "email": "cs.cornell.edu;fb.com;cs.cornell.edu;fb.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Cornell University;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.cornell.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Cornell;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Instance-Aware Semantic Segmentation via Multi-Task Network Cascades",
        "session": "Semantic Segmentation",
        "status": "Oral",
        "track": "main",
        "pid": "13",
        "author_site": "Jifeng Dai, Kaiming He, Jian Sun",
        "author": "Jifeng Dai; Kaiming He; Jian Sun",
        "abstract": "Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multi-task Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems.  The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf",
        "aff": "Microsoft Research; Microsoft Research; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1033026,
        "gs_citation": 1791,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7342374495730352680&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Instance-Level Segmentation for Autonomous Driving With Deep Densely Connected MRFs",
        "session": "Segmentation and Saliency",
        "status": "Poster",
        "track": "main",
        "pid": "72",
        "author_site": "Ziyu Zhang, Sanja Fidler, Raquel Urtasun",
        "author": "Ziyu Zhang; Sanja Fidler; Raquel Urtasun",
        "abstract": "Our aim is to provide a pixel-wise instance-level labeling of a monocular image in the context of autonomous driving. We build on recent work [Zhang et al., ICCV15] that trained a convolutional neural net to predict instance labeling in local image patches, extracted exhaustively in a stride from an image.  A simple Markov random field model using several heuristics was then proposed in [Zhang et al., ICCV15] to derive a globally consistent instance labeling of the image. In this paper, we formulate the global labeling problem with a novel densely connected Markov random field  and show how to encode various intuitive potentials in a way that is amenable to  efficient mean field inference [Krahenbuhl et al., NIPS11]. Our potentials encode the compatibility between the global labeling and the patch-level predictions, contrast-sensitive smoothness as well as the fact that separate regions form different instances. Our experiments on the challenging KITTI benchmark [Geiger et al., CVPR12] demonstrate that our method achieves a significant performance boost over the baseline [Zhang et al., ICCV15].",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Instance-Level_Segmentation_for_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1814220,
        "gs_citation": 292,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14167978569782502021&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu;cs.toronto.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Instance-Level_Segmentation_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Instance-Level Video Segmentation From Object Tracks",
        "session": "Semantic Video Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "70",
        "author_site": "Guillaume Seguin, Piotr Bojanowski, R\u00e9mi Lajugie, Ivan Laptev",
        "author": "Guillaume Seguin; Piotr Bojanowski; Remi Lajugie; Ivan Laptev",
        "abstract": "We address the problem of segmenting multiple object instances in complex videos. Our method does not require manual pixel-level annotation for training, and relies instead on readily-available object detectors or visual object tracking only. Given object bounding boxes at input, we cast video segmentation as a weakly-supervised learning problem. Our proposed objective combines (a) a discriminative clustering term for background segmentation, (b) a spectral clustering one for grouping pixels of same object instances, and (c) linear constraints enabling instance-level segmentation. We propose a convex relaxation of this problem and solve it efficiently using the Frank-Wolfe algorithm. We report results and compare our method to several baselines on a new video dataset for multi-instance person segmentation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Seguin_Instance-Level_Video_Segmentation_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1090416,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13702892177954196723&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Seguin_Instance-Level_Video_Segmentation_CVPR_2016_paper.html"
    },
    {
        "title": "InterActive: Inter-Layer Activeness Propagation",
        "session": "Feature Extraction and Description",
        "status": "Poster",
        "track": "main",
        "pid": "29",
        "author_site": "Lingxi Xie, Liang Zheng, Jingdong Wang, Alan L. Yuille, Qi Tian",
        "author": "Lingxi Xie; Liang Zheng; Jingdong Wang; Alan L. Yuille; Qi Tian",
        "abstract": "An increasing number of computer vision tasks can be tackled with deep features, which are the intermediate outputs of a pre-trained Convolutional Neural Network. Despite the astonishing performance, deep features extracted from low-level neurons are still below satisfaction, arguably because they cannot access the spatial context contained in the higher layers. In this paper, we present InterActive, a novel algorithm which computes the activeness of neurons and network connections. Activeness is propagated through a neural network in a top-down manner, carrying high-level context and improving the descriptive power of low-level and mid-level neurons. Visualization indicates that neuron activeness can be interpreted as spatial-weighted neuron responses. We achieve state-of-the-art classification performance on a wide range of image datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_InterActive_Inter-Layer_Activeness_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12798849036565926769&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xie_InterActive_Inter-Layer_Activeness_CVPR_2016_paper.html"
    },
    {
        "title": "Interactive Segmentation on RGBD Images via Cue Selection",
        "session": "Segmentation and Contour Detection",
        "status": "Spotlight",
        "track": "main",
        "pid": "17",
        "author_site": "Jie Feng, Brian Price, Scott Cohen, Shih-Fu Chang",
        "author": "Jie Feng; Brian Price; Scott Cohen; Shih-Fu Chang",
        "abstract": "Interactive image segmentation is an important problem in computer vision with many applications including image editing, object recognition and image retrieval. Most existing interactive segmentation methods only operate on color images. Until recently, very few works have been proposed to leverage depth information from low-cost sensors to improve interactive segmentation. While these methods achieve better results than color-based methods, they are still limited in either using depth as an additional color channel or simply combining depth with color in a linear way. We propose a novel interactive segmentation algorithm which can incorporate multiple feature cues like color, depth, and normals in an unified graph cut framework to leverage these cues more effectively. A key contribution of our method is that it automatically selects a single cue to be used at each pixel, based on the intuition that only one cue is necessary to determine the segmentation label locally.  This is achieved by optimizing over both segmentation labels and cue labels, using terms designed to decide where both the segmentation and label cues should change. Our algorithm thus produces not only the segmentation mask but also a cue label map that indicates where each cue contributes to the final result. Extensive experiments on five large scale RGBD datasets show that our proposed algorithm performs significantly better than both other color-based and RGBD based algorithms in reducing the amount of user inputs as well as increasing segmentation accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Feng_Interactive_Segmentation_on_CVPR_2016_paper.pdf",
        "aff": "Columbia University; Adobe Research; Adobe Research; Columbia University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1597243,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2827229308788605250&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cs.columbia.edu;cs.columbia.edu;adobe.com;adobe.com",
        "email": "cs.columbia.edu;cs.columbia.edu;adobe.com;adobe.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Feng_Interactive_Segmentation_on_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Columbia University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.columbia.edu;https://research.adobe.com",
        "aff_unique_abbr": "Columbia;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Inverting Visual Representations With Convolutional Networks",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "30",
        "author_site": "Alexey Dosovitskiy, Thomas Brox",
        "author": "Alexey Dosovitskiy; Thomas Brox",
        "abstract": "Feature representations, both hand-designed and learned ones, are often hard to analyze and interpret, even when they are extracted from visual data. We propose a new approach to study image representations by inverting them with an up-convolutional neural network. We apply the method to shallow representations (HOG, SIFT, LBP), as well as to deep networks. For shallow representations our approach provides significantly better reconstructions than existing methods, revealing that there is surprisingly rich information contained in these features. Inverting a deep network trained on ImageNet provides several insights into the properties of the feature representation learned by the network. Most strikingly, the colors and the rough contours of an image can be reconstructed from activations in higher network layers and even from the predicted class probabilities.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Dosovitskiy_Inverting_Visual_Representations_CVPR_2016_paper.pdf",
        "aff": "University of Freiburg; University of Freiburg",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Dosovitskiy_Inverting_Visual_Representations_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1669158,
        "gs_citation": 838,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13363611608288523650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.uni-freiburg.de;cs.uni-freiburg.de",
        "email": "cs.uni-freiburg.de;cs.uni-freiburg.de",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Dosovitskiy_Inverting_Visual_Representations_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Freiburg",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-freiburg.de",
        "aff_unique_abbr": "UoF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Isometric Non-Rigid Shape-From-Motion in Linear Time",
        "session": "Non-Rigid Reconstruction and Motion Analysis",
        "status": "Oral",
        "track": "main",
        "pid": "14",
        "author_site": "Shaifali Parashar, Daniel Pizarro, Adrien Bartoli",
        "author": "Shaifali Parashar; Daniel Pizarro; Adrien Bartoli",
        "abstract": "We study Isometric Non-Rigid Shape-from-Motion (Iso-NRSfM): given multiple intrinsically calibrated monocular images, we want to reconstruct the time-varying 3D shape of an object undergoing isometric deformations.   We show that Iso-NRSfM is solvable from the warps (the inter-image geometric transformations).   We propose a new theoretical framework based on Riemmanian manifolds to represent the unknown 3D surfaces, as embeddings of the camera's retinal planes.   This allows us to use the manifolds' metric tensor and Christoffel Symbol fields, which we prove are related across images by simple rules depending only on the warps.   This forms a set of important theoretical results.   Using the infinitesimal planarity formulation, it then allows us to derive a system of two quartics in two variables for each image pair.   The sum-of-squares of these polynomials is independent of the number of images and can be solved globally, forming a well-posed problem for N >= 3 images, whose solution directly leads to the surface's normal field.   The proposed method outperforms existing work in terms of accuracy and computation cost on synthetic and real datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Parashar_Isometric_Non-Rigid_Shape-From-Motion_CVPR_2016_paper.pdf",
        "aff": "ALCoV-ISIT, UMR 6284 CNRS / Universit \u00b4e d\u2019Auvergne, Clermont-Ferrand, France+Geintra Research Group, Universidad de Alcal \u00b4a, Alcal \u00b4a de Henares, Spain; ALCoV-ISIT, UMR 6284 CNRS / Universit \u00b4e d\u2019Auvergne, Clermont-Ferrand, France; ALCoV-ISIT, UMR 6284 CNRS / Universit \u00b4e d\u2019Auvergne, Clermont-Ferrand, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 992465,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1899230922331338026&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Parashar_Isometric_Non-Rigid_Shape-From-Motion_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Universit\u00e9 d\u2019Auvergne;Universidad de Alcal\u00e1",
        "aff_unique_dep": "ALCoV-ISIT, UMR 6284 CNRS;Geintra Research Group",
        "aff_unique_url": "https://www.univ-auvergne.fr;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0+1;0;0",
        "aff_campus_unique": "Clermont-Ferrand;Alcal\u00e1 de Henares",
        "aff_country_unique_index": "0+1;0;0",
        "aff_country_unique": "France;Spain"
    },
    {
        "title": "Iterative Instance Segmentation",
        "session": "Semantic Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "68",
        "author_site": "Ke Li, Bharath Hariharan, Jitendra Malik",
        "author": "Ke Li; Bharath Hariharan; Jitendra Malik",
        "abstract": "Existing methods for pixel-wise labelling tasks generally disregard the underlying structure of labellings, often leading to predictions that are visually implausible. While incorporating structure into the model should improve prediction quality, doing so is challenging - manually specifying the form of structural constraints may be impractical and inference often becomes intractable even if structural constraints are given. We sidestep this problem by reducing structured prediction to a sequence of unconstrained prediction problems and demonstrate that this approach is capable of automatically discovering priors on shape, contiguity of region predictions and smoothness of region contours from data without any a priori specification. On the instance segmentation task, this method outperforms the state-of-the-art, achieving a mean AP^r of 63.6% at 50% overlap and 43.3% at 70% overlap.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Iterative_Instance_Segmentation_CVPR_2016_paper.pdf",
        "aff": "UC Berkeley; Facebook AI Research; UC Berkeley",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Li_Iterative_Instance_Segmentation_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3946362,
        "gs_citation": 185,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17364638429898443539&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "eecs.berkeley.edu;fb.com;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;fb.com;eecs.berkeley.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Iterative_Instance_Segmentation_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Meta",
        "aff_unique_dep": ";Facebook AI Research",
        "aff_unique_url": "https://www.berkeley.edu;https://research.facebook.com",
        "aff_unique_abbr": "UC Berkeley;FAIR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Joint Learning of Single-Image and Cross-Image Representations for Person Re-Identification",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "56",
        "author_site": "Faqiang Wang, Wangmeng Zuo, Liang Lin, David Zhang, Lei Zhang",
        "author": "Faqiang Wang; Wangmeng Zuo; Liang Lin; David Zhang; Lei Zhang",
        "abstract": "Person re-identification has been usually solved as either the matching of single-image representation (SIR) or the classification of cross-image representation (CIR). In this work, we exploit the connection between these two categories of methods, and propose a joint learning framework to unify SIR and CIR using convolutional neural network (CNN). Specifically, our deep architecture contains one shared sub-network together with two sub-networks that extract the SIRs of given images and the CIRs of given image pairs, respectively. The SIR sub-network is required to be computed once for each image (in both the probe and gallery sets), and the depth of the CIR sub-network is required to be minimal to reduce computational burden. Therefore, the two types of representation can be jointly optimized for pursuing better matching accuracy with moderate computational cost. Furthermore, the representations learned with pairwise comparison and triplet comparison objectives can be combined to improve matching performance. Experiments on the CUHK03, CUHK01 and VIPeR datasets show that the proposed method can achieve favorable accuracy while compared with state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Joint_Learning_of_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China + Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Sun Yat-sen University, Guangzhou, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China + Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Dept. of Computing, The Hong Kong Polytechnic University, Hong Kong, China",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wang_Joint_Learning_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1719006,
        "gs_citation": 518,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17300211957607759012&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "163.com;gmail.com;ieee.org;comp.polyu.edu.hk;comp.polyu.edu.hk",
        "email": "163.com;gmail.com;ieee.org;comp.polyu.edu.hk;comp.polyu.edu.hk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Joint_Learning_of_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;2;0+1;1",
        "aff_unique_norm": "Harbin Institute of Technology;Hong Kong Polytechnic University;Sun Yat-sen University",
        "aff_unique_dep": "School of Computer Science and Technology;Dept. of Computing;",
        "aff_unique_url": "http://www.hit.edu.cn/;https://www.polyu.edu.hk;http://www.sysu.edu.cn/",
        "aff_unique_abbr": "HIT;PolyU;SYSU",
        "aff_campus_unique_index": "0+1;0;2;0+1;1",
        "aff_campus_unique": "Harbin;Hong Kong;Guangzhou",
        "aff_country_unique_index": "0+0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Joint Multiview Segmentation and Localization of RGB-D Images Using Depth-Induced Silhouette Consistency",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "26",
        "author_site": "Chi Zhang, Zhiwei Li, Rui Cai, Hongyang Chao, Yong Rui",
        "author": "Chi Zhang; Zhiwei Li; Rui Cai; Hongyang Chao; Yong Rui",
        "abstract": "In this paper, we propose an RGB-D camera localization approach which takes an effective geometry constraint, i.e. silhouette consistency, into consideration. Unlike existing approaches which usually assume the silhouettes are provided, we consider more practical scenarios and generate the silhouettes for multiple views on the fly. To obtain a set of accurate silhouettes, precise camera poses are required to propagate segmentation cues across views. To perform better localization, accurate silhouettes are needed to constrain camera poses. Therefore the two problems are intertwined with each other and require a joint treatment. Facilitated by the available depth, we introduce a simple but effective silhouette consistency energy term that binds traditional appearance-based multiview segmentation cost and RGB-D frame-to-frame matching cost together. Optimization of the problem w.r.t. binary segmentation masks and camera poses naturally fits in the graph cut minimization framework and the Gauss-Newton non-linear least-squares method respectively. Experiments show that the proposed approach achieves state-of-the-arts performance on both tasks of image segmentation and camera localization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Joint_Multiview_Segmentation_CVPR_2016_paper.pdf",
        "aff": "Sun Yat-Sen University; Microsoft Research; SYSU-CMU Shunde International Joint Research Institute, P.R. China; Sun Yat-Sen University; Microsoft Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1036233,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13163462890932249072&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Joint_Multiview_Segmentation_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;0;1",
        "aff_unique_norm": "Sun Yat-sen University;Microsoft;SYSU-CMU Shunde International Joint Research Institute",
        "aff_unique_dep": ";Microsoft Research;",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://www.microsoft.com/en-us/research;",
        "aff_unique_abbr": "SYSU;MSR;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shunde",
        "aff_country_unique_index": "0;1;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Joint Probabilistic Matching Using m-Best Solutions",
        "session": "Matching and Alignment",
        "status": "Oral",
        "track": "main",
        "pid": "15",
        "author_site": "Seyed Hamid Rezatofighi, Anton Milan, Zhen Zhang, Qinfeng Shi, Anthony Dick, Ian Reid",
        "author": "Seyed Hamid Rezatofighi; Anton Milan; Zhen Zhang; Qinfeng Shi; Anthony Dick; Ian Reid",
        "abstract": "Matching between two sets of objects is typically approached by finding the object pairs that collectively maximize the joint matching score. In this paper, we argue that this single solution does not necessarily lead to the optimal matching accuracy and that general one-to-one assignment problems can be improved by considering multiple hypotheses before computing the final similarity measure. To that end, we propose to utilize the marginal distributions for each entity. Previously, this idea has been neglected mainly because exact marginalization is intractable due to a combinatorial number of all possible matching permutations. Here, we propose a generic approach to efficiently approximate the marginal distributions by exploiting the m-best solutions of the original problem. This approach not only improves the matching solution, but also provides more accurate ranking of the results, because of the extra information included in the marginal distribution. We validate our claim on two distinct objectives: (i) person re-identification and temporal matching modelled as an integer linear program, and (ii) feature point matching using a quadratic cost function. Our experiments confirm that marginalization indeed leads to superior performance compared to the single (nearly) optimal solution, yielding state-of-the-art results in both applications on standard benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Rezatofighi_Joint_Probabilistic_Matching_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Rezatofighi_Joint_Probabilistic_Matching_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 32345395,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14349452807172436139&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Rezatofighi_Joint_Probabilistic_Matching_CVPR_2016_paper.html"
    },
    {
        "title": "Joint Recovery of Dense Correspondence and Cosegmentation in Two Images",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "49",
        "author_site": "Tatsunori Taniai, Sudipta N. Sinha, Yoichi Sato",
        "author": "Tatsunori Taniai; Sudipta N. Sinha; Yoichi Sato",
        "abstract": "We propose a new technique to jointly recover cosegmentation and dense per-pixel correspondence in two images. Our method parameterizes the correspondence field using piecewise similarity transformations and recovers a mapping between the estimated common \"foreground\" regions in the two images allowing them to be precisely aligned. Our formulation is based on a hierarchical Markov random field model with segmentation and transformation labels. The hierarchical structure uses nested image regions to constrain inference across multiple scales. Unlike prior hierarchical methods which assume that the structure is given, our proposed iterative technique dynamically recovers the structure as a variable along with the labeling. This joint inference is performed in an energy minimization framework using iterated graph cuts. We evaluate our method on a new dataset of 400 image pairs with manually obtained ground truth, where it outperforms state-of-the-art methods designed specifically for either cosegmentation or correspondence estimation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Taniai_Joint_Recovery_of_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Taniai_Joint_Recovery_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1920659,
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7929615086848294906&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Taniai_Joint_Recovery_of_CVPR_2016_paper.html"
    },
    {
        "title": "Joint Training of Cascaded CNN for Face Detection",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "46",
        "author_site": "Hongwei Qin, Junjie Yan, Xiu Li, Xiaolin Hu",
        "author": "Hongwei Qin; Junjie Yan; Xiu Li; Xiaolin Hu",
        "abstract": "Cascade has been widely used in face detection, where classifier with low computation cost can be firstly used to shrink most of the background while keeping the recall. The cascade in detection is popularized by seminal Viola-Jones framework and then widely used in other pipelines, such as DPM and CNN. However, to our best knowledge, most of the previous detection methods use cascade in a greedy manner, where previous stages in cascade are fixed when training a new stage. So optimizations of different CNNs are isolated. In this paper, we propose joint training to achieve end-to-end optimization for CNN cascade. We show that the back propagation algorithm used in training CNN can be naturally used in training CNN cascade. We present how jointly training can be conducted on naive CNN cascade and more sophisticated region proposal network (RPN) and fast R-CNN. Experiments on face detection benchmarks verify the advantages of the joint training.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Qin_Joint_Training_of_CVPR_2016_paper.pdf",
        "aff": "Grad. School at Shenzhen, Tsinghua University+Dept. of Automation, Tsinghua University; Dept. of Computer Science and Technology, Tsinghua University+SenseTime; Grad. School at Shenzhen, Tsinghua University+Dept. of Automation, Tsinghua University; Dept. of Computer Science and Technology, Tsinghua University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1265273,
        "gs_citation": 280,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7322536854821290801&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "mails.tsinghua.edu.cn;outlook.com;sz.tsinghua.edu.cn;tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;outlook.com;sz.tsinghua.edu.cn;tsinghua.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Qin_Joint_Training_of_CVPR_2016_paper.html",
        "aff_unique_index": "0+0;0+1;0+0;0",
        "aff_unique_norm": "Tsinghua University;SenseTime",
        "aff_unique_dep": "Graduate School;",
        "aff_unique_url": "http://www.tsinghua.edu.cn;https://www.sensetime.com",
        "aff_unique_abbr": "THU;SenseTime",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Shenzhen;",
        "aff_country_unique_index": "0+0;0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Joint Unsupervised Deformable Spatio-Temporal Alignment of Sequences",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "38",
        "author_site": "Lazaros Zafeiriou, Epameinondas Antonakos, Stefanos Zafeiriou, Maja Pantic",
        "author": "Lazaros Zafeiriou; Epameinondas Antonakos; Stefanos Zafeiriou; Maja Pantic",
        "abstract": "Typically, the problems of spatial and temporal alignment of sequences are considered disjoint. That is, in order to align two sequences, a methodology that (non)-rigidly aligns the images is first applied, followed by temporal alignment of the obtained aligned images. In this paper, we propose the first, to the best of our knowledge, methodology that can jointly spatio-temporally align two sequences, which display highly deformable texture-varying objects. We show that by treating the problems of deformable spatial and temporal alignment jointly, we achieve better results than considering the problems independent. Furthermore, we show that deformable spatio-temporal alignment of faces can be performed in an unsupervised manner (i.e., without employing face trackers or building person-specific deformable models).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zafeiriou_Joint_Unsupervised_Deformable_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zafeiriou_Joint_Unsupervised_Deformable_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1004715,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4593630380715679205&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zafeiriou_Joint_Unsupervised_Deformable_CVPR_2016_paper.html"
    },
    {
        "title": "Joint Unsupervised Learning of Deep Representations and Image Clusters",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "64",
        "author_site": "Jianwei Yang, Devi Parikh, Dhruv Batra",
        "author": "Jianwei Yang; Devi Parikh; Dhruv Batra",
        "abstract": "In this paper, we propose a recurrent framework for joint unsupervised learning of deep representations and image clusters. In our framework, successive operations in a clustering algorithm are expressed as steps in a recurrent process, stacked on top of representations output by a Convolutional Neural Network (CNN). During training, image clusters and representations are updated jointly: image clustering is conducted in the forward pass, while representation learning in the backward pass. Our key idea behind this framework is that good representations are beneficial to image clustering and clustering results provide supervisory signals to representation learning. By integrating two processes into a single model with a unified weighted triplet loss function and optimizing it end-to-end, we can obtain not only more powerful representations, but also more precise image clusters. Extensive experiments show that our method outperforms the state-of-the-art on image clustering across a variety of image datasets. Moreover, the learned representations generalize well when transferred to other tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_Joint_Unsupervised_Learning_CVPR_2016_paper.pdf",
        "aff": "Virginia Tech; Virginia Tech; Virginia Tech",
        "project": "",
        "github": "https://github.com/jwyang/joint-unsupervised-learning",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Yang_Joint_Unsupervised_Learning_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 844534,
        "gs_citation": 1078,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1604679329796491368&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "vt.edu;vt.edu;vt.edu",
        "email": "vt.edu;vt.edu;vt.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_Joint_Unsupervised_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Virginia Tech",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.vt.edu",
        "aff_unique_abbr": "VT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Jointly Modeling Embedding and Translation to Bridge Video and Language",
        "session": "Image & Video Captioning and Descriptions",
        "status": "Oral",
        "track": "main",
        "pid": "5",
        "author_site": "Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui",
        "author": "Yingwei Pan; Tao Mei; Ting Yao; Houqiang Li; Yong Rui",
        "abstract": "Automatically describing video content with natural language is a fundamental challenge of computer vision. Recurrent Neural Networks (RNNs), which models sequence dynamics, has attracted increasing attention on visual interpretation. However, most existing approaches generate a word locally with the given previous words and the visual content, while the relationship between sentence semantics and visual content is not holistically exploited. As a result, the generated sentences may be contextually correct but the semantics (e.g., subjects, verbs or objects) are not true.  This paper presents a novel unified framework, named Long Short-Term Memory with visual-semantic Embedding (LSTM-E), which can simultaneously explore the learning of LSTM and visual-semantic embedding. The former aims to locally maximize the probability of generating the next word given previous words and visual content, while the latter is to create a visual-semantic embedding space for enforcing the relationship between the semantics of the entire sentence and visual content. The experiments on YouTube2Text dataset show that our proposed LSTM-E achieves to-date the best published performance in generating natural sentences: 45.3% and 31.0% in terms of BLEU@4 and METEOR, respectively. Superior performances are also reported on two movie description datasets (M-VAD and MPII-MD). In addition, we demonstrate that LSTM-E outperforms several state-of-the-art techniques in predicting Subject-Verb-Object (SVO) triplets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.pdf",
        "aff": "University of Science and Technology of China, Hefei, China+Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; University of Science and Technology of China, Hefei, China; Microsoft Research, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1564895,
        "gs_citation": 716,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6864224843988769888&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "gmail.com;microsoft.com;microsoft.com;ustc.edu.cn;microsoft.com",
        "email": "gmail.com;microsoft.com;microsoft.com;ustc.edu.cn;microsoft.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;1;0;1",
        "aff_unique_norm": "University of Science and Technology of China;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "http://www.ustc.edu.cn;https://www.microsoft.com/en-us/research/group/microsoft-research-asia",
        "aff_unique_abbr": "USTC;MSR",
        "aff_campus_unique_index": "0+1;1;1;0;1",
        "aff_campus_unique": "Hefei;Beijing",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Just Look at the Image: Viewpoint-Specific Surface Normal Prediction for Improved Multi-View Reconstruction",
        "session": "3D Reconstruction",
        "status": "Spotlight",
        "track": "main",
        "pid": "20",
        "author_site": "Silvano Galliani, Konrad Schindler",
        "author": "Silvano Galliani; Konrad Schindler",
        "abstract": "We present a multi-view reconstruction method that combines conventional multi-view stereo (MVS) with appearance-based normal prediction, to obtain dense and accurate 3D surface models. Reliable surface normals reconstructed from multi-view correspondence serve as training data for a convolutional neural network (CNN), which predicts continuous normal vectors from raw image patches. By training from known points in the same image,  the prediction is specifically tailored to the materials and lighting conditions of the particular scene, as well as to the precise camera viewpoint. It is therefore a lot easier to learn than generic single-view normal estimation. The estimated normal maps, together with the known depth values  from MVS, are integrated to dense depth maps, which in turn are fused into a 3D model. Experiments on the DTU dataset show that our method delivers 3D reconstructions  with the same accuracy as MVS, but with significantly higher completeness.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Galliani_Just_Look_at_CVPR_2016_paper.pdf",
        "aff": "Photogrammetry and Remote Sensing, ETH Zurich; Photogrammetry and Remote Sensing, ETH Zurich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1941776,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8604901474133473149&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "geod.baug.ethz.ch;geod.baug.ethz.ch",
        "email": "geod.baug.ethz.ch;geod.baug.ethz.ch",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Galliani_Just_Look_at_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Photogrammetry and Remote Sensing",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Kernel Approximation via Empirical Orthogonal Decomposition for Unsupervised Feature Learning",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "72",
        "author_site": "Yusuke Mukuta, Tatsuya Harada",
        "author": "Yusuke Mukuta; Tatsuya Harada",
        "abstract": "Kernel approximation methods are important tools for various machine learning problems. There are two major methods used to approximate the kernel function: the Nystrom method and the random features method.   However, the Nystrom method requires relatively high-complexity post-processing to calculate a solution and the random features method does not provide sufficient generalization performance. In this paper, we propose a method that has good generalization performance without high-complexity postprocessing via empirical orthogonal decomposition using the probability distribution estimated from training data. We provide a bound for the approximation error of the proposed method. Our experiments show that the proposed method is better than the random features method and comparable with the Nystrom method in terms of the approximation error and classification accuracy. We also show that hierarchical feature extraction using our kernel approximation demonstrates better performance than the existing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mukuta_Kernel_Approximation_via_CVPR_2016_paper.pdf",
        "aff": "The University of Tokyo; The University of Tokyo",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Mukuta_Kernel_Approximation_via_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1680216,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14913904350705634839&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp",
        "email": "mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mukuta_Kernel_Approximation_via_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Kernel Sparse Subspace Clustering on Symmetric Positive Definite Manifolds",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "65",
        "author_site": "Ming Yin, Yi Guo, Junbin Gao, Zhaoshui He, Shengli Xie",
        "author": "Ming Yin; Yi Guo; Junbin Gao; Zhaoshui He; Shengli Xie",
        "abstract": "Sparse subspace clustering (SSC), as one of the most successful subspace clustering methods, has achieved notable clustering accuracy in computer vision tasks. However, SSC applies only to vector data in Euclidean space. As such, there is still no satisfactory approach to solve  subspace clustering by self-expressive principle for symmetric positive definite(SPD) matrices which is very useful in computer vision. In this paper, by embedding the SPD matrices into a Reproducing Kernel Hilbert Space (RKHS), a kernel subspace clustering method is constructed on the SPD manifold through an appropriate Log-Euclidean kernel, termed as kernel sparse subspace clustering on the SPD Riemannian manifold  (KSSCR). By exploiting the intrinsic Riemannian Geometry within data, KSSCR can effectively characterize the geodesic distance between SPD matrices to uncover the underlying subspace structure. Experimental results on several famous database demonstrate that the proposed method achieves better clustering results than the state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yin_Kernel_Sparse_Subspace_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6406102,
        "gs_citation": 134,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9613423698667280515&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yin_Kernel_Sparse_Subspace_CVPR_2016_paper.html"
    },
    {
        "title": "Kinematic Structure Correspondences via Hypergraph Matching",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "46",
        "author_site": "Hyung Jin Chang, Tobias Fischer, Maxime Petit, Martina Zambelli, Yiannis Demiris",
        "author": "Hyung Jin Chang; Tobias Fischer; Maxime Petit; Martina Zambelli; Yiannis Demiris",
        "abstract": "In this paper, we present a novel framework for finding the kinematic structure correspondence between two objects in videos via hypergraph matching. In contrast to prior appearance and graph alignment based matching methods which have been applied among two similar static images, the proposed method finds correspondences between two dynamic kinematic structures of heterogeneous objects in videos. Our main contributions can be summarised as follows: (i) casting the kinematic structure correspondence problem into a hypergraph matching problem, incorporating multi-order similarities with normalising weights, (ii) a structural topology similarity measure by a new topology constrained subgraph isomorphism aggregation, (iii) a kinematic correlation measure between pairwise nodes, and (iv) a combinatorial local motion similarity measure using geodesic distance on the Riemannian manifold. We demonstrate the robustness and accuracy of our method through a number of experiments on complex articulated synthetic and real data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chang_Kinematic_Structure_Correspondences_CVPR_2016_paper.pdf",
        "aff": "Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom; Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom; Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom; Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom; Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Chang_Kinematic_Structure_Correspondences_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 16147447,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8118268051416163064&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chang_Kinematic_Structure_Correspondences_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Imperial College London",
        "aff_unique_dep": "Department of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.imperial.ac.uk",
        "aff_unique_abbr": "Imperial College",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "LOMo: Latent Ordinal Model for Facial Analysis in Videos",
        "session": "People and Faces",
        "status": "Spotlight",
        "track": "main",
        "pid": "31",
        "author_site": "Karan Sikka, Gaurav Sharma, Marian Bartlett",
        "author": "Karan Sikka; Gaurav Sharma; Marian Bartlett",
        "abstract": "We study the problem of facial analysis in videos. Our first contribution is a novel weakly supervised learning method that models the video event (pain, expression etc.) as a sequence of automatically mined, discriminative sub-events (eg. neutral face, raising brows, contracting lips). The proposed model is inspired by the recent works on Multiple Instance Learning and latent SVM/HCRF- it extends such frameworks to model the ordinal or temporal aspect in the videos, approximately. We show consistent improvements over relevant competitive baselines on four challenging and publicly available video based facial analysis datasets for prediction of expression, clinical pain and intent in dyadic conversations. In combination with complimentary features, we report state-of-the-art results on these datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sikka_LOMo_Latent_Ordinal_CVPR_2016_paper.pdf",
        "aff": "UCSD, USA+Machine Perception Lab, University of California San Diego; MPI for Informatics, Germany+IIT Kanpur, India+CSE, Indian Institute of Technology Kanpur; UCSD, USA+Machine Perception Lab, University of California San Diego",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 962752,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2236325541024540673&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "; ; ",
        "email": "; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sikka_LOMo_Latent_Ordinal_CVPR_2016_paper.html",
        "aff_unique_index": "0+0;1+2+2;0+0",
        "aff_unique_norm": "University of California, San Diego;Max Planck Institute for Informatics;Indian Institute of Technology Kanpur",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://ucsd.edu;https://www.mpi-inf.mpg.de;https://www.iitk.ac.in",
        "aff_unique_abbr": "UCSD;MPII;IITK",
        "aff_campus_unique_index": "0+1;3;0+1",
        "aff_campus_unique": "La Jolla;San Diego;;Kanpur",
        "aff_country_unique_index": "0+0;1+2+2;0+0",
        "aff_country_unique": "United States;Germany;India"
    },
    {
        "title": "Laplacian Patch-Based Image Synthesis",
        "session": "Image Enhancement, Restoration, and Texture",
        "status": "Poster",
        "track": "main",
        "pid": "50",
        "author_site": "Joo Ho Lee, Inchang Choi, Min H. Kim",
        "author": "Joo Ho Lee; Inchang Choi; Min H. Kim",
        "abstract": "Patch-based image synthesis has been enriched with global optimization on the image pyramid. Successively, the gradient-based synthesis has improved structural coherence and details. However, the gradient operator is directional and inconsistent and requires computing multiple operators. It also introduces a significantly heavy computational burden to solve the Poisson equation that often accompanies artifacts in non-integrable gradient fields. In this paper, we propose a patch-based synthesis using a Laplacian pyramid to improve searching correspondence with enhanced awareness of edge structures. Contrary to the gradient operators, the Laplacian pyramid has the advantage of being isotropic in detecting changes to provide more consistent performance in decomposing the base structure and the detailed localization. Furthermore, it does not require heavy computation as it employs approximation by the differences of Gaussians. We examine the potentials of the Laplacian pyramid for enhanced edge-aware correspondence search. We demonstrate the effectiveness of the Laplacian-based approach over the state-of-the-art patch-based image synthesis methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lee_Laplacian_Patch-Based_Image_CVPR_2016_paper.pdf",
        "aff": "Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST)",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Lee_Laplacian_Patch-Based_Image_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2443969,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17316200502395984559&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "vclab.kaist.ac.kr;vclab.kaist.ac.kr;vclab.kaist.ac.kr",
        "email": "vclab.kaist.ac.kr;vclab.kaist.ac.kr;vclab.kaist.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lee_Laplacian_Patch-Based_Image_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Large Scale Hard Sample Mining With Monte Carlo Tree Search",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "62",
        "author_site": "Olivier Can\u00e9vet, Fran\u00e7ois Fleuret",
        "author": "Olivier Canevet; Francois Fleuret",
        "abstract": "We investigate an efficient strategy to collect false positives from very large training sets in the context of object detection. Our approach scales up the standard bootstrapping procedure by using a hierarchical decomposition of an image collection which reflects the statistical regularity of the detector's responses.  Based on that decomposition, our procedure uses a Monte Carlo Tree Search to prioritize the sampling toward sub-families of images which have been observed to be rich in false positives, while maintaining a fraction of the sampling toward unexplored sub-families of images. The resulting procedure increases substantially the proportion of false positive samples among the visited ones compared to a naive uniform sampling.  We apply experimentally this new procedure to face detection with a collection of 100,000 background images and to pedestrian detection with 32,000 images. We show that for two standard detectors, the proposed strategy cuts the number of images to visit by half to obtain the same amount of false positives and the same final performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Canevet_Large_Scale_Hard_CVPR_2016_paper.pdf",
        "aff": "Idiap Research Institut, Switzerland + \u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne (EPFL), Switzerland; Idiap Research Institut, Switzerland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Canevet_Large_Scale_Hard_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 771923,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7980498790904512797&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "idiap.ch;idiap.ch",
        "email": "idiap.ch;idiap.ch",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Canevet_Large_Scale_Hard_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Idiap Research Institute;EPFL",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.idiap.ch;https://www.epfl.ch",
        "aff_unique_abbr": "Idiap;EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Large Scale Semi-Supervised Object Detection Using Visual and Semantic Knowledge Transfer",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "67",
        "author_site": "Yuxing Tang, Josiah Wang, Boyang Gao, Emmanuel Dellandr\u00e9a, Robert Gaizauskas, Liming Chen",
        "author": "Yuxing Tang; Josiah Wang; Boyang Gao; Emmanuel Dellandrea; Robert Gaizauskas; Liming Chen",
        "abstract": "Deep CNN-based object detection systems have achieved remarkable success on several large-scale object detection benchmarks. However, training such detectors requires a large number of labeled bounding boxes, which are more difficult to obtain than image-level annotations. Previous work addresses this issue by transforming image-level classifiers into object detectors. This is done by modeling the differences between the two on categories with both image-level and bounding box annotations, and transferring this information to convert classifiers to detectors for categories without bounding box annotations. We improve this previous work by incorporating knowledge about object similarities from visual and semantic domains during the transfer process. The intuition behind our proposed method is that visually and semantically similar categories should exhibit more common transferable properties than dissimilar categories, e.g. a better detector would result by transforming the differences between a dog classifier and a dog detector onto the cat class, than would by transforming from the violin class. Experimental results on the challenging ILSVRC2013 detection dataset demonstrate that each of our proposed object similarity based knowledge transfer methods outperforms the baseline methods. We found strong evidence that visual similarity and semantic relatedness are complementary for the task, and when combined notably improve detection, achieving state-of-the-art detection performance in a semi-supervised setting.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tang_Large_Scale_Semi-Supervised_CVPR_2016_paper.pdf",
        "aff": "Ecole Centrale de Lyon, France; University of Sheffield, UK; Istituto Italiano di Tecnologia, Italy; Ecole Centrale de Lyon, France; University of Sheffield, UK; Ecole Centrale de Lyon, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3124929,
        "gs_citation": 173,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=355755664699112487&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ec-lyon.fr;sheffield.ac.uk;iit.it;ec-lyon.fr;sheffield.ac.uk;ec-lyon.fr",
        "email": "ec-lyon.fr;sheffield.ac.uk;iit.it;ec-lyon.fr;sheffield.ac.uk;ec-lyon.fr",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tang_Large_Scale_Semi-Supervised_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;0;1;0",
        "aff_unique_norm": "Ecole Centrale de Lyon;University of Sheffield;Istituto Italiano di Tecnologia",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ec-lyon.fr;https://www.sheffield.ac.uk;https://www.iit.it",
        "aff_unique_abbr": "ECL;Sheffield;IIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0;1;0",
        "aff_country_unique": "France;United Kingdom;Italy"
    },
    {
        "title": "Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "43",
        "author_site": "Amin Jourabloo, Xiaoming Liu",
        "author": "Amin Jourabloo; Xiaoming Liu",
        "abstract": "Large-pose face alignment is a very challenging problem in computer vision, which is used as a prerequisite for many important vision tasks, e.g, face recognition and 3D face reconstruction. Recently, there have been a few attempts to solve this problem, but still more research is needed to achieve highly accurate results. In this paper, we propose a face alignment method for large-pose face images, by combining the powerful cascaded CNN regressor method and 3DMM. We formulate the face alignment as a 3DMM fitting problem, where the camera projection matrix and 3D shape parameters are estimated by a cascade of CNN-based regressors. The dense 3D shape allows us to design pose-invariant appearance features for effective CNN learning. Extensive experiments are conducted on the challenging databases (AFLW and AFW), with comparison to the state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Jourabloo_Large-Pose_Face_Alignment_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science and Engineering, Michigan State University, East Lansing MI 48824; Department of Computer Science and Engineering, Michigan State University, East Lansing MI 48824",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 840022,
        "gs_citation": 410,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14886621971099119460&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "msu.edu;msu.edu",
        "email": "msu.edu;msu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Jourabloo_Large-Pose_Face_Alignment_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Michigan State University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.msu.edu",
        "aff_unique_abbr": "MSU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "East Lansing",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Large-Scale Location Recognition and the Geometric Burstiness Problem",
        "session": "Recognition Beyond Objects",
        "status": "Spotlight",
        "track": "main",
        "pid": "9",
        "author_site": "Torsten Sattler, Michal Havlena, Konrad Schindler, Marc Pollefeys",
        "author": "Torsten Sattler; Michal Havlena; Konrad Schindler; Marc Pollefeys",
        "abstract": "Visual location recognition is the task of determining the place depicted in a query image from a given database of geo-tagged images. Location recognition is often cast as an image retrieval problem and recent research has almost exclusively focused on improving the chance that a relevant database image is ranked high enough after retrieval. The implicit assumption is that the number of inliers found by spatial verification can be used to distinguish between a related and an unrelated database photo with high precision. In this paper, we show that this assumption does not hold for large datasets due to the appearance of geometric bursts, i.e., sets of visual elements appearing in similar geometric configurations in unrelated database photos. We propose algorithms for detecting and handling geometric bursts. Although conceptually simple, using the proposed weighting schemes dramatically improves the recall that can be achieved when high precision is required compared to the standard re-ranking based on the inlier count. Our approach is easy to implement and can easily be integrated into existing location recognition systems",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sattler_Large-Scale_Location_Recognition_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, ETH Z\u00fcrich, Switzerland; Computer Vision Laboratory, ETH Z\u00fcrich, Switzerland; Institute of Geodesy and Photogrammetry, ETH Z\u00fcrich, Switzerland; Department of Computer Science, ETH Z\u00fcrich, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1586902,
        "gs_citation": 191,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17927923785525346342&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "inf.ethz.ch;vision.ee.ethz.ch;geod.baug.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;vision.ee.ethz.ch;geod.baug.ethz.ch;inf.ethz.ch",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sattler_Large-Scale_Location_Recognition_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Z\u00fcrich",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Large-Scale Semantic 3D Reconstruction: An Adaptive Multi-Resolution Model for Multi-Class Volumetric Labeling",
        "session": "Semantic Segmentation",
        "status": "Oral",
        "track": "main",
        "pid": "16",
        "author_site": "Maro\u0161 Bl\u00e1ha, Christoph Vogel, Audrey Richard, Jan D. Wegner, Thomas Pock, Konrad Schindler",
        "author": "Maros Blaha; Christoph Vogel; Audrey Richard; Jan D. Wegner; Thomas Pock; Konrad Schindler",
        "abstract": "We propose an adaptive multi-resolution formulation of semantic 3D reconstruction. Given a set of images of a scene, semantic 3D reconstruction aims to densely reconstruct both the 3D shape of the scene and a segmentation into semantic object classes. Jointly reasoning about shape and class allows one to take into account class-specific shape priors (e.g., building walls should be smooth and vertical, and vice versa smooth, vertical surfaces are likely to be building walls), leading to improved reconstruction results. So far, semantic 3D reconstruction methods have been limited to small scenes and low resolution, because of their large memory footprint and computational cost. To scale them up to large scenes, we propose a hierarchical scheme which refines the reconstruction only in regions that are likely to contain a surface, exploiting the fact that both high spatial resolution and high numerical precision are only required in those regions. Our scheme amounts to solving a sequence of convex optimizations while progressively removing constraints, in such a way that the energy, in each iteration, is the tightest possible approximation of the underlying energy at full resolution. In our experiments the method saves up to 98% memory and 95% computation time, without any loss of accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Blaha_Large-Scale_Semantic_3D_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Blaha_Large-Scale_Semantic_3D_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1130190,
        "gs_citation": 119,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11958134952792638665&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Blaha_Large-Scale_Semantic_3D_CVPR_2016_paper.html"
    },
    {
        "title": "Latent Embeddings for Zero-Shot Classification",
        "session": "Language and Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "8",
        "author_site": "Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, Bernt Schiele",
        "author": "Yongqin Xian; Zeynep Akata; Gaurav Sharma; Quynh Nguyen; Matthias Hein; Bernt Schiele",
        "abstract": "We present a novel latent embedding model for learning a compatibility function between image and class embeddings, in the context of zero-shot classification. The proposed method augments the state-of-the-art bilinear compatibility model by incorporating latent variables. Instead of learning a single bilinear map, it learns a collection of maps with the selection, of which map to use, being a latent variable for the current image-class pair. We train the model with a ranking based objective function which penalizes incorrect rankings of the true class for a given image. We empirically demonstrate that our model improves the state-of-the-art for various class embeddings consistently on three challenging publicly available datasets for the zero-shot setting. Moreover, our method leads to visually highly interpretable results with clear clusters of different fine-grained object properties that correspond to different latent variable maps.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xian_Latent_Embeddings_for_CVPR_2016_paper.pdf",
        "aff": "MPI for Informatics; MPI for Informatics; MPI for Informatics+IIT Kanpur; Saarland University; Saarland University; MPI for Informatics",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2162320,
        "gs_citation": 888,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2270096778718786860&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xian_Latent_Embeddings_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0+1;2;2;0",
        "aff_unique_norm": "Max Planck Institute for Informatics;Indian Institute of Technology Kanpur;Saarland University",
        "aff_unique_dep": "Informatics;;",
        "aff_unique_url": "https://www.mpi-inf.mpg.de;https://www.iitk.ac.in;https://www.uni-saarland.de",
        "aff_unique_abbr": "MPII;IITK;UdS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Kanpur",
        "aff_country_unique_index": "0;0;0+1;0;0;0",
        "aff_country_unique": "Germany;India"
    },
    {
        "title": "Latent Factor Guided Convolutional Neural Networks for Age-Invariant Face Recognition",
        "session": "Face Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "37",
        "author_site": "Yandong Wen, Zhifeng Li, Yu Qiao",
        "author": "Yandong Wen; Zhifeng Li; Yu Qiao",
        "abstract": "While considerable progresses have been made on face recognition, age-invariant face recognition (AIFR) still remains a major challenge in real world applications of face recognition systems. The major difficulty of AIFR arises from the fact that the facial appearance is subject to significant intra-personal changes caused by the aging process over time. In order to address this problem, we propose a novel deep face recognition framework to learn the age-invariant deep face features through a carefully designed CNN model. To the best of our knowledge, this is the first attempt to show the effectiveness of deep CNNs in advancing the state-of-the-art of AIFR. Extensive experiments are conducted on several public domain face aging datasets (MORPH Album2, FGNET, and CACD-VS) to demonstrate the effectiveness of the proposed model over the state-of-the-art. We also verify the excellent generalization of our new model on the famous LFW dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wen_Latent_Factor_Guided_CVPR_2016_paper.pdf",
        "aff": "School of Electronic and Information Engineering, South China University of Technology+Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1867108,
        "gs_citation": 209,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7269259464751818336&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "siat.ac.cn;siat.ac.cn;siat.ac.cn",
        "email": "siat.ac.cn;siat.ac.cn;siat.ac.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wen_Latent_Factor_Guided_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;1",
        "aff_unique_norm": "South China University of Technology;Shenzhen Institute of Advanced Technology",
        "aff_unique_dep": "School of Electronic and Information Engineering;Shenzhen Key Lab of Comp. Vis. & Pat. Rec.",
        "aff_unique_url": "https://www.scut.edu.cn;http://www.siat.ac.cn",
        "aff_unique_abbr": "SCUT;SIAT",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Latent Variable Graphical Model Selection Using Harmonic Analysis: Applications to the Human Connectome Project (HCP)",
        "session": "Computational Photography and Biomedical Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "20",
        "author_site": "Won Hwa Kim, Hyunwoo J. Kim, Nagesh Adluru, Vikas Singh",
        "author": "Won Hwa Kim; Hyunwoo J. Kim; Nagesh Adluru; Vikas Singh",
        "abstract": "A major goal of imaging studies such as the (ongoing) Human Connectome Project (HCP) is to characterize the structural network map of the human brain and identify its associations with covariates such as genotype, risk factors, and so on that correspond to an individual. But the set of image derived measures and the set of covariates are both large, so we must first estimate a 'parsimonious' set of relations between the measurements. For instance, a Gaussian graphical model will show conditional independences between the random variables, which can then be used to setup specific hypothesis based analyses downstream. But most such data involve a large list of 'latent' variables that remain unobserved, yet affect the 'observed' variables sustantially. Accounting for such latent variables falls outside the scope of standard inverse covariance matrix estimation, and is tackled via highly specialized optimization methods. This paper offers a unique harmonic analysis view of this problem. By casting the estimation of the precision matrix in terms of a composition of low-frequency latent variables and high-frequency sparse terms, we show how the problem can be formulated using a new wavelet-type expansion in non-Euclidean spaces. Our formalization poses the estimation problem entirely in the frequency space and shows how it can be solved by a simple sub-gradient scheme (involving a single variable). We provide a compelling set of scientific results on   500 scans from the recently released HCP data where our algorithm recovers highly interpretable and sparse conditional dependencies between brain connectivity pathways and well-known covariates.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kim_Latent_Variable_Graphical_CVPR_2016_paper.pdf",
        "aff": "Dept. of Computer Sciences, University of Wisconsin, Madison, WI, U.S.A.+Dept. of Biostatistics & Med. Informatics, University of Wisconsin, Madison, WI, U.S.A.; Dept. of Computer Sciences, University of Wisconsin, Madison, WI, U.S.A.+Dept. of Biostatistics & Med. Informatics, University of Wisconsin, Madison, WI, U.S.A.; Dept. of Biostatistics & Med. Informatics, University of Wisconsin, Madison, WI, U.S.A.; Waisman Center, Madison, WI, U.S.A.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1206152,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11170347269265611809&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kim_Latent_Variable_Graphical_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1;1;2",
        "aff_unique_norm": "University of Wisconsin-Madison;University of Wisconsin;Waisman Center",
        "aff_unique_dep": "Department of Computer Sciences;Dept. of Biostatistics & Med. Informatics;",
        "aff_unique_url": "https://www.wisc.edu;https://www.wisc.edu;",
        "aff_unique_abbr": "UW-Madison;UW;",
        "aff_campus_unique_index": "0+0;0+0;0;0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Layered Scene Decomposition via the Occlusion-CRF",
        "session": "Segmentation and Contour Detection",
        "status": "Spotlight",
        "track": "main",
        "pid": "18",
        "author_site": "Chen Liu, Pushmeet Kohli, Yasutaka Furukawa",
        "author": "Chen Liu; Pushmeet Kohli; Yasutaka Furukawa",
        "abstract": "This paper addresses the challenging problem of perceiving the hidden or occluded geometry of the scene depicted in any given RGBD image. Unlike other image labeling problems such as image segmentation where each pixel needs to be assigned a single label, layered decomposition requires us to assign multiple labels to pixels. We propose a novel \"Occlusion-CRF\" model that allows for the integration of sophisticated priors to regularize the solution space and enables the automatic inference of the layer decomposition. We use a generalization of the Fusion Move algorithm to perform Maximum a Posterior (MAP) inference on the model that can handle the large label sets needed to represent multiple surface assignments to each pixel. We have evaluated the proposed model and the inference algorithm on many RGBD images of cluttered indoor scenes. Our experiments show that not only is our model able to explain occlusions but it also enables automatic inpainting of occluded/invisible surfaces.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Layered_Scene_Decomposition_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Liu_Layered_Scene_Decomposition_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1297454,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10110252127834386979&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liu_Layered_Scene_Decomposition_CVPR_2016_paper.html"
    },
    {
        "title": "Learned Binary Spectral Shape Descriptor for 3D Shape Correspondence",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "30",
        "author_site": "Jin Xie, Meng Wang, Yi Fang",
        "author": "Jin Xie; Meng Wang; Yi Fang",
        "abstract": "Dense 3D shape correspondence is an important problem in computer vision and computer graphics. Recently, the local shape descriptor based 3D shape correspondence approaches have been widely studied, where the local shape descriptor is a real-valued vector to characterize the geometrical structure of the shape. Different from these real-valued local shape descriptors, in this paper, we propose to learn a novel binary spectral shape descriptor with the deep neural network for 3D shape correspondence. The binary spectral shape descriptor can require less storage space and enable fast matching. First, based on the eigenvectors of the Laplace-Beltrami operator, we construct a neural network to form a nonlinear spectral representation to characterize the shape. Then, for the defined positive and negative points on the shapes, we train the constructed neural network by minimizing the errors between the outputs and their corresponding binary descriptors, minimizing the variations of the outputs of the positive points and maximizing the variations of the outputs of the negative points, simultaneously. Finally, we binarize the output of the neural network to form the binary spectral shape descriptor for shape correspondence. The proposed binary spectral shape descriptor is evaluated on the  SCAPE and TOSCA 3D shape datasets for shape correspondence. The experimental results demonstrate the effectiveness of the proposed binary shape descriptor for the shape correspondence task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_Learned_Binary_Spectral_CVPR_2016_paper.pdf",
        "aff": "NYU Multimedia and Visual Computing Lab, UAE + Department of Electrical and Computer Engineering, New York University Abu Dhabi; NYU Multimedia and Visual Computing Lab, UAE + Department of Electrical and Computer Engineering, New York University Abu Dhabi; NYU Multimedia and Visual Computing Lab, UAE + Department of Electrical and Computer Engineering, NYU Tandon School of Engineering",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1158972,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7256736433216954294&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "nyu.edu;nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu;nyu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xie_Learned_Binary_Spectral_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1;0+2",
        "aff_unique_norm": "New York University;New York University Abu Dhabi;New York University Tandon School of Engineering",
        "aff_unique_dep": "Multimedia and Visual Computing Lab;Department of Electrical and Computer Engineering;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.nyu.edu;https://nyuad.nyu.edu;https://engineering.nyu.edu",
        "aff_unique_abbr": "NYU;NYU Abu Dhabi;NYU Tandon",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Abu Dhabi;Brooklyn",
        "aff_country_unique_index": "0+0;0+0;0+1",
        "aff_country_unique": "United Arab Emirates;United States"
    },
    {
        "title": "Learning Action Maps of Large Environments via First-Person Vision",
        "session": "Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "62",
        "author_site": "Nicholas Rhinehart, Kris M. Kitani",
        "author": "Nicholas Rhinehart; Kris M. Kitani",
        "abstract": "When people observe and interact with physical spaces, they are able to associate functionality to regions in the environment. Our goal is to automate functional understanding of large spaces by leveraging activity demonstrations recorded from an ego-centric viewpoint. The method we describe enables functionality estimation in both large scenes where people have behaved, as well as novel scenes where no behaviors are available. Our method learns and predicts \"Action Maps\", which encode the ability for a user to perform activities at various locations. With the usage of an egocentric camera to observe demonstrations, our method scales with the size of the scene without the need for mounting multiple static surveillance cameras, and is well-suited to the task of observing activities up-close. We demonstrate that by capturing appearance-based attributes of the environment and associating these attributes with activity demonstrations, our mathematical framework allows for the prediction of Action Maps in new environments. Additionally, we take a preliminary look at the breadth of applicability of Action Maps by demonstrating a proof-of-concept application in which they are used in concert with activity detections to perform localization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Rhinehart_Learning_Action_Maps_CVPR_2016_paper.pdf",
        "aff": "The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Rhinehart_Learning_Action_Maps_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2023363,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13040625835556994915&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Rhinehart_Learning_Action_Maps_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "The Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Activity Progression in LSTMs for Activity Detection and Early Detection",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "48",
        "author_site": "Shugao Ma, Leonid Sigal, Stan Sclaroff",
        "author": "Shugao Ma; Leonid Sigal; Stan Sclaroff",
        "abstract": "In this work we improve training of temporal deep models to better learn activity progression for activity detection and early detection. Conventionally, when training a Recurrent Neural Network, specifically a Long Short Term Memory (LSTM) model, the training loss only considers classification error. However, we argue that the detection score of the correct activity category or the detection score margin between the correct and incorrect categories should be monotonically non-decreasing as the model observes more of the activity. We design novel ranking losses that directly penalize the model on violation of such monotonicities, which are used together with classification loss in training of LSTM models. Evaluation on ActivityNet shows significant benefits of the proposed ranking losses in both activity detection and early detection tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ma_Learning_Activity_Progression_CVPR_2016_paper.pdf",
        "aff": "Boston University; Disney Research; Boston University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2006084,
        "gs_citation": 502,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14914349505732598083&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "bu.edu;disneyresearch.com;bu.edu",
        "email": "bu.edu;disneyresearch.com;bu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ma_Learning_Activity_Progression_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Boston University;Disney Research",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bu.edu;https://research.disney.com",
        "aff_unique_abbr": "BU;Disney Research",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Aligned Cross-Modal Representations From Weakly Aligned Data",
        "session": "Scene and Image Classification",
        "status": "Poster",
        "track": "main",
        "pid": "73",
        "author_site": "Llu\u00eds Castrej\u00f3n, Yusuf Aytar, Carl Vondrick, Hamed Pirsiavash, Antonio Torralba",
        "author": "Lluis Castrejon; Yusuf Aytar; Carl Vondrick; Hamed Pirsiavash; Antonio Torralba",
        "abstract": "People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize cross-modal scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Castrejon_Learning_Aligned_Cross-Modal_CVPR_2016_paper.pdf",
        "aff": "University of Toronto; MIT CSAIL; MIT CSAIL; University of Maryland - Baltimore County; MIT CSAIL",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2842425,
        "gs_citation": 204,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13045745172112329002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.toronto.edu;csail.mit.edu;mit.edu;umbc.edu;csail.mit.edu",
        "email": "cs.toronto.edu;csail.mit.edu;mit.edu;umbc.edu;csail.mit.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Castrejon_Learning_Aligned_Cross-Modal_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "University of Toronto;Massachusetts Institute of Technology;University of Maryland, Baltimore County",
        "aff_unique_dep": ";Computer Science and Artificial Intelligence Laboratory;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.csail.mit.edu;https://www.umbc.edu",
        "aff_unique_abbr": "U of T;MIT CSAIL;UMBC",
        "aff_campus_unique_index": "1;1;2;1",
        "aff_campus_unique": ";Cambridge;Baltimore County",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Learning Attributes Equals Multi-Source Domain Generalization",
        "session": "Language and Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "10",
        "author_site": "Chuang Gan, Tianbao Yang, Boqing Gong",
        "author": "Chuang Gan; Tianbao Yang; Boqing Gong",
        "abstract": "Attributes possess appealing properties and benefit many computer vision problems, such as object recognition, learning with humans in the loop, and  image retrieval. Whereas the existing work mainly pursues utilizing attributes for various computer vision problems, we contend that the most basic problem---how to accurately and robustly detect attributes from images---has been left under explored. Especially, the existing work rarely explicitly tackles the need that attribute detectors should generalize well across different categories, including those previously unseen. Noting that this is analogous to the objective of multi-source domain generalization, if we treat each category as a domain, we provide a novel perspective to attribute detection and propose to gear the techniques in multi-source domain generalization  for the purpose of learning cross-category generalizable attribute detectors. We validate our understanding and approach with extensive experiments on four challenging datasets and three different problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gan_Learning_Attributes_Equals_CVPR_2016_paper.pdf",
        "aff": "IIIS, Tsinghua University; University of Iowa; CRCV, U. of Central Florida",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 935552,
        "gs_citation": 256,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5955772588489505470&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "gmail.com;uiowa.edu;crcv.ucf.edu",
        "email": "gmail.com;uiowa.edu;crcv.ucf.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gan_Learning_Attributes_Equals_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Tsinghua University;University of Iowa;University of Central Florida",
        "aff_unique_dep": "Institute for Interdisciplinary Information Sciences;;CRCV",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.uiowa.edu;https://www.ucf.edu",
        "aff_unique_abbr": "THU;UIowa;UCF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Learning Compact Binary Descriptors With Unsupervised Deep Neural Networks",
        "session": "Feature Matching and Indexing",
        "status": "Poster",
        "track": "main",
        "pid": "45",
        "author_site": "Kevin Lin, Jiwen Lu, Chu-Song Chen, Jie Zhou",
        "author": "Kevin Lin; Jiwen Lu; Chu-Song Chen; Jie Zhou",
        "abstract": "In this paper, we propose a new unsupervised deep learning approach called DeepBit to learn compact binary descriptor for efficient visual object matching. Unlike most existing binary descriptors which were designed with random projections or linear hash functions, we develop a deep neural network to learn binary descriptors in a unsupervised manner. We enforce three criterions on binary codes which are learned at the top layer of our network: 1) minimal loss quantization, 2) evenly distributed codes and 3) uncorrelated bits. Then, we learn the parameters of the networks with a back-propagation technique. Experimental results on three different visual analysis tasks including image matching, image retrieval, and object recognition clearly demonstrate the effectiveness of the proposed approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lin_Learning_Compact_Binary_CVPR_2016_paper.pdf",
        "aff": "Institute of Information Science, Academia Sinica, Taipei, Taiwan; Department of Automation, Tsinghua University, Beijing, China; Institute of Information Science, Academia Sinica, Taipei, Taiwan; Department of Automation, Tsinghua University, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2161017,
        "gs_citation": 431,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3415716566019331345&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff_domain": "iis.sinica.edu.tw;tsinghua.edu.cn;iis.sinica.edu.tw;tsinghua.edu.cn",
        "email": "iis.sinica.edu.tw;tsinghua.edu.cn;iis.sinica.edu.tw;tsinghua.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lin_Learning_Compact_Binary_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Academia Sinica;Tsinghua University",
        "aff_unique_dep": "Institute of Information Science;Department of Automation",
        "aff_unique_url": "https://www.sinica.edu.tw;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "AS;THU",
        "aff_campus_unique_index": "0;1;0;1",
        "aff_campus_unique": "Taiwan;Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Cross-Domain Landmarks for Heterogeneous Domain Adaptation",
        "session": "Transfer Learning",
        "status": "Poster",
        "track": "main",
        "pid": "57",
        "author_site": "Yao-Hung Hubert Tsai, Yi-Ren Yeh, Yu-Chiang Frank Wang",
        "author": "Yao-Hung Hubert Tsai; Yi-Ren Yeh; Yu-Chiang Frank Wang",
        "abstract": "While domain adaptation (DA) aims to associate the learning tasks across data domains, heterogeneous domain adaptation (HDA) particularly deals with learning from cross-domain data which are of different types of features. In other words, for HDA, data from source and target domains are observed in separate feature spaces and thus exhibit distinct distributions. In this paper, we propose a novel learning algorithm of Cross-Domain Landmark Selection (CDLS) for solving the above task. With the goal of deriving a domain-invariant feature subspace for HDA, our CDLS is able to identify representative cross-domain data, including the unlabeled ones in the target domain, for performing adaptation. In addition, the adaptation capabilities of such cross-domain landmarks can be determined accordingly. This is the reason why our CDLS is able to achieve promising HDA performance when comparing to state-of-the-art HDA methods. We conduct classification experiments using data across different features, domains, and modalities. The effectiveness of our proposed method can be successfully verified.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tsai_Learning_Cross-Domain_Landmarks_CVPR_2016_paper.pdf",
        "aff": "Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan; Department of Mathematics, National Kaohsiung Normal University, Kaohsiung, Taiwan; Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Tsai_Learning_Cross-Domain_Landmarks_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4988272,
        "gs_citation": 244,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6443305003059778406&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "gmail.com;nknu.edu.tw;citi.sinica.edu.tw",
        "email": "gmail.com;nknu.edu.tw;citi.sinica.edu.tw",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tsai_Learning_Cross-Domain_Landmarks_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Academia Sinica;National Kaohsiung Normal University",
        "aff_unique_dep": "Research Center for IT Innovation;Department of Mathematics",
        "aff_unique_url": "https://www.sinica.edu.tw;https://www.nknu.edu.tw",
        "aff_unique_abbr": "AS;NKNU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Deep Feature Representations With Domain Guided Dropout for Person Re-Identification",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "52",
        "author_site": "Tong Xiao, Hongsheng Li, Wanli Ouyang, Xiaogang Wang",
        "author": "Tong Xiao; Hongsheng Li; Wanli Ouyang; Xiaogang Wang",
        "abstract": "Learning generic and robust feature representations with data from multiple domains for the same problem is of great value, especially for the problems that have multiple datasets but none of them are large enough to provide abundant data variations. In this work, we present a pipeline for learning deep feature representations from multiple domains with Convolutional Neural Networks (CNNs). When training a CNN with data from all the domains, some neurons learn representations shared across several domains, while some others are effective only for a specific one. Based on this important observation, we propose a Domain Guided Dropout algorithm to improve the feature learning procedure. Experiments show the effectiveness of our pipeline and the proposed algorithm. Our methods on the person re-identification problem outperform state-of-the-art methods on multiple datasets by large margins.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xiao_Learning_Deep_Feature_CVPR_2016_paper.pdf",
        "aff": "Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 972681,
        "gs_citation": 1196,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14693788551611879825&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xiao_Learning_Deep_Feature_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Deep Features for Discriminative Localization",
        "session": "Scene and Image Classification",
        "status": "Poster",
        "track": "main",
        "pid": "71",
        "author_site": "Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba",
        "author": "Bolei Zhou; Aditya Khosla; Agata Lapedriza; Aude Oliva; Antonio Torralba",
        "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "http://cnnlocalization.csail.mit.edu",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1447775,
        "gs_citation": 13283,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2686826201088684972&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html"
    },
    {
        "title": "Learning Deep Representation for Imbalanced Classification",
        "session": "Learning and Optimization",
        "status": "Spotlight",
        "track": "main",
        "pid": "9",
        "author_site": "Chen Huang, Yining Li, Chen Change Loy, Xiaoou Tang",
        "author": "Chen Huang; Yining Li; Chen Change Loy; Xiaoou Tang",
        "abstract": "Data in vision domain often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary classification methods based on deep convolutional neural network (CNN) typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain both inter-cluster and inter-class margins. This tighter constraint effectively reduces the class imbalance inherent in the local data neighborhood. We show that the margins can be easily deployed in standard deep learning framework through quintuplet instance sampling and the associated triple-header hinge loss. The representation learned by our approach, when combined with a simple k-nearest neighbor (kNN) algorithm, shows significant improvements over existing methods on both high- and low-level vision classification tasks that exhibit imbalanced class distribution.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Learning_Deep_Representation_CVPR_2016_paper.pdf",
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong + SenseTime Group Limited; Department of Information Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2246402,
        "gs_citation": 1349,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8603436457174735633&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Huang_Learning_Deep_Representation_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;0+2;0+2",
        "aff_unique_norm": "Chinese University of Hong Kong;SenseTime Group Limited;Chinese Academy of Sciences",
        "aff_unique_dep": "Department of Information Engineering;;Shenzhen Institutes of Advanced Technology",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.sensetime.com;http://www.siat.cas.cn",
        "aff_unique_abbr": "CUHK;SenseTime;SIAT",
        "aff_campus_unique_index": "0;0;0+2;0+2",
        "aff_campus_unique": "Hong Kong SAR;;Shenzhen",
        "aff_country_unique_index": "0+0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Learning Deep Representations of Fine-Grained Visual Descriptions",
        "session": "Language and Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "6",
        "author_site": "Scott Reed, Zeynep Akata, Honglak Lee , Bernt Schiele",
        "author": "Scott Reed; Zeynep Akata; Honglak Lee; Bernt Schiele",
        "abstract": "State-of-the-art methods for zero-shot visual recognition formulate learning as a joint embedding problem of images and side information. In these formulations the current best complement to visual features are attributes: manually-encoded vectors describing shared characteristics among categories. Despite good performance, attributes have limitations: (1) finer-grained recognition requires commensurately more attributes, and (2) attributes do not provide a natural language interface. We propose to overcome these limitations by training neural language models from scratch; i.e. without pre-training and only consuming words and characters. Our proposed models train end-to-end to align with the fine-grained and category-specific content of images. Natural language provides a flexible and compact way of encoding only the salient visual aspects for distinguishing categories. By training on raw text, our model can do inference on raw text as well, providing humans a familiar mode both for annotation and retrieval. Our model achieves strong performance on zero-shot text-based image retrieval and significantly outperforms the attribute-based state-of-the-art for zero-shot classification on the Caltech-UCSD Birds 200-2011 dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Reed_Learning_Deep_Representations_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1656690,
        "gs_citation": 1105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9792549731155959756&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Reed_Learning_Deep_Representations_CVPR_2016_paper.html"
    },
    {
        "title": "Learning Deep Structure-Preserving Image-Text Embeddings",
        "session": "Images and Language",
        "status": "Poster",
        "track": "main",
        "pid": "49",
        "author_site": "Liwei Wang, Yin Li, Svetlana Lazebnik",
        "author": "Liwei Wang; Yin Li; Svetlana Lazebnik",
        "abstract": "This paper proposes a method for learning joint embeddings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities. The network is trained using a large margin objective that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature. Extensive experiments show that our approach gains significant improvements in accuracy for image-to-text and text-to-image retrieval. Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of phrase localization on the Flickr30K Entities dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Learning_Deep_Structure-Preserving_CVPR_2016_paper.pdf",
        "aff": "University of Illinois at Urbana-Champaign; Georgia Institute of Technology; University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 736211,
        "gs_citation": 983,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7266256581917835642&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "illinois.edu;gatech.edu;illinois.edu",
        "email": "illinois.edu;gatech.edu;illinois.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Learning_Deep_Structure-Preserving_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Georgia Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.gatech.edu",
        "aff_unique_abbr": "UIUC;Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Dense Correspondence via 3D-Guided Cycle Consistency",
        "session": "Matching and Alignment",
        "status": "Oral",
        "track": "main",
        "pid": "13",
        "author_site": "Tinghui Zhou, Philipp Kr\u00e4henbuhl, Mathieu Aubry, Qixing Huang, Alexei A. Efros",
        "author": "Tinghui Zhou; Philipp Krahenbuhl; Mathieu Aubry; Qixing Huang; Alexei A. Efros",
        "abstract": "Discriminative deep learning approaches have shown impressive results for problems where human-labeled ground truth is plentiful, but what about tasks where labels are difficult or impossible to obtain? This paper tackles one such problem: establishing dense visual correspondence across different object instances. For this task, although we do not know what the ground-truth is, we know it should be consistent across instances of that category. We exploit this consistency as a supervisory signal to train a convolutional neural network to predict cross-instance correspondences between pairs of images depicting objects of the same category. For each pair of training images we find an appropriate 3D CAD model and render two synthetic views to link in with the pair, establishing a correspondence flow 4-cycle. We use ground-truth synthetic-to-synthetic correspondences, provided by the rendering engine, to train a ConvNet to predict synthetic-to-real, real-to-real and real-to-synthetic correspondences that are cycle-consistent with the ground-truth. At test time, no CAD models are required. We demonstrate that our end-to-end trained ConvNet supervised by cycle-consistency outperforms state-of-the-art pairwise matching methods in correspondence-related tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Learning_Dense_Correspondence_CVPR_2016_paper.pdf",
        "aff": "UC Berkeley; UC Berkeley; ENPC ParisTech; TTI-Chicago; UC Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1864456,
        "gs_citation": 453,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12702329419414795610&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Learning_Dense_Correspondence_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University of California, Berkeley;\u00c9cole des Ponts ParisTech;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.ponts.org;https://www.tti-chicago.org",
        "aff_unique_abbr": "UC Berkeley;ENPC;TTI",
        "aff_campus_unique_index": "0;0;1;2;0",
        "aff_campus_unique": "Berkeley;Paris;Chicago",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United States;France"
    },
    {
        "title": "Learning From the Mistakes of Others: Matching Errors in Cross-Dataset Learning",
        "session": "Statistical Methods and Transfer Learning",
        "status": "Spotlight",
        "track": "main",
        "pid": "19",
        "author_site": "Viktoriia Sharmanska, Novi Quadrianto",
        "author": "Viktoriia Sharmanska; Novi Quadrianto",
        "abstract": "Can we learn about object classes in images by looking at a collection of relevant 3D models? Or if we want to learn about human (inter-)actions in images, can we benefit from videos or abstract illustrations that show these actions? A common aspect of these settings is the availability of additional or privileged data that can be exploited at training time and that will not be available and not of interest at test time. We seek to generalize the learning with privileged information (LUPI) framework, which requires additional information to be defined per image, to the setting where additional information is a data collection about the task of interest. Our framework minimises the distribution mismatch between errors made in images and in privileged data. The proposed method is tested on four publicly available datasets: Image+ClipArt, Image+3Dobject, and Image+Video. Experimental results reveal that our new LUPI paradigm naturally addresses the cross-dataset learning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sharmanska_Learning_From_the_CVPR_2016_paper.pdf",
        "aff": "SMiLe CLiNiC, University of Sussex, Brighton, UK; SMiLe CLiNiC, University of Sussex, Brighton, UK",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Sharmanska_Learning_From_the_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2858198,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15726163826984421744&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;sussex.ac.uk",
        "email": "gmail.com;sussex.ac.uk",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sharmanska_Learning_From_the_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Sussex",
        "aff_unique_dep": "SMiLe CLiNiC",
        "aff_unique_url": "https://www.sussex.ac.uk",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Brighton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Learning Local Image Descriptors With Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions",
        "session": "Learning and Optimization",
        "status": "Spotlight",
        "track": "main",
        "pid": "10",
        "author_site": "Vijay Kumar B G, Gustavo Carneiro, Ian Reid",
        "author": "Vijay Kumar B G; Gustavo Carneiro; Ian Reid",
        "abstract": "Recent innovations in training deep convolutional neural network (ConvNet) models have motivated the design of new methods to automatically learn local image descriptors. The latest deep ConvNets proposed for this task consist of a siamese network that is trained by penalising misclassification of pairs of local image patches. Current results from machine learning show that replacing this siamese by a triplet network can improve the classification accuracy in several problems, but this has yet to be demonstrated for local image descriptor learning. Moreover, current siamese and triplet networks have been trained with stochastic gradient descent that computes the gradient from individual pairs or triplets of local image patches, which can make them prone to overfitting. In this paper, we first propose the use of triplet networks for the problem of local image descriptor learning. Furthermore, we also propose the use of a global loss that minimises the overall classification error of all patches present in the training set, which can improve the generalisation capability of the model. Using the UBC benchmark dataset for comparing local image descriptors, we show that the triplet network produces a more accurate embedding than the siamese network in terms of the UBC dataset errors. Moreover, we also demonstrate that a combination of the triplet and global losses produces the best embedding in the field, using this triplet network. Finally, we also show that the use of the central-surround siamese network trained with the global loss produces the best result of the field on the UBC dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/G_Learning_Local_Image_CVPR_2016_paper.pdf",
        "aff": "The University of Adelaide; The University of Adelaide; The University of Adelaide",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/G_Learning_Local_Image_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 11878677,
        "gs_citation": 394,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16664058184834200955&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/G_Learning_Local_Image_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Adelaide",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Learning Multi-Domain Convolutional Neural Networks for Visual Tracking",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "54",
        "author_site": "Hyeonseob Nam, Bohyung Han",
        "author": "Hyeonseob Nam; Bohyung Han",
        "abstract": "We propose a novel visual tracking algorithm based on the representations from a discriminatively trained Convolutional Neural Network (CNN). Our algorithm pretrains a CNN using a large set of videos with tracking ground-truths to obtain a generic target representation. Our network is composed of shared layers and multiple branches of domain-specific layers, where domains correspond to individual training sequences and each branch is responsible for binary classification to identify target in each domain. We train each domain in the network iteratively to obtain generic target representations in the shared layers. When tracking a target in a new sequence, we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer, which is updated online. Online tracking is performed by evaluating the candidate windows randomly sampled around the previous target state. The proposed algorithm illustrates outstanding performance in existing tracking benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Nam_Learning_Multi-Domain_Convolutional_CVPR_2016_paper.pdf",
        "aff": "Dept. of Computer Science and Engineering, POSTECH, Korea; Dept. of Computer Science and Engineering, POSTECH, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2452558,
        "gs_citation": 3331,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17336344330227401207&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 14,
        "aff_domain": "postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Nam_Learning_Multi-Domain_Convolutional_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "POSTECH",
        "aff_unique_dep": "Dept. of Computer Science and Engineering",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Learning Online Smooth Predictors for Realtime Camera Planning Using Recurrent Decision Trees",
        "session": "Non-Rigid Reconstruction and Motion Analysis",
        "status": "Oral",
        "track": "main",
        "pid": "15",
        "author_site": "Jianhui Chen, Hoang M. Le, Peter Carr, Yisong Yue, James J. Little",
        "author": "Jianhui Chen; Hoang M. Le; Peter Carr; Yisong Yue; James J. Little",
        "abstract": "We study the problem of online prediction for realtime camera planning, where the goal is to predict smooth trajectories that correctly track and frame objects of interest (e.g., players in a basketball game). The conventional approach for training predictors does not directly consider temporal consistency, and often produces undesirable jitter. Although post-hoc smoothing (e.g., via a Kalman filter) can mitigate this issue to some degree, it is not ideal due to overly stringent modeling assumptions (e.g., Gaussian noise). We propose a recurrent decision tree framework that can directly incorporate temporal consistency into a data-driven predictor, as well as a learning algorithm that can efficiently learn such temporally smooth models. Our approach does not require any post-processing, making online smooth predictions much easier to generate when the noise model is unknown. We apply our approach to sports broadcasting: given noisy player detections, we learn where the camera should look based on human demonstrations. Our experiments exhibit significant improvements over conventional baselines and showcase the practicality of our approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Learning_Online_Smooth_CVPR_2016_paper.pdf",
        "aff": "University of British Columbia; California Institute of Technology; Disney Research; California Institute of Technology; University of British Columbia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Chen_Learning_Online_Smooth_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3772250,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5116829790287094523&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "cs.ubc.ca;caltech.edu;disneyresearch.com;caltech.edu;cs.ubc.ca",
        "email": "cs.ubc.ca;caltech.edu;disneyresearch.com;caltech.edu;cs.ubc.ca",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Learning_Online_Smooth_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;1;0",
        "aff_unique_norm": "University of British Columbia;California Institute of Technology;Disney Research",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ubc.ca;https://www.caltech.edu;https://research.disney.com",
        "aff_unique_abbr": "UBC;Caltech;Disney Research",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Pasadena",
        "aff_country_unique_index": "0;1;1;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Learning Reconstruction-Based Remote Gaze Estimation",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "45",
        "author_site": "Pei Yu, Jiahuan Zhou, Ying Wu",
        "author": "Pei Yu; Jiahuan Zhou; Ying Wu",
        "abstract": "It is a challenging problem to accurately estimate gazes from low-resolution eye images that do not provide fine and detailed features for eyes. Existing methods attempt to establish the mapping between the visual appearance space to the gaze space. Different from the direct regression approach, the reconstruction-based approach represents appearance and gaze via local linear reconstruction in their own spaces. A common treatment is to use the same local reconstruction in the two spaces, i.e., the reconstruction weights in the appearance space are transferred to the gaze space for gaze reconstruction. However, this questionable treatment is taken for granted but has never been justified, leading to significant errors in gaze estimation. This paper is focused on the study of this fundamental issue. It shows that the distance metric in the appearance space needs to be adjusted, before the same reconstruction can be used. A novel method is proposed to learn the metric, such that the affinity structure of the appearance space under this new metric is as close as possible to the affinity structure of the gaze space under the normal Euclidean metric. Furthermore, the local affinity structure invariance is utilized to further regularize the solution to the reconstruction weights, so as to obtain a more robust and accurate solution. Effectiveness of the proposed method is validated and demonstrated through extensive experiments on different subjects.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yu_Learning_Reconstruction-Based_Remote_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Yu_Learning_Reconstruction-Based_Remote_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 575044,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10406633462227138305&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yu_Learning_Reconstruction-Based_Remote_CVPR_2016_paper.html"
    },
    {
        "title": "Learning Relaxed Deep Supervision for Better Edge Detection",
        "session": "Edge Contour Detection",
        "status": "Poster",
        "track": "main",
        "pid": "25",
        "author_site": "Yu Liu, Michael S. Lew",
        "author": "Yu Liu; Michael S. Lew",
        "abstract": "We propose using relaxed deep supervision (RDS) within convolutional neural networks for edge detection. The conventional deep supervision utilizes the general ground-truth to guide intermediate predictions. Instead, we build hierarchical supervisory signals with additional relaxed labels to consider the diversities in deep neural networks. We begin by capturing the relaxed labels from simple detectors (e.g. Canny).Then we merge them with the general ground-truth to generate the RDS. Finally we employ the RDS to supervise the edge network following a coarse-to-fine paradigm. These relaxed labels can be seen as some false positives that are difficult to be classified. We consider these false positives in the supervision, and are able to achieve high performance for better edge detection. We compensate for the lack of training images by capturing coarse edge annotations from a large dataset of image segmentations to pretrain the model. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on the well-known BSDS500 dataset (ODS F-score of .792) and obtains superior cross-dataset generalization results on NYUD dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Learning_Relaxed_Deep_CVPR_2016_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 31076072,
        "gs_citation": 167,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12331482896020965033&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liu_Learning_Relaxed_Deep_CVPR_2016_paper.html"
    },
    {
        "title": "Learning Sparse High Dimensional Filters: Image Filtering, Dense CRFs and Bilateral Neural Networks",
        "session": "Statistical Methods and Learning",
        "status": "Poster",
        "track": "main",
        "pid": "71",
        "author_site": "Varun Jampani, Martin Kiefel, Peter V. Gehler",
        "author": "Varun Jampani; Martin Kiefel; Peter V. Gehler",
        "abstract": "Bilateral filters have wide spread use due to their edge-preserving properties. The common use case is to manually choose a parametric filter type, usually a Gaussian filter. In this paper, we will generalize the parametrization and in particular derive a gradient descent algorithm so the filter parameters can be learned from data. This derivation allows to learn high dimensional linear filters that operate in sparsely populated feature spaces. We build on the permutohedral lattice construction for efficient filtering. The ability to learn more general forms of high-dimensional filters can be used in several diverse applications. First, we demonstrate the use in applications where single filter applications are desired for runtime reasons. Further, we show how this algorithm can be used to learn the pairwise potentials in densely connected conditional random fields and apply these to different image segmentation tasks. Finally, we introduce layers of bilateral filters in CNN and propose bilateral neural networks for the use of high-dimensional sparse data. This view provides new ways to encode model structure into network architectures. A diverse set of experiments empirically validates the usage of general forms of filters.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Jampani_Learning_Sparse_High_CVPR_2016_paper.pdf",
        "aff": "Max Planck Institute for Intelligent Systems, T\u00fcbingen, 72076, Germany+Bernstein Center for Computational Neuroscience, T\u00fcbingen, 72076, Germany; Max Planck Institute for Intelligent Systems, T\u00fcbingen, 72076, Germany+Bernstein Center for Computational Neuroscience, T\u00fcbingen, 72076, Germany; Max Planck Institute for Intelligent Systems, T\u00fcbingen, 72076, Germany+Bernstein Center for Computational Neuroscience, T\u00fcbingen, 72076, Germany",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Jampani_Learning_Sparse_High_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2130418,
        "gs_citation": 244,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9252799423276676751&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;tuebingen.mpg.de;tuebingen.mpg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Jampani_Learning_Sparse_High_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;Bernstein Center for Computational Neuroscience",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mpi-is.mpg.de;",
        "aff_unique_abbr": "MPI-IS;",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "T\u00fcbingen",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Learning Structured Inference Neural Networks With Label Relations",
        "session": "Scene and Image Classification",
        "status": "Poster",
        "track": "main",
        "pid": "75",
        "author_site": "Hexiang Hu, Guang-Tong Zhou, Zhiwei Deng, Zicheng Liao, Greg Mori",
        "author": "Hexiang Hu; Guang-Tong Zhou; Zhiwei Deng; Zicheng Liao; Greg Mori",
        "abstract": "Images of scenes have various objects as well as abundant attributes, and diverse levels of visual categorization are possible. A natural image could be assigned with fine-grained labels that describe major components, coarse-grained labels that depict high level abstraction or a set of labels that reveal attributes. Such categorization at different concept layers can be modeled with label graphs encoding label information. In this paper, we exploit this rich information with a state-of-art deep learning framework, and propose a generic structured model that leverages diverse label relations to improve image classification performance. Our approach employs a novel stacked label prediction neural network, capturing both inter-level and intra-level label semantics. We evaluate our method on benchmark image datasets, and empirical results illustrate the efficacy of our model.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hu_Learning_Structured_Inference_CVPR_2016_paper.pdf",
        "aff": "School of Computing Science, Simon Fraser University; School of Computing Science, Simon Fraser University; School of Computing Science, Simon Fraser University; College of Computer Science and Technology, Zhejiang University; School of Computing Science, Simon Fraser University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1370971,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14598200323189462346&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "sfu.ca;sfu.ca;sfu.ca;zju.edu.cn;cs.sfu.ca",
        "email": "sfu.ca;sfu.ca;sfu.ca;zju.edu.cn;cs.sfu.ca",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hu_Learning_Structured_Inference_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Simon Fraser University;Zhejiang University",
        "aff_unique_dep": "School of Computing Science;College of Computer Science and Technology",
        "aff_unique_url": "https://www.sfu.ca;http://www.zju.edu.cn",
        "aff_unique_abbr": "SFU;ZJU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Burnaby;",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Canada;China"
    },
    {
        "title": "Learning Temporal Regularity in Video Sequences",
        "session": "Video Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "79",
        "author_site": "Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K. Roy-Chowdhury, Larry S. Davis",
        "author": "Mahmudul Hasan; Jonghyun Choi; Jan Neumann; Amit K. Roy-Chowdhury; Larry S. Davis",
        "abstract": "Perceiving meaningful activities in a long video sequence is a challenging problem due to ambiguous definition of `meaningfulness' as well as clutters in the scene. We approach this problem by learning a generative model for regular motion patterns (termed as regularity) using multiple sources with very limited supervision. Specifically, we propose two methods that are built upon the autoencoders for their ability to work with little to no supervision. We first leverage the conventional handcrafted spatio-temporal local features and learn a fully connected autoencoder on them. Second, we build a fully convolutional feed-forward autoencoder to learn both the local features and the classifiers as an end-to-end learning framework. Our model can capture the regularities from multiple datasets. We evaluate our methods in both qualitative and quantitative ways - showing the learned regularity of videos in various aspects and demonstrating competitive performance on anomaly detection datasets as an application.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hasan_Learning_Temporal_Regularity_CVPR_2016_paper.pdf",
        "aff": "UC Riverside\u2020; Comcast Labs, DC\u2020; Comcast Labs, DC\u2020; UC Riverside\u2020; University of Maryland, College Park\u2021",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5703764,
        "gs_citation": 1598,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18114172998595365218&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ucr.edu;ece.ucr.edu;cable.comcast.com;cable.comcast.com;umiacs.umd.edu",
        "email": "ucr.edu;ece.ucr.edu;cable.comcast.com;cable.comcast.com;umiacs.umd.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hasan_Learning_Temporal_Regularity_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;0;2",
        "aff_unique_norm": "University of California, Riverside;Comcast Labs;University of Maryland",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucr.edu;https://www.comcastlabs.com;https://www/umd.edu",
        "aff_unique_abbr": "UCR;;UMD",
        "aff_campus_unique_index": "0;1;1;0;2",
        "aff_campus_unique": "Riverside;DC;College Park",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning Transferrable Knowledge for Semantic Segmentation With Deep Convolutional Neural Network",
        "session": "Semantic Parsing and Segmentation",
        "status": "Spotlight",
        "track": "main",
        "pid": "19",
        "author_site": "Seunghoon Hong, Junhyuk Oh, Honglak Lee , Bohyung Han",
        "author": "Seunghoon Hong; Junhyuk Oh; Honglak Lee; Bohyung Han",
        "abstract": "We propose a novel weakly-supervised semantic segmentation algorithm based on Deep Convolutional Neural Net- work (DCNN). Contrary to existing weakly-supervised approaches, our algorithm exploits auxiliary segmentation an- notations available for different categories to guide segmentations on images with only image-level class labels. To make segmentation knowledge transferrable across categories, we design a decoupled encoder-decoder architecture with attention model. In this architecture, the model generates spatial highlights of each category presented in images using an attention model, and subsequently per- forms binary segmentation for each highlighted region using decoder. Combining attention model, the decoder trained with segmentation annotations in different categories boosts accuracy of weakly-supervised semantic segmentation. The proposed algorithm demonstrates substantially improved performance compared to the state-of-the- art weakly-supervised techniques in PASCAL VOC 2012 dataset when our model is trained with the annotations in 60 exclusive categories in Microsoft COCO dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 221,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=34212388069086163&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper.html"
    },
    {
        "title": "Learning Weight Uncertainty With Stochastic Gradient MCMC for Shape Classification",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "40",
        "author_site": "Chunyuan Li, Andrew Stevens, Changyou Chen, Yunchen Pu, Zhe Gan, Lawrence Carin",
        "author": "Chunyuan Li; Andrew Stevens; Changyou Chen; Yunchen Pu; Zhe Gan; Lawrence Carin",
        "abstract": "Learning the representation of shape cues in 2D & 3D objects for recognition is a fundamental task in computer vision. Deep neural networks (DNNs) have shown promising performance on this task.  Due to the large variability of shapes,  accurate recognition relies on good estimates of model uncertainty, ignored in traditional training of DNNs, typically learned via  stochastic optimization. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (SG-MCMC) to learn weight uncertainty in DNNs. It yields principled Bayesian interpretations for the commonly used Dropout/DropConnect techniques and incorporates them into the SG-MCMC framework. Extensive experiments on 2D & 3D shape datasets and various DNN models demonstrate the superiority of the proposed approach over stochastic optimization. Our approach yields higher recognition accuracy when used in conjunction with Dropout and Batch-Normalization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Learning_Weight_Uncertainty_CVPR_2016_paper.pdf",
        "aff": "Duke University; Duke University; Duke University; Duke University; Duke University; Duke University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1636539,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11285078028149418311&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Learning_Weight_Uncertainty_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning With Side Information Through Modality Hallucination",
        "session": "Object Detection 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "8",
        "author_site": "Judy Hoffman, Saurabh Gupta, Trevor Darrell",
        "author": "Judy Hoffman; Saurabh Gupta; Trevor Darrell",
        "abstract": "We present a modality hallucination architecture for training an RGB object detection model which incorporates depth side information at training time. Our convolutional hallucination network learns a new and complementary RGB image representation which is taught to mimic convolutional mid-level features from a depth network. At test time images are processed jointly through the RGB and hallucination networks to produce improved detection performance. Thus, our method transfers information commonly extracted from depth training data to a network which can extract that information from the RGB counterpart. We present results on the standard NYUDv2 dataset and report improvement on the RGB detection task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hoffman_Learning_With_Side_CVPR_2016_paper.pdf",
        "aff": "EECS Department, UC Berkeley; EECS Department, UC Berkeley; EECS Department, UC Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 819822,
        "gs_citation": 286,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16220295494227055413&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hoffman_Learning_With_Side_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "EECS Department",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning a Discriminative Null Space for Person Re-Identification",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "51",
        "author_site": "Li Zhang, Tao Xiang, Shaogang Gong",
        "author": "Li Zhang; Tao Xiang; Shaogang Gong",
        "abstract": "Most existing person re-identification (re-id) methods focus on learning the optimal distance metrics across camera views. Typically a person's appearance is represented using features of thousands of dimensions, whilst only hundreds of training samples are available due to the difficulties in collecting matched training images. With the number of training samples much smaller than the feature dimension, the existing methods thus face the classic small sample size (SSS) problem and have to resort to dimensionality reduction techniques and/or matrix regularisation, which lead to loss of discriminative power. In this work, we propose to overcome the SSS problem in re-id distance metric learning by matching people in a discriminative  null space of the training data. In this null space, images of the same person are collapsed into a single point thus minimising the within-class scatter to the extreme and maximising the relative between-class separation simultaneously. Importantly, it has a fixed dimension, a closed-form solution and is very efficient to compute. Extensive experiments carried out on five person re-identification benchmarks including VIPeR, PRID2011, CUHK01, CUHK03 and Market1501 show that such a simple approach beats the state-of-the-art alternatives, often by a big margin.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Learning_a_Discriminative_CVPR_2016_paper.pdf",
        "aff": "Queen Mary University of London; Queen Mary University of London; Queen Mary University of London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 651108,
        "gs_citation": 791,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6697095554433865478&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk",
        "email": "qmul.ac.uk;qmul.ac.uk;qmul.ac.uk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Learning_a_Discriminative_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Queen Mary University of London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.qmul.ac.uk",
        "aff_unique_abbr": "QMUL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Learning to Assign Orientations to Feature Points",
        "session": "Matching and Alignment",
        "status": "Oral",
        "track": "main",
        "pid": "12",
        "author_site": "Kwang Moo Yi, Yannick Verdie, Pascal Fua, Vincent Lepetit",
        "author": "Kwang Moo Yi; Yannick Verdie; Pascal Fua; Vincent Lepetit",
        "abstract": "We show how to train a Convolutional Neural Network to assign a canonical orientation to feature points given an image patch centered on the feature point.  Our method improves feature point matching upon the state-of-the art and can be used in conjunction with any existing rotation sensitive descriptors.  To avoid the tedious and almost impossible task of finding a target orientation to learn, we propose to use Siamese networks which implicitly find the optimal orientations during training. We also propose a new type of activation function for Neural Networks that generalizes the popular ReLU, maxout, and PReLU activation functions.  This novel activation performs better for our task.  We validate the effectiveness of our method extensively with four existing datasets, including two non-planar datasets, as well as our own dataset.  We show that we outperform the state-of-the-art without the need of retraining for each dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yi_Learning_to_Assign_CVPR_2016_paper.pdf",
        "aff": "Computer Vision Laboratory, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)+1; Computer Vision Laboratory, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)+1; Computer Vision Laboratory, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL); Institute for Computer Graphics and Vision, Graz University of Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Yi_Learning_to_Assign_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 5256813,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6336226218463269740&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch;icg.tugraz.at",
        "email": "epfl.ch;epfl.ch;epfl.ch;icg.tugraz.at",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yi_Learning_to_Assign_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;2",
        "aff_unique_norm": "EPFL;;Graz University of Technology",
        "aff_unique_dep": "Computer Vision Laboratory;;Institute for Computer Graphics and Vision",
        "aff_unique_url": "https://www.epfl.ch;;https://www.tugraz.at",
        "aff_unique_abbr": "EPFL;;TU Graz",
        "aff_campus_unique_index": "0;0;0;2",
        "aff_campus_unique": "Lausanne;;Graz",
        "aff_country_unique_index": "0;0;0;2",
        "aff_country_unique": "Switzerland;;Austria"
    },
    {
        "title": "Learning to Co-Generate Object Proposals With a Deep Structured Network",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "33",
        "author_site": "Zeeshan Hayder, Xuming He, Mathieu Salzmann",
        "author": "Zeeshan Hayder; Xuming He; Mathieu Salzmann",
        "abstract": "Generating object proposals has become a key component of modern object detection pipelines. However, most existing methods generate the object candidates independently of each other. In this paper, we present an approach to co-generating object proposals in multiple images, thus leveraging the collective power of multiple object candidates. In particular, we introduce a deep structured network that jointly predicts the objectness scores and the bounding box locations of multiple object candidates. Our deep structured network consists of a fully-connected Conditional Random Field built on top of a set of deep Convolutional Neural Networks, which learn features to model both the individual object candidate and the similarity between multiple candidates. To train our deep structured network, we develop an end-to-end learning algorithm that, by unrolling the CRF inference procedure, lets us backpropagate the loss gradient throughout the entire structured network. We demonstrate the effectiveness of our approach on two benchmark datasets, showing significant improvement over state-of-the-art object proposal algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hayder_Learning_to_Co-Generate_CVPR_2016_paper.pdf",
        "aff": "Australian National University + NICTA; Australian National University + NICTA; CVLab, EPFL, Switzerland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Hayder_Learning_to_Co-Generate_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1340465,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16810131801764513563&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "anu.edu.au;anu.edu.au;epfl.ch",
        "email": "anu.edu.au;anu.edu.au;epfl.ch",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hayder_Learning_to_Co-Generate_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1;2",
        "aff_unique_norm": "Australian National University;National Information and Communications Technology Australia;EPFL",
        "aff_unique_dep": ";;CVLab",
        "aff_unique_url": "https://www.anu.edu.au;https://www.nicta.com.au;https://cvlab.epfl.ch",
        "aff_unique_abbr": "ANU;NICTA;EPFL",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;1",
        "aff_country_unique": "Australia;Switzerland"
    },
    {
        "title": "Learning to Localize Little Landmarks",
        "session": "Feature Extraction and Description",
        "status": "Poster",
        "track": "main",
        "pid": "28",
        "author_site": "Saurabh Singh, Derek Hoiem, David Forsyth",
        "author": "Saurabh Singh; Derek Hoiem; David Forsyth",
        "abstract": "We interact everyday with tiny objects such as the door handle of a car or the light switch in a room. These little landmarks are barely visible and hard to localize in images. We describe a method to find such landmarks by finding a sequence of latent landmarks, each with a prediction model. Each latent landmark predicts the next in sequence, and the last localizes the target landmark. For example, to find the door handle of a car, our method learns to start with a latent landmark near the wheel, as it is globally distinctive; subsequent latent landmarks use the context from the earlier ones to get closer to the target. Our method is supervised solely by the location of the little landmark and displays strong performance on more difficult variants of established tasks and on two new tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Singh_Learning_to_Localize_CVPR_2016_paper.pdf",
        "aff": "University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign",
        "project": "http://vision.cs.illinois.edu/projects/litland/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1876923,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15666512515704032404&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Singh_Learning_to_Localize_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning to Match Aerial Images With Deep Attentive Architectures",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "55",
        "author_site": "Hani Altwaijry, Eduard Trulls, James Hays, Pascal Fua, Serge Belongie",
        "author": "Hani Altwaijry; Eduard Trulls; James Hays; Pascal Fua; Serge Belongie",
        "abstract": "Image matching is a fundamental problem in Computer Vision. In the context of feature-based matching, SIFT and its variants have long excelled in a wide array of applications. However, for ultra-wide baselines, as in the case of aerial images captured under large camera rotations, the appearance variation goes beyond the reach of SIFT and RANSAC. In this paper we propose a data-driven, deep learning-based approach that sidesteps local correspondence by framing the problem as a classification task. Furthermore, we demonstrate that local correspondences can still be useful. To do so we incorporate an attention mechanism to produce a set of probable matches, which allows us to further increase performance. We train our models on a dataset of urban aerial imagery consisting of `same' and `different' pairs, collected for this purpose, and characterize the problem via a human study with annotations from Amazon Mechanical Turk. We demonstrate that our models outperform the state-of-the-art on ultra-wide baseline matching and approach human accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Altwaijry_Learning_to_Match_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, Cornell University+Cornell Tech; Computer Vision Laboratory, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL); School of Interactive Computing, College of Computing, Georgia Institute of Technology; Computer Vision Laboratory, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL); Department of Computer Science, Cornell University+Cornell Tech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1803051,
        "gs_citation": 93,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8517179728571615532&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Altwaijry_Learning_to_Match_CVPR_2016_paper.html",
        "aff_unique_index": "0+0;1;2;1;0+0",
        "aff_unique_norm": "Cornell University;EPFL;Georgia Institute of Technology",
        "aff_unique_dep": "Department of Computer Science;Computer Vision Laboratory;School of Interactive Computing",
        "aff_unique_url": "https://www.cornell.edu;https://www.epfl.ch;https://www.gatech.edu",
        "aff_unique_abbr": "Cornell;EPFL;Georgia Tech",
        "aff_campus_unique_index": "1;2;3;2;1",
        "aff_campus_unique": ";New York City;Lausanne;Atlanta",
        "aff_country_unique_index": "0+0;1;0;1;0+0",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "title": "Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation",
        "session": "Biomedical Image Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "26",
        "author_site": "Hoo-Chang Shin, Kirk Roberts, Le Lu, Dina Demner-Fushman, Jianhua Yao, Ronald M. Summers",
        "author": "Hoo-Chang Shin; Kirk Roberts; Le Lu; Dina Demner-Fushman; Jianhua Yao; Ronald M. Summers",
        "abstract": "Despite the recent advances in automatically describing image contents, their applications have been mostly limited to image caption datasets containing natural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep learning model to efficiently detect a disease from an image and annotate its contexts (e.g., location, severity and the affected organs). We employ a publicly available radiology dataset of chest x-rays and their reports, and use its image annotations to mine disease names to train convolutional neural networks (CNNs). In doing so, we adopt various regularization techniques to circumvent the large normal-vs-diseased cases bias. Recurrent neural networks (RNNs) are then trained to describe the contexts of a detected disease, based on the deep CNN features. Moreover, we introduce a novel approach to use the weights of the already trained pair of CNN/RNN on the domain-specific image/text dataset, to infer the joint image/text contexts for composite image labeling. Significantly improved image annotation results are demonstrated using the recurrent neural cascade model by taking the joint image/text contexts into account.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shin_Learning_to_Read_CVPR_2016_paper.pdf",
        "aff": "Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, Clinical Center; Lister Hill National Center for Biomedical Communications, National Library of Medicine; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, Clinical Center; Lister Hill National Center for Biomedical Communications, National Library of Medicine; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, Clinical Center; Imaging Biomarkers and Computer-Aided Diagnosis Laboratory, Radiology and Imaging Sciences, Clinical Center",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Shin_Learning_to_Read_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1906726,
        "gs_citation": 490,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1933337627185265206&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "nih.gov;nih.gov;nih.gov;mail.nih.gov;cc.nih.gov;nih.gov",
        "email": "nih.gov;nih.gov;nih.gov;mail.nih.gov;cc.nih.gov;nih.gov",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shin_Learning_to_Read_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1;0;0",
        "aff_unique_norm": "Clinical Center;National Library of Medicine",
        "aff_unique_dep": "Radiology and Imaging Sciences;Lister Hill National Center for Biomedical Communications",
        "aff_unique_url": "https://www.cc.nih.gov;https://www.nlm.nih.gov",
        "aff_unique_abbr": ";NLM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Learning to Select Pre-Trained Deep Representations With Bayesian Evidence Framework",
        "session": "Learning and CNN Architectures",
        "status": "Oral",
        "track": "main",
        "pid": "3",
        "author_site": "Yong-Deok Kim, Taewoong Jang, Bohyung Han, Seungjin Choi",
        "author": "Yong-Deok Kim; Taewoong Jang; Bohyung Han; Seungjin Choi",
        "abstract": "We propose a Bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated on top of a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitken's delta-squared process, which accelerates convergence of fixed point update. The proposed Bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our Bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kim_Learning_to_Select_CVPR_2016_paper.pdf",
        "aff": "Software R&D Center, Device Solutions, Samsung Electronics, Korea + POSTECH, Korea; Stradvision Inc., Korea + POSTECH, Korea; Department of Computer Science and Engineering, POSTECH, Korea; Department of Computer Science and Engineering, POSTECH, Korea",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1147529,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10191120229115737910&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "samsung.com;stradvision.com;postech.ac.kr;postech.ac.kr",
        "email": "samsung.com;stradvision.com;postech.ac.kr;postech.ac.kr",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kim_Learning_to_Select_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;2+1;3;3",
        "aff_unique_norm": "Samsung;Pohang University of Science and Technology;Stradvision Inc.;POSTECH",
        "aff_unique_dep": "Software R&D Center, Device Solutions;;;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.samsung.com;https://www.postech.ac.kr;https://www.stradvision.com;https://www.postech.ac.kr",
        "aff_unique_abbr": "Samsung;POSTECH;Stradvision;POSTECH",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Pohang",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Learnt Quasi-Transitive Similarity for Retrieval From Large Collections of Faces",
        "session": "Face Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "36",
        "author_site": "Ognjen Arandjelovi\u0107",
        "author": "Ognjen Arandjelovic",
        "abstract": "We are interested in identity-based retrieval of face sets from large unlabelled collections acquired in uncontrolled environments. Given a baseline algorithm for measuring the similarity of two face sets, the meta-algorithm introduced in this paper seeks to leverage the structure of the data corpus to make the best use of the available baseline. In particular, we show how partial transitivity of inter-personal similarity can be exploited to improve the retrieval of particularly challenging sets which poorly match the query under the baseline measure. We: (i) describe the use of proxy sets as a means of computing the similarity between two sets, (ii) introduce transitivity meta-features based on the similarity of salient modes of appearance variation between sets, (iii) show how quasi-transitivity can be learnt from such features without any labelling or manual intervention, and (iv) demonstrate the effectiveness of the proposed methodology through experiments on the notoriously challenging YouTube database.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Arandjelovic_Learnt_Quasi-Transitive_Similarity_CVPR_2016_paper.pdf",
        "aff": "University of St Andrews, United Kingdom",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1666888,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16054200820594368283&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 15,
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "author_num": 1,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Arandjelovic_Learnt_Quasi-Transitive_Similarity_CVPR_2016_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of St Andrews",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.st-andrews.ac.uk",
        "aff_unique_abbr": "St Andrews",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Less Is More: Zero-Shot Learning From Online Textual Documents With Noise Suppression",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "81",
        "author_site": "Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton van den Hengel",
        "author": "Ruizhi Qiao; Lingqiao Liu; Chunhua Shen; Anton van den Hengel",
        "abstract": "Classifying a visual concept merely from its associated online textual source, such as a Wikipedia article, is an attractive research topic in zero-shot learning because it alleviates the burden of manually collecting semantic attributes. Several recent works have pursued this approach by exploring various ways of connecting the visual and text domains. This paper revisits this idea by stepping further to consider one important factor: the textual representation is usually too noisy for the zero-shot learning application. This consideration motivates us to design a simple-but-effective zero-shot learning method capable of suppressing noise in the text. More specifically, we propose an l_2,1-norm based objective function which can simultaneously suppress the noisy signal in the text and learn a function to match the text document and visual features. We also develop an optimization algorithm to efficiently solve the resulting problem. By conducting experiments on two large datasets, we demonstrate that the proposed method significantly outperforms the competing methods which rely on online information sources but without explicit noise suppression. We further make an in-depth analysis of the proposed method and provide insight as to what kind of information in documents is useful for zero-shot learning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Qiao_Less_Is_More_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3921717,
        "gs_citation": 238,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15842401513666476229&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Qiao_Less_Is_More_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Adelaide",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Linear Shape Deformation Models With Local Support Using Graph-Based Structured Matrix Factorisation",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "36",
        "author_site": "Florian Bernard, Peter Gemmar, Frank Hertel, Jorge Goncalves, Johan Thunberg",
        "author": "Florian Bernard; Peter Gemmar; Frank Hertel; Jorge Goncalves; Johan Thunberg",
        "abstract": "Representing 3D shape deformations by high-dimensional linear models has many applications in computer vision and medical imaging. Commonly, using Principal Components Analysis a low-dimensional subspace of the high-dimensional shape space is determined. However, the resulting factors (the most dominant eigenvectors of the covariance matrix) have global support, i.e. changing the coefficient of a single factor deforms the entire shape. Based on matrix factorisation with sparsity and graph-based regularisation terms, we present a method to obtain deformation factors with local support. The benefits include better flexibility and interpretability as well as the possibility of interactively deforming shapes locally. We demonstrate that for brain shapes our method outperforms the state of the art in local support models with respect to generalisation and sparse reconstruction, whereas for body shapes our method gives more realistic deformations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bernard_Linear_Shape_Deformation_CVPR_2016_paper.pdf",
        "aff": "Centre Hospitalier de Luxembourg, Luxembourg + Luxembourg Centre for Systems Biomedicine, University of Luxembourg, Luxembourg; Luxembourg Centre for Systems Biomedicine, University of Luxembourg, Luxembourg + Trier University of Applied Sciences, Trier, Germany; Centre Hospitalier de Luxembourg, Luxembourg; Luxembourg Centre for Systems Biomedicine, University of Luxembourg, Luxembourg; Luxembourg Centre for Systems Biomedicine, University of Luxembourg, Luxembourg",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Bernard_Linear_Shape_Deformation_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 3383908,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11488360344178840593&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bernard_Linear_Shape_Deformation_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1+2;0;1;1",
        "aff_unique_norm": "Centre Hospitalier de Luxembourg;University of Luxembourg;Trier University of Applied Sciences",
        "aff_unique_dep": ";Luxembourg Centre for Systems Biomedicine;",
        "aff_unique_url": "https://www.chl.lu;https://wwwen.unil.lu;https://www.fh-trier.de",
        "aff_unique_abbr": "CHL;UniLu;",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Trier",
        "aff_country_unique_index": "0+0;0+1;0;0;0",
        "aff_country_unique": "Luxembourg;Germany"
    },
    {
        "title": "LocNet: Improving Localization Accuracy for Object Detection",
        "session": "Object Recognition and Detection",
        "status": "Oral",
        "track": "main",
        "pid": "4",
        "author_site": "Spyros Gidaris, Nikos Komodakis",
        "author": "Spyros Gidaris; Nikos Komodakis",
        "abstract": "We propose a novel object localization methodology with the purpose of boosting the localization accuracy of state-of-the-art object detection systems. Our model, given a search region, aims at returning the bounding box of an object of interest inside this region. To accomplish its goal, it relies on assigning conditional probabilities to each row and column of this region, where these probabilities provide useful information regarding the location of the boundaries of the object inside the search region and allow the accurate inference of the object bounding box under a simple probabilistic framework.   For implementing our localization model, we make use of a convolutional neural network architecture that is properly adapted for this task, called LocNet. We show experimentally that LocNet achieves a very significant improvement on the mAP for high IoU thresholds on PASCAL VOC2007 test set and that it can be very easily coupled with recent state-of-the-art object detection systems, helping them to boost their performance. Finally, we demonstrate that our detection approach can achieve high detection accuracy even when it is given as input a set of sliding windows, thus proving that it is independent of box proposal methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gidaris_LocNet_Improving_Localization_CVPR_2016_paper.pdf",
        "aff": "Universite Paris Est, Ecole des Ponts ParisTech; Universite Paris Est, Ecole des Ponts ParisTech",
        "project": "",
        "github": "https://github.com/gidariss/LocNet",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1477718,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4561350794180192627&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "imagine.enpc.fr;enpc.fr",
        "email": "imagine.enpc.fr;enpc.fr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gidaris_LocNet_Improving_Localization_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universite Paris Est",
        "aff_unique_dep": "Ecole des Ponts ParisTech",
        "aff_unique_url": "https://www.univ-Paris-est.fr",
        "aff_unique_abbr": "UPE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Local Background Enclosure for RGB-D Salient Object Detection",
        "session": "Object Detection 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "9",
        "author_site": "David Feng, Nick Barnes, Shaodi You, Chris McCarthy",
        "author": "David Feng; Nick Barnes; Shaodi You; Chris McCarthy",
        "abstract": "Recent work in salient object detection has considered the incorporation of depth cues from RGB-D images. In most cases, depth contrast is used as the main feature. However, areas of high contrast in background regions cause false positives for such methods, as the background frequently contains regions that are highly variable in depth. Here, we propose a novel RGB-D saliency feature. Local Background Enclosure (LBE) captures the spread of angular directions which are background with respect to the candidate region and the object that it is part of. We show that our feature improves over state-of-the-art RGB-D saliency approaches as well as RGB methods on the RGBD1000 and NJUDS2000 datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Feng_Local_Background_Enclosure_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Feng_Local_Background_Enclosure_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2188052,
        "gs_citation": 246,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12881481819869654327&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Feng_Local_Background_Enclosure_CVPR_2016_paper.html"
    },
    {
        "title": "Logistic Boosting Regression for Label Distribution Learning",
        "session": "Statistical Methods and Learning",
        "status": "Poster",
        "track": "main",
        "pid": "75",
        "author_site": "Chao Xing, Xin Geng, Hui Xue",
        "author": "Chao Xing; Xin Geng; Hui Xue",
        "abstract": "Label Distribution Learning (LDL) is a general learning framework which includes both single label and multi-label learning as its special cases. One of the main assumptions made in traditional LDL algorithms is the derivation of the parametric model as the maximum entropy model. While it is a reasonable assumption without additional information, there is no particular evidence supporting it in the problem of LDL. Alternatively, using a general LDL model family to approximate this parametric model can avoid the potential influence of the specific model. In order to learn this general model family, this paper uses a method called Logistic Boosting Regression (LogitBoost) which can be seen as an additive weighted function regression from the statistical viewpoint. For each step, we can fit individual weighted regression function (base learner) to realize the optimization gradually. The base learners are chosen as weighted regression tree and vector tree, which constitute two algorithms named LDLogitBoost and AOSO-LDLogitBoost in this paper. Experiments on facial expression recognition, crowd opinion prediction on movies and apparent age estimation show that LDLogitBoost and AOSO-LDLogitBoost can achieve better performance than traditional LDL algorithms as well as other LogitBoost algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xing_Logistic_Boosting_Regression_CVPR_2016_paper.pdf",
        "aff": "Key Lab of Computer Network and Information Integration (Ministry of Education), School of Computer Science and Engineering, Southeast University, Nanjing 211189, China; Key Lab of Computer Network and Information Integration (Ministry of Education), School of Computer Science and Engineering, Southeast University, Nanjing 211189, China; Key Lab of Computer Network and Information Integration (Ministry of Education), School of Computer Science and Engineering, Southeast University, Nanjing 211189, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1139472,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6174882665584498251&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "email": "seu.edu.cn;seu.edu.cn;seu.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xing_Logistic_Boosting_Regression_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Southeast University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.seu.edu.cn/",
        "aff_unique_abbr": "SEU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Longitudinal Face Modeling via Temporal Deep Restricted Boltzmann Machines",
        "session": "Biologically Inspired Vision",
        "status": "Poster",
        "track": "main",
        "pid": "51",
        "author_site": "Chi Nhan Duong, Khoa Luu, Kha Gia Quach, Tien D. Bui",
        "author": "Chi Nhan Duong; Khoa Luu; Kha Gia Quach; Tien D. Bui",
        "abstract": "Modeling the face aging process is a challenging task due to large and non-linear variations present in different stages of face development. This paper presents a deep model approach for face age progression that can efficiently capture the non-linear aging process and automatically synthesize a series of age-progressed faces in various age ranges. In this approach, we first decompose the long-term age progress into a sequence of short-term changes and model it as a face sequence. The Temporal Deep Restricted Boltzmann Machines based age progression model together with the prototype faces are then constructed to learn the aging transformation between faces in the sequence. In addition, to enhance the wrinkles of faces in the later age ranges, the wrinkle models are further constructed using Restricted Boltzmann Machines to capture their variations in different facial regions. The geometry constraints are also taken into account in the last step for more consistent age-progressed results. The proposed approach is evaluated using various face aging databases, i.e. FG-NET, Cross-Age Celebrity Dataset (CACD) and MORPH, and  our collected large-scale aging database named AginG Faces in the Wild (AGFW). In addition, when ground-truth age is not available for input image, our proposed system is able to automatically estimate the age of the input face before aging process is employed.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Duong_Longitudinal_Face_Modeling_CVPR_2016_paper.pdf",
        "aff": "Computer Science and Software Engineering, Concordia University, Montr \u00b4eal, Qu \u00b4ebec, Canada; CyLab Biometrics Center and the Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Computer Science and Software Engineering, Concordia University, Montr \u00b4eal, Qu \u00b4ebec, Canada; Computer Science and Software Engineering, Concordia University, Montr \u00b4eal, Qu \u00b4ebec, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2418525,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14337820921779036474&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "encs.concordia.ca;andrew.cmu.edu;encs.concordia.ca;encs.concordia.ca",
        "email": "encs.concordia.ca;andrew.cmu.edu;encs.concordia.ca;encs.concordia.ca",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Duong_Longitudinal_Face_Modeling_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Concordia University;Carnegie Mellon University",
        "aff_unique_dep": "Computer Science and Software Engineering;Department of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.concordia.ca;https://www.cmu.edu",
        "aff_unique_abbr": "Concordia;CMU",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Montr\u00e9al;Pittsburgh",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "Loss Functions for Top-k Error: Analysis and Insights",
        "session": "Supervised Learning",
        "status": "Poster",
        "track": "main",
        "pid": "75",
        "author_site": "Maksim Lapin, Matthias Hein, Bernt Schiele",
        "author": "Maksim Lapin; Matthias Hein; Bernt Schiele",
        "abstract": "In order to push the performance on realistic computer vision tasks, the number of classes in modern benchmark datasets has significantly increased in recent years. This increase in the number of classes comes along with increased ambiguity between the class labels, raising the question if top-1 error is the right performance measure. In this paper, we provide an extensive comparison and evaluation of established multiclass methods comparing their top-k performance both from a practical as well as from a theoretical perspective. Moreover, we introduce novel top-k loss functions as modifications of the softmax and the multiclass SVM losses and provide efficient optimization schemes for them. In the experiments, we compare on various datasets all of the proposed and established methods for top-k error optimization. An interesting insight of this paper is that the softmax loss yields competitive top-k performance for all k simultaneously. For a specific top-k error, our new top- k losses lead typically to further improvements while being faster to train than the softmax.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lapin_Loss_Functions_for_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Lapin_Loss_Functions_for_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2478054,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8274102661177970754&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lapin_Loss_Functions_for_CVPR_2016_paper.html"
    },
    {
        "title": "MCMC Shape Sampling for Image Segmentation With Nonparametric Shape Priors",
        "session": "Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "44",
        "author_site": "Ertunc Erdil, Sinan Yildirim, M\u00fcjdat Cetin, Tolga Tasdizen",
        "author": "Ertunc Erdil; Sinan Yildirim; Mujdat Cetin; Tolga Tasdizen",
        "abstract": "Segmenting images of low quality or with missing data is a challenging problem. Integrating statistical prior information about the shapes to be segmented can improve the segmentation results significantly. Most shape-based segmentation algorithms optimize an energy functional and find a point estimate for the object to be segmented. This does not provide a measure of the degree of confidence in that result, neither does it provide a picture of other probable solutions based on the data and the priors. With a statistical view, addressing these issues would involve the problem of characterizing the posterior densities of the shapes of the objects to be segmented. For such characterization, we propose a Markov chain Monte Carlo (MCMC) sampling-based image segmentation algorithm that uses statistical shape priors. In addition to better characterization of the statistical structure of the problem, such an approach would also have the potential to address issues with getting stuck at local optima, suffered by existing shape-based segmentation methods. Our approach is able to characterize the posterior probability density in the space of shapes through its samples, and to return multiple solutions, potentially from different modes of a multimodal probability density, which would be encountered, e.g., in segmenting objects from multiple shape classes. We present promising results on a variety of data sets. We also provide an extension for segmenting shapes of objects with parts that can go through independent shape variations. This extension involves the use of local shape priors on object parts and provides robustness to limitations in shape training data size.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Erdil_MCMC_Shape_Sampling_CVPR_2016_paper.pdf",
        "aff": "Sabanci University; Sabanci University; Sabanci University; Utah University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Erdil_MCMC_Shape_Sampling_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2429940,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10120624892037604332&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "sabanciuniv.edu;sabanciuniv.edu;sabanciuniv.edu;sci.utah.edu",
        "email": "sabanciuniv.edu;sabanciuniv.edu;sabanciuniv.edu;sci.utah.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Erdil_MCMC_Shape_Sampling_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Sabanci University;University of Utah",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sabanciuniv.edu/;https://www.utah.edu",
        "aff_unique_abbr": "SU;U of U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "T\u00fcrkiye;United States"
    },
    {
        "title": "MDL-CW: A Multimodal Deep Learning Framework With Cross Weights",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "37",
        "author_site": "Sarah Rastegar, Mahdieh Soleymani, Hamid R. Rabiee, Seyed Mohsen Shojaee",
        "author": "Sarah Rastegar; Mahdieh Soleymani; Hamid R. Rabiee; Seyed Mohsen Shojaee",
        "abstract": "Deep learning has received much attention as of the most powerful approaches for multimodal representation learning in recent years. An ideal model for multimodal data can reason about missing modalities using the available ones, and usually provides more information when multiple modalities are being considered. All the previous deep models contain separate modality-specific networks and find a shared representation on top of those networks. Therefore, they only consider high level interactions between modalities to find a joint representation for them. In this paper, we propose a multimodal deep learning framework (MDL-CW) that exploits the cross weights between representation of modalities, and try to gradually learn interactions of the modalities in a deep network manner (from low to high level interactions). Moreover, we theoretically show that considering these interactions provide more intra-modality information, and introduce a multi-stage pre-training method that is based on the properties of multi-modal data. In the proposed framework, as opposed to the existing deep methods for multi-modal data, we try to reconstruct the representation of each modality at a given level, with representation of other modalities in the previous layer. Extensive experimental results show that the proposed model outperforms state-of-the-art information retrieval methods for both image and text queries on the PASCAL-sentence and SUN-Attribute databases.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Rastegar_MDL-CW_A_Multimodal_CVPR_2016_paper.pdf",
        "aff": "AICT Innovation Center, Department of Computer Engineering, Sharif University of Technology; AICT Innovation Center, Department of Computer Engineering, Sharif University of Technology; AICT Innovation Center, Department of Computer Engineering, Sharif University of Technology; AICT Innovation Center, Department of Computer Engineering, Sharif University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 776210,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14307189674080215437&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "sharif.edu; ; ; ",
        "email": "sharif.edu; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Rastegar_MDL-CW_A_Multimodal_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Sharif University of Technology",
        "aff_unique_dep": "Department of Computer Engineering",
        "aff_unique_url": "https://www.sharif.edu",
        "aff_unique_abbr": "SUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Iran"
    },
    {
        "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language",
        "session": "Video and Language",
        "status": "Poster",
        "track": "main",
        "pid": "79",
        "author_site": "Jun Xu, Tao Mei, Ting Yao, Yong Rui",
        "author": "Jun Xu; Tao Mei; Ting Yao; Yong Rui",
        "abstract": "While there has been increasing interest in the task of describing video with natural language, current computer vision algorithms are still severely limited in terms of the variability and complexity of the videos and their associated language that they can recognize.  This is in part due to the simplicity of current benchmarks, which mostly focus on specific fine-grained domains with limited videos and simple descriptions. While researchers have provided several benchmark datasets for image captioning, we are not aware of any large-scale video description dataset with comprehensive categories yet diverse video content.  In this paper we present MSR-VTT (standing for \"ABC-Video to Text\") which is a new large-scale video benchmark for video understanding, especially the emerging task of translating video to text.  This is achieved by collecting 257 popular queries from a commercial video search engine, with 118 videos for each query. In its current version, MSR-VTT provides 10K web video clips with 38.7 hours and 200K clip-sentence pairs in total, covering the most comprehensive categories and diverse visual content, and representing the largest dataset in terms of sentence and vocabulary. Each clip is annotated with about 20 natural sentences by 1,327 AMT workers.  We present a detailed analysis of MSR-VTT in comparison to a complete set of existing datasets, together with a summarization of different state-of-the-art video-to-text approaches.  We also provide an extensive evaluation of these approaches on this dataset, showing that the hybrid Recurrent Neural Network-based approach, which combines single-frame and motion representations with soft-attention pooling strategy, yields the best generalization capability on MSR-VTT.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_MSR-VTT_A_Large_CVPR_2016_paper.pdf",
        "aff": "Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1035630,
        "gs_citation": 2460,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16072082614386877525&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xu_MSR-VTT_A_Large_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/microsoft-research-asia",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Macroscopic Interferometry: Rethinking Depth Estimation With Frequency-Domain Time-Of-Flight",
        "session": "Vision With Alternative Sensors",
        "status": "Oral",
        "track": "main",
        "pid": "15",
        "author_site": "Achuta Kadambi, Jamie Schiel, Ramesh Raskar",
        "author": "Achuta Kadambi; Jamie Schiel; Ramesh Raskar",
        "abstract": "A form of meter-scale, macroscopic interferometry is proposed using conventional time-of-flight (ToF) sensors. Today, ToF sensors use phase-based sampling, where the phase delay between emitted and received, high-frequency signals encodes distance. This paper examines an alternative ToF architecture, inspired by micron-scale, microscopic interferometry, that relies only on frequency sampling: we refer to our proposed macroscopic technique as Frequency-Domain Time of Flight (FD-ToF). The proposed architecture offers several benefits over existing phase ToF systems, such as robustness to phase wrapping and implicit resolution of multi-path interference, all while capturing the same number of subframes. A prototype camera is constructed to demonstrate macroscopic interferometry at meter scale.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kadambi_Macroscopic_Interferometry_Rethinking_CVPR_2016_paper.pdf",
        "aff": "MIT Media Lab; MIT Media Lab; MIT Media Lab",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3629935,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7208733580863025249&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kadambi_Macroscopic_Interferometry_Rethinking_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Media Lab",
        "aff_unique_url": "http://www.media.mit.edu/",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Manifold SLIC: A Fast Method to Compute Content-Sensitive Superpixels",
        "session": "Segmentation and Saliency",
        "status": "Poster",
        "track": "main",
        "pid": "70",
        "author_site": "Yong-Jin Liu, Cheng-Chi Yu, Min-Jing Yu, Ying He",
        "author": "Yong-Jin Liu; Cheng-Chi Yu; Min-Jing Yu; Ying He",
        "abstract": "Superpixels are perceptually meaningful atomic regions that can effectively capture image features. Among various methods for computing uniform superpixels, simple linear iterative clustering (SLIC) is popular due to its simplicity and high performance. In this paper, we extend SLIC to compute content-sensitive superpixels, i.e., small superpixels in content-dense regions (e.g., with high intensity or color variation) and large superpixels in content-sparse regions. Rather than the conventional SLIC method that clusters pixels in R5, we map the image I to a 2-dimensional manifold M in R5, whose area elements are a good measure of the content density in I. We propose an efficient method to compute restricted centroidal Voronoi tessellation (RCVT) --- a uniform tessellation --- on M, which induces the content-sensitive superpixels in I. Unlike other algorithms that characterize content-sensitivity by geodesic distances, manifold SLIC tackles the problem by measuring areas of Voronoi cells on M, which can be computed at a very low cost. As a result, it runs 10 times faster than the state-of-the-art content-sensitive superpixels algorithm. We evaluate manifold SLIC and seven representative methods on the BSDS500 benchmark and observe that our method outperforms the existing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Manifold_SLIC_A_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Liu_Manifold_SLIC_A_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 181,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12540496799936149362&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liu_Manifold_SLIC_A_CVPR_2016_paper.html"
    },
    {
        "id": "e4751b57f7",
        "title": "Marr Revisited: 2D-3D Alignment via Surface Normal Prediction",
        "site": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bansal_Marr_Revisited_2D-3D_CVPR_2016_paper.html",
        "author": "Aayush Bansal; Bryan Russell; Abhinav Gupta",
        "abstract": "We introduce an approach that leverages surface normal predictions, along with appearance cues, to retrieve 3D models for objects depicted in 2D still images from a large CAD object library.  Critical to the success of our approach is the ability to recover accurate surface normals for objects in the depicted scene.  We introduce a skip-network model built on the pre-trained Oxford VGG convolutional neural network for surface normal prediction.  Our model achieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surface normal prediction, and recovers fine object detail compared to previous methods.  Furthermore, we develop a two-stream network over the input image and predicted surface normals that jointly learns pose and style for CAD model retrieval.  When using the predicted surface normals, our two-stream network matches prior work using surface normals computed from RGB-D images on the task of pose prediction, and achieves state of the art when using RGB-D input.  Finally, our two-stream network allows us to retrieve CAD models that better match the style and pose of a depicted object compared with baseline approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bansal_Marr_Revisited_2D-3D_CVPR_2016_paper.pdf",
        "aff": "Carnegie Mellon University; Adobe Research; Carnegie Mellon University",
        "project": "http://www.cs.cmu.edu/~aayushb/marrRevisited/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2471009,
        "gs_citation": 317,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18179702541606201825&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "; ; ",
        "email": "; ; ",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.cmu.edu;https://research.adobe.com",
        "aff_unique_abbr": "CMU;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Material Classification Using Raw Time-Of-Flight Measurements",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "51",
        "author_site": "Shuochen Su, Felix Heide, Robin Swanson, Jonathan Klein, Clara Callenberg, Matthias Hullin, Wolfgang Heidrich",
        "author": "Shuochen Su; Felix Heide; Robin Swanson; Jonathan Klein; Clara Callenberg; Matthias Hullin; Wolfgang Heidrich",
        "abstract": "We propose a material classification method using raw time-of-flight (ToF) measurements. ToF cameras capture the correlation between a reference signal and the temporal response of material to incident illumination. Such measurements encode unique signatures of the material, i.e. the degree of subsurface scattering inside a volume. Subsequently, it offers an orthogonal domain of feature representation compared to conventional spatial and angular reflectance-based approaches. We demonstrate the effectiveness, robustness, and efficiency of our method through experiments and comparisons of real-world materials.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Su_Material_Classification_Using_CVPR_2016_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Su_Material_Classification_Using_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1731762,
        "gs_citation": 72,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11707876856834443309&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Su_Material_Classification_Using_CVPR_2016_paper.html"
    },
    {
        "title": "Memory Efficient Max Flow for Multi-Label Submodular MRFs",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "61",
        "author_site": "Thalaiyasingam Ajanthan, Richard Hartley, Mathieu Salzmann",
        "author": "Thalaiyasingam Ajanthan; Richard Hartley; Mathieu Salzmann",
        "abstract": "Multi-label submodular Markov Random Fields (MRFs) have been shown to be solvable using max-flow based on an encoding of the labels proposed by Ishikawa, in which each variable X_i is represented by l nodes (where l is the number of labels) arranged in a column. However, this method in general requires 2l^2 edges for each pair of neighbouring variables. This makes it inapplicable to realistic problems with many variables and labels, due to excessive memory requirement. In this paper, we introduce a variant of the max-flow algorithm that requires much less storage. Consequently, our algorithm makes it possible to optimally solve multi-label submodular problems involving large numbers of variables and labels on a standard computer.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ajanthan_Memory_Efficient_Max_CVPR_2016_paper.pdf",
        "aff": "Australian National University & NICTA*; Australian National University & NICTA*; CVLAB, EPFL",
        "project": "",
        "github": "https://github.com/tajanthan/memf",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Ajanthan_Memory_Efficient_Max_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 816298,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18245370782641573586&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ajanthan_Memory_Efficient_Max_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Australian National University;EPFL",
        "aff_unique_dep": ";CVLAB",
        "aff_unique_url": "https://www.anu.edu.au;https://cvlab.epfl.ch",
        "aff_unique_abbr": "ANU;EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Australia;Switzerland"
    },
    {
        "title": "Metric Learning as Convex Combinations of Local Models With Generalization Guarantees",
        "session": "Supervised Learning",
        "status": "Poster",
        "track": "main",
        "pid": "76",
        "author_site": "Valentina Zantedeschi, R\u00e9mi Emonet, Marc Sebban",
        "author": "Valentina Zantedeschi; Remi Emonet; Marc Sebban",
        "abstract": "Over the past ten years, metric learning allowed the improvement of the numerous machine learning approaches that manipulate distances or similarities. In this field, local metric learning has been shown to be very efficient, especially to take into account non linearities in the data and better capture the peculiarities of the application of interest. However, it is well known that local metric learning (i) can entail overfitting and (ii) face difficulties to compare two instances that are assigned to two different local models. In this paper, we address these two issues by introducing a novel metric learning algorithm that linearly combines local models (C2LM). Starting from a partition of the space in regions and a model (a score function) for each region, C2LM defines a metric between points as a weighted combination of the models. A weight vector is learned for each pair of regions, and a spatial regularization ensures that the weight vectors evolve smoothly and that nearby models are favored in the combination. The proposed approach has the particularity of working in a regression setting, of working implicitly at different scales, and of being generic enough so that it is applicable to similarities and distances. We prove theoretical guarantees of the approach using the framework of algorithmic robustness. We carry out experiments with datasets using both distances (perceptual color distances, using Mahalanobis-like distances) and similarities (semantic word similarities, using bilinear forms), showing that C2LM consistently improves regression accuracy even in the case where the amount of training data is small.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zantedeschi_Metric_Learning_as_CVPR_2016_paper.pdf",
        "aff": "Univ Lyon, UJM-Saint-Etienne, CNRS, Institut d Optique Graduate School, Laboratoire Hubert Curien UMR 5516, F-42023, SAINT-ETIENNE, France; Univ Lyon, UJM-Saint-Etienne, CNRS, Institut d Optique Graduate School, Laboratoire Hubert Curien UMR 5516, F-42023, SAINT-ETIENNE, France; Univ Lyon, UJM-Saint-Etienne, CNRS, Institut d Optique Graduate School, Laboratoire Hubert Curien UMR 5516, F-42023, SAINT-ETIENNE, France",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zantedeschi_Metric_Learning_as_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 592294,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15953816770307081790&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "univ-st-etienne.fr;univ-st-etienne.fr;univ-st-etienne.fr",
        "email": "univ-st-etienne.fr;univ-st-etienne.fr;univ-st-etienne.fr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zantedeschi_Metric_Learning_as_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universite Lyon",
        "aff_unique_dep": "Institut d Optique Graduate School",
        "aff_unique_url": "https://www.univ-lyon.fr",
        "aff_unique_abbr": "Univ Lyon",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Saint-Etienne",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Min Norm Point Algorithm for Higher Order MRF-MAP Inference",
        "session": "Learning and Optimization",
        "status": "Spotlight",
        "track": "main",
        "pid": "8",
        "author_site": "Ishant Shanu, Chetan Arora, Parag Singla",
        "author": "Ishant Shanu; Chetan Arora; Parag Singla",
        "abstract": "Many tasks in computer vision and machine learning can be modelled as the inference problems in an MRF-MAP formulation and can be reduced to minimizing a submodular function. Using higher order clique potentials to model complex dependencies between pixels improves the performance but the current state of the art inference algorithms fail to scale for larger clique sizes. We adapt a well known Min Norm Point algorithm from mathematical optimization literature to exploit the sum of submodular structure found in the MRF-MAP formulation. Unlike some contemporary methods, we do not make any assumptions (other than submodularity) on the type of the clique potentials. Current state of the art inference algorithms for general submodular function takes many hours for problems with clique size 16, and fail to scale beyond. On the other hand, our algorithm is highly efficient and can perform optimal inference in few seconds even on clique size an order of magnitude larger. The proposed algorithm can even scale to clique sizes of many hundreds, unlocking the usage of really large size cliques for MRF-MAP inference problems in computer vision. We demonstrate the efficacy of our approach by experimenting on synthetic as well as real datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shanu_Min_Norm_Point_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 618337,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=196794908177626101&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shanu_Min_Norm_Point_CVPR_2016_paper.html"
    },
    {
        "title": "Minimizing the Maximal Rank",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "63",
        "author_site": "Erik Bylow, Carl Olsson, Fredrik Kahl, Mikael Nilsson",
        "author": "Erik Bylow; Carl Olsson; Fredrik Kahl; Mikael Nilsson",
        "abstract": "In computer vision, many problems can be formulated as finding a low rank approximation of a given measurement matrix. Ideally, if all elements of the measurement matrix are available, this is easily solved in the L2-norm using factorization. However, in practice this is rarely the case. Lately, this problem has been addressed using different approaches, one is to replace the rank term by the convex nuclear norm, another is to derive the convex envelope of the rank term plus a data term. In the latter case, matrices are divided into sub-matrices and the envelope is computed for each sub-block individually. In this paper a new convex envelope is derived which takes all sub-matrices into account simultaneously. This leads to a simpler formulation, using only one parameter, for applications where one seeks low rank approximations of multiple matrices with the same rank. We show in this paper how our general framework can be used for manifold denoising of several images at once, as well as just denoising one image. We get comparable results to other well-known methods and our framework can also be used for other applications such as linear shape models.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bylow_Minimizing_the_Maximal_CVPR_2016_paper.pdf",
        "aff": "Centre for Mathematical Sciences, Lund University, Sweden; Centre for Mathematical Sciences, Lund University, Sweden; Centre for Mathematical Sciences, Lund University, Sweden + Department of Signals and Systems, Chalmers University of Technology, Sweden; Centre for Mathematical Sciences, Lund University, Sweden",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 771927,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16095577961173449918&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "maths.lth.se;maths.lth.se;chalmers.se;maths.lth.se",
        "email": "maths.lth.se;maths.lth.se;chalmers.se;maths.lth.se",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bylow_Minimizing_the_Maximal_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "Lund University;Chalmers University of Technology",
        "aff_unique_dep": "Centre for Mathematical Sciences;Department of Signals and Systems",
        "aff_unique_url": "https://www.lunduniversity.lu.se;https://www.chalmers.se",
        "aff_unique_abbr": ";Chalmers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "title": "Mining 3D Key-Pose-Motifs for Action Recognition",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "41",
        "author_site": "Chunyu Wang, Yizhou Wang, Alan L. Yuille",
        "author": "Chunyu Wang; Yizhou Wang; Alan L. Yuille",
        "abstract": "Recognizing an action from a sequence of 3D skeletal poses is a challenging task. First, different actors may perform the same action in various performing styles. Second, the estimated poses are sometimes inaccurate due to sensory noises. These challenges can cause large variations between instances of the same class. Third, the datasets are usually small, with only a few actors performing few repetitions of each action. Hence training complex classifiers risks over-fitting the data. We address this task by mining a set of key-pose-motifs for each action class. A key-pose-motif contains a set of ordered  poses or  action units(a short sequence of poses), which are required to be close but not necessarily adjacent in the action sequences. The representation is robust to style variations and outlier poses. The key-pose-motifs are represented in terms of a dictionary using soft-quantization (probabilities) to deal with inaccuracies caused by quantization. We propose an efficient algorithm to mine key-pose-motifs taking into account these probabilities. We classify a sequence by matching it to the motifs of each class and select the class that maximizes the matching score. This simple classifier obtains state-of-the-art performance on two benchmark datasets and outperforms a deep network approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Mining_3D_Key-Pose-Motifs_CVPR_2016_paper.pdf",
        "aff": "Nat\u2019l Eng. Lab. for Video Technology, Cooperative Medianet Innovation Center, Key Lab. of Machine Perception (MoE), Sch\u2019l of EECS, Peking University, Beijing, 100871, China; Nat\u2019l Eng. Lab. for Video Technology, Cooperative Medianet Innovation Center, Key Lab. of Machine Perception (MoE), Sch\u2019l of EECS, Peking University, Beijing, 100871, China; Department of Statistics, University of California, Los Angeles (UCLA), USA + Departments of Cognitive Science and Computer Science, John Hopkins University, Baltimore, MD 21218",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1096514,
        "gs_citation": 105,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15907086723490342256&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "pku.edu.cn;pku.edu.cn;stat.ucla.edu",
        "email": "pku.edu.cn;pku.edu.cn;stat.ucla.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Mining_3D_Key-Pose-Motifs_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1+2",
        "aff_unique_norm": "Peking University;University of California, Los Angeles;John Hopkins University",
        "aff_unique_dep": "School of Electronics Engineering and Computer Science;Department of Statistics;Departments of Cognitive Science and Computer Science",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.ucla.edu;https://www.jhu.edu",
        "aff_unique_abbr": "Peking U;UCLA;JHU",
        "aff_campus_unique_index": "0;0;1+2",
        "aff_campus_unique": "Beijing;Los Angeles;Baltimore",
        "aff_country_unique_index": "0;0;1+1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Mining Discriminative Triplets of Patches for Fine-Grained Classification",
        "session": "Fine Grained Categorization",
        "status": "Poster",
        "track": "main",
        "pid": "43",
        "author_site": "Yaming Wang, Jonghyun Choi, Vlad Morariu, Larry S. Davis",
        "author": "Yaming Wang; Jonghyun Choi; Vlad Morariu; Larry S. Davis",
        "abstract": "Fine-grained classification involves distinguishing between similar sub-categories based on subtle differences in highly localized regions; therefore, accurate localization of discriminative regions remains a major challenge.  We describe a patch-based framework to address this problem. We introduce triplets of patches with geometric constraints to improve the accuracy of patch localization, and automatically mine discriminative geometrically-constrained triplets for classification. The resulting approach only requires object bounding boxes. Its effectiveness is demonstrated using four publicly available fine-grained datasets, on which it outperforms or obtains comparable results to the state-of-the-art in classification.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Mining_Discriminative_Triplets_CVPR_2016_paper.pdf",
        "aff": "University of Maryland, College Park, MD 20742, USA; Comcast Labs DC, Washington, DC 20005, USA; University of Maryland, College Park, MD 20742, USA; University of Maryland, College Park, MD 20742, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1551126,
        "gs_citation": 165,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13106690910087787996&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "email": "umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu;umiacs.umd.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Mining_Discriminative_Triplets_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Maryland;Comcast Labs",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;https://www.comcastlabs.com",
        "aff_unique_abbr": "UMD;Comcast Labs",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "College Park;Washington, DC",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Mirror Surface Reconstruction Under an Uncalibrated Camera",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "30",
        "author_site": "Kai Han, Kwan-Yee K. Wong, Dirk Schnieders, Miaomiao Liu",
        "author": "Kai Han; Kwan-Yee K. Wong; Dirk Schnieders; Miaomiao Liu",
        "abstract": "This paper addresses the problem of mirror surface reconstruction, and a solution based on observing the reflections of a moving reference plane on the mirror surface is proposed. Unlike previous approaches which require tedious work to calibrate the camera, our method can recover both the camera intrinsics and extrinsics together with the mirror surface from reflections of the reference plane under at least three unknown distinct poses. Our previous work has demonstrated that 3D poses of the reference plane can be registered in a common coordinate system using reflection correspondences established across images. This leads to a bunch of registered 3D lines formed from the reflection correspondences. Given these lines, we first derive an analytical solution to recover the camera projection matrix through estimating the line projection matrix. We then optimize the camera projection matrix by minimizing reprojection errors computed based on a cross-ratio formulation. The mirror surface is finally reconstructed based on the optimized cross-ratio constraint. Experimental results on both synthetic and real data are presented, which demonstrate the feasibility and accuracy of our method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Han_Mirror_Surface_Reconstruction_CVPR_2016_paper.pdf",
        "aff": "The University of Hong Kong; The University of Hong Kong; The University of Hong Kong; NICTA+CECS, ANU",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1831105,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4760974392059706694&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cs.hku.hk;cs.hku.hk;cs.hku.hk;nicta.com.au",
        "email": "cs.hku.hk;cs.hku.hk;cs.hku.hk;nicta.com.au",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Han_Mirror_Surface_Reconstruction_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1+2",
        "aff_unique_norm": "University of Hong Kong;National Information and Communications Technology Australia;Australian National University",
        "aff_unique_dep": ";;College of Engineering and Computer Science",
        "aff_unique_url": "https://www.hku.hk;https://www.nicta.com.au;https://www.anu.edu.au",
        "aff_unique_abbr": "HKU;NICTA;ANU",
        "aff_campus_unique_index": "0;0;0;",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;1+1",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Mixture of Bilateral-Projection Two-Dimensional Probabilistic Principal Component Analysis",
        "session": "Statistical Methods and Learning",
        "status": "Poster",
        "track": "main",
        "pid": "72",
        "author_site": "Fujiao Ju, Yanfeng Sun, Junbin Gao, Simeng Liu, Yongli Hu, Baocai Yin",
        "author": "Fujiao Ju; Yanfeng Sun; Junbin Gao; Simeng Liu; Yongli Hu; Baocai Yin",
        "abstract": "The probabilistic principal component analysis (PPCA) is built upon a global linear mapping, with which it is insufficient to model complex data variation. This paper proposes a mixture of bilateral-projection probabilistic principal component analysis model (mixB2DPPCA) on 2D data. With multi-components in the mixture, this model can be seen as a `soft' cluster algorithm and has capability of modeling data with complex structures. A Bayesian inference scheme has been proposed based on the variational EM (Expectation-Maximization) approach for learning model parameters.   Experiments on some publicly available databases show that the performance of mixB2DPPCA has been largely improved, resulting in more accurate reconstruction errors and recognition rates than the existing PCA-based algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ju_Mixture_of_Bilateral-Projection_CVPR_2016_paper.pdf",
        "aff": "College of Metropolitan Transportation, Beijing University of Technology; College of Metropolitan Transportation, Beijing University of Technology; Discipline of Business Analytics, The University of Sydney Business School, The University of Sydney; College of Metropolitan Transportation, Beijing University of Technology; College of Metropolitan Transportation, Beijing University of Technology; Faculty of Electronic Information and Electrical Engineering, College of Computer Science, Dalian University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 806422,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15885380012344418852&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "emails.bjut.edu.cn;emails.bjut.edu.cn;bjut.edu.cn;bjut.edu.cn;bjut.edu.cn;sydney.edu.au",
        "email": "emails.bjut.edu.cn;emails.bjut.edu.cn;bjut.edu.cn;bjut.edu.cn;bjut.edu.cn;sydney.edu.au",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ju_Mixture_of_Bilateral-Projection_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0;0;2",
        "aff_unique_norm": "Beijing University of Technology;University of Sydney;Dalian University of Technology",
        "aff_unique_dep": "College of Metropolitan Transportation;The University of Sydney Business School;Faculty of Electronic Information and Electrical Engineering, College of Computer Science",
        "aff_unique_url": "http://www.bjut.edu.cn;https://www.sydney.edu.au;http://en.dlut.edu.cn/",
        "aff_unique_abbr": "BJUT;USYD;DUT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Sydney",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Mnemonic Descent Method: A Recurrent Process Applied for End-To-End Face Alignment",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "42",
        "author_site": "George Trigeorgis, Patrick Snape, Mihalis A. Nicolaou, Epameinondas Antonakos, Stefanos Zafeiriou",
        "author": "George Trigeorgis; Patrick Snape; Mihalis A. Nicolaou; Epameinondas Antonakos; Stefanos Zafeiriou",
        "abstract": "Cascaded regression has recently become the method of choice for solving non-linear least squares problems such as deformable image alignment. Given a sizeable training set, cascaded regression learns a set of generic rules that are sequentially applied to minimise the least squares problem.  Despite the success of cascaded regression for problems such as face alignment and head pose estimation, there are several shortcomings arising in the strategies proposed thus far.  Specifically, (a) the regressors are learnt independently, (b) the descent directions may cancel one another out and  (c) handcrafted features (e.g., HoGs, SIFT etc.) are mainly used to drive the cascade, which may be sub-optimal for the task at hand. In this paper, we propose a combined and jointly trained convolutional recurrent neural network architecture that allows the training of an end-to-end to system that attempts to alleviate the aforementioned drawbacks. The recurrent module facilitates the joint optimisation of the regressors by assuming the cascades form a nonlinear dynamical system, in effect fully utilising the information between all cascade levels by introducing a memory unit that shares information across all levels. The convolutional module allows the network to extract features that are specialised for the task at hand and are experimentally shown to outperform hand-crafted features. We show that the application of the proposed architecture for the problem of face alignment results in a strong improvement over the current state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Trigeorgis_Mnemonic_Descent_Method_CVPR_2016_paper.pdf",
        "aff": "Department of Computing, Imperial College London, UK; Department of Computing, Imperial College London, UK; Department of Computing, Goldsmiths, University of London, UK; Department of Computing, Imperial College London, UK; Department of Computing, Imperial College London, UK + Center for Machine Vision and Signal Analysis, University of Oulu, Finland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Trigeorgis_Mnemonic_Descent_Method_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2915016,
        "gs_citation": 444,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12624673997919539116&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;gold.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;gold.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Trigeorgis_Mnemonic_Descent_Method_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0;0+2",
        "aff_unique_norm": "Imperial College London;University of London;University of Oulu",
        "aff_unique_dep": "Department of Computing;Department of Computing;Center for Machine Vision and Signal Analysis",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.gold.ac.uk;https://www.oulu.fi",
        "aff_unique_abbr": "Imperial;Goldsmiths;",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "London;Goldsmiths;",
        "aff_country_unique_index": "0;0;0;0;0+1",
        "aff_country_unique": "United Kingdom;Finland"
    },
    {
        "id": "319cd49659",
        "title": "Modality and Component Aware Feature Fusion For RGB-D Scene Classification",
        "site": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Modality_and_Component_CVPR_2016_paper.html",
        "author": "Anran Wang; Jianfei Cai; Jiwen Lu; Tat-Jen Cham",
        "abstract": "While convolutional neural networks (CNN) have been excellent for object recognition, the greater spatial variability in scene images typically meant that the standard full-image CNN features are suboptimal for scene classification. In this paper, we investigate a framework allowing greater spatial flexibility, in which the Fisher vector (FV) encoded distribution of local CNN features, obtained from a multitude of region proposals per image, is considered instead. The CNN features are computed from an augmented pixel-wise representation comprising multiple modalities of RGB, HHA and surface normals, as extracted from RGB-D data. More significantly, we make two postulates: (1) component sparsity --- that only a small variety of region proposals and their corresponding FV GMM components contribute to scene discriminability, and (2) modal non-sparsity --- within these discriminative components, all modalities have important contribution. In our framework, these are implemented through regularization terms applying group lasso to GMM components and exclusive group lasso across modalities. By learning and combining regressors for both proposal-based FV features and global CNN features, we were able to achieve state-of-the-art scene classification performance on the SUNRGBD Dataset and NYU Depth Dataset V2.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Modality_and_Component_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16544115645271038935&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4
    },
    {
        "title": "Monocular 3D Object Detection for Autonomous Driving",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "70",
        "author_site": "Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, Raquel Urtasun",
        "author": "Xiaozhi Chen; Kaustav Kundu; Ziyu Zhang; Huimin Ma; Sanja Fidler; Raquel Urtasun",
        "abstract": "The goal of this paper is to perform 3D object detection in single monocular images in the domain of autonomous driving.  Our method first aims to generate a set of candidate class-specific object proposals, which are then run through a standard CNN pipeline to obtain high-quality object detections. The focus of this paper is on proposal generation. In particular, we propose a probabilistic model that places object candidates in 3D using a prior on ground-plane. We then score each candidate box projected to the image plane via several intuitive potentials such as semantic segmentation, contextual information, size and location priors and typical object shape. The weights in our model are trained with S-SVM. Experiments show that our object proposal generation approach significantly outperforms all monocular baselines, and achieves the best detection performance on the challenging KITTI benchmark, among the published monocular competitors.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Monocular_3D_Object_CVPR_2016_paper.pdf",
        "aff": "Department of Electronic Engineering, Tsinghua University; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Electronic Engineering, Tsinghua University; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Chen_Monocular_3D_Object_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2049289,
        "gs_citation": 1263,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16308656757263045318&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "mails.tsinghua.edu.cn;cs.toronto.edu;cs.toronto.edu;tsinghua.edu.cn;cs.toronto.edu;cs.toronto.edu",
        "email": "mails.tsinghua.edu.cn;cs.toronto.edu;cs.toronto.edu;tsinghua.edu.cn;cs.toronto.edu;cs.toronto.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Monocular_3D_Object_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;0;1;1",
        "aff_unique_norm": "Tsinghua University;University of Toronto",
        "aff_unique_dep": "Department of Electronic Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.utoronto.ca",
        "aff_unique_abbr": "THU;U of T",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Toronto",
        "aff_country_unique_index": "0;1;1;0;1;1",
        "aff_country_unique": "China;Canada"
    },
    {
        "title": "Monocular Depth Estimation Using Neural Regression Forest",
        "session": "3D Reconstruction",
        "status": "Spotlight",
        "track": "main",
        "pid": "23",
        "author_site": "Anirban Roy, Sinisa Todorovic",
        "author": "Anirban Roy; Sinisa Todorovic",
        "abstract": "This paper presents a novel deep architecture, called  neural regression forest (NRF), for depth estimation from a single image. NRF combines random forests and convolutional neural networks (CNNs). Scanning windows extracted from the image represent samples which are passed down the trees of NRF for predicting their depth. At every tree node, the sample is filtered with a CNN associated with that node. Results of the convolutional filtering are passed to left and right children nodes, i.e., corresponding CNNs, with a Bernoulli probability, until the leaves, where depth estimations are made. CNNs at every node are designed to have fewer parameters than seen in recent work, but their stacked processing along a path in the tree effectively amounts to a deeper CNN. NRF allows for parallelizable training of all \"shallow\" CNNs, and efficient enforcing of smoothness in depth estimation results. Our evaluation on the benchmark Make3D and NYUv2 datasets demonstrates that NRF outperforms the state of the art, and gracefully handles gradually decreasing training datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Roy_Monocular_Depth_Estimation_CVPR_2016_paper.pdf",
        "aff": "Oregon State University; Oregon State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 928401,
        "gs_citation": 398,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14666124706581997232&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "eecs.oregonstate.edu;eecs.oregonstate.edu",
        "email": "eecs.oregonstate.edu;eecs.oregonstate.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Roy_Monocular_Depth_Estimation_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Moral Lineage Tracing",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "67",
        "author_site": "Florian Jug, Evgeny Levinkov, Corinna Blasse, Eugene W. Myers, Bjoern Andres",
        "author": "Florian Jug; Evgeny Levinkov; Corinna Blasse; Eugene W. Myers; Bjoern Andres",
        "abstract": "Lineage tracing, the tracking of living cells as they move and divide, is a central problem in biological image analysis. Solutions, called lineage forests, are key to understanding how the structure of multicellular organisms emerges. We propose an integer linear program (ILP) whose feasible solutions define, for every image in a sequence, a decomposition into cells (segmentation) and, across images, a lineage forest of cells (tracing). In this ILP, path-cut inequalities enforce the morality of lineages, i.e., the constraint that cells do not merge. To find feasible solutions of this NP-hard problem, with certified bounds to the global optimum, we define efficient separation procedures and apply these as part of a branch-and-cut algorithm. To show the effectiveness of this approach, we analyze feasible solutions for real microscopy data in terms of bounds and run-time, and by their weighted edit distance to lineage forests traced by humans.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Jug_Moral_Lineage_Tracing_CVPR_2016_paper.pdf",
        "aff": "Max Planck Institute of Molecular Cell Biology and Genetics, Dresden; Max Planck Institute for Informatics, Saarbr\u00fccken; Max Planck Institute of Molecular Cell Biology and Genetics, Dresden; Max Planck Institute of Molecular Cell Biology and Genetics, Dresden; Max Planck Institute for Informatics, Saarbr\u00fccken",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Jug_Moral_Lineage_Tracing_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 14935556,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17808667570582758062&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Jug_Moral_Lineage_Tracing_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;0;1",
        "aff_unique_norm": "Max Planck Institute of Molecular Cell Biology and Genetics;Max Planck Institute for Informatics",
        "aff_unique_dep": "Molecular Cell Biology and Genetics;",
        "aff_unique_url": "https://www.mpi-cbg.de;https://mpi-inf.mpg.de",
        "aff_unique_abbr": "MPI-CBG;MPII",
        "aff_campus_unique_index": "0;1;0;0;1",
        "aff_campus_unique": "Dresden;Saarbr\u00fccken",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Motion From Structure (MfS): Searching for 3D Objects in Cluttered Point Trajectories",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "37",
        "author_site": "Jayakorn Vongkulbhisal, Ricardo Cabral, Fernando De la Torre, Jo\u00e3o P. Costeira",
        "author": "Jayakorn Vongkulbhisal; Ricardo Cabral; Fernando De la Torre; Joao P. Costeira",
        "abstract": "Object detection has been a long standing problem in computer vision, and state-of-the-art approaches rely on the use of sophisticated features and/or classifiers. However, these learning-based approaches heavily depend on the quality and quantity of labeled data, and do not generalize well to extreme poses or textureless objects.   In this work, we explore the use of 3D shape models to detect objects in videos in an unsupervised manner. We call this problem Motion from Structure (MfS): given a set of point trajectories and a 3D model of the object of interest, find a subset of trajectories that correspond to the 3D model and estimate its alignment (i.e., compute the motion matrix). MfS is related to Structure from Motion (SfM) and motion segmentation problems: unlike SfM, the structure of the object is known but the correspondence between the trajectories and the object is unknown; unlike motion segmentation, the MfS problem incorporates 3D structure, providing robustness to tracking mismatches and outliers. Experiments illustrate how our MfS algorithm outperforms alternative approaches in both synthetic data and real videos extracted from YouTube.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Vongkulbhisal_Motion_From_Structure_CVPR_2016_paper.pdf",
        "aff": "ISR - Instituto Superior T\u00e9cnico, Lisboa, Portugal+Carnegie Mellon University, Pittsburgh, PA, USA; ISR - Instituto Superior T\u00e9cnico, Lisboa, Portugal+Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; ISR - Instituto Superior T\u00e9cnico, Lisboa, Portugal",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Vongkulbhisal_Motion_From_Structure_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2347357,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4205571196066746842&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "andrew.cmu.edu;cmu.edu;cs.cmu.edu;isr.ist.utl.pt",
        "email": "andrew.cmu.edu;cmu.edu;cs.cmu.edu;isr.ist.utl.pt",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Vongkulbhisal_Motion_From_Structure_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1;1;0",
        "aff_unique_norm": "Instituto Superior T\u00e9cnico;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www IST.pt;https://www.cmu.edu",
        "aff_unique_abbr": "ISR;CMU",
        "aff_campus_unique_index": "0+1;0+1;1;0",
        "aff_campus_unique": "Lisboa;Pittsburgh",
        "aff_country_unique_index": "0+1;0+1;1;0",
        "aff_country_unique": "Portugal;United States"
    },
    {
        "title": "MovieQA: Understanding Stories in Movies Through Question-Answering",
        "session": "High Level Semantics",
        "status": "Spotlight",
        "track": "main",
        "pid": "9",
        "author_site": "Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, Sanja Fidler",
        "author": "Makarand Tapaswi; Yukun Zhu; Rainer Stiefelhagen; Antonio Torralba; Raquel Urtasun; Sanja Fidler",
        "abstract": "We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 14,944 questions about 408 movies with high semantic diversity. The questions range from simpler \"Who\" did \"What\" to \"Whom\", to \"Why\" and \"How\" certain events occurred. Each question comes with a set of five possible answers; a correct one and four deceiving answers provided by human annotators. Our dataset is unique in that it contains multiple sources of information -- video clips, plots, subtitles, scripts, and DVS. We analyze our data through various statistics and methods. We further extend existing QA techniques to show that question-answering with such open-ended semantics is hard. We make this data set public along with an evaluation benchmark to encourage inspiring work in this challenging domain.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tapaswi_MovieQA_Understanding_Stories_CVPR_2016_paper.pdf",
        "aff": "Karlsruhe Institute of Technology; University of Toronto; Karlsruhe Institute of Technology; Massachusetts Institute of Technology; University of Toronto; University of Toronto",
        "project": "http://movieqa.cs.toronto.edu",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 955561,
        "gs_citation": 875,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9733167792031628523&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "kit.edu;cs.toronto.edu;kit.edu;csail.mit.edu;cs.toronto.edu;cs.toronto.edu",
        "email": "kit.edu;cs.toronto.edu;kit.edu;csail.mit.edu;cs.toronto.edu;cs.toronto.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tapaswi_MovieQA_Understanding_Stories_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;2;1;1",
        "aff_unique_norm": "Karlsruhe Institute of Technology;University of Toronto;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.kit.edu;https://www.utoronto.ca;https://web.mit.edu",
        "aff_unique_abbr": "KIT;U of T;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2;1;1",
        "aff_country_unique": "Germany;Canada;United States"
    },
    {
        "title": "Multi-Cue Zero-Shot Learning With Strong Supervision",
        "session": "Language and Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "7",
        "author_site": "Zeynep Akata, Mateusz Malinowski, Mario Fritz, Bernt Schiele",
        "author": "Zeynep Akata; Mateusz Malinowski; Mario Fritz; Bernt Schiele",
        "abstract": "Scaling up visual category recognition to large numbers of classes remains challenging. A promising research direction is zero-shot learning, which does not require any training data to recognize new classes, but rather relies on some form of auxiliary information describing the new classes. Ultimately, this may allow to use textbook knowledge that humans employ to learn about new classes by transferring knowledge from classes they know well. The most successful zero-shot learning approaches currently require a particular type of auxiliary information -- namely attribute annotations performed by humans -- that is not readily available for most classes. Our goal is to circumvent this bottleneck by substituting such annotations by extracting multiple pieces of information from multiple unstructured text sources readily available on the web. To compensate for the weaker form of auxiliary information, we incorporate stronger supervision in the form of semantic part annotations on the classes from which we transfer knowledge. We achieve our goal by a joint embedding framework that maps multiple text parts as well as multiple semantic parts into a common space. Our results consistently and significantly improve on the state-of-the-art in zero-short recognition and retrieval.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Akata_Multi-Cue_Zero-Shot_Learning_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 586288,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7329246890664906090&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Akata_Multi-Cue_Zero-Shot_Learning_CVPR_2016_paper.html"
    },
    {
        "title": "Multi-Label Ranking From Positive and Unlabeled Data",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "63",
        "author_site": "Atsushi Kanehira, Tatsuya Harada",
        "author": "Atsushi Kanehira; Tatsuya Harada",
        "abstract": "In this paper, we specifically examine the training of a multi-label classifier from data with incompletely assigned labels. This problem is fundamentally important in many multi-label applications because it is almost impossible for human annotators to assign a complete set of labels, although their judgments are reliable. In other words, a multi-label dataset usually has properties by which (1) assigned labels are definitely positive and (2) some labels are absent but are still considered positive. Such a setting has been studied as a positive and unlabeled (PU) classification problem in a binary setting. We treat incomplete label assignment problems as a multi-label PU ranking, which is an extension of classical binary PU problems to the well-studied rank-based multi-label classification. We derive the conditions that should be satisfied to cancel the negative effects of label incompleteness. Our experimentally obtained results demonstrate the effectiveness of these conditions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kanehira_Multi-Label_Ranking_From_CVPR_2016_paper.pdf",
        "aff": "The University of Tokyo; The University of Tokyo",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Kanehira_Multi-Label_Ranking_From_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1391409,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3353081263806676132&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp",
        "email": "mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kanehira_Multi-Label_Ranking_From_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Multi-Oriented Text Detection With Fully Convolutional Networks",
        "session": "Document Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "40",
        "author_site": "Zheng Zhang, Chengquan Zhang, Wei Shen, Cong Yao, Wenyu Liu, Xiang Bai",
        "author": "Zheng Zhang; Chengquan Zhang; Wei Shen; Cong Yao; Wenyu Liu; Xiang Bai",
        "abstract": "In this paper, we propose an unconventional approach for text detection in natural images. Both global and local cues are taken into account for localizing text lines in a coarse-to-fine procedure. First, a Fully Convolutional Network (FCN) model is trained for predicting a salient map of text regions in a holistic manner. Then, a set of hypotheses text lines are estimated by combining the salient map and MSER components. Finally, another FCN classifier is used for predicting the centroid of each character, in order to remove the false hypotheses. The framework is general for handling texts in multiple orientations, languages and fonts. The proposed method consistently achieves the state-of-the-art performance on three text detection benchmarks: MSRA-TD500, ICDAR2015, and ICDAR2013.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Multi-Oriented_Text_Detection_CVPR_2016_paper.pdf",
        "aff": "School of Electronic Information and Communications, Huazhong University of Science and Technology; School of Electronic Information and Communications, Huazhong University of Science and Technology; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai University; School of Electronic Information and Communications, Huazhong University of Science and Technology; School of Electronic Information and Communications, Huazhong University of Science and Technology; School of Electronic Information and Communications, Huazhong University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhang_Multi-Oriented_Text_Detection_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1277900,
        "gs_citation": 740,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4660690942997100394&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "gmail.com;gmail.com;hust.edu.cn; ; ; ",
        "email": "gmail.com;gmail.com;hust.edu.cn; ; ; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Multi-Oriented_Text_Detection_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "Huazhong University of Science and Technology;Shanghai University",
        "aff_unique_dep": "School of Electronic Information and Communications;Key Laboratory of Specialty Fiber Optics and Optical Access Networks",
        "aff_unique_url": "http://www.hust.edu.cn;https://www.shu.edu.cn",
        "aff_unique_abbr": "HUST;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-Scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation",
        "session": "Semantic Segmentation",
        "status": "Oral",
        "track": "main",
        "pid": "12",
        "author_site": "Shu Liu, Xiaojuan Qi, Jianping Shi, Hong Zhang, Jiaya Jia",
        "author": "Shu Liu; Xiaojuan Qi; Jianping Shi; Hong Zhang; Jiaya Jia",
        "abstract": "Aiming at simultaneous detection and segmentation (SDS), we propose a proposal-free framework, which detect and segment object instances via mid-level patches. We design a unified trainable network on patches, which is followed by a fast and effective patch aggregation algorithm to infer object instances. Our method benefits from end-to-end training. Without object proposal generation, computation time can also be reduced. In experiments, our method yields results 62.1% and 61.8% in terms of mAPr on VOC2012 segmentation val and VOC2012 SDS val, which are state-of-the-art at the time of submission. We also report results on Microsoft COCO test-std/test-dev dataset in this paper.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Multi-Scale_Patch_Aggregation_CVPR_2016_paper.pdf",
        "aff": "The Chinese University of Hong Kong; The Chinese University of Hong Kong; SenseTime Group Limited; The Chinese University of Hong Kong; The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1410737,
        "gs_citation": 110,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12168070664668095635&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;sensetime.com;cse.cuhk.edu.hk;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;sensetime.com;cse.cuhk.edu.hk;cse.cuhk.edu.hk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liu_Multi-Scale_Patch_Aggregation_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong;SenseTime Group Limited",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.sensetime.com",
        "aff_unique_abbr": "CUHK;SenseTime",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-View Deep Network for Cross-View Classification",
        "session": "Face Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "32",
        "author_site": "Meina Kan, Shiguang Shan, Xilin Chen",
        "author": "Meina Kan; Shiguang Shan; Xilin Chen",
        "abstract": "Cross-view recognition that intends to classify samples between different views is an important problem in computer vision. The large discrepancy between different even heterogenous views make this problem quite challenging. To eliminate the complex (maybe even highly nonlinear) view discrepancy for favorable cross-view recognition, we propose a multi-view deep network (MvDN), which seeks for a non-linear discriminant and view-invariant representation shared between multiple views. Specifically, our proposed MvDN network consists of two sub-networks, view-specific sub-network attempting to remove view-specific variations and the following common sub-network attempting to obtain common representation shared by all views. As the objective of MvDN network, the Fisher loss, i.e. the Rayleigh quotient objective, is calculated from the samples of all views so as to guide the learning of the whole network. As a result, the representation from the topmost layers of the MvDN network is robust to view discrepancy, and also discriminative. The experiments of face recognition across pose and face recognition across feature type on three datasets with 13 and 2 views respectively demonstrate the superiority of the proposed method, especially compared to the typical linear ones.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kan_Multi-View_Deep_Network_CVPR_2016_paper.pdf",
        "aff": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + CAS Center for Excellence in Brain Science and Intelligence Technology; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + CAS Center for Excellence in Brain Science and Intelligence Technology; Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing, 100190, China + CAS Center for Excellence in Brain Science and Intelligence Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 797383,
        "gs_citation": 214,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13580686416553975049&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kan_Multi-View_Deep_Network_CVPR_2016_paper.html",
        "aff_unique_index": "0+0;0+0;0+0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology",
        "aff_unique_url": "http://www.cas.ac.cn",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multi-View People Tracking via Hierarchical Trajectory Composition",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "50",
        "author_site": "Yuanlu Xu, Xiaobai Liu, Yang Liu, Song-Chun Zhu",
        "author": "Yuanlu Xu; Xiaobai Liu; Yang Liu; Song-Chun Zhu",
        "abstract": "This paper presents a hierarchical composition approach for multi-view object tracking. The key idea is to adaptively exploit multiple cues in both 2D and 3D, e.g., ground occupancy consistency, appearance similarity, motion coherence etc., which are mutually complementary while tracking the humans of interests over time. While feature online selection has been extensively studied in the past literature, it remains unclear how to effectively schedule these cues for the tracking purpose especially when encountering various challenges, e.g. occlusions, conjunctions, and appearance variations. To do so, we propose a hierarchical composition model and re-formulate multi-view multi-object tracking as a problem of compositional structure optimization. We setup a set of composition criteria, each of which corresponds to one particular cue. The hierarchical composition process is pursued by exploiting different criteria, which impose constraints between a graph node and its offsprings in the hierarchy. We learn the composition criteria using MLE on annotated data and efficiently construct the hierarchical graph by an iterative greedy pursuit algorithm. In the experiments, we demonstrate superior performance of our approach on three public datasets, one of which is newly created by us to test various challenges in multi-view multi-object tracking.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_Multi-View_People_Tracking_CVPR_2016_paper.pdf",
        "aff": "Dept. Computer Science and Statistics, University of California, Los Angeles (UCLA); Dept. Computer Science, San Diego State University (SDSU); Dept. Computer Science and Statistics, University of California, Los Angeles (UCLA); Dept. Computer Science and Statistics, University of California, Los Angeles (UCLA)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3879983,
        "gs_citation": 184,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3108177853883919562&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.ucla.edu;mail.sdsu.edu;ucla.edu;stat.ucla.edu",
        "email": "cs.ucla.edu;mail.sdsu.edu;ucla.edu;stat.ucla.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xu_Multi-View_People_Tracking_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of California, Los Angeles;San Diego State University",
        "aff_unique_dep": "Dept. Computer Science and Statistics;Department of Computer Science",
        "aff_unique_url": "https://www.ucla.edu;https://www.sdsu.edu",
        "aff_unique_abbr": "UCLA;SDSU",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Los Angeles;San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multicamera Calibration From Visible and Mirrored Epipoles",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "37",
        "author_site": "Andrey Bushnevskiy, Lorenzo Sorgi, Bodo Rosenhahn",
        "author": "Andrey Bushnevskiy; Lorenzo Sorgi; Bodo Rosenhahn",
        "abstract": "Multicamera rigs are used in a large number of 3D Vision applications, such as 3D modeling, motion capture or telepresence and a robust calibration is of utmost importance in order to achieve a high accuracy results. In many practical configurations the cameras in a rig are arranged in such a way, that they can observe each other, in other words a number of epipoles correspond to the real image points. In this paper we propose a solution for the automatic recovery of the external calibration of a multicamera system by enforcing only simple geometrical constraints, arising from the epipole visibility, without using any calibration object, such as checkerboards, laser pointers or similar. Additionally, we introduce an extension of the method that handles the case of epipoles being visible in the reflection of a planar mirror, which makes the algorithm suitable for the calibration of any multicamera system, irrespective of the number of cameras and their actual mutual visibility, and furthermore we remark that it requires only one or a few images per camera and therefore features a high speed and usability. We produce an evidence of the algorithm effectiveness by presenting a wide set of tests performed on synthetic as well as real datasets and we compare the results with those obtained using a traditional LED-based algorithm. The real datasets have been captured using a multicamera Virtual Reality (VR) rig and a spherical dome configuration for 3D reconstruction.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bushnevskiy_Multicamera_Calibration_From_CVPR_2016_paper.pdf",
        "aff": "Technicolor Research & Innovation; Technicolor Research & Innovation; Leibniz University Hannover",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Bushnevskiy_Multicamera_Calibration_From_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1289082,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1202305304230808430&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "; ; ",
        "email": "; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bushnevskiy_Multicamera_Calibration_From_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Technicolor;Leibniz University Hannover",
        "aff_unique_dep": "Research & Innovation;",
        "aff_unique_url": "https://www.technicolor.com/en;https://www.leibniz.uni-hannover.de",
        "aff_unique_abbr": "Technicolor;LUH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "France;Germany"
    },
    {
        "title": "Multilinear Hyperplane Hashing",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "61",
        "author_site": "Xianglong Liu, Xinjie Fan, Cheng Deng, Zhujin Li, Hao Su, Dacheng Tao",
        "author": "Xianglong Liu; Xinjie Fan; Cheng Deng; Zhujin Li; Hao Su; Dacheng Tao",
        "abstract": "Hashing has become an increasingly popular technique for fast nearest neighbor search in large databases. Despite its successful progress in classic point-to-point search, there are few studies regarding point-to-hyperplane search, which has strong practical capabilities of scaling up in many applications like active learning with SVMs. Existing hyperplane hashing methods enable the fast search based on the randomly generated hash codes, but still suffer from a low collision probability and thus usually require long codes for a satisfying performance. To overcome this problem, this paper proposes a multilinear hyperplane hashing that generates a hash bit using multiple linear projections. Our theoretical analysis shows that as a product of an even number of random linear projections, the multilinear hash function possesses an increasing power of locality sensitivity to the hyperplane queries. To leverage its sensitivity to the angle distance, we further introduce an angular quantization based learning framework for compact multilinear hashing, which considerably boosts the search performance with less hash bits. Experiments with applications to large-scale (up to one million) active learning on two datasets demonstrate the overall superiority of the proposed approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Multilinear_Hyperplane_Hashing_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Liu_Multilinear_Hyperplane_Hashing_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 840319,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11115944543944637229&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liu_Multilinear_Hyperplane_Hashing_CVPR_2016_paper.html"
    },
    {
        "title": "Multimodal Spontaneous Emotion Corpus for Human Behavior Analysis",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "44",
        "author_site": "Zheng Zhang, Jeff M. Girard, Yue Wu, Xing Zhang, Peng Liu, Umur Ciftci, Shaun Canavan, Michael Reale, Andy Horowitz, Huiyuan Yang, Jeffrey F. Cohn, Qiang Ji, Lijun Yin",
        "author": "Zheng Zhang; Jeff M. Girard; Yue Wu; Xing Zhang; Peng Liu; Umur Ciftci; Shaun Canavan; Michael Reale; Andy Horowitz; Huiyuan Yang; Jeffrey F. Cohn; Qiang Ji; Lijun Yin",
        "abstract": "Emotion is expressed in multiple modalities, yet most research has considered at most one or two. This stems in part from the lack of large, diverse, well-annotated, multimodal databases with which to develop and test algorithms. We present a well-annotated, multimodal, multidimensional spontaneous emotion corpus of 140 participants. Emotion inductions were highly varied. Data were acquired from a variety of sensors of the face that included high-resolution 3D dynamic imaging, high-resolution 2D video, and thermal (infrared) sensing, and contact physiological sensors that included electrical conductivity of the skin, respiration, blood pressure, and heart rate. Facial expression was annotated for both the occurrence and intensity of facial action units from 2D video by experts in the Facial Action Coding System (FACS). The corpus further includes derived features from 3D, 2D, and IR (infrared) sensors and baseline results for facial expression and action unit detection. The entire corpus will be made available to the research community.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Multimodal_Spontaneous_Emotion_CVPR_2016_paper.pdf",
        "aff": "Binghamton University; University of Pittsburgh; Rensselaer Polytechnic Institute; Binghamton University; Binghamton University; Binghamton University; Binghamton University; Binghamton University; Binghamton University; Binghamton University; University of Pittsburgh; Rensselaer Polytechnic Institute; Binghamton University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 598335,
        "gs_citation": 521,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17415432229106953498&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "cs.binghamton.edu;pitt.edu;rpi.edu; ; ; ; ;sunyit.edu; ; ; ; ;",
        "email": "cs.binghamton.edu;pitt.edu;rpi.edu; ; ; ; ;sunyit.edu; ; ; ; ;",
        "author_num": 13,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Multimodal_Spontaneous_Emotion_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;0;0;0;0;0;0;0;1;2;0",
        "aff_unique_norm": "Binghamton University;University of Pittsburgh;Rensselaer Polytechnic Institute",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.binghamton.edu;https://www.pitt.edu;https://www.rpi.edu",
        "aff_unique_abbr": "Binghamton;Pitt;RPI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Multiple Model Fitting as a Set Coverage Problem",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "31",
        "author_site": "Luca Magri, Andrea Fusiello",
        "author": "Luca Magri; Andrea Fusiello",
        "abstract": "This paper deals with the extraction of multiple models from noisy or outlier-contaminated data.  We cast the multi-model fitting problem in terms of set covering, deriving a simple and effective method that  generalizes Ransac to multiple models and  deals with intersecting  structures and outliers in a straightforward and principled manner, while avoiding the typical shortcomings of sequential approaches and those of clustering. The method compares favourably against the state-of-the-art on simulated and publicly available real datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Magri_Multiple_Model_Fitting_CVPR_2016_paper.pdf",
        "aff": "Computer Science Dept. - University of Verona; DPIA - University of Udine",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3255593,
        "gs_citation": 102,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18049836263306999976&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "gmail.com;uniud.it",
        "email": "gmail.com;uniud.it",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Magri_Multiple_Model_Fitting_CVPR_2016_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Verona;University of Udine",
        "aff_unique_dep": "Computer Science Dept.;DPIA",
        "aff_unique_url": "https://www.univr.it;https://www.uniud.it",
        "aff_unique_abbr": "UniVR;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy"
    },
    {
        "title": "Multispectral Images Denoising by Intrinsic Tensor Sparsity Regularization",
        "session": "Image Processing and Restoration",
        "status": "Spotlight",
        "track": "main",
        "pid": "21",
        "author_site": "Qi Xie, Qian Zhao, Deyu Meng, Zongben Xu, Shuhang Gu, Wangmeng Zuo, Lei Zhang",
        "author": "Qi Xie; Qian Zhao; Deyu Meng; Zongben Xu; Shuhang Gu; Wangmeng Zuo; Lei Zhang",
        "abstract": "Multispectral images (MSI) can help deliver more faithful representation for real scenes than the traditional image system, and enhance the performance of many computer vision tasks. In real cases, however, an MSI is always corrupted by various noises. In this paper, we propose a new tensor-based denoising approach by fully considering two intrinsic characteristics underlying an MSI, i.e., the global correlation along spectrum (GCS) and nonlocal self-similarity across space (NSS). In specific, we construct a new tensor sparsity measure, called intrinsic tensor sparsity (ITS) measure, which encodes both sparsity insights delivered by the most typical Tucker and CANDECOMP/PARAFAC (CP) low-rank decomposition for a general tensor. Then we build a new MSI denoising model by applying the proposed ITS measure on tensors formed by non-local similar patches within the MSI. The intrinsic GCS and NSS knowledge can then be efficiently explored under the regularization of this tensor sparsity measure to finely rectify the recovery of a MSI from its corruption. A series of experiments on simulated and real MSI denoising problems show that our method outperforms all state-of-the-arts under comprehensive quantitative performance measures.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_Multispectral_Images_Denoising_CVPR_2016_paper.pdf",
        "aff": "Xi\u2019an Jiaotong University; Xi\u2019an Jiaotong University; Xi\u2019an Jiaotong University; Xi\u2019an Jiaotong University; The Hong Kong Polytechnic University; Harbin Institute of Technology; The Hong Kong Polytechnic University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Xie_Multispectral_Images_Denoising_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2986903,
        "gs_citation": 280,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2654180268617810922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "stu.xjtu.edu.cn;gmail.com;mail.xjtu.edu.cn;mail.xjtu.edu.cn;gmail.com;hit.edu.cn;comp.polyu.edu.hk",
        "email": "stu.xjtu.edu.cn;gmail.com;mail.xjtu.edu.cn;mail.xjtu.edu.cn;gmail.com;hit.edu.cn;comp.polyu.edu.hk",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xie_Multispectral_Images_Denoising_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;1;2;1",
        "aff_unique_norm": "Xi'an Jiao Tong University;Hong Kong Polytechnic University;Harbin Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.xjtu.edu.cn;https://www.polyu.edu.hk;http://www.hit.edu.cn/",
        "aff_unique_abbr": "XJTU;PolyU;HIT",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Hong Kong SAR;Harbin",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Multivariate Regression on the Grassmannian for Predicting Novel Domains",
        "session": "Transfer Learning",
        "status": "Poster",
        "track": "main",
        "pid": "56",
        "author_site": "Yongxin Yang, Timothy M. Hospedales",
        "author": "Yongxin Yang; Timothy M. Hospedales",
        "abstract": "We study the problem of predicting how to recognise visual objects in novel domains with neither labelled nor unlabelled training data. Domain adaptation is now an established research area due to its value in ameliorating the issue of domain shift between train and test data. However, it is conventionally assumed that domains are discrete entities, and that at least unlabelled data is provided in testing domains. In this paper, we consider the case where domains are parametrised by a vector of continuous values (e.g., time, lighting or view angle). We aim to use such domain metadata to predict novel domains for recognition. This allows a recognition model to be pre-calibrated for a new domain in advance (e.g., future time or view angle) without waiting for data collection and re-training. We achieve this by posing the problem as one of multivariate regression on the Grassmannian, where we regress a domain's subspace (point on the Grassmannian) against an independent vector of domain parameters. We derive two novel methodologies to achieve this challenging task: a direct kernel regression, and an indirect method with better extrapolation properties. We evaluate our methods on two cross-domain visual recognition benchmarks, where they perform close to the upper bound of full data domain adaptation. This demonstrates that data is not necessary for domain adaptation if a domain can be parametrically described.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_Multivariate_Regression_on_CVPR_2016_paper.pdf",
        "aff": "Queen Mary, University of London; Queen Mary, University of London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1127353,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18361072385130426504&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "qmul.ac.uk;qmul.ac.uk",
        "email": "qmul.ac.uk;qmul.ac.uk",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_Multivariate_Regression_on_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Queen Mary, University of London",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.qmul.ac.uk",
        "aff_unique_abbr": "QMUL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Multiview Image Completion With Space Structure Propagation",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "52",
        "author_site": "Seung-Hwan Baek, Inchang Choi, Min H. Kim",
        "author": "Seung-Hwan Baek; Inchang Choi; Min H. Kim",
        "abstract": "We present a multiview image completion method that provides geometric consistency among different views by propagating space structures. Since a user specifies the region to be completed in one of multiview photographs casually taken in a scene, the proposed method enables us to complete the set of photographs with geometric consistency by creating or removing structures on the specified region. The proposed method incorporates photographs to estimate dense depth maps. We initially complete color as well as depth from a view, and then facilitate two stages of structure propagation and structure-guided completion. Structure propagation optimizes space topology in the scene across photographs, while structure-guide completion enhances, and completes local image structure of both depth and color in multiple photographs with structural coherence by searching nearest neighbor fields in relevant views. We demonstrate the effectiveness of the proposed method in completing multiview images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Baek_Multiview_Image_Completion_CVPR_2016_paper.pdf",
        "aff": "Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST); Korea Advanced Institute of Science and Technology (KAIST)",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Baek_Multiview_Image_Completion_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2408133,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12257172444841157833&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff_domain": "vclab.kaist.ac.kr;vclab.kaist.ac.kr;vclab.kaist.ac.kr",
        "email": "vclab.kaist.ac.kr;vclab.kaist.ac.kr;vclab.kaist.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Baek_Multiview_Image_Completion_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis",
        "session": "Events, Activities, and Surveillance",
        "status": "Poster",
        "track": "main",
        "pid": "27",
        "author_site": "Amir Shahroudy, Jun Liu, Tian-Tsong Ng, Gang Wang",
        "author": "Amir Shahroudy; Jun Liu; Tian-Tsong Ng; Gang Wang",
        "abstract": "Recent approaches in depth-based human activity analysis achieved outstanding performance and proved the effectiveness of 3D representation for classification of action classes. Currently available depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of training samples, distinct class labels, camera views and variety of subjects. In this paper we introduce a large-scale dataset for RGB+D human action recognition with more than 56 thousand video samples and 4 million frames, collected from 40 distinct subjects. Our dataset contains 60 different action classes including daily, mutual, and health-related actions. In addition, we propose a new recurrent neural network structure to model the long-term temporal correlation of the features for each body part, and utilize them for better action classification. Experimental results show the advantages of applying deep learning methods over state-of-the-art hand-crafted features on the suggested cross-subject and cross-view evaluation criteria for our dataset. The introduction of this large scale dataset will enable the community to apply, develop and adapt various data-hungry learning techniques for the task of depth-based and RGB+D-based human activity analysis.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shahroudy_NTU_RGBD_A_CVPR_2016_paper.pdf",
        "aff": "School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore+Institute for Infocomm Research, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Institute for Infocomm Research, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1377626,
        "gs_citation": 3655,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10407150499851480949&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;i2r.a-star.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg;i2r.a-star.edu.sg;ntu.edu.sg",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shahroudy_NTU_RGBD_A_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;1;0",
        "aff_unique_norm": "Nanyang Technological University;Institute for Infocomm Research",
        "aff_unique_dep": "School of Electrical and Electronic Engineering;",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "NTU;I2R",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Singapore;",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Natural Language Object Retrieval",
        "session": "Image & Video Captioning and Descriptions",
        "status": "Oral",
        "track": "main",
        "pid": "1",
        "author_site": "Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, Trevor Darrell",
        "author": "Ronghang Hu; Huazhe Xu; Marcus Rohrbach; Jiashi Feng; Kate Saenko; Trevor Darrell",
        "abstract": "In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hu_Natural_Language_Object_CVPR_2016_paper.pdf",
        "aff": "University of California, Berkeley; Tsinghua University; University of California, Berkeley + ICSI, Berkeley; National University of Singapore; University of Massachusetts, Lowell; University of California, Berkeley",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Hu_Natural_Language_Object_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1675994,
        "gs_citation": 664,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14726251364509932278&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "eecs.berkeley.edu;mails.tsinghua.edu.cn;eecs.berkeley.edu;nus.edu.sg;cs.uml.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;mails.tsinghua.edu.cn;eecs.berkeley.edu;nus.edu.sg;cs.uml.edu;eecs.berkeley.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hu_Natural_Language_Object_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0+2;3;4;0",
        "aff_unique_norm": "University of California, Berkeley;Tsinghua University;International Computer Science Institute;National University of Singapore;University of Massachusetts Lowell",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.tsinghua.edu.cn;https://icsi.berkeley.edu;https://www.nus.edu.sg;https://www.uml.edu",
        "aff_unique_abbr": "UC Berkeley;THU;ICSI;NUS;UMass Lowell",
        "aff_campus_unique_index": "0;0+0;2;0",
        "aff_campus_unique": "Berkeley;;Lowell",
        "aff_country_unique_index": "0;1;0+0;2;0;0",
        "aff_country_unique": "United States;China;Singapore"
    },
    {
        "title": "Needle-Match: Reliable Patch Matching Under High Uncertainty",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "47",
        "author_site": "Or Lotan, Michal Irani",
        "author": "Or Lotan; Michal Irani",
        "abstract": "Reliable patch-matching forms the basis for many algorithms (super-resolution, denoising, inpainting, etc.) However, when the image quality deteriorates (by noise, blur or  geometric distortions), the reliability of patch-matching deteriorates as well. Matched patches in the degraded image, do not necessarily imply similarity of the underlying patches in the (unknown) high-quality image. This restricts the applicability of patch-based methods. In this paper we present a patch representation called \"Needle\", which consists of small multi-scale versions of the patch and its immediate surrounding region. While the patch at the finest image scale is severely degraded, the degradation decreases dramatically in coarser needle scales, revealing reliable information for matching. We show that the Needle is  robust to many types of image degradations, leads to matches faithful to the underlying high-quality patches, and to improvement in existing patch-based methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lotan_Needle-Match_Reliable_Patch_CVPR_2016_paper.pdf",
        "aff": "Dept. of Computer Science and Applied Mathematics, The Weizmann Institute of Science, ISRAEL; Dept. of Computer Science and Applied Mathematics, The Weizmann Institute of Science, ISRAEL",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1926035,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3842978303138908552&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lotan_Needle-Match_Reliable_Patch_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Weizmann Institute of Science",
        "aff_unique_dep": "Dept. of Computer Science and Applied Mathematics",
        "aff_unique_url": "https://www.weizmann.ac.il",
        "aff_unique_abbr": "Weizmann",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "NetVLAD: CNN Architecture for Weakly Supervised Place Recognition",
        "session": "Learning and CNN Architectures",
        "status": "Oral",
        "track": "main",
        "pid": "1",
        "author_site": "Relja Arandjelovi\u0107, Petr Gronat, Akihiko Torii, Tomas Pajdla, Josef Sivic",
        "author": "Relja Arandjelovic; Petr Gronat; Akihiko Torii; Tomas Pajdla; Josef Sivic",
        "abstract": "We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the \"Vector of Locally Aggregated Descriptors\" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.pdf",
        "aff": "INRIA*; INRIA*; Tokyo Tech\u2020; CTU in Prague\u2021; INRIA*",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1330990,
        "gs_citation": 3700,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6182379016997971259&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 30,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "INRIA;Tokyo Institute of Technology;Czech Technical University in Prague",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.inria.fr;https://www.titech.ac.jp;https://www.ctu.cz",
        "aff_unique_abbr": "INRIA;Tokyo Tech;CTU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Prague",
        "aff_country_unique_index": "0;0;1;2;0",
        "aff_country_unique": "France;Japan;Czech Republic"
    },
    {
        "title": "Neural Module Networks",
        "session": "Image Captioning and Question Answering",
        "status": "Oral",
        "track": "main",
        "pid": "5",
        "author_site": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein",
        "author": "Jacob Andreas; Marcus Rohrbach; Trevor Darrell; Dan Klein",
        "abstract": "Visual question answering is fundamentally compositional in nature---a question like \"where is the dog?\" shares substructure with questions like \"what color is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions.  We describe a procedure for constructing and learning _neural module networks_, which compose collections of jointly-trained neural \"modules\" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Andreas_Neural_Module_Networks_CVPR_2016_paper.pdf",
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 619649,
        "gs_citation": 1501,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8968885959793783275&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "email": "eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu;eecs.berkeley.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Andreas_Neural_Module_Networks_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Newtonian Scene Understanding: Unfolding the Dynamics of Objects in Static Images",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "53",
        "author_site": "Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi",
        "author": "Roozbeh Mottaghi; Hessam Bagherinezhad; Mohammad Rastegari; Ali Farhadi",
        "abstract": "In this paper, we study the challenging problem of predicting the dynamics of objects in static images. Given a query object in an image, our goal is to provide a physical understanding of the object in terms of the forces acting upon it and its long term motion as response to those forces. Direct and explicit estimation of the forces and the motion of objects from a single image is extremely challenging. We define intermediate physical abstractions called Newtonian scenarios and introduce Newtonian Neural Network (N^3) that learns to map a single image to a state in a Newtonian scenario. Our experimental evaluations show that our method can reliably predict dynamics of a query object from a single image. In addition, our approach can provide physical reasoning that supports the predicted dynamics in terms of velocity and force vectors. To spur research in this direction we compiled Visual Newtonian Dynamics (VIND) dataset that includes more than 6000 videos aligned with Newtonian scenarios represented using game engines, and more than 4500 still images with their ground truth dynamics.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mottaghi_Newtonian_Scene_Understanding_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Mottaghi_Newtonian_Scene_Understanding_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1854895,
        "gs_citation": 183,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4949176770230105466&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mottaghi_Newtonian_Scene_Understanding_CVPR_2016_paper.html"
    },
    {
        "title": "Noisy Label Recovery for Shadow Detection in Unfamiliar Domains",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "81",
        "author_site": "Tom\u00e1s F. Yago Vicente, Minh Hoai, Dimitris Samaras",
        "author": "Tomas F. Yago Vicente; Minh Hoai; Dimitris Samaras",
        "abstract": "Recent shadow detection algorithms have shown initial success on small datasets of images from specific domains. However, shadow detection on broader image domains is still challenging due to the lack of annotated training data. This is due to the intense manual labor in annotating shadow data. In this paper we propose \"lazy annotation\", an efficient annotation method where an annotator only needs to mark the important shadow areas and some non-shadow areas. This yields data with noisy labels that are not yet useful for training a shadow detector. We address the problem of label noise by jointly learning a shadow region classifier and recovering the labels in the training set. We consider the training labels as unknowns and formulate the label recovery problem as the minimization of the sum of squared leave-one-out errors of a Least Squares SVM, which can be efficiently optimized. Experimental results show that a classifier trained with recovered labels achieves comparable performance to a classifier trained on the properly annotated data. These results suggest a feasible approach to address the task of detecting shadows in an unfamiliar domain: collecting and lazily annotating some images from the new domain for training. As will be demonstrated, this approach outperforms methods that rely on precisely annotated but less relevant datasets. Initial results suggest more general applicability.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Vicente_Noisy_Label_Recovery_CVPR_2016_paper.pdf",
        "aff": "Stony Brook University, Stony Brook, NY 11794, USA; Stony Brook University, Stony Brook, NY 11794, USA; Stony Brook University, Stony Brook, NY 11794, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2018863,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=507302830846778968&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu;cs.stonybrook.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Vicente_Noisy_Label_Recovery_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stony Brook University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stonybrook.edu",
        "aff_unique_abbr": "SBU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stony Brook",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Non-Local Image Dehazing",
        "session": "Image Processing and Restoration",
        "status": "Spotlight",
        "track": "main",
        "pid": "19",
        "author_site": "Dana Berman, Tali treibitz, Shai Avidan",
        "author": "Dana Berman; Tali treibitz; Shai Avidan",
        "abstract": "Haze limits visibility and reduces image contrast in outdoor images. The degradation is different for every pixel and depends on the distance of the scene point from the camera. This dependency is expressed in the transmission coefficients, that control the scene attenuation and amount of haze in every pixel. Previous methods solve the single image dehazing problem using various patch-based priors. We, on the other hand, propose an algorithm based on a new, non-local prior. The algorithm relies on the assumption that colors of a haze-free image are well approximated by a few hundred distinct colors, that form tight clusters in RGB space. Our key observation is that pixels in a given cluster are often non-local, i.e., they are spread over the entire image plane and are located at different distances from the camera. In the presence of haze these varying distances translate to different transmission coefficients. Therefore, each color cluster in the clear image becomes a line in RGB space, that we term a haze-line. Using these haze-lines, our algorithm recovers both the distance map and the haze-free image. The algorithm is linear in the size of the image, deterministic and requires no training. It performs well on a wide variety of images and is competitive with other state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Berman_Non-Local_Image_Dehazing_CVPR_2016_paper.pdf",
        "aff": "Tel Aviv University; University of Haifa; Tel Aviv University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Berman_Non-Local_Image_Dehazing_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 6163547,
        "gs_citation": 1812,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2607954299947683842&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "post.tau.ac.il;univ.haifa.ac.il;eng.tau.ac.il",
        "email": "post.tau.ac.il;univ.haifa.ac.il;eng.tau.ac.il",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Berman_Non-Local_Image_Dehazing_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Tel Aviv University;University of Haifa",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tau.ac.il;https://www.haifa.ac.il",
        "aff_unique_abbr": "TAU;UoH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Object Co-Segmentation via Graph Optimized-Flexible Manifold Ranking",
        "session": "Segmentation and Saliency",
        "status": "Poster",
        "track": "main",
        "pid": "74",
        "author_site": "Rong Quan, Junwei Han, Dingwen Zhang, Feiping Nie",
        "author": "Rong Quan; Junwei Han; Dingwen Zhang; Feiping Nie",
        "abstract": "Aiming at automatically discovering the common objects contained in a set of relevant images and segmenting them as foreground simultaneously, object co-segmentation has become an active research topic in recent years. Although a number of approaches have been proposed to address this problem, many of them are designed with the misleading assumption, unscalable prior, or low flexibility and thus still suffer from certain limitations, which reduces their capability in the real-world scenarios. To alleviate these limitations, we propose a novel two-stage co-segmentation framework, which introduces the weak background prior to establish a globally close- loop graph to represent the common object and union background separately. Then a novel graph optimized-flexible manifold ranking algorithm is proposed to flexibly optimize the graph connection and node labels to co-segment the common objects. Experiments on three image datasets demonstrate that our method outperforms other state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Quan_Object_Co-Segmentation_via_CVPR_2016_paper.pdf",
        "aff": "School of Automation; School of Automation; School of Automation; School of Computer Science and Center for OPTIMAL",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1166673,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17684781846596763482&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "rongquan0806; junweihan2010; zhangdingwen2006yyy;gmail.com",
        "email": "rongquan0806; junweihan2010; zhangdingwen2006yyy;gmail.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Quan_Object_Co-Segmentation_via_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "School of Automation;School of Computer Science",
        "aff_unique_dep": "Automation;Computer Science",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "title": "Object Contour Detection With a Fully Convolutional Encoder-Decoder Network",
        "session": "Segmentation and Contour Detection",
        "status": "Spotlight",
        "track": "main",
        "pid": "21",
        "author_site": "Jimei Yang, Brian Price, Scott Cohen, Honglak Lee , Ming-Hsuan Yang",
        "author": "Jimei Yang; Brian Price; Scott Cohen; Honglak Lee; Ming-Hsuan Yang",
        "abstract": "We develop a deep learning algorithm for contour detection with a fully convolutional encoder-decoder network. Different from previous low-level edge detection, our algorithm focuses on detecting higher-level object contours. Our network is trained end-to-end on PASCAL VOC with refined ground truth from inaccurate polygon annotations, yielding much higher precision in object contour detection than previous methods. We find that the learned model generalizes well to unseen object classes from the same supercategories on MS COCO and can match state-of-the-art edge detection on BSDS500 with fine-tuning. By combining with the multiscale combinatorial grouping algorithm, our method can generate high-quality segmented object proposals, which significantly advance the state-of-the-art on PASCAL VOC (improving average recall from 0.62 to 0.67) with a relatively small amount of candidates (1660 per image).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_Object_Contour_Detection_CVPR_2016_paper.pdf",
        "aff": "Adobe Research; Adobe Research; Adobe Research; University of Michigan, Ann Arbor; UC Merced",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2356327,
        "gs_citation": 478,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16237790414276114850&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "adobe.com;adobe.com;adobe.com;umich.edu;ucmerced.edu",
        "email": "adobe.com;adobe.com;adobe.com;umich.edu;ucmerced.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_Object_Contour_Detection_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;2",
        "aff_unique_norm": "Adobe;University of Michigan;University of California, Merced",
        "aff_unique_dep": "Adobe Research;;",
        "aff_unique_url": "https://research.adobe.com;https://www.umich.edu;https://www.ucmerced.edu",
        "aff_unique_abbr": "Adobe;UM;UCM",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Ann Arbor;Merced",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Object Detection From Video Tubelets With Convolutional Neural Networks",
        "session": "Object Detection 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "7",
        "author_site": "Kai Kang, Wanli Ouyang, Hongsheng Li, Xiaogang Wang",
        "author": "Kai Kang; Wanli Ouyang; Hongsheng Li; Xiaogang Wang",
        "abstract": "Deep Convolution Neural Networks (CNNs) have shown impressive performance in various vision tasks such as image classification, object detection and semantic segmentation. For object detection, particularly in still images, the performance has been significantly increased last year thanks to powerful deep networks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNN features (R-CNN)). The lately introduced ImageNet task on object detection from video (VID) brings the object detection task into video domain, in which objects' locations at each frame are required to be annotated with bounding boxes. In this work, we introduce a complete framework for the VID task based on still-image object detection and general object tracking. Their relations and contributions in the VID task are thoroughly studied and evaluated. In addition, a temporal convolution network is proposed to incorporate temporal information to regularize the detection results and shows its effectiveness for the task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kang_Object_Detection_From_CVPR_2016_paper.pdf",
        "aff": "Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong",
        "project": "",
        "github": "https://github.com/myfavouritekk/vdetlib",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3011769,
        "gs_citation": 513,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16780606977802410220&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kang_Object_Detection_From_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Object Skeleton Extraction in Natural Images by Fusing Scale-Associated Deep Side Outputs",
        "session": "Edge Contour Detection",
        "status": "Poster",
        "track": "main",
        "pid": "24",
        "author_site": "Wei Shen, Kai Zhao, Yuan Jiang, Yan Wang, Zhijiang Zhang, Xiang Bai",
        "author": "Wei Shen; Kai Zhao; Yuan Jiang; Yan Wang; Zhijiang Zhang; Xiang Bai",
        "abstract": "Object skeleton is a useful cue for object detection, complementary to the object contour, as it provides a structural representation to describe the relationship among object parts. While object skeleton extraction in natural images is a very challenging problem, as it requires the extractor to be able to capture both local and global image context to determine the intrinsic scale of each skeleton pixel. Existing methods rely on per-pixel based multi-scale feature computation, which results in difficult modeling and high time consumption. In this paper, we present a fully convolutional network with multiple scale-associated side outputs to address this problem. By observing the relationship between the receptive field sizes of the sequential stages in the network and the skeleton scales they can capture, we introduce a scale-associated side output to each stage. We impose supervision to different stages by guiding the scale-associated side outputs toward groundtruth skeletons of different scales. The responses of the multiple scale-associated side outputs are then fused in a scale-specific way to localize skeleton pixels with multiple scales effectively. Our method achieves promising results on two skeleton extraction datasets, and significantly outperforms other competitors.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shen_Object_Skeleton_Extraction_CVPR_2016_paper.pdf",
        "aff": "Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai University; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai University; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai University; Rapid-Rich Object Search Lab, Nanyang Technological University; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai University; School of Electronic Information and Communications, Huazhong University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Shen_Object_Skeleton_Extraction_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1992765,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5336405681600608578&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "shu.edu.cn; ; ; ; ;hust.edu.cn",
        "email": "shu.edu.cn; ; ; ; ;hust.edu.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shen_Object_Skeleton_Extraction_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;0;2",
        "aff_unique_norm": "Shanghai University;Nanyang Technological University;Huazhong University of Science and Technology",
        "aff_unique_dep": "Key Laboratory of Specialty Fiber Optics and Optical Access Networks;Rapid-Rich Object Search Lab;School of Electronic Information and Communications",
        "aff_unique_url": "https://www.shu.edu.cn;https://www.ntu.edu.sg;http://www.hust.edu.cn",
        "aff_unique_abbr": ";NTU;HUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "title": "Object Tracking via Dual Linear Structured SVM and Explicit Feature Map",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "51",
        "author_site": "Jifeng Ning, Jimei Yang, Shaojie Jiang, Lei Zhang, Ming-Hsuan Yang",
        "author": "Jifeng Ning; Jimei Yang; Shaojie Jiang; Lei Zhang; Ming-Hsuan Yang",
        "abstract": "Structured support vector machine (SSVM) based methods has demonstrated encouraging performance in recent object tracking benchmarks. However, the complex and expensive optimization limits their deployment in real-world applications. In this paper, we present a simple yet efficient dual linear SSVM (DLSSVM) algorithm to enable fast learning and execution during tracking. By analyzing the dual variables, we propose a primal classifier update formula where the learning step size is computed in closed form. This online learning method significantly improves the robustness of the proposed linear SSVM with low computational cost. Second, we approximate the intersection kernel for feature representations with an explicit feature map to further improve tracking performance. Finally, we extend the proposed DLSSVM tracker in a multiscale manner to address the \"drift\" problem. Experimental results on large benchmark datasets with 50 and 100 video sequences show that the proposed DLSSVM tracking algorithm achieves state-of-the-art performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ning_Object_Tracking_via_CVPR_2016_paper.pdf",
        "aff": "College of Information Engineering, Northwest A&F University, China; Adobe Research, USA; College of Information Engineering, Northwest A&F University, China; Department of Computing, The Hong Kong Polytechnic University, China; Electrical Engineering and Computer Science, University of California at Merced, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1094899,
        "gs_citation": 297,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16188676883928422587&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 15,
        "aff_domain": "sina.com;adobe.com;126.com;comp.polyu.edu.hk;ucmerced.edu",
        "email": "sina.com;adobe.com;126.com;comp.polyu.edu.hk;ucmerced.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ning_Object_Tracking_via_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;2;3",
        "aff_unique_norm": "Northwest A&F University;Adobe;Hong Kong Polytechnic University;University of California at Merced",
        "aff_unique_dep": "College of Information Engineering;Adobe Research;Department of Computing;Electrical Engineering and Computer Science",
        "aff_unique_url": "http://www.nwsuaf.edu.cn;https://research.adobe.com;https://www.polyu.edu.hk;https://www.ucmerced.edu",
        "aff_unique_abbr": ";Adobe;PolyU;UC Merced",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Merced",
        "aff_country_unique_index": "0;1;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Object-Proposal Evaluation Protocol is 'Gameable'",
        "session": "Object Detection 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "9",
        "author_site": "Neelima Chavali, Harsh Agrawal, Aroma Mahendru, Dhruv Batra",
        "author": "Neelima Chavali; Harsh Agrawal; Aroma Mahendru; Dhruv Batra",
        "abstract": "Object proposals have quickly become the de-facto pre-processing step in a number of vision pipelines (for object detection, object discovery, and other tasks). Their performance is usually evaluated on partially annotated datasets. In this paper, we argue that the choice of using a partially annotated dataset for evaluation of object proposals is problematic -- as we demonstrate via a thought experiment, the evaluation protocol is 'gameable', in the sense that progress under this protocol does not necessarily correspond to a \"better\" category independent object proposal algorithm.   To alleviate this problem, we: (1) Introduce a nearly-fully annotated version of PASCAL VOC dataset, which serves as a test-bed to check if object proposal techniques are overfitting to a particular list of categories. (2) Perform an exhaustive evaluation of object proposal methods on our introduced nearly-fully annotated PASCAL dataset and perform cross-dataset generalization experiments; and (3) Introduce a diagnostic experiment to detect the bias capacity in an object proposal algorithm. This tool circumvents the need to collect a densely annotated dataset, which can be expensive and cumbersome to collect. Finally, we have released an easy-to-use toolbox which combines various publicly available implementations of object proposal algorithms which standardizes the proposal generation and evaluation so that new methods can be added and evaluated on different datasets. We hope that the results presented in the paper will motivate the community to test the category independence of various object proposal methods by carefully choosing the evaluation protocol.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chavali_Object-Proposal_Evaluation_Protocol_CVPR_2016_paper.pdf",
        "aff": "Virginia Tech; Virginia Tech; Virginia Tech; Virginia Tech",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Chavali_Object-Proposal_Evaluation_Protocol_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 910583,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1139359172793052939&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "vt.edu;vt.edu;vt.edu;vt.edu",
        "email": "vt.edu;vt.edu;vt.edu;vt.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chavali_Object-Proposal_Evaluation_Protocol_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Virginia Tech",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.vt.edu",
        "aff_unique_abbr": "VT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Occlusion Boundary Detection via Deep Exploration of Context",
        "session": "Edge Contour Detection",
        "status": "Poster",
        "track": "main",
        "pid": "26",
        "author_site": "Huan Fu, Chaohui Wang, Dacheng Tao, Michael J. Black",
        "author": "Huan Fu; Chaohui Wang; Dacheng Tao; Michael J. Black",
        "abstract": "Occlusion boundaries contain rich perceptual information about the underlying scene structure. They also provide important cues in many visual perception tasks such as scene understanding, object recognition, and segmentation. In this paper, we improve occlusion boundary detection via enhanced exploration of contextual information (e.g., local structural boundary patterns, observations from surrounding regions, and temporal context), and in doing so develop a novel approach based on convolutional neural networks (CNNs) and conditional random fields (CRFs). Experimental results demonstrate that our detector significantly outperforms the state-of-the-art (e.g., improving the F-measure from 0.62 to 0.71 on the commonly used CMU benchmark). Last but not least, we empirically assess the roles of several important components of the proposed detector, so as to validate the rationale behind this approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Fu_Occlusion_Boundary_Detection_CVPR_2016_paper.pdf",
        "aff": "QCIS and FEIT, University of Technology Sydney; Universit \u00b4e Paris-Est, LIGM - CNRS UMR 8049; QCIS and FEIT, University of Technology Sydney; Max Planck Institute for Intelligent Systems, T \u00a8ubingen, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 665966,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4345732813383956717&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "student.uts.edu.au;u-pem.fr;uts.edu.au;tuebingen.mpg.de",
        "email": "student.uts.edu.au;u-pem.fr;uts.edu.au;tuebingen.mpg.de",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Fu_Occlusion_Boundary_Detection_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of Technology Sydney;Universit\u00e9 Paris-Est;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "QCIS and FEIT;LIGM - CNRS UMR 8049;",
        "aff_unique_url": "https://www.uts.edu.au;https://www.univ-Paris12.fr;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "UTS;UPE;MPI-IS",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Sydney;;T\u00fcbingen",
        "aff_country_unique_index": "0;1;0;2",
        "aff_country_unique": "Australia;France;Germany"
    },
    {
        "title": "Occlusion-Free Face Alignment: Deep Regression Networks Coupled With De-Corrupt AutoEncoders",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "43",
        "author_site": "Jie Zhang, Meina Kan, Shiguang Shan, Xilin Chen",
        "author": "Jie Zhang; Meina Kan; Shiguang Shan; Xilin Chen",
        "abstract": "Face alignment or facial landmark detection plays an important role in many computer vision applications, e.g., face recognition, facial expression recognition, face animation, etc. However, the performance of face alignment system degenerates severely when occlusions occur. In this work, we propose a novel face alignment method, which cascades several Deep Regression networks coupled with De-corrupt Autoencoders (denoted as DRDA) to explicitly handle partial occlusion problem. Different from the previous works that can only detect occlusions and discard the occluded parts, our proposed de-corrupt autoencoder network can automatically recover the genuine appearance for the occluded parts and the recovered parts can be leveraged together with those non-occluded parts for more accurate alignment. By coupling de-corrupt autoencoders with deep regression networks, a deep alignment model robust to partial occlusions is achieved. Besides, our method can localize occluded regions rather than merely predict whether the landmarks are occluded. Experiments on two challenging occluded face datasets demonstrate that our method significantly outperforms the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Occlusion-Free_Face_Alignment_CVPR_2016_paper.pdf",
        "aff": "Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China+University of Chinese Academy of Sciences, Beijing 100049, China; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China+CAS Center for Excellence in Brain Science and Intelligence Technology; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China+CAS Center for Excellence in Brain Science and Intelligence Technology; Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1041483,
        "gs_citation": 128,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11586651499637256752&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn",
        "email": "vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Occlusion-Free_Face_Alignment_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+0;0+0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "On Benefits of Selection Diversity via Bilevel Exclusive Sparsity",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "69",
        "author_site": "Haichuan Yang, Yijun Huang, Lam Tran, Ji Liu, Shuai Huang",
        "author": "Haichuan Yang; Yijun Huang; Lam Tran; Ji Liu; Shuai Huang",
        "abstract": "Sparse feature (dictionary) selection is critical for various tasks in computer vision, machine learning, and pattern recognition to avoid overfitting. While extensive research efforts have been conducted on feature selection using sparsity and group sparsity, we note that there has been a lack of development on applications where there is a particular preference on diversity. That is, the selected features are expected to come from different groups or categories. This diversity preference is motivated from many real-world applications such as advertisement recommendation, privacy image classification, and design of survey.   In this paper, we proposed a general bilevel exclusive sparsity formulation to pursue the diversity by restricting the overall sparsity and the sparsity in each group. To solve the proposed formulation that is NP hard in general, a heuristic procedure is proposed. The main contributions in this paper include: 1) A linear convergence rate is established for the proposed algorithm; 2) The provided theoretical error bound improves the approaches such as L_1 norm and L_0 types methods which only use the overall sparsity and the quantitative benefits of using the diversity sparsity is provided. To the best of our knowledge, this is the first work to show the theoretical benefits of using the diversity sparsity; 3) Extensive empirical studies are provided to validate the proposed formulation, algorithm, and theory.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_On_Benefits_of_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, University of Rochester + Goergen Institute for Data Science, University of Rochester; Department of Computer Science, University of Rochester + Goergen Institute for Data Science, University of Rochester; Department of Computer Science, University of Rochester + Goergen Institute for Data Science, University of Rochester; Department of Computer Science, University of Rochester + Goergen Institute for Data Science, University of Rochester; Department of Industrial and Systems Engineering, University of Washington",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Yang_On_Benefits_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 760761,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3333275400054946190&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com;gmail.com;gmail.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_On_Benefits_of_CVPR_2016_paper.html",
        "aff_unique_index": "0+0;0+0;0+0;0+0;1",
        "aff_unique_norm": "University of Rochester;University of Washington",
        "aff_unique_dep": "Department of Computer Science;Department of Industrial and Systems Engineering",
        "aff_unique_url": "https://www.rochester.edu;https://www.washington.edu",
        "aff_unique_abbr": "U of R;UW",
        "aff_campus_unique_index": "1;1;1;1;2",
        "aff_campus_unique": ";Rochester;Seattle",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "One-Shot Learning of Scene Locations via Feature Trajectory Transfer",
        "session": "Language and Vision",
        "status": "Spotlight",
        "track": "main",
        "pid": "9",
        "author_site": "Roland Kwitt, Sebastian Hegenbart, Marc Niethammer",
        "author": "Roland Kwitt; Sebastian Hegenbart; Marc Niethammer",
        "abstract": "The appearance of (outdoor) scenes changes considerably with the strength of certain transient attributes, such as \"rainy\", \"dark\" or \"sunny\". Obviously, this also affects the representation of an image in feature space, e.g., as activations at a certain CNN layer, and consequently impacts scene recognition performance. In this work, we investigate the variability in these transient attributes as a rich source of information for studying how image representations change as a function of attribute strength. In particular, we leverage a recently introduced dataset with fine-grain annotations to estimate feature trajectories for a collection of transient attributes and then show how these trajectories can be transferred to new image representations. This enables us to synthesize new data along the transferred trajectories with respect to the dimensions of the space spanned by the transient attributes. Applicability of this concept is demonstrated on the problem of one-shot recognition of scene locations. We show that data synthesized via feature trajectory transfer considerably boosts recognition performance, (1) with respect to baselines and (2) in combination with state-of-the-art approaches in one-shot learning.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kwitt_One-Shot_Learning_of_CVPR_2016_paper.pdf",
        "aff": "University of Salzburg, Austria; University of Salzburg, Austria; UNC Chapel Hill, NC, United States",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1274344,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8407355888666085322&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;gmail.com;cs.unc.edu",
        "email": "gmail.com;gmail.com;cs.unc.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kwitt_One-Shot_Learning_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Salzburg;University of North Carolina at Chapel Hill",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-salzburg.at;https://www.unc.edu",
        "aff_unique_abbr": "USAL;UNC Chapel Hill",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chapel Hill",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Austria;United States"
    },
    {
        "title": "Online Collaborative Learning for Open-Vocabulary Visual Classifiers",
        "session": "Large Scale Visual Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "59",
        "author_site": "Hanwang Zhang, Xindi Shang, Wenzhuo Yang, Huan Xu, Huanbo Luan, Tat-Seng Chua",
        "author": "Hanwang Zhang; Xindi Shang; Wenzhuo Yang; Huan Xu; Huanbo Luan; Tat-Seng Chua",
        "abstract": "We focus on learning open-vocabulary visual classifiers, which scale up to a large portion of natural language vocabulary (e.g., over tens of thousands of classes). In particular, the training data are large-scale weakly labeled Web images since it is difficult to acquire sufficient well-labeled data at this category scale.  In this paper, we propose a novel online learning paradigm towards this challenging task. Different from traditional N-way independent classifiers that generally fail to handle the extremely sparse and inter-related labels, our classifiers learn from continuous label embeddings discovered by collaboratively decomposing the sparse image-label matrix. Leveraging on the structure of the proposed collaborative learning formulation, we develop an efficient online algorithm that can jointly learn the label embeddings and visual classifiers. The algorithm can learn over 30,000 classes of 1,000 training images within 1 second on a standard GPU. Extensively experimental results on four benchmarks demonstrate the effectiveness of our method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Online_Collaborative_Learning_CVPR_2016_paper.pdf",
        "aff": "National University of Singapore; National University of Singapore; National University of Singapore; National University of Singapore; Tsinghua University; National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1707132,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17111513923057606168&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;gmail.com;nus.edu.sg;nus.edu.sg;gmail.com;nus.edu.sg",
        "email": "gmail.com;gmail.com;nus.edu.sg;nus.edu.sg;gmail.com;nus.edu.sg",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Online_Collaborative_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "National University of Singapore;Tsinghua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "NUS;THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "title": "Online Detection and Classification of Dynamic Hand Gestures With Recurrent 3D Convolutional Neural Network",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "45",
        "author_site": "Pavlo Molchanov, Xiaodong Yang, Shalini Gupta, Kihwan Kim, Stephen Tyree, Jan Kautz",
        "author": "Pavlo Molchanov; Xiaodong Yang; Shalini Gupta; Kihwan Kim; Stephen Tyree; Jan Kautz",
        "abstract": "Automatic detection and classification of dynamic hand gestures in real-world systems intended for human computer interaction is challenging as: 1) there is a large diversity in how people perform gestures, making detection and classification difficult; 2) the system must work online in order to avoid noticeable lag between performing a gesture and its classification; in fact, a negative lag (classification before the gesture is finished) is desirable, as feedback to the user can then be truly instantaneous. In this paper, we address these challenges with a recurrent three-dimensional convolutional neural network that performs simultaneous detection and classification of dynamic hand gestures from multi-modal data. We employ connectionist temporal classification to train the network to predict class labels from in-progress gestures in unsegmented input streams. In order to validate our method, we introduce a new challenging multi-modal dynamic hand gesture dataset captured with depth, color and stereo-IR sensors. On this challenging dataset, our gesture recognition system achieves an accuracy of 83.8%, outperforms competing state-of-the-art algorithms, and approaches human accuracy of 88.4%. Moreover, our method achieves state-of-the-art performance on SKIG and ChaLearn2014 benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Molchanov_Online_Detection_and_CVPR_2016_paper.pdf",
        "aff": "NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Molchanov_Online_Detection_and_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1213068,
        "gs_citation": 822,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3040705073843151431&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com",
        "email": "nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Molchanov_Online_Detection_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "NVIDIA",
        "aff_unique_dep": "NVIDIA Corporation",
        "aff_unique_url": "https://www.nvidia.com",
        "aff_unique_abbr": "NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Online Learning With Bayesian Classification Trees",
        "session": "Statistical Methods and Transfer Learning",
        "status": "Spotlight",
        "track": "main",
        "pid": "21",
        "author_site": "Samuel Rota Bul\u00f2, Peter Kontschieder",
        "author": "Samuel Rota Bulo; Peter Kontschieder",
        "abstract": "Randomized classification trees are among the most popular machine learning tools and found successful applications in many areas. Although this classifier was originally designed as offline learning algorithm, there has been an increased interest in the last years to provide an online variant. In this paper, we propose an online learning algorithm for classification trees that adheres to Bayesian principles. In contrast to state-of-the-art approaches that produce large forests with complex trees, we aim at constructing small ensembles consisting of shallow trees with high generalization capabilities. Experiments on benchmark machine learning and body part recognition datasets show superior performance over state-of-the-art approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bulo_Online_Learning_With_CVPR_2016_paper.pdf",
        "aff": "FBK-irst, Trento, Italy; Microsoft Research, Cambridge, UK + Mapillary, Graz, Austria",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Bulo_Online_Learning_With_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 544986,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17848522833455159869&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "fbk.eu;gmail.com",
        "email": "fbk.eu;gmail.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bulo_Online_Learning_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+2",
        "aff_unique_norm": "Fondazione Bruno Kessler;Microsoft;Mapillary",
        "aff_unique_dep": "irst;Microsoft Research;",
        "aff_unique_url": "https://www.fbk.eu;https://www.microsoft.com/en-us/research;https://www.mapillary.com",
        "aff_unique_abbr": "FBK;MSR;",
        "aff_campus_unique_index": "0;1+2",
        "aff_campus_unique": "Trento;Cambridge;Graz",
        "aff_country_unique_index": "0;1+2",
        "aff_country_unique": "Italy;United Kingdom;Austria"
    },
    {
        "title": "Online Multi-Object Tracking via Structural Constraint Event Aggregation",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "67",
        "author_site": "Ju Hong Yoon, Chang-Ryeol Lee, Ming-Hsuan Yang, Kuk-Jin Yoon",
        "author": "Ju Hong Yoon; Chang-Ryeol Lee; Ming-Hsuan Yang; Kuk-Jin Yoon",
        "abstract": "Multi-object tracking (MOT) becomes more challenging when objects of interest have similar appearances. In that case, the motion cues are particularly useful for discriminating multiple objects. However, for online 2D MOT in scenes acquired from moving cameras, observable motion cues are complicated by global camera movements and thus not always smooth or predictable. To deal with such unexpected camera motion for online 2D MOT, a structural motion constraint between objects has been utilized thanks to its robustness to camera motion. In this paper, we propose a new data association method that effectively exploits structural motion constraints in the presence of large camera motion. In addition, to further improve the robustness of data association against mis-detections and clutters, a novel event aggregation approach is developed to integrate structural constraints in assignment costs for online MOT. Experimental results on a large number of datasets demonstrate the effectiveness of the proposed algorithm for online 2D MOT.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yoon_Online_Multi-Object_Tracking_CVPR_2016_paper.pdf",
        "aff": "KETI; CV Lab., GIST; UC Merced; CV Lab., GIST",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1152525,
        "gs_citation": 226,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4920498154035304648&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "keti.re.kr;gist.ac.kr;ucmerced.edu;gist.ac.kr",
        "email": "keti.re.kr;gist.ac.kr;ucmerced.edu;gist.ac.kr",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yoon_Online_Multi-Object_Tracking_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Korea Electronics and Telecommunications Institute;Gwangju Institute of Science and Technology;University of California, Merced",
        "aff_unique_dep": ";Computer Vision Laboratory;",
        "aff_unique_url": "http://www.keti.re.kr;https://www.gist.ac.kr;https://www.ucmerced.edu",
        "aff_unique_abbr": "KETI;GIST;UCM",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Gwangju;Merced",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "title": "Online Reconstruction of Indoor Scenes From RGB-D Streams",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "26",
        "author_site": "Hao Wang, Jun Wang, Wang Liang",
        "author": "Hao Wang; Jun Wang; Wang Liang",
        "abstract": "A system capable of performing robust online volumetric reconstruction of indoor scenes based on input from a handheld RGB-D camera is presented. Our system is powered by a two-pass reconstruction scheme. The first pass tracks camera poses at video rate and simultaneously constructs a pose graph on-the-fly. The tracker operates in real-time, which allows the reconstruction results to be visualized during the scanning process. Live visual feedbacks makes the scanning operation fast and intuitive. Upon termination of scanning, the second pass takes place to handle loop closures and reconstruct the final model using globally refined camera trajectories. The system is online with low delay and returns a dense model of sufficient accuracy. The beauty of this system lies in its speed, accuracy, simplicity and ease of implementation when compared to previous methods. We demonstrate the performance of our system on several real-world scenes and quantitatively assess the modeling accuracy with respect to ground truth models obtained from a LIDAR scanner.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Online_Reconstruction_of_CVPR_2016_paper.pdf",
        "aff": "Baidu Research - Institute of Deep Learning; Baidu Research - Institute of Deep Learning; Baidu Research - Institute of Deep Learning",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1618888,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10082904202021209386&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Online_Reconstruction_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Baidu",
        "aff_unique_dep": "Institute of Deep Learning",
        "aff_unique_url": "https://research.baidu.com",
        "aff_unique_abbr": "Baidu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Optical Flow With Semantic Segmentation and Localized Layers",
        "session": "Video Analysis 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "11",
        "author_site": "Laura Sevilla-Lara, Deqing Sun, Varun Jampani, Michael J. Black",
        "author": "Laura Sevilla-Lara; Deqing Sun; Varun Jampani; Michael J. Black",
        "abstract": "Existing optical flow methods make generic, spatially homogeneous, assumptions about the spatial structure of the flow. In reality, optical flow varies across an image depending on object class. Simply put, different objects move differently. Here we exploit recent advances in static semantic scene segmentation to segment the image into objects of different types. We define different models of image motion in these regions depending on the type of object. For example, the road motion with homographies, vegetation with spatially smooth flow, and independently moving objects like cars and planes with affine+deviations. We then pose the flow estimation problem using a novel formulation of  localized layers, which addresses limitations of traditional layered models for dealing with complex scene motion. Our  semantic flow method achieves the lowest error of any published method in the KITTI-2015 flow benchmark and produces qualitatively better flow and segmentation than recent top methods on a wide range of natural videos.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sevilla-Lara_Optical_Flow_With_CVPR_2016_paper.pdf",
        "aff": "MPI for Intelligent Systems; NVIDIA+Harvard University; MPI for Intelligent Systems; MPI for Intelligent Systems",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1913166,
        "gs_citation": 251,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5957770392766384609&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "tuebingen.mpg.de;nvidia.com;tuebingen.mpg.de;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;nvidia.com;tuebingen.mpg.de;tuebingen.mpg.de",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sevilla-Lara_Optical_Flow_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+2;0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems;NVIDIA;Harvard University",
        "aff_unique_dep": ";NVIDIA Corporation;",
        "aff_unique_url": "https://www.mpi-is.mpg.de;https://www.nvidia.com;https://www.harvard.edu",
        "aff_unique_abbr": "MPI-IS;NVIDIA;Harvard",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+1;0;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "title": "Optimal Relative Pose With Unknown Correspondences",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "25",
        "author_site": "Johan Fredriksson, Viktor Larsson, Carl Olsson, Fredrik Kahl",
        "author": "Johan Fredriksson; Viktor Larsson; Carl Olsson; Fredrik Kahl",
        "abstract": "Previous work on estimating the epipolar geometry of two views relies on being able to reliably match feature points based on appearance. In this paper, we go one step further and show that it is feasible to compute both the epipolar geometry and the correspondences at the same time based on geometry only. We do this in a globally optimal manner. Our approach is based on an efficient branch and bound technique in combination with bipartite matching to solve the correspondence problem. We rely on several recent works to obtain good bounding functions to battle the combinatorial explosion of possible matchings. It is experimentally demonstrated that more difficult cases can be handled and that more inlier correspondences can be obtained by being less restrictive in the matching phase.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Fredriksson_Optimal_Relative_Pose_CVPR_2016_paper.pdf",
        "aff": "Lund University; Lund University; Lund University; Lund University+Chalmers University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 7226548,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14898085731147102443&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "maths.lth.se;maths.lth.se;maths.lth.se;chalmers.se",
        "email": "maths.lth.se;maths.lth.se;maths.lth.se;chalmers.se",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Fredriksson_Optimal_Relative_Pose_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Lund University;Chalmers University of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.lunduniversity.lu.se;https://www.chalmers.se",
        "aff_unique_abbr": "LU;Chalmers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "Sweden"
    },
    {
        "title": "Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace Clustering",
        "session": "Grouping and Optimization Methods",
        "status": "Oral",
        "track": "main",
        "pid": "15",
        "author_site": "Chong You, Chun-Guang Li, Daniel P. Robinson, Ren\u00e9 Vidal",
        "author": "Chong You; Chun-Guang Li; Daniel P. Robinson; Rene Vidal",
        "abstract": "State-of-the-art subspace clustering methods are based on expressing each data point as a linear combination of other data points while regularizing the matrix of coefficients with l_1, l_2 or nuclear norms. l_1 regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad theoretical conditions, but the clusters may not be connected. l_2 and nuclear norm regularization often improve connectivity, but give a subspace-preserving affinity only for independent subspaces. Mixed l_1, l_2 and nuclear norm regularizations offer a balance between the subspace-preserving and connectedness properties, but this comes at the cost of increased computational complexity. This paper studies the geometry of the elastic net regularizer (a mixture of the l_1 and l_2 norms) and uses it to derive a provably correct and scalable active set method for finding the optimal coefficients. Our geometric analysis also provides a theoretical justification and a geometric interpretation for the balance between the connectedness (due to l_2 regularization) and subspace-preserving (due to l_1 regularization) properties for elastic net subspace clustering. Our experiments show that the proposed active set method not only achieves state-of-the-art clustering performance, but also efficiently handles large-scale datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/You_Oracle_Based_Active_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 953943,
        "gs_citation": 318,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5705901109833357513&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/You_Oracle_Based_Active_CVPR_2016_paper.html"
    },
    {
        "title": "Ordinal Regression With Multiple Output CNN for Age Estimation",
        "session": "Face and Gesture",
        "status": "Oral",
        "track": "main",
        "pid": "40",
        "author_site": "Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, Gang Hua",
        "author": "Zhenxing Niu; Mo Zhou; Le Wang; Xinbo Gao; Gang Hua",
        "abstract": "To address the non-stationary property of aging patterns, age estimation can be cast as an ordinal regression problem. However, the processes of extracting features and learning a regression model are often separated and optimized independently in previous work. In this paper, we propose an End-to-End learning approach to address ordinal regression problems using deep Convolutional Neural Network, which could simultaneously conduct feature learning and regression modeling. In particular, an ordinal regression problem is transformed into a series of binary classification sub-problems. And we propose a multiple output CNN learning algorithm to collectively solve these classification sub-problems, so that the correlation between these tasks could be explored. In addition, we publish an Asian Face Age Dataset (AFAD) containing more than 160K facial images with precise age ground-truths, which is the largest public age dataset to date. To the best of our knowledge, this is the first work to address ordinal regression problems by using CNN, and achieves the state-of-the-art performance on both the MORPH and AFAD datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Niu_Ordinal_Regression_With_CVPR_2016_paper.pdf",
        "aff": "Xidian University; Xidian University; Xi\u2019an Jiaotong University; Xidian University; Microsoft Research Asia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1068781,
        "gs_citation": 813,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3552477597454115511&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "gmail.com;gmail.com;mail.xjtu.edu.cn;mail.xidian.edu.cn;gmail.com",
        "email": "gmail.com;gmail.com;mail.xjtu.edu.cn;mail.xidian.edu.cn;gmail.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Niu_Ordinal_Regression_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0;2",
        "aff_unique_norm": "Xidian University;Xi'an Jiao Tong University;Microsoft",
        "aff_unique_dep": ";;Research",
        "aff_unique_url": "http://www.xidian.edu.cn/;https://www.xjtu.edu.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "Xidian;XJTU;MSR Asia",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "POD: Discovering Primary Objects in Videos Based on Evolutionary Refinement of Object Recurrence, Background, and Primary Object Models",
        "session": "Events, Activities, and Surveillance",
        "status": "Poster",
        "track": "main",
        "pid": "33",
        "author_site": "Yeong Jun Koh, Won-Dong Jang, Chang-Su Kim",
        "author": "Yeong Jun Koh; Won-Dong Jang; Chang-Su Kim",
        "abstract": "A primary object discovery (POD) algorithm for a video sequence is proposed in this work, which is capable of discovering a primary object, as well as identifying noisy frames that do not contain the object. First, we generate object proposals for each frame. Then, we bisect each proposal into foreground and background regions, and extract features from each region. By superposing the foreground and background features, we build the object recurrence model, the background model, and the primary object model. We develop an iterative scheme to refine each model evolutionary using the information in the other models. Finally, using the evolved primary object model, we select candidate proposals and locate the bounding box of a primary object by merging the proposals selectively. Experimental results on a challenging dataset demonstrate that the proposed POD algorithm extract primary objects accurately and robustly.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Koh_POD_Discovering_Primary_CVPR_2016_paper.pdf",
        "aff": "Korea University; Korea University; Korea University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Koh_POD_Discovering_Primary_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2753221,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16248382566665973082&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mcl.korea.ac.kr;mcl.korea.ac.kr;korea.ac.kr",
        "email": "mcl.korea.ac.kr;mcl.korea.ac.kr;korea.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Koh_POD_Discovering_Primary_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Korea University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.korea.ac.kr",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "be14edbfb1",
        "title": "PPP: Joint Pointwise and Pairwise Image Label Prediction",
        "site": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_PPP_Joint_Pointwise_CVPR_2016_paper.html",
        "author": "Yilin Wang; Suhang Wang; Jiliang Tang; Huan Liu; Baoxin Li",
        "abstract": "Pointwise label and Pairwise label are both widely used in computer vision tasks. For example, supervised image classification and annotation approaches use pointwise label, while attribute-based image relative learning often adopts pairwise labels. These two types of labels are often considered independently and most existing efforts utilize them separately. However, pointwise labels in image classification and tag annotation are inherently related to the pairwise labels. For example, an image labeled with \"coast\" and annotated with \"beach, sea, sand, sky\" is more likely to have a higher ranking score in terms of the attribute \"open\"; while \"men shoes\" ranked highly on the attribute \"formal\" are likely to be annotated with \"leather, lace up\" than \"buckle, fabric\".  The existence of potential relations between pointwise labels and pairwise labels motivates us to fuse them together for jointly addressing related vision tasks. In particular, we provide a principled way to capture the relations between class labels, tags and attributes; and propose a novel framework PPP(Pointwise and Pairwise image label Prediction),  which is based on overlapped group structure extracted from the pointwise-pairwise-label bipartite graph. With experiments on benchmark datasets, we demonstrate that the proposed framework achieves superior performance on three vision tasks compared to the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_PPP_Joint_Pointwise_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, Arizona State Univerity; Department of Computer Science, Arizona State Univerity; Yahoo Research; Department of Computer Science, Arizona State Univerity; Department of Computer Science, Arizona State Univerity",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 657946,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16059200496921223815&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "asu.edu;asu.edu;yahoo-inc.com;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;yahoo-inc.com;asu.edu;asu.edu",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Arizona State University;Yahoo",
        "aff_unique_dep": "Department of Computer Science;Yahoo Research",
        "aff_unique_url": "https://www.asu.edu;https://research.yahoo.com",
        "aff_unique_abbr": "ASU;Yahoo Research",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "PSyCo: Manifold Span Reduction for Super Resolution",
        "session": "Deblurring and Super-Resolution",
        "status": "Poster",
        "track": "main",
        "pid": "37",
        "author_site": "Eduardo P\u00e9rez-Pellitero, Jordi Salvador, Javier Ruiz-Hidalgo, Bodo Rosenhahn",
        "author": "Eduardo Perez-Pellitero; Jordi Salvador; Javier Ruiz-Hidalgo; Bodo Rosenhahn",
        "abstract": "The main challenge in Super Resolution (SR) is to discover the mapping between the low- and high-resolution manifolds of image patches, a complex ill-posed problem which has recently been addressed through piecewise linear regression with promising results. In this paper we present a novel regression-based SR algorithm that benefits from an extended knowledge of the structure of both manifolds. We propose a transform that collapses the 16 variations induced from the dihedral group of transforms (i.e. rotations, vertical and horizontal reflections) and antipodality (i.e. diametrically opposed points in the unitary sphere) into a single primitive. The key idea of our transform is to study the different dihedral elements as a group of symmetries within the high-dimensional manifold. We obtain the respective set of mirror-symmetry axes by means of a frequency analysis of the dihedral elements, and we use them to collapse the redundant variability through a modified symmetry distance. The experimental validation of our algorithm shows the effectiveness of our approach, which obtains competitive quality with a dictionary of as little as 32 atoms (reducing other methods' dictionaries by at least a factor of 32) and further pushing the state-of-the-art with a 1024 atoms dictionary.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Perez-Pellitero_PSyCo_Manifold_Span_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1626178,
        "gs_citation": 63,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6465533717714367888&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Perez-Pellitero_PSyCo_Manifold_Span_CVPR_2016_paper.html"
    },
    {
        "title": "Pairwise Decomposition of Image Sequences for Active Multi-View Recognition",
        "session": "Video Understanding",
        "status": "Oral",
        "track": "main",
        "pid": "3",
        "author_site": "Edward Johns, Stefan Leutenegger, Andrew J. Davison",
        "author": "Edward Johns; Stefan Leutenegger; Andrew J. Davison",
        "abstract": "A multi-view image sequence provides a much richer capacity for object recognition than from a single image. However, most existing solutions to multi-view recognition typically adopt hand-crafted, model-based geometric methods, which do not readily embrace recent trends in deep learning. We propose to bring Convolutional Neural Networks to generic multi-view recognition, by decomposing an image sequence into a set of image pairs, classifying each pair independently, and then learning an object classifier by weighting the contribution of each pair. This allows for recognition over arbitrary camera trajectories, without requiring explicit training over the potentially infinite number of camera paths and lengths. Building these pairwise relationships then naturally extends to the next-best-view problem in an active recognition framework. To achieve this, we train a second Convolutional Neural Network to map directly from an observed image to next viewpoint. Finally, we incorporate this into a trajectory optimisation task, whereby the best recognition confidence is sought for a given trajectory length. We present state-of-the-art results in both guided and unguided multi-view recognition on the ModelNet dataset, and show how our method can be used with depth images, greyscale images, or both.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Johns_Pairwise_Decomposition_of_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 678285,
        "gs_citation": 308,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8256112855567005741&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Johns_Pairwise_Decomposition_of_CVPR_2016_paper.html"
    },
    {
        "title": "Pairwise Linear Regression Classification for Image Set Retrieval",
        "session": "Face Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "34",
        "author_site": "Qingxiang Feng, Yicong Zhou, Rushi Lan",
        "author": "Qingxiang Feng; Yicong Zhou; Rushi Lan",
        "abstract": "This paper proposes the pairwise linear regression classification (PLRC) for image set retrieval. In PLRC, we first define a new concept of the unrelated subspace and introduce two strategies to constitute the unrelated subspace. In order to increase the information of maximizing the query set and the unrelated image set, we introduce a combination metric for two new classifiers based on two constitution strategies of the unrelated subspace. Extensive experiments on six well-known databases prove that the performance of PLRC is better than that of DLRC and several state-of-the-art classifiers for different vision recognition tasks: cluster-based face recognition, video-based face recognition, object recognition and action recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Feng_Pairwise_Linear_Regression_CVPR_2016_paper.pdf",
        "aff": "Department of Computer and Information Science, University of Macau; Department of Computer and Information Science, University of Macau; Department of Computer and Information Science, University of Macau",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 579956,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2884673255518071826&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "gmail.com;umac.mo;gmail.com",
        "email": "gmail.com;umac.mo;gmail.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Feng_Pairwise_Linear_Regression_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Macau",
        "aff_unique_dep": "Department of Computer and Information Science",
        "aff_unique_url": "https://www.um.edu.mo",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Macau SAR",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Pairwise Matching Through Max-Weight Bipartite Belief Propagation",
        "session": "Feature Matching and Indexing",
        "status": "Poster",
        "track": "main",
        "pid": "47",
        "author_site": "Zhen Zhang, Qinfeng Shi, Julian McAuley, Wei Wei, Yanning Zhang, Anton van den Hengel",
        "author": "Zhen Zhang; Qinfeng Shi; Julian McAuley; Wei Wei; Yanning Zhang; Anton van den Hengel",
        "abstract": "Feature matching is a key problem in computer vision and pattern recognition. One way to encode the essential interdependence between potential feature matches is to cast the problem as inference in a graphical model, though recently alternatives such as spectral methods, or approaches based on the convex-concave procedure have achieved the state-of-the-art. Here we revisit the use of graphical models for feature matching, and propose a belief propagation scheme which exhibits the following advantages: (1) we explicitly enforce one-to-one matching constraints; (2) we offer a tighter relaxation of the original cost function than previous graphical-model-based approaches; and (3) our sub-problems decompose into max-weight bipartite matching, which can be solved efficiently, leading to orders-of-magnitude reductions in execution time. Experimental results show that the proposed algorithm produces results superior to those of the current state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Pairwise_Matching_Through_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science and Engineering, Northwestern Polytechnical University, Xi\u2019an, China; School of Computer Science, The University of Adelaide, Australia; Computer Science and Engineering Department, University of California, San Diego, USA; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi\u2019an, China; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi\u2019an, China; School of Computer Science, The University of Adelaide, Australia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhang_Pairwise_Matching_Through_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1247118,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16861137801269852510&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Pairwise_Matching_Through_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;0;0;1",
        "aff_unique_norm": "Northwestern Polytechnical University;University of Adelaide;University of California, San Diego",
        "aff_unique_dep": "School of Computer Science and Engineering;School of Computer Science;Computer Science and Engineering Department",
        "aff_unique_url": "http://www.nwpu.edu.cn;https://www.adelaide.edu.au;https://www.ucsd.edu",
        "aff_unique_abbr": "NPU;Adelaide;UCSD",
        "aff_campus_unique_index": "0;2;0;0",
        "aff_campus_unique": "Xi'an;;San Diego",
        "aff_country_unique_index": "0;1;2;0;0;1",
        "aff_country_unique": "China;Australia;United States"
    },
    {
        "title": "Panoramic Stereo Videos With a Single Camera",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "78",
        "author_site": "Rajat Aggarwal, Amrisha Vohra, Anoop M. Namboodiri",
        "author": "Rajat Aggarwal; Amrisha Vohra; Anoop M. Namboodiri",
        "abstract": "We present a practical solution for generating 360 degree stereo panoramic videos using a single camera. Current approaches either use a moving camera that captures multiple images of a scene, which are then stitched together to form the final panorama, or use multiple cameras that are synchronized. A moving camera limits the solution to static scenes, while multi-camera solutions require dedicated calibrated setups. Our approach improves upon the existing solutions in two significant ways: It solves the problem using a single camera, thus minimizing the calibration problem and providing us the ability to convert any digital camera into a panoramic stereo capture device. It captures all the light rays required for stereo panoramas in a single frame using a compact custom designed mirror, thus making the design practical to manufacture and easier to use. We analyze several properties of the design as well as present panoramic stereo and depth estimation results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Aggarwal_Panoramic_Stereo_Videos_CVPR_2016_paper.pdf",
        "aff": "Kohli Center on Intelligent Systems, International Institute of Information Technology- Hyderabad, India; Kohli Center on Intelligent Systems, International Institute of Information Technology- Hyderabad, India; Kohli Center on Intelligent Systems, International Institute of Information Technology- Hyderabad, India",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Aggarwal_Panoramic_Stereo_Videos_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2062319,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14366343759243854738&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "email": "research.iiit.ac.in;research.iiit.ac.in;iiit.ac.in",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Aggarwal_Panoramic_Stereo_Videos_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "International Institute of Information Technology",
        "aff_unique_dep": "Kohli Center on Intelligent Systems",
        "aff_unique_url": "https://iiit Hyderabad.ac.in",
        "aff_unique_abbr": "IIIT Hyderabad",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hyderabad",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "title": "Parametric Object Motion From Blur",
        "session": "Deblurring and Super-Resolution",
        "status": "Poster",
        "track": "main",
        "pid": "38",
        "author_site": "Jochen Gast, Anita Sellent, Stefan Roth",
        "author": "Jochen Gast; Anita Sellent; Stefan Roth",
        "abstract": "Motion blur can adversely affect a number of vision tasks, hence it is generally considered a nuisance. We instead treat motion blur as a useful signal that allows to compute the motion of objects from a single image. Drawing on the success of joint segmentation and parametric motion models in the context of optical flow estimation, we propose a parametric object motion model combined with a segmentation mask to exploit localized, non-uniform motion blur. Our parametric image formation model is differentiable w.r.t. the motion parameters, which enables us to generalize marginal-likelihood techniques from uniform blind deblurring to localized, non-uniform blur. A two-stage pipeline, first in derivative space and then in image space, allows to estimate both parametric object motion as well as a motion segmentation from a single image alone. Our experiments demonstrate its ability to cope with very challenging cases of object motion blur.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gast_Parametric_Object_Motion_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Gast_Parametric_Object_Motion_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1653516,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15551528242769189701&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gast_Parametric_Object_Motion_CVPR_2016_paper.html"
    },
    {
        "title": "Part-Stacked CNN for Fine-Grained Visual Categorization",
        "session": "Fine Grained Categorization",
        "status": "Poster",
        "track": "main",
        "pid": "44",
        "author_site": "Shaoli Huang, Zhe Xu, Dacheng Tao, Ya Zhang",
        "author": "Shaoli Huang; Zhe Xu; Dacheng Tao; Ya Zhang",
        "abstract": "In the context of fine-grained visual categorization, the ability to interpret models as human-understandable visual manuals is sometimes as important as achieving high classification accuracy. In this paper, we propose a novel Part-Stacked CNN architecture that explicitly explains the fine-grained recognition process by modeling subtle differences from object parts. Based on manually-labeled strong part annotations, the proposed architecture consists of a fully convolutional network to locate multiple object parts and a two-stream classification network that encodes object-level and part-level cues simultaneously. By adopting a set of sharing strategies between the computation of multiple object parts, the proposed architecture is very efficient running at 20 frames/sec during inference. Experimental results on the CUB-200-2011 dataset reveal the effectiveness of the proposed architecture, from multiple perspectives of classification accuracy, model interpretability, and efficiency. Being able to provide interpretable recognition results in realtime, the proposed method is believed to be effective in practical applications.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Part-Stacked_CNN_for_CVPR_2016_paper.pdf",
        "aff": "Centre for Quantum Computation & Intelligent Systems and Faculty of Engineering and Information Technology, University of Technology Sydney, Australia; Centre for Quantum Computation & Intelligent Systems and Faculty of Engineering and Information Technology, University of Technology Sydney, Australia + Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; Centre for Quantum Computation & Intelligent Systems and Faculty of Engineering and Information Technology, University of Technology Sydney, Australia; Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1046957,
        "gs_citation": 589,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3247016502012318332&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "student.uts.edu.au;sjtu.edu.cn;uts.edu.au;sjtu.edu.cn",
        "email": "student.uts.edu.au;sjtu.edu.cn;uts.edu.au;sjtu.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Huang_Part-Stacked_CNN_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;0+1;0;1",
        "aff_unique_norm": "University of Technology Sydney;Shanghai Jiao Tong University",
        "aff_unique_dep": "Centre for Quantum Computation & Intelligent Systems;Cooperative Medianet Innovation Center",
        "aff_unique_url": "https://www.uts.edu.au;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "UTS;SJTU",
        "aff_campus_unique_index": "0;0+1;0;1",
        "aff_campus_unique": "Sydney;Shanghai",
        "aff_country_unique_index": "0;0+1;0;1",
        "aff_country_unique": "Australia;China"
    },
    {
        "title": "Patch-Based Convolutional Neural Network for Whole Slide Tissue Image Classification",
        "session": "Computational Photography and Biomedical Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "18",
        "author_site": "Le Hou, Dimitris Samaras, Tahsin M. Kurc, Yi Gao, James E. Davis, Joel H. Saltz",
        "author": "Le Hou; Dimitris Samaras; Tahsin M. Kurc; Yi Gao; James E. Davis; Joel H. Saltz",
        "abstract": "Convolutional Neural Networks (CNN) are state-of-the-art models for many image classification tasks. However, to recognize cancer subtypes automatically, training a CNN on gigapixel resolution Whole Slide Tissue Images (WSI) is currently computationally impossible. The differentiation of cancer subtypes is based on cellular-level visual features observed on image patch scale. Therefore, we argue that in this situation, training a patch-level classifier on image patches will perform better than or similar to an image-level classifier. The challenge becomes how to intelligently combine patch-level classification results and model the fact that not all patches will be discriminative. We propose to train a decision fusion model to aggregate patch-level predictions given by patch-level CNNs, which to the best of our knowledge has not been shown before. Furthermore, we formulate a novel Expectation-Maximization (EM) based method that automatically locates discriminative patches robustly by utilizing the spatial relationships of patches. We apply our method to the classification of glioma and non-small-cell lung carcinoma cases into subtypes. The classification accuracy of our method is similar to the inter-observer agreement between pathologists. Although it is impossible to train CNNs on WSIs, we experimentally demonstrate using a comparable non-cancer dataset of smaller images that a patch-based CNN can outperform an image-based CNN.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hou_Patch-Based_Convolutional_Neural_CVPR_2016_paper.pdf",
        "aff": "Dept. of Computer Science, Stony Brook University; Dept. of Computer Science, Stony Brook University; Dept. of Biomedical Informatics, Stony Brook University + Oak Ridge National Laboratory; Dept. of Biomedical Informatics, Stony Brook University + Dept. of Applied Mathematics and Statistics, Stony Brook University; Dept. of Pathology, Stony Brook Hospital; Dept. of Biomedical Informatics, Stony Brook University + Dept. of Pathology, Stony Brook Hospital + Cancer Center, Stony Brook Hospital",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5175375,
        "gs_citation": 1032,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8322940461640959541&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu;stonybrook.edu;stonybrookmedicine.edu;stonybrookmedicine.edu;stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu;stonybrook.edu;stonybrookmedicine.edu;stonybrookmedicine.edu;stonybrook.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hou_Patch-Based_Convolutional_Neural_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0+1;0+0;2;0+2+2",
        "aff_unique_norm": "Stony Brook University;Oak Ridge National Laboratory;Stony Brook Hospital",
        "aff_unique_dep": "Department of Computer Science;;Dept. of Pathology",
        "aff_unique_url": "https://www.stonybrook.edu;https://www.ornl.gov;https://www.stonybrookmedicine.edu",
        "aff_unique_abbr": "SBU;ORNL;",
        "aff_campus_unique_index": "0;0;0;0+0;0+0",
        "aff_campus_unique": "Stony Brook;",
        "aff_country_unique_index": "0;0;0+0;0+0;0;0+0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "PatchBatch: A Batch Augmented Loss for Optical Flow",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "48",
        "author_site": "David Gadot, Lior Wolf",
        "author": "David Gadot; Lior Wolf",
        "abstract": "We propose a new pipeline for optical flow computation, based on Deep Learning techniques. We suggest using a Siamese CNN to independently, and in parallel, compute the descriptors of both images. The learned descriptors are then compared efficiently using the L2 norm and do not require network processing of patch pairs. The success of the method is based on an innovative loss function that computes higher moments of the loss distributions for each training batch. Combined with an Approximate Nearest Neighbor patch matching method and a flow interpolation technique, state of the art performance is obtained on the most challenging and competitive optical flow benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gadot_PatchBatch_A_Batch_CVPR_2016_paper.pdf",
        "aff": "The Blavatnik School of Computer Science, Tel Aviv University; The Blavatnik School of Computer Science, Tel Aviv University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 522880,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7628169965113129231&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gadot_PatchBatch_A_Batch_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Tel Aviv University",
        "aff_unique_dep": "Blavatnik School of Computer Science",
        "aff_unique_url": "https://www.tau.ac.il",
        "aff_unique_abbr": "TAU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tel Aviv",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Patches, Planes and Probabilities: A Non-Local Prior for Volumetric 3D Reconstruction",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "27",
        "author_site": "Ali Osman Ulusoy, Michael J. Black, Andreas Geiger",
        "author": "Ali Osman Ulusoy; Michael J. Black; Andreas Geiger",
        "abstract": "In this paper, we propose a non-local structured prior for volumetric multi-view 3D reconstruction. Towards this goal, we present a novel Markov random field model based on ray potentials in which assumptions about large 3D surface patches such as planarity or Manhattan world constraints can be efficiently encoded as probabilistic priors. We further derive an inference algorithm that reasons jointly about voxels, pixels and image segments, and estimates marginal distributions of appearance, occupancy, depth, normals and planarity. Key to tractable inference is a novel hybrid representation that spans both voxel and pixel space and that integrates non-local information from 2D image segmentations in a principled way. We compare our non-local prior to commonly employed local smoothness assumptions and a variety of state-of-the-art volumetric reconstruction baselines on challenging outdoor scenes with textureless and reflective surfaces. Our experiments indicate that regularizing over larger distances has the potential to resolve ambiguities where local regularizers fail.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ulusoy_Patches_Planes_and_CVPR_2016_paper.pdf",
        "aff": "Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany; Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1823641,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6063243362817544510&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "tue.mpg.de;tue.mpg.de;tue.mpg.de",
        "email": "tue.mpg.de;tue.mpg.de;tue.mpg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ulusoy_Patches_Planes_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "T\u00fcbingen",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Pedestrian Detection Inspired by Appearance Constancy and Shape Symmetry",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "59",
        "author_site": "Jiale Cao, Yanwei Pang, Xuelong Li",
        "author": "Jiale Cao; Yanwei Pang; Xuelong Li",
        "abstract": "The discrimination and simplicity of features are very important for effective and efficient pedestrian detection. However, most state-of-the-art methods are unable to achieve good tradeoff between accuracy and efficiency. Inspired by some simple inherent attributes of pedestrians (i.e., appearance constancy and shape symmetry), we propose two new types of non-neighboring features (NNF): side-inner difference features (SIDF) and symmetrical similarity features (SSF). SIDF can characterize the difference between the background and pedestrian and the difference between the pedestrian contour and its inner part. SSF can capture the symmetrical similarity of pedestrian shape. However, it's difficult for neighboring features to have such above characterization abilities. Finally, we propose to combine both non-neighboring and neighboring features for pedestrian detection. It's found that nonneighboring features can further decrease the average miss rate by 4.44%. Experimental results on INRIA and Caltech pedestrian datasets demonstrate the effectiveness and efficiency of the proposed method. Compared to the state-ofthe- art methods without using CNN, our method achieves the best detection performance on Caltech, outperforming the second best method (i.e., Checkboards) by 1.63%.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cao_Pedestrian_Detection_Inspired_CVPR_2016_paper.pdf",
        "aff": "School of Electronic Information Engineering, Tianjin University; School of Electronic Information Engineering, Tianjin University; Xi\u2019an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1156557,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6141760967052516438&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "tju.edu.cn;tju.edu.cn;opt.ac.cn",
        "email": "tju.edu.cn;tju.edu.cn;opt.ac.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cao_Pedestrian_Detection_Inspired_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Tianjin University;Xi'an Institute of Optics and Precision Mechanics",
        "aff_unique_dep": "School of Electronic Information Engineering;Optics and Precision Mechanics",
        "aff_unique_url": "http://www.tju.edu.cn;http://www.xiopia.ac.cn",
        "aff_unique_abbr": "Tianjin University;XIOP",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Xi'an",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Person Re-Identification by Multi-Channel Parts-Based CNN With Improved Triplet Loss Function",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "61",
        "author_site": "De Cheng, Yihong Gong, Sanping Zhou, Jinjun Wang, Nanning Zheng",
        "author": "De Cheng; Yihong Gong; Sanping Zhou; Jinjun Wang; Nanning Zheng",
        "abstract": "Person re-identification across cameras remains a very challenging problem, especially when there are no overlapping fields of view between cameras. In this paper, we present a novel multi-channel parts-based convolutional neural network (CNN) model under the triplet framework for person re-identification. Specifically, the proposed CNN model consists of multiple channels to jointly learn both the global full body and local body-parts features of the input persons. The CNN model is trained by an improved triplet loss function that serves to pull the instances of the same person closer, and at the same time push the instances belonging to different persons farther from each other in the learned feature space. Extensive comparative evaluations demonstrate that our proposed method significantly outperforms many state-of-the-art approaches, including both traditional and deep network-based ones, on the challenging i-LIDS, VIPeR, PRID2011 and CUHK01 datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cheng_Person_Re-Identification_by_CVPR_2016_paper.pdf",
        "aff": "Institute of Arti\ufb01cial Intelligence and Robotics, Xi\u2019an Jiaotong University, Xi\u2019an, Shaanxi, P.R. China; Institute of Arti\ufb01cial Intelligence and Robotics, Xi\u2019an Jiaotong University, Xi\u2019an, Shaanxi, P.R. China; Institute of Arti\ufb01cial Intelligence and Robotics, Xi\u2019an Jiaotong University, Xi\u2019an, Shaanxi, P.R. China; Institute of Arti\ufb01cial Intelligence and Robotics, Xi\u2019an Jiaotong University, Xi\u2019an, Shaanxi, P.R. China; Institute of Arti\ufb01cial Intelligence and Robotics, Xi\u2019an Jiaotong University, Xi\u2019an, Shaanxi, P.R. China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1341889,
        "gs_citation": 1630,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4198871869166461824&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cheng_Person_Re-Identification_by_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Xi'an Jiao Tong University",
        "aff_unique_dep": "Institute of Artificial Intelligence and Robotics",
        "aff_unique_url": "http://www.xjtu.edu.cn",
        "aff_unique_abbr": "XJTU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Xi'an",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Personalizing Human Video Pose Estimation",
        "session": "Actions and Human Pose",
        "status": "Oral",
        "track": "main",
        "pid": "4",
        "author_site": "James Charles, Tomas Pfister, Derek Magee, David Hogg, Andrew Zisserman",
        "author": "James Charles; Tomas Pfister; Derek Magee; David Hogg; Andrew Zisserman",
        "abstract": "We propose a personalized ConvNet pose estimator that automatically adapts itself to the uniqueness of a person's appearance to improve pose estimation in long videos.   We make the following contributions: (i) we show that given a few high-precision pose annotations, e.g. from a generic ConvNet pose estimator, additional annotations can be generated throughout the video using a combination of image-based matching for temporally distant frames, and dense optical flow for temporally local frames; (ii) we develop an occlusion aware self-evaluation model that is able to automatically select the high-quality and reject the erroneous additional annotations; and (iii) we demonstrate that these high-quality annotations can be used to fine-tune a ConvNet pose estimator and thereby personalize it to lock on to key discriminative features of the person's appearance. The outcome is a substantial improvement in the pose estimates for the target video using the personalized ConvNet compared to the original generic ConvNet.  Our method outperforms the state of the art (including top ConvNet methods) by a large margin on three standard benchmarks, as well as on a new challenging YouTube video dataset. Furthermore, we show that training from the automatically generated annotations can be used to improve the performance of a generic ConvNet on other benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Charles_Personalizing_Human_Video_CVPR_2016_paper.pdf",
        "aff": "University of Leeds; University of Oxford; University of Leeds; University of Leeds; University of Oxford",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Charles_Personalizing_Human_Video_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1149362,
        "gs_citation": 132,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2239726860122880856&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "leeds.ac.uk;robots.ox.ac.uk;leeds.ac.uk;leeds.ac.uk;robots.ox.ac.uk",
        "email": "leeds.ac.uk;robots.ox.ac.uk;leeds.ac.uk;leeds.ac.uk;robots.ox.ac.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Charles_Personalizing_Human_Video_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;0;1",
        "aff_unique_norm": "University of Leeds;University of Oxford",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.leeds.ac.uk;https://www.ox.ac.uk",
        "aff_unique_abbr": "Leeds;Oxford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Picking Deep Filter Responses for Fine-Grained Image Recognition",
        "session": "Fine Grained Categorization",
        "status": "Poster",
        "track": "main",
        "pid": "40",
        "author_site": "Xiaopeng Zhang, Hongkai Xiong, Wengang Zhou, Weiyao Lin, Qi Tian",
        "author": "Xiaopeng Zhang; Hongkai Xiong; Wengang Zhou; Weiyao Lin; Qi Tian",
        "abstract": "Recognizing fine-grained sub-categories such as birds and dogs is extremely challenging due to the highly localized and subtle differences in some specific parts. Most previous works rely on object/part level annotations to build part-based representation, which is demanding in practical applications. This paper proposes an automatic fine-grained recognition approach which is free of any object/part annotation at both training and testing stages. Our method explores a unified framework based on two steps of deep filter response picking. The first picking step is to find distinctive filters which respond to specific patterns significantly and consistently, and learn a set of part detectors via iteratively alternating between new positive sample mining and part model retraining. The second picking step is to pool deep filter responses via spatially weighted combination of Fisher Vectors. We conditionally pick deep filter responses to encode them into the final representation, which considers the importance of filter responses themselves. Integrating all these techniques produces a much more powerful framework, and experiments conducted on CUB-200-2011 and Stanford Dogs demonstrate the superiority of our proposed algorithm over the existing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Picking_Deep_Filter_CVPR_2016_paper.pdf",
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; University of Science and Technology of China; Shanghai Jiao Tong University; University of Texas at San Antonio",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1865823,
        "gs_citation": 401,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11927290233570713285&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;ustc.edu.cn;sjtu.edu.cn;cs.utsa.edu",
        "email": "sjtu.edu.cn;sjtu.edu.cn;ustc.edu.cn;sjtu.edu.cn;cs.utsa.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Picking_Deep_Filter_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0;2",
        "aff_unique_norm": "Shanghai Jiao Tong University;University of Science and Technology of China;University of Texas at San Antonio",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.sjtu.edu.cn;http://www.ustc.edu.cn;https://www.utsa.edu",
        "aff_unique_abbr": "SJTU;USTC;UTSA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Antonio",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Piecewise-Planar 3D Approximation From Wide-Baseline Stereo",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "32",
        "author_site": "C\u00e9dric Verleysen, Christophe De Vleeschouwer",
        "author": "Cedric Verleysen; Christophe De Vleeschouwer",
        "abstract": "This paper approximates the 3D geometry of a scene by a small number of 3D planes. The method is especially suited to man-made scenes, and only requires two calibrated wide-baseline views as inputs. It relies on the computation of a dense but noisy 3D point cloud, as for example obtained by matching DAISY descriptors between the views.  It then segments one of the two reference images, and adopts a multi-model fitting process to assign a 3D plane to each region, when the region is not detected as occluded. A pool of 3D plane hypotheses is first derived from the 3D point cloud, to include planes that reasonably approximate the part of the 3D point cloud observed from each reference view between randomly selected triplets of 3D points. The hypothesis-to-region assignment problem is then formulated as an energy-minimization problem, which simultaneously optimizes an original data-fidelity term, the assignment smoothness over neighboring regions, and the number of assigned planar proxies. The synthesis of intermediate viewpoints demonstrates the effectiveness of our 3D reconstruction, and thereby the relevance of our proposed data-fidelity metric.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Verleysen_Piecewise-Planar_3D_Approximation_CVPR_2016_paper.pdf",
        "aff": "ICTEAM institute, Universit\u00e9 catholique de Louvain (UCL), Louvain-la-Neuve, Belgium; ICTEAM institute, Universit\u00e9 catholique de Louvain (UCL), Louvain-la-Neuve, Belgium",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Verleysen_Piecewise-Planar_3D_Approximation_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 6503505,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7837274920298491096&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff_domain": "uclouvain.be;uclouvain.be",
        "email": "uclouvain.be;uclouvain.be",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Verleysen_Piecewise-Planar_3D_Approximation_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universit\u00e9 catholique de Louvain",
        "aff_unique_dep": "ICTEAM institute",
        "aff_unique_url": "https://www.uclouvain.be",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Louvain-la-Neuve",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "title": "Pose-Aware Face Recognition in the Wild",
        "session": "Face Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "31",
        "author_site": "Iacopo Masi, Stephen Rawls, G\u00e9rard Medioni, Prem Natarajan",
        "author": "Iacopo Masi; Stephen Rawls; Gerard Medioni; Prem Natarajan",
        "abstract": "We propose a method to push the frontiers of unconstrained face recognition in the wild, focusing on the problem of extreme pose variations. As opposed to current techniques which either expect a single model to learn pose invariance through massive amounts of training data, or which normalize images to a single frontal pose, our method explicitly tackles pose variation by using multiple pose-specific models and rendered face images.  We leverage deep Convolutional Neural Networks (CNNs) to learn discriminative representations we call Pose-Aware Models (PAMs) using 500K images from the CASIA WebFace dataset. We present a comparative evaluation on the new IARPA Janus Benchmark A (IJB-A) and PIPA datasets. On these datasets PAMs achieve remarkably better performance than commercial products and surprisingly also outperform methods that are specifically fine-tuned on the target dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Masi_Pose-Aware_Face_Recognition_CVPR_2016_paper.pdf",
        "aff": "USC Institute for Robotics and Intelligent Systems (IRIS), Los Angeles, CA; USC Information Sciences Institute (ISI), Marina Del Rey, CA; USC Institute for Robotics and Intelligent Systems (IRIS), Los Angeles, CA; USC Information Sciences Institute (ISI), Marina Del Rey, CA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3350944,
        "gs_citation": 321,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8112909669524398760&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "usc.edu;isi.edu;usc.edu;isi.edu",
        "email": "usc.edu;isi.edu;usc.edu;isi.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Masi_Pose-Aware_Face_Recognition_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "Institute for Robotics and Intelligent Systems",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;1;0;1",
        "aff_campus_unique": "Los Angeles;Marina Del Rey",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Predicting Motivations of Actions by Leveraging Text",
        "session": "Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "79",
        "author_site": "Carl Vondrick, Deniz Oktay, Hamed Pirsiavash, Antonio Torralba",
        "author": "Carl Vondrick; Deniz Oktay; Hamed Pirsiavash; Antonio Torralba",
        "abstract": "Understanding human actions is a key problem in computer vision. However, recognizing actions is only the first step of understanding what a person is doing. In this paper, we introduce the problem of predicting why a person has performed an action in images. This problem has many applications in human activity understanding, such as anticipating or explaining an action. To study this problem, we introduce a new dataset of people performing actions annotated with likely motivations. However, the information in an image alone may not be sufficient to automatically solve this task. Since humans can rely on their lifetime of experiences to infer motivation, we propose to give computer vision systems access to some of these experiences by using recently developed natural language models to mine knowledge stored in massive amounts of text. While we are still far away from fully understanding motivation, our results suggest that transferring knowledge from language into vision can help machines understand why people in images might be performing an action.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Vondrick_Predicting_Motivations_of_CVPR_2016_paper.pdf",
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology; University of Maryland, Baltimore County; Massachusetts Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1971414,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16394667338567838004&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mit.edu;mit.edu;umbc.edu;mit.edu",
        "email": "mit.edu;mit.edu;umbc.edu;mit.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Vondrick_Predicting_Motivations_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of Maryland, Baltimore County",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.umbc.edu",
        "aff_unique_abbr": "MIT;UMBC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Baltimore County",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Predicting When Saliency Maps Are Accurate and Eye Fixations Consistent",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "58",
        "author_site": "Anna Volokitin, Michael Gygli, Xavier Boix",
        "author": "Anna Volokitin; Michael Gygli; Xavier Boix",
        "abstract": "Many computational models of visual attention use image features and machine learning techniques to predict eye fixation locations as saliency maps. Recently, the success of Deep Convolutional Neural Networks (DCNNs) for object recognition has opened a new avenue for computational models of visual attention due to the tight link between visual attention and object recognition. In this paper, we show that using features from DCNNs for object recognition we can make predictions that enrich the information provided by saliency models. Namely, we can estimate the reliability of a saliency model from the raw image, which serves as a meta-saliency measure that may be used to select the best saliency algorithm for an image. Analogously, the consistency of the eye fixations among  subjects, i.e. the agreement between the eye fixation locations of different subjects, can also be predicted and used by a designer to assess whether subjects reach a consensus about salient image locations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Volokitin_Predicting_When_Saliency_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3085522,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7423507047352192341&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Volokitin_Predicting_When_Saliency_CVPR_2016_paper.html"
    },
    {
        "title": "Predicting the Where and What of Actors and Actions Through Online Action Localization",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "42",
        "author_site": "Khurram Soomro, Haroon Idrees, Mubarak Shah",
        "author": "Khurram Soomro; Haroon Idrees; Mubarak Shah",
        "abstract": "This paper proposes a novel approach to tackle the challenging problem of 'online action localization' which entails predicting actions and their locations as they happen in a video. Typically, action localization or recognition is performed in an offline manner where all the frames in the video are processed together and action labels are not predicted for the future. This dis-allows timely localization of actions - an important consideration for surveillance tasks. In our approach, given a batch of frames from the immediate past in a video, we estimate pose and over- segment the current frame into superpixels. Next, we discriminatively train an actor foreground model on the superpixels using the pose bounding boxes. A Conditional Random Field with superpixels as nodes, and edges connecting spatio-temporal neighbors is used to obtain action segments. The action confidence is predicted using dynamic programming on SVM scores obtained on short segments of the video, thereby capturing sequential information of the actions. The issue of visual drift is handled by updating the appearance model and pose refinement in an online manner. Lastly, we introduce a new measure to quantify the performance of action prediction (i.e. online action localization), which analyzes how the prediction accuracy varies as a function of observed portion of the video. Our experiments suggest that despite using only a few frames to localize actions at each time instant, we are able to predict the action and obtain competitive results to state-of-the-art offline methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Soomro_Predicting_the_Where_CVPR_2016_paper.pdf",
        "aff": "Center for Research in Computer Vision (CRCV), University of Central Florida (UCF); Center for Research in Computer Vision (CRCV), University of Central Florida (UCF); Center for Research in Computer Vision (CRCV), University of Central Florida (UCF)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1895399,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=202708545690053624&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.ucf.edu;cs.ucf.edu;cs.ucf.edu",
        "email": "cs.ucf.edu;cs.ucf.edu;cs.ucf.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Soomro_Predicting_the_Where_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Central Florida",
        "aff_unique_dep": "Center for Research in Computer Vision (CRCV)",
        "aff_unique_url": "https://www.ucf.edu",
        "aff_unique_abbr": "UCF",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "UCF",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Primary Object Segmentation in Videos via Alternate Convex Optimization of Foreground and Background Distributions",
        "session": "Video Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "75",
        "author_site": "Won-Dong Jang, Chulwoo Lee, Chang-Su Kim",
        "author": "Won-Dong Jang; Chulwoo Lee; Chang-Su Kim",
        "abstract": "An unsupervised video object segmentation algorithm, which discovers a primary object in a video sequence automatically, is proposed in this work. We introduce three energies in terms of foreground and background probability distributions: Markov, spatiotemporal, and antagonistic energies. Then, we minimize a hybrid of the three energies to separate a primary object from its background. However, the hybrid energy is nonconvex. Therefore, we develop the alternate convex optimization (ACO) scheme, which decomposes the nonconvex optimization into two quadratic programs. Moreover, we propose the forward-backward strategy, which performs the segmentation sequentially from the first to the last frames and then vice versa, to exploit temporal correlations. Experimental results on extensive datasets demonstrate that the proposed ACO algorithm outperforms the state-of-the-art techniques significantly.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Jang_Primary_Object_Segmentation_CVPR_2016_paper.pdf",
        "aff": "Korea University; Northumbria University; Korea University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1849179,
        "gs_citation": 87,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3499460786466312523&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "mcl.korea.ac.kr;northumbria.ac.uk;korea.ac.kr",
        "email": "mcl.korea.ac.kr;northumbria.ac.uk;korea.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Jang_Primary_Object_Segmentation_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Korea University;Northumbria University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.korea.ac.kr;https://www.northumbria.ac.uk",
        "aff_unique_abbr": "KU;Northumbria",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "South Korea;United Kingdom"
    },
    {
        "title": "Principled Parallel Mean-Field Inference for Discrete Random Fields",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "59",
        "author_site": "Pierre Baqu\u00e9, Timur Bagautdinov, Fran\u00e7ois Fleuret, Pascal Fua",
        "author": "Pierre Baque; Timur Bagautdinov; Francois Fleuret; Pascal Fua",
        "abstract": "Mean-field variational  inference is one  of the  most popular approaches to  inference in discrete random fields.  Standard mean-field optimization is based on coordinate descent and in many situations can be impractical. Thus, in practice, various parallel  techniques are used, which  either rely on ad hoc smoothing with heuristically set parameters, or put strong constraints on the type of models. In this paper, we propose a novel proximal gradient-based approach to optimizing the variational objective. It is naturally parallelizable and easy to implement.  We  prove  its convergence, and then  demonstrate that,  in practice,  it yields faster  convergence and often  finds better optima than  more traditional mean-field optimization techniques. Moreover, our method is less sensitive to the choice of parameters.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Baque_Principled_Parallel_Mean-Field_CVPR_2016_paper.pdf",
        "aff": "CVLab, EPFL, Lausanne, Switzerland; CVLab, EPFL, Lausanne, Switzerland; CVLab, EPFL, Lausanne, Switzerland + IDIAP, Martigny, Switzerland; CVLab, EPFL, Lausanne, Switzerland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Baque_Principled_Parallel_Mean-Field_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 700647,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15525459605584018764&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Baque_Principled_Parallel_Mean-Field_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "EPFL;IDIAP",
        "aff_unique_dep": "CVLab;",
        "aff_unique_url": "https://www.epfl.ch;https://www.idiap.ch",
        "aff_unique_abbr": "EPFL;",
        "aff_campus_unique_index": "0;0;0+1;0",
        "aff_campus_unique": "Lausanne;Martigny",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Prior-Less Compressible Structure From Motion",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "36",
        "author_site": "Chen Kong, Simon Lucey",
        "author": "Chen Kong; Simon Lucey",
        "abstract": "Many non-rigid 3D structures are not modelled well through a low-rank subspace assumption.  This is problematic when it comes to their reconstruction through Structure from Motion (SfM).  We argue in this paper that a more expressive and general assumption can be made around compressible 3D structures.  The vision community, however, has hitherto struggled to formulate effective strategies for recovering such structures after projection without the aid of additional priors (e.g. temporal ordering, rigid substructures, etc.).  In this paper we present a \"prior-less\" approach to solve compressible SfM.  Specifically, we demonstrate how the problem of SfM - assuming compressible 3D structures - can be theoretically characterized as a block sparse dictionary learning problem.  We validate our approach experimentally by demonstrating reconstructions of 3D structures that are intractable using current state-of-the-art low-rank SfM approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kong_Prior-Less_Compressible_Structure_CVPR_2016_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Kong_Prior-Less_Compressible_Structure_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1843974,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7241925528425539346&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kong_Prior-Less_Compressible_Structure_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "ProNet: Learning to Propose Object-Specific Boxes for Cascaded Neural Networks",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "49",
        "author_site": "Chen Sun, Manohar Paluri, Ronan Collobert, Ram Nevatia, Lubomir Bourdev",
        "author": "Chen Sun; Manohar Paluri; Ronan Collobert; Ram Nevatia; Lubomir Bourdev",
        "abstract": "This paper aims to classify and locate objects accurately and efficiently, without using bounding box annotations. It is challenging as objects in the wild could appear at arbitrary locations and in different scales. In this paper, we propose a novel classification architecture ProNet based on convolutional neural networks. It uses computationally efficient neural networks to propose image regions that are likely to contain objects, and applies more powerful but slower networks on the proposed regions. The basic building block is a multi-scale fully-convolutional network which assigns object confidence scores to boxes at different locations and scales. We show that such networks can be trained effectively using image-level annotations, and can be connected into cascades or trees for efficient object classification. ProNet outperforms previous state-of-the-art significantly on PASCAL VOC 2012 and MS COCO datasets for object classification and point-based localization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sun_ProNet_Learning_to_CVPR_2016_paper.pdf",
        "aff": "USC; Facebook AI Research; Facebook AI Research; USC; UC Berkeley",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1233541,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15474166213507477294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "usc.edu;usc.edu;fb.com;fb.com;gmail.com",
        "email": "usc.edu;usc.edu;fb.com;fb.com;gmail.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sun_ProNet_Learning_to_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;0;2",
        "aff_unique_norm": "University of Southern California;Meta;University of California, Berkeley",
        "aff_unique_dep": ";Facebook AI Research;",
        "aff_unique_url": "https://www.usc.edu;https://research.facebook.com;https://www.berkeley.edu",
        "aff_unique_abbr": "USC;FAIR;UC Berkeley",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Los Angeles;;Berkeley",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Progressive Feature Matching With Alternate Descriptor Selection and Correspondence Enrichment",
        "session": "Feature Extraction and Matching",
        "status": "Poster",
        "track": "main",
        "pid": "37",
        "author_site": "Yuan-Ting Hu, Yen-Yu Lin",
        "author": "Yuan-Ting Hu; Yen-Yu Lin",
        "abstract": "We address two difficulties in establishing an accurate system for image matching. First, image matching relies on the descriptor for feature extraction, but the optimal descriptor often varies from image to image, or even patch to patch. Second, conventional matching approaches carry out geometric checking on a small set of correspondence candidates due to the concern of efficiency. It may result in restricted performance in recall. We aim at tackling the two issues by integrating adaptive descriptor selection and progressive candidate enrichment into image matching. We consider that the two integrated components are complementary: The high-quality matching yielded by adaptively selected descriptors helps in exploring more plausible candidates, while the enriched candidate set serves as a better reference for descriptor selection. It motivates us to formulate image matching as a joint optimization problem, in which adaptive descriptor selection and progressive correspondence enrichment are alternately conducted. Our approach is comprehensively evaluated and compared with the state-of-the-art approaches on two benchmarks. The promising results manifest its effectiveness.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hu_Progressive_Feature_Matching_CVPR_2016_paper.pdf",
        "aff": "Academia Sinica, Taiwan; Academia Sinica, Taiwan",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 832942,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14377210084538208730&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hu_Progressive_Feature_Matching_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Academia Sinica",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sinica.edu.tw",
        "aff_unique_abbr": "Academia Sinica",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Progressive Prioritized Multi-View Stereo",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "23",
        "author_site": "Alex Locher, Michal Perdoch, Luc Van Gool",
        "author": "Alex Locher; Michal Perdoch; Luc Van Gool",
        "abstract": "This work proposes a progressive patch based multi-view stereo algorithm able to deliver a dense point cloud at any time.  This enables an immediate feedback on the reconstruction process in a user centric scenario.  With increasing processing time, the model is improved in terms of resolution and accuracy.  The algorithm explicitly handles input images with varying effective scale and creates visually pleasing point clouds.  A priority scheme assures that the limited computational power is invested in scene parts, where the user is most interested in or the overall error can be reduced the most.  The architecture of the proposed pipeline allows fast processing times in large scenes using a pure open-source CPU implementation.  We show the performance of our algorithm on challenging standard datasets as well as on real-world scenes and compare it to the baseline.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Locher_Progressive_Prioritized_Multi-View_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1216493,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4005571281634015364&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Locher_Progressive_Prioritized_Multi-View_CVPR_2016_paper.html"
    },
    {
        "title": "Progressively Parsing Interactional Objects for Fine Grained Action Detection",
        "session": "Events, Activities, and Surveillance",
        "status": "Poster",
        "track": "main",
        "pid": "28",
        "author_site": "Bingbing Ni, Xiaokang Yang, Shenghua Gao",
        "author": "Bingbing Ni; Xiaokang Yang; Shenghua Gao",
        "abstract": "Fine grained video action analysis often requires reliable detection and tracking of various interacting objects and human body parts, denoted as interactional object parsing. However, most of the previous methods based on either independent or joint object detection might suffer from high model complexity and challenging image content, e.g., illumination/pose/appearance/scale variation, motion, occlusion etc. In this work, we propose an end-to-end system based on recursive neural network to perform frame by frame interactional object parsing, which can alleviate the difficulty through a incremental manner. Our key innovation is that: instead of jointly outputting all object detections at once, for each frame, we use a set of long-short term memory (LSTM) nodes to incrementally refine the detections. After passing each LSTM node, more object detections are consolidated and thus more contextual information could be utilized to determine more difficult object detections. Extensive experiments on two benchmark fine grained activity datasets demonstrate that our proposed algorithm achieves better interacting object detection performance, which in turn boosts the action recognition performance over the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ni_Progressively_Parsing_Interactional_CVPR_2016_paper.pdf",
        "aff": "Shanghai Jiaotong University; Shanghai Jiaotong University; ShanghaiTech University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 541993,
        "gs_citation": 95,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16074033258834232542&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;shanghaitech.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;shanghaitech.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ni_Progressively_Parsing_Interactional_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;ShanghaiTech University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.shanghaitech.edu.cn",
        "aff_unique_abbr": "SJTU;ShanghaiTech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Proposal Flow",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "48",
        "author_site": "Bumsub Ham, Minsu Cho, Cordelia Schmid, Jean Ponce",
        "author": "Bumsub Ham; Minsu Cho; Cordelia Schmid; Jean Ponce",
        "abstract": "Finding image correspondences remains a challenging problem in the presence of intra-class variations and large changes in scene layout. Semantic flow methods are designed to handle images depicting different instances of the same object or scene category.  We introduce a novel approach to semantic flow, dubbed proposal flow, that establishes reliable correspondences using object proposals. Unlike prevailing semantic flow approaches that operate on pixels or regularly sampled local regions, proposal flow benefits from the characteristics of modern object proposals, that exhibit high repeatability at multiple scales, and can take advantage of both local and geometric consistency constraints among proposals. We also show that proposal flow can effectively be transformed into a conventional dense flow field. We introduce a new dataset that can be used to evaluate both general semantic flow techniques and region-based approaches such as proposal flow. We use this benchmark to compare different matching algorithms, object proposals, and region features within proposal flow, to the state of the art in semantic flow. This comparison, along with experiments on standard datasets, demonstrates that proposal flow significantly outperforms existing semantic flow methods in various settings.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ham_Proposal_Flow_CVPR_2016_paper.pdf",
        "aff": "Inria+\u00b4Ecole Normale Sup\u00e9rieure / PSL Research University; Inria+\u00b4Ecole Normale Sup\u00e9rieure / PSL Research University; Inria Grenoble Rh\u00f4ne-Alpes, Laboratoire Jean Kuntzmann; \u00b4Ecole Normale Sup\u00e9rieure / PSL Research University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3206508,
        "gs_citation": 164,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12719637162662981156&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 19,
        "aff_domain": "inria.fr;inria.fr;inria.fr;ens.fr",
        "email": "inria.fr;inria.fr;inria.fr;ens.fr",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ham_Proposal_Flow_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1;2;1",
        "aff_unique_norm": "INRIA;Ecole Normale Sup\u00e9rieure;INRIA Grenoble Rh\u00f4ne-Alpes",
        "aff_unique_dep": ";;Laboratoire Jean Kuntzmann",
        "aff_unique_url": "https://www.inria.fr;https://www.ens.fr;https://www.inria.fr/grenoble",
        "aff_unique_abbr": "Inria;ENS;Inria",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Grenoble",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "Proximal Riemannian Pursuit for Large-Scale Trace-Norm Minimization",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "62",
        "author_site": "Mingkui Tan, Shijie Xiao, Junbin Gao, Dong Xu, Anton van den Hengel, Qinfeng Shi",
        "author": "Mingkui Tan; Shijie Xiao; Junbin Gao; Dong Xu; Anton van den Hengel; Qinfeng Shi",
        "abstract": "Trace-norm regularization plays an important role in many areas such as machine learning and computer vision. Solving trace-norm regularized Trace-norm regularization plays an important role in many areas such as computer vision and machine learning. When solving general large-scale trace-norm regularized problems, existing methods may be computationally expensive due to many high-dimensional truncated singular value decompositions (SVDs) or the unawareness of matrix ranks. In this paper, we propose a proximal Riemannian pursuit (PRP) paradigm which addresses a sequence of trace-norm regularized subproblems defined on nonlinear matrix varieties. To address the subproblem, we extend the proximal gradient method on vector space to nonlinear matrix varieties, in which the SVDs of intermediate solutions are maintained by cheap low-rank QR decompositions, therefore making the proposed method more scalable. Empirical studies on several tasks, such as matrix completion and low-rank representation based subspace clustering, demonstrate the competitive performance of the proposed paradigms over existing methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tan_Proximal_Riemannian_Pursuit_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Tan_Proximal_Riemannian_Pursuit_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 789689,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7273933718446291172&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tan_Proximal_Riemannian_Pursuit_CVPR_2016_paper.html"
    },
    {
        "title": "Pull the Plug? Predicting If Computers or Humans Should Segment Images",
        "session": "Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "41",
        "author_site": "Danna Gurari, Suyog Jain, Margrit Betke, Kristen Grauman",
        "author": "Danna Gurari; Suyog Jain; Margrit Betke; Kristen Grauman",
        "abstract": "Foreground object segmentation is a critical step for many image analysis tasks.  While automated methods can produce high-quality results, their failures disappoint users in need of practical solutions.  We propose a resource allocation framework for predicting how best to allocate a fixed budget of human annotation effort in order to collect higher quality segmentations for a given batch of images and automated methods.  The framework is based on a proposed prediction module that estimates the quality of given algorithm-drawn segmentations.  We demonstrate the value of the framework for two novel tasks related to \"pulling the plug\" on computer and human annotators.  Specifically, we implement two systems that automatically decide, for a batch of images, when to replace 1) humans with computers to create coarse segmentations required to initialize segmentation tools and 2) computers with humans to create final, fine-grained segmentations.  Experiments demonstrate the advantage of relying on a mix of human and computer efforts over relying on either resource alone for segmenting objects in three diverse datasets representing visible, phase contrast microscopy, and fluorescence microscopy images.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gurari_Pull_the_Plug_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 799920,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=65412859911916424&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gurari_Pull_the_Plug_CVPR_2016_paper.html"
    },
    {
        "title": "Quantized Convolutional Neural Networks for Mobile Devices",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "29",
        "author_site": "Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, Jian Cheng",
        "author": "Jiaxiang Wu; Cong Leng; Yuhang Wang; Qinghao Hu; Jian Cheng",
        "abstract": "Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response. Extensive experiments on the ILSVRC-12 benchmark demonstrate 4 6x speed-up and 15 20x compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wu_Quantized_Convolutional_Neural_CVPR_2016_paper.pdf",
        "aff": "National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wu_Quantized_Convolutional_Neural_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1240103,
        "gs_citation": 1586,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4949425006385395544&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wu_Quantized_Convolutional_Neural_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation",
        "aff_unique_url": "http://www.ia.cas.cn",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "RAID-G: Robust Estimation of Approximate Infinite Dimensional Gaussian With Application to Material Recognition",
        "session": "Statistical Methods and Learning",
        "status": "Poster",
        "track": "main",
        "pid": "69",
        "author_site": "Qilong Wang, Peihua Li, Wangmeng Zuo, Lei Zhang",
        "author": "Qilong Wang; Peihua Li; Wangmeng Zuo; Lei Zhang",
        "abstract": "Infinite dimensional covariance descriptors can provide richer and more discriminative information than their low dimensional counterparts. In this paper, we propose a novel image descriptor, namely, robust approximate infinite dimensional Gaussian (RAID-G). The challenges of RAID-G mainly lie on two aspects: (1) description of infinite dimensional Gaussian is difficult due to its non-linear Riemannian geometric structure and the infinite dimensional setting, hence effective approximation is necessary; (2) traditional maximum likelihood estimation (MLE) is not robust to high (even infinite) dimensional covariance matrix in Gaussian setting. To address these challenges, explicit feature mapping (EFM) is first introduced for effective approximation of infinite dimensional Gaussian induced by additive kernel function, and then a new regularized MLE method based on von Neumann divergence is proposed for robust estimation of covariance matrix. The EFM and proposed regularized MLE allow a closed-form of RAID-G, which is very efficient and effective for high dimensional features. We extend RAID-G by using the outputs of deep convolutional neural networks as original features, and apply it to material recognition. Our approach is evaluated on five material benchmarks and one fine-grained benchmark. It achieves 84.9% accuracy on FMD and 86.3% accuracy on UIUC material database, which are much higher than state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_RAID-G_Robust_Estimation_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wang_RAID-G_Robust_Estimation_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17524051613372868840&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_RAID-G_Robust_Estimation_CVPR_2016_paper.html"
    },
    {
        "title": "RAW Image Reconstruction Using a Self-Contained sRGB-JPEG Image With Only 64 KB Overhead",
        "session": "Image Processing and Restoration",
        "status": "Spotlight",
        "track": "main",
        "pid": "17",
        "author_site": "Rang M. H. Nguyen, Michael S. Brown",
        "author": "Rang M. H. Nguyen; Michael S. Brown",
        "abstract": "Most camera images are saved as 8-bit standard RGB (sRGB) compressed JPEGs.  Even when JPEG compression is set to its highest quality, the encoded sRGB image has been significantly processed in terms of color and tone manipulation.   This makes sRGB-JPEG images undesirable for many computer vision tasks that assume a direct relationship between pixel values and incoming light.  For such applications, the RAW image format is preferred, as RAW represents a minimally processed, sensor-specific RGB image with higher dynamic range that is linear with respect to scene radiance.   The drawback with RAW images, however, is that they require large amounts of storage and are not well-supported by many imaging applications.   To address this issue, we present a method to encode the necessary metadata within an sRGB image to reconstruct a high-quality RAW image.   Our approach requires no calibration of the camera and can reconstruct the original RAW to within  0.3% error with only a 64 KB overhead for the additional data.  More importantly, our output is a fully self-contained 100% complainant sRGB-JPEG file that can be used as-is, not affecting any existing image workflow - the RAW image can be extracted when needed, or ignored otherwise.  We detail our approach and show its effectiveness against competing strategies.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Nguyen_RAW_Image_Reconstruction_CVPR_2016_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3763663934301135202&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Nguyen_RAW_Image_Reconstruction_CVPR_2016_paper.html"
    },
    {
        "title": "RIFD-CNN: Rotation-Invariant and Fisher Discriminative Convolutional Neural Networks for Object Detection",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "67",
        "author_site": "Gong Cheng, Peicheng Zhou, Junwei Han",
        "author": "Gong Cheng; Peicheng Zhou; Junwei Han",
        "abstract": "Thanks to the powerful feature representations obtained through deep convolutional neural network (CNN), the performance of object detection has recently been substantially boosted. Despite the remarkable success, the problems of object rotation, within-class variability, and between-class similarity remain several major challenges. To address these problems, this paper proposes a novel and effective method to learn a rotation-invariant and Fisher discriminative CNN (RIFD-CNN) model. This is achieved by introducing and learning a rotation-invariant layer and a Fisher discriminative layer, respectively, on the basis of the existing high-capacity CNN architectures. Specifically, the rotation-invariant layer is trained by imposing an explicit regularization constraint on the objective function that enforces invariance on the CNN features before and after rotating. The Fisher discriminative layer is trained by imposing the Fisher discrimination criterion on the CNN features so that they have small within-class scatter but large between-class separation. In the experiments, we comprehensively evaluate the proposed method for object detection task on a public available aerial image dataset and the PASCAL VOC 2007 dataset. State-of-the-art results are achieved compared with the existing baseline methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cheng_RIFD-CNN_Rotation-Invariant_and_CVPR_2016_paper.pdf",
        "aff": "School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2758234,
        "gs_citation": 204,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7301211001543171162&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nwpu.edu.cn;gmail.com;nwpu.edu.cn",
        "email": "nwpu.edu.cn;gmail.com;nwpu.edu.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cheng_RIFD-CNN_Rotation-Invariant_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Northwestern Polytechnical University",
        "aff_unique_dep": "School of Automation",
        "aff_unique_url": "https://www.nwpu.edu.cn",
        "aff_unique_abbr": "NPU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Xi'an",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Rain Streak Removal Using Layer Priors",
        "session": "Image Enhancement, Restoration, and Texture",
        "status": "Poster",
        "track": "main",
        "pid": "51",
        "author_site": "Yu Li, Robby T. Tan, Xiaojie Guo, Jiangbo Lu, Michael S. Brown",
        "author": "Yu Li; Robby T. Tan; Xiaojie Guo; Jiangbo Lu; Michael S. Brown",
        "abstract": "This paper addresses the problem of rain streak removal from a single image. Rain streaks impair visibility of an image and introduce undesirable interference that can severely affect the performance of computer vision algorithms.   Rain streak removal can be formulated as a layer decomposition problem, with a rain streak layer superimposed on a background layer containing the true scene content. Existing decomposition methods that address this problem employ either dictionary learning methods or impose a low rank structure on the appearance of the rain streaks.   While these methods can improve the overall visibility, they tend to leave too many rain streaks in the background image or over-smooth the background image.   In this paper, we propose an effective method that uses simple patch-based priors for both the background and rain layers.  These priors are based on Gaussian mixture models and can accommodate multiple orientations and scales of the rain streaks. This simple approach removes rain streaks better than the existing methods qualitatively and quantitatively.  We overview our method and demonstrate its effectiveness over prior work on a number of examples.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Rain_Streak_Removal_CVPR_2016_paper.pdf",
        "aff": "Advanced Digital Sciences Center, Singapore; Yale-NUS College, Singapore + National University of Singapore, Singapore; State Key Lab of Information Security, IIE, CAS, China; Advanced Digital Sciences Center, Singapore; National University of Singapore, Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1856144,
        "gs_citation": 1084,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4421295834015997808&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Rain_Streak_Removal_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+2;3;0;2",
        "aff_unique_norm": "Advanced Digital Sciences Center;Yale-NUS College;National University of Singapore;Institute of Information Engineering, Chinese Academy of Sciences",
        "aff_unique_dep": ";;;State Key Lab of Information Security",
        "aff_unique_url": ";https://www.yale-nus.edu.sg;https://www.nus.edu.sg;http://iie.cas.cn",
        "aff_unique_abbr": ";Yale-NUS;NUS;IIE, CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;1;0;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "title": "Random Features for Sparse Signal Classification",
        "session": "Learning and Optimization",
        "status": "Spotlight",
        "track": "main",
        "pid": "12",
        "author_site": "Jen-Hao Rick Chang, Aswin C. Sankaranarayanan, B. V. K. Vijaya Kumar",
        "author": "Jen-Hao Rick Chang; Aswin C. Sankaranarayanan; B. V. K. Vijaya Kumar",
        "abstract": "Random features is an approach for kernel-based inference on large datasets. In this paper, we derive  performance guarantees for random features on signals, like images, that enjoy sparse representations and show that the number of random features required to achieve a desired approximation of the kernel similarity matrix can be significantly smaller for sparse signals. Based on this, we propose a scheme termed compressive random features that first obtains low-dimensional projections of a dataset and, subsequently, derives random features on the low-dimensional projections. This scheme provides significant improvements in signal dimensionality, computational time, and storage costs over traditional random features while enjoying similar theoretical  guarantees for achieving  inference performance. We support our claims by providing empirical results across many datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chang_Random_Features_for_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3902922,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14215491210575090532&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chang_Random_Features_for_CVPR_2016_paper.html"
    },
    {
        "title": "ReD-SFA: Relation Discovery Based Slow Feature Analysis for Trajectory Clustering",
        "session": "Video Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "81",
        "author_site": "Zhang Zhang, Kaiqi Huang, Tieniu Tan, Peipei Yang, Jun Li",
        "author": "Zhang Zhang; Kaiqi Huang; Tieniu Tan; Peipei Yang; Jun Li",
        "abstract": "For spectral embedding/clustering, it is still an open problem on how to construct an relation graph to reflect the intrinsic structures in data. In this paper, we proposed an approach, named Relation Discovery based Slow Feature Analysis (ReD-SFA), for feature learning and graph construction simultaneously. Given an initial graph with only a few nearest but most reliable pairwise relations, new reliable relations are discovered by an assumption of reliability preservation, i.e., the reliable relations will preserve their reliabilities in the learnt projection subspace. We formulate the idea as a cross entropy (CE) minimization problem to reduce the discrepancy between two Bernoulli distributions parameterized by the updated distances and the existing relation graph respectively. Furthermore, to overcome the imbalanced distribution of samples, a Boosting-like strategy is proposed to balance the discovered relations over all clusters. To evaluate the proposed method, extensive experiments are performed with various trajectory clustering tasks, including motion segmentation, time series clustering and crowd detection. The results demonstrate that ReD-SFA can discover reliable intra-cluster relations with high precision, and competitive clustering performance can be achieved in comparison with state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_ReD-SFA_Relation_Discovery_CVPR_2016_paper.pdf",
        "aff": "CRIPAC & NLPR, Institute of Automation, Chinese Academy of Sciences; CRIPAC & NLPR, Institute of Automation, Chinese Academy of Sciences; CRIPAC & NLPR, Institute of Automation, Chinese Academy of Sciences; CRIPAC & NLPR, Institute of Automation, Chinese Academy of Sciences; Centre for Quantum Computation and Intelligent Systems, University of Technology Sydney",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1205838,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15725534281181863760&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;uts.edu.au",
        "email": "nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;uts.edu.au",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_ReD-SFA_Relation_Discovery_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Technology Sydney",
        "aff_unique_dep": "Institute of Automation;Centre for Quantum Computation and Intelligent Systems",
        "aff_unique_url": "http://www.ia.cas.cn;https://www.uts.edu.au",
        "aff_unique_abbr": "CAS;UTS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Sydney",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Real-Time Action Recognition With Enhanced Motion Vector CNNs",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "49",
        "author_site": "Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, Hanli Wang",
        "author": "Bowen Zhang; Limin Wang; Zhe Wang; Yu Qiao; Hanli Wang",
        "abstract": "The deep two-stream architecture exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Real-Time_Action_Recognition_CVPR_2016_paper.pdf",
        "aff": "Shenzhen key lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China + Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China; Shenzhen key lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China + Computer Vision Lab, ETH Zurich, Switzerland; Shenzhen key lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Shenzhen key lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 585528,
        "gs_citation": 546,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15027773333209163725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Real-Time_Action_Recognition_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+2;0;0;1",
        "aff_unique_norm": "Shenzhen Institute of Advanced Technology;Tongji University;ETH Zurich",
        "aff_unique_dep": "key lab of Computer Vision & Pattern Recognition;Key Laboratory of Embedded System and Service Computing;Computer Vision Lab",
        "aff_unique_url": "http://www.siat.ac.cn;https://www.tongji.edu.cn;https://www.ethz.ch",
        "aff_unique_abbr": "SIAT;Tongji;ETHZ",
        "aff_campus_unique_index": "0+1;0;0;0;1",
        "aff_campus_unique": "Shenzhen;Shanghai;",
        "aff_country_unique_index": "0+0;0+1;0;0;0",
        "aff_country_unique": "China;Switzerland"
    },
    {
        "title": "Real-Time Depth Refinement for Specular Objects",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "63",
        "author_site": "Roy Or-El, Rom Hershkovitz, Aaron Wetzler, Guy Rosman, Alfred M. Bruckstein, Ron Kimmel",
        "author": "Roy Or-El; Rom Hershkovitz; Aaron Wetzler; Guy Rosman; Alfred M. Bruckstein; Ron Kimmel",
        "abstract": "The introduction of consumer RGB-D scanners set off a major boost in 3D computer vision research. Yet, the precision of existing depth scanners is not accurate enough to recover fine details of a scanned object. While modern shading based depth refinement methods have been proven to work well with Lambertian objects, they break down in the presence of specularities. We present a novel shape from shading framework that addresses this issue and enhances both diffuse and specular objects' depth profiles. We take advantage of the built-in monochromatic IR projector and IR images of the RGB-D scanners and present a lighting model that accounts for the specular regions in the input image. Using this model, we reconstruct the depth map in real-time. Both quantitative tests and visual evaluations prove that the proposed method produces state of the art depth reconstruction results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Or-El_Real-Time_Depth_Refinement_CVPR_2016_paper.pdf",
        "aff": "Technion, Israel Institute of Technology; Technion, Israel Institute of Technology; Technion, Israel Institute of Technology; Computer Science and Arti\ufb01cial Intelligence Lab, MIT; Technion, Israel Institute of Technology; Technion, Israel Institute of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2082092,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2793196943257214947&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "cs.technion.ac.il;campus.technion.ac.il;cs.technion.ac.il;csail.mit.edu;cs.technion.ac.il;cs.technion.ac.il",
        "email": "cs.technion.ac.il;campus.technion.ac.il;cs.technion.ac.il;csail.mit.edu;cs.technion.ac.il;cs.technion.ac.il",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Or-El_Real-Time_Depth_Refinement_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "Israel Institute of Technology;Massachusetts Institute of Technology",
        "aff_unique_dep": ";Computer Science and Arti\ufb01cial Intelligence Lab",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://www.csail.mit.edu",
        "aff_unique_abbr": "Technion;MIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "title": "Real-Time Salient Object Detection With a Minimum Spanning Tree",
        "session": "Object Detection 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "8",
        "author_site": "Wei-Chih Tu , Shengfeng He, Qingxiong Yang, Shao-Yi Chien",
        "author": "Wei-Chih Tu; Shengfeng He; Qingxiong Yang; Shao-Yi Chien",
        "abstract": "In this paper, we present a real-time salient object detection system based on the minimum spanning tree. Due to the fact that background regions are typically connected to the image boundaries, salient objects can be extracted by computing the distances to the boundaries. However, measuring the image boundary connectivity efficiently is a challenging problem. Existing methods either rely on superpixel representation to reduce the processing units or approximate the distance transform. Instead, we propose an exact and iteration free solution on a minimum spanning tree. The minimum spanning tree representation of an image inherently reveals the object geometry information in a scene. Meanwhile, it largely reduces the search space of shortest paths, resulting an efficient and high quality distance transform algorithm. We further introduce a boundary dissimilarity measure to compliment the shortage of distance transform for salient object detection. Extensive evaluations show that the proposed algorithm achieves the leading performance compared to the state-of-the-art methods in terms of efficiency and accuracy.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tu_Real-Time_Salient_Object_CVPR_2016_paper.pdf",
        "aff": "Graduate Institute of Electronics Engineering, National Taiwan University; Department of Computer Science, City University of Hong Kong; School of Information Science and Technology, University of Science and Technology of China; Graduate Institute of Electronics Engineering, National Taiwan University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2841190,
        "gs_citation": 340,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1433349204174365899&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tu_Real-Time_Salient_Object_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "National Taiwan University;City University of Hong Kong;University of Science and Technology of China",
        "aff_unique_dep": "Graduate Institute of Electronics Engineering;Department of Computer Science;School of Information Science and Technology",
        "aff_unique_url": "https://www.ntu.edu.tw;https://www.cityu.edu.hk;http://www.ustc.edu.cn",
        "aff_unique_abbr": "NTU;CityU;USTC",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Taiwan;Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network",
        "session": "Deblurring and Super-Resolution",
        "status": "Poster",
        "track": "main",
        "pid": "41",
        "author_site": "Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel Rueckert, Zehan Wang",
        "author": "Wenzhe Shi; Jose Caballero; Ferenc Huszar; Johannes Totz; Andrew P. Aitken; Rob Bishop; Daniel Rueckert; Zehan Wang",
        "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the  super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shi_Real-Time_Single_Image_CVPR_2016_paper.pdf",
        "aff": "Magic Pony Technology; Magic Pony Technology; Magic Pony Technology; Magic Pony Technology; Magic Pony Technology; Magic Pony Technology; Imperial College London; Magic Pony Technology",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Shi_Real-Time_Single_Image_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1691995,
        "gs_citation": 8215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2184221316251559363&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "magicpony.technology;magicpony.technology;magicpony.technology;magicpony.technology;magicpony.technology;magicpony.technology;imperial.ac.uk;magicpony.technology",
        "email": "magicpony.technology;magicpony.technology;magicpony.technology;magicpony.technology;magicpony.technology;magicpony.technology;imperial.ac.uk;magicpony.technology",
        "author_num": 8,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shi_Real-Time_Single_Image_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0;0;1;0",
        "aff_unique_norm": "Magic Pony Technology;Imperial College London",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://magicponytechnology.com;https://www.imperial.ac.uk",
        "aff_unique_abbr": ";ICL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Recognizing Activities of Daily Living With a Wrist-Mounted Camera",
        "session": "Activity Recognition",
        "status": "Spotlight",
        "track": "main",
        "pid": "8",
        "author_site": "Katsunori Ohnishi, Atsushi Kanehira, Asako Kanezaki, Tatsuya Harada",
        "author": "Katsunori Ohnishi; Atsushi Kanehira; Asako Kanezaki; Tatsuya Harada",
        "abstract": "We present a novel dataset and a novel algorithm for recognizing activities of daily living (ADL) from a first-person wearable camera. Handled objects are crucially important for egocentric ADL recognition. For specific examination of objects related to users' actions separately from other objects in an environment, many previous works have addressed the detection of handled objects in images captured from head-mounted and chest-mounted cameras. Nevertheless, detecting handled objects is not always easy because they tend to appear small in images. They can be occluded by a user's body. As described herein, we mount a camera on a user's wrist. A wrist-mounted camera can capture handled objects at a large scale, and thus it enables us to skip the object detection process. To compare a wrist-mounted camera and a head-mounted camera, we also developed a novel and publicly available dataset that includes videos and annotations of daily activities captured simultaneously by both cameras. Additionally, we propose a discriminative video representation that retains spatial and temporal information after encoding the frame descriptors extracted by convolutional neural networks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ohnishi_Recognizing_Activities_of_CVPR_2016_paper.pdf",
        "aff": "Graduate School of Information Science and Technology, The University of Tokyo; Graduate School of Information Science and Technology, The University of Tokyo; Graduate School of Information Science and Technology, The University of Tokyo; Graduate School of Information Science and Technology, The University of Tokyo",
        "project": "http://www.mi.t.u-tokyo.ac.jp/static/projects/miladl/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Ohnishi_Recognizing_Activities_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2607424,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16012555392906067607&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp",
        "email": "mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ohnishi_Recognizing_Activities_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "Graduate School of Information Science and Technology",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Recognizing Car Fluents From Video",
        "session": "Video Understanding",
        "status": "Oral",
        "track": "main",
        "pid": "2",
        "author_site": "Bo Li, Tianfu Wu, Caiming Xiong, Song-Chun Zhu",
        "author": "Bo Li; Tianfu Wu; Caiming Xiong; Song-Chun Zhu",
        "abstract": "Physical fluents, a term originally used by Newton [40], refers to time-varying object states in dynamic scenes. In this paper, we are interested in inferring the fluents of vehicles from video. For example, a door (hood, trunk) is open or closed through various actions, light is blinking to turn. Recognizing these fluents has broad applications, yet have received scant attention in the computer vision literature. Car fluent recognition entails a unified framework for car detection, car part localization and part status recognition, which is made difficult by large structural and appearance variations, low resolutions and occlusions. This paper learns a spatial-temporal And-Or hierarchical model to represent car fluents. The learning of this model is formulated under the latent structural SVM framework. Since there are no publicly related dataset, we collect and annotate a car fluent dataset consisting of car videos with diverse fluents. In experiments, the proposed method outperforms several highly related baseline methods in terms of car fluent recognition and car part localization.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Recognizing_Car_Fluents_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Li_Recognizing_Car_Fluents_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6061399,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7374941721044822878&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Recognizing_Car_Fluents_CVPR_2016_paper.html"
    },
    {
        "title": "Recognizing Emotions From Abstract Paintings Using Non-Linear Matrix Completion",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "74",
        "author_site": "Xavier Alameda-Pineda, Elisa Ricci, Yan Yan, Nicu Sebe",
        "author": "Xavier Alameda-Pineda; Elisa Ricci; Yan Yan; Nicu Sebe",
        "abstract": "Advanced computer vision and machine learning techniques tried to automatically categorize the emotions elicited by abstract paintings with limited success. Since the annotation of the emotional content is highly resource-consuming, datasets of abstract paintings are either constrained in size or partially annotated. Consequently, it is natural to address the targeted task within a transductive framework. Intuitively, the use of multi-label classification techniques is desirable so to synergically exploit the relations between multiple latent variables, such as emotional content, technique, author, etc. A very popular approach for transductive multi-label recognition under linear classification settings is matrix completion. In this study we introduce non-linear matrix completion (NLMC), thus extending classical linear matrix completion techniques to the non-linear case. Together with the theory grounding the model, we propose an efficient optimization solver. As shown by our extensive experimental validation on two publicly available datasets, NLMC outperforms state-of-the-art methods when recognizing emotions from abstract paintings.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Alameda-Pineda_Recognizing_Emotions_From_CVPR_2016_paper.pdf",
        "aff": "University of Trento; Fondazione Bruno Kessler + University of Perugia; University of Trento; University of Trento",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Alameda-Pineda_Recognizing_Emotions_From_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3326324,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14693488989481910232&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "unitn.it;fbk.eu;unitn.it;unitn.it",
        "email": "unitn.it;fbk.eu;unitn.it;unitn.it",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Alameda-Pineda_Recognizing_Emotions_From_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+2;0;0",
        "aff_unique_norm": "University of Trento;Fondazione Bruno Kessler;University of Perugia",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unitn.it;https://www.fbk.eu;https://www.unipg.it",
        "aff_unique_abbr": "UniTN;FBK;Unipg",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "title": "Recognizing Micro-Actions and Reactions From Paired Egocentric Videos",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "40",
        "author_site": "Ryo Yonetani, Kris M. Kitani, Yoichi Sato",
        "author": "Ryo Yonetani; Kris M. Kitani; Yoichi Sato",
        "abstract": "We aim to understand the dynamics of social interactions between two people by recognizing their actions and reactions using a head-mounted camera. Our work will impact several first-person vision tasks that need the detailed understanding of social interactions, such as automatic video summarization of group events and assistive systems. To recognize micro-level actions and reactions, such as slight shifts in attention, subtle nodding, or small hand actions, where only subtle body motion is apparent, we propose to use paired egocentric videos recorded by two interacting people. We show that the first-person and second-person points-of-view features of two people, enabled by paired egocentric videos, are complementary and essential for reliably recognizing micro-actions and reactions. We also build a new dataset of dyadic (two-persons) interactions that comprises more than 1000 pairs of egocentric videos to enable systematic evaluations on the task of micro-action and reaction recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yonetani_Recognizing_Micro-Actions_and_CVPR_2016_paper.pdf",
        "aff": "The University of Tokyo; Carnegie Mellon University; The University of Tokyo",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1111187,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9164722219871689703&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "iis.u-tokyo.ac.jp;cs.cmu.edu;iis.u-tokyo.ac.jp",
        "email": "iis.u-tokyo.ac.jp;cs.cmu.edu;iis.u-tokyo.ac.jp",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yonetani_Recognizing_Micro-Actions_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Tokyo;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.cmu.edu",
        "aff_unique_abbr": "UTokyo;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Japan;United States"
    },
    {
        "title": "Recombinator Networks: Learning Coarse-To-Fine Feature Aggregation",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "48",
        "author_site": "Sina Honari, Jason Yosinski, Pascal Vincent, Christopher Pal",
        "author": "Sina Honari; Jason Yosinski; Pascal Vincent; Christopher Pal",
        "abstract": "Deep neural networks with alternating convolutional, max-pooling and decimation layers are widely used in state of the art architectures for computer vision. Max-pooling purposefully discards precise spatial information in order to create features that are more robust, and typically organized as lower resolution spatial feature maps. On some tasks, such as whole-image classification, max-pooling derived features are well suited; however, for tasks requiring precise localization, such as pixel level prediction and segmentation, max-pooling destroys exactly the information required to perform well. Precise localization may be preserved by shallow convnets without pooling but at the expense of robustness.  Can we have our max-pooled multi-layered cake and eat it too? Several papers have proposed summation and concatenation based methods for combining upsampled coarse, abstract features with finer features to produce robust pixel level predictions. Here we introduce another model --- dubbed Recombinator Networks --- where coarse features inform finer features early in their formation such that finer features can make use of several layers of computation in deciding how to use coarse features. The model is trained once, end-to-end  and performs better than summation-based architectures, reducing the error from the previous state of the art on two facial keypoint datasets, AFW and AFLW, by 30% and beating the current state-of-the-art on 300W without using extra data. We improve performance even further by adding a denoising prediction model based on a novel convnet formulation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Honari_Recombinator_Networks_Learning_CVPR_2016_paper.pdf",
        "aff": "University of Montreal; Cornell University; University of Montreal+CIFAR; Ecole Polytechnique of Montreal",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Honari_Recombinator_Networks_Learning_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2551720,
        "gs_citation": 156,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15999616770055672219&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "iro.umontreal.ca;iro.umontreal.ca;cs.cornell.edu;polymtl.ca",
        "email": "iro.umontreal.ca;iro.umontreal.ca;cs.cornell.edu;polymtl.ca",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Honari_Recombinator_Networks_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0+2;3",
        "aff_unique_norm": "University of Montreal;Cornell University;Canadian Institute for Advanced Research;Ecole Polytechnique de Montreal",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://wwwumontreal.ca;https://www.cornell.edu;https://www.cifar.ca;https://www.polymtl.ca",
        "aff_unique_abbr": "UM;Cornell;CIFAR;Polytechnique Montreal",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Montreal",
        "aff_country_unique_index": "0;1;0+0;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "title": "ReconNet: Non-Iterative Reconstruction of Images From Compressively Sensed Measurements",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "48",
        "author_site": "Kuldeep Kulkarni, Suhas Lohit, Pavan Turaga, Ronan Kerviche, Amit Ashok",
        "author": "Kuldeep Kulkarni; Suhas Lohit; Pavan Turaga; Ronan Kerviche; Amit Ashok",
        "abstract": "The goal of this paper is to present a non-iterative and more importantly an extremely fast algorithm to reconstruct images from compressively sensed (CS) random measurements.  To this end, we propose a novel convolutional neural network (CNN) architecture which takes in CS measurements of an image as input and outputs  an intermediate  reconstruction.  We call this network, ReconNet. The intermediate reconstruction is fed into an off-the-shelf denoiser to obtain the final reconstructed image. On a standard dataset of  images we show significant improvements in reconstruction results (both in terms of PSNR and time complexity) over state-of-the-art iterative CS reconstruction algorithms at various measurement rates. Further, through qualitative experiments on real data collected using our block SPC (single pixel camera), we show that our network is highly robust to sensor noise and can recover visually better quality images than competitive algorithms at extremely low sensing rates of 0.1 and 0.04. To demonstrate that our algorithm can recover semantically informative images even at a low measurement rate of 0.01, we present a very robust proof of concept real-time visual tracking application.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kulkarni_ReconNet_Non-Iterative_Reconstruction_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Kulkarni_ReconNet_Non-Iterative_Reconstruction_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6642839,
        "gs_citation": 854,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10572586649813712124&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kulkarni_ReconNet_Non-Iterative_Reconstruction_CVPR_2016_paper.html"
    },
    {
        "title": "Reconstructing Shapes and Appearances of Thin Film Objects Using RGB Images",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "80",
        "author_site": "Yoshie Kobayashi, Tetsuro Morimoto, Imari Sato, Yasuhiro Mukaigawa, Takao Tomono, Katsushi Ikeuchi",
        "author": "Yoshie Kobayashi; Tetsuro Morimoto; Imari Sato; Yasuhiro Mukaigawa; Takao Tomono; Katsushi Ikeuchi",
        "abstract": "Reconstruction of shapes and appearances of thin film objects can be applied to many fields such as industrial inspection, biological analysis, and archeology research. However, it comes with many challenging issues because the appearances of thin film can change dramatically depending on view and light directions. The appearance is deeply dependent on not only the shapes but also the optical parameters of thin film. In this paper, we propose a novel method to estimate shapes and film thickness. First, we narrow down candidates of zenith angle by degree of polarization and determine it by the intensity of thin film which increases monotonically along the zenith angle. Second, we determine azimuth angle from occluding boundaries. Finally, we estimate the film thickness by comparing a look-up table of color along the thickness and zenith angle with captured images. We experimentally evaluated the accuracy of estimated shapes and appearances and found that our proposed method is effective.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kobayashi_Reconstructing_Shapes_and_CVPR_2016_paper.pdf",
        "aff": "The University of Tokyo; Toppan Printing Co. Ltd.; National Institute of Informatics; Nara Institute of Science and Technology; Toppan Printing Co. Ltd.; The University of Tokyo",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 934358,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6568430408362610611&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cvl.iis.u-tokyo.ac.jp;toppan.co.jp;nii.co.jp;is.naist.jp;toppan.co.jp;cvl.iis.u-tokyo.ac.jp",
        "email": "cvl.iis.u-tokyo.ac.jp;toppan.co.jp;nii.co.jp;is.naist.jp;toppan.co.jp;cvl.iis.u-tokyo.ac.jp",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kobayashi_Reconstructing_Shapes_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;3;1;0",
        "aff_unique_norm": "University of Tokyo;Toppan Printing Company Limited;National Institute of Informatics;Nara Institute of Science and Technology",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.toppan.co.jp;https://www.nii.ac.jp/;https://www.nist.go.jp",
        "aff_unique_abbr": "UTokyo;Toppan;NII;NIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "60",
        "author_site": "Andreas Doumanoglou, Rigas Kouskouridas, Sotiris Malassiotis, Tae-Kyun Kim",
        "author": "Andreas Doumanoglou; Rigas Kouskouridas; Sotiris Malassiotis; Tae-Kyun Kim",
        "abstract": "Object detection and 6D pose estimation in the crowd (scenes with multiple object instances, severe foreground occlusions and background distractors), has become an important problem in many rapidly evolving technological areas such as robotics and augmented reality. Single shot-based 6D pose estimators with manually designed features are still unable to tackle the above challenges, motivating the research towards unsupervised feature learning and next-best-view estimation. In this work, we present a complete framework for both single shot-based 6D object pose estimation and next-best-view prediction based on Hough Forests, the state of the art object pose estimator that performs classification and regression jointly. Rather than using manually designed features we a) propose an unsupervised feature learnt from depth-invariant patches using a Sparse Autoencoder and b) offer an extensive evaluation of various state of the art features. Furthermore, taking advantage of the clustering performed in the leaf nodes of Hough Forests, we learn to estimate the reduction of uncertainty in other views, formulating the problem of selecting the next-best-view. To further improve pose estimation, we propose an improved joint registration and hypotheses verification module as a final refinement step to reject false detections. We provide two additional challenging datasets inspired from realistic scenarios to extensively evaluate the state of the art and our framework. One is related to domestic environments and the other depicts a bin-picking scenario mostly found in industrial settings. We show that our framework significantly outperforms state of the art both on public and on our datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Doumanoglou_Recovering_6D_Object_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Doumanoglou_Recovering_6D_Object_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 3098567,
        "gs_citation": 275,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9874412080704139604&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Doumanoglou_Recovering_6D_Object_CVPR_2016_paper.html"
    },
    {
        "title": "Recovering Transparent Shape From Time-Of-Flight Distortion",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "64",
        "author_site": "Kenichiro Tanaka, Yasuhiro Mukaigawa, Hiroyuki Kubo, Yasuyuki Matsushita, Yasushi Yagi",
        "author": "Kenichiro Tanaka; Yasuhiro Mukaigawa; Hiroyuki Kubo; Yasuyuki Matsushita; Yasushi Yagi",
        "abstract": "This paper presents a method for recovering shape and normal of a transparent object from a single viewpoint using a Time-of-Flight (ToF) camera. Our method is built upon the fact that the speed of light varies with the refractive index of the medium and therefore the depth measurement of a transparent object with a ToF camera may be distorted. We show that, from this ToF distortion, the refractive light path can be uniquely determined by estimating a single parameter. We estimate this parameter by introducing a surface normal consistency between the one determined by a light path candidate and the other computed from the corresponding shape. The proposed method is evaluated by both simulation and real-world experiments and shows faithful transparent shape recovery.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tanaka_Recovering_Transparent_Shape_CVPR_2016_paper.pdf",
        "aff": "Osaka University+Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology; Osaka University; Osaka University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Tanaka_Recovering_Transparent_Shape_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3311971,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2504797194029221609&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "am.sanken.osaka-u.ac.jp;am.sanken.osaka-u.ac.jp;ist.osaka-u.ac.jp;is.naist.jp;is.naist.jp",
        "email": "am.sanken.osaka-u.ac.jp;am.sanken.osaka-u.ac.jp;ist.osaka-u.ac.jp;is.naist.jp;is.naist.jp",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tanaka_Recovering_Transparent_Shape_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;1;0;0",
        "aff_unique_norm": "Osaka University;Nara Institute of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.osaka-u.ac.jp;https://www.nist.go.jp",
        "aff_unique_abbr": "Osaka U;NIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "5accc64e32",
        "title": "Recovering the Missing Link: Predicting Class-Attribute Associations for Unsupervised Zero-Shot Learning",
        "site": "https://openaccess.thecvf.com/content_cvpr_2016/html/Al-Halah_Recovering_the_Missing_CVPR_2016_paper.html",
        "author": "Ziad Al-Halah; Makarand Tapaswi; Rainer Stiefelhagen",
        "abstract": "Collecting training images for all visual categories is not only expensive but also impractical. Zero-shot learning (ZSL), especially using attributes, offers a pragmatic solution to this problem. However, at test time most attribute-based methods require a full description of attribute associations for each unseen class. Providing these associations is time consuming and often requires domain specific knowledge. In this work, we aim to carry out attribute-based zero-shot classification in an unsupervised manner. We propose an approach to learn relations that couples class embeddings with their corresponding attributes. Given only the name of an unseen class, the learned relationship model is used to automatically predict the class-attribute associations. Furthermore, our model facilitates transferring attributes across data sets without additional effort. Integrating knowledge from multiple sources results in a significant additional improvement in performance. We evaluate on two public data sets: Animals with Attributes and aPascal/aYahoo. Our approach outperforms state-of-the-art methods in both predicting class-attribute associations and unsupervised ZSL by a large margin.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Al-Halah_Recovering_the_Missing_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Al-Halah_Recovering_the_Missing_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 126,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11505538641884551832&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3
    },
    {
        "title": "Recurrent Attention Models for Depth-Based Person Identification",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "50",
        "author_site": "Albert Haque, Alexandre Alahi, Li Fei-Fei",
        "author": "Albert Haque; Alexandre Alahi; Li Fei-Fei",
        "abstract": "We present an attention-based model that reasons on human body shape and motion dynamics to identify individuals in the absence of RGB information, hence in the dark. Our approach leverages unique 4D spatio-temporal signatures to address the identification problem across days. Formulated as a reinforcement learning task, our model is based on a combination of convolutional and recurrent neural networks with the goal of identifying small, discriminative regions indicative of human identity. We demonstrate that our model produces state-of-the-art results on several published datasets given only depth images. We further study the robustness of our model towards viewpoint, appearance, and volumetric changes. Finally, we share insights gleaned from interpretable 2D, 3D, and 4D visualizations of our model's spatio-temporal attention.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Haque_Recurrent_Attention_Models_CVPR_2016_paper.pdf",
        "aff": "Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 24329326,
        "gs_citation": 210,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2188162667859239094&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Haque_Recurrent_Attention_Models_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Recurrent Attentional Networks for Saliency Detection",
        "session": "Semantic Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "69",
        "author_site": "Jason Kuen, Zhenhua Wang, Gang Wang",
        "author": "Jason Kuen; Zhenhua Wang; Gang Wang",
        "abstract": "Convolutional-deconvolution networks can be adopted to perform end-to-end saliency detection. But, they do not work well with objects of multiple scales. To overcome such a limitation, in this work, we propose a recurrent attentional convolutional-deconvolution network (RACDNN). Using spatial transformer and recurrent network units, RACDNN is able to iteratively attend to selected image sub-regions to perform saliency refinement progressively. Besides tackling the scale problem, RACDNN can also learn context-aware features from past iterations to enhance saliency refinement in future iterations. Experiments on several challenging saliency detection datasets validate the effectiveness of RACDNN, and show that RACDNN outperforms state-of-the-art saliency detection methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kuen_Recurrent_Attentional_Networks_CVPR_2016_paper.pdf",
        "aff": "School of Electrical and Electronic Engineering, Nanyang Technological University; School of Electrical and Electronic Engineering, Nanyang Technological University; School of Electrical and Electronic Engineering, Nanyang Technological University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 861890,
        "gs_citation": 296,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13926249278179238497&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kuen_Recurrent_Attentional_Networks_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Electrical and Electronic Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Recurrent Convolutional Network for Video-Based Person Re-Identification",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "60",
        "author_site": "Niall McLaughlin, Jesus Martinez del Rincon, Paul Miller",
        "author": "Niall McLaughlin; Jesus Martinez del Rincon; Paul Miller",
        "abstract": "In this paper we propose a novel recurrent neural network architecture for video-based person re-identification. Given the video sequence of a person, features are extracted from each frame using a convolutional neural network that incorporates a recurrent final layer, which allows information to flow between time-steps. The features from all time-steps are then combined using temporal pooling to give an overall appearance feature for the complete sequence. The convolutional network, recurrent layer, and temporal pooling layer, are jointly trained to act as a feature extractor for video-based re-identification using a Siamese network architecture. Our approach makes use of colour and optical flow information in order to capture appearance and motion information which is useful for video re-identification. Experiments are conduced on the iLIDS-VID and PRID-2011 datasets to show that this approach outperforms existing methods of video-based re-identification.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/McLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper.pdf",
        "aff": "Centre for Secure Information Technologies (CSIT); Centre for Secure Information Technologies (CSIT); Centre for Secure Information Technologies (CSIT)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 799352,
        "gs_citation": 746,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8014109838783690986&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": "qub.ac.uk; ; ",
        "email": "qub.ac.uk; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/McLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Centre for Secure Information Technologies",
        "aff_unique_dep": "Secure Information Technologies",
        "aff_unique_url": "",
        "aff_unique_abbr": "CSIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Recurrent Face Aging",
        "session": "Computational Photography and Faces",
        "status": "Oral",
        "track": "main",
        "pid": "13",
        "author_site": "Wei Wang, Zhen Cui, Yan Yan, Jiashi Feng, Shuicheng Yan, Xiangbo Shu, Nicu Sebe",
        "author": "Wei Wang; Zhen Cui; Yan Yan; Jiashi Feng; Shuicheng Yan; Xiangbo Shu; Nicu Sebe",
        "abstract": "Modeling the aging process of human face is important for cross-age face verification and recognition. In this paper, we introduce a recurrent face aging (RFA) framework based on a recurrent neural network which can identify the ages of people from 0 to 80. Due to the lack of labeled face data of the same person captured in a long range of ages, traditional face aging models usually split the ages into discrete groups and learn a one-step face feature transformation for each pair of adjacent age groups. However, those methods neglect the in-between evolving states between the adjacent age groups and the synthesized faces often suffer from severe ghosting artifacts. Since human face aging is a smooth progression, it is more appropriate to age the face by going through smooth transition states. In this way, the ghosting artifacts can be effectively eliminated and the intermediate aged faces between two discrete age groups can also be obtained. Towards this target, we employ a two-layer gated recurrent unit as the basic recurrent module whose bottom layer encodes a young face to a latent representation and the top layer decodes the representation to a corresponding older face. The experimental results demonstrate our proposed RFA provides better aging faces over other state-of-the-art age progression methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Recurrent_Face_Aging_CVPR_2016_paper.pdf",
        "aff": "Department of Information Engineering and Computer Science, University of Trento, Italy; Research Center for Learning Science, Southeast University, Nanjing, China; Department of Information Engineering and Computer Science, University of Trento, Italy; Department of Electrical and Computer Engineering, National University of Singapore+360 Arti\ufb01cial Intelligence Institute, China; Department of Electrical and Computer Engineering, National University of Singapore; Department of Electrical and Computer Engineering, National University of Singapore; Department of Information Engineering and Computer Science, University of Trento, Italy",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1369220,
        "gs_citation": 231,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17194182546995604938&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "unitn.it;seu.edu.cn;unitn.it;nus.edu.sg;gmail.com;nus.edu.sg;unitn.it",
        "email": "unitn.it;seu.edu.cn;unitn.it;nus.edu.sg;gmail.com;nus.edu.sg;unitn.it",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Recurrent_Face_Aging_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;2+3;2;2;0",
        "aff_unique_norm": "University of Trento;Southeast University;National University of Singapore;360 Arti\ufb01cial Intelligence Institute",
        "aff_unique_dep": "Department of Information Engineering and Computer Science;Research Center for Learning Science;Department of Electrical and Computer Engineering;",
        "aff_unique_url": "https://www.unitn.it;https://www.seu.edu.cn/;https://www.nus.edu.sg;",
        "aff_unique_abbr": "UniTN;SEU;NUS;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Nanjing",
        "aff_country_unique_index": "0;1;0;2+1;2;2;0",
        "aff_country_unique": "Italy;China;Singapore"
    },
    {
        "title": "Recurrently Target-Attending Tracking",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "73",
        "author_site": "Zhen Cui, Shengtao Xiao, Jiashi Feng, Shuicheng Yan",
        "author": "Zhen Cui; Shengtao Xiao; Jiashi Feng; Shuicheng Yan",
        "abstract": "Robust visual tracking is a challenging task in computer vision. Due to the accumulation and propagation of estimation error, model drifting often occurs and degrades the tracking performance. To mitigate this problem, in this paper we propose a novel tracking method called Recurrently Target-attending Tracking (RTT). RTT attempts to identify and exploit those reliable parts which are beneficial for the overall tracking process. To bypass occlusion and discover reliable components, multi-directional Recurrent Neural Networks (RNNs) are employed in RTT to capture long-range contextual cues by traversing a candidate spatial region from multiple directions. The produced confidence maps from the RNNs are employed to adaptively regularize the learning of discriminative correlation filters by suppressing clutter background noises while making full use of the information from reliable parts. To solve the weighted correlation filters, we especially derive an efficient closed-form solution with a sharp reduction in computation complexity. Extensive experiments demonstrate that our proposed RTT is more competitive over those correlation filter based methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cui_Recurrently_Target-Attending_Tracking_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1316777,
        "gs_citation": 197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=63445254340133617&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cui_Recurrently_Target-Attending_Tracking_CVPR_2016_paper.html"
    },
    {
        "title": "Recursive Recurrent Nets With Attention Modeling for OCR in the Wild",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "79",
        "author_site": "Chen-Yu Lee, Simon Osindero",
        "author": "Chen-Yu Lee; Simon Osindero",
        "abstract": "We present recursive recurrent neural networks with attention modeling (R2AM) for lexicon-free optical character recognition in natural scene images. The primary advantages of the proposed method are: (1) use of recursive convolutional neural networks (CNNs), which allow for parametrically efficient and effective image feature extraction; (2) an implicitly learned character-level language model, embodied in a recurrent neural network which avoids the need to use N-grams; and (3) the use of a soft-attention mechanism, allowing the model to selectively exploit image features in a coordinated way, and allowing for end-to-end training within a standard backpropagation framework. We validate our method with state-of-the-art performance on challenging benchmark datasets: Street View Text, IIIT5k, ICDAR and Synth90k.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lee_Recursive_Recurrent_Nets_CVPR_2016_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Lee_Recursive_Recurrent_Nets_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 638,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15344704226949352942&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lee_Recursive_Recurrent_Nets_CVPR_2016_paper.html"
    },
    {
        "title": "Refining Architectures of Deep Convolutional Neural Networks",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "77",
        "author_site": "Sukrit Shankar, Duncan Robertson, Yani Ioannou, Antonio Criminisi, Roberto Cipolla",
        "author": "Sukrit Shankar; Duncan Robertson; Yani Ioannou; Antonio Criminisi; Roberto Cipolla",
        "abstract": "Deep Convolutional Neural Networks (CNNs) have recently evinced immense success for various image recognition tasks. However, a question of paramount importance is somewhat unanswered in deep learning research - is the selected CNN optimal for the dataset in terms of accuracy and model size?  In this paper, we intend to answer this question and introduce a novel strategy that alters the architecture of a given CNN for a specified dataset, to potentially enhance the original accuracy while possibly reducing the model size. We use two operations for architecture refinement, viz. stretching and symmetrical splitting. Stretching increases the number of hidden units (nodes) in a  given CNN layer, while a symmetrical split of say K between two layers separates the input and output channels into K equal groups, and connects only the corresponding input-output channel groups.  Our procedure starts with a pre-trained CNN for a given dataset, and optimally decides the stretch and split factors across the network to refine the architecture. We empirically demonstrate the necessity of the two operations.   We evaluate our approach on two natural scenes attributes datasets, SUN Attributes  and CAMIT-NSAD, with architectures of GoogleNet and VGG-11, that are quite contrasting in their construction. We justify our choice of datasets, and show that they are interestingly distinct from each other, and together pose a challenge to our architectural refinement algorithm. Our results substantiate the usefulness of the proposed method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shankar_Refining_Architectures_of_CVPR_2016_paper.pdf",
        "aff": "Machine Intelligence Lab, University of Cambridge, UK+Microsoft Research Cambridge, UK; Microsoft Research Cambridge, UK; Machine Intelligence Lab, University of Cambridge, UK; Microsoft Research Cambridge, UK; Machine Intelligence Lab, University of Cambridge, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1149834,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1136794491312349478&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff_domain": "cam.ac.uk;microsoft.com;cam.ac.uk;microsoft.com;cam.ac.uk",
        "email": "cam.ac.uk;microsoft.com;cam.ac.uk;microsoft.com;cam.ac.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shankar_Refining_Architectures_of_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;0;1;0",
        "aff_unique_norm": "University of Cambridge;Microsoft",
        "aff_unique_dep": "Machine Intelligence Lab;Research",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.microsoft.com/en-us/research/group/cambridge",
        "aff_unique_abbr": "Cambridge;MSR Cambridge",
        "aff_campus_unique_index": "0+0;0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Region Ranking SVM for Image Classification",
        "session": "Scene and Image Classification",
        "status": "Poster",
        "track": "main",
        "pid": "78",
        "author_site": "Zijun Wei, Minh Hoai",
        "author": "Zijun Wei; Minh Hoai",
        "abstract": "The success of an image classification algorithm largely depends on how it incorporates local information in the global decision.  Popular approaches such as average-pooling and max-pooling are suboptimal in many situations. In this paper we propose Region Ranking SVM(RRSVM), a novel method for  pooling local information from multiple regions. RRSVM exploits the correlation of local regions in an image, and it jointly learns a region evaluation function and a scheme for integrating multiple regions.  Experiments on PASCAL VOC 2007, VOC 2012, and ILSVRC2014 datasets show that RRSVM outperforms the methods that use the same feature type and extract features from the same set of local regions. IRSVM achieves similar to or better than the state-of-the-art performance on all datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wei_Region_Ranking_SVM_CVPR_2016_paper.pdf",
        "aff": "Stony Brook University, Stony Brook, NY 11794, USA; Stony Brook University, Stony Brook, NY 11794, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 721211,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3793330744209861242&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wei_Region_Ranking_SVM_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stony Brook University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stonybrook.edu",
        "aff_unique_abbr": "SBU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stony Brook",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Regularity-Driven Facade Matching Between Aerial and Street Views",
        "session": "Recognition Beyond Objects",
        "status": "Spotlight",
        "track": "main",
        "pid": "10",
        "author_site": "Mark Wolff, Robert T. Collins, Yanxi Liu",
        "author": "Mark Wolff; Robert T. Collins; Yanxi Liu",
        "abstract": "We present an approach for detecting and matching building facades between aerial view and street-view images. We exploit the regularity of urban scene facades as captured by their lattice structures and deduced from median-tiles' shape context, color, texture and spatial similarities. Our experimental results demonstrate effective matching of oblique and partially-occluded facades between aerial and ground views. Quantitative comparisons for automated urban scene facade matching from three cities show superior performance of our method over baseline SIFT, Root-SIFT and the more sophisticated Scale-Selective Self-Similarity and Binary Coherent Edge descriptors. We also illustrate regularity-based applications of occlusion removal from street views and higher-resolution texture-replacement in aerial views.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wolff_Regularity-Driven_Facade_Matching_CVPR_2016_paper.pdf",
        "aff": "School of Electrical Engineering and Computer Science, The Pennsylvania State University; School of Electrical Engineering and Computer Science, The Pennsylvania State University; School of Electrical Engineering and Computer Science, The Pennsylvania State University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wolff_Regularity-Driven_Facade_Matching_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1823741,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6789219959952658467&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "psu.edu;cse.psu.edu;cse.psu.edu",
        "email": "psu.edu;cse.psu.edu;cse.psu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wolff_Regularity-Driven_Facade_Matching_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Pennsylvania State University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.psu.edu",
        "aff_unique_abbr": "PSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Regularizing Long Short Term Memory With 3D Human-Skeleton Sequences for Action Recognition",
        "session": "Actions and Human Pose",
        "status": "Oral",
        "track": "main",
        "pid": "3",
        "author_site": "Behrooz Mahasseni, Sinisa Todorovic",
        "author": "Behrooz Mahasseni; Sinisa Todorovic",
        "abstract": "This paper argues that large-scale action recognition in video can be greatly improved by providing an additional modality in training data -- namely, 3D human-skeleton sequences -- aimed at complementing poorly represented or missing features of human actions in the training videos. For recognition, we use Long Short Term Memory (LSTM) grounded via a deep Convolutional Neural Network (CNN) onto the video. Training of LSTM is regularized using the output of another encoder LSTM (eLSTM) grounded on 3D human-skeleton training data. For such regularized training of LSTM, we modify the standard backpropagation through time (BPTT) in order to address the well-known issues with gradient descent in constraint optimization. Our evaluation on three benchmark datasets -- Sports-1M, HMDB-51, and UCF101 -- shows accuracy improvements from 5.3% up to 17.4% relative to the state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mahasseni_Regularizing_Long_Short_CVPR_2016_paper.pdf",
        "aff": "Oregon State University; Oregon State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1611757,
        "gs_citation": 199,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13753315808905975223&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "eecs.oregonstate.edu;eecs.oregonstate.edu",
        "email": "eecs.oregonstate.edu;eecs.oregonstate.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mahasseni_Regularizing_Long_Short_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Oregon State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://oregonstate.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Reinforcement Learning for Visual Object Detection",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "68",
        "author_site": "Stefan Mathe, Aleksis Pirinen, Cristian Sminchisescu",
        "author": "Stefan Mathe; Aleksis Pirinen; Cristian Sminchisescu",
        "abstract": "One of the most widely used strategies for visual object detection is based on exhaustive spatial hypothesis search. While methods like sliding windows have been successful and effective for many years, they are still brute-force, independent of the image content and the visual category being searched. In this paper we present formally rigorous sequential models that accumulate evidence collected at a small set of image locations in order to detect visual objects effectively. By formulating sequential search as reinforcement learning of the search policy (including the stopping condition), our fully trainable model can explicitly balance for each class, specifically, the conflicting goals of exploration -- sampling more image regions for better accuracy --, and exploitation -- stopping the search efficiently when sufficiently confident in the target's location. The methodology is general and applicable to any detector response function. We report encouraging results in the PASCAL VOC 2012 object detection test set showing that the proposed methodology achieves almost  two orders of magnitude speed-up over sliding window methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mathe_Reinforcement_Learning_for_CVPR_2016_paper.pdf",
        "aff": "Department of Mathematics, Faculty of Engineering, Lund University + Institute of Mathematics of the Romanian Academy + Department of Computer Science, University of Toronto; Department of Mathematics, Faculty of Engineering, Lund University; Department of Mathematics, Faculty of Engineering, Lund University + Institute of Mathematics of the Romanian Academy",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 882291,
        "gs_citation": 203,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13923477455177472160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "imar.ro;math.lth.se;math.lth.se",
        "email": "imar.ro;math.lth.se;math.lth.se",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mathe_Reinforcement_Learning_for_CVPR_2016_paper.html",
        "aff_unique_index": "0+1+2;0;0+1",
        "aff_unique_norm": "Lund University;Romanian Academy;University of Toronto",
        "aff_unique_dep": "Department of Mathematics;Institute of Mathematics;Department of Computer Science",
        "aff_unique_url": "https://www.lunduniversity.lu.se;https://www.math.ro/;https://www.utoronto.ca",
        "aff_unique_abbr": "LU;IMAR;U of T",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Toronto",
        "aff_country_unique_index": "0+1+2;0;0+1",
        "aff_country_unique": "Sweden;Romania;Canada"
    },
    {
        "title": "Relaxation-Based Preprocessing Techniques for Markov Random Field Inference",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "57",
        "author_site": "Chen Wang, Ramin Zabih",
        "author": "Chen Wang; Ramin Zabih",
        "abstract": "Markov Random Fields (MRFs) are a widely used graphical model, but the inference problem is NP-hard. For first-order MRFs with binary labels, Dead End Elimination (DEE) and QPBO can find the optimal labeling for some variables; the much harder case of larger label sets has been addressed by Kovtun and related methods which impose substantial computational overhead. We describe an efficient algorithm to correctly label a subset of the variables for arbitrary MRFs, with particularly good performance on binary MRFs. We propose a sufficient condition to check if a partial labeling is optimal, which is a generalization of DEE's purely local test. We give a hierarchy of relaxations that provide larger optimal partial labelings at the cost of additional computation. Empirical studies were conducted on several benchmarks, using expansion moves for inference. Our algorithm runs in a few seconds, and improves the speed of MRF inference with expansion moves by a factor of 1.5 to 12.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Relaxation-Based_Preprocessing_Techniques_CVPR_2016_paper.pdf",
        "aff": "Cornell University; Cornell University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wang_Relaxation-Based_Preprocessing_Techniques_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1181854,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3623192201354052581&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cs.cornell.edu;cs.cornell.edu",
        "email": "cs.cornell.edu;cs.cornell.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Relaxation-Based_Preprocessing_Techniques_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Removing Clouds and Recovering Ground Observations in Satellite Image Sequences via Temporally Contiguous Robust Matrix Completion",
        "session": "Image Enhancement, Restoration, and Texture",
        "status": "Poster",
        "track": "main",
        "pid": "53",
        "author_site": "Jialei Wang, Peder A. Olsen, Andrew R. Conn, Aur\u00e9lie C. Lozano",
        "author": "Jialei Wang; Peder A. Olsen; Andrew R. Conn; Aurelie C. Lozano",
        "abstract": "We consider the problem of removing and replacing clouds in satellite image sequences, which has a wide range of applications in remote sensing. Our approach first detects and removes the cloud-contaminated part of the image sequences, then recovers the missing scenes from the clean parts by the proposed \"TECROMAC\" (TEmporally Contiguous RObust MAtrix Completion) objective. The objective function balances temporal smoothness with a low rank solution while staying close to the original observations. The matrix where the rows are pixels and columns are the days of the image has low-rank because the pixels reflect land-types such as vegetation, roads and lakes and there are relatively few of these. We provide efficient optimization algorithms for TECROMAC, so we can run on images containing millions of pixels. Empirical results on real satellite image sequences as well as simulated data demonstrate that our approach is able to recover underlying images from heavily cloud-contaminated observations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Removing_Clouds_and_CVPR_2016_paper.pdf",
        "aff": "University of Chicago; IBM T.J. Watson Research Center; IBM T.J. Watson Research Center; IBM T.J. Watson Research Center",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1349153,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2438469098167237248&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff_domain": "uchicago.edu;us.ibm.com;us.ibm.com;us.ibm.com",
        "email": "uchicago.edu;us.ibm.com;us.ibm.com;us.ibm.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Removing_Clouds_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Chicago;IBM",
        "aff_unique_dep": ";Research Center",
        "aff_unique_url": "https://www.uchicago.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "UChicago;IBM",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";T.J. Watson",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Rethinking the Inception Architecture for Computer Vision",
        "session": "Large Scale Visual Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "60",
        "author_site": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna",
        "author": "Christian Szegedy; Vincent Vanhoucke; Sergey Ioffe; Jon Shlens; Zbigniew Wojna",
        "abstract": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible. We benchmark our methods on the ILSVRC 2012 classification challenge validation set and demonstrate substantial gains over the state of the art via to carefully factorized convolutions and aggressive regularization: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf",
        "aff": "Google Inc.; Google Inc.; Google Inc.; Google Inc.; University College London",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 531573,
        "gs_citation": 39189,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1692140599533045894&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 26,
        "aff_domain": "google.com;google.com;google.com;google.com;gmail.com",
        "email": "google.com;google.com;google.com;google.com;gmail.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Google;University College London",
        "aff_unique_dep": "Google;",
        "aff_unique_url": "https://www.google.com;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Google;UCL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "title": "Reversible Recursive Instance-Level Object Segmentation",
        "session": "Segmentation and Saliency",
        "status": "Poster",
        "track": "main",
        "pid": "68",
        "author_site": "Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Zequn Jie, Jiashi Feng, Liang Lin, Shuicheng Yan",
        "author": "Xiaodan Liang; Yunchao Wei; Xiaohui Shen; Zequn Jie; Jiashi Feng; Liang Lin; Shuicheng Yan",
        "abstract": "In this work, we propose a novel Reversible Recursive Instance-level Object Segmentation (R2-IOS) framework to address the challenging instance-level object segmentation task. R2-IOS consists of a reversible proposal refinement sub-network that predicts bounding box offsets for refining the object proposal locations, and an instance-level segmentation sub-network that generates the foreground mask of the dominant object instance in each proposal. By being recursive, R2-IOS iteratively optimizes the two sub-networks during joint training, in which the refined object proposals and improved segmentation predictions are alternately fed into each other to progressively increase the network capabilities. By being reversible, the proposal refinement sub-network adaptively determines an optimal number of refinement iterations required for each proposal during both training and testing. Furthermore, to handle multiple overlapped instances within a proposal, an  instance-aware denoising autoencoder is introduced into the segmentation sub-network to distinguish the dominant object from other distracting instances. Extensive experiments on the challenging PASCAL VOC 2012 benchmark well demonstrate the superiority of R2-IOS over other state-of-the-art methods. In particular, the AP^r over 20 classes at 0.5 IoU achieves 66.7%, which significantly outperforms the results of 58.7% by PFN[15] and 46.3% by[17].",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liang_Reversible_Recursive_Instance-Level_CVPR_2016_paper.pdf",
        "aff": "Sun Yat-sen University; National University of Singapore; Adobe Research; National University of Singapore; National University of Singapore; Sun Yat-sen University+National University of Singapore; National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1107074,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4150871884318956617&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ieee.org; ; ; ; ; ; ",
        "email": "ieee.org; ; ; ; ; ; ",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liang_Reversible_Recursive_Instance-Level_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;1;1;0+1;1",
        "aff_unique_norm": "Sun Yat-sen University;National University of Singapore;Adobe",
        "aff_unique_dep": ";;Adobe Research",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://www.nus.edu.sg;https://research.adobe.com",
        "aff_unique_abbr": "SYSU;NUS;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;1;1;0+1;1",
        "aff_country_unique": "China;Singapore;United States"
    },
    {
        "title": "Robust 3D Hand Pose Estimation in Single Depth Images: From Single-View CNN to Multi-View CNNs",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "61",
        "author_site": "Liuhao Ge, Hui Liang, Junsong Yuan, Daniel Thalmann",
        "author": "Liuhao Ge; Hui Liang; Junsong Yuan; Daniel Thalmann",
        "abstract": "Articulated hand pose estimation plays an important role in human-computer interaction. Despite the recent progress, the accuracy of existing methods is still not satisfactory, partially due to the difficulty of embedded high-dimensional and non-linear regression problem. Different from the existing discriminative methods that regress for the hand pose with a single depth image, we propose to first project the query depth image onto three orthogonal planes and utilize these multi-view projections to regress for 2D heat-maps which estimate the joint positions on each plane. These multi-view heat-maps are then fused to produce final 3D hand pose estimation with learned pose priors. Experiments show that the proposed method largely outperforms state-of-the-arts on a challenging dataset. Moreover, a cross-dataset experiment also demonstrates the good generalization ability of the proposed method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ge_Robust_3D_Hand_CVPR_2016_paper.pdf",
        "aff": "Institute for Media Innovation, Nanyang Technological University, Singapore; Institute for Media Innovation, Nanyang Technological University, Singapore; Institute for Media Innovation, Nanyang Technological University, Singapore; Institute for Media Innovation, Nanyang Technological University, Singapore",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Ge_Robust_3D_Hand_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1047675,
        "gs_citation": 375,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9079859435738543977&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "email": "e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ge_Robust_3D_Hand_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "Institute for Media Innovation",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "title": "Robust Kernel Estimation With Outliers Handling for Image Deblurring",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "58",
        "author_site": "Jinshan Pan, Zhouchen Lin, Zhixun Su, Ming-Hsuan Yang",
        "author": "Jinshan Pan; Zhouchen Lin; Zhixun Su; Ming-Hsuan Yang",
        "abstract": "Estimating blur kernels from real world images is a challenging problem as the linear image formation assumption does not hold when significant outliers, such as saturated pixels and non-Gaussian noise, are present. While some existing non-blind deblurring algorithms can deal with outliers to a certain extent, few blind deblurring methods are developed to well estimate the blur kernels from the blurred images with outliers. In this paper, we present an algorithm to address this problem by exploiting reliable edges and removing outliers in the intermediate latent images, thereby estimating blur kernels robustly. We analyze the effects of outliers on kernel estimation and show that most state-of-the-art blind deblurring methods may recover delta kernels when blurred images contain significant outliers. We propose a robust energy function which describes the properties of outliers for the final latent image restoration. Furthermore, we show that the proposed algorithm can be applied to improve existing methods to deblur images with outliers. Extensive experiments on different kinds of challenging blurry images with significant amount of outliers demonstrate the proposed algorithm performs favorably against the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Robust_Kernel_Estimation_CVPR_2016_paper.pdf",
        "aff": "School of Mathematical Sciences, Dalian University of Technology + Electrical Engineering and Computer Science, University of California at Merced; Key Laboratory of Machine Perception (MOE), School of EECS, Peking University + Cooperative Medianet Innovation Center, Shanghai Jiaotong University; School of Mathematical Sciences, Dalian University of Technology + National Engineering Research Center of Digital Life; Electrical Engineering and Computer Science, University of California at Merced",
        "project": "http://vllab.ucmerced.edu/~jinshan/projects/outlier-deblur/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Pan_Robust_Kernel_Estimation_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3426556,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3185623664875135648&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff_domain": "ucmerced.edu;pku.edu.cn;dlut.edu.cn;ucmerced.edu",
        "email": "ucmerced.edu;pku.edu.cn;dlut.edu.cn;ucmerced.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Pan_Robust_Kernel_Estimation_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;2+3;0+4;1",
        "aff_unique_norm": "Dalian University of Technology;University of California, Merced;Peking University;Shanghai Jiao Tong University;National Engineering Research Center of Digital Life",
        "aff_unique_dep": "School of Mathematical Sciences;Electrical Engineering and Computer Science;School of EECS;Cooperative Medianet Innovation Center;",
        "aff_unique_url": "http://en.dlut.edu.cn/;https://www.ucmerced.edu;http://www.pku.edu.cn;https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "DUT;UC Merced;PKU;SJTU;",
        "aff_campus_unique_index": "0+1;;0;1",
        "aff_campus_unique": "Dalian;Merced;",
        "aff_country_unique_index": "0+1;0+0;0+0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Robust Light Field Depth Estimation for Noisy Scene With Occlusion",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "65",
        "author_site": "Williem, In Kyu Park",
        "author": "W. Williem; In Kyu Park",
        "abstract": "Light field depth estimation is an essential part of many light field applications. Numerous algorithms have been developed using various light field characteristics. However, conventional methods fail when handling noisy scene with occlusion. To remedy this problem, we present a light field depth estimation method which is more robust to occlusion and less sensitive to noise. Novel data costs using angular entropy metric and adaptive defocus response are introduced. Integration of both data costs improves the occlusion and noise invariant capability significantly. Cost volume filtering and graph cut optimization are utilized to improve the accuracy of the depth map. Experimental results confirm that the proposed method is robust and achieves high quality depth maps in various scenes. The proposed method outperforms the state-of-the-art light field depth estimation methods in qualitative and quantitative evaluation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Williem_Robust_Light_Field_CVPR_2016_paper.pdf",
        "aff": "Dept. of Information and Communication Engineering, Inha University; Dept. of Information and Communication Engineering, Inha University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2737870,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=270453946757370658&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "inha.edu;inha.ac.kr",
        "email": "inha.edu;inha.ac.kr",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Williem_Robust_Light_Field_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Inha University",
        "aff_unique_dep": "Dept. of Information and Communication Engineering",
        "aff_unique_url": "https://www.inha.edu/",
        "aff_unique_abbr": "Inha",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Robust Multi-Body Feature Tracker: A Segmentation-Free Approach",
        "session": "Video Analysis 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "6",
        "author_site": "Pan Ji, Hongdong Li, Mathieu Salzmann, Yiran Zhong",
        "author": "Pan Ji; Hongdong Li; Mathieu Salzmann; Yiran Zhong",
        "abstract": "Feature tracking is a fundamental problem in computer vision with applications in various tasks including 3D reconstruction and visual SLAM. While many methods have been devoted to making these tasks robust to noise and outliers, less attention has been attracted to improving the feature tracking itself.  This paper introduces a novel multi-body feature tracker that takes advantage of the multi-body rigidity assumption to improve tracking robustness. A conventional approach to addressing this problem would consist of alternating between solving two subtasks: motion segmentation and feature tracking under rigidity constraints for each segment. This approach, however, requires knowing the number of motions, as well as assigning points to motion groups, which is typically sensitive to the motion estimates. By contrast, here, we introduce a segmentation-free solution to multi-body feature tracking that bypasses the motion assignment step and reduces to solving a series of subproblems with closed-form solutions. Our experiments demonstrate the benefits of our approach in terms of tracking accuracy and robustness to noise.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ji_Robust_Multi-Body_Feature_CVPR_2016_paper.pdf",
        "aff": "ANU, Canberra; ANU, Canberra; EPFL, Switzerland; ANU, Canberra",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 654885,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16995293646933097738&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "anu.edu.au;anu.edu.au;epfl.ch;anu.edu.au",
        "email": "anu.edu.au;anu.edu.au;epfl.ch;anu.edu.au",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ji_Robust_Multi-Body_Feature_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Australian National University;EPFL",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.anu.edu.au;https://www.epfl.ch",
        "aff_unique_abbr": "ANU;EPFL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Canberra;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "Australia;Switzerland"
    },
    {
        "title": "Robust Optical Flow Estimation of Double-Layer Images Under Transparency or Reflection",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "69",
        "author_site": "Jiaolong Yang, Hongdong Li, Yuchao Dai, Robby T. Tan",
        "author": "Jiaolong Yang; Hongdong Li; Yuchao Dai; Robby T. Tan",
        "abstract": "This paper deals with a challenging, frequently encountered, yet not properly investigated problem in two-frame optical flow estimation. That is, the input frames are compounds of two imaging layers -- one desired background layer of the scene, and one distracting, possibly moving layer due to transparency or reflection. In this situation, the conventional brightness constancy constraint -- the cornerstone of most existing optical flow methods -- will no longer be valid. In this paper, we propose a robust solution to this problem. The proposed method performs both optical flow estimation, and image layer separation. It exploits a generalized double-layer brightness consistency constraint connecting these two tasks, and utilizes the priors for both of them. Experiments on both synthetic data and real images have confirmed the efficacy of the proposed method. To the best of our knowledge, this is the first attempt towards handling generic optical flow fields of two-frame images containing transparency or reflection.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_Robust_Optical_Flow_CVPR_2016_paper.pdf",
        "aff": "BIT & ANU; ANU & ACRV; CECS, ANU; Yale-NUS College",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2216123,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9497094344966570402&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "anu.edu.au; ; ; ",
        "email": "anu.edu.au; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_Robust_Optical_Flow_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Beijing Institute of Technology;Australian National University;Yale-NUS College",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.bit.edu.cn/;https://www.anu.edu.au;https://www.yale-nus.edu.sg",
        "aff_unique_abbr": "BIT;ANU;Yale-NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;2",
        "aff_country_unique": "China;Australia;Singapore"
    },
    {
        "title": "Robust Scene Text Recognition With Automatic Rectification",
        "session": "Document Analysis",
        "status": "Poster",
        "track": "main",
        "pid": "41",
        "author_site": "Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, Xiang Bai",
        "author": "Baoguang Shi; Xinggang Wang; Pengyuan Lyu; Cong Yao; Xiang Bai",
        "abstract": "Recognizing text in natural images is a challenging task with many unsolved problems. Different from those in documents, words in natural images often possess irregular shapes, which are caused by perspective distortion, curved character placement, etc. We propose RARE (Robust text recognizer with Automatic REctification), a recognition model that is robust to irregular text. RARE is a specially-designed deep neural network, which consists of a Spatial Transformer Network (STN) and a Sequence Recognition Network (SRN). In testing, an image is firstly rectified via a predicted Thin-Plate-Spline (TPS) transformation, into a more \"readable\" image for the following SRN, which recognizes text through a sequence recognition approach. We show that the model is able to recognize several types of irregular text, including perspective text and curved text. RARE is end-to-end trainable, requiring only images and associated text labels, making it convenient to train and deploy the model in practical systems. State-of-the-art or highly-competitive performance achieved on several benchmarks well demonstrates the effectiveness of the proposed model.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shi_Robust_Scene_Text_CVPR_2016_paper.pdf",
        "aff": "School of Electronic Information and Communications, Huazhong University of Science and Technology; School of Electronic Information and Communications, Huazhong University of Science and Technology; School of Electronic Information and Communications, Huazhong University of Science and Technology; School of Electronic Information and Communications, Huazhong University of Science and Technology; School of Electronic Information and Communications, Huazhong University of Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 785047,
        "gs_citation": 820,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5485227057530207744&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "gmail.com; ; ; ;hust.edu.cn",
        "email": "gmail.com; ; ; ;hust.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shi_Robust_Scene_Text_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Huazhong University of Science and Technology",
        "aff_unique_dep": "School of Electronic Information and Communications",
        "aff_unique_url": "http://www.hust.edu.cn",
        "aff_unique_abbr": "HUST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Robust Tensor Factorization With Unknown Noise",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "71",
        "author_site": "Xi'ai Chen, Zhi Han, Yao Wang, Qian Zhao, Deyu Meng, Yandong Tang",
        "author": "Xi'ai Chen; Zhi Han; Yao Wang; Qian Zhao; Deyu Meng; Yandong Tang",
        "abstract": "Because of the limitations of matrix factorization, such as losing spatial structure information, the concept of tensor factorization has been applied for the recovery of a low dimensional subspace from high dimensional visual data. Generally, the recovery is achieved by minimizing the loss function between the observed data and the factorization representation. Under different assumptions of the noise distribution, the loss functions are in various forms, like L1 and L2 norms. However, real data are often corrupted by noise with an unknown distribution. Then any specific form of loss function for one specific kind of noise often fails to tackle such real data with unknown noise. In this paper, we propose a tensor factorization algorithm to model the noise as a Mixture of Gaussians (MoG). As MoG has the ability of universally approximating any hybrids of continuous distributions, our algorithm can effectively recover the low dimensional subspace from various forms of noisy observations. The parameters of MoG are estimated under the EM framework and through a new developed algorithm of weighted low-rank tensor factorization (WLRTF). The effectiveness of our algorithm are substantiated by extensive experiments on both of synthetic data and real image data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Robust_Tensor_Factorization_CVPR_2016_paper.pdf",
        "aff": "State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences + University of Chinese Academy of Sciences; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences + Xi\u2019an Jiaotong University; Xi\u2019an Jiaotong University; Xi\u2019an Jiaotong University; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3207903,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6031939804226982419&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff_domain": "sia.cn;sia.cn;sia.cn;gmail.com;gmail.com;mail.xjtu.edu.cn",
        "email": "sia.cn;sia.cn;sia.cn;gmail.com;gmail.com;mail.xjtu.edu.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Robust_Tensor_Factorization_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;0+2;2;2;0",
        "aff_unique_norm": "Shenyang Institute of Automation;University of Chinese Academy of Sciences;Xi'an Jiao Tong University",
        "aff_unique_dep": "State Key Laboratory of Robotics;;",
        "aff_unique_url": "http://www.sia.cas.cn;http://www.ucas.ac.cn;https://www.xjtu.edu.cn",
        "aff_unique_abbr": ";UCAS;XJTU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Robust Visual Place Recognition With Graph Kernels",
        "session": "Vision For Robotics",
        "status": "Poster",
        "track": "main",
        "pid": "80",
        "author_site": "Elena Stumm, Christopher Mei, Simon Lacroix, Juan Nieto, Marco Hutter, Roland Siegwart",
        "author": "Elena Stumm; Christopher Mei; Simon Lacroix; Juan Nieto; Marco Hutter; Roland Siegwart",
        "abstract": "A novel method for visual place recognition is introduced and evaluated, demonstrating robustness to perceptual aliasing and observation noise. This is achieved by increasing discrimination through a more structured representation of visual observations. Estimation of observation likelihoods are based on graph kernel formulations, utilizing both the structural and visual information encoded in covisibility graphs. The proposed probabilistic model is able to circumvent the typically difficult and expensive posterior normalization procedure by exploiting the information available in visual observations. Furthermore, the place recognition complexity is independent of the size of the map. Results show improvements over the state-of-the-art on a diverse set of both public datasets and novel experiments, highlighting the benefit of the approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Stumm_Robust_Visual_Place_CVPR_2016_paper.pdf",
        "aff": "LAAS-CNRS; LAAS-CNRS; LAAS-CNRS; Autonomous Systems Lab; Autonomous Systems Lab; Autonomous Systems Lab",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1553176,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9855604146679385277&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "laas.fr;laas.fr;laas.fr;ethz.ch;ethz.ch;ethz.ch",
        "email": "laas.fr;laas.fr;laas.fr;ethz.ch;ethz.ch;ethz.ch",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Stumm_Robust_Visual_Place_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;1;1",
        "aff_unique_norm": "LAAS-CNRS;Autonomous Systems Lab",
        "aff_unique_dep": ";Autonomous Systems",
        "aff_unique_url": "https://www.laas.fr/;",
        "aff_unique_abbr": "LAAS-CNRS;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France;"
    },
    {
        "title": "Robust, Real-Time 3D Tracking of Multiple Objects With Similar Appearances",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "52",
        "author": "Taiki Sekii",
        "abstract": "This paper proposes a novel method for tracking multiple moving objects and recovering their three-dimensional (3D) models separately using multiple calibrated cameras. For robustly tracking objects with similar appearances, the proposed method uses geometric information regarding 3D scene structure rather than appearance. A major limitation of previous techniques is foreground confusion, in which the shapes of objects and/or ghosting artifacts are ignored and are hence not appropriately specified in foreground regions. To overcome this limitation, our method classifies foreground voxels into targets (objects and artifacts) in each frame using a novel, probabilistic two-stage framework. This is accomplished by step-wise application of a track graph describing how targets interact and the maximum a posteriori expectation-maximization algorithm for the estimation of target parameters. We introduce mixture models with semiparametric component distributions regarding 3D target shapes. In order to not confuse artifacts with objects of interest, we automatically detect and track artifacts based on a closed-world assumption. Experimental results show that our method outperforms state-of-the-art trackers on seven public sequences while achieving real-time performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sekii_Robust_Real-Time_3D_CVPR_2016_paper.pdf",
        "aff": "Panasonic System Networks R&D Lab. Co., Ltd.",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Sekii_Robust_Real-Time_3D_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 834580,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7335752035130772952&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "jp.panasonic.com",
        "email": "jp.panasonic.com",
        "author_num": 1,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sekii_Robust_Real-Time_3D_CVPR_2016_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "Panasonic System Networks R&D Lab. Co., Ltd.",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.panasonic.com/global",
        "aff_unique_abbr": "PSN",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Rolling Rotations for Recognizing Human Actions From 3D Skeletal Data",
        "session": "Statistical Methods and Learning",
        "status": "Poster",
        "track": "main",
        "pid": "73",
        "author_site": "Raviteja Vemulapalli, Rama Chellapa",
        "author": "Raviteja Vemulapalli; Rama Chellapa",
        "abstract": "Recently, skeleton-based human action recognition has been receiving significant attention from various research communities due to the availability of depth sensors and real-time depth-based 3D skeleton estimation algorithms. In this work, we use rolling maps for recognizing human actions from 3D skeletal data. The rolling map is a well-defined mathematical concept that has not been explored much by the vision community. First, we represent each skeleton using the relative 3D rotations between various body parts. Since 3D rotations are members of the special orthogonal group SO(3), our skeletal representation becomes a point in the Lie group SO(3) X ... X SO(3), which is also a Riemannian manifold. Then, using this representation, we model human actions as curves in this Lie group. Since classification of curves in this non-Euclidean space is a difficult task, we unwrap the action curves onto the Lie algebra (which is a vector space) by combining the logarithm map with rolling maps, and perform classification in the Lie algebra. Experimental results on three action datasets show that the proposed approach performs equally well or better when compared to state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Vemulapalli_Rolling_Rotations_for_CVPR_2016_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 259,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11820962091191008072&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Vemulapalli_Rolling_Rotations_for_CVPR_2016_paper.html"
    },
    {
        "title": "Rolling Shutter Absolute Pose Problem With Known Vertical Direction",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "35",
        "author_site": "Cenek Albl, Zuzana Kukelova, Tomas Pajdla",
        "author": "Cenek Albl; Zuzana Kukelova; Tomas Pajdla",
        "abstract": "We present a solution to the rolling shutter (RS) absolute camera pose problem with known vertical direction. Our new solver, R5Pup, is an extension of the general minimal solution R6P, which uses a double linearized RS camera model initialized by the standard perspective P3P. Here, thanks to using known vertical directions, we avoid double linearization and can get the camera absolute pose directly from the RS model without the initialization by a standard P3P. Moreover, we need only five 2D-to-3D matches while R6P needed six such matches. We demonstrate in simulated and real experiments that our new R5Pup is robust, fast and a very practical method for absolute camera pose computation for modern cameras on mobile devices. We compare our R5Pup to the state of the art RS and perspective methods and demonstrate that it outperforms them when vertical direction is known in the range of accuracy available on modern mobile devices. We also demonstrate that when using R5Pup solver in structure from motion (SfM) pipelines, it is better to transform already reconstructed scenes into the standard position, rather than using hard constraints on the verticality of up vectors.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Albl_Rolling_Shutter_Absolute_CVPR_2016_paper.pdf",
        "aff": "Czech Technical University in Prague, Faculty of Electrical engineering, 166 27 Praha 6, Technicka 2, Czech Republic; Microsoft Research Ltd, 21 Station Road, Cambridge CB1 2FB, UK; Czech Technical University in Prague, Faculty of Electrical engineering, 166 27 Praha 6, Technicka 2, Czech Republic",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1266916,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1449202629378060633&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "cmp.felk.cvut.cz;microsoft.com;cmp.felk.cvut.cz",
        "email": "cmp.felk.cvut.cz;microsoft.com;cmp.felk.cvut.cz",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Albl_Rolling_Shutter_Absolute_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Czech Technical University in Prague;Microsoft",
        "aff_unique_dep": "Faculty of Electrical Engineering;Microsoft Research",
        "aff_unique_url": "https://www.cvut.cz;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "CTU;MSR",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Prague;Cambridge",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Czech Republic;United Kingdom"
    },
    {
        "title": "Rolling Shutter Camera Relative Pose: Generalized Epipolar Geometry",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "37",
        "author_site": "Yuchao Dai, Hongdong Li, Laurent Kneip",
        "author": "Yuchao Dai; Hongdong Li; Laurent Kneip",
        "abstract": "The vast majority of modern consumer-grade cameras employ a rolling shutter mechanism. In dynamic geometric computer vision applications such as visual SLAM, the so-called rolling shutter effect therefore needs to be properly taken into account. A dedicated relative pose solver appears to be the first problem to solve, as it is of eminent importance to bootstrap any derivation of multi-view geometry. However, despite its significance, it has received inadequate attention to date.  This paper presents a detailed investigation of the geometry of the rolling shutter relative pose problem. We introduce the rolling shutter essential matrix, and establish its link to existing models such as the push-broom cameras, summarized in a clean hierarchy of multi-perspective cameras. The generalization of well-established concepts from epipolar geometry is completed by a definition of the Sampson distance in the rolling shutter case. The work is concluded with a careful investigation of the introduced epipolar geometry for rolling shutter cameras on several dedicated benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Dai_Rolling_Shutter_Camera_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Dai_Rolling_Shutter_Camera_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1311973,
        "gs_citation": 94,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15643254259456026786&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Dai_Rolling_Shutter_Camera_CVPR_2016_paper.html"
    },
    {
        "title": "Rotational Crossed-Slit Light Field",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "66",
        "author_site": "Nianyi Li, Haiting Lin, Bilin Sun, Mingyuan Zhou, Jingyi Yu",
        "author": "Nianyi Li; Haiting Lin; Bilin Sun; Mingyuan Zhou; Jingyi Yu",
        "abstract": "Light fields (LFs) are image-based representation that records the radiance along all rays along every direction through every point in space. Traditionally LFs are acquired by using a 2D grid of evenly spaced pinhole cameras or by translating a pinhole camera along the 2D grid using a robot arm. In this paper, we present a novel LF sampling scheme by exploiting a special non-centric camera called the crossed-slit or XSlit camera. An XSlit camera acquires rays that simultaneously pass through two oblique slits. We show that, instead of translating the camera as in the pinhole case, we can effectively sample the LF by rotating individual or both slits while keeping the camera fixed. This leads a \"fixed-location\" LF acquisition scheme. We further show through theoretical analysis and experiments that the resulting XSlit LFs provide several advantages: they provide more dense spatial-angular sampling, are amenable multi-view stereo matching and volumetric reconstruction, and can synthesize unique refocusing effects.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Rotational_Crossed-Slit_Light_CVPR_2016_paper.pdf",
        "aff": "University of Delaware, Newark, DE, USA; University of Delaware, Newark, DE, USA; University of Delaware, Newark, DE, USA; University of Delaware, Newark, DE, USA; ShanghaiTech University, Shanghai, China + University of Delaware, Newark, DE, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 871751,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12840885929903367574&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "eecis.udel.edu;eecis.udel.edu;eecis.udel.edu;eecis.udel.edu;shanghaitech.edu.cn",
        "email": "eecis.udel.edu;eecis.udel.edu;eecis.udel.edu;eecis.udel.edu;shanghaitech.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Rotational_Crossed-Slit_Light_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;1+0",
        "aff_unique_norm": "University of Delaware;ShanghaiTech University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.udel.edu;http://www.shanghaitech.edu.cn",
        "aff_unique_abbr": "UD;ShanghaiTech",
        "aff_campus_unique_index": "0;0;0;0;1+0",
        "aff_campus_unique": "Newark;Shanghai",
        "aff_country_unique_index": "0;0;0;0;1+0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "SPDA-CNN: Unifying Semantic Part Detection and Abstraction for Fine-Grained Recognition",
        "session": "Fine Grained Categorization",
        "status": "Poster",
        "track": "main",
        "pid": "41",
        "author_site": "Han Zhang, Tao Xu, Mohamed Elhoseiny, Xiaolei Huang, Shaoting Zhang, Ahmed Elgammal, Dimitris Metaxas",
        "author": "Han Zhang; Tao Xu; Mohamed Elhoseiny; Xiaolei Huang; Shaoting Zhang; Ahmed Elgammal; Dimitris Metaxas",
        "abstract": "Most convolutional neural networks (CNNs) lack midlevel layers that model semantic parts of objects. This limits CNN-based methods from reaching their full potential in detecting and utilizing small semantic parts in recognition. Introducing such mid-level layers can facilitate the extraction of part-specific features which can be utilized for better recognition performance. This is particularly important in the domain of fine-grained recognition.  In this paper, we propose a new CNN architecture that integrates semantic part detection and abstraction (SPDA-CNN) for fine-grained classification. The proposed network has two sub-networks: one for detection and one for recognition. The detection sub-network has a novel top-down proposal method to generate small semantic part candidates for detection. The classification sub-network introduces novel part layers that extract features from parts detected by the detection sub-network, and combine them for recognition. As a result, the proposed architecture provides an end-to-end network that performs detection, localization of multiple semantic parts, and whole object recognition within one framework that shares the computation of convolutional filters. Our method outperforms state-of-the-art methods with a large margin for small parts detection (e.g. our precision of 93.40% vs the best previous precision of 74.00% for detecting the head on CUB-2011). It also compares favorably to the existing state-of-the-art on fine-grained classification, e.g. it achieves 85.14% accuracy on CUB-2011.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_SPDA-CNN_Unifying_Semantic_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, Rutgers University; Department of Computer Science and Engineering, Lehigh University; Department of Computer Science, Rutgers University; Department of Computer Science and Engineering, Lehigh University; Department of Computer Science, University of North Carolina at Charlotte; Department of Computer Science, Rutgers University; Department of Computer Science, Rutgers University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 817018,
        "gs_citation": 382,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5421481579953430412&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": "rutgers.edu; ;rutgers.edu; ;uncc.edu;rutgers.edu;rutgers.edu",
        "email": "rutgers.edu; ;rutgers.edu; ;uncc.edu;rutgers.edu;rutgers.edu",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_SPDA-CNN_Unifying_Semantic_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1;2;0;0",
        "aff_unique_norm": "Rutgers University;Lehigh University;University of North Carolina at Charlotte",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science and Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.rutgers.edu;https://www.lehigh.edu;https://www.uncc.edu",
        "aff_unique_abbr": "Rutgers;Lehigh;UNCC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Charlotte",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "STCT: Sequentially Training Convolutional Networks for Visual Tracking",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "65",
        "author_site": "Lijun Wang, Wanli Ouyang, Xiaogang Wang, Huchuan Lu",
        "author": "Lijun Wang; Wanli Ouyang; Xiaogang Wang; Huchuan Lu",
        "abstract": "Due to the limited amount of training samples, fine-tuning pre-trained deep models online is prone to over-fitting. In this paper, we propose a sequential training method for convolutional neural networks (CNNs) to effectively transfer pre-trained deep features for online applications. We regard a CNN as an ensemble with each channel of the output feature map as an individual base learner. Each base learner is trained using different loss criterions to reduce correlation and avoid over-training. To achieve the best ensemble online, all the base learners are sequentially sampled into the ensemble via important sampling. To further improve the robustness of each base learner, we propose to train the convolutional layers with random binary masks, which serves as a regularization to enforce each base learner to focus on different input features.  The proposed online training method is applied to visual tracking problem by transferring deep features trained on massive annotated visual data and is shown to significantly improve tracking performance. Extensive experiments are conducted on two challenging benchmark data set and demonstrate that our tracking algorithm can outperform state-of-the-art methods with a considerable margin.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_STCT_Sequentially_Training_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1454794,
        "gs_citation": 330,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18338588447290986583&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_STCT_Sequentially_Training_CVPR_2016_paper.html"
    },
    {
        "title": "SVBRDF-Invariant Shape and Reflectance Estimation From Light-Field Cameras",
        "session": "3D Shape Reconstruction",
        "status": "Oral",
        "track": "main",
        "pid": "17",
        "author_site": "Ting-Chun Wang, Manmohan Chandraker, Alexei A. Efros, Ravi Ramamoorthi",
        "author": "Ting-Chun Wang; Manmohan Chandraker; Alexei A. Efros; Ravi Ramamoorthi",
        "abstract": "Light-field cameras have recently emerged as a powerful tool for one-shot passive 3D shape capture. However, obtaining the shape of glossy objects like metals, plastics or ceramics remains challenging, since standard Lambertian cues like photo-consistency cannot be easily applied. In this paper, we derive a spatially-varying (SV)BRDF-invariant theory for recovering 3D shape and reflectance from light-field cameras. Our key theoretical insight is a novel analysis of diffuse plus single-lobe SVBRDFs under a light-field setup. We show that, although direct shape recovery is not possible, an equation relating depths and normals can still be derived. Using this equation, we then propose using a polynomial (quadratic) shape prior to resolve the shape ambiguity. Once shape is estimated, we can also recover the reflectance. We present extensive synthetic data on the entire MERL BRDF dataset, as well as a number of real examples to validate the theory, where we simultaneously recover shape and BRDFs from a single image taken with a Lytro Illum camera.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_SVBRDF-Invariant_Shape_and_CVPR_2016_paper.pdf",
        "aff": "UC Berkeley; NEC Labs; UC Berkeley; UC San Diego",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2460448,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10983211140164887867&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "berkeley.edu;nec-labs.com;eecs.berkeley.edu;cs.ucsd.edu",
        "email": "berkeley.edu;nec-labs.com;eecs.berkeley.edu;cs.ucsd.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_SVBRDF-Invariant_Shape_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of California, Berkeley;NEC Laboratories;University of California, San Diego",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.nec-labs.com;https://www.ucsd.edu",
        "aff_unique_abbr": "UC Berkeley;NEC Labs;UCSD",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Berkeley;;San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Saliency Guided Dictionary Learning for Weakly-Supervised Image Parsing",
        "session": "Semantic Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "65",
        "author_site": "Baisheng Lai, Xiaojin Gong",
        "author": "Baisheng Lai; Xiaojin Gong",
        "abstract": "In this paper, we propose a novel method to perform weakly-supervised image parsing based on the dictionary learning framework. To deal with the challenges caused by the label ambiguities, we design a saliency guided weight assignment scheme to boost the discriminative dictionary learning. More specifically, with a collection of tagged images, the proposed method first conducts saliency detection and automatically infers the confidence for each semantic class to be foreground or background. These clues are then incorporated to learn the dictionaries, the weights, as well as the sparse representation coefficients in the meanwhile. Once obtained the coefficients of a superpixel, we use a sparse representation classifier to determine its semantic label. The approach is validated on the MSRC21, PASCAL VOC07, and VOC12 datasets. Experimental results demonstrate the encouraging performance of our approach in comparison with some state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lai_Saliency_Guided_Dictionary_CVPR_2016_paper.pdf",
        "aff": "College of Information Science & Electronic Engineering, Zhejiang University, Hangzhou, Zhejiang, P. R. China; College of Information Science & Electronic Engineering, Zhejiang University, Hangzhou, Zhejiang, P. R. China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1381090,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10813317269517218169&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lai_Saliency_Guided_Dictionary_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "College of Information Science & Electronic Engineering",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hangzhou",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Saliency Unified: A Deep Architecture for Simultaneous Eye Fixation Prediction and Salient Object Segmentation",
        "session": "Biologically Inspired Vision",
        "status": "Poster",
        "track": "main",
        "pid": "52",
        "author_site": "Srinivas S. S. Kruthiventi, Vennela Gudisa, Jaley H. Dholakiya, R. Venkatesh Babu",
        "author": "Srinivas S. S. Kruthiventi; Vennela Gudisa; Jaley H. Dholakiya; R. Venkatesh Babu",
        "abstract": "Human eye fixations often correlate with locations of salient objects in the scene. However, only a handful of approaches have attempted to simultaneously address the related aspects of eye fixations and object saliency. In this work, we propose a deep convolutional neural network (CNN) capable of predicting eye fixations and segmenting salient objects in a unified framework. We design the initial network layers, shared between both the tasks, such that they capture the object level semantics and the global contextual aspects of saliency, while the deeper layers of the network address task specific aspects. In addition, our network captures saliency at multiple scales via inception-style convolution blocks. Our network shows a significant improvement over the current state-of-the-art for both eye fixation prediction and salient object segmentation across a number of challenging datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kruthiventi_Saliency_Unified_A_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 604264,
        "gs_citation": 185,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14529700088202633492&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kruthiventi_Saliency_Unified_A_CVPR_2016_paper.html"
    },
    {
        "title": "Sample and Filter: Nonparametric Scene Parsing via Efficient Filtering",
        "session": "Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "65",
        "author_site": "Mohammad Najafi, Sarah Taghavi Namin, Mathieu Salzmann, Lars Petersson",
        "author": "Mohammad Najafi; Sarah Taghavi Namin; Mathieu Salzmann; Lars Petersson",
        "abstract": "Scene parsing has attracted a lot of attention in computer vision. While parametric models have proven effective for this task, they cannot easily incorporate new training data. By contrast, nonparametric approaches, which bypass any learning phase and directly transfer the labels from the training data to the query images, can readily exploit new labeled samples as they become available. Unfortunately, because of the computational cost of their label transfer procedures, state-of-the-art nonparametric methods typically filter out most training images to only keep a few  relevant ones to label the query. As such, these methods throw away many images that still contain valuable information and generally obtain an unbalanced set of labeled samples. In this paper, we introduce a nonparametric approach to scene parsing that follows a sample-and-filter strategy. More specifically, we propose to sample labeled superpixels according to an image similarity score, which allows us to obtain a balanced set of samples. We then formulate label transfer as an efficient filtering procedure, which lets us exploit more labeled samples than existing techniques. Our experiments evidence the benefits of our approach over state-of-the-art nonparametric methods on two benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Najafi_Sample_and_Filter_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14748021124836547239&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Najafi_Sample_and_Filter_CVPR_2016_paper.html"
    },
    {
        "title": "Sample-Specific SVM Learning for Person Re-Identification",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "55",
        "author_site": "Ying Zhang, Baohua Li, Huchuan Lu, Atshushi Irie, Xiang Ruan",
        "author": "Ying Zhang; Baohua Li; Huchuan Lu; Atshushi Irie; Xiang Ruan",
        "abstract": "Person re-identification addresses the problem of matching people across disjoint camera views and extensive efforts have been made to seek either the robust feature representation or the discriminative matching metrics. However, most existing approaches focus on learning a fixed distance metric for all instance pairs, while ignoring the individuality of each person. In this paper, we formulate the person re-identification problem as an imbalanced classification problem and learn a classifier specifically for each pedestrian such that the matching model is highly tuned to the individual's appearance. To establish correspondence between feature space and classifier space, we propose a Least Square Semi-Coupled Dictionary Learning (LSSCDL) algorithm to learn a pair of dictionaries and a mapping function efficiently. Extensive experiments on a series of challenging databases demonstrate that the proposed algorithm performs favorably against the state-of-the-art approaches, especially on the rank-1 recognition rate.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Sample-Specific_SVM_Learning_CVPR_2016_paper.pdf",
        "aff": "Dalian University of Technology; Dalian University of Technology; Dalian University of Technology; OMRON Corporation; OMRON Corporation",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 656337,
        "gs_citation": 226,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3186667548285691432&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Sample-Specific_SVM_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Dalian University of Technology;OMRON Corporation",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.dlut.edu.cn/;https://www.omron.com",
        "aff_unique_abbr": "DUT;OMRON",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;1",
        "aff_country_unique": "China;Japan"
    },
    {
        "title": "Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit",
        "session": "Grouping and Optimization Methods",
        "status": "Oral",
        "track": "main",
        "pid": "14",
        "author_site": "Chong You, Daniel Robinson, Ren\u00e9 Vidal",
        "author": "Chong You; Daniel Robinson; Rene Vidal",
        "abstract": "Subspace clustering methods based on ell_1, l_2 or nuclear norm regularization have become very popular due to their simplicity, theoretical guarantees and empirical success. However, the choice of the regularizer can greatly impact both theory and practice. For instance, ell_1 regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad conditions e.g., arbitrary subspaces and corrupted data). However, it requires solving a large scale convex optimization problem. On the other hand, l_2 and nuclear norm regularization provide efficient closed form solutions, but require very strong assumptions to guarantee a subspace-preserving affinity, e.g., independent subspaces and uncorrupted data. In this paper we study a subspace clustering method based on orthogonal matching pursuit. We show that the method is both computationally efficient and guaranteed to give a subspace-preserving affinity under broad conditions. Experiments on synthetic data verify our theoretical analysis, and applications in handwritten digit and face clustering show that our approach achieves the best trade off between accuracy and efficiency. Moreover, our approach is the first one to handle 100,000 data points.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/You_Scalable_Sparse_Subspace_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 752348,
        "gs_citation": 459,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2145047173014549367&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/You_Scalable_Sparse_Subspace_CVPR_2016_paper.html"
    },
    {
        "title": "Scale-Aware Alignment of Hierarchical Image Segmentation",
        "session": "Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "39",
        "author_site": "Yuhua Chen, Dengxin Dai, Jordi Pont-Tuset, Luc Van Gool",
        "author": "Yuhua Chen; Dengxin Dai; Jordi Pont-Tuset; Luc Van Gool",
        "abstract": "Image segmentation is a key component in many computer vision systems, and it is recovering a prominent spot in the literature as methods improve and overcome their limitations. The outputs of most recent algorithms are in the form of a hierarchical segmentation, which provides segmentation at different scales in a single tree-like structure. Commonly, these hierarchical methods start from some low-level features, and are not aware of the scale information of the different regions in them. As such, one might need to work on many different levels of the hierarchy to find the objects in the scene. This work tries to modify the existing hierarchical algorithm by improving their alignment, that is, by trying to modify the depth of the regions in the tree to better couple depth and scale. To do so, we first train a regressor to predict the scale of regions using mid-level features. We then define the anchor slice as the set of regions that better balance between over-segmentation and under-segmentation. The output of our method is an improved hierarchy, re-aligned by the anchor slice. To demonstrate the power of our method, we perform comprehensive experiments, which show that our method, as a post-processing step, can significantly improve the quality of the hierarchical segmentation representations, and ease the usage of hierarchical image segmentation to high-level vision tasks such as object segmentation. We also prove that the improvement generalizes well across different algorithms and datasets, with a low computational cost.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Scale-Aware_Alignment_of_CVPR_2016_paper.pdf",
        "aff": "Computer Vision Lab, ETH Zurich; Computer Vision Lab, ETH Zurich; Computer Vision Lab, ETH Zurich; Computer Vision Lab, ETH Zurich+VISICS, ESAT/PSI, KU Leuven",
        "project": "",
        "github": "https://github.com/yuhuayc/alignhier",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1512566,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8891068641020090104&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Scale-Aware_Alignment_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "ETH Zurich;KU Leuven",
        "aff_unique_dep": "Computer Vision Lab;VISICS, ESAT/PSI",
        "aff_unique_url": "https://www.ethz.ch;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;KU Leuven",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Zurich;",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Switzerland;Belgium"
    },
    {
        "title": "Scene Labeling Using Sparse Precision Matrix",
        "session": "Semantic Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "67",
        "author_site": "Nasim Souly, Mubarak Shah",
        "author": "Nasim Souly; Mubarak Shah",
        "abstract": "Scene labeling task is to segment the image into meaningful regions and categorize them into classes of objects which comprised the image. Commonly used methods typically find the local features for each segment and label them using classifiers. Afterwards, labeling is smoothed in order to make sure that neighboring regions receive similar labels. However, these methods ignore expressive connections between labels and non-local dependencies among regions. In this paper, we propose to use a sparse estimation of precision matrix (also called concentration matrix), which is the inverse of covariance matrix  of data obtained by graphical lasso to find interaction between labels and regions. To do this, we formulate the problem as an energy minimization over a graph, whose structure is captured by applying sparse constraint on the elements of the precision matrix. This graph encodes (or represents) only significant interactions and avoids a fully connected graph, which is typically used to reflect the long distance associations. We use local and global information to achieve better labeling. We assess our approach on three datasets and obtained promising results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Souly_Scene_Labeling_Using_CVPR_2016_paper.pdf",
        "aff": ";",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6401041754533398880&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Souly_Scene_Labeling_Using_CVPR_2016_paper.html"
    },
    {
        "title": "Scene Recognition With CNNs: Objects, Scales and Dataset Bias",
        "session": "Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "61",
        "author_site": "Luis Herranz, Shuqiang Jiang, Xiangyang Li",
        "author": "Luis Herranz; Shuqiang Jiang; Xiangyang Li",
        "abstract": "Since scenes are composed in part of objects, accurate recognition of scenes requires knowledge about both scenes and objects. In this paper we address two related problems: 1) scale induced dataset bias in multi-scale convolutional neural network (CNN) architectures, and 2) how to combine effectively scene-centric and object-centric knowledge (i.e. Places and ImageNet) in CNNs. An earlier attempt, Hybrid-CNN, showed that incorporating ImageNet did not help much. Here we propose an alternative method taking the scale into account, resulting in significant recognition gains. By analyzing the response of ImageNet-CNNs and Places-CNNs at different scales we find that both operate in different scale ranges, so using the same network for all the scales induces dataset bias resulting in limited performance. Thus, adapting the feature extractor to each particular scale (i.e. scale-specific CNNs) is crucial to improve recognition, since the objects in the scenes have their specific range of scales. Experimental results show that the recognition accuracy highly depends on the scale, and that simple yet carefully chosen multi-scale combinations of ImageNet-CNNs and Places-CNNs, can push the state-of-the-art recognition accuracy in SUN397 up to 66.26% (and even 70.17% with deeper architectures, comparable to human performance).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Herranz_Scene_Recognition_With_CVPR_2016_paper.pdf",
        "aff": "Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS); Institute of Computer Technology, CAS, Beijing, 100190, China; Institute of Computer Technology, CAS, Beijing, 100190, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 743918,
        "gs_citation": 258,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7079907706154402741&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn",
        "email": "vipl.ict.ac.cn;vipl.ict.ac.cn;vipl.ict.ac.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Herranz_Scene_Recognition_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences",
        "aff_unique_dep": "Key Laboratory of Intelligent Information Processing",
        "aff_unique_url": "http://www.cas.cn",
        "aff_unique_abbr": "CAS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation",
        "session": "Semantic Segmentation",
        "status": "Oral",
        "track": "main",
        "pid": "14",
        "author_site": "Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, Jian Sun",
        "author": "Di Lin; Jifeng Dai; Jiaya Jia; Kaiming He; Jian Sun",
        "abstract": "Large-scale data are of crucial importance for learning semantic segmentation models, but annotating per-pixel masks is a tedious and inefficient procedure. We note that for the topic of interactive image segmentation, scribbles are very widely used in academic research and commercial software, and are recognized as one of the most user-friendly ways of interacting. In this paper, we propose to use scribbles to annotate images, and develop an algorithm to train convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model that jointly propagates information from scribbles to unmarked pixels and learns network parameters. We present competitive object semantic segmentation results on the PASCAL VOC dataset by using scribbles as annotations. Scribbles are also favored for annotating stuff (e.g., water, sky, grass) that has no well-defined shape, and our method shows excellent results on the PASCAL-CONTEXT dataset thanks to extra inexpensive scribble annotations.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lin_ScribbleSup_Scribble-Supervised_Convolutional_CVPR_2016_paper.pdf",
        "aff": "The Chinese University of Hong Kong; Microsoft Research; The Chinese University of Hong Kong; Microsoft Research; Microsoft Research",
        "project": "http://research.microsoft.com/en-us/um/people/jifdai/downloads/scribble_sup",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2047306,
        "gs_citation": 1333,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1344368443978557455&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "link.cuhk.edu.hk;microsoft.com;link.cuhk.edu.hk;microsoft.com;microsoft.com",
        "email": "link.cuhk.edu.hk;microsoft.com;link.cuhk.edu.hk;microsoft.com;microsoft.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lin_ScribbleSup_Scribble-Supervised_Convolutional_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1;1",
        "aff_unique_norm": "Chinese University of Hong Kong;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "CUHK;MSR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;1;0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Seeing Behind the Camera: Identifying the Authorship of a Photograph",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "50",
        "author_site": "Christopher Thomas, Adriana Kovashka",
        "author": "Christopher Thomas; Adriana Kovashka",
        "abstract": "We introduce the novel problem of identifying the photographer behind a photograph. To explore the feasibility of current computer vision techniques to address this problem, we created a new dataset of over 180,000 images taken by 41 well-known photographers. Using this dataset, we examined the effectiveness of a variety of features (low and high-level, including CNN features) at identifying the photographer. We also trained a new deep convolutional neural network for this task. Our results show that high-level features greatly outperform low-level features. We provide qualitative results using these learned models that give insight into our method's ability to distinguish between photographers, and allow us to draw interesting conclusions about what specific photographers shoot. We also demonstrate two applications of our method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Thomas_Seeing_Behind_the_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, University of Pittsburgh; Department of Computer Science, University of Pittsburgh",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Thomas_Seeing_Behind_the_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2182897,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9787922882948184145&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.pitt.edu;cs.pitt.edu",
        "email": "cs.pitt.edu;cs.pitt.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Thomas_Seeing_Behind_the_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pittsburgh",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.pitt.edu",
        "aff_unique_abbr": "Pitt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Seeing Through the Human Reporting Bias: Visual Classifiers From Noisy Human-Centric Labels",
        "session": "Scene and Image Classification",
        "status": "Poster",
        "track": "main",
        "pid": "72",
        "author_site": "Ishan Misra, C. Lawrence Zitnick, Margaret Mitchell, Ross Girshick",
        "author": "Ishan Misra; C. Lawrence Zitnick; Margaret Mitchell; Ross Girshick",
        "abstract": "When human annotators are given a choice about what to label in an image, they apply their own subjective judgments on what to ignore and what to mention.  We refer to these noisy \"human-centric\" annotations as exhibiting human reporting bias.  Examples of such annotations include image tags and keywords found on photo sharing sites, or in datasets containing image captions. In this paper, we use these noisy annotations for learning visually correct image classifiers. Such annotations do not use consistent vocabulary, and miss a significant amount of the information present in an image; however, we demonstrate that the noise in these annotations exhibits structure and can be modeled. We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels. Our results are highly interpretable for reporting \"what's in the image\" versus \"what's worth saying.\"  We demonstrate the algorithm's efficacy along a variety of metrics and datasets, including MS COCO and Yahoo Flickr 100M. We show significant improvements over traditional algorithms for both image classification and image captioning, doubling the performance of existing methods in some cases.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Misra_Seeing_Through_the_CVPR_2016_paper.pdf",
        "aff": "Carnegie Mellon University; Facebook AI Research; Microsoft Research; Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2585549,
        "gs_citation": 273,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4912438996777567274&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cmu.edu;fb.com;microsoft.com;fb.com",
        "email": "cmu.edu;fb.com;microsoft.com;fb.com",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Misra_Seeing_Through_the_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Carnegie Mellon University;Meta;Microsoft",
        "aff_unique_dep": ";Facebook AI Research;Microsoft Research",
        "aff_unique_url": "https://www.cmu.edu;https://research.facebook.com;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "CMU;FAIR;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Self-Adaptive Matrix Completion for Heart Rate Estimation From Face Videos Under Realistic Conditions",
        "session": "Computational Photography and Faces",
        "status": "Oral",
        "track": "main",
        "pid": "15",
        "author_site": "Sergey Tulyakov, Xavier Alameda-Pineda, Elisa Ricci, Lijun Yin, Jeffrey F. Cohn, Nicu Sebe",
        "author": "Sergey Tulyakov; Xavier Alameda-Pineda; Elisa Ricci; Lijun Yin; Jeffrey F. Cohn; Nicu Sebe",
        "abstract": "Recent studies in computer vision have shown that, while practically invisible to a human observer, skin color changes due to blood flow can be captured on face videos and, surprisingly, be used to estimate the heart rate (HR). While considerable progress has been made in the last few years, still many issues remain open. In particular, state-of-the-art approaches are not robust enough to operate in natural conditions (e.g. in case of spontaneous movements, facial expressions, or illumination changes). Opposite to previous approaches that estimate the HR by processing all the skin pixels inside a fixed region of interest, we introduce a strategy to dynamically select face regions useful for robust HR estimation. Our approach, inspired by recent advances on matrix completion theory, allows us to predict the HR while simultaneously discover the best regions of the face to be used for estimation. Thorough experimental evaluation conducted on public benchmarks suggests that the proposed approach significantly outperforms state-of-the-art HR estimation methods in naturalistic conditions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tulyakov_Self-Adaptive_Matrix_Completion_CVPR_2016_paper.pdf",
        "aff": "University of Trento; University of Trento; Fondazione Bruno Kessler + University of Perugia; State University of New York at Binghamton; Robotics Institute, Carnegie Mellon University + Department of Psychology, University of Pittsburgh; University of Trento",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1460369,
        "gs_citation": 413,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5066842903989697842&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "unitn.it;unitn.it;fbk.eu;cs.binghamton.edu;pitt.edu;unitn.it",
        "email": "unitn.it;unitn.it;fbk.eu;cs.binghamton.edu;pitt.edu;unitn.it",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tulyakov_Self-Adaptive_Matrix_Completion_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1+2;3;4+5;0",
        "aff_unique_norm": "University of Trento;Fondazione Bruno Kessler;University of Perugia;State University of New York at Binghamton;Carnegie Mellon University;University of Pittsburgh",
        "aff_unique_dep": ";;;;Robotics Institute;Department of Psychology",
        "aff_unique_url": "https://www.unitn.it;https://www.fbk.eu;https://www.unipg.it;https://www.binghamton.edu;https://www.cmu.edu;https://www.pitt.edu",
        "aff_unique_abbr": "UniTN;FBK;Unipg;SUNY Binghamton;CMU;Pitt",
        "aff_campus_unique_index": ";1;2",
        "aff_campus_unique": ";Binghamton;Pittsburgh",
        "aff_country_unique_index": "0;0;0+0;1;1+1;0",
        "aff_country_unique": "Italy;United States"
    },
    {
        "title": "Semantic 3D Reconstruction With Continuous Regularization and Ray Potentials Using a Visibility Consistency Constraint",
        "session": "3D Reconstruction",
        "status": "Spotlight",
        "track": "main",
        "pid": "18",
        "author_site": "Nikolay Savinov, Christian H\u00e4ne, \u013dubor Ladick\u00fd, Marc Pollefeys",
        "author": "Nikolay Savinov; Christian Hane; Lubor Ladicky; Marc Pollefeys",
        "abstract": "We propose an approach for dense semantic 3D reconstruction which uses a data term that is defined as potentials over viewing rays, combined with continuous surface area penalization. Our formulation is a convex relaxation which we augment with a crucial non-convex constraint that ensures exact handling of visibility. To tackle the non-convex minimization problem, we propose a majorize-minimize type strategy which converges to a critical point. We demonstrate the benefits of using the non-convex constraint experimentally. For the geometry-only case, we set a new state of the art on two datasets of the commonly used Middlebury multi-view stereo benchmark. Moreover, our general-purpose formulation directly reconstructs thin objects, which are usually treated with specialized algorithms. A qualitative evaluation on the dense semantic 3D reconstruction task shows that we improve significantly over previous methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Savinov_Semantic_3D_Reconstruction_CVPR_2016_paper.pdf",
        "aff": "ETH Z \u00a8urich, Switzerland; ETH Z \u00a8urich, Switzerland; ETH Z \u00a8urich, Switzerland; ETH Z \u00a8urich, Switzerland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Savinov_Semantic_3D_Reconstruction_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2620077,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18086771611965129134&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Savinov_Semantic_3D_Reconstruction_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Semantic Channels for Fast Pedestrian Detection",
        "session": "Object Detection 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "11",
        "author_site": "Arthur Daniel Costea, Sergiu Nedevschi",
        "author": "Arthur Daniel Costea; Sergiu Nedevschi",
        "abstract": "Pedestrian detection and semantic segmentation are high potential tasks for many real-time applications. However most of the top performing approaches provide state of art results at high computational costs. In this work we propose a fast solution for achieving state of art results for both pedestrian detection and semantic segmentation.  As baseline for pedestrian detection we use sliding windows over cost efficient multiresolution filtered LUV+HOG channels. We use the same channels for classifying pixels into eight semantic classes. Using short range and long range multiresolution channel features we achieve more robust  segmentation results compared to traditional codebook based approaches at much lower computational costs. The resulting segmentations are used as additional semantic channels in order to achieve a more powerful pedestrian detector. To also achieve fast pedestrian detection we employ a multiscale detection scheme based on a single flexible pedestrian model and a single image scale. The proposed solution provides competitive results on both pedestrian detection and semantic segmentation benchmarks at 8 FPS on CPU and at 15 FPS on GPU, being the fastest top performing approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Costea_Semantic_Channels_for_CVPR_2016_paper.pdf",
        "aff": "Image Processing and Pattern Recognition Research Center, Technical University of Cluj-Napoca, Romania; Image Processing and Pattern Recognition Research Center, Technical University of Cluj-Napoca, Romania",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 876216,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9588883586092819041&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "cs.utcluj.ro;cs.utcluj.ro",
        "email": "cs.utcluj.ro;cs.utcluj.ro",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Costea_Semantic_Channels_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technical University of Cluj-Napoca",
        "aff_unique_dep": "Image Processing and Pattern Recognition Research Center",
        "aff_unique_url": "https://www.utcluj.ro",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Romania"
    },
    {
        "title": "Semantic Filtering",
        "session": "Vision For Graphics",
        "status": "Poster",
        "track": "main",
        "pid": "78",
        "author": "Qingxiong Yang",
        "abstract": "Edge-preserving image operations aim at smoothing an image without blurring the edges. Many excellent edge-preserving filtering techniques have been proposed recently to reduce the computational complexity or/and separate different scale structures. They normally adopt a user-selected scale measurement to control the detail/texture smoothing. However, natural photos contain objects of different sizes which cannot be described by a single scale measurement. On the other hand, edge/contour detection/analysis is closely related to edge-preserving filtering and has achieved significant progress recently. Nevertheless, most of the state-of-the-art filtering techniques ignore the success in this area. Inspired by the fact that learning-based edge detectors/classifiers significantly outperform traditional manually-designed detectors, this paper proposes a learning-based edge-preserving filtering technique. It synergistically combines the efficiency of the recursive filter and the effectiveness of the recent edge detector for scale-aware edge-preserving filtering. Unlike previous filtering methods, the propose filter can efficiently extract subjectively-meaningful structures from natural scenes containing multiple-scale objects.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_Semantic_Filtering_CVPR_2016_paper.pdf",
        "aff": "School of Information Science and Technology, University of Science and Technology of China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3043148,
        "gs_citation": 49,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16716872608595033282&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "",
        "email": "",
        "author_num": 1,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_Semantic_Filtering_CVPR_2016_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "School of Information Science and Technology",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "title": "Semantic Image Segmentation With Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform",
        "session": "Semantic Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "81",
        "author_site": "Liang-Chieh Chen, Jonathan T. Barron, George Papandreou, Kevin Murphy, Alan L. Yuille",
        "author": "Liang-Chieh Chen; Jonathan T. Barron; George Papandreou; Kevin Murphy; Alan L. Yuille",
        "abstract": "Deep convolutional neural networks (CNNs) are the backbone of state-of-art semantic image segmentation systems. Recent work has shown that complementing CNNs with fully-connected conditional random fields (CRFs) can significantly enhance their object localization accuracy, yet dense CRF inference is computationally expensive. We propose replacing the fully-connected CRF with domain transform (DT), a modern edge-preserving filtering method in which the amount of smoothing is controlled by a reference edge map. Domain transform filtering is several times faster than dense CRF inference and we show that it yields comparable semantic segmentation results, accurately capturing object boundaries. Importantly, our formulation allows learning the reference edge map from intermediate CNN features instead of using the image gradient magnitude as in standard DT filtering. This produces task-specific edges in an end-to-end trainable system optimizing the target semantic segmentation quality.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Semantic_Image_Segmentation_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Chen_Semantic_Image_Segmentation_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2860291,
        "gs_citation": 451,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10522050088175928905&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Semantic_Image_Segmentation_CVPR_2016_paper.html"
    },
    {
        "title": "Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer",
        "session": "Semantic Video Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "71",
        "author_site": "Jun Xie, Martin Kiefel, Ming-Ting Sun, Andreas Geiger",
        "author": "Jun Xie; Martin Kiefel; Ming-Ting Sun; Andreas Geiger",
        "abstract": "This supplementary material provides additional illustrations, visualizations and experiments. We start by showing the color coding and label mapping used for the semantic and instance label results in the paper. Then we provide more details about the 3D fold/curb detection and parameter settings that are used in the paper. Next, we provide additional quantitative and qualitative semi-dense inference results for both semantic and instance segmentation. Finally, we show the ability of our method to annotate 3D point clouds with semantic and instance labels which is a byproduct of our approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xie_Semantic_Instance_Annotation_CVPR_2016_paper.pdf",
        "aff": "University of Washington; MPI for Intelligent Systems T\u00fcbingen; University of Washington; MPI for Intelligent Systems T\u00fcbingen",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Xie_Semantic_Instance_Annotation_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2037904,
        "gs_citation": 220,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1307913752818718315&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "uw.edu;tue.mpg.de;uw.edu;tue.mpg.de",
        "email": "uw.edu;tue.mpg.de;uw.edu;tue.mpg.de",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xie_Semantic_Instance_Annotation_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Washington;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://www.mpituebingen.mpg.de",
        "aff_unique_abbr": "UW;MPI-IS",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";T\u00fcbingen",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "Semantic Object Parsing With Local-Global Long Short-Term Memory",
        "session": "Semantic Parsing and Segmentation",
        "status": "Spotlight",
        "track": "main",
        "pid": "17",
        "author_site": "Xiaodan Liang, Xiaohui Shen, Donglai Xiang, Jiashi Feng, Liang Lin, Shuicheng Yan",
        "author": "Xiaodan Liang; Xiaohui Shen; Donglai Xiang; Jiashi Feng; Liang Lin; Shuicheng Yan",
        "abstract": "Semantic object parsing is a fundamental task for understanding objects in detail in computer vision community, where incorporating multi-level contextual information is critical for achieving such fine-grained pixel-level recognition. Prior methods often leverage the contextual information through post-processing predicted confidence maps. In this work, we propose a novel deep Local-Global Long Short-Term Memory (LG-LSTM) architecture to seamlessly incorporate short-distance and long-distance spatial dependencies into the feature learning over all pixel positions. In each LG-LSTM layer, local guidance from neighboring positions and global guidance from the whole image are imposed on each position to better exploit complex local and global contextual information. Individual LSTMs for distinct spatial dimensions are also utilized to intrinsically capture various spatial layouts of semantic parts in the images, yielding distinct hidden and memory cells of each position for each dimension. In our parsing approach, several LG-LSTM layers are stacked and appended to the intermediate convolutional layers to directly enhance visual features, allowing network parameters to be learned in an end-to-end way. The long chains of sequential computation by stacked LG-LSTM layers also enable each pixel to sense a much larger region for inference benefiting from the memorization of previous dependencies in all positions along all dimensions. Comprehensive evaluations on three public datasets well demonstrate the significant superiority of our LG-LSTM over other state-of-the-art methods for object parsing.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liang_Semantic_Object_Parsing_CVPR_2016_paper.pdf",
        "aff": "Sun Yat-sen University; Adobe Research; Sun Yat-sen University; National University of Singapore; Sun Yat-sen University; National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 933762,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3998686864110061245&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "ieee.org; ; ; ; ; ",
        "email": "ieee.org; ; ; ; ; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liang_Semantic_Object_Parsing_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;2;0;2",
        "aff_unique_norm": "Sun Yat-sen University;Adobe;National University of Singapore",
        "aff_unique_dep": ";Adobe Research;",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://research.adobe.com;https://www.nus.edu.sg",
        "aff_unique_abbr": "SYSU;Adobe;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2;0;2",
        "aff_country_unique": "China;United States;Singapore"
    },
    {
        "title": "Semantic Segmentation With Boundary Neural Fields",
        "session": "Semantic Image Segmentation",
        "status": "Poster",
        "track": "main",
        "pid": "62",
        "author_site": "Gedas Bertasius, Jianbo Shi, Lorenzo Torresani",
        "author": "Gedas Bertasius; Jianbo Shi; Lorenzo Torresani",
        "abstract": "The state-of-the-art in semantic segmentation is currently represented by fully convolutional networks (FCNs). However, FCNs use large receptive fields and many pooling layers, both of which cause blurring and low spatial resolution in the deep layers. As a result FCNs tend to produce segmentations that are poorly localized around object boundaries. Prior work has attempted to address this issue in post-processing steps, for example using a color-based CRF on top of the FCN predictions. However, these approaches require additional parameters and low-level features that are difficult to tune and integrate into the original network architecture. Additionally, most CRFs use color-based pixel affinities, which are not well suited for semantic segmentation and lead to spatially disjoint predictions.  To overcome these problems, we introduce a Boundary Neural Field (BNF), which is a global energy model integrating FCN predictions with boundary cues. The boundary information is used to enhance semantic segment coherence and to improve object localization. Specifically, we first show that the convolutional filters of semantic FCNs provide good features for boundary detection. We then employ the predicted boundaries to define pairwise potentials in our energy. Finally, we show that our energy decomposes semantic segmentation into multiple binary problems, which can be relaxed for efficient global optimization. We report extensive experiments demonstrating that minimization of our global boundary-based energy yields results superior to prior globalization methods, both quantitatively  as well as qualitatively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bertasius_Semantic_Segmentation_With_CVPR_2016_paper.pdf",
        "aff": "University of Pennsylvania; University of Pennsylvania; Dartmouth College",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 918305,
        "gs_citation": 260,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=834813032685295451&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "seas.upenn.edu;seas.upenn.edu;dartmouth.edu",
        "email": "seas.upenn.edu;seas.upenn.edu;dartmouth.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bertasius_Semantic_Segmentation_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Pennsylvania;Dartmouth College",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;https://www.dartmouth.edu",
        "aff_unique_abbr": "UPenn;Dartmouth",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Semi-Supervised Vocabulary-Informed Learning",
        "session": "Learning and CNN Architectures",
        "status": "Oral",
        "track": "main",
        "pid": "5",
        "author_site": "Yanwei Fu, Leonid Sigal",
        "author": "Yanwei Fu; Leonid Sigal",
        "abstract": "Despite significant progress in object categorization, in recent years, a number of important challenges remain; mainly, ability to learn from limited labeled data and ability to recognize object classes within large, potentially open, set of labels. Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, allowing former to inform the latter but not vice versa. We propose the notion of semi-supervised vocabulary-informed learning to alleviate the above mentioned challenges and address problems of supervised, zero-shot and open set recognition using a unified framework. Specifically, we propose a maximum margin framework for semantic manifold-based recognition that incorporates distance constraints from (both supervised and unsupervised) vocabulary atoms, ensuring that labeled samples are projected closest to their correct prototypes, in the embedding space, than to others. We show that resulting model shows improvements in supervised, zero-shot, and large open set recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Fu_Semi-Supervised_Vocabulary-Informed_Learning_CVPR_2016_paper.pdf",
        "aff": "Disney Research; Disney Research",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Fu_Semi-Supervised_Vocabulary-Informed_Learning_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3628293,
        "gs_citation": 169,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8804436794520846814&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "qmul.ac.uk;disneyresearch.com",
        "email": "qmul.ac.uk;disneyresearch.com",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Fu_Semi-Supervised_Vocabulary-Informed_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Disney Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://research.disney.com",
        "aff_unique_abbr": "Disney Research",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "SemiContour: A Semi-Supervised Learning Approach for Contour Detection",
        "session": "Edge Contour Detection",
        "status": "Poster",
        "track": "main",
        "pid": "27",
        "author_site": "Zizhao Zhang, Fuyong Xing, Xiaoshuang Shi, Lin Yang",
        "author": "Zizhao Zhang; Fuyong Xing; Xiaoshuang Shi; Lin Yang",
        "abstract": "Supervised contour detection methods usually require many labeled training images to obtain satisfactory performance. However, a large set of annotated data might be unavailable or extremely labor intensive. In this paper, we investigate the usage of semi-supervised learning (SSL) to obtain competitive detection accuracy with very limited training data (three labeled images). Specifically, we propose a semi-supervised structured ensemble learning approach for contour detection built on structured random forests (SRF). To allow SRF to be applicable to unlabeled data, we present an effective sparse representation approach to capture inherent structure in image patches by finding a compact and discriminative low-dimensional subspace representation in an unsupervised manner, enabling the incorporation of abundant unlabeled patches with their estimated structured labels to help SRF perform better node splitting. We re-examine the role of sparsity and propose a novel and fast sparse coding algorithm to boost the overall learning efficiency. To the best of our knowledge, this is the first attempt to apply SSL for contour detection. Extensive experiments on the BSDS500 segmentation dataset and the NYU Depth dataset demonstrate the superiority of the proposed method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_SemiContour_A_Semi-Supervised_CVPR_2016_paper.pdf",
        "aff": "University of Florida, Gainesville, FL 32611, USA; University of Florida, Gainesville, FL 32611, USA; University of Florida, Gainesville, FL 32611, USA; University of Florida, Gainesville, FL 32611, USA",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1349166,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8034284116496483739&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cise.ufl.edu;ufl.edu;ufl.edu;bme.ufl.edu",
        "email": "cise.ufl.edu;ufl.edu;ufl.edu;bme.ufl.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_SemiContour_A_Semi-Supervised_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Florida",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ufl.edu",
        "aff_unique_abbr": "UF",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Gainesville",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Seven Ways to Improve Example-Based Single Image Super Resolution",
        "session": "Deblurring and Super-Resolution",
        "status": "Poster",
        "track": "main",
        "pid": "40",
        "author_site": "Radu Timofte, Rasmus Rothe, Luc Van Gool",
        "author": "Radu Timofte; Rasmus Rothe; Luc Van Gool",
        "abstract": "In this paper we present seven techniques that everybody should know to improve example-based single image super resolution (SR): 1) augmentation of data, 2) use of large dictionaries with efficient search structures, 3) cascading, 4) image self-similarities, 5) back projection refinement, 6) enhanced prediction by consistency check, and 7) context reasoning. We validate our seven techniques on standard SR benchmarks (i.e. Set5, Set14, B100) and methods (i.e. A+, SRCNN, ANR, Zeyde, Yang) and achieve substantial improvements.The techniques are widely applicable and require no changes or only minor adjustments of the SR methods.  Moreover, our Improved A+ (IA) method sets new state-of-the-art results outperforming A+ by up to 0.9dB on average PSNR whilst maintaining a low time complexity.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Timofte_Seven_Ways_to_CVPR_2016_paper.pdf",
        "aff": "CVL, D-ITET, ETH Zurich; CVL, D-ITET, ETH Zurich; KU Leuven+ETH Zurich",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2613839,
        "gs_citation": 526,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1561986395495840770&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Timofte_Seven_Ways_to_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "ETH Zurich;Katholieke Universiteit Leuven",
        "aff_unique_dep": "D-ITET;",
        "aff_unique_url": "https://www.ethz.ch;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;KU Leuven",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "Switzerland;Belgium"
    },
    {
        "title": "Shallow and Deep Convolutional Networks for Saliency Prediction",
        "session": "Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "64",
        "author_site": "Junting Pan, Elisa Sayrol, Xavier Giro-i-Nieto, Kevin McGuinness, Noel E. O'Connor",
        "author": "Junting Pan; Elisa Sayrol; Xavier Giro-i-Nieto; Kevin McGuinness; Noel E. O'Connor",
        "abstract": "The prediction of salient areas in images has been traditionally addressed with hand-crafted features based on neuroscience principles. This paper, however, addresses the problem with a completely data-driven approach by training a convolutional neural network (convnet). The learning process is formulated as a minimization of a loss function that measures the Euclidean distance of the predicted saliency map with the provided ground truth. The recent publication of large datasets of saliency prediction has provided enough data to train end-to-end architectures that are both fast and accurate. Two designs are proposed: a shallow convnet trained from scratch, and a another deeper solution whose first three layers are adapted from another network trained for classification. To the authors knowledge, these are the first end-to-end CNNs trained and tested for the purpose of saliency prediction.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Shallow_and_Deep_CVPR_2016_paper.pdf",
        "aff": "Image Processing Group, Universitat Politecnica de Catalunya, Barcelona, Catalonia/Spain; Image Processing Group, Universitat Politecnica de Catalunya, Barcelona, Catalonia/Spain; Image Processing Group, Universitat Politecnica de Catalunya, Barcelona, Catalonia/Spain; Insight Center for Data Analytics, Dublin City University, Dublin, Ireland; Insight Center for Data Analytics, Dublin City University, Dublin, Ireland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 477317,
        "gs_citation": 587,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5385541179594468316&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "upc.edu; ; ;insight-centre.org; ",
        "email": "upc.edu; ; ;insight-centre.org; ",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Pan_Shallow_and_Deep_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Universitat Politecnica de Catalunya;Dublin City University",
        "aff_unique_dep": "Image Processing Group;Insight Center for Data Analytics",
        "aff_unique_url": "https://www.upc.edu;https://www.dcu.ie",
        "aff_unique_abbr": "UPC;DCU",
        "aff_campus_unique_index": "0;0;0;1;1",
        "aff_campus_unique": "Barcelona;Dublin",
        "aff_country_unique_index": "0;0;0;1;1",
        "aff_country_unique": "Spain;Ireland"
    },
    {
        "title": "Shape Analysis With Hyperbolic Wasserstein Distance",
        "session": "Shape Representations and Matching",
        "status": "Poster",
        "track": "main",
        "pid": "54",
        "author_site": "Jie Shi, Wen Zhang, Yalin Wang",
        "author": "Jie Shi; Wen Zhang; Yalin Wang",
        "abstract": "Shape space is an active research field in computer vision study. The shape distance defined in a shape space may provide a simple and refined index to represent a unique shape. Wasserstein distance defines a Riemannian metric for the Wasserstein space. It intrinsically measures the similarities between shapes and is robust to image noise. Thus it has the potential for the 3D shape indexing and classification research. While the algorithms for computing Wasserstein distance have been extensively studied, most of them only work for genus-0 surfaces. This paper proposes a novel framework to compute Wasserstein distance between general topological surfaces with hyperbolic metric. The computational algorithms are based on Ricci flow, hyperbolic harmonic map, and hyperbolic power Voronoi diagram and the method is general and robust. We apply our method to study human facial expression, longitudinal brain cortical morphometry with normal aging, and cortical shape classification in Alzheimer's disease (AD). Experimental results demonstrate that our method may be used as an effective shape index, which outperforms some other standard shape measures in our AD versus healthy control classification study.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shi_Shape_Analysis_With_CVPR_2016_paper.pdf",
        "aff": "School of Computing, Informatics, and Decision Systems Engineering, Arizona State University; School of Computing, Informatics, and Decision Systems Engineering, Arizona State University; School of Computing, Informatics, and Decision Systems Engineering, Arizona State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1078413,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1062439972393380640&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shi_Shape_Analysis_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "School of Computing, Informatics, and Decision Systems Engineering",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Shortlist Selection With Residual-Aware Distance Estimator for K-Nearest Neighbor Search",
        "session": "Image Indexing and Retrieval",
        "status": "Poster",
        "track": "main",
        "pid": "55",
        "author_site": "Jae-Pil Heo, Zhe Lin, Xiaohui Shen, Jonathan Brandt, Sung-eui Yoon",
        "author": "Jae-Pil Heo; Zhe Lin; Xiaohui Shen; Jonathan Brandt; Sung-eui Yoon",
        "abstract": "In this paper, we introduce a novel shortlist computation algorithm for approximate, high-dimensional nearest neighbor search. Our method relies on a novel distance estimator: the residual-aware distance estimator, that accounts for the residual distances of data points to their respective quantized centroids, and uses it for accurate shortlist computation.  Furthermore, we perform the residual-aware distance estimation with little additional memory and computational cost through simple pre-computation methods for inverted index and multi-index schemes. Because it modifies the initial shortlist collection phase, our new algorithm is applicable to most inverted indexing methods that use vector quantization. We have tested the proposed method with the inverted index and multi-index on a diverse set of benchmarks including up to one billion data points with varying dimensions, and found that our method robustly improves the accuracy of shortlists (up to 127% relatively higher) over the state-of-the-art techniques with a comparable or even faster computational cost.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Heo_Shortlist_Selection_With_CVPR_2016_paper.pdf",
        "aff": "KAIST; Adobe Research; Adobe Research; Adobe Research; KAIST",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2062291,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12630916701144803367&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Heo_Shortlist_Selection_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.kaist.ac.kr;https://research.adobe.com",
        "aff_unique_abbr": "KAIST;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "title": "Siamese Instance Search for Tracking",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "70",
        "author_site": "Ran Tao, Efstratios Gavves, Arnold W.M. Smeulders",
        "author": "Ran Tao; Efstratios Gavves; Arnold W.M. Smeulders",
        "abstract": "In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-the-art tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the most similar patch by a learned matching function. The strength of the matching function comes from being extensively trained generically, i.e., without any data of the target, using a Siamese deep neural network, which we design for tracking. Once learned, the matching function is used as is, without any adapting, to track previously unseen targets. It turns out that the learned matching function is so powerful that a simple tracker built upon it, coined Siamese INstance search Tracker, SINT, which only uses the original observation of the target from the first frame, suffices to reach state-of-the-art performance. Further, we show the proposed tracker even allows for target re-identification after the target was absent for a complete video shot.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tao_Siamese_Instance_Search_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Tao_Siamese_Instance_Search_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3435078,
        "gs_citation": 1421,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7171796048899667723&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tao_Siamese_Instance_Search_CVPR_2016_paper.html"
    },
    {
        "title": "Similarity Learning With Spatial Constraints for Person Re-Identification",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "54",
        "author_site": "Dapeng Chen, Zejian Yuan, Badong Chen, Nanning Zheng",
        "author": "Dapeng Chen; Zejian Yuan; Badong Chen; Nanning Zheng",
        "abstract": "Pose variation remains one of the major factors that adversely affect the accuracy of person re-identification. Such variation is not arbitrary as body parts (e.g. head, torso, legs) have relative stable spatial distribution. Breaking down the variability of global appearance regarding the spatial distribution potentially benefits the person matching. We therefore learn a novel similarity function, which consists of multiple sub-similarity measurements with each taking in charge of a subregion. In particular, we take advantage of the recently proposed polynomial feature map to describe the matching within each subregion, and inject all the feature maps into a unified framework. The framework not only  outputs similarity measurements for different regions, but also makes a better consistency among them. Our framework can collaborate local similarities as well as global similarity to exploit their complementary strength. It is flexible to incorporate multiple visual cues to further elevate the performance. In experiments, we analyze the effectiveness of the major components. The results on four datasets show significant and consistent improvements over the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chen_Similarity_Learning_With_CVPR_2016_paper.pdf",
        "aff": "Xi\u2019an Jiaotong University, China; Xi\u2019an Jiaotong University, China; Xi\u2019an Jiaotong University, China; Xi\u2019an Jiaotong University, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1393407,
        "gs_citation": 398,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7267345938055797604&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "foxmail.com;mail.xjtu.edu.cn;mail.xjtu.edu.cn;mail.xjtu.edu.cn",
        "email": "foxmail.com;mail.xjtu.edu.cn;mail.xjtu.edu.cn;mail.xjtu.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chen_Similarity_Learning_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Xi'an Jiao Tong University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://en.xjtu.edu.cn/",
        "aff_unique_abbr": "XJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Similarity Metric For Curved Shapes In Euclidean Space",
        "session": "Shape Representations and Matching",
        "status": "Poster",
        "track": "main",
        "pid": "53",
        "author_site": "Girum G. Demisse, Djamila Aouada, Bj\u00f6rn Ottersten",
        "author": "Girum G. Demisse; Djamila Aouada; Bjorn Ottersten",
        "abstract": "In this paper, we introduce a similarity metric for curved shapes that can be described, distinctively, by ordered points. The proposed method represents a given curve as a point in the deformation space, the direct product of rigid transformation matrices, such that the  successive action of the matrices on a fixed starting point reconstructs the full curve. In general, both open and closed curves are represented in the deformation space modulo shape orientation and orientation preserving diffeomorphisms. The use of direct product Lie groups to represent curved shapes led to an explicit formula for geodesic curves and the formulation of a similarity metric between shapes by the L2-norm on the Lie algebra. Additionally, invariance to reparametrization or estimation of point correspondence between shapes is performed as an intermediate step for computing geodesics. Furthermore, since there is no computation of differential quantities on the curves, our representation is more robust to local perturbations and needs no pre-smoothing. We compare our method with the elastic shape metric defined through the square root velocity (SRV) mapping, and other shape matching approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Demisse_Similarity_Metric_For_CVPR_2016_paper.pdf",
        "aff": "Interdisciplinary Center for Security, Reliability and Trust; Interdisciplinary Center for Security, Reliability and Trust; Interdisciplinary Center for Security, Reliability and Trust",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2065653,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18045031655341422529&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": "uni.lu;uni.lu;uni.lu",
        "email": "uni.lu;uni.lu;uni.lu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Demisse_Similarity_Metric_For_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Interdisciplinary Center for Security, Reliability and Trust",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wwwen.uni.lu/icstrt",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Luxembourg"
    },
    {
        "title": "Simultaneous Clustering and Model Selection for Tensor Affinities",
        "session": "Learning and Optimization",
        "status": "Spotlight",
        "track": "main",
        "pid": "6",
        "author_site": "Zhuwen Li, Shuoguang Yang, Loong-Fah Cheong, Kim-Chuan Toh",
        "author": "Zhuwen Li; Shuoguang Yang; Loong-Fah Cheong; Kim-Chuan Toh",
        "abstract": "Estimating the number of clusters remains a difficult model selection problem. We consider this problem in the domain where the affinity relations involve groups of more than two nodes. Building on the previous formulation for the pairwise affinity case, we exploit the mathematical structures in the higher order case. We express the original minimal-rank and positive semi-definite (PSD) constraints in a form amenable for numerical implementation, as the original constraints are either intractable or even undefined in general in the higher order case. To scale to large problem sizes, we also propose an alternative formulation, so that it can be efficiently solved via stochastic optimization in an online fashion. We evaluate our algorithm with different applications to demonstrate its superiority, and show it can adapt to varying levels of unbalancedness of clusters.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Simultaneous_Clustering_and_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Li_Simultaneous_Clustering_and_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 574265,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3875170599929247366&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Simultaneous_Clustering_and_CVPR_2016_paper.html"
    },
    {
        "title": "Simultaneous Estimation of Near IR BRDF and Fine-Scale Surface Geometry",
        "session": "Computational Photography and Biomedical Applications",
        "status": "Spotlight",
        "track": "main",
        "pid": "21",
        "author_site": "Gyeongmin Choe, Srinivasa G. Narasimhan, In So Kweon",
        "author": "Gyeongmin Choe; Srinivasa G. Narasimhan; In So Kweon",
        "abstract": "Near-Infrared (NIR) images of most materials exhibit less texture or albedo variations making them beneficial for vision tasks such as intrinsic image decomposition and structured light depth estimation. Understanding the reflectance properties (BRDF) of materials in the NIR wavelength range can be further useful for many photometric methods including shape from shading and inverse rendering. However, even with less albedo variation, many materials e.g. fabrics, leaves, etc. exhibit complex fine-scale surface detail making it hard to accurately estimate BRDF. In this paper, we present an approach to simultaneously estimate NIR BRDF and fine-scale surface details by imaging materials under different IR lighting and viewing directions. This is achieved by an iterative scheme that alternately estimates surface detail and NIR BRDF of materials. Our setup does not require complicated gantries or calibration and we present the first NIR dataset of 100 materials including a variety of fabrics (knits, weaves, cotton, satin, leather), and organic (skin, leaves, jute, trunk, fur) and inorganic materials (plastic, concrete, carpet). The NIR BRDFs measured from material samples are used with a shape-from-shading algorithm to demonstrate fine-scale reconstruction of objects from a single NIR image.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Choe_Simultaneous_Estimation_of_CVPR_2016_paper.pdf",
        "aff": "Korea Advanced Institute of Science and Technology (KAIST)1; Carnegie Mellon University (CMU)2; Korea Advanced Institute of Science and Technology (KAIST)1",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 12495155,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12975955982102927985&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "rcv.kaist.ac.kr;cs.cmu.edu;kaist.ac.kr",
        "email": "rcv.kaist.ac.kr;cs.cmu.edu;kaist.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Choe_Simultaneous_Estimation_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.cmu.edu",
        "aff_unique_abbr": "KAIST;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "title": "Simultaneous Optical Flow and Intensity Estimation From an Event Camera",
        "session": "Vision With Alternative Sensors",
        "status": "Oral",
        "track": "main",
        "pid": "14",
        "author_site": "Patrick Bardow, Andrew J. Davison, Stefan Leutenegger",
        "author": "Patrick Bardow; Andrew J. Davison; Stefan Leutenegger",
        "abstract": "Event cameras are bio-inspired vision sensors which mimic retinas to measure per-pixel intensity change rather than outputting an actual intensity image. This proposed paradigm shift away from traditional frame cameras offers significant potential advantages: namely avoiding high data rates, dynamic range limitations and motion blur. Unfortunately, however, established computer vision algorithms may not at all be applied directly to event cameras.  Methods proposed so far to reconstruct images, estimate optical flow, track a camera and reconstruct a scene come with severe restrictions on the environment or on the motion of the camera, e.g. allowing only rotation. Here, we propose, to the best of our knowledge, the first algorithm to simultaneously recover the motion field and brightness image, while the camera undergoes a generic motion through any scene. Our approach employs minimisation of a cost function that contains the asynchronous event data as well as spatial and temporal regularisation within a sliding window time interval. Our implementation relies on GPU-based optimisation and runs in near real-time. In a series of examples, we demonstrate the successful operation of our framework, including in situations where conventional cameras heavily suffer from dynamic range limitations or motion blur.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bardow_Simultaneous_Optical_Flow_CVPR_2016_paper.pdf",
        "aff": "Dyson Robotics Laboratory at Imperial College, Dept. of Computing, Imperial College London, UK; Dyson Robotics Laboratory at Imperial College, Dept. of Computing, Imperial College London, UK; Dyson Robotics Laboratory at Imperial College, Dept. of Computing, Imperial College London, UK",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 975910,
        "gs_citation": 377,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17672702656636727461&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "email": "imperial.ac.uk;imperial.ac.uk;imperial.ac.uk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bardow_Simultaneous_Optical_Flow_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Imperial College London",
        "aff_unique_dep": "Dept. of Computing",
        "aff_unique_url": "https://www.imperial.ac.uk",
        "aff_unique_abbr": "ICL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Single Image Camera Calibration With Lenticular Arrays for Augmented Reality",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "28",
        "author_site": "Ian Schillebeeckx, Robert Pless",
        "author": "Ian Schillebeeckx; Robert Pless",
        "abstract": "We consider the problem of camera pose estimation for a scenario where the camera may have continuous and unknown changes in its focal length.  Understanding frame by frame changes in camera focal length is vital to accurately estimating camera pose and vital to accurately render virtual objects in a scene with the correct perspective. However, most approaches to camera calibration require geometric constraints from many frames or the observation of a 3D calibration object --- both of which may not be feasible in augmented reality settings. This paper introduces a calibration objects based on a flat lenticular array that creates a color coded light-field whose observed color changes depending on the angle from which it is viewed.  We derive an approach to estimate the focal length of the camera and the relative pose of an object from a single image.  We characterize the performance of camera calibration across various focal lengths and camera models, and we demonstrate the advantages of the focal length estimation in rendering a virtual object in a video with constant zooming.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Schillebeeckx_Single_Image_Camera_CVPR_2016_paper.pdf",
        "aff": "Washington University in St. Louis; Washington University in St. Louis",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Schillebeeckx_Single_Image_Camera_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 601010,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14119560661039117571&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "wustl.edu;wustl.edu",
        "email": "wustl.edu;wustl.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Schillebeeckx_Single_Image_Camera_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Washington University in St. Louis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://wustl.edu",
        "aff_unique_abbr": "WashU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "St. Louis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Single Image Object Modeling Based on BRDF and R-Surfaces Learning",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "67",
        "author_site": "Fabrizio Natola, Valsamis Ntouskos, Fiora Pirri, Marta Sanzari",
        "author": "Fabrizio Natola; Valsamis Ntouskos; Fiora Pirri; Marta Sanzari",
        "abstract": "A methodology for 3D surface modeling from a single image is proposed.  The principal novelty is concave and specular surface modeling without any externally imposed prior. The main idea of the method is to use BRDFs and generated rendered surfaces, to transfer the normal field, computed for the generated samples, to the unknown surface.  The transferred information is adequate to blow and sculpt the segmented image mask in to a bas-relief of the object. The object surface is further refined basing on a  photo-consistency formulation that relates for error minimization the original image and the modeled object.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Natola_Single_Image_Object_CVPR_2016_paper.pdf",
        "aff": "ALCOR Lab, DIAG, Sapienza University of Rome; ALCOR Lab, DIAG, Sapienza University of Rome; ALCOR Lab, DIAG, Sapienza University of Rome; ALCOR Lab, DIAG, Sapienza University of Rome",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Natola_Single_Image_Object_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 4149317,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15288942828095109455&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "diag.uniroma1.it;diag.uniroma1.it;diag.uniroma1.it;diag.uniroma1.it",
        "email": "diag.uniroma1.it;diag.uniroma1.it;diag.uniroma1.it;diag.uniroma1.it",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Natola_Single_Image_Object_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Sapienza University of Rome",
        "aff_unique_dep": "ALCOR Lab, DIAG",
        "aff_unique_url": "https://www.sapienza.uniroma.it",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "title": "Single-Image Crowd Counting via Multi-Column Convolutional Neural Network",
        "session": "Scene Understanding",
        "status": "Poster",
        "track": "main",
        "pid": "63",
        "author_site": "Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, Yi Ma",
        "author": "Yingying Zhang; Desen Zhou; Siqin Chen; Shenghua Gao; Yi Ma",
        "abstract": "This paper aims to develop a method that can accurately estimate the crowd count from an individual image with arbitrary crowd density and arbitrary perspective. To this end,we have proposed a simple but effective Multi-column Convolutional Neural Network (MCNN) architecture to map the image to its crowd density map. The proposed MCNN allows the input image to be of arbitrary size or resolution. By utilizing filters with receptive fields of different sizes, the features learned by each column CNN are adaptive to variations in people/head size due to perspective effect or image resolution. Furthermore, the true density map is computed accurately based on geometry-adaptive kernels which do not need knowing the perspective map of the input image. Since exiting crowd counting datasets do not adequately cover all the challenging situations considered in our work, we have collected and labelled a large new dataset that includes 1198 images with about 330,000 heads annotated. On this challenging new dataset, as well as all existing datasets, we conduct extensive experiments to verify the effectiveness of the proposed model and method. In particular, with the proposed simple MCNN model, our method outperforms all existing methods. In addition, experiments show that our model, once trained on one dataset, can be readily transferred to a new dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Single-Image_Crowd_Counting_CVPR_2016_paper.pdf",
        "aff": "Shanghaitech University; Shanghaitech University; Shanghaitech University; Shanghaitech University; Shanghaitech University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1906962,
        "gs_citation": 2492,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14001575375343241953&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "shanghaitech.edu.cn;shanghaitech.edu.cn;shanghaitech.edu.cn;shanghaitech.edu.cn;shanghaitech.edu.cn",
        "email": "shanghaitech.edu.cn;shanghaitech.edu.cn;shanghaitech.edu.cn;shanghaitech.edu.cn;shanghaitech.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Single-Image_Crowd_Counting_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "ShanghaiTech University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.shanghaitech.edu.cn",
        "aff_unique_abbr": "ShanghaiTech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Situation Recognition: Visual Semantic Role Labeling for Image Understanding",
        "session": "Face, Gesture, & Situation Recognition: Algorithms and Datasets",
        "status": "Oral",
        "track": "main",
        "pid": "26",
        "author_site": "Mark Yatskar, Luke Zettlemoyer, Ali Farhadi",
        "author": "Mark Yatskar; Luke Zettlemoyer; Ali Farhadi",
        "abstract": "This paper introduces situation recognition, the problem of producing a concise summary of the situation an image depicts including: (1) the main activity (e.g., clipping), (2) the participating actors, objects, substances, and locations (e.g., man, shears, sheep, wool, and field) and most importantly (3) the roles these participants play in the activity (e.g., the man is clipping, the shears are his tool, the wool is being clipped from the sheep, and the clipping is in a field). We use FrameNet, a verb and role lexicon developed by linguists, to define a large space of possible situations and collect a large-scale dataset containing over 500 activities, 1,700 roles, 11,000 objects, 125,000 images, and 200,000 unique situations. We also introduce structured prediction baselines and show that, in activity-centric images, situation-driven prediction of objects and activities  outperforms independent object and activity recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yatskar_Situation_Recognition_Visual_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2500617,
        "gs_citation": 321,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14769542088507071062&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yatskar_Situation_Recognition_Visual_CVPR_2016_paper.html"
    },
    {
        "title": "Sketch Me That Shoe",
        "session": "Object Recognition and Detection",
        "status": "Oral",
        "track": "main",
        "pid": "5",
        "author_site": "Qian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales, Chen-Change Loy",
        "author": "Qian Yu; Feng Liu; Yi-Zhe Song; Tao Xiang; Timothy M. Hospedales; Chen-Change Loy",
        "abstract": "We investigate the problem of fine-grained sketch-based image retrieval (SBIR), where free-hand human sketches are used as queries to perform instance-level retrieval of images. This is an extremely challenging task because (i) visual comparisons not only need to be fine-grained but also executed cross-domain, (ii) free-hand (finger) sketches are highly abstract, making fine-grained matching harder, and most importantly (iii) annotated cross-domain sketch-photo datasets required for training are scarce, challenging many state-of-the-art machine learning techniques.     In this paper, for the first time, we address all these challenges, providing a step towards the capabilities that would underpin a commercial sketch-based image retrieval application. We introduce a new database of 1,432 sketch-photo pairs from two categories with 32,000 fine-grained triplet ranking annotations. We then develop a deep triplet-ranking model for instance-level SBIR with a novel data augmentation and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data. Extensive experiments are carried out to contribute a variety of insights into the challenges of data sufficiency and over-fitting avoidance when training deep networks for fine-grained cross-domain ranking tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yu_Sketch_Me_That_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 540,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9932742786237910509&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yu_Sketch_Me_That_CVPR_2016_paper.html"
    },
    {
        "title": "SketchNet: Sketch Classification With Web Images",
        "session": "Fine Grained Categorization",
        "status": "Poster",
        "track": "main",
        "pid": "37",
        "author_site": "Hua Zhang, Si Liu, Changqing Zhang, Wenqi Ren, Rui Wang, Xiaochun Cao",
        "author": "Hua Zhang; Si Liu; Changqing Zhang; Wenqi Ren; Rui Wang; Xiaochun Cao",
        "abstract": "In this study, we present a weakly supervised approach that discovers the discriminative structures of sketch images, given pairs of sketch images and web images. In contrast to traditional approaches that use global appearance features or relay on keypoint features, our aim is to automatically learn the shared latent structures that exist between sketch images and real images, even when there are  significant appearance differences across its relevant real images. To accomplish this, we propose a deep convolutional neural network, named SketchNet. We firstly develop a triplet composed of sketch, positive and negative real image as the input of our neural network. To discover the coherent visual structures between the sketch and its positive pairs, we introduce the softmax as the loss function. Then a ranking mechanism is introduced to make the positive pairs obtain a higher score comparing over negative ones to achieve robust representation. Finally, we formalize above-mentioned constrains into the unified objective function, and create an ensemble feature representation to describe the sketch images. Experiments on the TU-Berlin sketch benchmark demonstrate the effectiveness of our model and show that deep feature representation brings substantial improvements over other state-of-the-art methods on sketch classification.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_SketchNet_Sketch_Classification_CVPR_2016_paper.pdf",
        "aff": "State Key Laboratory of Information Security (SKLOIS), Institute of Information Engineering, Chinese Academy of Sciences; State Key Laboratory of Information Security (SKLOIS), Institute of Information Engineering, Chinese Academy of Sciences; School of Computer Science and Technology, Tianjin University; School of Computer Science and Technology, Tianjin University; State Key Laboratory of Information Security (SKLOIS), Institute of Information Engineering, Chinese Academy of Sciences; State Key Laboratory of Information Security (SKLOIS), Institute of Information Engineering, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2799920,
        "gs_citation": 192,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10284801534555120059&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "iie.ac.cn;iie.ac.cn;tju.edu.cn;gmail.com;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;tju.edu.cn;gmail.com;iie.ac.cn;iie.ac.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_SketchNet_Sketch_Classification_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;1;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;Tianjin University",
        "aff_unique_dep": "State Key Laboratory of Information Security (SKLOIS);School of Computer Science and Technology",
        "aff_unique_url": "http://www.iiis.cas.cn;http://www.tju.edu.cn",
        "aff_unique_abbr": "CAS;Tianjin University",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Sliced Wasserstein Kernels for Probability Distributions",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "76",
        "author_site": "Soheil Kolouri, Yang Zou, Gustavo K. Rohde",
        "author": "Soheil Kolouri; Yang Zou; Gustavo K. Rohde",
        "abstract": "Optimal transport distances, otherwise known as Wasserstein distances, have recently drawn ample attention in computer vision and machine learning as powerful discrepancy measures for probability distributions. The recent developments on alternative formulations of the optimal transport have allowed for faster solutions to the problem and have revamped their practical applications in machine learning. In this paper, we exploit the widely used kernel methods and provide a family of provably positive definite kernels based on the Sliced Wasserstein distance and demonstrate the benefits of these kernels in a variety of learning tasks. Our work provides a new perspective on the application of optimal transport flavored distances through kernel methods in machine learning tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kolouri_Sliced_Wasserstein_Kernels_CVPR_2016_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4664954,
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13551647068634041927&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;cmu.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kolouri_Sliced_Wasserstein_Kernels_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Slicing Convolutional Neural Network for Crowd Video Understanding",
        "session": "People and Faces",
        "status": "Spotlight",
        "track": "main",
        "pid": "35",
        "author_site": "Jing Shao, Chen-Change Loy, Kai Kang, Xiaogang Wang",
        "author": "Jing Shao; Chen-Change Loy; Kai Kang; Xiaogang Wang",
        "abstract": "Learning and capturing both appearance and dynamic representations are pivotal for crowd video understanding. Convolutional Neural Networks (CNNs) have shown its remarkable potential in learning appearance representations from images. However, the learning of dynamic representation, and how it can be effectively combined with appearance features for video analysis, remains an open problem. In this study, we propose a novel spatio-temporal CNN, named Slicing CNN (S-CNN), based on the decomposition of 3D feature maps into 2D spatio- and 2D temporal-slices representations. The decomposition brings unique advantages: (1) the model is capable of capturing dynamics of different semantic units such as groups and objects, (2) it learns separated appearance and dynamic representations while keeping proper interactions between them, and (3) it exploits the selectiveness of spatial filters to discard irrelevant background clutter for crowd understanding. We demonstrate the effectiveness of the proposed S-CNN model on the WWW crowd video dataset for attribute recognition and observe significant performance improvements to the state-of-the-art methods (62.55% from 51.84% [21]).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shao_Slicing_Convolutional_Neural_CVPR_2016_paper.pdf",
        "aff": "Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1702224,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2124755833605126034&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "ee.cuhk.edu.hk;ie.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ie.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shao_Slicing_Convolutional_Neural_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Slow and Steady Feature Analysis: Higher Order Temporal Coherence in Video",
        "session": "Video Analysis 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "7",
        "author_site": "Dinesh Jayaraman, Kristen Grauman",
        "author": "Dinesh Jayaraman; Kristen Grauman",
        "abstract": "How can unlabeled video augment visual learning? Existing methods perform \"slow\" feature analysis, encouraging temporal coherence, where the image representations of temporally close frames to exhibit only small differences.  While this standard approach captures the fact that high-level visual signals change slowly over time, it fails to capture *how* the visual content changes.  We propose to generalize slow feature analysis to \"steady\" feature analysis.  The key idea is to impose a prior that higher order derivatives in the learned feature space must be small.  To this end, we train a convolutional neural network with a regularizer that minimizes a contrastive loss on tuples of sequential frames from unlabeled video.  Focusing on the case of triplets of frames, the proposed method encourages that feature changes over time should be smooth, i.e., similar to the most recent changes.  Using five diverse image and video datasets, including unlabeled YouTube and KITTI videos, we demonstrate our method's impact on object recognition, scene classification, and action recognition tasks.  We further show that our features learned from unlabeled video can even surpass a standard heavily supervised pretraining approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Jayaraman_Slow_and_Steady_CVPR_2016_paper.pdf",
        "aff": "UT Austin; UT Austin",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 799182,
        "gs_citation": 176,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4267798624110044991&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "cs.utexas.edu;cs.utexas.edu",
        "email": "cs.utexas.edu;cs.utexas.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Jayaraman_Slow_and_Steady_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Social LSTM: Human Trajectory Prediction in Crowded Spaces",
        "session": "Video Analysis 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "22",
        "author_site": "Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, Silvio Savarese",
        "author": "Alexandre Alahi; Kratarth Goel; Vignesh Ramanathan; Alexandre Robicquet; Li Fei-Fei; Silvio Savarese",
        "abstract": "Humans navigate complex crowded environments based on social conventions: they respect personal space, yielding right-of-way and avoid collisions. In our work, we propose a data-driven approach to learn these human-human interactions for predicting their future trajectories. This is in contrast to traditional approaches which use hand-crafted functions such as Social forces. We present a new Long Short-Term Memory (LSTM) model which  jointly reasons across multiple individuals in a scene. Different from the conventional LSTM, we share the information between multiple LSTMs through a new pooling layer. This layer pools the hidden representation from LSTMs corresponding to neighboring trajectories to capture interactions within this neighborhood. We demonstrate the performance of our method on several public datasets. Our model outperforms previous forecasting methods by more than 42% . We also analyze the trajectories predicted by our model to demonstrate social behaviours such as collision avoidance and group movement, learned by our model.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Alahi_Social_LSTM_Human_CVPR_2016_paper.pdf",
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University; Stanford University; Stanford University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1058799,
        "gs_citation": 4143,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=814983128189795991&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "email": "cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu;cs.stanford.edu",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Alahi_Social_LSTM_Human_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Soft-Segmentation Guided Object Motion Deblurring",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "49",
        "author_site": "Jinshan Pan, Zhe Hu, Zhixun Su, Hsin-Ying Lee, Ming-Hsuan Yang",
        "author": "Jinshan Pan; Zhe Hu; Zhixun Su; Hsin-Ying Lee; Ming-Hsuan Yang",
        "abstract": "Object motion blur is a challenging problem as the foreground and the background in the scenes undergo different types of image degradation due to movements in various directions and speed. Most object motion deblurring methods address this problem by segmenting blurred images into regions where different kernels are estimated and applied for restoration. Segmentation on blurred images is difficult due to ambiguous pixels between regions, but it plays an important role for object motion deblurring. To address these problems, we propose a novel model for object motion deblurring. The proposed model is developed based on a maximum a posterior formulation in which soft-segmentation is incorporated for object layer estimation. We propose an efficient algorithm to jointly estimate object segmentation and camera motion where each layer can be deblurred well under the guidance of the soft-segmentation. Experimental results demonstrate that the proposed algorithm performs favorably against the state-of-the-art object motion deblurring methods on challenging scenarios.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Soft-Segmentation_Guided_Object_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "http://vllab.ucmerced.edu/~jinshan/projects/object-deblur/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2326954,
        "gs_citation": 122,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5005267167633729997&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Pan_Soft-Segmentation_Guided_Object_CVPR_2016_paper.html"
    },
    {
        "title": "Solving Small-Piece Jigsaw Puzzles by Growing Consensus",
        "session": "Feature Matching and Indexing",
        "status": "Poster",
        "track": "main",
        "pid": "46",
        "author_site": "Kilho Son, daniel Moreno, James Hays, David B. Cooper",
        "author": "Kilho Son; daniel Moreno; James Hays; David B. Cooper",
        "abstract": "In this paper, we present a novel computational puzzle solver for square-piece image jigsaw puzzles with no prior information such as piece orientation, anchor pieces or resulting dimension of the puzzle. By \"piece\" we mean a square dxd block of pixels, where we investigate pieces as small as 7x7 pixels. To reconstruct such challenging puzzles, we aim to search for piece configurations which maximize the size of consensus (i.e. grid or loop) configurations which represent a geometric consensus or agreement among pieces. Pieces are considered for addition to the existing assemblies if these pieces increase the size of the consensus configurations. In contrast to previous puzzle solvers which goal for assemblies maximizing compatibility measures between all pairs of pieces and thus depend heavily on the pairwise compatibility measure used, our new approach reduces the dependency on the pairwise compatibility measures which become increasingly uninformative at small scales and instead exploits geometric agreement among pieces. Our contribution also includes an improved pairwise compatibility measure which exploits directional derivative information along adjoining boundaries of the pieces. For the challenging unknown orientation piece puzzles where the size of pieces is small, we reduce assembly error by up to 75% compared with previous algorithms for standard datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Son_Solving_Small-Piece_Jigsaw_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Son_Solving_Small-Piece_Jigsaw_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15088276940177784822&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Son_Solving_Small-Piece_Jigsaw_CVPR_2016_paper.html"
    },
    {
        "title": "Solving Temporal Puzzles",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "64",
        "author_site": "Caglayan Dicle, Burak Yilmaz, Octavia Camps, Mario Sznaier",
        "author": "Caglayan Dicle; Burak Yilmaz; Octavia Camps; Mario Sznaier",
        "abstract": "Many physical phenomena, within short time windows, can be explained by low order differential relations. In a discrete world, these relations can be described using low order difference equations or equivalently low order auto regressive (AR) models. In this paper, based on this intuition, we propose an algorithm for solving time-sort temporal puzzles, defined as scrambled time series that need to be sorted out. We frame this highly combinatorial problem using a mixed-integer semi definite programming formulation and show how to turn it into a mixed-integer linear programming problem by using the recently introduced atomic norm framework. Our experiments show the effectiveness and generality of our approach in different scenarios.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Dicle_Solving_Temporal_Puzzles_CVPR_2016_paper.pdf",
        "aff": "Dept of Electrical and Computer Engineering, Northeastern University; Dept of Electrical and Computer Engineering, Northeastern University; Dept of Electrical and Computer Engineering, Northeastern University; Dept of Electrical and Computer Engineering, Northeastern University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Dicle_Solving_Temporal_Puzzles_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 808113,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14400639891229615271&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "coe.neu.edu;gmail.com;coe.neu.edu;coe.neu.edu",
        "email": "coe.neu.edu;gmail.com;coe.neu.edu;coe.neu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Dicle_Solving_Temporal_Puzzles_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "Dept of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.northeastern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Some Like It Hot - Visual Guidance for Preference Prediction",
        "session": "People and Faces",
        "status": "Spotlight",
        "track": "main",
        "pid": "28",
        "author_site": "Rasmus Rothe, Radu Timofte, Luc Van Gool",
        "author": "Rasmus Rothe; Radu Timofte; Luc Van Gool",
        "abstract": "For people first impressions of someone are of determining importance. They are hard to alter through further information. This begs the question if a computer can reach the same judgement. Earlier research has already pointed out that age, gender, and average attractiveness can be estimated with reasonable precision. We improve the state-of-the-art, but also predict - based on someone's known preferences - how much that particular person is attracted to a novel face. Our computational pipeline comprises a face detector, convolutional neural networks for the extraction of deep features, standard support vector regression for gender, age and facial beauty, and - as the main novelties - visual regularized collaborative filtering to infer inter-person preferences as well as a novel regression technique for handling visual queries without rating history. We validate the method using a very large dataset from a dating site as well as images from celebrities. Our experiments yield convincing results, i.e. we predict 76% of the ratings correctly solely based on an image, and reveal some sociologically relevant conclusions. We also validate our collaborative filtering solution on the standard MovieLens rating dataset, augmented with movie posters, to predict an individual's movie rating. We demonstrate our algorithms on howhot.io which went viral around the Internet with more than 50 million pictures evaluated in the first month.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Rothe_Some_Like_It_CVPR_2016_paper.pdf",
        "aff": "CVL, ETH Zurich; CVL, ETH Zurich; KU Leuven + ETH Zurich",
        "project": "http://howhot.io",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1951878,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10480127312959606609&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff_domain": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "email": "vision.ee.ethz.ch;vision.ee.ethz.ch;vision.ee.ethz.ch",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Rothe_Some_Like_It_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1+0",
        "aff_unique_norm": "ETH Zurich;Katholieke Universiteit Leuven",
        "aff_unique_dep": "Computer Vision Laboratory;",
        "aff_unique_url": "https://www.ethz.ch;https://www.kuleuven.be",
        "aff_unique_abbr": "ETHZ;KU Leuven",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1+0",
        "aff_country_unique": "Switzerland;Belgium"
    },
    {
        "title": "Sparse Coding and Dictionary Learning With Linear Dynamical Systems",
        "session": "Grouping and Optimization Methods",
        "status": "Oral",
        "track": "main",
        "pid": "16",
        "author_site": "Wenbing Huang, Fuchun Sun, Lele Cao, Deli Zhao, Huaping Liu, Mehrtash Harandi",
        "author": "Wenbing Huang; Fuchun Sun; Lele Cao; Deli Zhao; Huaping Liu; Mehrtash Harandi",
        "abstract": "Linear Dynamical Systems (LDSs) are the fundamental tools for encoding spatio-temporal data in various disciplines. To enhance the performance of LDSs, in this paper, we address the challenging issue of performing sparse coding on the space of LDSs, where both data and dictionary atoms are LDSs. Rather than approximate the extended observability with a finite-order matrix, we represent the space of LDSs by an infinite Grassmannian consisting of the orthonormalized extended observability subspaces. Via a homeomorphic mapping, such Grassmannian is embedded into the space of symmetric matrices, where a tractable objective function can be derived for sparse coding. Then, we propose an efficient method to learn the system parameters of the dictionary atoms explicitly, by imposing the symmetric constraint to the transition matrices of the data and dictionary systems. Moreover, we combine the state covariance into the algorithm formulation, thus further promoting the performance of the models with symmetric transition matrices. Comparative experimental evaluations reveal the superior performance of proposed methods on various tasks including video classification and tactile recognition.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Sparse_Coding_and_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science and Technology, Tsinghua University, State Key Lab. of Intelligent Technology and Systems, Tsinghua National Lab. for Information Science and Technology (TNList); Department of Computer Science and Technology, Tsinghua University, State Key Lab. of Intelligent Technology and Systems, Tsinghua National Lab. for Information Science and Technology (TNList); Department of Computer Science and Technology, Tsinghua University, State Key Lab. of Intelligent Technology and Systems, Tsinghua National Lab. for Information Science and Technology (TNList); Australian National University & NICTA, Australia; Department of Computer Science and Technology, Tsinghua University, State Key Lab. of Intelligent Technology and Systems, Tsinghua National Lab. for Information Science and Technology (TNList); Australian National University & NICTA, Australia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Huang_Sparse_Coding_and_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 991792,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4539978688183734846&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;mails.tsinghua.edu.cn;gmail.com;mail.tsinghua.edu.cn;nicta.com.au",
        "email": "mails.tsinghua.edu.cn;mail.tsinghua.edu.cn;mails.tsinghua.edu.cn;gmail.com;mail.tsinghua.edu.cn;nicta.com.au",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Huang_Sparse_Coding_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;0;1",
        "aff_unique_norm": "Tsinghua University;Australian National University",
        "aff_unique_dep": "Department of Computer Science and Technology;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.anu.edu.au",
        "aff_unique_abbr": "THU;ANU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;1",
        "aff_country_unique": "China;Australia"
    },
    {
        "title": "Sparse Coding for Classification via Discrimination Ensemble",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "58",
        "author_site": "Yuhui Quan, Yong Xu, Yuping Sun, Yan Huang, Hui Ji",
        "author": "Yuhui Quan; Yong Xu; Yuping Sun; Yan Huang; Hui Ji",
        "abstract": "Discriminative sparse coding has emerged as a promising technique in image analysis and recognition, which couples the process of classifier training and the process of dictionary learning for improving the discriminability of sparse codes.  Many existing approaches consider only a simple single linear classifier whose discriminative power is rather weak. In this paper, we proposed a discriminative sparse coding method which jointly learns a dictionary for sparse coding and an ensemble classifier for discrimination. The ensemble classifier is composed of a set of linear predictors and constructed via both subsampling on data and subspace projection on sparse codes. The advantages of the proposed method over the existing ones are multi-fold: better discriminability of sparse codes, weaker dependence on peculiarities of training data, and more expressibility of classifier for classification. These advantages are also justified in the experiments, as our method outperformed several state-of-the-art methods in several recognition tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Quan_Sparse_Coding_for_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2363125,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16518654430775931052&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Quan_Sparse_Coding_for_CVPR_2016_paper.html"
    },
    {
        "title": "Sparse Coding for Third-Order Super-Symmetric Tensor Descriptors With Application to Texture Recognition",
        "session": "Learning and Optimization",
        "status": "Spotlight",
        "track": "main",
        "pid": "11",
        "author_site": "Piotr Koniusz, Anoop Cherian",
        "author": "Piotr Koniusz; Anoop Cherian",
        "abstract": "Super-symmetric tensors - a higher-order extension of scatter matrices - are becoming increasingly popular in machine learning and computer vision for modeling data statistics, co-occurrences, or even as visual descriptors. They were shown recently to outperform second-order approaches, however, the size of these tensors are exponential in the data dimensionality, which is a significant concern. In this paper, we study third-order super-symmetric tensor descriptors in the context of dictionary learning and sparse coding. For this purpose, we propose a novel non-linear third-order texture descriptor. Our goal is to approximate these tensors as sparse conic combinations of atoms from a learned dictionary. Apart from the significant benefits to tensor compression that this framework offers, our experiments demonstrate that the sparse coefficients produced by this scheme lead to better aggregation of high-dimensional data and showcase superior performance on two common computer vision tasks compared to the state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Koniusz_Sparse_Coding_for_CVPR_2016_paper.pdf",
        "aff": "National ICT Australia (NICTA), Canberra Research Laboratory; ARC Centre of Excellence for Robotic Vision, Australian National University, Canberra",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 916947,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11903138135607025425&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "nicta.com.au;anu.edu.au",
        "email": "nicta.com.au;anu.edu.au",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Koniusz_Sparse_Coding_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "National ICT Australia;Australian National University",
        "aff_unique_dep": ";ARC Centre of Excellence for Robotic Vision",
        "aff_unique_url": "https://www.nicta.com.au;https://www.anu.edu.au",
        "aff_unique_abbr": "NICTA;ANU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Canberra",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "Sparse to Dense 3D Reconstruction From Rolling Shutter Images",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "33",
        "author_site": "Olivier Saurer, Marc Pollefeys, Gim Hee Lee",
        "author": "Olivier Saurer; Marc Pollefeys; Gim Hee Lee",
        "abstract": "It is well known that the rolling shutter effect in images captured with a moving rolling shutter camera causes inaccuracies to 3D reconstructions. The problem is further aggravated with weak visual connectivity from wide baseline images captured with a fast moving camera. In this paper, we propose and implement a pipeline for sparse to dense 3D construction with wide baseline images captured from a fast moving rolling shutter camera. pecifically, we propose a cost function for Bundle Adjustment (BA) that models the rolling shutter effect, incorporates GPS/INS readings, and enforces pairwise smoothness between neighboring poses. We optimize over the 3D structures, camera poses and velocities. We also introduce a novel interpolation scheme for the rolling shutter plane sweep stereo algorithm that allows us to achieve a 7x speed up in the depth map computations for dense reconstruction without losing accuracy. We evaluate our proposed pipeline over a 2.6km image sequence captured with a rolling shutter camera mounted on a moving car.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Saurer_Sparse_to_Dense_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, ETH Z\u00fcrich, Switzerland; Department of Computer Science, ETH Z\u00fcrich, Switzerland; Department of Mechanical Engineering, National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4115457,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7245231540939161003&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;nus.edu.sg",
        "email": "inf.ethz.ch;inf.ethz.ch;nus.edu.sg",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Saurer_Sparse_to_Dense_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "ETH Zurich;National University of Singapore",
        "aff_unique_dep": "Department of Computer Science;Department of Mechanical Engineering",
        "aff_unique_url": "https://www.ethz.ch;https://www.nus.edu.sg",
        "aff_unique_abbr": "ETHZ;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Switzerland;Singapore"
    },
    {
        "title": "Sparseness Meets Deepness: 3D Human Pose Estimation From Monocular Video",
        "session": "Human Pose Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "45",
        "author_site": "Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Konstantinos G. Derpanis, Kostas Daniilidis",
        "author": "Xiaowei Zhou; Menglong Zhu; Spyridon Leonardos; Konstantinos G. Derpanis; Kostas Daniilidis",
        "abstract": "This paper addresses the challenge of 3D full-body human pose estimation from a monocular image sequence. Here, two cases are considered: (i) the image locations of the human joints are provided and (ii) the image locations of joints are unknown. In the former case, a novel approach is introduced that integrates a sparsity-driven 3D geometric prior and temporal smoothness. In the latter case, the former case is extended by treating the image locations of the joints as latent variables in order to take into account considerable uncertainties in 2D joint locations. A deep fully convolutional network is trained to predict the uncertainty maps of the 2D joint locations. The 3D pose estimates are realized via an Expectation-Maximization algorithm over the entire sequence, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Empirical evaluation on the Human3.6M dataset shows that the proposed approaches achieve greater 3D pose estimation accuracy over  state-of-the-art baselines.  Further, the proposed approach outperforms a publicly available 2D pose estimation baseline on the challenging PennAction dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Sparseness_Meets_Deepness_CVPR_2016_paper.pdf",
        "aff": "University of Pennsylvania\u2020; University of Pennsylvania\u2020; University of Pennsylvania\u2020; Ryerson University\u2021; University of Pennsylvania\u2020",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhou_Sparseness_Meets_Deepness_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 900074,
        "gs_citation": 545,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11435747535047539866&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhou_Sparseness_Meets_Deepness_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Pennsylvania;Ryerson University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;https://www.ryerson.ca",
        "aff_unique_abbr": "UPenn;Ryerson",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "Sparsifying Neural Network Connections for Face Recognition",
        "session": "Face Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "33",
        "author_site": "Yi Sun, Xiaogang Wang, Xiaoou Tang",
        "author": "Yi Sun; Xiaogang Wang; Xiaoou Tang",
        "abstract": "This paper proposes to learn high-performance deep ConvNets with sparse neural connections, referred to as sparse ConvNets, for face recognition. The sparse ConvNets are learned in an iterative way, each time one additional layer is sparsified and the entire model is re-trained given the initial weights learned in previous iterations. One important finding is that directly training the sparse ConvNet from scratch failed to find good solutions for face recognition, while using a previously learned denser model to properly initialize a sparser model is critical to continue learning effective features for face recognition. This paper also proposes a new neural correlation-based weight selection criterion and empirically verifies its effectiveness in selecting informative connections from previously learned models in each iteration. When taking a moderately sparse structure 26%-76% of weights in the dense model), the proposed sparse ConvNet model significantly improves the face recognition performance of the previous state-of-the-art DeepID2+ models given the same training data, while it keeps the performance of the baseline model with only 12% of the original parameters.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sun_Sparsifying_Neural_Network_CVPR_2016_paper.pdf",
        "aff": "SenseTime Group; Department of Electronic Engineering, The Chinese University of Hong Kong + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 396726,
        "gs_citation": 187,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2255856663648914492&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "sensetime.com;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "sensetime.com;ee.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sun_Sparsifying_Neural_Network_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+2;1+2",
        "aff_unique_norm": "SenseTime Group;Chinese University of Hong Kong;Chinese Academy of Sciences",
        "aff_unique_dep": ";Department of Electronic Engineering;Shenzhen Institutes of Advanced Technology",
        "aff_unique_url": "https://www.sensetime.com;https://www.cuhk.edu.hk;http://www.siat.cas.cn",
        "aff_unique_abbr": "SenseTime;CUHK;SIAT",
        "aff_campus_unique_index": "1+2;1+2",
        "aff_campus_unique": ";Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Spatially Binned ROC: A Comprehensive Saliency Metric",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "56",
        "author_site": "Calden Wloka, John Tsotsos",
        "author": "Calden Wloka; John Tsotsos",
        "abstract": "A recent trend in saliency algorithm development is large-scale benchmarking and algorithm ranking with ground truth provided by datasets of human fixations. In order to accommodate the strong bias humans have toward central fixations, it is common to replace traditional ROC metrics with a shuffled ROC metric which uses randomly sampled fixations from other images in the database as the negative set. However, the shuffled ROC introduces a number of problematic elements, including a fundamental assumption that it is possible to separate visual salience and image spatial arrangement.  We argue that it is more informative to directly measure the effect of spatial bias on algorithm performance rather than try to correct for it. To capture and quantify these known sources of bias, we propose a novel metric for measuring saliency algorithm performance: the spatially binned ROC (spROC). This metric provides direct insight into the spatial biases of a saliency algorithm without sacrificing the intuitive raw performance evaluation of traditional ROC measurements. By quantitatively measuring the bias in saliency algorithms, researchers will be better equipped to select and optimize the most appropriate algorithm for a given task. We use a baseline measure of inherent algorithm bias to show that Adaptive Whitening Saliency (AWS) [14], Attention by Information Maximization (AIM) [8], and Dynamic Visual Attention (DVA) [20] provide the least spatially biased results, suiting them for tasks in which there is no information about the underlying spatial bias of the stimuli, whereas algorithms such as Graph Based Visual Saliency (GBVS) [18] and Context-Aware Saliency (CAS) [15] have a significant inherent central bias.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wloka_Spatially_Binned_ROC_CVPR_2016_paper.pdf",
        "aff": "Electrical Engineering and Computer Science Department, York University, Toronto; Electrical Engineering and Computer Science Department, York University, Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 829309,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6305338164515596897&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cse.yorku.ca;cse.yorku.ca",
        "email": "cse.yorku.ca;cse.yorku.ca",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wloka_Spatially_Binned_ROC_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "York University",
        "aff_unique_dep": "Electrical Engineering and Computer Science Department",
        "aff_unique_url": "https://www.yorku.ca",
        "aff_unique_abbr": "York U",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Spatiotemporal Bundle Adjustment for Dynamic 3D Reconstruction",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "23",
        "author_site": "Minh Vo, Srinivasa G. Narasimhan, Yaser Sheikh",
        "author": "Minh Vo; Srinivasa G. Narasimhan; Yaser Sheikh",
        "abstract": "Bundle adjustment jointly optimizes camera intrinsics and extrinsics and 3D point triangulation to reconstruct a static scene. The triangulation constraint however is invalid for moving points captured in multiple unsynchronized videos and bundle adjustment is not purposed to estimate the temporal alignment between cameras. In this paper, we present a spatiotemporal bundle adjustment approach that jointly optimizes four coupled sub-problems: estimating camera intrinsics and extrinsics, triangulating 3D static points, as well as subframe temporal alignment between cameras and estimating 3D trajectories of dynamic points. Key to our joint optimization is the careful integration of physics-based motion priors within the reconstruction pipeline, validated on a large motion capture corpus. We present an end-to-end pipeline that takes multiple uncalibrated and unsynchronized video streams and produces a dynamic reconstruction of the event. Because the videos are aligned with sub-frame precision, we reconstruct 3D trajectories of unconstrained outdoor activities at much higher temporal resolution than the input videos.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Vo_Spatiotemporal_Bundle_Adjustment_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Vo_Spatiotemporal_Bundle_Adjustment_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4865731782373688424&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Vo_Spatiotemporal_Bundle_Adjustment_CVPR_2016_paper.html"
    },
    {
        "title": "Split and Match: Example-Based Adaptive Patch Sampling for Unsupervised Style Transfer",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "59",
        "author_site": "Oriel Frigo, Neus Sabater, Julie Delon, Pierre Hellier",
        "author": "Oriel Frigo; Neus Sabater; Julie Delon; Pierre Hellier",
        "abstract": "This paper presents a novel unsupervised method to transfer the style of an example image to a source image. The complex notion of image style is here considered as a local texture transfer, eventually coupled with a global color transfer. For the local texture transfer, we propose a new method based on an adaptive patch partition that captures the style of the example image and preserves the structure of the source image. More precisely, this example-based partition predicts how well a source patch matches an example patch. Results on various images show that our method outperforms the most recent techniques.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Frigo_Split_and_Match_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2037128,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7619759086647323498&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Frigo_Split_and_Match_CVPR_2016_paper.html"
    },
    {
        "title": "Stacked Attention Networks for Image Question Answering",
        "session": "Image Captioning and Question Answering",
        "status": "Oral",
        "track": "main",
        "pid": "3",
        "author_site": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola",
        "author": "Zichao Yang; Xiaodong He; Jianfeng Gao; Li Deng; Alex Smola",
        "abstract": "This paper presents stacked attention networks (SANs)that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf",
        "aff": "Carnegie Mellon University; Microsoft Research; Microsoft Research; Microsoft Research; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Yang_Stacked_Attention_Networks_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 749390,
        "gs_citation": 2435,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14027637653932093716&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff_domain": "cs.cmu.edu;microsoft.com;microsoft.com;microsoft.com;smola.org",
        "email": "cs.cmu.edu;microsoft.com;microsoft.com;microsoft.com;smola.org",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_Stacked_Attention_Networks_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.cmu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "CMU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Staple: Complementary Learners for Real-Time Tracking",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "68",
        "author_site": "Luca Bertinetto, Jack Valmadre, Stuart Golodetz, Ondrej Miksik, Philip H. S. Torr",
        "author": "Luca Bertinetto; Jack Valmadre; Stuart Golodetz; Ondrej Miksik; Philip H. S. Torr",
        "abstract": "Correlation Filter-based trackers have recently achieved excellent performance, showing great robustness to challenging situations exhibiting motion blur and illumination changes. However, since the model that they learn depends strongly on the spatial layout of the tracked object, they are notoriously sensitive to deformation. Models based on colour statistics have complementary traits: they cope well with variation in shape, but suffer when illumination is not consistent throughout a sequence. Moreover, colour distributions alone can be insufficiently discriminative. In this paper, we show that a simple tracker combining complementary cues in a ridge regression framework can operate faster than 80 FPS and outperform not only all entries in the popular VOT14 competition, but also recent and far more sophisticated trackers according to multiple benchmarks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bertinetto_Staple_Complementary_Learners_CVPR_2016_paper.pdf",
        "aff": "University of Oxford; University of Oxford; University of Oxford; University of Oxford; University of Oxford",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Bertinetto_Staple_Complementary_Learners_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1421497,
        "gs_citation": 2208,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7127186926810478238&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 10,
        "aff_domain": "eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk",
        "email": "eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk;eng.ox.ac.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bertinetto_Staple_Complementary_Learners_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Stereo Matching With Color and Monochrome Cameras in Low-Light Conditions",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "32",
        "author_site": "Hae-Gon Jeon, Joon-Young Lee, Sunghoon Im, Hyowon Ha, In So Kweon",
        "author": "Hae-Gon Jeon; Joon-Young Lee; Sunghoon Im; Hyowon Ha; In So Kweon",
        "abstract": "Consumer devices with stereo cameras have become popular because of their low-cost depth sensing capability. However, those systems usually suffer from low imaging quality and inaccurate depth acquisition under low-light conditions. To address the problem, we present a new stereo matching method with a color and monochrome camera pair. We focus on the fundamental trade-off that monochrome cameras have much better light-efficiency than color-filtered cameras. Our key ideas involve compensating for the radiometric difference between two cross-spectral images and taking full advantage of complementary data. Consequently, our method produces both an accurate depth map and high-quality images, which are applicable for various depth-aware image processing. Our method is evaluated using various datasets and the performance of our depth estimation consistently outperforms state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Jeon_Stereo_Matching_With_CVPR_2016_paper.pdf",
        "aff": "Robotics and Computer Vision Lab., KAIST; Adobe Research; Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST; Robotics and Computer Vision Lab., KAIST",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 5341493,
        "gs_citation": 60,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12140617977930861111&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "rcv.kaist.ac.kr;adobe.com;rcv.kaist.ac.kr;rcv.kaist.ac.kr;kaist.ac.kr",
        "email": "rcv.kaist.ac.kr;adobe.com;rcv.kaist.ac.kr;rcv.kaist.ac.kr;kaist.ac.kr",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Jeon_Stereo_Matching_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "KAIST;Adobe",
        "aff_unique_dep": "Robotics and Computer Vision Lab.;Adobe Research",
        "aff_unique_url": "https://www.kaist.ac.kr;https://research.adobe.com",
        "aff_unique_abbr": "KAIST;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "title": "Structural Correlation Filter for Robust Visual Tracking",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "56",
        "author_site": "Si Liu, Tianzhu Zhang, Xiaochun Cao, Changsheng Xu",
        "author": "Si Liu; Tianzhu Zhang; Xiaochun Cao; Changsheng Xu",
        "abstract": "In this paper, we propose a novel structural correlation filter (SCF) model for robust visual tracking. The proposed SCF model takes part-based tracking strategies into account in a correlation filter tracker, and exploits circular shifts of all parts for their motion modeling to preserve target object structure. Compared with existing correlation filter trackers, our proposed tracker has several advantages: (1) Due to the part strategy, the learned structural correlation filters are less sensitive to partial occlusion, and have computational efficiency and robustness. (2) The learned filters are able to not only distinguish the parts from the background as the traditional correlation filters, but also exploit the intrinsic relationship among local parts via spatial constraints to preserve object structure. (3) The learned correlation filters not only make most parts share similar motion, but also tolerate outlier parts that have different motion. Both qualitative and quantitative evaluations on challenging benchmark image sequences demonstrate that the proposed SCF tracking algorithm performs favorably against several state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Liu_Structural_Correlation_Filter_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3347353,
        "gs_citation": 202,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11116242374354224060&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Liu_Structural_Correlation_Filter_CVPR_2016_paper.html"
    },
    {
        "title": "Structural-RNN: Deep Learning on Spatio-Temporal Graphs",
        "session": "Learning and CNN Architectures",
        "status": "Oral",
        "track": "main",
        "pid": "2",
        "author_site": "Ashesh Jain, Amir R. Zamir, Silvio Savarese, Ashutosh Saxena",
        "author": "Ashesh Jain; Amir R. Zamir; Silvio Savarese; Ashutosh Saxena",
        "abstract": "Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatio-temporal graphs are a popular  tool for imposing such high-level intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks (RNNs). We develop a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower  new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Jain_Structural-RNN_Deep_Learning_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Jain_Structural-RNN_Deep_Learning_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "gs_citation": 1477,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18108454464973674206&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Jain_Structural-RNN_Deep_Learning_CVPR_2016_paper.html"
    },
    {
        "title": "Structure From Motion With Objects",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "38",
        "author_site": "Marco Crocco, Cosimo Rubino, Alessio Del Bue",
        "author": "Marco Crocco; Cosimo Rubino; Alessio Del Bue",
        "abstract": "This paper shows for the first time that is possible to reconstruct the position of rigid objects and to jointly recover affine camera calibration solely from a set of object detections in a video sequence. In practice, this work can be considered as the extension of Tomasi and Kanade factorization method using objects. Instead of using points to form a rank constrained measurement matrix, we can form a matrix with similar rank properties using 2D object detection proposals. In detail, we first fit an ellipse onto the image plane at each bounding box as given by the object detector. The collection of all the ellipses in the dual space is used to create a measurement matrix that gives a specific rank constraint. This matrix can be factorised and metrically upgraded in order to provide the affine camera matrices and the 3D position of the objects as an ellipsoid. Moreover, we recover the full 3D quadric thus giving additional information about object occupancy and 3D pose. Finally, we also show that 2D points measurements can be seamlessly included in the framework to reduce the number of objects required. This last aspect unifies the classical point-based Tomasi and Kanade approach with objects in a unique framework. Experiments with synthetic and real data show the feasibility of our approach for the affine camera case.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Crocco_Structure_From_Motion_CVPR_2016_paper.pdf",
        "aff": "Pattern Analysis and Computer Vision Department (PA VIS) + Visual Geometry and Modelling Lab (VGM) + Istituto Italiano di Tecnologia; Pattern Analysis and Computer Vision Department (PA VIS) + Visual Geometry and Modelling Lab (VGM) + Istituto Italiano di Tecnologia; Pattern Analysis and Computer Vision Department (PA VIS) + Visual Geometry and Modelling Lab (VGM) + Istituto Italiano di Tecnologia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1286286,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3324932070302874546&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "iit.it; ; ",
        "email": "iit.it; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Crocco_Structure_From_Motion_CVPR_2016_paper.html",
        "aff_unique_index": "0+1+2;0+1+2;0+1+2",
        "aff_unique_norm": "Pattern Analysis and Computer Vision Department;Visual Geometry and Modelling Lab;Istituto Italiano di Tecnologia",
        "aff_unique_dep": "Computer Vision;Visual Geometry and Modelling;",
        "aff_unique_url": ";;https://www.iit.it",
        "aff_unique_abbr": "PA VIS;VGM;IIT",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";Italy"
    },
    {
        "title": "Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "24",
        "author_site": "Zhiwei Deng, Arash Vahdat, Hexiang Hu, Greg Mori",
        "author": "Zhiwei Deng; Arash Vahdat; Hexiang Hu; Greg Mori",
        "abstract": "Rich semantic relations are important in a variety of visual recognition problems.  As a concrete example, group activity recognition involves the interactions and relative spatial relations of a set of people in a scene. State of the art recognition methods center on deep learning approaches for training highly effective, complex classifiers for interpreting images.  However, bridging the relatively low-level concepts output by these methods to interpret higher-level compositional scenes remains a challenge.  Graphical models are a standard tool for this task. In this paper, we propose a method to integrate graphical models and deep neural networks into a joint framework. Instead of using a traditional inference method, we use a sequential inference modeled by a recurrent neural network. Beyond this, the appropriate structure for inference can be learned by imposing gates on edges between nodes.  Empirical results on group activity recognition demonstrate the potential of this model to handle highly structured learning tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Deng_Structure_Inference_Machines_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science, Simon Fraser University, Canada; School of Computer Science, Simon Fraser University, Canada; School of Computer Science, Simon Fraser University, Canada; School of Computer Science, Simon Fraser University, Canada",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 722597,
        "gs_citation": 308,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1790183069021005427&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "sfu.ca;sfu.ca;sfu.ca;cs.sfu.ca",
        "email": "sfu.ca;sfu.ca;sfu.ca;cs.sfu.ca",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Deng_Structure_Inference_Machines_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Simon Fraser University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.sfu.ca",
        "aff_unique_abbr": "SFU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "title": "Structure-From-Motion Revisited",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "34",
        "author_site": "Johannes L. Sch\u00f6nberger, Jan-Michael Frahm",
        "author": "Johannes L. Schonberger; Jan-Michael Frahm",
        "abstract": "Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental reconstruction systems have tremendously advanced in all regards, robustness, accuracy, completeness, and scalability remain the key problems towards building a truly general-purpose pipeline. We propose a new SfM technique that improves upon the state of the art to make a further step towards this ultimate goal. The full reconstruction pipeline is released to the public as an open-source implementation.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf",
        "aff": "University of North Carolina at Chapel Hill + Eidgen\u00f6ssische Technische Hochschule Z\u00fcrich; University of North Carolina at Chapel Hill",
        "project": "",
        "github": "https://github.com/colmap/colmap",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Schonberger_Structure-From-Motion_Revisited_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1833331,
        "gs_citation": 7288,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5556310561359757303&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff_domain": "inf.ethz.ch;cs.unc.edu",
        "email": "inf.ethz.ch;cs.unc.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "University of North Carolina;Eidgen\u00f6ssische Technische Hochschule Z\u00fcrich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unc.edu;https://www.ethz.ch",
        "aff_unique_abbr": "UNC;ETH Z\u00fcrich",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chapel Hill;",
        "aff_country_unique_index": "0+1;0",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "title": "Structured Feature Learning for Pose Estimation",
        "session": "Human Pose Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "18",
        "author_site": "Xiao Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang",
        "author": "Xiao Chu; Wanli Ouyang; Hongsheng Li; Xiaogang Wang",
        "abstract": "In this paper, we propose a structured feature learning framework to reason the correlation among body joints at the feature level in human pose estimation. Different from existing approaches of modeling structures on score maps or predicted labels, feature maps preserve substantially richer descriptions of body joints. The relationships between feature maps of joints are captured with the introduced geometrical transform kernels, which can be easily implemented with a convolution layer. Features and their relationships are jointly learned in an end-to-end learning system. A bi-directional tree structured model is proposed, so that the feature channels at a body joint can well receive information from other joints. The proposed framework improves feature learning substantially. With very simple post processing, it reaches the best mean PCP on the LSP and FLIC datasets. Compared with the baseline of learning features at each joint separately with ConvNet, the mean PCP has been improved by 18% on FLIC. The code is released to the public.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chu_Structured_Feature_Learning_CVPR_2016_paper.pdf",
        "aff": "Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong; Department of Electronic Engineering, The Chinese University of Hong Kong",
        "project": "http://www.ee.cuhk.edu.hk/~xgwang/projectpage_structured_feature_pose.html",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2789323,
        "gs_citation": 330,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3018725053497293684&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "email": "ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk;ee.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chu_Structured_Feature_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Department of Electronic Engineering",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Structured Feature Similarity With Explicit Feature Map",
        "session": "Feature Matching and Indexing",
        "status": "Poster",
        "track": "main",
        "pid": "48",
        "author": "Takumi Kobayashi",
        "abstract": "Feature matching is a fundamental process in a variety of computer vision tasks. Beyond the standard L2 metric, various methods to measure similarity between features have been proposed mainly on the assumption that the features are defined in a histogram form. On the other hand, in a field of image quality assessment, SSIM produces effective similarity between images, taking the place of L2 metric. In this paper, we propose a feature similarity measurement method based on the SSIM. Unlike the previous methods, the proposed method is built on not a histogram form but a tensor structure of a feature array extracted such as on spatial grids, in order to construct effective SSIM-based similarity measure of high robustness which is a key requirement in feature matching. In addition, we provide the explicit feature map such that the proposed similarity metric is embedded as a dot product. It contributes to significant speedup in similarity measurement as well as to feature transformation toward an effective vector form to which linear classifiers are directly applicable. In the experiments on various tasks, the proposed method exhibits favorable performance in both feature matching and classification.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kobayashi_Structured_Feature_Similarity_CVPR_2016_paper.pdf",
        "aff": "National Institute of Advanced Industrial Science and Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1355077,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4504488848892897329&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "aist.go.jp",
        "email": "aist.go.jp",
        "author_num": 1,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kobayashi_Structured_Feature_Similarity_CVPR_2016_paper.html",
        "aff_unique_index": "0",
        "aff_unique_norm": "National Institute of Advanced Industrial Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.aist.go.jp",
        "aff_unique_abbr": "AIST",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Structured Prediction of Unobserved Voxels From a Single Depth Image",
        "session": "3D Shape Reconstruction",
        "status": "Oral",
        "track": "main",
        "pid": "15",
        "author_site": "Michael Firman, Oisin Mac Aodha, Simon Julier, Gabriel J. Brostow",
        "author": "Michael Firman; Oisin Mac Aodha; Simon Julier; Gabriel J. Brostow",
        "abstract": "Building a complete 3D model of a scene, given only a single depth image, is underconstrained. To gain a full volumetric model, one needs either multiple views, or a single view together with a library of unambiguous 3D models that will fit the shape of each individual object in the scene.  We hypothesize that objects of dissimilar semantic classes often share similar 3D shape components, enabling a limited dataset to model the shape of a wide range of objects, and hence estimate their hidden geometry. Exploring this hypothesis, we propose an algorithm that can complete the unobserved geometry of tabletop-sized objects, based on a supervised model trained on already available volumetric elements. Our model maps from a local observation in a single depth image to an estimate of the surface shape in the surrounding neighborhood. We validate our approach both qualitatively and quantitatively on a range of indoor object collections and challenging real scenes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Firman_Structured_Prediction_of_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 933645,
        "gs_citation": 206,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=434127024807489907&as_sdt=40005&sciodt=0,10&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Firman_Structured_Prediction_of_CVPR_2016_paper.html"
    },
    {
        "title": "Structured Receptive Fields in CNNs",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "38",
        "author_site": "J\u00f6rn-Henrik Jacobsen, Jan van Gemert, Zhongyu Lou, Arnold W. M. Smeulders",
        "author": "Jorn-Henrik Jacobsen; Jan van Gemert; Zhongyu Lou; Arnold W. M. Smeulders",
        "abstract": "Learning powerful feature representations with CNNs is hard when training data are limited. Pre-training is one way to overcome this, but it requires large datasets sufficiently similar to the target domain. Another option is to design priors into the model, which can range from tuned hyperparameters to fully engineered representations like Scattering Networks. We combine these ideas into structured receptive field networks, a model which has a fixed filter basis and yet retains the flexibility of CNNs. This flexibility is achieved by expressing receptive fields in CNNs as a weighted sum over a fixed basis which is similar in spirit to Scattering Networks. The key difference is that we learn arbitrary effective filter sets from the basis rather than modeling the filters. This approach explicitly connects classical multiscale image analysis with general CNNs. With structured receptive field networks, we improve considerably over unstructured CNNs for small and medium dataset scenarios as well as over Scattering for large datasets. We validate our findings on ILSVRC2012, Cifar-10, Cifar-100 and MNIST. As a realistic small dataset example, we show state-of-the-art classification results on popular 3D MRI brain-disease datasets where pre-training is difficult due to a lack of large public datasets in a similar domain.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Jacobsen_Structured_Receptive_Fields_CVPR_2016_paper.pdf",
        "aff": "University of Amsterdam, The Netherlands; University of Amsterdam, The Netherlands + TU Delft, The Netherlands; University of Amsterdam, The Netherlands; University of Amsterdam, The Netherlands",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Jacobsen_Structured_Receptive_Fields_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 850775,
        "gs_citation": 142,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10489181891792603154&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "uva.nl;tudelft.nl;uva.nl;uva.nl",
        "email": "uva.nl;tudelft.nl;uva.nl;uva.nl",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Jacobsen_Structured_Receptive_Fields_CVPR_2016_paper.html",
        "aff_unique_index": "0;0+1;0;0",
        "aff_unique_norm": "University of Amsterdam;Delft University of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uva.nl;https://www.tudelft.nl",
        "aff_unique_abbr": "UvA;TU Delft",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Delft",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "title": "Structured Regression Gradient Boosting",
        "session": "Supervised Learning",
        "status": "Poster",
        "track": "main",
        "pid": "74",
        "author_site": "Ferran Diego, Fred A. Hamprecht",
        "author": "Ferran Diego; Fred A. Hamprecht",
        "abstract": "We propose a new way to train a structured output prediction model. More specifically, we train  nonlinear data terms in a Gaussian Conditional Random Field (GCRF) by a generalized version of gradient boosting. The approach is evaluated on three challenging regression benchmarks: vessel detection, single image depth estimation and image inpainting. These experiments suggest that the proposed boosting framework matches or exceeds the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Diego_Structured_Regression_Gradient_CVPR_2016_paper.pdf",
        "aff": "Heidelberg Collaboratory for Image Processing (HCI) + Interdisciplinary Center for Scienti\ufb01c Computing (IWR), University of Heidelberg, Heidelberg 69115, Germany; Heidelberg Collaboratory for Image Processing (HCI) + Interdisciplinary Center for Scienti\ufb01c Computing (IWR), University of Heidelberg, Heidelberg 69115, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4165418,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_domain": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de",
        "email": "iwr.uni-heidelberg.de;iwr.uni-heidelberg.de",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Diego_Structured_Regression_Gradient_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Heidelberg Collaboratory for Image Processing;University of Heidelberg",
        "aff_unique_dep": "Image Processing;Interdisciplinary Center for Scienti\ufb01c Computing (IWR)",
        "aff_unique_url": "https://hci.iwr.uni-heidelberg.de/;https://www.uni-heidelberg.de",
        "aff_unique_abbr": "HCI;Uni Heidelberg",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Heidelberg",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Studying Very Low Resolution Recognition Using Deep Networks",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "26",
        "author_site": "Zhangyang Wang, Shiyu Chang, Yingzhen Yang, Ding Liu, Thomas S. Huang",
        "author": "Zhangyang Wang; Shiyu Chang; Yingzhen Yang; Ding Liu; Thomas S. Huang",
        "abstract": "Visual recognition research often assumes a sufficient resolution of the region of interest (ROI). That is usually violated in practice, inspiring us to explore the Very Low Resolution Recognition (VLRR) problem. Typically, the ROI in a VLRR problem can be smaller than 16 x16 pixels, and is challenging to be recognized even by human experts. We attempt to solve the VLRR problem using deep learning methods. Taking advantage of techniques primarily in super resolution, domain adaptation and robust regression, we formulate a dedicated deep learning method and demonstrate how these techniques are incorporated step by step. Any extra complexity, when introduced, is fully justified by both analysis and simulation results. The resulting Robust Partially Coupled Networks achieves feature enhancement and recognition simultaneously. It allows for both the flexibility to combat the LR-HR domain mismatch, and the robustness to outliers. Finally, the effectiveness of the proposed models is evaluated on three different VLRR tasks, including face identification, digit recognition and font recognition, all of which obtain very impressive performances.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Studying_Very_Low_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 751384,
        "gs_citation": 284,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11512989278363956953&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Studying_Very_Low_CVPR_2016_paper.html"
    },
    {
        "title": "Sublabel-Accurate Relaxation of Nonconvex Energies",
        "session": "Grouping and Optimization Methods",
        "status": "Oral",
        "track": "main",
        "pid": "17",
        "author_site": "Thomas M\u00f6llenhoff, Emanuel Laude, Michael Moeller, Jan Lellmann, Daniel Cremers",
        "author": "Thomas Mollenhoff; Emanuel Laude; Michael Moeller; Jan Lellmann; Daniel Cremers",
        "abstract": "We propose a novel spatially continuous framework for convex relaxations based on functional lifting. Our method can be interpreted as a sublabel-accurate solution to multilabel problems. We show that previously proposed functional lifting methods optimize an energy which is linear between two labels and hence require (often infinitely) many labels for a faithful approximation. In contrast, the proposed formulation is based on a piecewise convex approximation and therefore needs far fewer labels - see Fig. 1. In comparison to recent MRF-based approaches, our method is formulated in a spatially continuous setting and shows less grid bias. Moreover, in a local sense, our formulation is the tightest possible convex relaxation. It is easy to implement and allows an efficient primal-dual optimization on GPUs. We show the effectiveness of our approach on several computer vision problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mollenhoff_Sublabel-Accurate_Relaxation_of_CVPR_2016_paper.pdf",
        "aff": "TU M \u00a8unchen; TU M \u00a8unchen; TU M \u00a8unchen; University of L \u00a8ubeck; TU M \u00a8unchen",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Mollenhoff_Sublabel-Accurate_Relaxation_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1819830,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2696453362885372712&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "in.tum.de;in.tum.de;in.tum.de;mic.uni-luebeck.de;tum.de",
        "email": "in.tum.de;in.tum.de;in.tum.de;mic.uni-luebeck.de;tum.de",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mollenhoff_Sublabel-Accurate_Relaxation_of_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Technical University of Munich;University of L\u00fcbeck",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tum.de;https://www.uni-luebeck.de",
        "aff_unique_abbr": "TUM;UL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Munich;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Subspace Clustering With Priors via Sparse Quadratically Constrained Quadratic Programming",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "70",
        "author_site": "Yongfang Cheng, Yin Wang, Mario Sznaier, Octavia Camps",
        "author": "Yongfang Cheng; Yin Wang; Mario Sznaier; Octavia Camps",
        "abstract": "This paper considers the problem of recovering a subspace arrangement from noisy samples, potentially corrupted with outliers. Our main result shows that this problem  can be formulated as a convex semi-definite optimization problem subject to an additional  rank constrain that involves only a very small number of variables.  This is established by first reducing the problem to a (generically non-convex) quadratically constrained quadratic problem and then using its special sparse structure to find conditions guaranteeing that a suitably built convex relaxation is indeed exact.  When combined with the commonly used nuclear norm relaxation for rank, the results above lead to computationally efficient algorithms with optimality guarantees. A salient feature of the proposed approach is its ability to incorporate existing a-priori information about the noise,  co-ocurrences, and percentage of outliers. These results are illustrated  with several examples where the proposed algorithm is shown to outperform existing approaches.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cheng_Subspace_Clustering_With_CVPR_2016_paper.pdf",
        "aff": "Northeastern University; Northeastern University; Northeastern University; Northeastern University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1041755,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5935734558162641150&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "husky.neu.edu;husky.neu.edu;coe.neu.edu;coe.neu.edu",
        "email": "husky.neu.edu;husky.neu.edu;coe.neu.edu;coe.neu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cheng_Subspace_Clustering_With_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.northeastern.edu",
        "aff_unique_abbr": "NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Summary Transfer: Exemplar-Based Subset Selection for Video Summarization",
        "session": "Events, Activities, and Surveillance",
        "status": "Poster",
        "track": "main",
        "pid": "32",
        "author_site": "Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman",
        "author": "Ke Zhang; Wei-Lun Chao; Fei Sha; Kristen Grauman",
        "abstract": "Video summarization has unprecedented importance to help us digest, browse, and search today's ever-growing video collections. We propose a novel subset selection technique that leverages supervision in the form of human-created summaries to perform automatic keyframe-based video summarization. The main idea is to nonparametrically transfer summary structures from annotated videos to unseen test videos. We show how to extend our method to exploit semantic side information about the video's category/genre to guide the transfer process by those training videos semantically consistent with the test input. We also show how to generalize our method to subshot-based summarization, which not only reduces computational costs but also provides more flexible ways of defining visual similarity across subshots spanning several frames. We conduct extensive evaluation on several benchmarks and demonstrate promising results, outperforming existing methods in several settings.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Summary_Transfer_Exemplar-Based_CVPR_2016_paper.pdf",
        "aff": "University of Southern California; University of Southern California; University of California; University of Texas at Austin",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhang_Summary_Transfer_Exemplar-Based_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 819695,
        "gs_citation": 271,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5453177769562795207&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff_domain": "usc.edu;usc.edu;cs.ucla.edu;cs.utexas.edu",
        "email": "usc.edu;usc.edu;cs.ucla.edu;cs.utexas.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Summary_Transfer_Exemplar-Based_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of Southern California;University of California;University of Texas at Austin",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.usc.edu;https://www.universityofcalifornia.edu;https://www.utexas.edu",
        "aff_unique_abbr": "USC;UC;UT Austin",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Los Angeles;;Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Supervised Quantization for Similarity Search",
        "session": "Image Indexing and Retrieval",
        "status": "Poster",
        "track": "main",
        "pid": "56",
        "author_site": "Xiaojuan Wang, Ting Zhang, Guo-Jun Qi, Jinhui Tang, Jingdong Wang",
        "author": "Xiaojuan Wang; Ting Zhang; Guo-Jun Qi; Jinhui Tang; Jingdong Wang",
        "abstract": "In this paper, we address the problem of searching for semantically similar images from a large database. We present a compact coding approach, supervised quantization. Our approach simultaneously learns feature selection that linearly transforms the database points into a low-dimensional discriminative subspace, and quantizes the data points in the transformed space. The optimization criterion is that the quantized points not only approximate the transformed points accurately, but also are semantically separable: the points belonging to a class lie in a cluster that is not overlapped with other clusters corresponding to other classes, which is formulated as a classification problem. The experiments on several standard datasets show the superiority of our approach over the state-of-the art supervised hashing and unsupervised quantization algorithms.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Supervised_Quantization_for_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 690812,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17351204837938447287&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Supervised_Quantization_for_CVPR_2016_paper.html"
    },
    {
        "title": "Symmetry reCAPTCHA",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "66",
        "author_site": "Chris Funk, Yanxi Liu",
        "author": "Chris Funk; Yanxi Liu",
        "abstract": "This is a reaction to the poor performance of symmetry detection algorithms on real-world images, benchmarked since CVPR 2011. Our systematic study reveals significant difference between human labeled (reflection and rotation) symmetries on photos and the output of computer vision algorithms on the same photo set. We exploit this human-machine symmetry perception gap by proposing a novel symmetry-based Turing test. By leveraging a comprehensive user interface, we collected more than 78,000 symmetry labels from 400 Amazon Mechanical Turk raters on nearly 1,000 photos from the Microsoft COCO dataset. Using a set of ground-truth symmetries automatically generated from noisy human labels, the effectiveness of our work is evidenced by a separate test where over 96% success rate is achieved. We demonstrate statistically significant outcomes for using symmetry perception as a powerful, alternative, image-based reCAPTCHA.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Funk_Symmetry_reCAPTCHA_CVPR_2016_paper.pdf",
        "aff": "School of Electrical Engineering and Computer Science. The Pennsylvania State University; School of Electrical Engineering and Computer Science. The Pennsylvania State University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4775085,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12742324432482861068&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cse.psu.edu;cse.psu.edu",
        "email": "cse.psu.edu;cse.psu.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Funk_Symmetry_reCAPTCHA_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Pennsylvania State University",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.psu.edu",
        "aff_unique_abbr": "PSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Synthesized Classifiers for Zero-Shot Learning",
        "session": "Learning and CNN Architectures",
        "status": "Oral",
        "track": "main",
        "pid": "4",
        "author_site": "Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, Fei Sha",
        "author": "Soravit Changpinyo; Wei-Lun Chao; Boqing Gong; Fei Sha",
        "abstract": "Given semantic descriptions of object classes, zero-shot learning aims to accurately recognize objects of the unseen classes, from which no examples are available at the training stage, by associating them to the seen classes, from which labeled examples are provided. We propose to tackle this problem from the perspective of manifold learning. Our main idea is to align the semantic space that is derived from external information to the model space that concerns itself with recognizing visual features. To this end, we introduce a set of \"phantom\" object classes whose coordinates live in both the semantic space and the model space. Serving as bases in a dictionary, they can be optimized from labeled data such that the synthesized real object classifiers achieve optimal discriminative performance. We demonstrate superior accuracy of our approach over the state of the art on four benchmark datasets for zero-shot learning, including the full ImageNet Fall 2011 dataset with more than 20,000 unseen classes.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Changpinyo_Synthesized_Classifiers_for_CVPR_2016_paper.pdf",
        "aff": "U. of Southern California; U. of Southern California; U. of Central Florida; U. of California",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Changpinyo_Synthesized_Classifiers_for_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 543131,
        "gs_citation": 947,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9417352588100487015&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "usc.edu;usc.edu;crcv.ucf.edu;cs.ucla.edu",
        "email": "usc.edu;usc.edu;crcv.ucf.edu;cs.ucla.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Changpinyo_Synthesized_Classifiers_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "University of Southern California;University of Central Florida;University of California",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.usc.edu;https://www.ucf.edu;https://www.universityofcalifornia.edu",
        "aff_unique_abbr": "USC;UCF;UC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Synthetic Data for Text Localisation in Natural Images",
        "session": "Object Detection 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "6",
        "author_site": "Ankush Gupta, Andrea Vedaldi, Andrew Zisserman",
        "author": "Ankush Gupta; Andrea Vedaldi; Andrew Zisserman",
        "abstract": "In this paper we introduce a new method for text detection in natural images. The method comprises two contributions: First, a fast and scalable engine to generate synthetic images of text in clutter.  This engine overlays synthetic text to existing background images in a natural way, accounting for the local 3D scene geometry. Second, we use the synthetic images to train a Fully-Convolutional Regression Network (FCRN) which efficiently performs text detection and bounding-box regression at all locations and multiple scales in an image. We discuss the relation of FCRN to the recently-introduced YOLO detector, as well as other end-to-end object detection systems based on deep learning. The resulting detection network significantly out performs current methods for text detection in natural images, achieving an F-measure of 84.2% on the standard ICDAR 2013 benchmark. Furthermore, it can process 15 images per second on a GPU.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gupta_Synthetic_Data_for_CVPR_2016_paper.pdf",
        "aff": "Dept. of Engineering Science, University of Oxford; Dept. of Engineering Science, University of Oxford; Dept. of Engineering Science, University of Oxford",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Gupta_Synthetic_Data_for_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1078005,
        "gs_citation": 1904,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4348461873934081485&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk;robots.ox.ac.uk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gupta_Synthetic_Data_for_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "Dept. of Engineering Science",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Oxford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "TGIF: A New Dataset and Benchmark on Animated GIF Description",
        "session": "High Level Semantics",
        "status": "Spotlight",
        "track": "main",
        "pid": "10",
        "author_site": "Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, Jiebo Luo",
        "author": "Yuncheng Li; Yale Song; Liangliang Cao; Joel Tetreault; Larry Goldberg; Alejandro Jaimes; Jiebo Luo",
        "abstract": "With the recent popularity of animated GIFs on social media, there is need for ways to index them with rich metadata. To advance research on animated GIF understanding, we collected a new dataset, Tumblr GIF (TGIF), with 100K animated GIFs from Tumblr and 120K natural language descriptions obtained via crowdsourcing. The motivation for this work is to develop a testbed for image sequence description systems, where the task is to generate natural language descriptions for animated GIFs or video clips. To ensure a high quality dataset, we developed a series of novel quality controls to validate free-form text input from crowdworkers. We show that there is unambiguous association between visual content and natural language descriptions in our dataset, making it an ideal benchmark for the visual content captioning task. We perform extensive statistical analyses to compare our dataset to existing image and video description datasets. Next, we provide baseline results on the animated GIF description task, using three representative techniques: nearest neighbor, statistical machine translation, and recurrent neural networks. Finally, we show that models fine-tuned from our animated GIF description dataset can be helpful for automatic movie description.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_TGIF_A_New_CVPR_2016_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Li_TGIF_A_New_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3563334,
        "gs_citation": 330,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3998385582377859132&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_TGIF_A_New_CVPR_2016_paper.html"
    },
    {
        "title": "TI-Pooling: Transformation-Invariant Pooling for Feature Learning in Convolutional Neural Networks",
        "session": "Feature Extraction and Description",
        "status": "Poster",
        "track": "main",
        "pid": "31",
        "author_site": "Dmitry Laptev, Nikolay Savinov, Joachim M. Buhmann, Marc Pollefeys",
        "author": "Dmitry Laptev; Nikolay Savinov; Joachim M. Buhmann; Marc Pollefeys",
        "abstract": "In this paper we present a deep neural network topology that incorporates a simple to implement transformation-invariant pooling operator (TI-pooling). This operator is able to efficiently handle prior knowledge on nuisance variations in the data, such as rotation or scale changes. Most current methods usually make use of dataset augmentation to address this issue, but this requires larger number of model parameters and more training data, and results in significantly increased training time and larger chance of under- or overfitting. The main reason for these drawbacks is that that the learned model needs to capture adequate features for all the possible transformations of the input. On the other hand, we formulate features in convolutional neural networks to be transformation-invariant. We achieve that using parallel siamese architectures for the considered transformation set and applying the TI-pooling operator on their outputs before the fully-connected layers. We show that this topology internally finds the most optimal \"canonical\" instance of the input image for training and therefore limits the redundancy in learned features. This more efficient use of training data results in better performance on popular benchmark datasets with smaller number of parameters when comparing to standard convolutional neural networks with dataset augmentation and to other baselines.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 500856,
        "gs_citation": 328,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13989154003772074655&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Laptev_TI-Pooling_Transformation-Invariant_Pooling_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "Temporal Action Detection Using a Statistical Language Model",
        "session": "Activity Recognition",
        "status": "Spotlight",
        "track": "main",
        "pid": "11",
        "author_site": "Alexander Richard, Juergen Gall",
        "author": "Alexander Richard; Juergen Gall",
        "abstract": "While current approaches to action recognition on pre-segmented video clips already achieve high accuracies, temporal action detection is still far from comparably good results. Automatically locating and classifying the relevant action segments in videos of varying lengths proves to be a challenging task. We propose a novel method for temporal action detection including statistical length and language modeling to represent temporal and contextual structure. Our approach aims at globally optimizing the joint probability of three components, a length and language model and a discriminative action model, without making intermediate decisions. The problem of finding the most likely action sequence and the corresponding segment boundaries in an exponentially large search space is addressed by dynamic programming.  We provide an extensive evaluation of each model component on Thumos 14, a large action detection dataset, and report state-of-the-art results on three datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Richard_Temporal_Action_Detection_CVPR_2016_paper.pdf",
        "aff": "University of Bonn, Germany; University of Bonn, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 589181,
        "gs_citation": 267,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3336866546759363715&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "iai.uni-bonn.de;iai.uni-bonn.de",
        "email": "iai.uni-bonn.de;iai.uni-bonn.de",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Richard_Temporal_Action_Detection_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Bonn",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uni-bonn.de",
        "aff_unique_abbr": "UBonn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Temporal Action Localization With Pyramid of Score Distribution Features",
        "session": "Activity Recognition",
        "status": "Spotlight",
        "track": "main",
        "pid": "7",
        "author_site": "Jun Yuan, Bingbing Ni, Xiaokang Yang, Ashraf A. Kassim",
        "author": "Jun Yuan; Bingbing Ni; Xiaokang Yang; Ashraf A. Kassim",
        "abstract": "We investigate the feature design and classification architectures in temporal action localization. This application focuses on detecting and labeling actions in untrimmed videos, which brings more challenge than classifying pre-segmented videos. The major difficulty for action localization is the uncertainty of action occurrence and utilization of information from different scales. Two innovations are proposed to address this issue. First, we propose a Pyramid of Score Distribution Feature (PSDF) to capture the motion information at multiple resolutions centered at each detection window. This novel feature mitigates the influence of unknown action position and duration, and shows significant performance gain over previous detection approaches. Second, inter-frame consistency is further explored by incorporating PSDF into the state-of-the-art Recurrent Neural Networks, which gives additional performance gain in detecting actions in temporally untrimmed videos. We tested our action localization framework on the THUMOS'15 and MPII Cooking Activities Dataset, both of which show a large performance improvement over previous attempts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yuan_Temporal_Action_Localization_CVPR_2016_paper.pdf",
        "aff": "National University of Singapore; Shanghai Jiao Tong University; Shanghai Jiao Tong University; National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 582594,
        "gs_citation": 232,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13789766527418798408&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "nus.edu.sg;sjtu.edu.cn;sjtu.edu.cn;nus.edu.sg",
        "email": "nus.edu.sg;sjtu.edu.cn;sjtu.edu.cn;nus.edu.sg",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yuan_Temporal_Action_Localization_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "National University of Singapore;Shanghai Jiao Tong University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "NUS;SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "title": "Temporal Action Localization in Untrimmed Videos via Multi-Stage CNNs",
        "session": "Events, Activities, and Surveillance",
        "status": "Poster",
        "track": "main",
        "pid": "31",
        "author_site": "Zheng Shou, Dongang Wang, Shih-Fu Chang",
        "author": "Zheng Shou; Dongang Wang; Shih-Fu Chang",
        "abstract": "We address temporal action localization in untrimmed long videos. This is important because videos in real applications are usually unconstrained and contain multiple action instances plus video content of background scenes or other activities. To address this challenging issue, we exploit the effectiveness of deep networks in temporal action localization via three segment-based 3D ConvNets: (1) a proposal network identifies candidate segments in a long video that may contain actions; (2) a classification network learns one-vs-all action classification model to serve as initialization for the localization network; and (3) a localization network fine-tunes the learned classification network to localize each action instance. We propose a novel loss function for the localization network to explicitly consider temporal overlap and achieve high temporal localization accuracy. In the end, only the proposal network and the localization network are used during prediction. On two large-scale benchmarks, our approach achieves significantly superior performances compared with other state-of-the-art systems: mAP increases from 1.7% to 7.4% on MEXaction2 and increases from 15.0% to 19.0% on THUMOS 2014.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shou_Temporal_Action_Localization_CVPR_2016_paper.pdf",
        "aff": "Columbia University; Columbia University; Columbia University",
        "project": "",
        "github": "https://github.com/zhengshou/scnn/",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1007910,
        "gs_citation": 1194,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4402437974533685345&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "columbia.edu;columbia.edu;columbia.edu",
        "email": "columbia.edu;columbia.edu;columbia.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shou_Temporal_Action_Localization_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Temporal Epipolar Regions",
        "session": "Feature Matching and Indexing",
        "status": "Poster",
        "track": "main",
        "pid": "49",
        "author_site": "Mor Dar, Yael Moses",
        "author": "Mor Dar; Yael Moses",
        "abstract": "Dynamic events are often photographed by a number of people from different viewpoints at different times, resulting in an unconstrained set of images. Finding the corresponding moving features in each of the images allows us to extract information about objects of interest in the scene. Computing correspondence of moving features in such a set of images is considerably more challenging than computing correspondence in video due to possible significant differences in viewpoints and inconsistent timing between image captures. The prediction methods used in video for improving robustness and efficiency are not applicable to a set of still images. In this paper we propose a novel method to predict locations of an approximately linear moving feature point, given a small subset of correspondences and the temporal order of image captures. Our method extends the use of epipolar geometry to divide images into valid and invalid regions, termed Temporal Epipolar Regions (TERs). We formally prove that the location of a feature in a new image is restricted to valid TERs. We demonstrate the effectiveness of our method in reducing the search space for correspondence on both synthetic and challenging real world data, and show the improved matching.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Dar_Temporal_Epipolar_Regions_CVPR_2016_paper.pdf",
        "aff": "E\ufb01 Arazi School of Computer Science, The Interdisciplinary Center, Herzliya 46150, Israel; E\ufb01 Arazi School of Computer Science, The Interdisciplinary Center, Herzliya 46150, Israel",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Dar_Temporal_Epipolar_Regions_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1859542,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6680886547674707454&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff_domain": "post.idc.ac.il;idc.ac.il",
        "email": "post.idc.ac.il;idc.ac.il",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Dar_Temporal_Epipolar_Regions_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Interdisciplinary Center",
        "aff_unique_dep": "E\ufb01 Arazi School of Computer Science",
        "aff_unique_url": "https://www.idc.ac.il",
        "aff_unique_abbr": "IDC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Herzliya",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "Temporal Multimodal Learning in Audiovisual Speech Recognition",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "59",
        "author_site": "Di Hu, Xuelong Li, Xiaoqiang lu",
        "author": "Di Hu; Xuelong Li; Xiaoqiang lu",
        "abstract": "In view of the advantages of deep networks in producing useful representation, the generated features of different modality data (such as image, audio) can be jointly learned using Multimodal Restricted Boltzmann Machines (MRBM). Recently, audiovisual speech recognition based the MRBM has attracted much attention, and the MRBM shows its effectiveness in learning the joint representation across audiovisual modalities. However, the built networks have weakness in modeling the multimodal sequence which is the natural property of speech signal. In this paper, we will introduce a novel temporal multimodal deep learning architecture, named as Recurrent Temporal Multimodal RBM (RTMRBM), that models multimodal sequences by transforming the sequence of connected MRBMs into a probabilistic series model. Compared with existing multimodal networks, it's simple and efficient in learning temporal joint representation. We evaluate our model on audiovisual speech datasets, two public (AVLetters and AVLetters2) and one self-build. The experimental results demonstrate that our approach can obviously improve the accuracy of recognition compared with standard MRBM and the temporal model based on conditional RBM. In addition, RTMRBM still outperforms non-temporal multimodal deep networks in the presence of the weakness of long-term dependencies.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Hu_Temporal_Multimodal_Learning_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi\u2019an 710072, P. R. China; Xi\u2019an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi\u2019an 710119, P. R. China; Xi\u2019an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi\u2019an 710119, P. R. China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 487253,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13652862042706834557&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "mail.nwpu.edu.cn;opt.ac.cn;opt.ac.cn",
        "email": "mail.nwpu.edu.cn;opt.ac.cn;opt.ac.cn",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Hu_Temporal_Multimodal_Learning_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Northwestern Polytechnical University;Xi\u2019an Institute of Optics and Precision Mechanics",
        "aff_unique_dep": "School of Computer Science;Chinese Academy of Sciences",
        "aff_unique_url": "https://www.nwpu.edu.cn;",
        "aff_unique_abbr": "NWPU;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Xi'an",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "title": "Temporally Coherent 4D Reconstruction of Complex Dynamic Scenes",
        "session": "Non-Rigid Reconstruction and Motion Analysis",
        "status": "Oral",
        "track": "main",
        "pid": "12",
        "author_site": "Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, Adrian Hilton",
        "author": "Armin Mustafa; Hansung Kim; Jean-Yves Guillemaut; Adrian Hilton",
        "abstract": "This paper presents an approach for reconstruction of 4D temporally coherent models of complex dynamic scenes. No prior knowledge is required of scene structure or camera calibration allowing reconstruction from multiple moving cameras. Sparse-to-dense temporal correspondence is integrated with joint multi-view segmentation and reconstruction to obtain a complete 4D representation of static and dynamic objects. Temporal coherence is exploited to overcome visual ambiguities resulting in improved reconstruction of complex scenes. Robust joint segmentation and reconstruction of dynamic objects is achieved by introducing a geodesic star convexity constraint. Comparative evaluation is performed on a variety of unstructured indoor and outdoor dynamic scenes with hand-held cameras and multiple people. This demonstrates reconstruction of complete temporally coherent 4D scene models with improved non-rigid object segmentation and shape reconstruction.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mustafa_Temporally_Coherent_4D_CVPR_2016_paper.pdf",
        "aff": "CVSSP, University of Surrey, Guildford, United Kingdom; CVSSP, University of Surrey, Guildford, United Kingdom; CVSSP, University of Surrey, Guildford, United Kingdom; CVSSP, University of Surrey, Guildford, United Kingdom",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Mustafa_Temporally_Coherent_4D_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2552743,
        "gs_citation": 91,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2085678718790682266&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 20,
        "aff_domain": "surrey.ac.uk; ; ; ",
        "email": "surrey.ac.uk; ; ; ",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mustafa_Temporally_Coherent_4D_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Surrey",
        "aff_unique_dep": "CVSSP",
        "aff_unique_url": "https://www.surrey.ac.uk",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Guildford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "TenSR: Multi-Dimensional Tensor Sparse Representation",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "66",
        "author_site": "Na Qi, Yunhui Shi, Xiaoyan Sun, Baocai Yin",
        "author": "Na Qi; Yunhui Shi; Xiaoyan Sun; Baocai Yin",
        "abstract": "The conventional sparse model relies on data representation in the form of vectors. It represents the vector-valued or vectorized one dimensional (1D) version of an signal as a highly sparse linear combination of basis atoms from a large dictionary. The 1D modeling, though simple, ignores the inherent structure and breaks the local correlation inside multidimensional (MD) signals. It also dramatically increases the demand of memory as well as computational resources especially when dealing with high dimensional signals. In this paper, we propose a new sparse model TenSR based on tensor for MD data representation along with the corresponding MD sparse coding and MD dictionary learning algorithms. The proposed TenSR model is able to well approximate the structure in each mode inherent in MD signals with a series of adaptive separable structure dictionaries via dictionary learning. The proposed MD sparse coding algorithm by proximal method further reduces the computational cost significantly. Experimental results with real world MD signals, i.e. 3D Multi-spectral images, show the proposed TenSR greatly reduces both the computational and memory costs with competitive performance in comparison with the state-of-the-art sparse representation methods. We believe our proposed TenSR model is a promising way to empower the sparse representation especially for large scale high order signals.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Qi_TenSR_Multi-Dimensional_Tensor_CVPR_2016_paper.pdf",
        "aff": "Beijing Key Laboratory of Multimedia and Intelligent Software Technology, College of Metropolitan Transportation, Beijing University of Technology; Beijing Key Laboratory of Multimedia and Intelligent Software Technology, College of Metropolitan Transportation, Beijing University of Technology; Microsoft Research; Beijing Key Laboratory of Multimedia and Intelligent Software Technology, College of Metropolitan Transportation, Beijing University of Technology + Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2041349,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4640340186283209035&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "emails.bjut.edu.cn;bjut.edu.cn;microsoft.com;bjut.edu.cn",
        "email": "emails.bjut.edu.cn;bjut.edu.cn;microsoft.com;bjut.edu.cn",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Qi_TenSR_Multi-Dimensional_Tensor_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0+2",
        "aff_unique_norm": "Beijing University of Technology;Microsoft;Dalian University of Technology",
        "aff_unique_dep": "College of Metropolitan Transportation;Microsoft Research;Faculty of Electronic Information and Electrical Engineering",
        "aff_unique_url": "http://www.bjut.edu.cn;https://www.microsoft.com/en-us/research;http://en.dlut.edu.cn/",
        "aff_unique_abbr": "BJUT;MSR;DUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Tensor Power Iteration for Multi-Graph Matching",
        "session": "Shape Representations and Matching",
        "status": "Poster",
        "track": "main",
        "pid": "55",
        "author_site": "Xinchu Shi, Haibin Ling, Weiming Hu, Junliang Xing, Yanning Zhang",
        "author": "Xinchu Shi; Haibin Ling; Weiming Hu; Junliang Xing; Yanning Zhang",
        "abstract": "Due to its wide range of applications, matching between two graphs has been extensively studied and remains an active topic. By contrast, it is still under-exploited on how to jointly match multiple graphs, partly due to its intrinsic computational intractability. In this work, we address this challenging problem in a principled way under the rank-1 tensor approximation framework. In particular, we formulate multi-graph matching as a combinational optimization problem with two main ingredients: unary matching over graph vertices and structure matching over graph edges, both of which across multiple graphs. Then we propose an efficient power iteration solution for the resulted NP-hard optimization problem. The proposed algorithm has several advantages: 1) the intrinsic matching consistency across multiple graphs based on the high-order tensor optimization; 2) the free employment of powerful high-order node affinity; 3) the flexible integration between various types of node affinities and edge/hyper-edge affinities. Experiments on diverse and challenging datasets validate the effectiveness of the proposed approach in comparison with state-of-the-arts.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shi_Tensor_Power_Iteration_CVPR_2016_paper.pdf",
        "aff": "CAS Center for Excellence in Brain Science and Intelligence Technology, National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China; Department of Computer and Information Sciences, Temple University, Philadelphia, USA; CAS Center for Excellence in Brain Science and Intelligence Technology, National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China; CAS Center for Excellence in Brain Science and Intelligence Technology, National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China; School of Computer Science, Northwestern Polytechnical University, Xian, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1126607,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10992044417415676863&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "nlpr.ia.ac.cn;temple.edu;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nwpu.edu.cn",
        "email": "nlpr.ia.ac.cn;temple.edu;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nwpu.edu.cn",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shi_Tensor_Power_Iteration_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;0;2",
        "aff_unique_norm": "Chinese Academy of Sciences;Temple University;Northwestern Polytechnical University",
        "aff_unique_dep": "Center for Excellence in Brain Science and Intelligence Technology;Department of Computer and Information Sciences;School of Computer Science",
        "aff_unique_url": "https://www.cas.cn;https://www.temple.edu;https://www.nwpu.edu.cn",
        "aff_unique_abbr": "CAS;Temple;NPU",
        "aff_campus_unique_index": "0;1;0;0;2",
        "aff_campus_unique": "Beijing;Philadelphia;Xian",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Tensor Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Tensors via Convex Optimization",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "75",
        "author_site": "Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, Shuicheng Yan",
        "author": "Canyi Lu; Jiashi Feng; Yudong Chen; Wei Liu; Zhouchen Lin; Shuicheng Yan",
        "abstract": "This paper studies the Tensor Robust Principal Component (TRPCA) problem which extends the known Robust PCA to the tensor case. Our model is based on a new tensor Singular Value Decomposition (t-SVD)  and its induced tensor tubal rank and tensor nuclear norm. Consider that we have a 3-way tensor X in R^n*n*n_3 such that X=L_0+S_0, where  L_0 has low tubal rank and S_0 is sparse. Is that possible to recover both components? In this work, we prove that under certain suitable assumptions, we can recover both the low-rank and the sparse components exactly by simply solving a convex program  whose objective is a weighted combination of the tensor nuclear norm and the l1-norm, i.e.,  min L,E s.t. ||L||_*+lambda||E||_1 s.t. X=L+E. where lambda=1/sqrtmax(n_1,n_2)n_3. Interestingly, TRPCA involves RPCA as a special case when n_3=1 and thus it is a simple and elegant tensor extension of RPCA. Also numerical experiments verify our theory and the application for the image denoising demonstrates the effectiveness of our method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lu_Tensor_Robust_Principal_CVPR_2016_paper.pdf",
        "aff": "Department of Electrical and Computer Engineering, National University of Singapore; Department of Electrical and Computer Engineering, National University of Singapore; School of Operations Research and Information Engineering, Cornell University; Didi Research; Key Laboratory of Machine Perception (MOE), School of EECS, Peking University + Cooperative Medianet Innovation Center, Shanghai Jiaotong University; 360 AI Institute + Department of Electrical and Computer Engineering, National University of Singapore",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1164705,
        "gs_citation": 627,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10221511503986256024&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "gmail.com;nus.edu.sg;cornell.edu;ee.columbia.edu;pku.edu.cn;nus.edu.sg",
        "email": "gmail.com;nus.edu.sg;cornell.edu;ee.columbia.edu;pku.edu.cn;nus.edu.sg",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lu_Tensor_Robust_Principal_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;2;3+4;5+0",
        "aff_unique_norm": "National University of Singapore;Cornell University;Didi Research;Peking University;Shanghai Jiao Tong University;360 AI Institute",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;School of Operations Research and Information Engineering;;School of EECS;Cooperative Medianet Innovation Center;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.cornell.edu;https://www.didi.com;http://www.pku.edu.cn;https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "NUS;Cornell;Didi;PKU;SJTU;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2;2+2;2+0",
        "aff_country_unique": "Singapore;United States;China"
    },
    {
        "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding",
        "session": "Semantic Parsing and Segmentation",
        "status": "Spotlight",
        "track": "main",
        "pid": "20",
        "author_site": "Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele",
        "author": "Marius Cordts; Mohamed Omran; Sebastian Ramos; Timo Rehfeld; Markus Enzweiler; Rodrigo Benenson; Uwe Franke; Stefan Roth; Bernt Schiele",
        "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes.  To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.pdf",
        "aff": "Daimler AG R&D; TU Darmstadt; MPI Informatics; TU Dresden; Daimler AG R&D; MPI Informatics; Daimler AG R&D; TU Darmstadt; MPI Informatics",
        "project": "www.cityscapes-dataset.net",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Cordts_The_Cityscapes_Dataset_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 727866,
        "gs_citation": 15494,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1394466002617224745&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 21,
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "author_num": 9,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;3;0;2;0;1;2",
        "aff_unique_norm": "Daimler AG;Technische Universit\u00e4t Darmstadt;Max Planck Institute for Informatics;Technische Universit\u00e4t Dresden",
        "aff_unique_dep": "Research and Development;;Informatics;",
        "aff_unique_url": "https://www.daimler.com;https://www.tu-darmstadt.de;https://www.mpi-inf.mpg.de;https://www.tu-dresden.de",
        "aff_unique_abbr": "Daimler;TU Darmstadt;MPII;TUD",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Darmstadt",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "The Global Patch Collider",
        "session": "Matching and Alignment",
        "status": "Oral",
        "track": "main",
        "pid": "14",
        "author_site": "Shenlong Wang, Sean Ryan Fanello, Christoph Rhemann, Shahram Izadi, Pushmeet Kohli",
        "author": "Shenlong Wang; Sean Ryan Fanello; Christoph Rhemann; Shahram Izadi; Pushmeet Kohli",
        "abstract": "This paper proposes a novel extremely efficient, fully-parallelizable, task-specific algorithm for the computation of global point-wise correspondences in images and videos. Our algorithm, the Global Patch Collider, is based on detecting unique collisions between image points using a collection of learned tree structures that act as conditional hash functions. In contrast to conventional approaches that rely on pairwise distance computation, our algorithm isolates distinctive pixel pairs that hit the same leaf during traversal through multiple learned tree structures. The split functions stored at the intermediate nodes of the trees are trained to ensure that only visually similar patches or their geometric or photometric transformed versions fall into the same leaf node. The matching process involves passing all pixel positions in the images under analysis through the tree structures. We then compute matches by isolating points that uniquely collide with each other  ie. fell in the same empty leaf in multiple trees. Our algorithm is linear in the number of pixels but can be made constant time on a parallel computation architecture as the tree traversal for individual image points is decoupled. We demonstrate the efficacy of our method by using it to perform optical flow matching and stereo matching on some challenging benchmarks. Experimental results show that not only is our method extremely computationally efficient, but it is also able to match or outperform state of the art methods that are much more complex.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_The_Global_Patch_CVPR_2016_paper.pdf",
        "aff": "Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research + University of Toronto",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 6846062,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17857487985073018650&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_The_Global_Patch_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "Microsoft;University of Toronto",
        "aff_unique_dep": "Microsoft Research;",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.utoronto.ca",
        "aff_unique_abbr": "MSR;U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+1",
        "aff_country_unique": "United States;Canada"
    },
    {
        "title": "The MegaFace Benchmark: 1 Million Faces for Recognition at Scale",
        "session": "Face Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "35",
        "author_site": "Ira Kemelmacher-Shlizerman, Steven M. Seitz, Daniel Miller, Evan Brossard",
        "author": "Ira Kemelmacher-Shlizerman; Steven M. Seitz; Daniel Miller; Evan Brossard",
        "abstract": "Recent face recognition experiments on a major benchmark LFW show stunning performance--a number of algorithms achieve near to perfect score, surpassing human recognition rates. In this paper, we advocate evaluations at the million scale (LFW includes only 13K photos of 5K people). To this end, we have assembled the MegaFace dataset and created the first MegaFace challenge. Our dataset includes One Million photos that capture more than 690K different individuals. The challenge evaluates performance of algorithms with increasing numbers of \"distractors\" (going from 10 to 1M) in the gallery set. We present both identification and verification performance, evaluate performance with respect to pose and a person's age, and compare as a function of training data size (#photos and  #people).  We report results of state of the art  and baseline algorithms. The MegaFace dataset, baseline code, and evaluation scripts, are all publicly released for further experimentations at http://megaface.cs.washington.edu.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kemelmacher-Shlizerman_The_MegaFace_Benchmark_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2357076,
        "gs_citation": 1109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6051410257476935491&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kemelmacher-Shlizerman_The_MegaFace_Benchmark_CVPR_2016_paper.html"
    },
    {
        "title": "The Multiverse Loss for Robust Transfer Learning",
        "session": "Statistical Methods and Transfer Learning",
        "status": "Spotlight",
        "track": "main",
        "pid": "18",
        "author_site": "Etai Littwin, Lior Wolf",
        "author": "Etai Littwin; Lior Wolf",
        "abstract": "Deep learning techniques are renowned for supporting effective transfer learning. However, as we demonstrate, the transferred representations support only a few modes of separation and much of its dimensionality is unutilized. In this work we suggest to learn, in the source domain, multiple orthogonal classifiers. We prove that this leads to a reduced rank representation, which however supports more  discriminative directions. Interestingly, the softmax probabilities produced by the multiple classifiers are likely to be identical. Extensive experimental results further demonstrate the effectiveness of our method.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Littwin_The_Multiverse_Loss_CVPR_2016_paper.pdf",
        "aff": "Department of Electrical Engineering, Tel-Aviv University; The Blavatnik School of Computer Science, Tel Aviv University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Littwin_The_Multiverse_Loss_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 591187,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10151461815314088575&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "; ",
        "email": "; ",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Littwin_The_Multiverse_Loss_CVPR_2016_paper.html",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Tel-Aviv University;Tel Aviv University",
        "aff_unique_dep": "Department of Electrical Engineering;Blavatnik School of Computer Science",
        "aff_unique_url": "https://www.tau.ac.il;https://www.tau.ac.il",
        "aff_unique_abbr": "TAU;TAU",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Tel-Aviv;Tel Aviv",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "The Next Best Underwater View",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "79",
        "author_site": "Mark Sheinin, Yoav Y. Schechner",
        "author": "Mark Sheinin; Yoav Y. Schechner",
        "abstract": "To image in high resolution large and occlusion-prone scenes, a camera must move above and around. Degradation of visibility due to geometric occlusions and distances is exacerbated by scattering, when the scene is in a participating medium. Moreover, underwater and in other media, artificial lighting is needed. Overall, data quality depends on the observed surface, medium and the time-varying poses of the camera and light source. This work proposes to optimize camera and light poses as they move, so that the surface is scanned efficiently and the descattered recovery has the highest quality. The work generalizes the next best view concept of robot vision to scattering media and cooperative movable lighting. It also extends descattering to platforms that move optimally. The optimization criterion is information gain, taken from information theory. We exploit the existence of a prior rough 3D model, since underwater such a model is routinely obtained using sonar. We demonstrate this principle in a scaled-down setup.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sheinin_The_Next_Best_CVPR_2016_paper.pdf",
        "aff": "Viterbi Faculty of Electrical Engineering, Technion - Israel Inst. of Technology, Haifa 32000, Israel; Viterbi Faculty of Electrical Engineering, Technion - Israel Inst. of Technology, Haifa 32000, Israel",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Sheinin_The_Next_Best_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4032541,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2345753008773981003&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "campus.technion.ac.il;ee.technion.ac.il",
        "email": "campus.technion.ac.il;ee.technion.ac.il",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sheinin_The_Next_Best_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "Viterbi Faculty of Electrical Engineering",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Haifa",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes",
        "session": "Semantic Parsing and Segmentation",
        "status": "Spotlight",
        "track": "main",
        "pid": "22",
        "author_site": "German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, Antonio M. L\u00f3pez",
        "author": "German Ros; Laura Sellart; Joanna Materzynska; David Vazquez; Antonio M. Lopez",
        "abstract": "Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images; thus, having a sufficient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for  semantic segmentation -- in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show how the inclusion of SYNTHIA in the training stage significantly improves performance on the semantic segmentation task.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf",
        "aff": "Computer Vision Center+Computer Science Dept.; Computer Vision Center; Faculty of Mathematics; Computer Vision Center; Computer Vision Center+Computer Science Dept.",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 8735986,
        "gs_citation": 2880,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9178628328030932213&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": "cvc.uab.es;cvc.uab.es;unet.univie.ac.at;cvc.uab.es;cvc.uab.es",
        "email": "cvc.uab.es;cvc.uab.es;unet.univie.ac.at;cvc.uab.es;cvc.uab.es",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0;2;0;0+1",
        "aff_unique_norm": "Computer Vision Center;Computer Science Department;Faculty of Mathematics",
        "aff_unique_dep": ";Computer Science;Mathematics",
        "aff_unique_url": "https://www.cvc.uab.cat/;;",
        "aff_unique_abbr": "CVC;;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Spain;"
    },
    {
        "title": "The Solution Path Algorithm for Identity-Aware Multi-Object Tracking",
        "session": "Video Analysis 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "9",
        "author_site": "Shoou-I Yu, Deyu Meng, Wangmeng Zuo, Alexander Hauptmann",
        "author": "Shoou-I Yu; Deyu Meng; Wangmeng Zuo; Alexander Hauptmann",
        "abstract": "We propose an identity-aware multi-object tracker based on the solution path algorithm. Our tracker not only produces identity-coherent trajectories based on cues such as face recognition, but also has the ability to pinpoint potential tracking errors. The tracker is formulated as a quadratic optimization problem with L0 norm constraints, which we propose to solve with the solution path algorithm. The algorithm successively solves the same optimization problem but under different Lp norm constraints, where p gradually decreases from 1 to 0. Inspired by the success of the solution path algorithm in various machine learning tasks, this strategy is expected to converge to a better local minimum than directly minimizing the hardly solvable L0 norm or the roughly approximated L1 norm constraints. Furthermore, the acquired solution path complies with the \"decision making process\" of the tracker, which provides more insight to locating potential tracking errors. Experiments show that not only is our proposed tracker effective, but also the solution path enables automatic pinpointing of potential tracking failures, which can be readily utilized in an active learning framework to improve identity-aware multi-object tracking.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yu_The_Solution_Path_CVPR_2016_paper.pdf",
        "aff": "Carnegie Mellon University; Xi\u2019an Jiaotong University; Harbin Institute of Technology; Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Yu_The_Solution_Path_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1160106,
        "gs_citation": 51,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1574250711095122034&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "cs.cmu.edu;mail.xjtu.edu.cn;hit.edu.cn;cs.cmu.edu",
        "email": "cs.cmu.edu;mail.xjtu.edu.cn;hit.edu.cn;cs.cmu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yu_The_Solution_Path_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Carnegie Mellon University;Xi'an Jiao Tong University;Harbin Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.xjtu.edu.cn;http://www.hit.edu.cn/",
        "aff_unique_abbr": "CMU;XJTU;HIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Harbin",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Theory and Practice of Structure-From-Motion Using Affine Correspondences",
        "session": "3D Reconstruction",
        "status": "Spotlight",
        "track": "main",
        "pid": "19",
        "author_site": "Carolina Raposo, Jo\u00e3o P. Barreto",
        "author": "Carolina Raposo; Joao P. Barreto",
        "abstract": "Affine Correspondences (ACs) are more informative than Point Correspondences (PCs) that are used as input in mainstream algorithms for Structure-from-Motion (SfM). Since ACs enable to estimate models from fewer correspondences, its use can dramatically reduce the number of combinations during the iterative step of sample-and-test that exists in most SfM pipelines. However, using ACs instead of PCs as input for SfM passes by fully understanding the relations between ACs and multi-view geometry, as well as by establishing practical, effective AC-based algorithms. This article is a step forward into this direction, by providing a clear account about how ACs constrain the two-view geometry, and by proposing new algorithms for plane segmentation and visual odometry that compare favourably with respect to methods relying in PCs.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Raposo_Theory_and_Practice_CVPR_2016_paper.pdf",
        "aff": "Institute of Systems and Robotics, University of Coimbra; Institute of Systems and Robotics, University of Coimbra",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3746124,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8316099389518652212&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "isr.uc.pt;isr.uc.pt",
        "email": "isr.uc.pt;isr.uc.pt",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Raposo_Theory_and_Practice_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Coimbra",
        "aff_unique_dep": "Institute of Systems and Robotics",
        "aff_unique_url": "https://www.uc.pt",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Portugal"
    },
    {
        "title": "They Are Not Equally Reliable: Semantic Event Search Using Differentiated Concept Classifiers",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "42",
        "author_site": "Xiaojun Chang, Yao-Liang Yu, Yi Yang, Eric P. Xing",
        "author": "Xiaojun Chang; Yao-Liang Yu; Yi Yang; Eric P. Xing",
        "abstract": "Complex event detection on unconstrained Internet videos has seen much progress in recent years. However, state-of-the-art performance degrades dramatically when the number of positive training exemplars falls short. Since label acquisition is costly, laborious, and time-consuming, there is a real need to consider the much more challenging semantic event search problem, where no  example video is given. In this paper, we present a state-of-the-art event search system without any example videos. Relying on the key observation that events (e.g. dog show) are usually compositions of multiple mid-level concepts (e.g. \"dog,\" \"theater,\" and \"dog jumping\"), we first train a skip-gram model to measure the relevance of each concept with the event of interest. The relevant concept classifiers then cast votes on the test videos but their reliability, due to lack of labeled training videos, has been largely unaddressed. We propose to combine the concept classifiers based on a principled estimate of their accuracy on the unlabeled test videos. A novel warping technique is proposed to improve the performance and an efficient highly-scalable algorithm is provided to quickly solve the resulting optimization. We conduct extensive experiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV datasets, and achieve state-of-the-art performances.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chang_They_Are_Not_CVPR_2016_paper.pdf",
        "aff": "Centre for Quantum Computation and Intelligent Systems, University of Technology Sydney; Machine Learning Department, Carnegie Mellon University; Centre for Quantum Computation and Intelligent Systems, University of Technology Sydney; Machine Learning Department, Carnegie Mellon University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1203396,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11839742106809251719&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "gmail.com;cs.cmu.edu;uts.edu.au;cs.cmu.edu",
        "email": "gmail.com;cs.cmu.edu;uts.edu.au;cs.cmu.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chang_They_Are_Not_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Technology Sydney;Carnegie Mellon University",
        "aff_unique_dep": "Centre for Quantum Computation and Intelligent Systems;Machine Learning Department",
        "aff_unique_url": "https://www.uts.edu.au;https://www.cmu.edu",
        "aff_unique_abbr": "UTS;CMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Sydney;",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "Australia;United States"
    },
    {
        "title": "Thin-Slicing for Pose: Learning to Understand Pose Without Explicit Pose Estimation",
        "session": "Human Pose Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "42",
        "author_site": "Suha Kwak, Minsu Cho, Ivan Laptev",
        "author": "Suha Kwak; Minsu Cho; Ivan Laptev",
        "abstract": "We address the problem of learning a pose-aware, compact embedding that projects images with similar human poses to be placed close-by in the embedding space. The embedding function is built on a deep convolutional network, and trained with triplet-based rank constraints on real image data. This architecture allows us to learn a robust representation that captures  differences in human poses by effectively factoring out variations in clothing, background, and imaging conditions in the wild. For a variety of pose-related tasks, the proposed pose embedding provides a cost-efficient and natural alternative to explicit pose estimation, circumventing challenges of localizing body joints. We demonstrate the efficacy of the embedding on pose-based image retrieval and action recognition problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kwak_Thin-Slicing_for_Pose_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1381939,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2546703641254713918&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kwak_Thin-Slicing_for_Pose_CVPR_2016_paper.html"
    },
    {
        "title": "Three-Dimensional Object Detection and Layout Prediction Using Clouds of Oriented Gradients",
        "session": "Recognition and Parsing In 3D",
        "status": "Oral",
        "track": "main",
        "pid": "3",
        "author_site": "Zhile Ren, Erik B. Sudderth",
        "author": "Zhile Ren; Erik B. Sudderth",
        "abstract": "We develop new representations and algorithms for three-dimensional (3D) object detection and spatial layout prediction in cluttered indoor scenes. RGB-D images are traditionally described by local geometric features of the 3D point cloud. We propose a cloud of oriented gradient (COG) descriptor that links the 2D appearance and 3D pose of object categories, and thus accurately models how perspective projection affects perceived image boundaries. We also propose a \"Manhattan voxel\" representation which better captures the 3D room layout geometry of common indoor environments.  Effective classification rules are learned via a structured prediction framework that accounts for the intersection-over-union overlap of hypothesized 3D cuboids with human annotations, as well as orientation estimation errors. Contextual relationships among categories and layout are captured via a cascade of classifiers, leading to holistic scene hypotheses with improved accuracy. Our model is learned solely from annotated RGB-D images, without the benefit of CAD models, but nevertheless its performance substantially exceeds the state-of-the-art on the SUN RGB-D database. Avoiding CAD models allows easier learning of detectors for many object categories.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Ren_Three-Dimensional_Object_Detection_CVPR_2016_paper.pdf",
        "aff": "Department of Computer Science, Brown University, Providence, RI 02912, USA; Department of Computer Science, Brown University, Providence, RI 02912, USA",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Ren_Three-Dimensional_Object_Detection_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 27250839,
        "gs_citation": 180,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9474004503583403273&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";",
        "email": ";",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Ren_Three-Dimensional_Object_Detection_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Providence",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Top-Push Video-Based Person Re-Identification",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "62",
        "author_site": "Jinjie You, Ancong Wu, Xiang Li, Wei-Shi Zheng",
        "author": "Jinjie You; Ancong Wu; Xiang Li; Wei-Shi Zheng",
        "abstract": "Most existing person re-identification (re-id) models focus on matching still person images across disjoint camera views using the setting of either single-shot or multi-shot. Since limited information can be exploited from still images, it is hard (if not impossible) to overcome the occlusion, pose and camera-view change, and lighting variation problems. In comparison, video-based re-id methods can utilize extra space-time information, which contains much more rich cues for matching to overcome the mentioned problems. However, in this work, we find that when using video-based representation, some inter-class difference can be much more obscure than the one when using still-image-based representation, because different people could not only have similar appearance but also may have similar motions and actions which are hard to align. To solve this problem, we propose a top-push distance learning model (TDL), in which we integrate a top-push constrain, for matching video features of persons. The top-push constraint enforces the optimization on top-rank matching in re-id, so as to make the matching model more effective towards selecting more discriminative features to distinguish different persons. Our experiments show that the proposed video-based re-id framework outperforms the state-of-the-art video-based re-id methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/You_Top-Push_Video-Based_Person_CVPR_2016_paper.pdf",
        "aff": "Intelligence Science and System Lab, Sun Yat-sen University, China+Guangdong Provincial Key Laboratory of Computational Science, China; Intelligence Science and System Lab, Sun Yat-sen University, China+Guangdong Provincial Key Laboratory of Computational Science, China; Intelligence Science and System Lab, Sun Yat-sen University, China+Guangdong Provincial Key Laboratory of Computational Science, China; School of Data and Computer Science, Sun Yat-sen University, China+Guangdong Provincial Key Laboratory of Computational Science, China",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 995219,
        "gs_citation": 310,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12818327390883735215&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "gmail.com;mail2.sysu.edu.cn;gmail.com;ieee.org",
        "email": "gmail.com;mail2.sysu.edu.cn;gmail.com;ieee.org",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/You_Top-Push_Video-Based_Person_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Sun Yat-sen University;Guangdong Provincial Key Laboratory of Computational Science",
        "aff_unique_dep": "Intelligence Science and System Lab;Computational Science",
        "aff_unique_url": "http://www.sysu.edu.cn/;",
        "aff_unique_abbr": "SYSU;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Towards Open Set Deep Networks",
        "session": "Recognition Beyond Objects",
        "status": "Spotlight",
        "track": "main",
        "pid": "7",
        "author_site": "Abhijit Bendale, Terrance E. Boult",
        "author": "Abhijit Bendale; Terrance E. Boult",
        "abstract": "Deep networks have produced significant gains for various visual recognition problems, leading to high impact academic and commercial applications.  Recent work in deep networks highlighted that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images high confidence as that given class - deep network are easily fooled with images humans do not consider meaningful.  The closed set nature of deep networks forces them to choose from one of the known classes leading to such artifacts.  Recognition in the real world is open set, i.e. the recognition system should reject unknown/unseen classes at test time. We present a methodology to adapt deep networks for open set recognition, by introducing a new model layer, OpenMax, which estimates the probability of an input being from an unknown class.  A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. OpenMax allows rejection of \"fooling\" and unrelated open set images presented to the system; OpenMax greatly reduces the number of  obvious errors made by a deep network.  We prove that the OpenMax concept provides bounded open space risk, thereby formally providing an open set recognition solution. We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation data, and thousands of fooling and open set images. The proposed OpenMax model significantly outperforms open set recognition accuracy of basic deep networks as well as deep networks with thresholding of SoftMax probabilities.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bendale_Towards_Open_Set_CVPR_2016_paper.pdf",
        "aff": "University of Colorado at Colorado Springs + Samsung Research America; University of Colorado at Colorado Springs",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Bendale_Towards_Open_Set_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 4118197,
        "gs_citation": 1979,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3571743951915089896&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "vast.uccs.edu;vast.uccs.edu",
        "email": "vast.uccs.edu;vast.uccs.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bendale_Towards_Open_Set_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "University of Colorado;Samsung",
        "aff_unique_dep": ";Samsung Research America",
        "aff_unique_url": "https://www.uccs.edu;https://www.samsung.com/us/careers/research/",
        "aff_unique_abbr": "UC-CS;SRA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Colorado Springs;",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Trace Quotient Meets Sparsity: A Method for Learning Low Dimensional Image Representations",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "77",
        "author_site": "Xian Wei, Hao Shen, Martin Kleinsteuber",
        "author": "Xian Wei; Hao Shen; Martin Kleinsteuber",
        "abstract": "This paper presents an algorithm that allows to learn low dimensional representations of images in an unsupervised manner. The core idea is to combine two criteria that play important roles in unsupervised representation learning, namely sparsity and trace quotient. The former is known to be a convenient tool to identify underlying factors, and the latter is known as a disentanglement of underlying discriminative factors. In this work, we develop a generic cost function for learning jointly a sparsifying dictionary and a dimensionality reduction transformation. It leads to several counterparts of classic low dimensional representation methods, such as Principal Component Analysis, Local Linear Embedding, and Laplacian Eigenmap. Our proposed optimisation algorithm leverages the efficiency of geometric optimisation on Riemannian manifolds and a closed form solution to the elastic net problem.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wei_Trace_Quotient_Meets_CVPR_2016_paper.pdf",
        "aff": "Department of Electrical Engineering and Information Technology, Technische Universit \u00a8at M \u00a8unchen, Arcisstr. 21, 80333 Munich, Germany; Department of Electrical Engineering and Information Technology, Technische Universit \u00a8at M \u00a8unchen, Arcisstr. 21, 80333 Munich, Germany; Department of Electrical Engineering and Information Technology, Technische Universit \u00a8at M \u00a8unchen, Arcisstr. 21, 80333 Munich, Germany",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 18096742,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14383191843575738261&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": "tum.de;tum.de;tum.de",
        "email": "tum.de;tum.de;tum.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wei_Trace_Quotient_Meets_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technische Universit\u00e4t M\u00fcnchen",
        "aff_unique_dep": "Department of Electrical Engineering and Information Technology",
        "aff_unique_url": "https://www.tum.de",
        "aff_unique_abbr": "TUM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Munich",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Track and Segment: An Iterative Unsupervised Approach for Video Object Proposals",
        "session": "Video Analysis 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "19",
        "author_site": "Fanyi Xiao, Yong Jae Lee",
        "author": "Fanyi Xiao; Yong Jae Lee",
        "abstract": "We present an unsupervised approach that generates a diverse, ranked set of bounding box and segmentation video object proposals---spatio-temporal tubes that localize the foreground objects---in an unannotated video.  In contrast to previous unsupervised methods that either track regions initialized in an arbitrary frame or train a fixed model over a cluster of regions, we instead discover a set of easy-to-group instances of an object and then iteratively update its appearance model to gradually detect harder instances in temporally-adjacent frames.  Our method first generates a set of spatio-temporal bounding box proposals, and then refines them to obtain pixel-wise segmentation proposals.  Through extensive experiments, we demonstrate state-of-the-art segmentation results on the SegTrack v2 dataset, and bounding box tracking results that perform competitively to state-of-the-art supervised tracking methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Xiao_Track_and_Segment_CVPR_2016_paper.pdf",
        "aff": "University of California, Davis; University of California, Davis",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1785883,
        "gs_citation": 145,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7096003580906316502&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "cs.ucdavis.edu;cs.ucdavis.edu",
        "email": "cs.ucdavis.edu;cs.ucdavis.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Xiao_Track_and_Segment_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, Davis",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucdavis.edu",
        "aff_unique_abbr": "UC Davis",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Davis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "56",
        "author_site": "Krishna Kumar Singh, Fanyi Xiao, Yong Jae Lee",
        "author": "Krishna Kumar Singh; Fanyi Xiao; Yong Jae Lee",
        "abstract": "The status quo approach to training object detectors requires expensive bounding box annotations.  Our framework takes a markedly different direction: we transfer tracked object boxes from weakly-labeled videos to weakly-labeled images to automatically generate pseudo ground-truth boxes, which replace manually annotated bounding boxes.  We first mine discriminative regions in the weakly-labeled image collection that frequently/rarely appear in the positive/negative images.  We then match those regions to videos and retrieve the corresponding tracked object boxes.  Finally, we design a hough transform algorithm to vote for the best box to serve as the pseudo GT for each image, and use them to train an object detector.  Together, these lead to state-of-the-art weakly-supervised detection results on the PASCAL 2007 and 2010 datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Singh_Track_and_Transfer_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3246440,
        "gs_citation": 80,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7558205986168856292&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Singh_Track_and_Transfer_CVPR_2016_paper.html"
    },
    {
        "title": "Traffic-Sign Detection and Classification in the Wild",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "66",
        "author_site": "Zhe Zhu, Dun Liang, Songhai Zhang, Xiaolei Huang, Baoli Li, Shimin Hu",
        "author": "Zhe Zhu; Dun Liang; Songhai Zhang; Xiaolei Huang; Baoli Li; Shimin Hu",
        "abstract": "Although promising results have been achieved in the areas of traffic-sign detection and classification, few works have provided simultaneous solutions to these two tasks for realistic real world images. We make two contributions to this problem. Firstly, we have created a large traffic-sign benchmark from 100000 Tencent Street View panoramas,      going beyond previous benchmarks. It provides 100000 images containing 30000 traffic-sign instances. These  images cover large variations in illuminance and weather conditions.  Each traffic-sign in the benchmark is annotated with a class label, its bounding box and pixel mask. We call this benchmark Tsinghua-Tencent 100K. Secondly, we demonstrate how a robust end-to-end convolutional neural network (CNN) can simultaneously detect and classify traffic-signs. Most previous CNN image processing solutions target objects that occupy a large proportion of an image, and such networks do not work well for target objects occupying only a small fraction of an image like the traffic-signs here. Experimental results show the robustness of our network and its superiority to alternatives. The benchmark, source code and the CNN model introduced in this paper is publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf",
        "aff": "TNList, Tsinghua University; TNList, Tsinghua University; TNList, Tsinghua University; Lehigh University; Tencent; TNList, Tsinghua University",
        "project": "http://cg.cs.tsinghua.edu.cn/traffic-sign/",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhu_Traffic-Sign_Detection_and_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1622070,
        "gs_citation": 1212,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9645825234574616389&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "gmail.com;gmail.com;tsinghua.edu.cn;cse.lehigh.edu;tencent.com;tsinghua.edu.cn",
        "email": "gmail.com;gmail.com;tsinghua.edu.cn;cse.lehigh.edu;tencent.com;tsinghua.edu.cn",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;2;0",
        "aff_unique_norm": "Tsinghua University;Lehigh University;Tencent",
        "aff_unique_dep": "TNList;;Tencent Holdings Limited",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.lehigh.edu;https://www.tencent.com",
        "aff_unique_abbr": "THU;Lehigh;Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "Training Region-Based Object Detectors With Online Hard Example Mining",
        "session": "Object Recognition and Detection",
        "status": "Oral",
        "track": "main",
        "pid": "1",
        "author_site": "Abhinav Shrivastava, Abhinav Gupta, Ross Girshick",
        "author": "Abhinav Shrivastava; Abhinav Gupta; Ross Girshick",
        "abstract": "The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been -- detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9% and 76.3% mAP on PASCAL VOC 2007 and 2012 respectively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf",
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Facebook AI Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 917701,
        "gs_citation": 3146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14496871685581355140&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;fb.com",
        "email": "cs.cmu.edu;cs.cmu.edu;fb.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Meta",
        "aff_unique_dep": ";Facebook AI Research",
        "aff_unique_url": "https://www.cmu.edu;https://research.facebook.com",
        "aff_unique_abbr": "CMU;FAIR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Trust No One: Low Rank Matrix Factorization Using Hierarchical RANSAC",
        "session": "Optimization",
        "status": "Poster",
        "track": "main",
        "pid": "56",
        "author_site": "Magnus Oskarsson, Kenneth Batstone, Kalle \u00c5str\u00f6m",
        "author": "Magnus Oskarsson; Kenneth Batstone; Kalle Astrom",
        "abstract": "In this paper we present a system for performing low rank matrix factorization. Low-rank matrix factorization is an essential problem in many areas including computer vision, with applications in e.g. affine structure-from-motion, photometric stereo, and non-rigid structure from motion. We specifically target structured data patterns, with outliers and large amounts of missing data. Using recently developed characterizations of minimal solutions to matrix factorization problems with missing data, we show how these can be used as building blocks in a hierarchical system that performs bootstrapping on all levels. This gives an robust and fast system, with state-of-the-art performance.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Oskarsson_Trust_No_One_CVPR_2016_paper.pdf",
        "aff": "Centre for Mathematical Sciences, Lund University, Sweden; Centre for Mathematical Sciences, Lund University, Sweden; Centre for Mathematical Sciences, Lund University, Sweden",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4725873,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3488949733136556112&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "maths.lth.se; ; ",
        "email": "maths.lth.se; ; ",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Oskarsson_Trust_No_One_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Lund University",
        "aff_unique_dep": "Centre for Mathematical Sciences",
        "aff_unique_url": "https://www.lunduniversity.lu.se",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Sweden"
    },
    {
        "title": "Two Illuminant Estimation and User Correction Preference",
        "session": "Low-Level Vision",
        "status": "Poster",
        "track": "main",
        "pid": "50",
        "author_site": "Dongliang Cheng, Abdelrahman Abdelhamed, Brian Price, Scott Cohen, Michael S. Brown",
        "author": "Dongliang Cheng; Abdelrahman Abdelhamed; Brian Price; Scott Cohen; Michael S. Brown",
        "abstract": "This paper examines the problem of white-balance correction when a scene contains two illuminations.  This is a two step process: 1) estimate the two illuminants; and 2) correct the image.   Existing methods attempt to estimate a spatially varying illumination map, however, results are error prone and the resulting illumination maps are too low-resolution to be used for proper spatially varying white-balance correction.  In addition, the spatially varying nature of these methods make them computationally intensive.   We show that this problem can be effectively addressed by not attempting to obtain a spatially varying illumination map, but instead by performing illumination estimation on large sub-regions of the image.   Our approach is able to detect when distinct illuminations are present in the image and accurately measure these illuminants.   Since our proposed strategy is not suitable for spatially varying image correction, a user study is performed to see if there is a preference for how the image should be corrected when two illuminants are present, but only a global correction can be applied.   The user study shows that when the illuminations are distinct, there is a preference for the outdoor illumination to be corrected resulting in warmer final result.  We use these collective findings to demonstrate an effective two illuminant estimation scheme that produces corrected images that users prefer.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Cheng_Two_Illuminant_Estimation_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3245066384631248998&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Cheng_Two_Illuminant_Estimation_CVPR_2016_paper.html"
    },
    {
        "title": "UAV Sensor Fusion With Latent-Dynamic Conditional Random Fields in Coronal Plane Estimation",
        "session": "Vision For Robotics",
        "status": "Poster",
        "track": "main",
        "pid": "79",
        "author_site": "Amir M. Rahimi, Raphael Ruschel, B.S. Manjunath",
        "author": "Amir M. Rahimi; Raphael Ruschel; B.S. Manjunath",
        "abstract": "We present a real-time body orientation estimation in a micro-Unmanned Air Vehicle video stream. This work is part of a fully autonomous UAV system which can maneuver to face a single individual in challenging outdoor environments. Our body orientation estimation consists of the following steps: (a) obtaining a set of visual appearance models for each body orientation, where each model is tagged with a set of scene information (obtained from sensors); (b) exploiting the mutual information of on-board sensors using latent-dynamic conditional random fields (LDCRF); (c) Characterizing each visual appearance model with the most discriminative sensor information; (d) fast estimation of body orientation during the test flights given the LDCRF parameters and the corresponding sensor readings. The key aspects of our approach is to add sparsity to the sensor readings with latent variables followed by long range dependency analysis. Experimental results obtained over real-time video streams demonstrate a significant improvement in both speed (15-fps) and accuracy (72%) compared to the state of the art techniques that only rely on visual data. Video demonstration of our autonomous flights (both from ground view and aerial view) are included in the supplementary material.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Rahimi_UAV_Sensor_Fusion_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 25869747,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16411853433596146989&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Rahimi_UAV_Sensor_Fusion_CVPR_2016_paper.html"
    },
    {
        "title": "Unbiased Photometric Stereo for Colored Surfaces: A Variational Approach",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "61",
        "author_site": "Yvain Qu\u00e9au, Roberto Mecca, Jean-Denis Durou",
        "author": "Yvain Queau; Roberto Mecca; Jean-Denis Durou",
        "abstract": "3D shape recovery using photometric stereo (PS) gained increasing attention in the computer vision community in the last three decades due to its ability to recover the thinnest geometric structures. Yet, the reliabiliy of PS for color images is difficult to guarantee, because existing methods are usually formulated as the sequential estimation of the colored albedos, the normals and the depth. Hence, the overall reliability depends on that of each subtask. In this work we propose a new formulation of color photometric stereo, based on image ratios, that makes the technique independent from the albedos. This allows the unbiased 3D-reconstruction of colored surfaces in a single step, by solving a system of linear PDEs using a variational approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Queau_Unbiased_Photometric_Stereo_CVPR_2016_paper.pdf",
        "aff": "IRIT, Universit \u00b4e de Toulouse, France; Department of Engineering, University of Cambridge, UK + Marie Curie fellow of the Istituto Nazionale di Alta Matematica, Italy; IRIT, Universit \u00b4e de Toulouse, France",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1716243,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7425621204683268409&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": "enseeiht.fr;eng.cam.ac.uk;irit.fr",
        "email": "enseeiht.fr;eng.cam.ac.uk;irit.fr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Queau_Unbiased_Photometric_Stereo_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "Universit\u00e9 de Toulouse;University of Cambridge;Istituto Nazionale di Alta Matematica",
        "aff_unique_dep": "IRIT;Department of Engineering;Marie Curie fellow",
        "aff_unique_url": "https://www.univ-toulouse.fr;https://www.cam.ac.uk;https://www.altamatematica.it",
        "aff_unique_abbr": "UT;Cambridge;INdAM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1+2;0",
        "aff_country_unique": "France;United Kingdom;Italy"
    },
    {
        "title": "Uncalibrated Photometric Stereo by Stepwise Optimization Using Principal Components of Isotropic BRDFs",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "60",
        "author_site": "Keisuke Midorikawa, Toshihiko Yamasaki, Kiyoharu Aizawa",
        "author": "Keisuke Midorikawa; Toshihiko Yamasaki; Kiyoharu Aizawa",
        "abstract": "The uncalibrated photometric stereo problem for non-Lambertian surfaces is challenging because of the large number of unknowns and its ill-posed nature stemming from unknown reflectance functions.  We propose a model that represents various isotropic reflectance functions by using the principal components of items in a dataset, and formulate the uncalibrated photometric stereo as a regression problem.  We then solve it by stepwise optimization utilizing principal components in order of their importance.   We have also developed two techniques that lead to convergence and highly accurate reconstruction, namely (1) a coarse-to-fine approach with normal grouping, and (2) a randomized multipoint search. Our experimental results with synthetic data showed that our method significantly outperformed previous methods. We also evaluated the algorithm in terms of real image data, where it gave good reconstruction results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Midorikawa_Uncalibrated_Photometric_Stereo_CVPR_2016_paper.pdf",
        "aff": "The University of Tokyo, Japan; The University of Tokyo, Japan; The University of Tokyo, Japan",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Midorikawa_Uncalibrated_Photometric_Stereo_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2529141,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8706706827013201081&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "hal.t.u-tokyo.ac.jp;hal.t.u-tokyo.ac.jp;hal.t.u-tokyo.ac.jp",
        "email": "hal.t.u-tokyo.ac.jp;hal.t.u-tokyo.ac.jp;hal.t.u-tokyo.ac.jp",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Midorikawa_Uncalibrated_Photometric_Stereo_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "title": "Uncertainty-Driven 6D Pose Estimation of Objects and Scenes From a Single RGB Image",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "36",
        "author_site": "Eric Brachmann, Frank Michel, Alexander Krull, Michael Ying Yang, Stefan Gumhold, carsten Rother",
        "author": "Eric Brachmann; Frank Michel; Alexander Krull; Michael Ying Yang; Stefan Gumhold; carsten Rother",
        "abstract": "In recent years, the task of estimating the 6D pose of object instances and complete scenes, i.e. camera localization, from a single input image has received considerable attention. Consumer RGB-D cameras have made this feasible, even for difficult, texture-less objects and scenes. In this work, we show that a single RGB image is sufficient to achieve visually convincing results. Our key concept is to model and exploit the uncertainty of the system at all stages of the processing pipeline. The uncertainty comes in the form of continuous distributions over 3D object coordinates and discrete distributions over object labels. We give three technical contributions. Firstly, we develop a regularized, auto-context regression framework which iteratively reduces uncertainty in object coordinate and object label predictions. Secondly, we introduce an efficient way to marginalize object coordinate distributions over depth. This is necessary to deal with missing depth information. Thirdly, we utilize the distributions over object labels to detect multiple objects simultaneously with a fixed budget of RANSAC hypotheses. We tested our system for object pose estimation and camera localization on commonly used data sets. We see a major improvement over competing systems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Brachmann_Uncertainty-Driven_6D_Pose_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Brachmann_Uncertainty-Driven_6D_Pose_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "gs_citation": 627,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11140125905516140423&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Brachmann_Uncertainty-Driven_6D_Pose_CVPR_2016_paper.html"
    },
    {
        "title": "Unconstrained Face Alignment via Cascaded Compositional Learning",
        "session": "Face and Gesture",
        "status": "Poster",
        "track": "main",
        "pid": "41",
        "author_site": "Shizhan Zhu, Cheng Li, Chen-Change Loy, Xiaoou Tang",
        "author": "Shizhan Zhu; Cheng Li; Chen-Change Loy; Xiaoou Tang",
        "abstract": "We present a practical approach to address the problem of unconstrained face alignment for a single image. In our unconstrained problem, we need to deal with large shape and appearance variations under extreme head poses and rich shape deformation. To equip cascaded regressors with the capability to handle global shape variation and irregular appearance-shape relation in the unconstrained scenario, we partition the optimisation space into multiple domains of homogeneous descent, and predict a shape as a composition of estimations from multiple domain-specific regressors. With a specially formulated learning objective and a novel tree splitting function, our approach is capable of estimating a robust and meaningful composition. In addition to achieving state-of-the-art accuracy over existing approaches, our framework is also an efficient solution (350 FPS), thanks to the on-the-fly domain exclusion mechanism and the capability of leveraging the fast pixel feature.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_Unconstrained_Face_Alignment_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "gs_citation": 217,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=444762884042012457&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_Unconstrained_Face_Alignment_CVPR_2016_paper.html"
    },
    {
        "title": "Unconstrained Salient Object Detection via Proposal Subset Optimization",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "47",
        "author_site": "Jianming Zhang, Stan Sclaroff, Zhe Lin, Xiaohui Shen, Brian Price, Radom\u00edr Mech",
        "author": "Jianming Zhang; Stan Sclaroff; Zhe Lin; Xiaohui Shen; Brian Price; Radomir Mech",
        "abstract": "We aim at detecting salient objects in unconstrained images. In unconstrained images, the number of salient objects (if any) varies from image to image, and is not given. We present a salient object detection system that directly outputs a compact set of detection windows, if any, for an input image. Our system leverages a Convolutional-Neural-Network model to generate location proposals of salient objects. Location proposals tend to be highly overlapping and noisy. Based on the Maximum a Posteriori principle, we propose a novel subset optimization framework to generate a compact set of detection windows out of noisy proposals. In experiments, we show that our subset optimization formulation greatly enhances the performance of our system, and our system attains 16-34% relative improvement in Average Precision compared with the state-of-the-art on three challenging salient object datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Unconstrained_Salient_Object_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Zhang_Unconstrained_Salient_Object_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1365125,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1702270574720050139&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Unconstrained_Salient_Object_CVPR_2016_paper.html"
    },
    {
        "title": "Understanding Real World Indoor Scenes With Synthetic Data",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "31",
        "author_site": "Ankur Handa, Viorica P\u0103tr\u0103ucean, Vijay Badrinarayanan, Simon Stent, Roberto Cipolla",
        "author": "Ankur Handa; Viorica Patraucean; Vijay Badrinarayanan; Simon Stent; Roberto Cipolla",
        "abstract": "Scene understanding is a prerequisite to many high level tasks for any automated intelligent machine operating in real world environments. Recent attempts with supervised learning have shown promise in this direction but also highlighted the need for enormous quantity of supervised data --- performance increases in proportion to the amount of data used. However, this quickly becomes prohibitive when considering the manual labour needed to collect such data. In this work, we focus our attention on depth based semantic per-pixel labelling as a scene understanding problem and show the potential of computer graphics to generate virtually unlimited labelled data from synthetic 3D scenes. By carefully synthesizing training data with appropriate noise models we show comparable performance to state-of-the-art RGBD systems on NYUv2 dataset despite using only depth data as input and set a benchmark on depth-based segmentation on SUN RGB-D dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Handa_Understanding_Real_World_CVPR_2016_paper.pdf",
        "aff": "Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1535470,
        "gs_citation": 448,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4279255482862568438&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "gmail.com;cam.ac.uk;cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "gmail.com;cam.ac.uk;cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Handa_Understanding_Real_World_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Unsupervised Cross-Dataset Transfer Learning for Person Re-Identification",
        "session": "Human ID",
        "status": "Poster",
        "track": "main",
        "pid": "58",
        "author_site": "Peixi Peng, Tao Xiang, Yaowei Wang, Massimiliano Pontil, Shaogang Gong, Tiejun Huang, Yonghong Tian",
        "author": "Peixi Peng; Tao Xiang; Yaowei Wang; Massimiliano Pontil; Shaogang Gong; Tiejun Huang; Yonghong Tian",
        "abstract": "Most existing person re-identification (Re-ID) approaches follow a supervised learning framework, in which a large number of labelled matching pairs are required for training. This severely limits their scalability in real-world applications. To overcome this limitation, we develop a novel cross-dataset transfer learning approach to learn a discriminative representation. It is unsupervised in the sense that the target dataset is completely unlabelled. Specifically, we present an multi-task dictionary learning method which is able to learn a dataset-shared but target-data-biased representation. Experimental results on five benchmark datasets demonstrate that the method significantly outperforms the state-of-the-art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Peng_Unsupervised_Cross-Dataset_Transfer_CVPR_2016_paper.pdf",
        "aff": ";;;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 539339,
        "gs_citation": 457,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9844039147587665708&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Peng_Unsupervised_Cross-Dataset_Transfer_CVPR_2016_paper.html"
    },
    {
        "title": "Unsupervised Learning From Narrated Instruction Videos",
        "session": "Image & Video Captioning and Descriptions",
        "status": "Oral",
        "track": "main",
        "pid": "3",
        "author_site": "Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, Simon Lacoste-Julien",
        "author": "Jean-Baptiste Alayrac; Piotr Bojanowski; Nishant Agrawal; Josef Sivic; Ivan Laptev; Simon Lacoste-Julien",
        "abstract": "We address the problem of automatically learning the main steps to complete a certain task, such as changing a car tire, from a set of narrated instruction videos. The contributions of this paper are three-fold. First, we develop a new unsupervised learning approach that takes advantage of the complementary nature of the input video and the associated narration. The method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Second, we collect and annotate a new challenging dataset of real-world instruction videos from the Internet. The dataset contains about 800,000 frames for five different tasks that include complex interactions between people and objects, and are captured in a variety of indoor and outdoor settings. Third, we experimentally demonstrate that the proposed method can automatically discover, in an unsupervised manner, the main steps to achieve the task and locate the steps in the input videos.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Alayrac_Unsupervised_Learning_From_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1533315,
        "gs_citation": 378,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16406830472339712794&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Alayrac_Unsupervised_Learning_From_CVPR_2016_paper.html"
    },
    {
        "title": "Unsupervised Learning of Discriminative Attributes and Visual Representations",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "67",
        "author_site": "Chen Huang, Chen Change Loy, Xiaoou Tang",
        "author": "Chen Huang; Chen Change Loy; Xiaoou Tang",
        "abstract": "Attributes offer useful mid-level features to interpret visual data. While most attribute learning methods are supervised by costly human-generated labels, we introduce a simple yet powerful unsupervised approach to learn and predict visual attributes directly from data. Given a large unlabeled image collection as input, we train deep Convolutional Neural Networks (CNNs) to output a set of discriminative, binary attributes often with semantic meanings. Specifically, we first train a CNN coupled with unsupervised discriminative clustering, and then use the cluster membership as a soft supervision to discover shared attributes from the clusters while maximizing their separability. The learned attributes are shown to be capable of encoding rich imagery properties from both natural images and contour patches. The visual representations learned in this way are also transferrable to other tasks such as object detection. We show other convincing results on the related tasks of image retrieval and classification, and contour detection.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Unsupervised_Learning_of_CVPR_2016_paper.pdf",
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong + SenseTime Group Limited; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Huang_Unsupervised_Learning_of_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 6036380,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1611471526477073702&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Huang_Unsupervised_Learning_of_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+2;0+2",
        "aff_unique_norm": "Chinese University of Hong Kong;SenseTime Group Limited;Chinese Academy of Sciences",
        "aff_unique_dep": "Department of Information Engineering;;Shenzhen Institutes of Advanced Technology",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.sensetime.com;http://www.siat.cas.cn",
        "aff_unique_abbr": "CUHK;SenseTime;SIAT",
        "aff_campus_unique_index": "0;0+2;0+2",
        "aff_campus_unique": "Hong Kong SAR;;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Unsupervised Learning of Edges",
        "session": "Image Processing and Restoration",
        "status": "Oral",
        "track": "main",
        "pid": "13",
        "author_site": "Yin Li, Manohar Paluri, James M. Rehg, Piotr Doll\u00e1r",
        "author": "Yin Li; Manohar Paluri; James M. Rehg; Piotr Dollar",
        "abstract": "Data-driven approaches for edge detection have proven effective and achieve top results on modern benchmarks. However, all current data-driven edge detectors require manual supervision for training in the form of hand-labeled region segments or object boundaries. Specifically, human annotators mark semantically meaningful edges which are subsequently used for training. Is this form of strong, high-level supervision actually necessary to learn to accurately detect edges? In this work we present a simple yet effective approach for training edge detectors without human supervision. To this end we utilize motion, and more specifically, the only input to our method is noisy semi-dense matches between frames. We begin with only a rudimentary knowledge of edges (in the form of image gradients), and alternate between improving motion estimation and edge detection in turn. Using a large corpus of video data, we show that edge detectors trained using our unsupervised scheme approach the performance of the same methods trained with full supervision (within 3-5%). Finally, we show that when using a deep network for the edge detector, our approach provides a novel pre-training scheme for object detection.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Unsupervised_Learning_of_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3237083,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4756436393704577929&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Unsupervised_Learning_of_CVPR_2016_paper.html"
    },
    {
        "title": "Using Self-Contradiction to Learn Confidence Measures in Stereo Vision",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "30",
        "author_site": "Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer, Horst Bischof",
        "author": "Christian Mostegel; Markus Rumpler; Friedrich Fraundorfer; Horst Bischof",
        "abstract": "Learned confidence measures gain increasing importance for outlier removal and quality improvement in stereo vision. However, acquiring the necessary training data is typically a tedious and time consuming task that involves manual interaction, active sensing devices and/or synthetic scenes. To overcome this problem, we propose a new, flexible, and scalable way for generating training data that only requires a set of stereo images as input. The key idea of our approach is to use different view points for reasoning about contradictions and consistencies between multiple depth maps generated with the same stereo algorithm. This enables us to generate a huge amount of training data in a fully automated manner. Among other experiments, we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the KITTI2012 dataset by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Mostegel_Using_Self-Contradiction_to_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Mostegel_Using_Self-Contradiction_to_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 2874425,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8838967834663532169&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Mostegel_Using_Self-Contradiction_to_CVPR_2016_paper.html"
    },
    {
        "title": "Using Spatial Order to Boost the Elimination of Incorrect Feature Matches",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "34",
        "author_site": "Lior Talker, Yael Moses, Ilan Shimshoni",
        "author": "Lior Talker; Yael Moses; Ilan Shimshoni",
        "abstract": "Correctly matching feature points in a pair of images is an important preprocessing step for many computer vision applications. In this paper we propose an efficient method for estimating the number of correct matches without explicitly computing them. In addition, our method estimates the region of overlap between the images. To this end, we propose to analyze the set of matches using the spatial order of the features, as projected to the x-axis of the image. The set of features in each image is thus represented by a sequence. This reduces the analysis of the matching problem to the analysis of the permutation between the sequences. Using the Kendall distance metric between permutations and natural assumptions on the distribution of the correct and incorrect matches, we show how to estimate the above-mentioned values. We demonstrate the usefulness of our method in two applications: (i) a new halting condition for RANSAC based epipolar geometry estimation methods that considerably reduce the running time, and (ii) discarding spatially unrelated image pairs in the Structure-from-Motion pipeline. Furthermore, our analysis allows to compute the probability that a given match is correct based on the estimated number of correct matches and the rank of the features within the sequences. Our experiments on a large number of synthetic and real data demonstrate the effectiveness of our method. For example, the running time of the image matching stage in the Structure-from-Motion pipeline may be reduced by about 99% while preserving about 80% of the correctly matched feature points.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Talker_Using_Spatial_Order_CVPR_2016_paper.pdf",
        "aff": "The University of Haifa; The Interdisciplinary Center; The University of Haifa",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Talker_Using_Spatial_Order_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2182212,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10015910654665682138&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "campus.haifa.ac.il;idc.ac.il;mis.haifa.ac.il",
        "email": "campus.haifa.ac.il;idc.ac.il;mis.haifa.ac.il",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Talker_Using_Spatial_Order_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Haifa;Interdisciplinary Center",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.haifa.ac.il;https://www.idc.ac.il",
        "aff_unique_abbr": "UoH;IDC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "title": "VLAD3: Encoding Dynamics of Deep Features for Action Recognition",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "49",
        "author_site": "Yingwei Li, Weixin Li, Vijay Mahadevan, Nuno Vasconcelos",
        "author": "Yingwei Li; Weixin Li; Vijay Mahadevan; Nuno Vasconcelos",
        "abstract": "Previous approaches to action recognition with deep features tend to process video frames only within a small temporal region, and do not model long-range dynamic information explicitly. However, such information is important for the accurate recognition of actions, especially for the discrimination of complex activities that share sub-actions, and when dealing with untrimmed videos. Here, we propose a representation, VLAD for Deep Dynamics (VLAD^3), that accounts for different levels of video dynamics. It captures short-term dynamics with deep convolutional neural network features, relying on linear dynamic systems (LDS) to model medium-range dynamics. To account for long-range inhomogeneous dynamics, a VLAD descriptor is derived for the LDS and pooled over the whole video, to arrive at the final VLAD^3 representation. An extensive evaluation was performed on Olympic Sports, UCF101 and THUMOS15, where the use of the VLAD^3 representation leads to state-of- the-art results.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_VLAD3_Encoding_Dynamics_CVPR_2016_paper.pdf",
        "aff": "University of California, San Diego; University of California, San Diego; Yahoo Research; University of California, San Diego",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 791959,
        "gs_citation": 111,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7083463578196580376&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "ucsd.edu;ucsd.edu;yahoo-inc.com;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;yahoo-inc.com;ucsd.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_VLAD3_Encoding_Dynamics_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of California, San Diego;Yahoo",
        "aff_unique_dep": ";Yahoo Research",
        "aff_unique_url": "https://www.ucsd.edu;https://research.yahoo.com",
        "aff_unique_abbr": "UCSD;Yahoo Research",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Variable Aperture Light Field Photography: Overcoming the Diffraction-Limited Spatio-Angular Resolution Tradeoff",
        "session": "Shape From X",
        "status": "Poster",
        "track": "main",
        "pid": "76",
        "author_site": "Julie Chang, Isaac Kauvar, Xuemei Hu, Gordon Wetzstein",
        "author": "Julie Chang; Isaac Kauvar; Xuemei Hu; Gordon Wetzstein",
        "abstract": "Light fields have many applications in machine vision, consumer photography, robotics, and microscopy. However, the prevalent resolution limits of existing light field imaging systems hinder widespread adoption. In this paper, we analyze fundamental resolution limits of light field cameras in the diffraction limit. We propose a sequential, coded-aperture-style acquisition scheme that optimizes the resolution of a light field reconstructed from multiple photographs captured from different perspectives and f-number settings. We also show that the proposed acquisition scheme facilitates high dynamic range light field imaging and demonstrate a proof-of-concept prototype system. With this work, we hope to advance our understanding of the resolution limits of light field photography and develop practical computational imaging systems to overcome them.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chang_Variable_Aperture_Light_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Chang_Variable_Aperture_Light_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 7827316,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1578870465565799903&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chang_Variable_Aperture_Light_CVPR_2016_paper.html"
    },
    {
        "title": "Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks",
        "session": "Image & Video Captioning and Descriptions",
        "status": "Oral",
        "track": "main",
        "pid": "4",
        "author_site": "Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, Wei Xu",
        "author": "Haonan Yu; Jiang Wang; Zhiheng Huang; Yi Yang; Wei Xu",
        "abstract": "We present an approach that exploits hierarchical Recurrent Neural Networks (RNNs) to tackle the video captioning problem, i.e., generating one or multiple sentences to describe a realistic video. Our hierarchical framework contains a sentence generator and a paragraph generator. The sentence generator produces one simple short sentence that describes a specific short video interval. It exploits both temporal- and spatial-attention mechanisms to selectively focus on visual elements during generation. The paragraph generator captures the inter-sentence dependency by taking as input the sentential embedding produced by the sentence generator, combining it with the paragraph history, and outputting the new initial state for the sentence generator. We evaluate our approach on two large-scale benchmark datasets: YouTubeClips and TACoS-MultiLevel. The experiments demonstrate that our approach significantly outperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and 0.305 respectively.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yu_Video_Paragraph_Captioning_CVPR_2016_paper.pdf",
        "aff": "Purdue University; Baidu Research - Institute of Deep Learning; Facebook + Baidu Research - Institute of Deep Learning; Baidu Research - Institute of Deep Learning; Baidu Research - Institute of Deep Learning",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1556096,
        "gs_citation": 742,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1019806543495834224&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "gmail.com;baidu.com;fb.com;baidu.com;baidu.com",
        "email": "gmail.com;baidu.com;fb.com;baidu.com;baidu.com",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yu_Video_Paragraph_Captioning_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2+1;1;1",
        "aff_unique_norm": "Purdue University;Baidu;Meta",
        "aff_unique_dep": ";Institute of Deep Learning;Facebook, Inc.",
        "aff_unique_url": "https://www.purdue.edu;https://research.baidu.com;https://www.facebook.com",
        "aff_unique_abbr": "Purdue;Baidu;FB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0+1;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "title": "Video Segmentation via Object Flow",
        "session": "Video Analysis 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "12",
        "author_site": "Yi-Hsuan Tsai, Ming-Hsuan Yang, Michael J. Black",
        "author": "Yi-Hsuan Tsai; Ming-Hsuan Yang; Michael J. Black",
        "abstract": "Video object segmentation is challenging due to fast moving objects, deforming shapes, and cluttered backgrounds. Optical flow can be used to propagate an object segmentation over time but, unfortunately, flow is often inaccurate, particularly around object boundaries. Such boundaries are precisely where we want our segmentation to be accurate. To obtain accurate segmentation across time, we propose an efficient algorithm that considers video segmentation and optical flow estimation simultaneously. For video segmentation, we formulate a principled, multi-scale, spatio-temporal objective function that uses optical flow to propagate information between frames. For optical flow estimation, particularly at object boundaries, we compute the flow independently in the segmented regions and recompose the results. We call the process object flow and demonstrate the effectiveness of jointly optimizing optical flow and video segmentation using an iterative scheme. Experiments on the SegTrack v2 and Youtube-Objects datasets show that the proposed algorithm performs favorably against the other state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Tsai_Video_Segmentation_via_CVPR_2016_paper.pdf",
        "aff": "UC Merced; UC Merced; MPI for Intelligent Systems",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Tsai_Video_Segmentation_via_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1587818,
        "gs_citation": 454,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11904442160900922772&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "ucmerced.edu;ucmerced.edu;tuebingen.mpg.de",
        "email": "ucmerced.edu;ucmerced.edu;tuebingen.mpg.de",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Tsai_Video_Segmentation_via_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, Merced;Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucmerced.edu;https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "UCM;MPI-IS",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Merced;",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "title": "Video-Story Composition via Plot Analysis",
        "session": "Activity Recognition",
        "status": "Spotlight",
        "track": "main",
        "pid": "10",
        "author_site": "Jinsoo Choi, Tae-Hyun Oh, In So Kweon",
        "author": "Jinsoo Choi; Tae-Hyun Oh; In So Kweon",
        "abstract": "We address the problem of composing a story out of multiple short video clips taken by a person during an activity or experience. Inspired by plot analysis of written stories, our method generates a sequence of video clips ordered in such a way that it reflects plot dynamics and content coherency. That is, given a set of multiple video clips, our method composes a video which we call a video-story. We define metrics on scene dynamics and coherency by dense optical flow features and a patch matching algorithm. Using these metrics, we define an objective function for the video-story. To efficiently search for the best video-story, we introduce a novel Branch-and-Bound algorithm which guarantees the global optimum. We collect the dataset consisting of 23 video sets from the web, resulting in a total of 236 individual video clips. With the acquired dataset, we perform extensive user studies involving 30 human subjects by which the effectiveness of our approach is quantitatively and qualitatively verified.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Choi_Video-Story_Composition_via_CVPR_2016_paper.pdf",
        "aff": "KAIST, Republic of Korea; KAIST, Republic of Korea; KAIST, Republic of Korea",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Choi_Video-Story_Composition_via_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 1074695,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5272509117031232962&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": "rcv.kaist.ac.kr;gmail.com;kaist.ac.kr",
        "email": "rcv.kaist.ac.kr;gmail.com;kaist.ac.kr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Choi_Video-Story_Composition_via_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Video2GIF: Automatic Generation of Animated GIFs From Video",
        "session": "Events, Activities, and Surveillance",
        "status": "Poster",
        "track": "main",
        "pid": "26",
        "author_site": "Michael Gygli, Yale Song, Liangliang Cao",
        "author": "Michael Gygli; Yale Song; Liangliang Cao",
        "abstract": "We introduce the novel problem of automatically generating animated GIFs from video. GIFs are short looping video with no sound, and a perfect combination between image and video that really capture our attention. GIFs tell a story, express emotion, turn events into humorous moments, and are the new wave of photojournalism. We pose the question: Can we automate the entirely manual and elaborate process of GIF creation by leveraging the plethora of user generated GIF content? We propose a Robust Deep RankNet that, given a video, generates a ranked list of its segments according to their suitability as GIF. We train our model to learn what visual content is often selected for GIFs by using over 100K user generated GIFs and their corresponding video sources. We effectively deal with the noisy web data by proposing a novel adaptive Huber loss in the ranking formulation. We show that our approach is robust to outliers and picks up several patterns that are frequently present in popular animated GIFs. On our new large-scale benchmark dataset, we show the advantage of our approach over several state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gygli_Video2GIF_Automatic_Generation_CVPR_2016_paper.pdf",
        "aff": "CVL, ETH Zurich; Yahoo Research; Yahoo Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1165358,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12642910115122488827&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 14,
        "aff_domain": "vision.ee.ethz.ch;yahoo-inc.com;yahoo-inc.com",
        "email": "vision.ee.ethz.ch;yahoo-inc.com;yahoo-inc.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gygli_Video2GIF_Automatic_Generation_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "ETH Zurich;Yahoo",
        "aff_unique_dep": "Computer Vision Laboratory;Yahoo Research",
        "aff_unique_url": "https://www.ethz.ch;https://research.yahoo.com",
        "aff_unique_abbr": "ETHZ;Yahoo Research",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "title": "Virtual Worlds as Proxy for Multi-Object Tracking Analysis",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "59",
        "author_site": "Adrien Gaidon, Qiao Wang, Yohann Cabon, Eleonora Vig",
        "author": "Adrien Gaidon; Qiao Wang; Yohann Cabon; Eleonora Vig",
        "abstract": "Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling.  In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds.  We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI, automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow.  We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance.  As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal.  We show these factors may affect drastically otherwise high-performing deep models for tracking.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gaidon_Virtual_Worlds_as_CVPR_2016_paper.pdf",
        "aff": "Computer Vision group, Xerox Research Center Europe, France; School of Electrical, Computer, and Energy Engineering and School of Arts, Media, and Engineering, Arizona State University, USA; Computer Vision group, Xerox Research Center Europe, France; Computer Vision group, Xerox Research Center Europe, France + German Aerospace Center",
        "project": "http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 855434,
        "gs_citation": 1462,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11727455440906017188&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": "xrce.xerox.com;asu.edu;xrce.xerox.com;dlr.de",
        "email": "xrce.xerox.com;asu.edu;xrce.xerox.com;dlr.de",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gaidon_Virtual_Worlds_as_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;0+2",
        "aff_unique_norm": "Xerox Research Center Europe;Arizona State University;German Aerospace Center",
        "aff_unique_dep": "Computer Vision group;School of Electrical, Computer, and Energy Engineering;",
        "aff_unique_url": "https://www.xerox.com/research-centers/europe.html;https://www.asu.edu;https://www.dlr.de",
        "aff_unique_abbr": ";ASU;DLR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0+2",
        "aff_country_unique": "France;United States;Germany"
    },
    {
        "title": "Visual Path Prediction in Complex Scenes With Crowded Moving Objects",
        "session": "Events, Actions, and Activity Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "44",
        "author_site": "YoungJoon Yoo, Kimin Yun, Sangdoo Yun, JongHee Hong, Hawook Jeong, Jin Young Choi",
        "author": "YoungJoon Yoo; Kimin Yun; Sangdoo Yun; JongHee Hong; Hawook Jeong; Jin Young Choi",
        "abstract": "This paper proposes a novel path prediction algorithm for progressing one step further than the existing works focusing on single target path prediction. In this paper, we consider moving dynamics of co-occurring objects for path prediction in a scene that includes crowded moving objects. To solve this problem, we first suggest a two-layered probabilistic model to find major movement patterns and their co-occurrence tendency. By utilizing the unsupervised learning results from the model, we present an algorithm to find the future location of any target object. Through extensive qualitative/quantitative experiments, we show that our algorithm can find a plausible future path in complex scenes with a large number of moving objects.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yoo_Visual_Path_Prediction_CVPR_2016_paper.pdf",
        "aff": "Perception and Intelligence Lab., School of ECE, ASRI, Seoul National University; Perception and Intelligence Lab., School of ECE, ASRI, Seoul National University; Perception and Intelligence Lab., School of ECE, ASRI, Seoul National University; Perception and Intelligence Lab., School of ECE, ASRI, Seoul National University; Samsung Electronics Co., Ltd; Perception and Intelligence Lab., School of ECE, ASRI, Seoul National University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Yoo_Visual_Path_Prediction_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1191391,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1924261334046745062&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr;samsung.com;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr;samsung.com;snu.ac.kr",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yoo_Visual_Path_Prediction_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "Seoul National University;Samsung",
        "aff_unique_dep": "School of ECE;Samsung Electronics",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.samsung.com",
        "aff_unique_abbr": "SNU;Samsung",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Seoul;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "title": "Visual Tracking Using Attention-Modulated Disintegration and Integration",
        "session": "Motion and Tracking",
        "status": "Poster",
        "track": "main",
        "pid": "57",
        "author_site": "Jongwon Choi, Hyung Jin Chang, Jiyeoup Jeong, Yiannis Demiris, Jin Young Choi",
        "author": "Jongwon Choi; Hyung Jin Chang; Jiyeoup Jeong; Yiannis Demiris; Jin Young Choi",
        "abstract": "In this paper, we present a novel attention-modulated visual tracking algorithm that decomposes an object into multiple cognitive units, and trains multiple elementary trackers in order to modulate the distribution of attention according to various feature and kernel types. In the integration stage it recombines the units to memorize and recognize the target object effectively. With respect to the elementary trackers, we present a novel attentional feature-based correlation filter (AtCF) that focuses on distinctive attentional features. The effectiveness of the proposed algorithm is validated through experimental comparison with state-of-the-art methods on widely-used tracking benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Choi_Visual_Tracking_Using_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Choi_Visual_Tracking_Using_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "gs_citation": 215,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4923123732085070946&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Choi_Visual_Tracking_Using_CVPR_2016_paper.html"
    },
    {
        "title": "Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes",
        "session": "Images and Language",
        "status": "Poster",
        "track": "main",
        "pid": "47",
        "author_site": "Satwik Kottur, Ramakrishna Vedantam, Jos\u00e9 M. F. Moura, Devi Parikh",
        "author": "Satwik Kottur; Ramakrishna Vedantam; Jose M. F. Moura; Devi Parikh",
        "abstract": "We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, although \"eats\" and \"stares at\" seem unrelated in text, they share semantics visually. When people are eating something, they also tend to stare at the food. Grounding diverse relations like \"eats\" and \"stares at\" into vision remains challenging, despite recent progress in vision. We note that the visual grounding of words depends on semantics, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained, visually grounded notions of semantic relatedness. We show improvements over text-only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets are available online.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kottur_Visual_Word2Vec_vis-w2v_CVPR_2016_paper.pdf",
        "aff": "Carnegie Mellon University; Virginia Tech; Carnegie Mellon University; Virginia Tech",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 670943,
        "gs_citation": 120,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17580740875274395402&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "andrew.cmu.edu;vt.edu;ece.cmu.edu;vt.edu",
        "email": "andrew.cmu.edu;vt.edu;ece.cmu.edu;vt.edu",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kottur_Visual_Word2Vec_vis-w2v_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Virginia Tech",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.vt.edu",
        "aff_unique_abbr": "CMU;VT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Visual7W: Grounded Question Answering in Images",
        "session": "Images and Language",
        "status": "Poster",
        "track": "main",
        "pid": "48",
        "author_site": "Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei",
        "author": "Yuke Zhu; Oliver Groth; Michael Bernstein; Li Fei-Fei",
        "abstract": "We have seen great progress in basic perceptual tasks such as object recognition and detection. However, AI models still fail to match humans in high-level vision tasks due to the lack of capacities for deeper reasoning. Recently the new task of visual question answering (QA) has been proposed to evaluate a model's capacity for deep image understanding. Previous works have established a loose, global association between QA sentences and images. However, many questions and answers, in practice, relate to local regions in the images. We establish a semantic link between textual descriptions and image regions by object-level grounding. It enables a new type of QA with visual answers, in addition to textual answers used in previous work. We study the visual QA tasks in a grounded setting with a large collection of 7W multiple-choice QA pairs. Furthermore, we evaluate human performance and several baseline models on the QA tasks. Finally, we propose a novel LSTM model with spatial attention to tackle the 7W QA tasks.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhu_Visual7W_Grounded_Question_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1991541,
        "gs_citation": 1097,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1446328645386125300&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhu_Visual7W_Grounded_Question_CVPR_2016_paper.html"
    },
    {
        "title": "Visualizing and Understanding Deep Texture Representations",
        "session": "Image Enhancement, Restoration, and Texture",
        "status": "Poster",
        "track": "main",
        "pid": "57",
        "author_site": "Tsung-Yu Lin, Subhransu Maji",
        "author": "Tsung-Yu Lin; Subhransu Maji",
        "abstract": "A number of recent approaches have used deep convolutional neural networks (CNNs) to build texture representations. Nevertheless, it is still unclear how these mod- els represent texture and invariances to categorical variations. This work conducts a systematic evaluation of recent CNN-based texture descriptors for recognition and attempts to understand the nature of invariances captured by these representations. First we show that the recently proposed bilinear CNN model [25] is an excellent generalpurpose texture descriptor and compares favorably to other CNN-based descriptors on various texture and scene recognition benchmarks. The model is translationally invariant and obtains better accuracy on the ImageNet dataset without requiring spatial jittering of data compared to corresponding models trained with spatial jittering. Based on recent work [13, 28] we propose a technique to visualize pre-images, providing a means for understanding categorical properties that are captured by these representations. Finally, we show preliminary results on how a unified parametric model of texture analysis and synthesis can be used for attribute-based image manipulation, e.g. to make an image more swirly, honeycombed, or knitted. The source code and additional visualizations are available at http://vis-www.cs.umass.edu/texture.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Lin_Visualizing_and_Understanding_CVPR_2016_paper.pdf",
        "aff": "University of Massachusetts, Amherst; University of Massachusetts, Amherst",
        "project": "http://vis-www.cs.umass.edu/texture",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 2285183,
        "gs_citation": 182,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2609377492021491314&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Lin_Visualizing_and_Understanding_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Visually Indicated Sounds",
        "session": "Computational Photography and Faces",
        "status": "Oral",
        "track": "main",
        "pid": "16",
        "author_site": "Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H. Adelson, William T. Freeman",
        "author": "Andrew Owens; Phillip Isola; Josh McDermott; Antonio Torralba; Edward H. Adelson; William T. Freeman",
        "abstract": "Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a \"real or fake\" psychophysical experiment, and that they convey significant information about material properties and physical interactions.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Owens_Visually_Indicated_Sounds_CVPR_2016_paper.pdf",
        "aff": "MIT; U.C. Berkeley+MIT; MIT; MIT; MIT; MIT+Google Research",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4608855,
        "gs_citation": 489,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16283889422271541176&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Owens_Visually_Indicated_Sounds_CVPR_2016_paper.html",
        "aff_unique_index": "0;1+0;0;0;0;0+2",
        "aff_unique_norm": "Massachusetts Institute of Technology;University of California, Berkeley;Google",
        "aff_unique_dep": ";;Google Research",
        "aff_unique_url": "https://web.mit.edu;https://www.berkeley.edu;https://research.google",
        "aff_unique_abbr": "MIT;UC Berkeley;Google Research",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Berkeley;Mountain View",
        "aff_country_unique_index": "0;0+0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Volumetric 3D Tracking by Detection",
        "session": "Video Analysis 2",
        "status": "Spotlight",
        "track": "main",
        "pid": "8",
        "author_site": "Chun-Hao Huang, Benjamin Allain, Jean-S\u00e9bastien Franco, Nassir Navab, Slobodan Ilic, Edmond Boyer",
        "author": "Chun-Hao Huang; Benjamin Allain; Jean-Sebastien Franco; Nassir Navab; Slobodan Ilic; Edmond Boyer",
        "abstract": "In this paper, we propose a new framework for 3D tracking by detection based on fully volumetric representations. On one hand, 3D tracking by detection has shown robust use in the context of interaction (Kinect) and surface tracking. On the other hand, volumetric representations have recently been proven efficient both for building 3D features and for addressing the 3D tracking problem. We leverage these benefits by unifying both families of approaches into a single, fully volumetric tracking-by-detection framework. We use a centroidal Voronoi tessellation (CVT) representation to compactly tessellate shapes with optimal discretization, construct a feature space, and perform the tracking according to the correspondences provided by trained random forests. Our results show improved tracking and training computational efficiency and improved memory performance. This in turn enables the use of larger training databases than state of the art approaches, which we leverage by proposing a cross-tracking subject training scheme to benefit from all subject sequences for all tracking situations, thus yielding better detection and less overfitting.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Huang_Volumetric_3D_Tracking_CVPR_2016_paper.pdf",
        "aff": ";;;;;",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Huang_Volumetric_3D_Tracking_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2986240651625212431&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Huang_Volumetric_3D_Tracking_CVPR_2016_paper.html"
    },
    {
        "title": "Volumetric and Multi-View CNNs for Object Classification on 3D Data",
        "session": "3D, Stereo, Matching, and Saliency Estimation",
        "status": "Spotlight",
        "track": "main",
        "pid": "38",
        "author_site": "Charles R. Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan, Leonidas J. Guibas",
        "author": "Charles R. Qi; Hao Su; Matthias Niessner; Angela Dai; Mengyuan Yan; Leonidas J. Guibas",
        "abstract": "3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-the-art methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multi-resolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Qi_Volumetric_and_Multi-View_CVPR_2016_paper.pdf",
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University; Stanford University; Stanford University",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Qi_Volumetric_and_Multi-View_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1174924,
        "gs_citation": 2061,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5558114292511619197&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Qi_Volumetric_and_Multi-View_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks",
        "session": "Deep Learning and CNNs",
        "status": "Poster",
        "track": "main",
        "pid": "21",
        "author_site": "Thibaut Durand, Nicolas Thome, Matthieu Cord",
        "author": "Thibaut Durand; Nicolas Thome; Matthieu Cord",
        "abstract": "In this paper, we introduce a novel framework for WEakly supervised Learning of Deep cOnvolutional neural Networks (WELDON). Our method is dedicated to automatically selecting relevant image regions from weak annotations, e.g. global image labels, and encompasses the following contributions. Firstly, WELDON leverages recent improvements on the Multiple Instance Learning paradigm,  i.e. negative evidence scoring and top instance selection. Secondly, the deep CNN is trained to optimize Average Precision, and fine-tuned on the target dataset with efficient computations due to convolutional feature sharing. A thorough experimental validation shows that WELDON outperforms state-of-the-art results on six different datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Durand_WELDON_Weakly_Supervised_CVPR_2016_paper.pdf",
        "aff": "Sorbonne Universit \u00b4es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris; Sorbonne Universit \u00b4es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris; Sorbonne Universit \u00b4es, UPMC Univ Paris 06, CNRS, LIP6 UMR 7606, 4 place Jussieu, 75005 Paris",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Durand_WELDON_Weakly_Supervised_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1369828,
        "gs_citation": 217,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2489903275528689521&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "lip6.fr;lip6.fr;lip6.fr",
        "email": "lip6.fr;lip6.fr;lip6.fr",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Durand_WELDON_Weakly_Supervised_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sorbonne Universit\u00e9s",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sorbonne-universite.fr",
        "aff_unique_abbr": "Sorbonne",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Paris",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "title": "WIDER FACE: A Face Detection Benchmark",
        "session": "Face, Gesture, & Situation Recognition: Algorithms and Datasets",
        "status": "Oral",
        "track": "main",
        "pid": "25",
        "author_site": "Shuo Yang, Ping Luo, Chen-Change Loy, Xiaoou Tang",
        "author": "Shuo Yang; Ping Luo; Chen-Change Loy; Xiaoou Tang",
        "abstract": "Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Yang_WIDER_FACE_A_CVPR_2016_paper.pdf",
        "aff": "Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China; Department of Information Engineering, The Chinese University of Hong Kong + Shenzhen Key Lab of Comp. Vis. & Pat. Rec., Shenzhen Institutes of Advanced Technology, CAS, China",
        "project": "http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 4987859,
        "gs_citation": 2425,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8866787776930184069&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 17,
        "aff_domain": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "email": "ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk;ie.cuhk.edu.hk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Yang_WIDER_FACE_A_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese University of Hong Kong;Shenzhen Institute of Advanced Technology",
        "aff_unique_dep": "Department of Information Engineering;Shenzhen Key Lab of Comp. Vis. & Pat. Rec.",
        "aff_unique_url": "https://www.cuhk.edu.hk;http://www.siat.ac.cn",
        "aff_unique_abbr": "CUHK;SIAT",
        "aff_campus_unique_index": "0+1;0+1;0+1;0+1",
        "aff_campus_unique": "Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "title": "Walk and Learn: Facial Attribute Representation Learning From Egocentric Video and Contextual Data",
        "session": "Recognition and Labeling",
        "status": "Oral",
        "track": "main",
        "pid": "4",
        "author_site": "Jing Wang, Yu Cheng, Rogerio Schmidt Feris",
        "author": "Jing Wang; Yu Cheng; Rogerio Schmidt Feris",
        "abstract": "The way people look in terms of facial attributes (ethnicity, hair color, facial hair, etc.) and the clothes or accessories they wear (sunglasses, hat, hoodies, etc.) is highly dependent on geo-location and weather condition, respectively. This work explores, for the first time, the use of this contextual information, as people with wearable cameras walk across different neighborhoods of a city, in order to learn a rich feature representation for facial attribute classification, without the costly manual annotation required by previous methods. By tracking the faces of casual walkers on more than 40 hours of egocentric video, we are able to cover tens of thousands of different identities and automatically extract nearly 5 million pairs of images connected by or from different face tracks, along with their weather and location context, under pose and lighting variations. These image pairs are then fed into a deep network that preserves similarity of images connected by the same track, in order to capture identity-related attribute features, and optimizes for location and weather prediction to capture additional facial attribute features. Finally, the network is fine-tuned with manually annotated samples. We perform an extensive experimental analysis on wearable data and two standard benchmark datasets based on web images (LFWA and CelebA). Our method outperforms by a large margin a network trained from scratch. Moreover, even without using manually annotated identity labels for pre-training as in previous methods, our approach achieves results that are better than the state of the art.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Walk_and_Learn_CVPR_2016_paper.pdf",
        "aff": "Northwestern University; IBM T. J. Watson; IBM T. J. Watson",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1276825,
        "gs_citation": 137,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2990940704877235260&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 12,
        "aff_domain": "u.northwestern.edu;us.ibm.com;us.ibm.com",
        "email": "u.northwestern.edu;us.ibm.com;us.ibm.com",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Walk_and_Learn_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Northwestern University;IBM",
        "aff_unique_dep": ";IBM T. J. Watson Research Center",
        "aff_unique_url": "https://www.northwestern.edu;https://www.ibm.com/research/watson",
        "aff_unique_abbr": "NU;IBM Watson",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "WarpNet: Weakly Supervised Matching for Single-View Reconstruction",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "24",
        "author_site": "Angjoo Kanazawa, David W. Jacobs, Manmohan Chandraker",
        "author": "Angjoo Kanazawa; David W. Jacobs; Manmohan Chandraker",
        "abstract": "We present an approach to matching images of objects in fine-grained datasets without using part annotations, with an application to the challenging problem of weakly supervised single-view reconstruction. This is in contrast to prior works that require part annotations, since matching objects across class and pose variations is challenging with appearance features alone. We overcome this challenge through a novel deep learning architecture, WarpNet, that aligns an object in one image with a different object in another. We exploit the structure of the fine-grained dataset to create artificial data for training this network in an unsupervised-discriminative learning approach. The output of the network acts as a spatial prior that allows generalization at test time to match real images across variations in appearance, viewpoint and articulation. On the CUB-200-2011 dataset of bird categories, we improve the AP over an appearance-only network by 13.6%. We further demonstrate that our WarpNet matches, together with the structure of fine-grained datasets, allow single-view reconstructions with quality comparable to using annotated point correspondences.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kanazawa_WarpNet_Weakly_Supervised_CVPR_2016_paper.pdf",
        "aff": "University of Maryland, College Park; University of Maryland, College Park; NEC Labs America",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Kanazawa_WarpNet_Weakly_Supervised_2016_CVPR_supplemental.zip",
        "arxiv": "",
        "pdf_size": 2800585,
        "gs_citation": 185,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6764770288496499158&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kanazawa_WarpNet_Weakly_Supervised_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Maryland;NEC Labs America",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;https://www.nec-labs.com",
        "aff_unique_abbr": "UMD;NEC LA",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "College Park;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "We Are Humor Beings: Understanding and Predicting Visual Humor",
        "session": "High Level Semantics",
        "status": "Spotlight",
        "track": "main",
        "pid": "6",
        "author_site": "Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh",
        "author": "Arjun Chandrasekaran; Ashwin K. Vijayakumar; Stanislaw Antol; Mohit Bansal; Dhruv Batra; C. Lawrence Zitnick; Devi Parikh",
        "abstract": "Humor is an integral part of human lives. Despite being tremendously impactful, it is perhaps surprising that we do not have a detailed understanding of humor yet. As interactions between humans and AI systems increase, it is imperative that these systems are taught to understand subtleties of human expressions such as humor. In this work, we are interested in the question - what content in a scene causes it to be funny? As a first step towards understanding visual humor, we analyze the humor manifested in abstract scenes and design computational models for them. We collect two datasets of abstract scenes that facilitate the study of humor at both the scene-level and the object-level. We analyze the funny scenes and explore the different types of humor depicted in them via human studies. We model two tasks that we believe demonstrate an understanding of some aspects of visual humor. The tasks involve predicting the funniness of a scene and altering the funniness of a scene. We show that our models perform well quantitatively, and qualitatively through human studies. Our datasets are publicly available.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Chandrasekaran_We_Are_Humor_CVPR_2016_paper.pdf",
        "aff": "Virginia Tech; Virginia Tech; Virginia Tech; TTI-Chicago; Virginia Tech; Facebook AI Research; Virginia Tech",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Chandrasekaran_We_Are_Humor_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1205400,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2070261576690076953&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff_domain": "vt.edu;vt.edu;vt.edu;ttic.edu;vt.edu;fb.com;vt.edu",
        "email": "vt.edu;vt.edu;vt.edu;ttic.edu;vt.edu;fb.com;vt.edu",
        "author_num": 7,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Chandrasekaran_We_Are_Humor_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;0;2;0",
        "aff_unique_norm": "Virginia Tech;Toyota Technological Institute at Chicago;Meta",
        "aff_unique_dep": ";;Facebook AI Research",
        "aff_unique_url": "https://www.vt.edu;https://www.tti-chicago.org;https://research.facebook.com",
        "aff_unique_abbr": "VT;TTI;FAIR",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "We Don't Need No Bounding-Boxes: Training Object Class Detectors Using Only Human Verification",
        "session": "Object Detection 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "11",
        "author_site": "Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, Vittorio Ferrari",
        "author": "Dim P. Papadopoulos; Jasper R. R. Uijlings; Frank Keller; Vittorio Ferrari",
        "abstract": "Training object class detectors typically requires a large set of images in which objects are annotated by bounding-boxes. However, manually drawing bounding-boxes is very time consuming. We propose a new scheme for training object detectors which only requires annotators to verify bounding-boxes produced automatically by the learning algorithm. Our scheme iterates between re-training the detector, re-localizing objects in the training images, and human verification. We use the verification signal both to improve re-training and to reduce the search space for re-localisation, which makes these steps different to what is normally done in a weakly supervised setting. Extensive experiments on PASCAL VOC 2007 show that (1) using human verification to update detectors and reduce the search space leads to the rapid production of high-quality bounding-box annotations; (2) our scheme delivers detectors performing almost as good as those trained in a fully supervised setting, without ever drawing any bounding-box; (3) as the verification task is very quick, our scheme substantially reduces total annotation time by a factor 6x-9x.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Papadopoulos_We_Dont_Need_CVPR_2016_paper.pdf",
        "aff": "University of Edinburgh; University of Edinburgh; University of Edinburgh; University of Edinburgh",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 3745172,
        "gs_citation": 179,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17898275618899924069&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff_domain": "ed.ac.uk;ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "email": "ed.ac.uk;ed.ac.uk;inf.ed.ac.uk;inf.ed.ac.uk",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Papadopoulos_We_Dont_Need_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Weakly Supervised Deep Detection Networks",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "63",
        "author_site": "Hakan Bilen, Andrea Vedaldi",
        "author": "Hakan Bilen; Andrea Vedaldi",
        "abstract": "Weakly supervised learning of object detection is an important problem in image understanding that still does not have a satisfactory solution.  In this paper, we address this problem by exploiting the power of deep convolutional neural networks pre-trained on large-scale image-level classification tasks. We propose a weakly supervised deep detection architecture that modifies one such network to operate at the level of image regions, performing simultaneously region selection and classification. Trained as an image classifier, the architecture implicitly learns object detectors that are better than alternative weakly supervised detection systems on the PASCAL VOC data. The model, which is a simple and elegant end-to-end architecture, outperforms standard data augmentation and fine-tuning techniques for the task of image-level classification as well.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Bilen_Weakly_Supervised_Deep_CVPR_2016_paper.pdf",
        "aff": "University of Oxford; University of Oxford",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 680288,
        "gs_citation": 1001,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5405093288658958492&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff_domain": "robots.ox.ac.uk;robots.ox.ac.uk",
        "email": "robots.ox.ac.uk;robots.ox.ac.uk",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Bilen_Weakly_Supervised_Deep_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oxford",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ox.ac.uk",
        "aff_unique_abbr": "Oxford",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "title": "Weakly Supervised Object Boundaries",
        "session": "Segmentation and Contour Detection",
        "status": "Spotlight",
        "track": "main",
        "pid": "20",
        "author_site": "Anna Khoreva, Rodrigo Benenson, Mohamed Omran, Matthias Hein, Bernt Schiele",
        "author": "Anna Khoreva; Rodrigo Benenson; Mohamed Omran; Matthias Hein; Bernt Schiele",
        "abstract": "State-of-the-art learning based boundary detection methods require extensive training data. Since labelling object boundaries is one of the most expensive types of annotations, there is a need to relax the requirement to carefully annotate images to make both the training more affordable and to extend the amount of training data. In this paper we propose a technique to generate weakly supervised annotations and show that bounding box annotations alone suffice to reach high-quality object boundaries without using any object-specific boundary annotations. With the proposed weak supervision techniques we achieve the top performance on the object boundary detection task, outperforming by a large margin the current fully supervised state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Khoreva_Weakly_Supervised_Object_CVPR_2016_paper.pdf",
        "aff": "Max Planck Institute for Informatics, Saarbr\u00fccken, Germany; Max Planck Institute for Informatics, Saarbr\u00fccken, Germany; Max Planck Institute for Informatics, Saarbr\u00fccken, Germany; Saarland University, Saarbr\u00fccken, Germany; Max Planck Institute for Informatics, Saarbr\u00fccken, Germany",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Khoreva_Weakly_Supervised_Object_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1327917,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=777409088782733398&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Khoreva_Weakly_Supervised_Object_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Max Planck Institute for Informatics;Saarland University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://mpi-inf.mpg.de;https://www.uni-saarland.de",
        "aff_unique_abbr": "MPII;UdS",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Saarbr\u00fccken",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "title": "Weakly Supervised Object Localization With Progressive Domain Adaptation",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "52",
        "author_site": "Dong Li, Jia-Bin Huang, Yali Li, Shengjin Wang, Ming-Hsuan Yang",
        "author": "Dong Li; Jia-Bin Huang; Yali Li; Shengjin Wang; Ming-Hsuan Yang",
        "abstract": "We address the problem of weakly supervised object localization where only image-level annotations are available for training. Many existing approaches tackle this problem through object proposal mining. However, a substantial amount of noise in object proposals causes ambiguities for learning discriminative object models. Such approaches are sensitive to model initialization and often converge to an undesirable local minimum. In this paper, we address this problem by progressive domain adaptation with two main steps: classification adaptation and detection adaptation. In classification adaptation, we transfer a pre-trained network to our multi-label classification task for recognizing the presence of a certain object in an image. In detection adaptation, we first use a mask-out strategy to collect class-specific object proposals and apply multiple instance learning to mine confident candidates. We then use these selected object proposals to fine-tune all the layers, resulting in a fully adapted detection network. We extensively evaluate the localization performance on the PASCAL VOC and ILSVRC datasets and demonstrate significant performance improvement over the state-of-the-art methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Weakly_Supervised_Object_CVPR_2016_paper.pdf",
        "aff": "Tsinghua University; University of Illinois, Urbana-Champaign; Tsinghua University; Tsinghua University; University of California, Merced",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Li_Weakly_Supervised_Object_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1952987,
        "gs_citation": 257,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6311347882938583835&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Li_Weakly_Supervised_Object_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;0;0;2",
        "aff_unique_norm": "Tsinghua University;University of Illinois;University of California, Merced",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://illinois.edu;https://www.ucmerced.edu",
        "aff_unique_abbr": "THU;UIUC;UC Merced",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Urbana-Champaign;Merced",
        "aff_country_unique_index": "0;1;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "title": "What If We Do Not Have Multiple Videos of the Same Action? -- Video Action Localization Using Web Images",
        "session": "Events, Activities, and Surveillance",
        "status": "Poster",
        "track": "main",
        "pid": "34",
        "author_site": "Waqas Sultani, Mubarak Shah",
        "author": "Waqas Sultani; Mubarak Shah",
        "abstract": "This paper tackles the problem of spatio-temporal action localization in a video without assuming the availability of multiple videos or any prior annotations. Action is localized by employing images downloaded from internet using action name.  Given web images, we first mitigate image noise using random walk framework and evade distracting backgrounds within images using image action proposals. Then,  given a video, we generate multiple spatio-temporal action proposals. We suppress camera and background generated proposals by exploiting optical flow gradients within proposal. To obtain the most action representative proposal, we propose to reconstruct  action proposals in the video by leveraging the action proposal in images. Moreover, we preserve the temporal smoothness of the video by introducing consensus regularization. Consensus regularization enforces consistency among coefficients vectors of multiple frames within proposal. %We reconstruct video action proposals from image action proposals while enforcing consistency across coefficient vectors of multiple frames by consensus regularization.  Finally, the video proposal that have the lowest reconstruction cost and is motion salient is considered as final action localization. Our  extensive experiments on trimmed as well as untrimmed datasets validate the effectiveness of proposed approach.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Sultani_What_If_We_CVPR_2016_paper.pdf",
        "aff": "Center for Research in Computer Vision (CRCV), University of Central Florida (UCF); Center for Research in Computer Vision (CRCV), University of Central Florida (UCF)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1910419,
        "gs_citation": 41,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7550221031516274729&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "knights.ucf.edu;crcv.ucf.edu",
        "email": "knights.ucf.edu;crcv.ucf.edu",
        "author_num": 2,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Sultani_What_If_We_CVPR_2016_paper.html",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Central Florida",
        "aff_unique_dep": "Center for Research in Computer Vision (CRCV)",
        "aff_unique_url": "https://www.ucf.edu",
        "aff_unique_abbr": "UCF",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "UCF",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "What Players Do With the Ball: A Physically Constrained Interaction Modeling",
        "session": "Video Analysis 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "23",
        "author_site": "Andrii Maksai, Xinchao Wang, Pascal Fua",
        "author": "Andrii Maksai; Xinchao Wang; Pascal Fua",
        "abstract": "Tracking the ball is critical for  video-based analysis of team sports. However, it is difficult,  especially in low-resolution images, due to  the small size of the ball, its  speed that creates motion  blur, and its often  being occluded by players.  In this  paper, we  propose a  generic and principled  approach to  modeling the interaction between  the ball  and the players  while also  imposing appropriate physical constraints on the ball's trajectory.  We show  that our approach, formulated  in terms of  a Mixed Integer  Program, is more robust and more accurate  than several state-of-the-art approaches on real-life volleyball, basketball, and soccer sequences.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Maksai_What_Players_Do_CVPR_2016_paper.pdf",
        "aff": "Computer Vision Laboratory, \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL); Computer Vision Laboratory, \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL); Computer Vision Laboratory, \u00b4Ecole Polytechnique F \u00b4ed\u00b4erale de Lausanne (EPFL)",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1311014,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9586804091118871529&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Maksai_What_Players_Do_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "Computer Vision Laboratory",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Lausanne",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "title": "What Sparse Light Field Coding Reveals About Scene Structure",
        "session": "3D Vision",
        "status": "Poster",
        "track": "main",
        "pid": "25",
        "author_site": "Ole Johannsen, Antonin Sulc, Bastian Goldluecke",
        "author": "Ole Johannsen; Antonin Sulc; Bastian Goldluecke",
        "abstract": "In this paper, we propose a novel method for depth estimation in light fields which employs a specifically designed sparse decomposition to leverage the depth-orientation relationship on its epipolar plane images. The proposed method learns the structure of the central view and uses this information to construct a light field dictionary for which groups of atoms correspond to unique disparities. This dictionary is then used to code a sparse representation of the light field. Analysing the coefficients of this representation with respect to the disparities of their corresponding atoms yields an accurate and robust estimate of depth. In addition, if the light field has multiple depth layers, such as for reflective or transparent surfaces, statistical analysis of the coefficients can be employed to infer the respective depth of the superimposed layers.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Johannsen_What_Sparse_Light_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1743133,
        "gs_citation": 133,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1683185290318189338&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Johannsen_What_Sparse_Light_CVPR_2016_paper.html"
    },
    {
        "title": "What Value Do Explicit High Level Concepts Have in Vision to Language Problems?",
        "session": "Images and Language",
        "status": "Poster",
        "track": "main",
        "pid": "22",
        "author_site": "Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, Anton van den Hengel",
        "author": "Qi Wu; Chunhua Shen; Lingqiao Liu; Anthony Dick; Anton van den Hengel",
        "abstract": "Much recent progress in Vision-to-Language (V2L) problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. In this paper we investigate whether this direct approach succeeds due to, or despite, the fact that it avoids the explicit representation of high-level information. We propose a method of incorporating high-level concepts into the successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art in both image captioning and visual question answering. We also show that the same mechanism can be used to introduce external semantic information and that doing so further improves performance. We achieve the best reported results on both image captioning and VQA on several benchmark datasets, and provide an analysis of the value of explicit high-level concepts in V2L problems.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wu_What_Value_Do_CVPR_2016_paper.pdf",
        "aff": "School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia; School of Computer Science, The University of Adelaide, Australia",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Wu_What_Value_Do_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 1070144,
        "gs_citation": 561,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17363114587431381168&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wu_What_Value_Do_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Adelaide",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.adelaide.edu.au",
        "aff_unique_abbr": "Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "What's Wrong With That Object? Identifying Images of Unusual Objects by Modelling the Detection Score Distribution",
        "session": "Recognition Beyond Objects",
        "status": "Spotlight",
        "track": "main",
        "pid": "8",
        "author_site": "Peng Wang, Lingqiao Liu, Chunhua Shen, Zi Huang, Anton van den Hengel, Heng Tao Shen",
        "author": "Peng Wang; Lingqiao Liu; Chunhua Shen; Zi Huang; Anton van den Hengel; Heng Tao Shen",
        "abstract": "This paper studies the challenging problem of identifying unusual instances of known objects in images within an \"open world\" setting. That is, we aim to find objects that are members of a known class, but which are not typical of that class. Thus the \"unusual object\" should be distinguished from both the \"regular object\" and the \"other objects\". Such unusual objects may be of interest in many applications such as surveillance or quality control. We propose to identify unusual objects by inspecting the distribution of object detection scores at multiple image regions. The key observation motivating our approach is that \"regular object\" images, \"unusual object\" images and \"other objects\" images exhibit different region-level scores in terms of both the score values and the spatial distributions. To model these distributions we propose to use Gaussian Processes (GP) to construct two separate generative models, one for the \"regular object\" and the other for the \"other objects\". More specifically, we design a new covariance function to simultaneously model the detection score at a single location and the score dependencies between multiple regions. We demonstrate that the proposed approach outperforms comparable methods on a new large dataset constructed for the purpose.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Whats_Wrong_With_CVPR_2016_paper.pdf",
        "aff": "The University of Queensland, Australia+The University of Adelaide, Australia; The University of Adelaide, Australia; The University of Adelaide, Australia; The University of Queensland, Australia; The University of Adelaide, Australia; The University of Queensland, Australia",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 795571,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=944885981319076211&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff_domain": "adelaide.edu.au; ; ; ; ; ",
        "email": "adelaide.edu.au; ; ; ; ; ",
        "author_num": 6,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Wang_Whats_Wrong_With_CVPR_2016_paper.html",
        "aff_unique_index": "0+1;1;1;0;1;0",
        "aff_unique_norm": "University of Queensland;University of Adelaide",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uq.edu.au;https://www.adelaide.edu.au",
        "aff_unique_abbr": "UQ;Adelaide",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "title": "When Naive Bayes Nearest Neighbors Meet Convolutional Neural Networks",
        "session": "Object Class Detection and Recognition",
        "status": "Poster",
        "track": "main",
        "pid": "65",
        "author_site": "Ilja Kuzborskij, Fabio Maria Carlucci, Barbara Caputo",
        "author": "Ilja Kuzborskij; Fabio Maria Carlucci; Barbara Caputo",
        "abstract": "Since Convolutional Neural Networks (CNNs) have become the leading learning paradigm in visual recognition, Naive Bayes Nearest Neighbor (NBNN)-based classifiers have lost momentum in the community. This is because (1) such algorithms cannot use CNN activations as input features; (2) they cannot be used as final layer of CNN architectures for end-to-end training , and (3) they are generally not scalable and hence cannot handle big data. This paper proposes a framework that addresses all these issues, thus bringing back NBNNs on the map. We solve the first by extracting CNN activations from local patches at multiple scale levels, similarly to [13]. We address simultaneously the second and third by proposing a scalable version of Naive Bayes Non-linear Learning (NBNL, [7]). Results obtained using pre-trained CNNs on standard scene and domain adaptation databases show the strength of our approach, opening a new season for NBNNs.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Kuzborskij_When_Naive_Bayes_CVPR_2016_paper.pdf",
        "aff": "Sapienza Rome University, Dept. of Computer, Control and Management Engineering, Italy+Idiap Research Institute, Switzerland+\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), Switzerland; Sapienza Rome University, Dept. of Computer, Control and Management Engineering, Italy; Sapienza Rome University, Dept. of Computer, Control and Management Engineering, Italy+Idiap Research Institute, Switzerland",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Kuzborskij_When_Naive_Bayes_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 876309,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7263740158526785807&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff_domain": "dis.uniroma1.it;dis.uniroma1.it;dis.uniroma1.it",
        "email": "dis.uniroma1.it;dis.uniroma1.it;dis.uniroma1.it",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Kuzborskij_When_Naive_Bayes_CVPR_2016_paper.html",
        "aff_unique_index": "0+1+2;0;0+1",
        "aff_unique_norm": "Sapienza University of Rome;Idiap Research Institute;EPFL",
        "aff_unique_dep": "Department of Computer, Control and Management Engineering;;",
        "aff_unique_url": "https://www.uniroma1.it;https://www.idiap.ch;https://www.epfl.ch",
        "aff_unique_abbr": "Sapienza;Idiap;EPFL",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Rome;",
        "aff_country_unique_index": "0+1+1;0;0+1",
        "aff_country_unique": "Italy;Switzerland"
    },
    {
        "title": "When VLAD Met Hilbert",
        "session": "Unsupervised, Semi-Supervised and Interactive Learning",
        "status": "Poster",
        "track": "main",
        "pid": "68",
        "author_site": "Mehrtash Harandi, Mathieu Salzmann, Fatih Porikli",
        "author": "Mehrtash Harandi; Mathieu Salzmann; Fatih Porikli",
        "abstract": "In many challenging visual recognition tasks where training data is limited,  Vectors of Locally Aggregated Descriptors (VLAD) have emerged as powerful image/video representations that compete with or outperform state-of-the-art approaches. In this paper, we address two fundamental limitations of VLAD:  its requirement for the local descriptors to have vector form and its restriction to linear classifiers due to its high-dimensionality. To this end, we introduce a kernelized version of VLAD. This not only lets us inherently exploit more sophisticated classification schemes, but also enables us to efficiently aggregate non-vector descriptors (e.g., manifold-valued data) in the VLAD framework. Furthermore, we propose an approximate formulation that allows us to accelerate the coding process while still benefiting from the properties of kernel VLAD. Our experiments demonstrate the effectiveness of our approach at handling manifold-valued data, such as covariance descriptors, on several classification tasks.  Our results also evidence the benefits of our nonlinear VLAD descriptors against the linear ones in Euclidean space using several standard benchmark datasets.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Harandi_When_VLAD_Met_CVPR_2016_paper.pdf",
        "aff": ";;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 460488,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12269090739638178288&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff_domain": ";;",
        "email": ";;",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Harandi_When_VLAD_Met_CVPR_2016_paper.html"
    },
    {
        "title": "Where to Look: Focus Regions for Visual Question Answering",
        "session": "High Level Semantics",
        "status": "Spotlight",
        "track": "main",
        "pid": "7",
        "author_site": "Kevin J. Shih, Saurabh Singh, Derek Hoiem",
        "author": "Kevin J. Shih; Saurabh Singh; Derek Hoiem",
        "abstract": "We present a method that learns to answer visual questions by selecting image regions relevant to the text-based query. Our method maps textual queries and visual features from various regions into a shared space where they are compared for relevance with an inner product. Our method exhibits significant improvements in answering questions such as \"what color,\" where it is necessary to evaluate a specific location, and \"what room,\" where it selectively identifies informative image regions. Our model is tested on the recently released VQA dataset, which features free-form human-annotated questions and answers.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Shih_Where_to_Look_CVPR_2016_paper.pdf",
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1065806,
        "gs_citation": 610,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6170652068032197021&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Shih_Where_to_Look_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "Yin and Yang: Balancing and Answering Binary Visual Questions",
        "session": "Images and Language",
        "status": "Poster",
        "track": "main",
        "pid": "50",
        "author_site": "Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, Devi Parikh",
        "author": "Peng Zhang; Yash Goyal; Douglas Summers-Stay; Dhruv Batra; Devi Parikh",
        "abstract": "The complex compositional structure of language makes problems at the intersection of vision and language challenging. But language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content.  This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI.  In this paper, we address binary Visual Question Answering (VQA) on abstract scenes. We formulate this problem as visual verification of concepts inquired in the questions. Specifically, we convert the question to a tuple that concisely summarizes the visual concept to be detected in the image. If the concept can be found in the image, the answer to the question is \"yes\", and otherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on the high-level semantics of the VQA task as opposed to the low-level recognition problems, and perhaps more importantly, (2) They provide us the modality to balance the dataset such that language priors are controlled, and the role of vision is essential. In particular, we collect fine-grained pairs of scenes for every question, such that the answer to the question is \"yes\" for one scene, and \"no\" for the other for the exact same question. Indeed, language priors alone do not perform better than chance on our balanced dataset. Moreover, our proposed approach matches the performance of a state-of-the-art VQA approach on the unbalanced dataset, and outperforms it on the balanced dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Yin_and_Yang_CVPR_2016_paper.pdf",
        "aff": "Virginia Tech\u2020; Virginia Tech\u2020; Army Research Laboratory\u2021; Virginia Tech\u2020; Virginia Tech\u2020",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 652128,
        "gs_citation": 439,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7137980739742294084&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "vt.edu;vt.edu;mail.mil;vt.edu;vt.edu",
        "email": "vt.edu;vt.edu;mail.mil;vt.edu;vt.edu",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Yin_and_Yang_CVPR_2016_paper.html",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Virginia Tech;Army Research Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.vt.edu;https://www.arl.army.mil",
        "aff_unique_abbr": "VT;ARL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "You Lead, We Exceed: Labor-Free Video Concept Learning by Jointly Exploiting Web Videos and Images",
        "session": "Video Analysis 1",
        "status": "Spotlight",
        "track": "main",
        "pid": "18",
        "author_site": "Chuang Gan, Ting Yao, Kuiyuan Yang, Yi Yang, Tao Mei",
        "author": "Chuang Gan; Ting Yao; Kuiyuan Yang; Yi Yang; Tao Mei",
        "abstract": "Video concept learning often requires a large set of training samples. In practice, however, acquiring noise-free training labels with sufficient positive examples is very expensive. A plausible solution for training data collection is by sampling from the vast quantities of images and videos on the Web. Such a solution is motivated by the assumption that the retrieved images or videos are highly correlated with the query. Still, a number of challenges remain. First, Web videos are often untrimmed. Thus, only parts of the videos are relevant to the query. Second, the retrieved Web images are always highly relevant to the issued query. However, thoughtlessly utilizing the images in the video domain may even hurt the performance due to the well known semantic drift and domain gap problems. As a result, a valid question is how Web images and videos interact for video concept learning. In this paper, we propose a Lead--Exceed Neural Network (LENN), which reinforces the training on Web images and videos in a curriculum manner. Specifically, the training proceeds by inputting frames of Web videos to obtain a network. The Web images are then filtered by the learnt network and the selected images are additionally fed into the network to enhance the architecture and further trim the videos. In addition, Long Short-Term Memory (LSTM) can be applied on the trimmed videos to explore temporal information. Encouraging results are reported on UCF101, TRECVID 2013 and 2014 MEDTest in the context of both action recognition and event detection. Without using human annotated exemplars, our proposed LENN can achieve 74.4% accuracy on UCF101 dataset.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Gan_You_Lead_We_CVPR_2016_paper.pdf",
        "aff": ";;;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 33235834,
        "gs_citation": 136,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12905861583338692713&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": ";;;;",
        "email": ";;;;",
        "author_num": 5,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Gan_You_Lead_We_CVPR_2016_paper.html"
    },
    {
        "title": "You Only Look Once: Unified, Real-Time Object Detection",
        "session": "Object Recognition and Detection",
        "status": "Oral",
        "track": "main",
        "pid": "3",
        "author_site": "Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi",
        "author": "Joseph Redmon; Santosh Divvala; Ross Girshick; Ali Farhadi",
        "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf",
        "aff": ";;;",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1270136,
        "gs_citation": 61920,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6382612685700818764&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 49,
        "aff_domain": ";;;",
        "email": ";;;",
        "author_num": 4,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Redmon_You_Only_Look_CVPR_2016_paper.html"
    },
    {
        "id": "e7e61404ab",
        "title": "Zero-Shot Learning via Joint Latent Similarity Embedding",
        "site": "https://openaccess.thecvf.com/content_cvpr_2016/html/Zhang_Zero-Shot_Learning_via_CVPR_2016_paper.html",
        "author": "Ziming Zhang; Venkatesh Saligrama",
        "abstract": "Zero-shot recognition (ZSR) deals with the problem of predicting class labels for target domain instances based on source domain side information (e.g. attributes) of unseen classes. We formulate ZSR as a binary prediction problem. Our resulting classifier is class-independent. It takes an arbitrary pair of source and target domain instances as input and predicts whether or not they come from the same class, i.e. whether there is a match. We model the posterior probability of a match since it is a sufficient statistic and propose a latent probabilistic model in this context. We develop a joint discriminative learning framework based on dictionary learning to jointly learn the parameters of our model for both domains, which ultimately leads to our class-independent classifier. Many of the existing embedding methods can be viewed as special cases of our probabilistic model. On ZSR our method shows 4.90% improvement over the state-of-the-art in accuracy averaged across four benchmark datasets. We also adapt ZSR method for zero-shot retrieval and show 22.45% improvement accordingly in mean average precision (mAP).",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Zero-Shot_Learning_via_CVPR_2016_paper.pdf",
        "aff": "Department of Electrical & Computer Engineering, Boston University; Department of Electrical & Computer Engineering, Boston University",
        "project": "",
        "github": "",
        "supp": "",
        "arxiv": "",
        "pdf_size": 1345619,
        "gs_citation": 433,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11042220739614162925&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff_domain": "bu.edu;bu.edu",
        "email": "bu.edu;bu.edu",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Boston University",
        "aff_unique_dep": "Department of Electrical & Computer Engineering",
        "aff_unique_url": "https://www.bu.edu",
        "aff_unique_abbr": "BU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "title": "iLab-20M: A Large-Scale Controlled Object Dataset to Investigate Deep Learning",
        "session": "Recognition and Detection",
        "status": "Poster",
        "track": "main",
        "pid": "78",
        "author_site": "Ali Borji, Saeed Izadi, Laurent Itti",
        "author": "Ali Borji; Saeed Izadi; Laurent Itti",
        "abstract": "Tolerance to image variations (e.g. translation, scale, pose, illumination, background) is an important desired property of any object recognition system, be it human or machine. Moving towards increasingly bigger datasets has been trending in computer vision especially with the emergence of highly popular deep learning models. While being very useful for learning invariance to object inter- and intra-class shape variability, these large-scale wild datasets are not very useful for learning  invariance to other parameters urging researchers to resort to other tricks for training a model. In this work, we introduce a large-scale synthetic dataset, which is freely and publicly available, and use it to answer several fundamental questions regarding selectivity and invariance properties of convolutional neural networks. Our dataset contains two parts: a) objects shot on a turntable: 15 categories, 8 rotation angles, 11 cameras on a semi-circular arch, 5 lighting conditions, 3 focus levels, variety of backgrounds (23.4 per instance) generating 1320 images per instance (about 22 million images in total), and b) scenes: in which a robotic arm takes pictures of objects on a 1:160 scale scene. We study: 1) invariance and selectivity of different CNN layers, 2) knowledge transfer from one object category to another, 3) systematic or random sampling of images to build a train set, 4) domain adaptation from synthetic to natural scenes, and 5) order of knowledge delivery to CNNs. We also discuss how our analyses can lead the field to develop more efficient deep learning methods.",
        "pdf": "https://openaccess.thecvf.com/content_cvpr_2016/papers/Borji_iLab-20M_A_Large-Scale_CVPR_2016_paper.pdf",
        "aff": "Center for Research in Computer Vision, University of Central Florida; Amirkabir University of Technology; University of Southern California",
        "project": "",
        "github": "",
        "supp": "https://openaccess.thecvf.com/content_cvpr_2016/supplemental/Borji_iLab-20M_A_Large-Scale_2016_CVPR_supplemental.pdf",
        "arxiv": "",
        "pdf_size": 3879345,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15952601634393281679&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff_domain": "crcv.ucf.edu;aut.ac.ir;usc.edu",
        "email": "crcv.ucf.edu;aut.ac.ir;usc.edu",
        "author_num": 3,
        "oa": "https://openaccess.thecvf.com/content_cvpr_2016/html/Borji_iLab-20M_A_Large-Scale_CVPR_2016_paper.html",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Central Florida;Amirkabir University of Technology;University of Southern California",
        "aff_unique_dep": "Center for Research in Computer Vision;;",
        "aff_unique_url": "https://www.ucf.edu;https://www.aut.ac.ir;https://www.usc.edu",
        "aff_unique_abbr": "UCF;AUT;USC",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Orlando;;Los Angeles",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Iran"
    }
]