[
    {
        "id": "2024.findings-naacl.30",
        "title": "A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Community models for malicious content detection, which take into account the context from a social graph alongside the content itself, have shown remarkable performance on benchmark datasets. Yet, misinformation and hate speech continue to propagate on social media networks. This mismatch can be partially attributed to the limitations of current evaluation setups that neglect the rapid evolution of online content and the underlying social graph. In this paper, we propose a novel evaluation setup for model generalisation based on our few-shot subgraph sampling approach. This setup tests for generalisation through few labelled examples in local explorations of a larger graph, emulating more realistic application settings. We show this to be a challenging inductive setup, wherein strong performance on the training graph is not indicative of performance on unseen tasks, domains, or graph structures. Lastly, we show that graph meta-learners trained with our proposed few-shot subgraph sampling outperform standard community models in the inductive setup.",
        "author": "Ivo Verhoeven; Pushkar Mishra; Rahel Beloch; Helen Yannakoudakis; Ekaterina Shutova",
        "authorids": "/i/ivo-verhoeven/; /p/pushkar-mishra/; /r/rahel-beloch/; /h/helen-yannakoudakis/; /e/ekaterina-shutova/",
        "bibtex": "@inproceedings{verhoeven-etal-2024-realistic,\n    title = \"A (More) Realistic Evaluation Setup for Generalisation of Community Models on Malicious Content Detection\",\n    author = \"Verhoeven, Ivo  and\n      Mishra, Pushkar  and\n      Beloch, Rahel  and\n      Yannakoudakis, Helen  and\n      Shutova, Ekaterina\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.30/\",\n    doi = \"10.18653/v1/2024.findings-naacl.30\",\n    pages = \"437--463\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.30.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.30/",
        "pdf_size": 772848,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=425124400832016143&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "ILLC, University of Amsterdam, The Netherlands; AI at Meta, London, United Kingdom; ILLC, University of Amsterdam, The Netherlands; Dept. of Informatics, King\u2019s College London, United Kingdom; ILLC, University of Amsterdam, The Netherlands",
        "aff_domain": "uva.nl;meta.com;uva.nl;kcl.ac.uk;uva.nl",
        "email": "uva.nl;meta.com;uva.nl;kcl.ac.uk;uva.nl",
        "github": "https://github.com/rahelbeloch/meta-learning-gnns",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "University of Amsterdam;Meta;King\u2019s College London",
        "aff_unique_dep": "ILLC;AI at Meta;Dept. of Informatics",
        "aff_unique_url": "https://www.uva.nl;https://meta.com;https://www.kcl.ac.uk",
        "aff_unique_abbr": "UvA;Meta AI;KCL",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Amsterdam;London;",
        "aff_country_unique_index": "0;1;0;1;0",
        "aff_country_unique": "Netherlands;United Kingdom"
    },
    {
        "id": "2024.naacl-long.52",
        "title": "A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a series of models on their verification abilities. Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification methods. Drawing from these observations, we offer suggestions for future research and practical applications of self-verification methods.",
        "author": "Ruixin Hong; Hongming Zhang; Xinyu Pang; Dong Yu; Changshui Zhang",
        "authorids": "/r/ruixin-hong/; /h/hongming-zhang/; /x/xinyu-pang/; /d/dong-yu/; /c/changshui-zhang/",
        "bibtex": "@inproceedings{hong-etal-2024-closer,\n    title = \"A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning\",\n    author = \"Hong, Ruixin  and\n      Zhang, Hongming  and\n      Pang, Xinyu  and\n      Yu, Dong  and\n      Zhang, Changshui\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.52/\",\n    doi = \"10.18653/v1/2024.naacl-long.52\",\n    pages = \"900--925\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.52.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.52/",
        "pdf_size": 608269,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1176424994530457219&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Institute for Artificial Intelligence, Tsinghua University (THUAI) + Beijing National Research Center for Information Science and Technology (BNRist) + Department of Automation, Tsinghua University, Beijing, P.R.China; Tencent AI Lab, Seattle; Institute for Artificial Intelligence, Tsinghua University (THUAI) + Beijing National Research Center for Information Science and Technology (BNRist) + Department of Automation, Tsinghua University, Beijing, P.R.China; Tencent AI Lab, Seattle; Institute for Artificial Intelligence, Tsinghua University (THUAI) + Beijing National Research Center for Information Science and Technology (BNRist) + Department of Automation, Tsinghua University, Beijing, P.R.China",
        "aff_domain": "mails.tsinghua.edu.cn;tencent.com; ; ;",
        "email": "mails.tsinghua.edu.cn;tencent.com; ; ;",
        "github": "https://github.com/Raising-hrx/FALLACIES",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1+0;2;0+1+0;2;0+1+0",
        "aff_unique_norm": "Tsinghua University;Beijing National Research Center for Information Science and Technology;Tencent",
        "aff_unique_dep": "Institute for Artificial Intelligence;;AI Lab",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://ai.tencent.com",
        "aff_unique_abbr": "THU;BNRist;Tencent AI Lab",
        "aff_campus_unique_index": "1;2;1;2;1",
        "aff_campus_unique": ";Beijing;Seattle",
        "aff_country_unique_index": "0+0+0;1;0+0+0;1;0+0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.245",
        "title": "A Comprehensive Study of Gender Bias in Chemical Named Entity Recognition Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Chemical named entity recognition (NER) models are used in many downstream tasks, from adverse drug reaction identification to pharmacoepidemiology. However, it is unknown whether these models work the same for everyone. Performance disparities can potentially cause harm rather than the intended good. This paper assesses gender-related performance disparities in chemical NER systems. We develop a framework for measuring gender bias in chemical NER models using synthetic data and a newly annotated corpus of over 92,405 words with self-identified gender information from Reddit. Our evaluation of multiple biomedical NER models reveals evident biases. For instance, synthetic data suggests that female names are frequently misclassified as chemicals, especially when it comes to brand name mentions. Additionally, we observe performance disparities between female- and male-associated data in both datasets. Many systems fail to detect contraceptives such as birth control. Our findings emphasize the biases in chemical NER models, urging practitioners to account for these biases in downstream applications.",
        "author": "Xingmeng Zhao; Ali Niazi; Anthony Rios",
        "authorids": "/x/xingmeng-zhao/; /a/ali-niazi/; /a/anthony-rios/",
        "bibtex": "@inproceedings{zhao-etal-2024-comprehensive,\n    title = \"A Comprehensive Study of Gender Bias in Chemical Named Entity Recognition Models\",\n    author = \"Zhao, Xingmeng  and\n      Niazi, Ali  and\n      Rios, Anthony\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.245/\",\n    doi = \"10.18653/v1/2024.naacl-long.245\",\n    pages = \"4360--4374\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.245.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.245/",
        "pdf_size": 299120,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=425920448546959739&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.naacl-short.47",
        "title": "A Continued Pretrained LLM Approach for Automatic Medical Note Generation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "LLMs are revolutionizing NLP tasks. However, the use of the most advanced LLMs, such as GPT-4, is often prohibitively expensive for most specialized fields. We introduce HEAL, the first continuously trained 13B LLaMA2-based LLM that is purpose-built for medical conversations and measured on automated scribing. Our results demonstrate that HEAL outperforms GPT-4 and PMC-LLaMA in PubMedQA, with an accuracy of 78.4%. It also achieves parity with GPT-4 in generating medical notes. Remarkably, HEAL surpasses GPT-4 and Med-PaLM 2 in identifying more correct medical concepts and exceeds the performance of human scribes and other comparable models in correctness and completeness.",
        "author": "Dong Yuan; Eti Rastogi; Gautam Naik; Sree Prasanna Rajagopal; Sagar Goyal; Fen Zhao; Bharath Chintagunta; Jeffrey Ward",
        "authorids": "/d/dong-yuan/; /e/eti-rastogi/; /g/gautam-naik/; /s/sree-prasanna-rajagopal/; /s/sagar-goyal/; /f/fen-zhao/; /b/bharath-chintagunta/; /j/jeffrey-ward/",
        "bibtex": "@inproceedings{yuan-etal-2024-continued,\n    title = \"A Continued Pretrained {LLM} Approach for Automatic Medical Note Generation\",\n    author = \"Yuan, Dong  and\n      Rastogi, Eti  and\n      Naik, Gautam  and\n      Rajagopal, Sree Prasanna  and\n      Goyal, Sagar  and\n      Zhao, Fen  and\n      Chintagunta, Bharath  and\n      Ward, Jeffrey\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.47/\",\n    doi = \"10.18653/v1/2024.naacl-short.47\",\n    pages = \"565--571\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.47.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.47/",
        "pdf_size": 140921,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7021786028769999217&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2024.findings-naacl.154",
        "title": "A Lightweight Mixture-of-Experts Neural Machine Translation Model with Stage-wise Training Strategy",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Dealing with language heterogeneity has always been one of the challenges in neural machine translation (NMT).The idea of using mixture-of-experts (MoE) naturally excels in addressing this issue by employing different experts to take responsibility for different problems.However, the parameter-inefficiency problem in MoE results in less performance improvement when boosting the number of parameters.Moreover, most of the MoE models are suffering from the training instability problem.This paper proposes MoA (Mixture-of-Adapters), a lightweight MoE-based NMT model that is trained via an elaborately designed stage-wise training strategy.With the standard Transformer as the backbone model, we introduce lightweight adapters as experts for easy expansion.To improve the parameter efficiency, we explicitly model and distill the language heterogeneity into the gating network with clustering.After freezing the gating network, we adopt the Gumbel-Max sampling as the routing scheme when training experts to balance the knowledge of generalization and specialization while preventing expert over-fitting.Empirical results show that MoA achieves stable improvements in different translation tasks by introducing much fewer extra parameters compared to other MoE baselines.Additionally, the performance evaluations on a multi-domain translation task illustrate the effectiveness of our training strategy.",
        "author": "Fan Zhang; Mei Tu; Song Liu; Jinyao Yan",
        "authorids": "/f/fan-zhang/; /m/mei-tu/; /s/song-liu/; /j/jinyao-yan/",
        "bibtex": "@inproceedings{zhang-etal-2024-lightweight,\n    title = \"A Lightweight Mixture-of-Experts Neural Machine Translation Model with Stage-wise Training Strategy\",\n    author = \"Zhang, Fan  and\n      Tu, Mei  and\n      Liu, Song  and\n      Yan, Jinyao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.154/\",\n    doi = \"10.18653/v1/2024.findings-naacl.154\",\n    pages = \"2381--2392\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.154.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.154/",
        "pdf_size": 431539,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14264592806945922102&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "State Key Laboratory of Media Convergence and Communication, Communication University of China+Samsung R&D Institute China-Beijing; Samsung R&D Institute China-Beijing; Samsung R&D Institute China-Beijing; State Key Laboratory of Media Convergence and Communication, Communication University of China",
        "aff_domain": "samsung.com;samsung.com;samsung.com;cuc.edu.cn",
        "email": "samsung.com;samsung.com;samsung.com;cuc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;1;0",
        "aff_unique_norm": "Communication University of China;Samsung",
        "aff_unique_dep": "State Key Laboratory of Media Convergence and Communication;R&D",
        "aff_unique_url": "http://www.cuc.edu.cn/;https://www.samsung.com/cn",
        "aff_unique_abbr": "CUC;SRI-C",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.141",
        "title": "A Likelihood Ratio Test of Genetic Relationship among Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Lexical resemblances among a group of languages indicate that the languages could be genetically related, i.e., they could have descended from a common ancestral language. However, such resemblances can arise by chance and, hence, need not always imply an underlying genetic relationship. Many tests of significance based on permutation of wordlists and word similarity measures appeared in the past to determine the statistical significance of such relationships. We demonstrate that although existing tests may work well for bilateral comparisons, i.e., on pairs of languages, they are either infeasible by design or are prone to yield false positives when applied to groups of languages or language families. To this end, inspired by molecular phylogenetics, we propose a likelihood ratio test to determine if given languages are related based on the proportion of invariant character sites in the aligned wordlists applied during tree inference. Further, we evaluate some language families and show that the proposed test solves the problem of false positives. Finally, we demonstrate that the test supports the existence of macro language families such as Nostratic and Macro-Mayan.",
        "author": "V.S.D.S.Mahesh Akavarapu; Arnab Bhattacharya",
        "authorids": "/v/v-s-d-s-mahesh-akavarapu/; /a/arnab-bhattacharya/",
        "bibtex": "@inproceedings{akavarapu-bhattacharya-2024-likelihood,\n    title = \"A Likelihood Ratio Test of Genetic Relationship among Languages\",\n    author = \"Akavarapu, V.S.D.S.Mahesh  and\n      Bhattacharya, Arnab\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.141/\",\n    doi = \"10.18653/v1/2024.naacl-long.141\",\n    pages = \"2559--2570\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.141.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.141/",
        "pdf_size": 1093348,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5019862458657386891&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Dept. of Computer Science and Engineering, Indian Institute of Technology Kanpur; Dept. of Computer Science and Engineering, Indian Institute of Technology Kanpur",
        "aff_domain": "cse.iitk.ac.in;cse.iitk.ac.in",
        "email": "cse.iitk.ac.in;cse.iitk.ac.in",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur",
        "aff_unique_dep": "Dept. of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitk.ac.in",
        "aff_unique_abbr": "IIT Kanpur",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kanpur",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2024.findings-naacl.213",
        "title": "A Measure for Transparent Comparison of Linguistic Diversity in Multilingual NLP Data Sets",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Typologically diverse benchmarks are increasingly created to track the progress achieved in multilingual NLP. Linguistic diversity of these data sets is typically measured as the number of languages or language families included in the sample, but such measures do not consider structural properties of the included languages. In this paper, we propose assessing linguistic diversity of a data set against a reference language sample as a means of maximising linguistic diversity in the long run. We represent languages as sets of features and apply a version of the Jaccard index suitable for comparing sets of measures. In addition to the features extracted from typological data bases, we propose an automatic text-based measure, which can be used as a means of overcoming the well-known problem of data sparsity in manually collected features. Our diversity score is interpretable in terms of linguistic features and can identify the types of languages that are not represented in a data set. Using our method, we analyse a range of popular multilingual data sets (UD, Bible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD). In addition to ranking these data sets, we find, for example, that (poly)synthetic languages are missing in almost all of them.",
        "author": "Tanja Samardzic; Ximena Gutierrez; Christian Bentz; Steven Moran; Olga Pelloni",
        "authorids": "/t/tanja-samardzic/; /x/ximena-gutierrez/; /c/christian-bentz/; /s/steven-moran/; /o/olga-pelloni/",
        "bibtex": "@inproceedings{samardzic-etal-2024-measure,\n    title = \"A Measure for Transparent Comparison of Linguistic Diversity in Multilingual {NLP} Data Sets\",\n    author = \"Samardzic, Tanja  and\n      Gutierrez, Ximena  and\n      Bentz, Christian  and\n      Moran, Steven  and\n      Pelloni, Olga\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.213/\",\n    doi = \"10.18653/v1/2024.findings-naacl.213\",\n    pages = \"3367--3382\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.213.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.213/",
        "pdf_size": 1026618,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8332142708165399899&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Language and Space Lab, University of Zurich;CEIICH, Universidad Nacional Aut\u00f3noma de M\u00e9xico;Dept. of General Linguistics, Eberhard-Karls-Universit\u00e4t T\u00fcbingen;Laboratory of Language Evolution, University of Neuch\u00e2tel;Telepathy Labs GmbH, Zurich, Switzerland",
        "aff_domain": "uzh.ch;;uni-tuebingen.de;unine.ch;telepathy.ch",
        "email": "uzh.ch;;uni-tuebingen.de;unine.ch;telepathy.ch",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;4",
        "aff_unique_norm": "University of Zurich;Universidad Nacional Aut\u00f3noma de M\u00e9xico;Eberhard-Karls-Universit\u00e4t T\u00fcbingen;University of Neuch\u00e2tel;Telepathy Labs GmbH",
        "aff_unique_dep": "Language and Space Lab;CEIICH;Dept. of General Linguistics;Laboratory of Language Evolution;",
        "aff_unique_url": "https://www.unizh.ch;https://www.unam.mx;https://www.uni-tuebingen.de/;https://www.unine.ch;",
        "aff_unique_abbr": "UZH;UNAM;Uni T\u00fcbingen;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0;0",
        "aff_country_unique": "Switzerland;Mexico;Germany"
    },
    {
        "id": "2024.naacl-short.14",
        "title": "A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy. While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped. Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria. To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs. We found that LLM evaluators achieve strong alignment to human-annotated scores and feedback and outperform alternative metrics, indicating their potential as multi-aspect, reference-free and interpretable evaluators for counter narrative evaluation.",
        "author": "Jaylen Jones; Lingbo Mo; Eric Fosler-Lussier; Huan Sun",
        "authorids": "/j/jaylen-jones/; /l/lingbo-mo/; /e/eric-fosler-lussier/; /h/huan-sun/",
        "bibtex": "@inproceedings{jones-etal-2024-multi,\n    title = \"A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models\",\n    author = \"Jones, Jaylen  and\n      Mo, Lingbo  and\n      Fosler-Lussier, Eric  and\n      Sun, Huan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.14/\",\n    doi = \"10.18653/v1/2024.naacl-short.14\",\n    pages = \"147--168\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.14.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.14/",
        "pdf_size": 5056046,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8002373043235009054&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University",
        "aff_domain": "osu.edu;osu.edu;cse.ohio-state.edu;osu.edu",
        "email": "osu.edu;osu.edu;cse.ohio-state.edu;osu.edu",
        "github": "https://github.com/OSU-NLP-Group/LLM-CN-Eval",
        "project": "https://getthetrollsout.org/",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Ohio State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.42",
        "title": "A Novel Paradigm Boosting Translation Capabilities of Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper presents a study on strategies to enhance the translation capabilities of large language models (LLMs) in the context of machine translation (MT) tasks. The paper proposes a novel paradigm consisting of three stages: Secondary Pre-training using Extensive Monolingual Data, Continual Pre-training with Interlinear Text Format Documents, and Leveraging Source-Language Consistent Instruction for Supervised Fine-Tuning. Previous research on LLMs focused on various strategies for supervised fine-tuning (SFT), but their effectiveness has been limited. While traditional machine translation approaches rely on vast amounts of parallel bilingual data, our paradigm highlights the importance of using smaller sets of high-quality bilingual data. We argue that the focus should be on augmenting LLMs\u2019 cross-lingual alignment abilities during pre-training rather than solely relying on extensive bilingual data during SFT. Experimental results conducted using the Llama2(CITATION)model, particularly on Chinese-Llama2(CITATION) after monolingual augmentation, demonstrate the improved translation capabilities of LLMs. A significant contribution of our approach lies in Stage2: Continual Pre-training with Interlinear Text Format Documents, which requires less than 1B training data, making our method highly efficient. Additionally, in Stage3, we observed that setting instructions consistent with the source language benefits the supervised fine-tuning process. Experimental results demonstrate that our approach surpasses previous work and achieves superior performance compared to models such as NLLB-54B(CITATION) and GPT3.5-text-davinci-003, despite having a significantly smaller parameter count of only 7B or 13B. This achievement establishes our method as a pioneering strategy in the field of machine translation.",
        "author": "Jiaxin Guo; Hao Yang; Zongyao Li; Daimeng Wei; Hengchao Shang; Xiaoyu Chen",
        "authorids": "/j/jiaxin-guo/; /h/hao-yang/; /z/zongyao-li/; /d/daimeng-wei/; /h/hengchao-shang/; /x/xiaoyu-chen/",
        "bibtex": "@inproceedings{guo-etal-2024-novel,\n    title = \"A Novel Paradigm Boosting Translation Capabilities of Large Language Models\",\n    author = \"Guo, Jiaxin  and\n      Yang, Hao  and\n      Li, Zongyao  and\n      Wei, Daimeng  and\n      Shang, Hengchao  and\n      Chen, Xiaoyu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.42/\",\n    doi = \"10.18653/v1/2024.findings-naacl.42\",\n    pages = \"639--649\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.42.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.42/",
        "pdf_size": 359034,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=503811016887522301&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China; Huawei Translation Services Center, Beijing, China",
        "aff_domain": "huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "email": "huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Huawei",
        "aff_unique_dep": "Translation Services Center",
        "aff_unique_url": "https://www.huawei.com",
        "aff_unique_abbr": "Huawei",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.203",
        "title": "A Novel Two-step Fine-tuning Framework for Transfer Learning in Low-Resource Neural Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Existing transfer learning methods for neural machine translation typically use a well-trained translation model (i.e., a parent model) of a high-resource language pair to directly initialize a translation model (i.e., a child model) of a low-resource language pair, and the child model is then fine-tuned with corresponding datasets. In this paper, we propose a novel two-step fine-tuning (TSFT) framework for transfer learning in low-resource neural machine translation. In the first step, we adjust the parameters of the parent model to fit the child language by using the child source data. In the second step, we transfer the adjusted parameters to the child model and fine-tune it with a proposed distillation loss for efficient optimization. Our experimental results on five low-resource translations demonstrate that our framework yields significant improvements over various strong transfer learning baselines. Further analysis demonstrated the effectiveness of different components in our framework.",
        "author": "Yuan Gao; Feng Hou; Ruili Wang",
        "authorids": "/y/yuan-gao/; /f/feng-hou/; /r/ruili-wang/",
        "bibtex": "@inproceedings{gao-etal-2024-novel,\n    title = \"A Novel Two-step Fine-tuning Framework for Transfer Learning in Low-Resource Neural Machine Translation\",\n    author = \"Gao, Yuan  and\n      Hou, Feng  and\n      Wang, Ruili\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.203/\",\n    doi = \"10.18653/v1/2024.findings-naacl.203\",\n    pages = \"3214--3224\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.203.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.203/",
        "pdf_size": 926889,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10274843752235775840&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Mathematical and Computational Science, Massey University, New Zealand; School of Mathematical and Computational Science, Massey University, New Zealand; School of Mathematical and Computational Science, Massey University, New Zealand",
        "aff_domain": "massey.ac.nz;massey.ac.nz;massey.ac.nz",
        "email": "massey.ac.nz;massey.ac.nz;massey.ac.nz",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massey University",
        "aff_unique_dep": "School of Mathematical and Computational Science",
        "aff_unique_url": "https://www.massey.ac.nz",
        "aff_unique_abbr": "Massey",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "New Zealand"
    },
    {
        "id": "2024.naacl-long.186",
        "title": "A Preference-driven Paradigm for Enhanced Translation with Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent research has shown that large language models (LLMs) can achieve remarkable translation performance through supervised fine-tuning (SFT) using only a small amount of parallel data. However, SFT simply instructs the model to imitate the reference translations at the token level, making it vulnerable to the noise present in the references. Hence, the assistance from SFT often reaches a plateau once the LLMs have achieved a certain level of translation capability, and further increasing the size of parallel data does not provide additional benefits. To overcome this plateau associated with imitation-based SFT, we propose a preference-based approach built upon the Plackett-Luce model. The objective is to steer LLMs towards a more nuanced understanding of translation preferences from a holistic view, while also being more resilient in the absence of gold translations. We further build a dataset named MAPLE to verify the effectiveness of our approach, which includes multiple translations of varying quality for each source sentence. Extensive experiments demonstrate the superiority of our approach in \u201cbreaking the plateau\u201d across diverse LLMs and test settings. Our in-depth analysis underscores the pivotal role of diverse translations and accurate preference scores in the success of our approach.",
        "author": "Dawei Zhu; Sony Trenous; Xiaoyu Shen; Dietrich Klakow; Bill Byrne; Eva Hasler",
        "authorids": "/d/dawei-zhu/; /s/sony-trenous/; /x/xiaoyu-shen/; /d/dietrich-klakow/; /b/bill-byrne/; /e/eva-hasler/",
        "bibtex": "@inproceedings{zhu-etal-2024-preference,\n    title = \"A Preference-driven Paradigm for Enhanced Translation with Large Language Models\",\n    author = \"Zhu, Dawei  and\n      Trenous, Sony  and\n      Shen, Xiaoyu  and\n      Klakow, Dietrich  and\n      Byrne, Bill  and\n      Hasler, Eva\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.186/\",\n    doi = \"10.18653/v1/2024.naacl-long.186\",\n    pages = \"3385--3403\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.186.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.186/",
        "pdf_size": 779528,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9456207860960328491&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Amazon AGI+Saarland University, Saarland Informatics Campus; Amazon AGI; Amazon AGI+Eastern Institute of Technology, Ningbo; Saarland University, Saarland Informatics Campus; Amazon AGI; Amazon AGI",
        "aff_domain": "amazon.com;amazon.com;amazon.com;uni-saarland.de;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;uni-saarland.de;amazon.com;amazon.com",
        "github": "https://github.com/amazon-science/preference-driven-mt",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;0+2;1;0;0",
        "aff_unique_norm": "Amazon;Saarland University;Eastern Institute of Technology",
        "aff_unique_dep": "Amazon AGI;;",
        "aff_unique_url": "https://www.amazon.com;https://www.uni-saarland.de;https://www.eit.edu.cn",
        "aff_unique_abbr": "Amazon;UdS;",
        "aff_campus_unique_index": "1;2;1",
        "aff_campus_unique": ";Saarland Informatics Campus;Ningbo",
        "aff_country_unique_index": "0+1;0;0+2;1;0;0",
        "aff_country_unique": "United States;Germany;China"
    },
    {
        "id": "2024.naacl-long.179",
        "title": "A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. We pretrain models on data curated (1) at different collection times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we find that temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we measure the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Third, we empirically validate that heterogeneous data sources, like books and web, are beneficial and warrant greater prioritization. To date, these experiments constitute the single largest publicly documented empirical study of the effects of pretraining data. Spanning 28 unique 1.5 billion parameter models pretrained from scratch, these findings validate, quantify, and expose many undocumented intuitions about text pretraining, which ultimately support more informed data-centric decisions in model development.",
        "author": "Shayne Longpre; Gregory Yauney; Emily Reif; Katherine Lee; Adam Roberts; Barret Zoph; Denny Zhou; Jason Wei; Kevin Robinson; David Mimno; Daphne Ippolito",
        "authorids": "/s/shayne-longpre/; /g/gregory-yauney/; /e/emily-reif/; /k/katherine-lee/; /a/adam-roberts/; /b/barret-zoph/; /d/denny-zhou/; /j/jason-wei/; /k/kevin-robinson/; /d/david-mimno/; /d/daphne-ippolito/",
        "bibtex": "@inproceedings{longpre-etal-2024-pretrainers,\n    title = \"A Pretrainer{'}s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, {\\&} Toxicity\",\n    author = \"Longpre, Shayne  and\n      Yauney, Gregory  and\n      Reif, Emily  and\n      Lee, Katherine  and\n      Roberts, Adam  and\n      Zoph, Barret  and\n      Zhou, Denny  and\n      Wei, Jason  and\n      Robinson, Kevin  and\n      Mimno, David  and\n      Ippolito, Daphne\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.179/\",\n    doi = \"10.18653/v1/2024.naacl-long.179\",\n    pages = \"3245--3276\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.179.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.179/",
        "pdf_size": 619970,
        "gs_citation": 155,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7369749861546445194&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "MIT+Google Research; Cornell+Google Research; Google Research; Google Research+Cornell; Google Research; OpenAI+Google Research; Google Research; OpenAI+Google Research; Google Research; Cornell+Google Research; Google Research+Carnegie Mellon University",
        "aff_domain": "mit.edu;cs.cornell.edu;cornell.edu;google.com;google.com;google.com;google.com;google.com;google.com;gmail.com;gmail.com",
        "email": "mit.edu;cs.cornell.edu;cornell.edu;google.com;google.com;google.com;google.com;google.com;google.com;gmail.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0+1;2+1;1;1+2;1;3+1;1;3+1;1;2+1;1+4",
        "aff_unique_norm": "Massachusetts Institute of Technology;Google;Cornell University;OpenAI;Carnegie Mellon University",
        "aff_unique_dep": ";Google Research;;;",
        "aff_unique_url": "https://web.mit.edu;https://research.google;https://www.cornell.edu;https://openai.com;https://www.cmu.edu",
        "aff_unique_abbr": "MIT;Google Research;Cornell;OpenAI;CMU",
        "aff_campus_unique_index": "1;1;1;1;1;1;1;1;1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0+0;0+0;0;0+0;0;0+0;0;0+0;0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.63",
        "title": "A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Based on Pre-trained Language Models (PLMs), event coreference resolution (ECR) systems have demonstrated outstanding performance in clustering coreferential events across documents. However, the state-of-the-art system exhibits an excessive reliance on the \u2018triggers lexical matching\u2019 spurious pattern in the input mention pair text. We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task. Leveraging the debiasing capability of counterfactual data augmentation, we develop a rationale-centric counterfactual data augmentation method with LLM-in-the-loop. This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation. Our approach achieves state-of-the-art performance on three popular cross-document ECR benchmarks and demonstrates robustness in out-of-domain scenarios.",
        "author": "Bowen Ding; Qingkai Min; Shengkun Ma; Yingjie Li; Linyi Yang; Yue Zhang",
        "authorids": "/b/bowen-ding/; /q/qingkai-min/; /s/shengkun-ma/; /y/yingjie-li/; /l/linyi-yang/; /y/yue-zhang/",
        "bibtex": "@inproceedings{ding-etal-2024-rationale,\n    title = \"A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution\",\n    author = \"Ding, Bowen  and\n      Min, Qingkai  and\n      Ma, Shengkun  and\n      Li, Yingjie  and\n      Yang, Linyi  and\n      Zhang, Yue\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.63/\",\n    doi = \"10.18653/v1/2024.naacl-long.63\",\n    pages = \"1112--1140\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.63.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.63/",
        "pdf_size": 4472162,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:bLEoi2RNSkMJ:scholar.google.com/&scioq=A+Rationale-centric+Counterfactual+Data+Augmentation+Method+for+Cross-Document+Event+Coreference+Resolution&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "Zhejiang University + School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study; School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study; Beijing University of Posts and Telecommunications; School of Engineering, Westlake University; School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study; School of Engineering, Westlake University + Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "westlake.edu.cn;westlake.edu.cn;bupt.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn",
        "email": "westlake.edu.cn;westlake.edu.cn;bupt.edu.cn;westlake.edu.cn;westlake.edu.cn;westlake.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1+2;1+2;3;1;1+2;1+2",
        "aff_unique_norm": "Zhejiang University;Westlake University;Westlake Institute for Advanced Study;Beijing University of Posts and Telecommunications",
        "aff_unique_dep": ";School of Engineering;Institute of Advanced Technology;",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.westlake.edu.cn;http://www.wias.org.cn/;http://www.bupt.edu.cn/",
        "aff_unique_abbr": "ZJU;;WIAS;BUPT",
        "aff_campus_unique_index": ";;1;;",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0+0;0+0;0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.40",
        "title": "A Robust Semantics-based Watermark for Large Language Model against Paraphrasing",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) have show their remarkable ability in various natural language tasks. However, there are concerns that LLMs are possible to be used improperly or even illegally. To prevent the malicious usage of LLMs, detecting LLM-generated text becomes crucial in the deployment of LLM applications. Watermarking is an effective strategy to detect the LLM-generated content by encoding a pre-defined secret watermark to facilitate the detection process. However, the majority of existing watermark methods leverage the simple hashes of precedent tokens to partition vocabulary. Such watermarks can be easily eliminated by paraphrase and, correspondingly, the detection effectiveness will be greatly compromised. Thus, to enhance the robustness against paraphrase, we propose a semantics-based watermark framework, SemaMark. It leverages the semantics as an alternative to simple hashes of tokens since the semantic meaning of the sentences will be likely preserved under paraphrase and the watermark can remain robust. Comprehensive experiments are conducted to demonstrate the effectiveness and robustness of SemaMark under different paraphrases.",
        "author": "Jie Ren; Han Xu; Yiding Liu; Yingqian Cui; Shuaiqiang Wang; Dawei Yin; Jiliang Tang",
        "authorids": "/j/jie-ren/; /h/han-xu/; /y/yiding-liu/; /y/yingqian-cui/; /s/shuaiqiang-wang/; /d/dawei-yin/; /j/jiliang-tang/",
        "bibtex": "@inproceedings{ren-etal-2024-robust,\n    title = \"A Robust Semantics-based Watermark for Large Language Model against Paraphrasing\",\n    author = \"Ren, Jie  and\n      Xu, Han  and\n      Liu, Yiding  and\n      Cui, Yingqian  and\n      Wang, Shuaiqiang  and\n      Yin, Dawei  and\n      Tang, Jiliang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.40/\",\n    doi = \"10.18653/v1/2024.findings-naacl.40\",\n    pages = \"613--625\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.40.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.40/",
        "pdf_size": 1356527,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10125251620278159541&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Michigan State University; Michigan State University; Baidu Inc.; Michigan State University; Baidu Inc.; Baidu Inc.; Michigan State University",
        "aff_domain": "msu.edu;msu.edu;gmail.com;msu.edu;gmail.com;acm.org;msu.edu",
        "email": "msu.edu;msu.edu;gmail.com;msu.edu;gmail.com;acm.org;msu.edu",
        "github": "github.com/renjie3/SemaMark",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;1;1;0",
        "aff_unique_norm": "Michigan State University;Baidu",
        "aff_unique_dep": ";Baidu Inc.",
        "aff_unique_url": "https://www.msu.edu;https://www.baidu.com",
        "aff_unique_abbr": "MSU;Baidu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.145",
        "title": "A School Student Essay Corpus for Analyzing Interactions of Argumentative Structure and Quality",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Learning argumentative writing is challenging. Besides writing fundamentals such as syntax and grammar, learners must select and arrange argument components meaningfully to create high-quality essays. To support argumentative writing computationally, one step is to mine the argumentative structure. When combined with automatic essay scoring, interactions of the argumentative structure and quality scores can be exploited for comprehensive writing support. Although studies have shown the usefulness of using information about the argumentative structure for essay scoring, no argument mining corpus with ground-truth essay quality annotations has been published yet. Moreover, none of the existing corpora contain essays written by school students specifically. To fill this research gap, we present a German corpus of 1,320 essays from school students of two age groups. Each essay has been manually annotated for argumentative structure and quality on multiple levels of granularity. We propose baseline approaches to argument mining and essay scoring, and we analyze interactions between both tasks, thereby laying the ground for quality-oriented argumentative writing support.",
        "author": "Maja Stahl; Nadine Michel; Sebastian Kilsbach; Julian Schmidtke; Sara Rezat; Henning Wachsmuth",
        "authorids": "/m/maja-stahl/; /n/nadine-michel/; /s/sebastian-kilsbach/; /j/julian-schmidtke/; /s/sara-rezat/; /h/henning-wachsmuth/",
        "bibtex": "@inproceedings{stahl-etal-2024-school,\n    title = \"A School Student Essay Corpus for Analyzing Interactions of Argumentative Structure and Quality\",\n    author = \"Stahl, Maja  and\n      Michel, Nadine  and\n      Kilsbach, Sebastian  and\n      Schmidtke, Julian  and\n      Rezat, Sara  and\n      Wachsmuth, Henning\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.145/\",\n    doi = \"10.18653/v1/2024.naacl-long.145\",\n    pages = \"2661--2674\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.145.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.145/",
        "pdf_size": 943865,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10722405336159525960&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Leibniz University Hannover, Institute of Artificial Intelligence; Paderborn University, Institute for German Language and Comparative Literature; Paderborn University, Institute for German Language and Comparative Literature; Leibniz University Hannover, Institute of Artificial Intelligence; Paderborn University, Institute for German Language and Comparative Literature; Leibniz University Hannover, Institute of Artificial Intelligence",
        "aff_domain": "ai.uni-hannover.de;uni-paderborn.de;uni-paderborn.de;stud.uni-hannover.de;uni-paderborn.de;ai.uni-hannover.de",
        "email": "ai.uni-hannover.de;uni-paderborn.de;uni-paderborn.de;stud.uni-hannover.de;uni-paderborn.de;ai.uni-hannover.de",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;0;1;0",
        "aff_unique_norm": "Leibniz University Hannover;Paderborn University",
        "aff_unique_dep": "Institute of Artificial Intelligence;Institute for German Language and Comparative Literature",
        "aff_unique_url": "https://www.uni-hannover.de;https://www.upb.de/",
        "aff_unique_abbr": "LUH;UPB",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.findings-naacl.260",
        "title": "A Study on Scaling Up Multilingual News Framing Analysis",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Media framing is the study of strategically selecting and presenting specific aspects of political issues to shape public opinion. Despite its relevance to almost all societies around the world, research has been limited due to the lack of available datasets and other resources. This study explores the possibility of dataset creation through crowdsourcing, utilizing non-expert annotators to develop training corpora. We first extend framing analysis beyond English news to a multilingual context (12 typologically diverse languages) through automatic translation. We also present a novel benchmark in Bengali and Portuguese on the immigration and same-sex marriage domains.Additionally, we show that a system trained on our crowd-sourced dataset, combined with other existing ones, leads to a 5.32 percentage point increase from the baseline, showing that crowdsourcing is a viable option. Last, we study the performance of large language models (LLMs) for this task, finding that task-specific fine-tuning is a better approach than employing bigger non-specialized models.",
        "author": "Syeda Sabrina Akter; Antonios Anastasopoulos",
        "authorids": "/s/syeda-sabrina-akter/; /a/antonios-anastasopoulos/",
        "bibtex": "@inproceedings{akter-anastasopoulos-2024-study,\n    title = \"A Study on Scaling Up Multilingual News Framing Analysis\",\n    author = \"Akter, Syeda Sabrina  and\n      Anastasopoulos, Antonios\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.260/\",\n    doi = \"10.18653/v1/2024.findings-naacl.260\",\n    pages = \"4156--4173\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.260.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.260/",
        "pdf_size": 711787,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:4PsJ2UhytTgJ:scholar.google.com/&scioq=A+Study+on+Scaling+Up+Multilingual+News+Framing+Analysis&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, George Mason University; Department of Computer Science, George Mason University",
        "aff_domain": "gmu.edu;gmu.edu",
        "email": "gmu.edu;gmu.edu",
        "github": "https://github.com/syedasabrina/Scaling-up-multilingual-framing-analysis.git",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "George Mason University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.gmu.edu",
        "aff_unique_abbr": "GMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.340",
        "title": "A Study on the Calibration of In-context Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Accurate uncertainty quantification is crucial for the safe deployment of machine learning models, and prior research has demonstrated improvements in the calibration of modern language models (LMs). We study in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examine the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations. Furthermore, we explore recalibration techniques and find that a scaling-binning calibrator can reduce calibration errors consistently.",
        "author": "Hanlin Zhang; YiFan Zhang; Yaodong Yu; Dhruv Madeka; Dean Foster; Eric Xing; Himabindu Lakkaraju; Sham Kakade",
        "authorids": "/h/hanlin-zhang/; /y/yifan-zhang/; /y/yaodong-yu/; /d/dhruv-madeka/; /d/dean-foster/; /e/eric-xing/; /h/himabindu-lakkaraju/; /s/sham-kakade/",
        "bibtex": "@inproceedings{zhang-etal-2024-study,\n    title = \"A Study on the Calibration of In-context Learning\",\n    author = \"Zhang, Hanlin  and\n      Zhang, YiFan  and\n      Yu, Yaodong  and\n      Madeka, Dhruv  and\n      Foster, Dean  and\n      Xing, Eric  and\n      Lakkaraju, Himabindu  and\n      Kakade, Sham\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.340/\",\n    doi = \"10.18653/v1/2024.naacl-long.340\",\n    pages = \"6118--6136\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.340.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.340/",
        "pdf_size": 642847,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9259017976506428729&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2024.naacl-long.366",
        "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks in various domains. Despite their impressive performance, they can be unreliable due to factual errors in their generations. Assessing their confidence and calibrating them across different tasks can help mitigate risks and enable LLMs to produce better generations. There has been a lot of recent research aiming to address this, but there has been no comprehensive overview to organize it and to outline the main lessons learned. The present survey aims to bridge this gap. In particular, we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration. We further discuss their applications and suggest promising directions for future work.",
        "author": "Jiahui Geng; Fengyu Cai; Yuxia Wang; Heinz Koeppl; Preslav Nakov; Iryna Gurevych",
        "authorids": "/j/jiahui-geng/; /f/fengyu-cai/; /y/yuxia-wang/; /h/heinz-koeppl/; /p/preslav-nakov/; /i/iryna-gurevych/",
        "bibtex": "@inproceedings{geng-etal-2024-survey,\n    title = \"A Survey of Confidence Estimation and Calibration in Large Language Models\",\n    author = \"Geng, Jiahui  and\n      Cai, Fengyu  and\n      Wang, Yuxia  and\n      Koeppl, Heinz  and\n      Nakov, Preslav  and\n      Gurevych, Iryna\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.366/\",\n    doi = \"10.18653/v1/2024.naacl-long.366\",\n    pages = \"6577--6595\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.366.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.366/",
        "pdf_size": 316143,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9402623568413844978&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Mohamed bin Zayed University of Artificial Intelligence; Technical University of Darmstadt; Mohamed bin Zayed University of Artificial Intelligence; Technical University of Darmstadt; Mohamed bin Zayed University of Artificial Intelligence; Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "mbzuai.ac.ae;tu-darmstadt.de;mbzuai.ac.ae;tu-darmstadt.de;mbzuai.ac.ae;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae;tu-darmstadt.de;mbzuai.ac.ae;tu-darmstadt.de;mbzuai.ac.ae;mbzuai.ac.ae",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;1;0;0",
        "aff_unique_norm": "Mohamed bin Zayed University of Artificial Intelligence;Technical University of Darmstadt",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://mbzuai.ac.ae;https://www.tu-darmstadt.de",
        "aff_unique_abbr": "MBZUAI;TUD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;1;0;0",
        "aff_country_unique": "United Arab Emirates;Germany"
    },
    {
        "id": "2024.naacl-long.159",
        "title": "A Survey of Meaning Representations \u2013 From Theory to Practical Utility",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Symbolic meaning representations of natural language text have been studied since at least the 1960s. With the availability of large annotated corpora, and more powerful machine learning tools, the field has recently seen several new developments. In this survey, we study today\u2019s most prominent Meaning Representation Frameworks. We shed light on their theoretical properties, as well as on their practical research environment, i.e., on datasets, parsers, applications, and future challenges.",
        "author": "Zacchary Sadeddine; Juri Opitz; Fabian Suchanek",
        "authorids": "/z/zacchary-sadeddine/; /j/juri-opitz/; /f/fabian-suchanek/",
        "bibtex": "@inproceedings{sadeddine-etal-2024-survey,\n    title = \"A Survey of Meaning Representations {--} From Theory to Practical Utility\",\n    author = \"Sadeddine, Zacchary  and\n      Opitz, Juri  and\n      Suchanek, Fabian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.159/\",\n    doi = \"10.18653/v1/2024.naacl-long.159\",\n    pages = \"2877--2892\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.159.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.159/",
        "pdf_size": 420327,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5169153651569194715&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris, France; University of Zurich, Switzerland; T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris, France",
        "aff_domain": "telecom-paris.fr;gmail.com;telecom-paris.fr",
        "email": "telecom-paris.fr;gmail.com;telecom-paris.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "T\u00e9l\u00e9com Paris;University of Zurich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.telecom-paris.fr;https://www.unizh.ch",
        "aff_unique_abbr": "T\u00e9l\u00e9com Paris;UZH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "France;Switzerland"
    },
    {
        "id": "2024.naacl-long.84",
        "title": "A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper proposes a methodology for generating and perturbing detailed derivations of equations at scale, aided by a symbolic engine, to evaluate the generalisability of Transformers to out-of-distribution mathematical reasoning problems. Instantiating the framework in the context of sequence classification tasks, we compare the capabilities of GPT-4, GPT-3.5, and a canon of fine-tuned BERT models, exploring the relationship between specific operators and generalisation failure via the perturbation of reasoning aspects such as symmetry and variable surface forms. Surprisingly, our empirical evaluation reveals that the average in-distribution performance of fine-tuned models surpasses GPT-3.5, and rivals GPT-4. However, perturbations to input reasoning can reduce their performance by up to 80 F1 points. Overall, the results suggest that the in-distribution performance of smaller open-source models may potentially rival GPT by incorporating appropriately structured derivation dependencies during training, and highlight a shared weakness between BERT and GPT involving a relative inability to decode indirect references to mathematical entities. We release the full codebase, constructed datasets, and fine-tuned models to encourage future progress in the field.",
        "author": "Jordan Meadows; Marco Valentino; Damien Teney; Andre Freitas",
        "authorids": "/j/jordan-meadows/; /m/marco-valentino/; /d/damien-teney/; /a/andre-freitas/",
        "bibtex": "@inproceedings{meadows-etal-2024-symbolic,\n    title = \"A Symbolic Framework for Evaluating Mathematical Reasoning and Generalisation with Transformers\",\n    author = \"Meadows, Jordan  and\n      Valentino, Marco  and\n      Teney, Damien  and\n      Freitas, Andre\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.84/\",\n    doi = \"10.18653/v1/2024.naacl-long.84\",\n    pages = \"1505--1523\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.84.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.84/",
        "pdf_size": 1011978,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16480993459002920179&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Manchester, United Kingdom; Idiap Research Institute, Switzerland; Idiap Research Institute, Switzerland; University of Manchester, United Kingdom+Idiap Research Institute, Switzerland",
        "aff_domain": "postgrad.manchester.ac.uk;idiap.ch;idiap.ch;idiap.ch",
        "email": "postgrad.manchester.ac.uk;idiap.ch;idiap.ch;idiap.ch",
        "github": "https://github.com/jmeadows17/transformers-for-calculus",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0+1",
        "aff_unique_norm": "University of Manchester;Idiap Research Institute",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.manchester.ac.uk;https://www.idiap.ch",
        "aff_unique_abbr": "UoM;Idiap",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0+1",
        "aff_country_unique": "United Kingdom;Switzerland"
    },
    {
        "id": "2024.findings-naacl.141",
        "title": "A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Multilingual modelling can improve machine translation for low-resource languages, partly through shared subword representations. This paper studies the role of subword segmentation in cross-lingual transfer. We systematically compare the efficacy of several subword methods in promoting synergy and preventing interference across different linguistic typologies. Our findings show that subword regularisation boosts synergy in multilingual modelling, whereas BPE more effectively facilitates transfer during cross-lingual fine-tuning. Notably, our results suggest that differences in orthographic word boundary conventions (the morphological granularity of written words) may impede cross-lingual transfer more significantly than linguistic unrelatedness. Our study confirms that decisions around subword modelling can be key to optimising the benefits of multilingual modelling.",
        "author": "Francois Meyer; Jan Buys",
        "authorids": "/f/francois-meyer/; /j/jan-buys/",
        "bibtex": "@inproceedings{meyer-buys-2024-systematic,\n    title = \"A Systematic Analysis of Subwords and Cross-Lingual Transfer in Multilingual Translation\",\n    author = \"Meyer, Francois  and\n      Buys, Jan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.141/\",\n    doi = \"10.18653/v1/2024.findings-naacl.141\",\n    pages = \"2194--2200\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.141.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.141/",
        "pdf_size": 209336,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5685774362178167577&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Cape Town; Department of Computer Science, University of Cape Town",
        "aff_domain": "uct.ac.za;cs.uct.ac.za",
        "email": "uct.ac.za;cs.uct.ac.za",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cape Town",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.uct.ac.za",
        "aff_unique_abbr": "UCT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Africa"
    },
    {
        "id": "2024.naacl-long.240",
        "title": "A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC). Current evaluations typically focus on a specific task known as Graded Change Detection (GCD). However, performance comparison across work are often misleading due to their reliance on diverse settings. In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions. We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, and compare models across these different levels. Our evaluation is performed across different languages on eight available benchmarks for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need for improving the modeling of word meanings, as well as focus on *how*, *when*, and *why* these meanings change, rather than solely focusing on the extent of semantic change.",
        "author": "Francesco Periti; Nina Tahmasebi",
        "authorids": "/f/francesco-periti/; /n/nina-tahmasebi/",
        "bibtex": "@inproceedings{periti-tahmasebi-2024-systematic,\n    title = \"A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change\",\n    author = \"Periti, Francesco  and\n      Tahmasebi, Nina\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.240/\",\n    doi = \"10.18653/v1/2024.naacl-long.240\",\n    pages = \"4262--4282\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.240.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.240/",
        "pdf_size": 1632720,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5734078379797076927&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Milan; University of Gothenburg",
        "aff_domain": "unimi.it;gu.se",
        "email": "unimi.it;gu.se",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Milan;University of Gothenburg",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unimi.it;https://www.gu.se",
        "aff_unique_abbr": "UniMi;GU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Italy;Sweden"
    },
    {
        "id": "2024.naacl-long.466",
        "title": "A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans\u2019 inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate such human biases, or are they able to overcome them? Focusing on the case of syllogisms\u2014inferences from two simple premises\u2014we show that, within the PaLM 2 family of transformer language models, larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases: they show sensitivity to the (irrelevant) ordering of the variables in the syllogism, and draw confident but incorrect inferences from particular syllogisms (syllogistic fallacies). Overall, we find that language models often mimic the human biases included in their training data, but are able to overcome them in some cases.",
        "author": "Tiwalayo Eisape; Michael Tessler; Ishita Dasgupta; Fei Sha; Sjoerd Steenkiste; Tal Linzen",
        "authorids": "/t/tiwalayo-eisape/; /m/michael-tessler/; /i/ishita-dasgupta/; /f/fei-sha/; /s/sjoerd-steenkiste/; /t/tal-linzen/",
        "bibtex": "@inproceedings{eisape-etal-2024-systematic,\n    title = \"A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models\",\n    author = \"Eisape, Tiwalayo  and\n      Tessler, Michael  and\n      Dasgupta, Ishita  and\n      Sha, Fei  and\n      Steenkiste, Sjoerd  and\n      Linzen, Tal\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.466/\",\n    doi = \"10.18653/v1/2024.naacl-long.466\",\n    pages = \"8425--8444\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.466.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.466/",
        "pdf_size": 5724866,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1488268212595618188&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Massachusetts Institute of Technology\u2020; Google DeepMind\u2021; Google Research\u00a7; Google Research\u00a7; Google Research\u00a7; Google Research\u00a7",
        "aff_domain": "mit.edu; ; ; ;google.com;google.com",
        "email": "mit.edu; ; ; ;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Google",
        "aff_unique_dep": ";Google DeepMind",
        "aff_unique_url": "https://web.mit.edu;https://deepmind.com",
        "aff_unique_abbr": "MIT;DeepMind",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;0;0;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2024.naacl-long.428",
        "title": "A Theory Guided Scaffolding Instruction Framework for LLM-Enabled Metaphor Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Metaphor detection is a challenging task in figurative language processing, which aims to distinguish between metaphorical and literal expressions in text. Existing methods tackle metaphor detection via training or fine-tuning discriminative models on labeled data. However, these approaches struggle to explain the underlying reasoning process behind the metaphorical/literal judgment. Recently, large language models (LLMs) have shown promise in language reasoning tasks. Although promising, LLM-based methods for metaphor detection and reasoning are still faced with the challenging issue of bringing the explainable concepts for metaphor reasoning and their linguistic manifestation. To fill this gap, we propose a novel Theory guided Scaffolding Instruction (TSI) framework that instructs an LLM to infer the underlying reasoning process of metaphor detection guided by metaphor theories for the first time. Our work is inspired by a pedagogical strategy called scaffolding instruction, which encourages educators to provide questioning and support as scaffolding so as to assist learners in constructing the understanding of pedagogical goals step by step. We first construct a metaphor knowledge graph grounded in metaphor theory which serves as the instructional structure to obtain a series of scaffolding questions, directing the LLM to incrementally generate the reasoning process for metaphor understanding through dialogue interactions. During this theory guided instruction process, we explore the LLM\u2019s mastery boundary and provide the relevant knowledge as scaffolding support when the question is beyond the LLM\u2019s capability. Experimental results verify that our method significantly outperforms both the LLM-based reasoning methods and the SOTA methods in metaphor detection, indicating the facilitation of metaphor and instruction theories in guiding LLM-based reasoning process.",
        "author": "Yuan Tian; Nan Xu; Wenji Mao",
        "authorids": "/y/yuan-tian/; /n/nan-xu/; /w/wenji-mao/",
        "bibtex": "@inproceedings{tian-etal-2024-theory,\n    title = \"A Theory Guided Scaffolding Instruction Framework for {LLM}-Enabled Metaphor Reasoning\",\n    author = \"Tian, Yuan  and\n      Xu, Nan  and\n      Mao, Wenji\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.428/\",\n    doi = \"10.18653/v1/2024.naacl-long.428\",\n    pages = \"7738--7755\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.428.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.428/",
        "pdf_size": 999728,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12048526836814825262&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences + School of Artificial Intelligence, University of Chinese Academy of Sciences; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences + Beijing Wenge Technology Co., Ltd; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences + School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "aff_domain": "ia.ac.cn;ia.ac.cn;ia.ac.cn",
        "email": "ia.ac.cn;ia.ac.cn;ia.ac.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Beijing Wenge Technology Co., Ltd",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.269",
        "title": "A Transformer with Stack Attention",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Natural languages are believed to be (mildly) context-sensitive. Despite underpinning remarkably capable large language models, transformers are unable to model many context-free language tasks. In an attempt to address this limitation in the modeling power of transformer-based language models, we propose augmenting them with a differentiable, stack-based attention mechanism. Our stack-basedattention mechanism can be incorporated into any transformer-based language model and adds a level of interpretability to the model. We show that the addition of our stack-based attention mechanism enables the transformer to model some, but not all, deterministic context-freelanguages.",
        "author": "Jiaoda Li; Jennifer White; Mrinmaya Sachan; Ryan Cotterell",
        "authorids": "/j/jiaoda-li/; /j/jennifer-white/; /m/mrinmaya-sachan/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{li-etal-2024-transformer,\n    title = \"A Transformer with Stack Attention\",\n    author = \"Li, Jiaoda  and\n      White, Jennifer  and\n      Sachan, Mrinmaya  and\n      Cotterell, Ryan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.269/\",\n    doi = \"10.18653/v1/2024.findings-naacl.269\",\n    pages = \"4318--4335\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.269.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.269/",
        "pdf_size": 556759,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10633319327867708957&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "ETH Z\u00fcrich; University of Cambridge; ETH Z\u00fcrich; ETH Z\u00fcrich",
        "aff_domain": "inf.ethz.ch;cam.ac.uk;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;cam.ac.uk;inf.ethz.ch;inf.ethz.ch",
        "github": "https://github.com/rycolab/stack-transformer",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "ETH Zurich;University of Cambridge",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.cam.ac.uk",
        "aff_unique_abbr": "ETHZ;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Switzerland;United Kingdom"
    },
    {
        "id": "2024.findings-naacl.78",
        "title": "A Tree-of-Thoughts to Broaden Multi-step Reasoning across Languages",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Reasoning methods, best exemplified by the well-known Chain-of-Thought (CoT), empower the reasoning abilities of Large Language Models (LLMs) by eliciting them to solve complex tasks in a step-by-step manner. Although they are achieving significant success, the ability to deliver multi-step reasoning remains limited to English because of the imbalance in the distribution of pre-training data, which makes other languages a barrier. In this paper, we propose Cross-lingual Tree-of-Thoughts (Cross-ToT), a method for aligning Cross-lingual CoT reasoning across languages. The proposed method, through a self-consistent cross-lingual prompting mechanism inspired by the Tree-of-Thoughts approach, provides multi-step reasoning paths in different languages that, during the steps, lead to the final solution. Experimental evaluations show that our method significantly outperforms existing prompting methods by reducing the number of interactions and achieving state-of-the-art performance.",
        "author": "Leonardo Ranaldi; Giulia Pucci; Federico Ranaldi; Elena Sofia Ruzzetti; Fabio Massimo Zanzotto",
        "authorids": "/l/leonardo-ranaldi/; /g/giulia-pucci/; /f/federico-ranaldi/; /e/elena-sofia-ruzzetti/; /f/fabio-massimo-zanzotto/",
        "bibtex": "@inproceedings{ranaldi-etal-2024-tree,\n    title = \"A Tree-of-Thoughts to Broaden Multi-step Reasoning across Languages\",\n    author = \"Ranaldi, Leonardo  and\n      Pucci, Giulia  and\n      Ranaldi, Federico  and\n      Ruzzetti, Elena Sofia  and\n      Zanzotto, Fabio Massimo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.78/\",\n    doi = \"10.18653/v1/2024.findings-naacl.78\",\n    pages = \"1229--1241\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.78.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.78/",
        "pdf_size": 940058,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15733700623480152733&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Human-Centric ART Group, Dep. of Enterprise Engineering, University of Rome Tor Vergata, Italy; Department of Computing Science, University of Aberdeen, UK; Human-Centric ART Group, Dep. of Enterprise Engineering, University of Rome Tor Vergata, Italy; Human-Centric ART Group, Dep. of Enterprise Engineering, University of Rome Tor Vergata, Italy; Human-Centric ART Group, Dep. of Enterprise Engineering, University of Rome Tor Vergata, Italy",
        "aff_domain": "uniroma2.it;uniroma2.it;uniroma2.it;uniroma2.it;uniroma2.it",
        "email": "uniroma2.it;uniroma2.it;uniroma2.it;uniroma2.it;uniroma2.it",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "University of Rome Tor Vergata;University of Aberdeen",
        "aff_unique_dep": "Department of Enterprise Engineering;Department of Computing Science",
        "aff_unique_url": "https://www.uniroma2.it;https://www.abdn.ac.uk",
        "aff_unique_abbr": ";Aberdeen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "Italy;United Kingdom"
    },
    {
        "id": "2024.naacl-long.76",
        "title": "A Universal Dependencies Treebank for Highland Puebla Nahuatl",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a Universal Dependencies (UD) treebank for Highland Puebla Nahuatl. The treebank is only the second such UD corpus for a Mexican language, and supplements an existing treebank for another Nahuatl variant. We describe the process of data collection, annotation decisions and interesting syntactic constructions, and discuss some similarities and differences between the Highland Puebla Nahuatl treebank and the existing Western Sierra Puebla Nahuatl treebank.",
        "author": "Robert Pugh; Francis Tyers",
        "authorids": "/r/robert-pugh/; /f/francis-tyers/",
        "bibtex": "@inproceedings{pugh-tyers-2024-universal,\n    title = \"A {U}niversal {D}ependencies Treebank for {H}ighland {P}uebla {N}ahuatl\",\n    author = \"Pugh, Robert  and\n      Tyers, Francis\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.76/\",\n    doi = \"10.18653/v1/2024.naacl-long.76\",\n    pages = \"1393--1403\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.76.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.76/",
        "pdf_size": 771685,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11338006953750860795&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Indiana University, Department of Linguistics; Indiana University, Department of Linguistics",
        "aff_domain": "iu.edu;iu.edu",
        "email": "iu.edu;iu.edu",
        "github": "https://github.com/UniversalDependencies/UDHighlandPueblaNahuatl-ITM",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indiana University",
        "aff_unique_dep": "Department of Linguistics",
        "aff_unique_url": "https://www.indiana.edu",
        "aff_unique_abbr": "IU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.118",
        "title": "A Wolf in Sheep\u2019s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as \u2018jailbreaks\u2019 can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, which compromises either generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze the failure of LLMs defense from the perspective of prompt execution priority, and propose corresponding defense strategies. We hope that our research can catalyze both the academic community and LLMs developers towards the provision of safer and more regulated LLMs. The code is available at https://github.com/NJUNLP/ReNeLLM.",
        "author": "Peng Ding; Jun Kuang; Dan Ma; Xuezhi Cao; Yunsen Xian; Jiajun Chen; Shujian Huang",
        "authorids": "/p/peng-ding/; /j/jun-kuang/; /d/dan-ma/; /x/xuezhi-cao/; /y/yunsen-xian/; /j/jiajun-chen/; /s/shujian-huang/",
        "bibtex": "@inproceedings{ding-etal-2024-wolf,\n    title = \"A Wolf in Sheep{'}s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily\",\n    author = \"Ding, Peng  and\n      Kuang, Jun  and\n      Ma, Dan  and\n      Cao, Xuezhi  and\n      Xian, Yunsen  and\n      Chen, Jiajun  and\n      Huang, Shujian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.118/\",\n    doi = \"10.18653/v1/2024.naacl-long.118\",\n    pages = \"2136--2153\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.118.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.118/",
        "pdf_size": 1280229,
        "gs_citation": 118,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6262871160089638908&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University; Meituan Inc., China; Meituan Inc., China; Meituan Inc., China; Meituan Inc., China; National Key Laboratory for Novel Software Technology, Nanjing University; National Key Laboratory for Novel Software Technology, Nanjing University",
        "aff_domain": "smail.nju.edu.cn;meituan.com;meituan.com;meituan.com;meituan.com;nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;meituan.com;meituan.com;meituan.com;meituan.com;nju.edu.cn;nju.edu.cn",
        "github": "https://github.com/NJUNLP/ReNeLLM",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;0;0",
        "aff_unique_norm": "Nanjing University;Meituan Inc.",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.meituan.com",
        "aff_unique_abbr": "Nanjing University;Meituan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.206",
        "title": "A Zero-Shot Monolingual Dual Stage Information Retrieval System for Spanish Biomedical Systematic Literature Reviews",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Systematic Reviews (SRs) are foundational in healthcare for synthesising evidence to inform clinical practices. Traditionally skewed towards English-language databases, SRs often exclude significant research in other languages, leading to potential biases. This study addresses this gap by focusing on Spanish, a language notably underrepresented in SRs. We present a foundational zero-shot dual information retrieval (IR) baseline system, integrating traditional retrieval methods with pre-trained language models and cross-attention re-rankers for enhanced accuracy in Spanish biomedical literature retrieval. Utilising the LILACS database, known for its comprehensive coverage of Latin American and Caribbean biomedical literature, we evaluate the approach with three real-life case studies in Spanish SRs. The findings demonstrate the system\u2019s efficacy and underscore the importance of query formulation. This study contributes to the field of IR by promoting language inclusivity and supports the development of more comprehensive and globally representative healthcare guidelines.",
        "author": "Regina Ofori-Boateng; Magaly Aceves-Martins; Nirmalie Wiratunga; Carlos Moreno-Garcia",
        "authorids": "/r/regina-ofori-boateng/; /m/magaly-aceves-martins/; /n/nirmalie-wiratunga/; /c/carlos-moreno-garcia/",
        "bibtex": "@inproceedings{ofori-boateng-etal-2024-zero,\n    title = \"A Zero-Shot Monolingual Dual Stage Information Retrieval System for {S}panish Biomedical Systematic Literature Reviews\",\n    author = \"Ofori-Boateng, Regina  and\n      Aceves-Martins, Magaly  and\n      Wiratunga, Nirmalie  and\n      Moreno-Garcia, Carlos\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.206/\",\n    doi = \"10.18653/v1/2024.naacl-long.206\",\n    pages = \"3725--3736\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.206.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.206/",
        "pdf_size": 317492,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:gckXCOrQtFUJ:scholar.google.com/&scioq=A+Zero-Shot+Monolingual+Dual+Stage+Information+Retrieval+System+for+Spanish+Biomedical+Systematic+Literature+Reviews&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-short.55",
        "title": "A diverse Multilingual News Headlines Dataset from around the World",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Babel Briefings is a novel dataset featuring 4.7 million news headlines from August 2020 to November 2021, across 30 languages and 54 locations worldwide with English translations of all articles included. Designed for natural language processing and media studies, it serves as a high-quality dataset for training or evaluating language models as well as offering a simple, accessible collection of articles, for example, to analyze global news coverage and cultural narratives. As a simple demonstration of the analyses facilitated by this dataset, we use a basic procedure using a TF-IDF weighted similarity metric to group articles into clusters about the same event. We then visualize the event signatures of the event showing articles of which languages appear over time, revealing intuitive features based on the proximity of the event and unexpectedness of the event. The dataset is available on [Kaggle](https://www.kaggle.com/datasets/felixludos/babel-briefings) and [HuggingFace](https://huggingface.co/datasets/felixludos/babel-briefings) with accompanying [GitHub](https://github.com/felixludos/babel-briefings) code.",
        "author": "Felix Leeb; Bernhard Sch\u00f6lkopf",
        "authorids": "/f/felix-leeb/; /b/bernhard-scholkopf/",
        "bibtex": "@inproceedings{leeb-scholkopf-2024-diverse,\n    title = \"A diverse Multilingual News Headlines Dataset from around the World\",\n    author = {Leeb, Felix  and\n      Sch{\\\"o}lkopf, Bernhard},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.55/\",\n    doi = \"10.18653/v1/2024.naacl-short.55\",\n    pages = \"647--652\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.55.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.55/",
        "pdf_size": 286771,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2303460574619515992&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Max Planck Institute for Intelligent Systems; Max Planck Institute for Intelligent Systems",
        "aff_domain": "tue.mpg.de; ",
        "email": "tue.mpg.de; ",
        "github": "",
        "project": "https://kaggle.com; https://huggingface.co",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Max Planck Institute for Intelligent Systems",
        "aff_unique_dep": "Intelligent Systems",
        "aff_unique_url": "https://www.mpi-is.mpg.de",
        "aff_unique_abbr": "MPI-IS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.371",
        "title": "ACLSum: A New Dataset for Aspect-based Summarization of Scientific Publications",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Extensive efforts in the past have been directed toward the development of summarization datasets. However, a predominant number of these resources have been (semi)-automatically generated, typically through web data crawling. This resulted in subpar resources for training and evaluating summarization systems, a quality compromise that is arguably due to the substantial costs associated with generating ground-truth summaries, particularly for diverse languages and specialized domains. To address this issue, we present ACLSum, a novel summarization dataset carefully crafted and evaluated by domain experts. In contrast to previous datasets, ACLSum facilitates multi-aspect summarization of scientific papers, covering challenges, approaches, and outcomes in depth. Through extensive experiments, we evaluate the quality of our resource and the performance of models based on pretrained language models (PLMs) and state-of-the-art large language models (LLMs). Additionally, we explore the effectiveness of extract-then-abstract versus abstractive end-to-end summarization within the scholarly domain on the basis of automatically discovered aspects. While the former performs comparably well to the end-to-end approach with pretrained language models regardless of the potential error propagation issue, the prompting-based approach with LLMs shows a limitation in extracting sentences from source documents.",
        "author": "Sotaro Takeshita; Tommaso Green; Ines Reinig; Kai Eckert; Simone Ponzetto",
        "authorids": "/s/sotaro-takeshita/; /t/tommaso-green/; /i/ines-reinig/; /k/kai-eckert/; /s/simone-paolo-ponzetto/",
        "bibtex": "@inproceedings{takeshita-etal-2024-aclsum,\n    title = \"{ACLS}um: A New Dataset for Aspect-based Summarization of Scientific Publications\",\n    author = \"Takeshita, Sotaro  and\n      Green, Tommaso  and\n      Reinig, Ines  and\n      Eckert, Kai  and\n      Ponzetto, Simone\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.371/\",\n    doi = \"10.18653/v1/2024.naacl-long.371\",\n    pages = \"6660--6675\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.371.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.371/",
        "pdf_size": 608061,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16405314228736987160&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Data and Web Science Group, University of Mannheim, Germany; Data and Web Science Group, University of Mannheim, Germany; Data and Web Science Group, University of Mannheim, Germany; Mannheim University of Applied Sciences, Mannheim, Germany; Data and Web Science Group, University of Mannheim, Germany",
        "aff_domain": "uni-mannheim.de;uni-mannheim.de;uni-mannheim.de;hs-mannheim.de;uni-mannheim.de",
        "email": "uni-mannheim.de;uni-mannheim.de;uni-mannheim.de;hs-mannheim.de;uni-mannheim.de",
        "github": "https://github.com/sobamchan/aclsum",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Mannheim;Mannheim University of Applied Sciences",
        "aff_unique_dep": "Data and Web Science Group;",
        "aff_unique_url": "https://www.uni-mannheim.de;https://www.mannheim.hs.de",
        "aff_unique_abbr": ";MUAS",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mannheim",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.findings-naacl.264",
        "title": "ADaPT: As-Needed Decomposition and Planning with Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3% higher in ALFWorld, 27% in WebShop, and 33% in TextCraft \u2013 a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.",
        "author": "Archiki Prasad; Alexander Koller; Mareike Hartmann; Peter Clark; Ashish Sabharwal; Mohit Bansal; Tushar Khot",
        "authorids": "/a/archiki-prasad/; /a/alexander-koller/; /m/mareike-hartmann/; /p/peter-clark/; /a/ashish-sabharwal/; /m/mohit-bansal/; /t/tushar-khot/",
        "bibtex": "@inproceedings{prasad-etal-2024-adapt,\n    title = \"{AD}a{PT}: As-Needed Decomposition and Planning with Language Models\",\n    author = \"Prasad, Archiki  and\n      Koller, Alexander  and\n      Hartmann, Mareike  and\n      Clark, Peter  and\n      Sabharwal, Ashish  and\n      Bansal, Mohit  and\n      Khot, Tushar\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.264/\",\n    doi = \"10.18653/v1/2024.findings-naacl.264\",\n    pages = \"4226--4252\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.264.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.264/",
        "pdf_size": 1159390,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17012189586575756608&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2024.findings-naacl.149",
        "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Assessing foundation models\u2019 abilities for human-level tasks is crucial for Artificial General Intelligence (AGI) development.Traditional benchmarks, which rely on artificial datasets, may not accurately represent these capabilities. In this paper, we introduce AGIEval, a novel bilingual benchmark designed to assess foundation models in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models on our benchmark. Impressively, we show that GPT-4 exceeds the average human performance in SAT, LSAT, and math contests, with 95% accuracy on SAT Math and 92.5% on the Chinese college entrance English exam. This demonstrates the exceptional performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks requiring complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal their strengths and limitations, providing valuable insights into future directions for enhancing general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a meaningful and robust evaluation of foundation models\u2019 performance in real-world scenarios.",
        "author": "Wanjun Zhong; Ruixiang Cui; Yiduo Guo; Yaobo Liang; Shuai Lu; Yanlin Wang; Amin Saied; Weizhu Chen; Nan Duan",
        "authorids": "/w/wanjun-zhong/; /r/ruixiang-cui/; /y/yiduo-guo/; /y/yaobo-liang/; /s/shuai-lu/; /y/yanlin-wang/; /a/amin-saied/; /w/weizhu-chen/; /n/nan-duan/",
        "bibtex": "@inproceedings{zhong-etal-2024-agieval,\n    title = \"{AGIE}val: A Human-Centric Benchmark for Evaluating Foundation Models\",\n    author = \"Zhong, Wanjun  and\n      Cui, Ruixiang  and\n      Guo, Yiduo  and\n      Liang, Yaobo  and\n      Lu, Shuai  and\n      Wang, Yanlin  and\n      Saied, Amin  and\n      Chen, Weizhu  and\n      Duan, Nan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.149/\",\n    doi = \"10.18653/v1/2024.findings-naacl.149\",\n    pages = \"2299--2314\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.149.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.149/",
        "pdf_size": 409880,
        "gs_citation": 447,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4809822709662168300&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft; Microsoft; Microsoft; Microsoft; Microsoft; Microsoft; Microsoft; Microsoft; Microsoft",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "https://github.com/ruixiangcui/AGIEval",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Corporation",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.136",
        "title": "ALBA: Adaptive Language-Based Assessments for Mental Health",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Mental health issues differ widely among individuals, with varied signs and symptoms. Recently, language-based assessments haveshown promise in capturing this diversity, but they require a substantial sample of words per person for accuracy. This work introducesthe task of Adaptive Language-Based Assessment (ALBA), which involves adaptively ordering questions while also scoring an individual\u2019s latent psychological trait using limited language responses to previous questions. To this end, we develop adaptive testing methods under two psychometric measurement theories: Classical Test Theory and Item Response Theory.We empirically evaluate ordering and scoring strategies, organizing into two new methods: a semi-supervised item response theory-basedmethod (ALIRT) and a supervised Actor-Critic model. While we found both methods to improve over non-adaptive baselines, We foundALIRT to be the most accurate and scalable, achieving the highest accuracy with fewer questions (e.g., Pearson r \u2248 0.93 after only 3 questions as compared to typically needing at least 7 questions). In general, adaptive language-based assessments of depression and anxiety were able to utilize a smaller sample of language without compromising validity or large computational costs.",
        "author": "Vasudha Varadarajan; Sverker Sikstr\u00f6m; Oscar Kjell; H. Andrew Schwartz",
        "authorids": "/v/vasudha-varadarajan/; /s/sverker-sikstrom/; /o/oscar-kjell/; /h/h-andrew-schwartz/",
        "bibtex": "@inproceedings{varadarajan-etal-2024-alba,\n    title = \"{ALBA}: Adaptive Language-Based Assessments for Mental Health\",\n    author = {Varadarajan, Vasudha  and\n      Sikstr{\\\"o}m, Sverker  and\n      Kjell, Oscar  and\n      Schwartz, H. Andrew},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.136/\",\n    doi = \"10.18653/v1/2024.naacl-long.136\",\n    pages = \"2466--2478\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.136.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.136/",
        "pdf_size": 785416,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3550233304779695317&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Stony Brook University; Department of Psychology, Lund University; Department of Psychology, Lund University; Department of Computer Science, Stony Brook University",
        "aff_domain": "cs.stonybrook.edu;psychology.lu.se;psychology.lu.se;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;psychology.lu.se;psychology.lu.se;cs.stonybrook.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Stony Brook University;Lund University",
        "aff_unique_dep": "Department of Computer Science;Department of Psychology",
        "aff_unique_url": "https://www.stonybrook.edu;https://www.lunduniversity.lu.se",
        "aff_unique_abbr": "SBU;LU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stony Brook;",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;Sweden"
    },
    {
        "id": "2024.naacl-short.30",
        "title": "ALOHa: A New Measure for Hallucination in Captioning Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene. The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations. Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories.",
        "author": "Suzanne Petryk; David Chan; Anish Kachinthaya; Haodi Zou; John Canny; Joseph Gonzalez; Trevor Darrell",
        "authorids": "/s/suzanne-petryk/; /d/david-chan/; /a/anish-kachinthaya/; /h/haodi-zou/; /j/john-canny/; /j/joseph-gonzalez/; /t/trevor-darrell/",
        "bibtex": "@inproceedings{petryk-etal-2024-aloha,\n    title = \"{ALOH}a: A New Measure for Hallucination in Captioning Models\",\n    author = \"Petryk, Suzanne  and\n      Chan, David  and\n      Kachinthaya, Anish  and\n      Zou, Haodi  and\n      Canny, John  and\n      Gonzalez, Joseph  and\n      Darrell, Trevor\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.30/\",\n    doi = \"10.18653/v1/2024.naacl-short.30\",\n    pages = \"342--357\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.30.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.30/",
        "pdf_size": 32767004,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11776672778144274626&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "https://davidmchan.github.io/aloha",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.35",
        "title": "ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Parameter-efficient fine-tuning (PEFT) is widely studied for its effectiveness and efficiency in the era of large language models. Low-rank adaptation (LoRA) has demonstrated commendable performance as a popular and representative method. However, it is implemented with a fixed intrinsic rank that might not be the ideal setting for the downstream tasks. Recognizing the need for more flexible downstream task adaptation, we extend the methodology of LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. First, we propose a novel method, AB-LoRA, that can effectively estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we gradually prune abundant and negatively impacting LoRA ranks and allocate the pruned LoRA budgets to important Transformer modules needing higher ranks. We have conducted experiments on various tasks, and the experimental results demonstrate that our ALoRA method can outperform the recent baselines with comparable tunable parameters.",
        "author": "Zequan Liu; Jiawen Lyn; Wei Zhu; Xing Tian; Yvette Graham",
        "authorids": "/z/zequan-liu/; /j/jiawen-lyn/; /w/wei-zhu/; /x/xing-tian/; /y/yvette-graham/",
        "bibtex": "@inproceedings{liu-etal-2024-alora,\n    title = \"{AL}o{RA}: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models\",\n    author = \"Liu, Zequan  and\n      Lyn, Jiawen  and\n      Zhu, Wei  and\n      Tian, Xing  and\n      Graham, Yvette\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.35/\",\n    doi = \"10.18653/v1/2024.naacl-long.35\",\n    pages = \"622--641\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.35.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.35/",
        "pdf_size": 532039,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15279904704602600157&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "RWTH Aachen University, Aachen, Germany; Trinity College Dublin, Dublin, Ireland; East China Normal University, Shanghai, China; Niuxin Network Technology Co., Ltd.; Trinity College Dublin, Dublin, Ireland",
        "aff_domain": "rwth-aachen.de;tcd.ie;stu.ecnu.edu.cn;niuxin-tech.com;tcd.ie",
        "email": "rwth-aachen.de;tcd.ie;stu.ecnu.edu.cn;niuxin-tech.com;tcd.ie",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;1",
        "aff_unique_norm": "RWTH Aachen University;Trinity College Dublin;East China Normal University;Niuxin Network Technology",
        "aff_unique_dep": ";;;Technology",
        "aff_unique_url": "https://www.rwth-aachen.de;https://www.tcd.ie;http://www.ecnu.edu.cn;",
        "aff_unique_abbr": "RWTH;TCD;ECNU;",
        "aff_campus_unique_index": "0;1;2;1",
        "aff_campus_unique": "Aachen;Dublin;Shanghai;",
        "aff_country_unique_index": "0;1;2;2;1",
        "aff_country_unique": "Germany;Ireland;China"
    },
    {
        "id": "2024.naacl-industry.32",
        "title": "AMA-LSTM: Pioneering Robust and Fair Financial Audio Analysis for Stock Volatility Prediction",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Stock volatility prediction is an important task in the financial industry. Recent multimodal methods have shown advanced results by combining text and audio information, such as earnings calls. However, these multimodal methods have faced two drawbacks. First, they often fail to yield reliable models and overfit the data due to their absorption of stochastic information from the stock market. Moreover, using multimodal models to predict stock volatility suffers from gender bias and lacks an efficient way to eliminate such bias. To address these aforementioned problems, we use adversarial training to generate perturbations that simulate the inherent stochasticity and bias, by creating areas resistant to random information around the input space to improve model robustness and fairness. Our comprehensive experiments on two real-world financial audio datasets reveal that this method exceeds the performance of current state-of-the-art solution. This confirms the value of adversarial training in reducing stochasticity and bias for stock volatility prediction tasks.",
        "author": "Shengkun Wang; Taoran Ji; Jianfeng He; Mariam ALMutairi; Dan Wang; Linhan Wang; Min Zhang; Chang-Tien Lu",
        "authorids": "/s/shengkun-wang/; /t/taoran-ji/; /j/jianfeng-he/; /m/mariam-almutairi/; /d/dan-wang/; /l/linhan-wang/; /m/min-zhang/; /c/chang-tien-lu/",
        "bibtex": "@inproceedings{wang-etal-2024-ama,\n    title = \"{AMA}-{LSTM}: Pioneering Robust and Fair Financial Audio Analysis for Stock Volatility Prediction\",\n    author = \"Wang, Shengkun  and\n      Ji, Taoran  and\n      He, Jianfeng  and\n      ALMutairi, Mariam  and\n      Wang, Dan  and\n      Wang, Linhan  and\n      Zhang, Min  and\n      Lu, Chang-Tien\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.32/\",\n    doi = \"10.18653/v1/2024.naacl-industry.32\",\n    pages = \"379--386\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.32.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.32/",
        "pdf_size": 716325,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1733078269465247626&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Virginia Tech; Department of Computer Science, Texas A&M University - Corpus Christi; Department of Computer Science, Virginia Tech; Department of Computer Science, Virginia Tech; School of Business, Stevens Institute of Technology; Department of Computer Science, Virginia Tech; Department of Computer Science, Virginia Tech; Department of Computer Science, Virginia Tech",
        "aff_domain": "vt.edu;tamucc.edu;vt.edu;vt.edu;stevens.edu;vt.edu;vt.edu;vt.edu",
        "email": "vt.edu;tamucc.edu;vt.edu;vt.edu;stevens.edu;vt.edu;vt.edu;vt.edu",
        "github": "https://github.com/hao1zhao/AMA-LSTM",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;0;0;2;0;0;0",
        "aff_unique_norm": "Virginia Tech;Texas A&M University - Corpus Christi;Stevens Institute of Technology",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science;School of Business",
        "aff_unique_url": "https://www.vt.edu;https://www.tamucc.edu;https://www.stevens.edu",
        "aff_unique_abbr": "VT;TAMUCC;SIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Corpus Christi",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.33",
        "title": "AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Ensuring factual consistency is crucial for natural language generation tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior works on evaluating factual consistency of summarization often take the entailment-based approaches that first generate perturbed (factual inconsistent) summaries and then train a classifier on the generated data to detect the factually inconsistencies during testing time. However, previous approaches generating perturbed summaries are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a framework that generates perturbed summaries using Abstract Meaning Representations (AMRs). Our approach parses factually consistent summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage. Additionally, we present a data selection module NegFilter based on natural language inference and BARTScore to ensure the quality of the generated negative samples. Experimental results demonstrate our approach significantly outperforms previous systems on the AggreFact-SOTA benchmark, showcasing its efficacy in evaluating factuality of abstractive summarization.",
        "author": "Haoyi Qiu; Kung-Hsiang Huang; Jingnong Qu; Nanyun Peng",
        "authorids": "/h/haoyi-qiu/; /k/kung-hsiang-huang/; /j/jingnong-qu/; /n/nanyun-peng/",
        "bibtex": "@inproceedings{qiu-etal-2024-amrfact,\n    title = \"{AMRF}act: Enhancing Summarization Factuality Evaluation with {AMR}-Driven Negative Samples Generation\",\n    author = \"Qiu, Haoyi  and\n      Huang, Kung-Hsiang  and\n      Qu, Jingnong  and\n      Peng, Nanyun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.33/\",\n    doi = \"10.18653/v1/2024.naacl-long.33\",\n    pages = \"594--608\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.33.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.33/",
        "pdf_size": 726952,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2951720711565720701&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, Los Angeles; University of Illinois Urbana-Champaign; University of California, Los Angeles; University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;illinois.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;illinois.edu;cs.ucla.edu;cs.ucla.edu",
        "github": "https://github.com/PlusLabNLP/AMRFact",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of California, Los Angeles;University of Illinois Urbana-Champaign",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucla.edu;https://illinois.edu",
        "aff_unique_abbr": "UCLA;UIUC",
        "aff_campus_unique_index": "0;1;0;0",
        "aff_campus_unique": "Los Angeles;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.20",
        "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.",
        "author": "Jon Saad-Falcon; Omar Khattab; Christopher Potts; Matei Zaharia",
        "authorids": "/j/jon-saad-falcon/; /o/omar-khattab/; /c/christopher-potts/; /m/matei-zaharia/",
        "bibtex": "@inproceedings{saad-falcon-etal-2024-ares,\n    title = \"{ARES}: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\",\n    author = \"Saad-Falcon, Jon  and\n      Khattab, Omar  and\n      Potts, Christopher  and\n      Zaharia, Matei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.20/\",\n    doi = \"10.18653/v1/2024.naacl-long.20\",\n    pages = \"338--354\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.20.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.20/",
        "pdf_size": 717417,
        "gs_citation": 130,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8307738739142450350&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Stanford University\u2217; Stanford University; Stanford University; Databricks and UC Berkeley",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;databricks.com",
        "email": "stanford.edu;stanford.edu;stanford.edu;databricks.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Stanford University;UC Berkeley",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "Stanford;UCB",
        "aff_campus_unique_index": "0;0;0;1",
        "aff_campus_unique": "Stanford;Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.455",
        "title": "ARM: Alignment with Residual Energy-Based Model",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While large language models (LLMs) trained with large-scale unsupervised learning acquire a wide variety of world knowledge and skills, its behavior does not necessarily align with human preferences. RLHF methods achieve successes in aligning LLM responses with human preferences and improving the controllability of LLM behavior with human instruction. However, RLHF methods are considerably complicated to implement, computationally expensive to train, and notoriously tricky to tune. In this work, we propose Alignment with Residual Energy-Based Model (ARM), as a simple and flexible alternative to RLHF methods. Our method is driven by an observation that we can learn an aligned policy by minimizing a forward Kullback\u2013Leibler (KL) divergence from a target policy (in the form of a residual energy-based model) to a parameteric policy (LLM), instead of a reverse KL as in RLHF methods. With samples from the energy-based target policy, we can leverage the power of DPO (or other offline methods) to learn an aligned policy efficiently. ARM is simple to implement and applicable in various data settings. Our extensive experiments demonstrate its strong performance across multiple datasets, compared to strong baselines like PPO, DPO.",
        "author": "Bo Pang; Caiming Xiong; Yingbo Zhou",
        "authorids": "/b/bo-pang/; /c/caiming-xiong/; /y/yingbo-zhou/",
        "bibtex": "@inproceedings{pang-etal-2024-arm,\n    title = \"{ARM}: Alignment with Residual Energy-Based Model\",\n    author = \"Pang, Bo  and\n      Xiong, Caiming  and\n      Zhou, Yingbo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.455/\",\n    doi = \"10.18653/v1/2024.naacl-long.455\",\n    pages = \"8225--8236\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.455.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.455/",
        "pdf_size": 468792,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=140400080494807647&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Salesforce AI Research; Salesforce AI Research; Salesforce AI Research",
        "aff_domain": "salesfoce.com; ; ",
        "email": "salesfoce.com; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Salesforce",
        "aff_unique_dep": "Salesforce AI Research",
        "aff_unique_url": "https://www.salesforce.com",
        "aff_unique_abbr": "Salesforce AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.279",
        "title": "ATG: Benchmarking Automated Theorem Generation for Generative Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Humans can develop new theorems to explore broader and more complex mathematical results.While current generative language models (LMs) have achieved significant improvement in automatically proving theorems, their ability to generate new or reusable theorems is still under-explored. Without the new theorems, current LMs struggle to prove harder theorems that are distant from the given hypotheses with the exponentially growing search space.More advanced theorem proving is if an agent (for instance, a generative LM) can leverage its creativity to generate new but also reasonable theorems that properly substitute part of a proof and also be saved as reusable knowledge for future theorem proving.Therefore, this paper proposes an Automated Theorem Generation (ATG) benchmark that evaluates whether an agent can automatically generate valuable (and possibly brand new) theorems that are applicable for downstream theorem proving as reusable knowledge. Specifically, we construct the ATG benchmark by splitting the Metamath library into three sets: axioms, library, and problem based on their proving depth.We conduct extensive experiments to investigate whether current LMs can generate theorems in the library and benefit the problem theorems proving. The results demonstrate that high-quality ATG data facilitates models\u2019 performances on downstream ATP. However, there is still room for current LMs to develop better ATG and generate more advanced and human-like theorems. We hope the new ATG challenge can shed some light on advanced complex theorem proving.",
        "author": "Xiaohan Lin; Qingxing Cao; Yinya Huang; Zhicheng Yang; Zhengying Liu; Zhenguo Li; Xiaodan Liang",
        "authorids": "/x/xiaohan-lin/; /q/qingxing-cao/; /y/yinya-huang/; /z/zhicheng-yang/; /z/zhengying-liu/; /z/zhenguo-li/; /x/xiaodan-liang/",
        "bibtex": "@inproceedings{lin-etal-2024-atg,\n    title = \"{ATG}: Benchmarking Automated Theorem Generation for Generative Language Models\",\n    author = \"Lin, Xiaohan  and\n      Cao, Qingxing  and\n      Huang, Yinya  and\n      Yang, Zhicheng  and\n      Liu, Zhengying  and\n      Li, Zhenguo  and\n      Liang, Xiaodan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.279/\",\n    doi = \"10.18653/v1/2024.findings-naacl.279\",\n    pages = \"4465--4480\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.279.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.279/",
        "pdf_size": 1900741,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5420413550846964196&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Shenzhen Campus of Sun Yat-Sen University; Shenzhen Campus of Sun Yat-Sen University; City University of Hong Kong; The Hong Kong University of Science and Technology (Guangzhou); Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; DarkMatter AI Research",
        "aff_domain": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;hotmail.com;gmail.com;huawei.com;huawei.com;gmail.com",
        "email": "mail2.sysu.edu.cn;mail2.sysu.edu.cn;hotmail.com;gmail.com;huawei.com;huawei.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;2;3;3;4",
        "aff_unique_norm": "Sun Yat-sen University;City University of Hong Kong;Hong Kong University of Science and Technology;Huawei;DarkMatter AI Research",
        "aff_unique_dep": ";;;Noah\u2019s Ark Lab;AI Research",
        "aff_unique_url": "http://www.sysu.edu.cn/;https://www.cityu.edu.hk;https://www.ust.hk;https://www.huawei.com;",
        "aff_unique_abbr": "SYSU;CityU;HKUST;Huawei;",
        "aff_campus_unique_index": "0;0;1;2",
        "aff_campus_unique": "Shenzhen;Hong Kong SAR;Guangzhou;",
        "aff_country_unique_index": "0;0;0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-demo.9",
        "title": "ATLAS: A System for PDF-centric Human Interaction Data Collection",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "The Portable Document Format (PDF) is a popular format for distributing digital documents. Datasets on PDF reading behaviors and interactions remain limited due to the challenges of instrumenting PDF readers for these data collection tasks. We present ATLAS, a data collection tool designed to better support researchers in collecting rich PDF-centric datasets from users. ATLAS supports researchers in programmatically creating a user interface for data collection that is ready to share with annotators. It includes a toolkit and an extensible schema to easily customize the data collection tasks for a variety of purposes, allowing collection of PDF annotations (e.g., highlights, drawings) as well as reading behavior analytics (e.g., page scroll, text selections). We open-source ATLAS1 to support future research efforts and review use cases of ATLAS that showcase our system\u2019s broad applicability.",
        "author": "Alexa Siu; Zichao Wang; Joshua Hoeflich; Naman Kapasi; Ani Nenkova; Tong Sun",
        "authorids": "/a/alexa-siu/; /z/zichao-wang/; /j/joshua-hoeflich/; /n/naman-kapasi/; /a/ani-nenkova/; /t/tong-sun/",
        "bibtex": "@inproceedings{siu-etal-2024-atlas,\n    title = \"{ATLAS}: A System for {PDF}-centric Human Interaction Data Collection\",\n    author = \"Siu, Alexa  and\n      Wang, Zichao  and\n      Hoeflich, Joshua  and\n      Kapasi, Naman  and\n      Nenkova, Ani  and\n      Sun, Tong\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.9/\",\n    doi = \"10.18653/v1/2024.naacl-demo.9\",\n    pages = \"87--96\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.9.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.9/",
        "pdf_size": 532335,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13181644226183558540&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Adobe Research; Adobe Research; Northwestern University; University of California, Berkeley; Adobe Research; Adobe Research",
        "aff_domain": "adobe.com;adobe.com; ;berkeley.edu;adobe.com;adobe.com",
        "email": "adobe.com;adobe.com; ;berkeley.edu;adobe.com;adobe.com",
        "github": "https://github.com/frictionlessweb/documentstudies",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;0;0",
        "aff_unique_norm": "Adobe;Northwestern University;University of California, Berkeley",
        "aff_unique_dep": "Adobe Research;;",
        "aff_unique_url": "https://research.adobe.com;https://www.northwestern.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "Adobe;NU;UC Berkeley",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.330",
        "title": "AWESOME: GPU Memory-constrained Long Document Summarization using Memory Mechanism and Global Salient Content",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Long document summarization systems are critical for domains with lengthy and jargon-laden text, yet they present significant challenges to researchers and developers with limited computing resources. Existing solutions mainly focus on efficient attentions or divide-and-conquer strategies. The former reduces theoretical time complexity, but is still memory-heavy. The latter methods sacrifice global context, leading to uninformative and incoherent summaries. This work aims to leverage the memory-efficient nature of divide-and-conquer methods while preserving global context. Concretely, our framework AWESOME uses two novel mechanisms: (1) External memory mechanisms track previously encoded document segments and their corresponding summaries, to enhance global document understanding and summary coherence. (2) Global salient content is further identified beforehand to augment each document segment to support its summarization. Extensive experiments on diverse genres of text, including government reports, meeting transcripts, screenplays, scientific papers, and novels, show that AWESOME produces summaries with improved informativeness, faithfulness, and coherence than competitive baselines on longer documents, while having a smaller GPU memory footprint.",
        "author": "Shuyang Cao; Lu Wang",
        "authorids": "/s/shuyang-cao/; /l/lu-wang/",
        "bibtex": "@inproceedings{cao-wang-2024-awesome,\n    title = \"{AWESOME}: {GPU} Memory-constrained Long Document Summarization using Memory Mechanism and Global Salient Content\",\n    author = \"Cao, Shuyang  and\n      Wang, Lu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.330/\",\n    doi = \"10.18653/v1/2024.naacl-long.330\",\n    pages = \"5925--5941\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.330.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.330/",
        "pdf_size": 476258,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10542288152239367811&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science and Engineering, University of Michigan; Computer Science and Engineering, University of Michigan",
        "aff_domain": "umich.edu;umich.edu",
        "email": "umich.edu;umich.edu",
        "github": "https://shuyangcao.github.io/projects/awesome/",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.252",
        "title": "AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.",
        "author": "Zhaowei Wang; Haochen Shi; Weiqi Wang; Tianqing Fang; Hongming Zhang; Sehyun Choi; Xin Liu; Yangqiu Song",
        "authorids": "/z/zhaowei-wang/; /h/haochen-shi/; /w/weiqi-wang/; /t/tianqing-fang/; /h/hongming-zhang/; /s/sehyun-choi/; /x/xin-liu/; /y/yangqiu-song/",
        "bibtex": "@inproceedings{wang-etal-2024-abspyramid,\n    title = \"{A}bs{P}yramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph\",\n    author = \"Wang, Zhaowei  and\n      Shi, Haochen  and\n      Wang, Weiqi  and\n      Fang, Tianqing  and\n      Zhang, Hongming  and\n      Choi, Sehyun  and\n      Liu, Xin  and\n      Song, Yangqiu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.252/\",\n    doi = \"10.18653/v1/2024.findings-naacl.252\",\n    pages = \"3991--4010\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.252.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.252/",
        "pdf_size": 1300272,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3383496888286647002&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science and Engineering, HKUST; Department of Computer Science and Engineering, HKUST; Department of Computer Science and Engineering, HKUST; Department of Computer Science and Engineering, HKUST; Tencent AI Lab, Bellevue, USA; Department of Computer Science and Engineering, HKUST; Department of Computer Science and Engineering, HKUST; Department of Computer Science and Engineering, HKUST",
        "aff_domain": "cse.ust.hk;connect.ust.hk;cse.ust.hk;cse.ust.hk;global.tencent.com;cse.ust.hk;cse.ust.hk;cse.ust.hk",
        "email": "cse.ust.hk;connect.ust.hk;cse.ust.hk;cse.ust.hk;global.tencent.com;cse.ust.hk;cse.ust.hk;cse.ust.hk",
        "github": "https://github.com/HKUST-KnowComp/AbsPyramid",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;1;0;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Tencent",
        "aff_unique_dep": "Department of Computer Science and Engineering;AI Lab",
        "aff_unique_url": "https://www.hkust.edu.hk;https://ai.tencent.com",
        "aff_unique_abbr": "HKUST;Tencent AI Lab",
        "aff_campus_unique_index": "0;0;0;0;1;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;Bellevue",
        "aff_country_unique_index": "0;0;0;0;1;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.72",
        "title": "Accurate Knowledge Distillation via n-best Reranking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose utilizing n-best reranking to enhance Sequence-Level Knowledge Distillation (Kim and Rush, 2016) where we extract pseudo-labels for student model\u2019s training data from top n-best hypotheses and leverage a diverse set of models with different inductive biases, objective functions or architectures, including some publicly-available large language models, to pick the highest-quality hypotheses as labels. The effectiveness of our proposal is validated through experiments on the WMT\u201921 German \u2194 English and Chinese \u2194 English translation tasks. Our results demonstrate that utilizing pseudo-labels generated by our n-best reranker leads to a significantly more accurate student model. In fact, our best student model achieves comparable accuracy to a large translation model from (Tran et al., 2021) with 4.7 billion parameters, while having two orders of magnitude fewer parameters.",
        "author": "Hendra Setiawan",
        "authorids": "/h/hendra-setiawan/",
        "bibtex": "@inproceedings{setiawan-2024-accurate,\n    title = \"Accurate Knowledge Distillation via n-best Reranking\",\n    author = \"Setiawan, Hendra\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.72/\",\n    doi = \"10.18653/v1/2024.naacl-long.72\",\n    pages = \"1330--1345\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.72.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.72/",
        "pdf_size": 292029,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:TUrNY7Y8SkUJ:scholar.google.com/&scioq=Accurate+Knowledge+Distillation+via+n-best+Reranking&hl=en&as_sdt=0,14",
        "gs_version_total": 0,
        "aff": "Apple",
        "aff_domain": "apple.com",
        "email": "apple.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Apple",
        "aff_unique_dep": "Apple Inc.",
        "aff_unique_url": "https://www.apple.com",
        "aff_unique_abbr": "Apple",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.450",
        "title": "AceGPT, Localizing Large Language Models in Arabic",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper is devoted to the development of a localized Large Language Model (LLM) specifically for Arabic, a language imbued with unique cultural characteristics inadequately addressed by current mainstream models. Significant concerns emerge when addressing cultural sensitivity and local values. To address this, the paper proposes a comprehensive solution that includes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT) utilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside Reinforcement Learning with AI Feedback (RLAIF) employing a reward model attuned to local culture and values. The goal is to cultivate culturally cognizant and value-aligned Arabic LLMs capable of accommodating the diverse, application-specific needs of Arabic-speaking communities. Comprehensive evaluations reveal that the resulting model, dubbed \u2018AceGPT\u2019, sets the state-of-the-art standard for open Arabic LLMs across various benchmarks. Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.",
        "author": "Huang Huang; Fei Yu; Jianqing Zhu; Xuening Sun; Hao Cheng; Song Dingjie; Zhihong Chen; Mosen Alharthi; Bang An; Juncai He; Ziche Liu; Junying Chen; Jianquan Li; Benyou Wang; Lian Zhang; Ruoyu Sun; Xiang Wan; Haizhou Li; Jinchao Xu",
        "authorids": "/h/huang-huang/; /f/fei-yu/; /j/jianqing-zhu/; /x/xuening-sun/; /h/hao-cheng/; /s/song-dingjie/; /z/zhihong-chen/; /m/mosen-alharthi/; /b/bang-an/; /j/juncai-he/; /z/ziche-liu/; /j/junying-chen/; /j/jianquan-li/; /b/benyou-wang/; /l/lian-zhang/; /r/ruoyu-sun/; /x/xiang-wan/; /h/haizhou-li/; /j/jinchao-xu/",
        "bibtex": "@inproceedings{huang-etal-2024-acegpt,\n    title = \"{A}ce{GPT}, Localizing Large Language Models in {A}rabic\",\n    author = \"Huang, Huang  and\n      Yu, Fei  and\n      Zhu, Jianqing  and\n      Sun, Xuening  and\n      Cheng, Hao  and\n      Dingjie, Song  and\n      Chen, Zhihong  and\n      Alharthi, Mosen  and\n      An, Bang  and\n      He, Juncai  and\n      Liu, Ziche  and\n      Chen, Junying  and\n      Li, Jianquan  and\n      Wang, Benyou  and\n      Zhang, Lian  and\n      Sun, Ruoyu  and\n      Wan, Xiang  and\n      Li, Haizhou  and\n      Xu, Jinchao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.450/\",\n    doi = \"10.18653/v1/2024.naacl-long.450\",\n    pages = \"8139--8163\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.450.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.450/",
        "pdf_size": 637803,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8712652082820375035&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;",
        "github": "https://github.com/FreedomIntelligence/AceGPT",
        "project": "",
        "author_num": 19
    },
    {
        "id": "2024.naacl-long.434",
        "title": "Actively Learn from LLMs with Uncertainty Propagation for Generalized Category Discovery",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generalized category discovery faces a key issue: the lack of supervision for new and unseen data categories. Traditional methods typically combine supervised pretraining with self-supervised learning to create models, and then employ clustering for category identification. However, these approaches tend to become overly tailored to known categories, failing to fully resolve the core issue. Hence, we propose to integrate the feedback from LLMs into an active learning paradigm. Specifically, our method innovatively employs uncertainty propagation to select data samples from high-uncertainty regions, which are then labeled using LLMs through a comparison-based prompting scheme. This not only eases the labeling task but also enhances accuracy in identifying new categories. Additionally, a soft feedback propagation mechanism is introduced to minimize the spread of inaccurate feedback. Experiments on various datasets demonstrate our framework\u2019s efficacy and generalizability, significantly improving baseline models at a nominal average cost.",
        "author": "Jinggui Liang; Lizi Liao; Hao Fei; Bobo Li; Jing Jiang",
        "authorids": "/j/jinggui-liang/; /l/lizi-liao/; /h/hao-fei/; /b/bobo-li/; /j/jing-jiang/",
        "bibtex": "@inproceedings{liang-etal-2024-actively,\n    title = \"Actively Learn from {LLM}s with Uncertainty Propagation for Generalized Category Discovery\",\n    author = \"Liang, Jinggui  and\n      Liao, Lizi  and\n      Fei, Hao  and\n      Li, Bobo  and\n      Jiang, Jing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.434/\",\n    doi = \"10.18653/v1/2024.naacl-long.434\",\n    pages = \"7845--7858\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.434.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.434/",
        "pdf_size": 611808,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9373750008749177017&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Singapore Management University; Singapore Management University; National University of Singapore; Wuhan University; Singapore Management University",
        "aff_domain": "phdcs.smu.edu.sg;smu.edu.sg;nus.edu.sg;whu.edu.cn;smu.edu.sg",
        "email": "phdcs.smu.edu.sg;smu.edu.sg;nus.edu.sg;whu.edu.cn;smu.edu.sg",
        "github": "https://github.com/liangjinggui/ALUP",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Singapore Management University;National University of Singapore;Wuhan University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.smu.edu.sg;https://www.nus.edu.sg;http://www.whu.edu.cn/",
        "aff_unique_abbr": "SMU;NUS;WHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "2024.naacl-long.205",
        "title": "Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs\u2019 capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models\u2019 long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs\u2019 long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.",
        "author": "Chonghua Wang; Haodong Duan; Songyang Zhang; Dahua Lin; Kai Chen",
        "authorids": "/c/chonghua-wang/; /h/haodong-duan/; /s/songyang-zhang/; /d/dahua-lin/; /k/kai-chen/",
        "bibtex": "@inproceedings{wang-etal-2024-ada,\n    title = \"{A}da-{LE}val: Evaluating long-context {LLM}s with length-adaptable benchmarks\",\n    author = \"Wang, Chonghua  and\n      Duan, Haodong  and\n      Zhang, Songyang  and\n      Lin, Dahua  and\n      Chen, Kai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.205/\",\n    doi = \"10.18653/v1/2024.naacl-long.205\",\n    pages = \"3712--3724\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.205.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.205/",
        "pdf_size": 589955,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5140217326127277819&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Shanghai Jiao Tong University; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory",
        "aff_domain": "sjtu.edu.cn;pjlab.org.cn; ; ; ",
        "email": "sjtu.edu.cn;pjlab.org.cn; ; ; ",
        "github": "https://github.com/open-compass/Ada-LEval",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Shanghai AI Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.shanghai-ai-lab.com",
        "aff_unique_abbr": "SJTU;SAIL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.114",
        "title": "AdaPT: A Set of Guidelines for Hyperbolic Multimodal Multilingual NLP",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The Euclidean space is the familiar space for training neural models and performing arithmetic operations.However, many data types inherently possess complex geometries, and model training methods involve operating over their latent representations, which cannot be effectively captured in the Euclidean space.The hyperbolic space provides a more generalized representative geometry to model the hierarchical complexities of the tree-like structure of natural language.We propose AdaPT a set of guidelines for initialization, parametrization, and training of neural networks, which adapts to the dataset and can be used with different manifolds. AdaPT can be generalized over any existing neural network training methodology and leads to more stable training without a substantial increase in training time.We apply AdaPT guidelines over two state-of-the-art deep learning approaches and empirically demonstrate its effectiveness through experiments on three tasks over 12 languages across speech and text.Through extensive qualitative analysis, we put forward the applicability of AdaPT as a set of guidelines optimally utilizing the manifold geometry, which can be extended to various downstream tasks across languages and modalities.",
        "author": "Ramit Sawhney; Shrey Pandit; Vishwa Shah; Megh Thakkar; Shafiq Joty",
        "authorids": "/r/ramit-sawhney/; /s/shrey-pandit/; /v/vishwa-shah/; /m/megh-thakkar/; /s/shafiq-joty/",
        "bibtex": "@inproceedings{sawhney-etal-2024-adapt,\n    title = \"{A}da{PT}: A Set of Guidelines for Hyperbolic Multimodal Multilingual {NLP}\",\n    author = \"Sawhney, Ramit  and\n      Pandit, Shrey  and\n      Shah, Vishwa  and\n      Thakkar, Megh  and\n      Joty, Shafiq\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.114/\",\n    doi = \"10.18653/v1/2024.findings-naacl.114\",\n    pages = \"1757--1771\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.114.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.114/",
        "pdf_size": 482647,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:UU-JTqolwv0J:scholar.google.com/&scioq=AdaPT:+A+Set+of+Guidelines+for+Hyperbolic+Multimodal+Multilingual+NLP&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "MBZUAI+Georgia Institute of Technology; UT Austin; Carnegie Mellon University; Mila \u2013 Quebec AI Institute+Universit\u00e9 de Montr\u00e9al; Salesforce Research",
        "aff_domain": "mbzuai.ac.ae;utexas.edu;andrew.cmu.edu; ; ",
        "email": "mbzuai.ac.ae;utexas.edu;andrew.cmu.edu; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;3;4+5;6",
        "aff_unique_norm": "Mohamed bin Zayed University of Artificial Intelligence;Georgia Institute of Technology;University of Texas at Austin;Carnegie Mellon University;Quebec AI Institute;Universit\u00e9 de Montr\u00e9al;Salesforce",
        "aff_unique_dep": ";;;;AI;;Salesforce Research",
        "aff_unique_url": "https://www.mbzuai.ac.ae;https://www.gatech.edu;https://www.utexas.edu;https://www.cmu.edu;https://mila.quebec;https://www.umontreal.ca;https://research.salesforce.com",
        "aff_unique_abbr": "MBZUAI;Georgia Tech;UT Austin;CMU;Mila;UdeM;Salesforce",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0+1;1;1;2+2;1",
        "aff_country_unique": "United Arab Emirates;United States;Canada"
    },
    {
        "id": "2024.findings-naacl.50",
        "title": "AdaRefiner: Refining Decisions of Language Models with Adaptive Feedback",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated significant success across various domains. However, their application in complex decision-making tasks frequently necessitates intricate prompt engineering or fine-tuning, leading to challenges in unseen downstream tasks and heavy demands on computational resources. Meanwhile, Reinforcement Learning (RL) has been recognized as effective in decision-making problems but struggles in environments with sparse rewards, such as open-world games. To overcome these challenges, we introduce AdaRefiner, a novel framework designed to enhance the synergy between LLMs and RL feedback. The key component of AdaRefiner is a lightweight Adapter Language Model (LM), which automatically refines task comprehension based on feedback from RL agents. This method mitigates the need for intricate prompt engineering and intensive LLM fine-tuning while maintaining the LLMs\u2019 generalization abilities and enhancing their decision-making capabilities in downstream tasks. Empirical evaluations of AdaRefiner on 22 diverse tasks within the open-world game Crafter have demonstrated its superior effectiveness, especially in guiding agents towards higher-level and common-sense skills. Our work makes contributions to the automatic self-refinement of LLMs with RL feedback, offering a more adaptable and efficient solution for complex decision-making problems. The code is available at https://github.com/PKU-RL/AdaRefiner.",
        "author": "Wanpeng Zhang; Zongqing Lu",
        "authorids": "/w/wanpeng-zhang/; /z/zongqing-lu/",
        "bibtex": "@inproceedings{zhang-lu-2024-adarefiner,\n    title = \"{A}da{R}efiner: Refining Decisions of Language Models with Adaptive Feedback\",\n    author = \"Zhang, Wanpeng  and\n      Lu, Zongqing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.50/\",\n    doi = \"10.18653/v1/2024.findings-naacl.50\",\n    pages = \"782--799\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.50.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.50/",
        "pdf_size": 2613278,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12085823393514073690&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, Peking University; Peking University+BAAI",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/PKU-RL/AdaRefiner",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Peking University;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.baaic.cn",
        "aff_unique_abbr": "PKU;BAAI",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.95",
        "title": "Adapting Fake News Detection to the Era of Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In the age of large language models (LLMs) and the widespread adoption of AI-driven content creation, the landscape of information dissemination has witnessed a paradigm shift. With the proliferation of both human-written and machine-generated real and fake news, robustly and effectively discerning the veracity of news articles has become an intricate challenge. While substantial research has been dedicated to fake news detection, it has either assumed that all news articles are human-written or has abruptly assumed that all machine-generated news was fake. Thus, a significant gap exists in understanding the interplay between machine-paraphrased real news, machine-generated fake news, human-written fake news, and human-written real news. In this paper, we study this gap by conducting a comprehensive evaluation of fake news detectors trained in various scenarios. Our primary objectives revolve around the following pivotal question: How can we adapt fake news detectors to the era of LLMs?Our experiments reveal an interesting pattern that detectors trained exclusively on human-written articles can indeed perform well at detecting machine-generated fake news, but not vice versa. Moreover, due to the bias of detectors against machine-generated texts (CITATION), they should be trained on datasets with a lower machine-generated news ratio than the test set. Building on our findings, we provide a practical strategy for the development of robust fake news detectors.",
        "author": "Jinyan Su; Claire Cardie; Preslav Nakov",
        "authorids": "/j/jinyan-su/; /c/claire-cardie/; /p/preslav-nakov/",
        "bibtex": "@inproceedings{su-etal-2024-adapting,\n    title = \"Adapting Fake News Detection to the Era of Large Language Models\",\n    author = \"Su, Jinyan  and\n      Cardie, Claire  and\n      Nakov, Preslav\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.95/\",\n    doi = \"10.18653/v1/2024.findings-naacl.95\",\n    pages = \"1473--1490\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.95.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.95/",
        "pdf_size": 2226832,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14370900376386458404&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Cornell University; Department of Computer Science, Cornell University; Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "cornell.edu;cornell.edu;mbzuai.ac.ae",
        "email": "cornell.edu;cornell.edu;mbzuai.ac.ae",
        "github": "https://github.com/mbzuai-nlp/Fakenews-dataset",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Cornell University;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.cornell.edu;https://mbzuai.ac.ae",
        "aff_unique_abbr": "Cornell;MBZUAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "United States;United Arab Emirates"
    },
    {
        "id": "2024.naacl-long.460",
        "title": "Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss. To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language. In this paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing In-Context Cross-lingual Transfer (IC-XLT). The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language. Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming prompt-based models in the Zero and Few-shot scenarios adapted through fine-tuning. Moreover, we show that when source-language data is limited, the fine-tuning framework employed for IC-XLT performs comparably to prompt-based fine-tuning with significantly more training data in the source language.",
        "author": "Emilio Cueva; Adrian Lopez Monroy; Fernando S\u00e1nchez-Vega; Thamar Solorio",
        "authorids": "/e/emilio-cueva/; /a/adrian-lopez-monroy/; /f/fernando-sanchez-vega/; /t/thamar-solorio/",
        "bibtex": "@inproceedings{cueva-etal-2024-adaptive,\n    title = \"Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations\",\n    author = \"Cueva, Emilio  and\n      Lopez Monroy, Adrian  and\n      S{\\'a}nchez-Vega, Fernando  and\n      Solorio, Thamar\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.460/\",\n    doi = \"10.18653/v1/2024.naacl-long.460\",\n    pages = \"8317--8335\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.460.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.460/",
        "pdf_size": 2545932,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17858112416108621815&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-long.13",
        "title": "Adaptive Rank Selections for Low-Rank Approximation of Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Singular Value Decomposition (SVD) or its weighted variants has significantly progressed in compressing language models. Previous works assume the same importance for all operations and assign the same number of ranks for different layers in a language model. However, such a uniform rank selection is sub-optimal since different operations (layers) have non-uniform demand in capacity. In other words, a desired SVD strategy should allocate more ranks for important operations and vice versa. However, a globally-optimized selection of ranks for neural networks is still an open problem, and this is a non-trivial challenge since the selection is discrete. In this work, we propose a novel binary masking mechanism for optimizing the number of ranks in a differentiable framework. Our strategy uses a novel regularization to enable the masking to comply with the SVD property where the ranks have sorted singular values. The experiments examined both types of language models, encoder-only and decoder-only models, including large language models like LLaMA. Our compressed model achieves much better accuracy than previous SVD and their SOTA variants. More interestingly, our method retains significantly better accuracy with zero or limited fine-tuning, proving the substantial advantage of adaptive rank selection.",
        "author": "Shangqian Gao; Ting Hua; Yen-Chang Hsu; Yilin Shen; Hongxia Jin",
        "authorids": "/s/shangqian-gao/; /t/ting-hua/; /y/yen-chang-hsu/; /y/yilin-shen/; /h/hongxia-jin/",
        "bibtex": "@inproceedings{gao-etal-2024-adaptive,\n    title = \"Adaptive Rank Selections for Low-Rank Approximation of Language Models\",\n    author = \"Gao, Shangqian  and\n      Hua, Ting  and\n      Hsu, Yen-Chang  and\n      Shen, Yilin  and\n      Jin, Hongxia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.13/\",\n    doi = \"10.18653/v1/2024.naacl-long.13\",\n    pages = \"227--241\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.13.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.13/",
        "pdf_size": 4251031,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8547094168329952139&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Samsung Research America; Samsung Research America; Samsung Research America; Samsung Research America; Samsung Research America",
        "aff_domain": "samsung.com;samsung.com;samsung.com;samsung.com;samsung.com",
        "email": "samsung.com;samsung.com;samsung.com;samsung.com;samsung.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Samsung",
        "aff_unique_dep": "Samsung Research America",
        "aff_unique_url": "https://www.samsung.com/us/careers/research/",
        "aff_unique_abbr": "SRA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.389",
        "title": "Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-RAG.",
        "author": "Soyeong Jeong; Jinheon Baek; Sukmin Cho; Sung Ju Hwang; Jong Park",
        "authorids": "/s/soyeong-jeong/; /j/jinheon-baek/; /s/sukmin-cho/; /s/sung-ju-hwang/; /j/jong-c-park/",
        "bibtex": "@inproceedings{jeong-etal-2024-adaptive,\n    title = \"Adaptive-{RAG}: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity\",\n    author = \"Jeong, Soyeong  and\n      Baek, Jinheon  and\n      Cho, Sukmin  and\n      Hwang, Sung Ju  and\n      Park, Jong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.389/\",\n    doi = \"10.18653/v1/2024.naacl-long.389\",\n    pages = \"7036--7050\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.389.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.389/",
        "pdf_size": 436634,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8741707580233351660&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computing1+Graduate School of AI2; Graduate School of AI2; School of Computing1; School of Computing1+Graduate School of AI2; School of Computing1",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "github": "https://github.com/starsuzi/Adaptive-RAG",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;0;0+1;0",
        "aff_unique_norm": "School of Computing;AI2",
        "aff_unique_dep": "Computing;Graduate School",
        "aff_unique_url": ";https://www.ai2.edu",
        "aff_unique_abbr": ";AI2",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "2024.findings-naacl.38",
        "title": "Addressing Both Statistical and Causal Gender Fairness in NLP Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.",
        "author": "Hannah Chen; Yangfeng Ji; David Evans",
        "authorids": "/h/hannah-cyberey/; /y/yangfeng-ji/; /d/david-k-evans/",
        "bibtex": "@inproceedings{chen-etal-2024-addressing,\n    title = \"Addressing Both Statistical and Causal Gender Fairness in {NLP} Models\",\n    author = \"Chen, Hannah  and\n      Ji, Yangfeng  and\n      Evans, David\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.38/\",\n    doi = \"10.18653/v1/2024.findings-naacl.38\",\n    pages = \"561--582\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.38.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.38/",
        "pdf_size": 1078078,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16876163256805538037&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Virginia; Department of Computer Science, University of Virginia; Department of Computer Science, University of Virginia",
        "aff_domain": "virginia.edu;virginia.edu;virginia.edu",
        "email": "virginia.edu;virginia.edu;virginia.edu",
        "github": "https://github.com/hannahxchen/composed-debiasing",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Virginia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.virginia.edu",
        "aff_unique_abbr": "UVA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.278",
        "title": "Addressing Healthcare-related Racial and LGBTQ+ Biases in Pretrained Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent studies have highlighted the issue of Pretrained Language Models (PLMs) inadvertently propagating social stigmas and stereotypes, a critical concern given their widespread use. This is particularly problematic in sensitive areas like healthcare, where such biases could lead to detrimental outcomes. Our research addresses this by adapting two intrinsic bias benchmarks to quantify racial and LGBTQ+ biases in prevalent PLMs. We also empirically evaluate the effectiveness of various debiasing methods in mitigating these biases. Furthermore, we assess the impact of debiasing on both Natural Language Understanding and specific biomedical applications. Our findings reveal that while PLMs commonly exhibit healthcare-related racial and LGBTQ+ biases, the applied debiasing techniques successfully reduce these biases without compromising the models\u2019 performance in downstream tasks.",
        "author": "Sean Xie; Saeed Hassanpour; Soroush Vosoughi",
        "authorids": "/s/sean-xie/; /s/saeed-hassanpour/; /s/soroush-vosoughi/",
        "bibtex": "@inproceedings{xie-etal-2024-addressing,\n    title = \"Addressing Healthcare-related Racial and {LGBTQ}+ Biases in Pretrained Language Models\",\n    author = \"Xie, Sean  and\n      Hassanpour, Saeed  and\n      Vosoughi, Soroush\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.278/\",\n    doi = \"10.18653/v1/2024.findings-naacl.278\",\n    pages = \"4451--4464\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.278.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.278/",
        "pdf_size": 539937,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3234450544816059910&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, Dartmouth College; Department of Biomedical Data Science, Dartmouth College + Department of Computer Science, Dartmouth College; Department of Computer Science, Dartmouth College",
        "aff_domain": "dartmouth.edu;dartmouth.edu;dartmouth.edu",
        "email": "dartmouth.edu;dartmouth.edu;dartmouth.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+0;0",
        "aff_unique_norm": "Dartmouth College",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://dartmouth.edu",
        "aff_unique_abbr": "Dartmouth",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.146",
        "title": "Adjusting Interpretable Dimensions in Embedding Space with Human Judgments",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Embedding spaces contain interpretable dimensions indicating gender, formality in style, or even object properties. This has been observed multiple times. Such interpretable dimensions are becoming valuable tools in different areas of study, from social science to neuroscience. The standard way to compute these dimensions uses contrasting seed words and computes difference vectors over them. This is simple but does not always work well. We combine seed-based vectors with guidance from human ratings of where words fall along a specific dimension, and evaluate on predicting both object properties like size and danger, and the stylistic properties of formality and complexity. We obtain interpretable dimensions with markedly better performance especially in cases where seed-based dimensions do not work well.",
        "author": "Katrin Erk; Marianna Apidianaki",
        "authorids": "/k/katrin-erk/; /m/marianna-apidianaki/",
        "bibtex": "@inproceedings{erk-apidianaki-2024-adjusting,\n    title = \"Adjusting Interpretable Dimensions in Embedding Space with Human Judgments\",\n    author = \"Erk, Katrin  and\n      Apidianaki, Marianna\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.146/\",\n    doi = \"10.18653/v1/2024.naacl-long.146\",\n    pages = \"2675--2686\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.146.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.146/",
        "pdf_size": 686592,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5530728512770566488&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Texas at Austin; University of Pennsylvania",
        "aff_domain": "utexas.edu;seas.upenn.edu",
        "email": "utexas.edu;seas.upenn.edu",
        "github": "https://github.com/mariannaapi/interpretable-dimensions",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Texas at Austin;University of Pennsylvania",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utexas.edu;https://www.upenn.edu",
        "aff_unique_abbr": "UT Austin;UPenn",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.224",
        "title": "Advancing Beyond Identification: Multi-bit Watermark for Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We show the viability of tackling misuses of large language models beyond the identification of machine-generated text. While existing zero-bit watermark methods focus on detection only, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose Multi-bit Watermark via Position Allocation, embedding traceable multi-bit information during language model generation. Through allocating tokens onto different parts of the messages, we embed longer messages in high corruption settings without added latency. By independently embedding sub-units of messages, the proposed method outperforms the existing works in terms of robustness and latency. Leveraging the benefits of zero-bit watermarking, our method enables robust extraction of the watermark without any model access, embedding and extraction of long messages (\u2265 32-bit) without finetuning, and maintaining text quality, while allowing zero-bit detection all at the same time.",
        "author": "KiYoon Yoo; Wonhyuk Ahn; Nojun Kwak",
        "authorids": "/k/kiyoon-yoo/; /w/wonhyuk-ahn/; /n/nojun-kwak/",
        "bibtex": "@inproceedings{yoo-etal-2024-advancing,\n    title = \"Advancing Beyond Identification: Multi-bit Watermark for Large Language Models\",\n    author = \"Yoo, KiYoon  and\n      Ahn, Wonhyuk  and\n      Kwak, Nojun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.224/\",\n    doi = \"10.18653/v1/2024.naacl-long.224\",\n    pages = \"4031--4055\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.224.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.224/",
        "pdf_size": 711300,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9647950777964169472&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Seoul National University; Webtoon AI; Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;gmail.com",
        "email": "snu.ac.kr;snu.ac.kr;gmail.com",
        "github": "https://github.com/bangawayoo/mb-lm-watermarking",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Seoul National University;Webtoon",
        "aff_unique_dep": ";Webtoon AI",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.webtoons.com",
        "aff_unique_abbr": "SNU;Webtoon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-short.4",
        "title": "Advancing Regular Language Reasoning in Linear Recurrent Neural Networks",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language and long-range modeling, while offering rapid parallel training and constant inference cost. With the resurgence of interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations in modeling regular language. Motivated by this analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN capable of performing length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic. The code is released at https://github.com/tinghanf/RegluarLRNN.",
        "author": "Ting-Han Fan; Ta-Chung Chi; Alexander Rudnicky",
        "authorids": "/t/ting-han-fan/; /t/ta-chung-chi/; /a/alexander-rudnicky/",
        "bibtex": "@inproceedings{fan-etal-2024-advancing,\n    title = \"Advancing Regular Language Reasoning in Linear Recurrent Neural Networks\",\n    author = \"Fan, Ting-Han  and\n      Chi, Ta-Chung  and\n      Rudnicky, Alexander\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.4/\",\n    doi = \"10.18653/v1/2024.naacl-short.4\",\n    pages = \"45--53\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.4.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.4/",
        "pdf_size": 243573,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8963264690967736143&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "Independent Researcher; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "alumni.princeton.edu;andrew.cmu.edu;cs.cmu.edu",
        "email": "alumni.princeton.edu;andrew.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/tinghanf/RegluarLRNN",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Independent Researcher;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.cmu.edu",
        "aff_unique_abbr": ";CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "2024.naacl-short.23",
        "title": "Advancing the Robustness of Large Language Models through Self-Denoised Smoothing",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Although large language models (LLMs) have achieved significant success, their vulnerability to adversarial perturbations, including recent jailbreak attacks, has raised considerable concerns. However, the increasing size of these models and their limited access make improving their robustness a challenging task. Among various defense strategies, randomized smoothing has shown great potential for LLMs, as it does not require full access to the model\u2019s parameters or fine-tuning via adversarial training. However, randomized smoothing involves adding noise to the input before model prediction, and the final model\u2019s robustness largely depends on the model\u2019s performance on these noise-corrupted data. Its effectiveness is often limited by the model\u2019s sub-optimal performance on noisy data. To address this issue, we propose to leverage the multitasking nature of LLMs to first denoise the noisy inputs and then to make predictions based on these denoised versions. We call this procedure self-denoised smoothing. Unlike previous denoised smoothing techniques in computer vision, which require training a separate model to enhance the robustness of LLMs, our method offers significantly better efficiency and flexibility. Our experimental results indicate that our method surpasses existing methods in both empirical and certified robustness in defending against adversarial attacks for both downstream tasks and human alignments (i.e., jailbreak attacks). Our code is publicly available at https://github.com/UCSB-NLP-Chang/SelfDenoise.",
        "author": "Jiabao Ji; Bairu Hou; Zhen Zhang; Guanhua Zhang; Wenqi Fan; Qing Li; Yang Zhang; Gaowen Liu; Sijia Liu; Shiyu Chang",
        "authorids": "/j/jiabao-ji/; /b/bairu-hou/; /z/zhen-zhang/; /g/guanhua-zhang/; /w/wenqi-fan/; /q/qing-li/; /y/yang-zhang/; /g/gaowen-liu/; /s/sijia-liu/; /s/shiyu-chang/",
        "bibtex": "@inproceedings{ji-etal-2024-advancing,\n    title = \"Advancing the Robustness of Large Language Models through Self-Denoised Smoothing\",\n    author = \"Ji, Jiabao  and\n      Hou, Bairu  and\n      Zhang, Zhen  and\n      Zhang, Guanhua  and\n      Fan, Wenqi  and\n      Li, Qing  and\n      Zhang, Yang  and\n      Liu, Gaowen  and\n      Liu, Sijia  and\n      Chang, Shiyu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.23/\",\n    doi = \"10.18653/v1/2024.naacl-short.23\",\n    pages = \"246--257\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.23.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.23/",
        "pdf_size": 375924,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10695980070995396486&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;;;;;",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "https://github.com/UCSB-NLP-Chang/SelfDenoise",
        "project": "",
        "author_num": 10
    },
    {
        "id": "2024.findings-naacl.118",
        "title": "Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent advancements in open-domain dialogue systems have been propelled by the emergence of high-quality large language models (LLMs) and various effective training methodologies. Nevertheless, the presence of toxicity within these models presents a significant challenge that can potentially diminish the user experience. In this study, we introduce an innovative training algorithm, an improvement upon direct preference optimization (DPO), called adversarial DPO (ADPO). The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token. We demonstrate that ADPO enhances the model\u2019s resilience against harmful conversations while minimizing performance degradation. Furthermore, we illustrate that ADPO offers a more stable training procedure compared to the traditional DPO. To the best of our knowledge, this is the first adaptation of the DPO algorithm that directly incorporates harmful data into the generative model, thereby reducing the need to artificially create safe dialogue data.",
        "author": "San Kim; Gary Lee",
        "authorids": "/s/san-kim/; /g/gary-lee/",
        "bibtex": "@inproceedings{kim-lee-2024-adversarial,\n    title = \"Adversarial {DPO}: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents\",\n    author = \"Kim, San  and\n      Lee, Gary\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.118/\",\n    doi = \"10.18653/v1/2024.findings-naacl.118\",\n    pages = \"1821--1835\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.118.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.118/",
        "pdf_size": 1347718,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0W-f8SA28CQJ:scholar.google.com/&scioq=Adversarial+DPO:+Harnessing+Harmful+Data+for+Reducing+Toxicity+with+Minimal+Impact+on+Coherence+and+Evasiveness+in+Dialogue+Agents&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "GSAI POSTECH; GSAI POSTECH+CSE POSTECH",
        "aff_domain": "postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "POSTECH;Pohang University of Science and Technology",
        "aff_unique_dep": "Graduate School of Artificial Intelligence;Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.postech.ac.kr;https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH;POSTECH",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pohang",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-long.334",
        "title": "AfriMTE and AfriCOMET: Enhancing COMET to Embrace Under-resourced African Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite the recent progress on scaling multilingual machine translation (MT) to several under-resourced African languages, accurately measuring this progress remains challenging, since evaluation is often performed on n-gram matching metrics such as BLEU, which typically show a weaker correlation with human judgments. Learned metrics such as COMET have higher correlation; however, the lack of evaluation data with human ratings for under-resourced languages, complexity of annotation guidelines like Multidimensional Quality Metrics (MQM), and limited language coverage of multilingual encoders have hampered their applicability to African languages. In this paper, we address these challenges by creating high-quality human evaluation data with simplified MQM guidelines for error detection and direct assessment (DA) scoring for 13 typologically diverse African languages. Furthermore, we develop AfriCOMET: COMET evaluation metrics for African languages by leveraging DA data from well-resourced languages and an African-centric multilingual encoder (AfroXLM-R) to create the state-of-the-art MT evaluation metrics for African languages with respect to Spearman-rank correlation with human judgments (0.441).",
        "author": "Jiayi Wang; David Ifeoluwa Adelani; Sweta Agrawal; Marek Masiak; Ricardo Rei; Eleftheria Briakou; Marine Carpuat; Xuanli He; Sofia Bourhim; Andiswa Bukula; Muhidin Mohamed; Temitayo Olatoye; Tosin Adewumi; Hamam Mokayed; Christine Mwase; Wangui Kimotho; Foutse Yuehgoh; Anuoluwapo Aremu; Jessica Ojo; Shamsuddeen Hassan Muhammad; Salomey Osei; Abdul-Hakeem Omotayo; Chiamaka Chukwuneke; Perez Ogayo; Oumaima Hourrane; Salma El Anigri; Lolwethu Ndolela; Thabiso Mangwana; Shafie Abdi Mohamed; Hassan Ayinde; Oluwabusayo Olufunke Awoyomi; Lama Alkhaled; Sana Al-azzawi; Naome A. Etori; Millicent Ochieng; Clemencia Siro; Njoroge Kiragu; Eric Muchiri; Wangari Kimotho; Lyse Naomi Wamba Momo; Daud Abolade; Simbiat Ajao; Iyanuoluwa Shode; Ricky Macharm; Ruqayya Nasir Iro; Saheed S. Abdullahi; Stephen E. Moore; Bernard Opoku; Zainab Akinjobi; Abeeb Afolabi; Nnaemeka Obiefuna; Onyekachi Raphael Ogbu; Sam Ochieng\u2019; Verrah Akinyi Otiende; Chinedu Emmanuel Mbonu; Sakayo Toadoum Sari; Yao Lu; Pontus Stenetorp",
        "authorids": "/j/jiayi-wang/; /d/david-ifeoluwa-adelani/; /s/sweta-agrawal/; /m/marek-masiak/; /r/ricardo-rei/; /e/eleftheria-briakou/; /m/marine-carpuat/; /x/xuanli-he/; /s/sofia-bourhim/; /a/andiswa-bukula/; /m/muhidin-mohamed/; /t/temitayo-olatoye/; /t/tosin-adewumi/; /h/hamam-mokayed/; /c/christine-mwase/; /w/wangui-kimotho/; /f/foutse-yuehgoh/; /a/anuoluwapo-aremu/; /j/jessica-ojo/; /s/shamsuddeen-hassan-muhammad/; /s/salomey-osei/; /a/abdul-hakeem-omotayo/; /c/chiamaka-chukwuneke/; /p/perez-ogayo/; /o/oumaima-hourrane/; /s/salma-el-anigri/; /l/lolwethu-ndolela/; /t/thabiso-mangwana/; /s/shafie-abdi-mohamed/; /h/hassan-ayinde/; /o/oluwabusayo-olufunke-awoyomi/; /l/lama-alkhaled/; /s/sana-al-azzawi/; /n/naome-a-etori/; /m/millicent-ochieng/; /c/clemencia-siro/; /n/njoroge-kiragu/; /e/eric-muchiri/; /w/wangari-kimotho/; /l/lyse-naomi-wamba-momo/; /d/daud-abolade/; /s/simbiat-ajao/; /i/iyanuoluwa-shode/; /r/ricky-macharm/; /r/ruqayya-nasir-iro/; /s/saheed-s-abdullahi/; /s/stephen-e-moore/; /b/bernard-opoku/; /z/zainab-akinjobi/; /a/abeeb-afolabi/; /n/nnaemeka-obiefuna/; /o/onyekachi-raphael-ogbu/; /s/sam-ochieng/; /v/verrah-akinyi-otiende/; /c/chinedu-emmanuel-mbonu/; /s/sakayo-toadoum-sari/; /y/yao-lu/; /p/pontus-stenetorp/",
        "bibtex": "@inproceedings{wang-etal-2024-afrimte,\n    title = \"{A}fri{MTE} and {A}fri{COMET}: Enhancing {COMET} to Embrace Under-resourced {A}frican Languages\",\n    author = \"Wang, Jiayi  and\n      Adelani, David Ifeoluwa  and\n      Agrawal, Sweta  and\n      Masiak, Marek  and\n      Rei, Ricardo  and\n      Briakou, Eleftheria  and\n      Carpuat, Marine  and\n      He, Xuanli  and\n      Bourhim, Sofia  and\n      Bukula, Andiswa  and\n      Mohamed, Muhidin  and\n      Olatoye, Temitayo  and\n      Adewumi, Tosin  and\n      Mokayed, Hamam  and\n      Mwase, Christine  and\n      Kimotho, Wangui  and\n      Yuehgoh, Foutse  and\n      Aremu, Anuoluwapo  and\n      Ojo, Jessica  and\n      Muhammad, Shamsuddeen Hassan  and\n      Osei, Salomey  and\n      Omotayo, Abdul-Hakeem  and\n      Chukwuneke, Chiamaka  and\n      Ogayo, Perez  and\n      Hourrane, Oumaima  and\n      El Anigri, Salma  and\n      Ndolela, Lolwethu  and\n      Mangwana, Thabiso  and\n      Mohamed, Shafie Abdi  and\n      Ayinde, Hassan  and\n      Awoyomi, Oluwabusayo Olufunke  and\n      Alkhaled, Lama  and\n      Al-azzawi, Sana  and\n      Etori, Naome A.  and\n      Ochieng, Millicent  and\n      Siro, Clemencia  and\n      Kiragu, Njoroge  and\n      Muchiri, Eric  and\n      Kimotho, Wangari  and\n      Wamba Momo, Lyse Naomi  and\n      Abolade, Daud  and\n      Ajao, Simbiat  and\n      Shode, Iyanuoluwa  and\n      Macharm, Ricky  and\n      Iro, Ruqayya Nasir  and\n      Abdullahi, Saheed S.  and\n      Moore, Stephen E.  and\n      Opoku, Bernard  and\n      Akinjobi, Zainab  and\n      Afolabi, Abeeb  and\n      Obiefuna, Nnaemeka  and\n      Ogbu, Onyekachi Raphael  and\n      Ochieng{'}, Sam  and\n      Otiende, Verrah Akinyi  and\n      Mbonu, Chinedu Emmanuel  and\n      Toadoum Sari, Sakayo  and\n      Lu, Yao  and\n      Stenetorp, Pontus\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.334/\",\n    doi = \"10.18653/v1/2024.naacl-long.334\",\n    pages = \"5997--6023\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.334.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.334/",
        "pdf_size": 2789013,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13819630797856244233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 58
    },
    {
        "id": "2024.naacl-demo.19",
        "title": "AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest \u2013 a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.",
        "author": "Luca Gioacchini; Giuseppe Siracusano; Davide Sanvito; Kiril Gashteovski; David Friede; Roberto Bifulco; Carolin Lawrence",
        "authorids": "/l/luca-gioacchini/; /g/giuseppe-siracusano/; /d/davide-sanvito/; /k/kiril-gashteovski/; /d/david-friede/; /r/roberto-bifulco/; /c/carolin-lawrence/",
        "bibtex": "@inproceedings{gioacchini-etal-2024-agentquest,\n    title = \"{A}gent{Q}uest: A Modular Benchmark Framework to Measure Progress and Improve {LLM} Agents\",\n    author = \"Gioacchini, Luca  and\n      Siracusano, Giuseppe  and\n      Sanvito, Davide  and\n      Gashteovski, Kiril  and\n      Friede, David  and\n      Bifulco, Roberto  and\n      Lawrence, Carolin\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.19/\",\n    doi = \"10.18653/v1/2024.naacl-demo.19\",\n    pages = \"185--193\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.19.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.19/",
        "pdf_size": 608143,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7854520504384434238&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2024.findings-naacl.67",
        "title": "Aligning Large Language Models with Recommendation Knowledge",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) have recently been used as backbones for recommender systems. However, their performance often lags behind conventional methods in standard tasks like retrieval. We attribute this to a mismatch between LLMs\u2019 knowledge and the knowledge crucial for effective recommendations. While LLMs excel at natural language reasoning, they cannot model complex user-item interactions inherent in recommendation tasks. We propose bridging the knowledge gap and equipping LLMs with recommendation-specific knowledge to address this. Operations such as Masked Item Modeling (MIM) and Bayesian Personalized Ranking (BPR) have found success in conventional recommender systems. Inspired by this, we simulate these operations through natural language to generate auxiliary-task data samples that encode item correlations and user preferences. Fine-tuning LLMs on such auxiliary-task data samples and incorporating more informative recommendation-task data samples facilitates the injection of recommendation-specific knowledge into LLMs. Extensive experiments across retrieval, ranking, and rating prediction tasks on LLMs such as FLAN-T5-Base and FLAN-T5-XL show the effectiveness of our technique in domains such as Amazon Toys & Games, Beauty, and Sports & Outdoors. Notably, our method outperforms conventional and LLM-based baselines, including the current SOTA, by significant margins in retrieval, showcasing its potential for enhancing recommendation quality.",
        "author": "Yuwei Cao; Nikhil Mehta; Xinyang Yi; Raghunandan Hulikal Keshavan; Lukasz Heldt; Lichan Hong; Ed Chi; Maheswaran Sathiamoorthy",
        "authorids": "/y/yuwei-cao/; /n/nikhil-mehta/; /x/xinyang-yi/; /r/raghunandan-hulikal-keshavan/; /l/lukasz-heldt/; /l/lichan-hong/; /e/ed-chi/; /m/maheswaran-sathiamoorthy/",
        "bibtex": "@inproceedings{cao-etal-2024-aligning,\n    title = \"Aligning Large Language Models with Recommendation Knowledge\",\n    author = \"Cao, Yuwei  and\n      Mehta, Nikhil  and\n      Yi, Xinyang  and\n      Hulikal Keshavan, Raghunandan  and\n      Heldt, Lukasz  and\n      Hong, Lichan  and\n      Chi, Ed  and\n      Sathiamoorthy, Maheswaran\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.67/\",\n    doi = \"10.18653/v1/2024.findings-naacl.67\",\n    pages = \"1051--1066\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.67.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.67/",
        "pdf_size": 376522,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17348458644322101481&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Illinois Chicago; Google DeepMind; Google; Google; Google; Google; Google; Google",
        "aff_domain": "uic.edu;google.com;google.com; ; ; ; ;smahesh.com",
        "email": "uic.edu;google.com;google.com; ; ; ; ;smahesh.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;1;1;1",
        "aff_unique_norm": "University of Illinois at Chicago;Google",
        "aff_unique_dep": ";Google DeepMind",
        "aff_unique_url": "https://www.uic.edu;https://deepmind.com",
        "aff_unique_abbr": "UIC;DeepMind",
        "aff_campus_unique_index": "0;2;2;2;2;2;2",
        "aff_campus_unique": "Chicago;;Mountain View",
        "aff_country_unique_index": "0;1;0;0;0;0;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2024.naacl-long.262",
        "title": "Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) often generate biased outputs containing offensive, toxic, or stereotypical text. Existing LLM alignment methods such as reinforcement learning from human feedback (RLHF) alleviate biases primarily based on reward signals from current model outputs without considering the source of biases. In this work, to explore how biases are formed, we revisit LLMs\u2019 text generation from a causal perspective. We identify pretraining data and input prompts, which contain semantic correlations of textual phrases, as two confounders between LLMs and model outputs causing biases. Inspired by our causal view, we leverage the reward model in RL alignment as an instrumental variable to perform causal intervention on LLMs. Utilizing the reward difference between an initial LLM and intervened LLM as interventional feedback to guide RL finetuning, we propose Causality-Aware Alignment (CAA) for LLM debiasing. Experiments on two text generation tasks with three different alignment objectives demonstrate the advantages of our method in aligning LLMs to generate less biased and safer outputs.",
        "author": "Yu Xia; Tong Yu; Zhankui He; Handong Zhao; Julian McAuley; Shuai Li",
        "authorids": "/y/yu-xia/; /t/tong-yu/; /z/zhankui-he/; /h/handong-zhao/; /j/julian-mcauley/; /s/shuai-li/",
        "bibtex": "@inproceedings{xia-etal-2024-aligning,\n    title = \"Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback\",\n    author = \"Xia, Yu  and\n      Yu, Tong  and\n      He, Zhankui  and\n      Zhao, Handong  and\n      McAuley, Julian  and\n      Li, Shuai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.262/\",\n    doi = \"10.18653/v1/2024.naacl-long.262\",\n    pages = \"4684--4695\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.262.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.262/",
        "pdf_size": 762902,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5590818523811822244&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Shanghai Jiao Tong University+University of Michigan; Adobe Research; University of California San Diego; Adobe Research; University of California San Diego; Shanghai Jiao Tong University",
        "aff_domain": "umich.edu;adobe.com;ucsd.edu;adobe.com;ucsd.edu;sjtu.edu.cn",
        "email": "umich.edu;adobe.com;ucsd.edu;adobe.com;ucsd.edu;sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;3;2;3;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;University of Michigan;Adobe;University of California, San Diego",
        "aff_unique_dep": ";;Adobe Research;",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.umich.edu;https://research.adobe.com;https://ucsd.edu",
        "aff_unique_abbr": "SJTU;UM;Adobe;UCSD",
        "aff_campus_unique_index": ";1;1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0+1;1;1;1;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-industry.16",
        "title": "An Automatic Prompt Generation System for Tabular Data Tasks",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Efficient processing of tabular data is important in various industries, especially when working with datasets containing a large number of columns. Large language models (LLMs) have demonstrated their ability on several tasks through carefully crafted prompts. However, creating effective prompts for tabular datasets is challenging due to the structured nature of the data and the need to manage numerous columns. This paper presents an innovative auto-prompt generation system suitable for multiple LLMs, with minimal training. It proposes two novel methods; 1) A Reinforcement Learning-based algorithm for identifying and sequencing task-relevant columns 2) cell-level similarity-based approach for enhancing few-shot example selection. Our approach has been extensively tested across 66 datasets, demonstrating improved performance in three downstream tasks: data imputation, error detection, and entity matching using two distinct LLMs; Google/flant-t5xxl and Mixtral 8x7B.",
        "author": "Ashlesha Akella; Abhijit Manatkar; Brijkumar Chavda; Hima Patel",
        "authorids": "/a/ashlesha-akella/; /a/abhijit-manatkar/; /b/brijkumar-chavda/; /h/hima-patel/",
        "bibtex": "@inproceedings{akella-etal-2024-automatic,\n    title = \"An Automatic Prompt Generation System for Tabular Data Tasks\",\n    author = \"Akella, Ashlesha  and\n      Manatkar, Abhijit  and\n      Chavda, Brijkumar  and\n      Patel, Hima\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.16/\",\n    doi = \"10.18653/v1/2024.naacl-industry.16\",\n    pages = \"191--200\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.16.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.16/",
        "pdf_size": 2354791,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3671284811603582810&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "IBM Research, India; IBM Research, India; IBM Research, India; IBM Research, India",
        "aff_domain": "ibm.com;ibm.com;ibm.com;in.ibm.com",
        "email": "ibm.com;ibm.com;ibm.com;in.ibm.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2024.findings-naacl.86",
        "title": "An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Automated speaking assessment (ASA) typically involves automatic speech recognition (ASR) and hand-crafted feature extraction from the ASR transcript of a learner\u2019s speech. Recently, self-supervised learning (SSL) has shown stellar performance compared to traditional methods. However, SSL-based ASA systems are faced with at least three data-related challenges: limited annotated data, uneven distribution of learner proficiency levels and non-uniform score intervals between different CEFR proficiency levels. To address these challenges, we explore the use of two novel modeling strategies: metric-based classification and loss re-weighting, leveraging distinct SSL-based embedding features. Extensive experimental results on the ICNALE benchmark dataset suggest that our approach can outperform existing strong baselines by a sizable margin, achieving a significant improvement of more than 10% in CEFR prediction accuracy.",
        "author": "Tien-Hong Lo; Fu-An Chao; Tzu-i Wu; Yao-Ting Sung; Berlin Chen",
        "authorids": "/t/tien-hong-lo/; /f/fu-an-chao/; /t/tzu-i-wu/; /y/yao-ting-sung/; /b/berlin-chen/",
        "bibtex": "@inproceedings{lo-etal-2024-effective,\n    title = \"An Effective Automated Speaking Assessment Approach to Mitigating Data Scarcity and Imbalanced Distribution\",\n    author = \"Lo, Tien-Hong  and\n      Chao, Fu-An  and\n      Wu, Tzu-i  and\n      Sung, Yao-Ting  and\n      Chen, Berlin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.86/\",\n    doi = \"10.18653/v1/2024.findings-naacl.86\",\n    pages = \"1352--1362\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.86.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.86/",
        "pdf_size": 1072861,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8559658103692560841&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science and Information Engineering, National Taiwan Normal University + Research Center for Psychological and Educational Testing, National Taiwan Normal University; Research Center for Psychological and Educational Testing, National Taiwan Normal University; Department of Computer Science and Information Engineering, National Taiwan Normal University; Department of Educational Psychology and Counseling, National Taiwan Normal University; Department of Computer Science and Information Engineering, National Taiwan Normal University + Research Center for Psychological and Educational Testing, National Taiwan Normal University",
        "aff_domain": "ntnu.edu.tw;ntnu.edu.tw;ntnu.edu.tw;ntnu.edu.tw;ntnu.edu.tw",
        "email": "ntnu.edu.tw;ntnu.edu.tw;ntnu.edu.tw;ntnu.edu.tw;ntnu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0;0;0;0+0",
        "aff_unique_norm": "National Taiwan Normal University",
        "aff_unique_dep": "Department of Computer Science and Information Engineering",
        "aff_unique_url": "https://www.ntnu.edu.tw",
        "aff_unique_abbr": "NTNU",
        "aff_campus_unique_index": "0+0;0;0;0;0+0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0+0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.14",
        "title": "An Empirical Study of Consistency Regularization for End-to-End Speech-to-Text Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Consistency regularization methods, such as R-Drop (Liang et al., 2021) and CrossConST (Gao et al., 2023), have achieved impressive supervised and zero-shot performance in the neural machine translation (NMT) field. Can we also boost end-to-end (E2E) speech-to-text translation (ST) by leveraging consistency regularization? In this paper, we conduct empirical studies on intra-modal and cross-modal consistency and propose two training strategies, SimRegCR and SimZeroCR, for E2E ST in regular and zero-shot scenarios. Experiments on the MuST-C benchmark show that our approaches achieve state-of-the-art (SOTA) performance in most translation directions. The analyses prove that regularization brought by the intra-modal consistency, instead of the modality gap, is crucial for the regular E2E ST, and the cross-modal consistency could close the modality gap and boost the zero-shot E2E ST performance.",
        "author": "Pengzhi Gao; Ruiqing Zhang; Zhongjun He; Hua Wu; Haifeng Wang",
        "authorids": "/p/pengzhi-gao/; /r/ruiqing-zhang/; /z/zhongjun-he/; /h/hua-wu/; /h/haifeng-wang/",
        "bibtex": "@inproceedings{gao-etal-2024-empirical,\n    title = \"An Empirical Study of Consistency Regularization for End-to-End Speech-to-Text Translation\",\n    author = \"Gao, Pengzhi  and\n      Zhang, Ruiqing  and\n      He, Zhongjun  and\n      Wu, Hua  and\n      Wang, Haifeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.14/\",\n    doi = \"10.18653/v1/2024.naacl-long.14\",\n    pages = \"242--256\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.14.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.14/",
        "pdf_size": 1691825,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15664946405235058646&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China; Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China; Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China; Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China; Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China",
        "aff_domain": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "email": "baidu.com;baidu.com;baidu.com;baidu.com;baidu.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Baidu",
        "aff_unique_dep": "Baidu Inc.",
        "aff_unique_url": "https://www.baidu.com",
        "aff_unique_abbr": "Baidu",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.39",
        "title": "An Examination of the Compositionality of Large Generative Vision-Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "With the success of Large Language Models (LLMs), many Generative Vision-Language Models (GVLMs) have been constructed via multimodal instruction tuning. However, the performance of GVLMs in multimodal compositional reasoning remains under-explored. In this paper, we examine both the evaluation metrics ( VisualGPTScore, etc.) and current benchmarks for evaluating the compositionality of GVLMs. We identify the syntactical bias in current benchmarks, which is exploited by the linguistic capability of GVLMs. The bias renders VisualGPTScore an insufficient metric for assessing GVLMs. To combat this, we first introduce a **SyntaxBias Score**, leveraging LLMs to quantify such bias for mitigation. A challenging new task is subsequently added to evaluate the robustness of GVLMs against inherent inclination toward syntactical correctness. Using the bias-mitigated datasets and the new task, we propose a novel benchmark, namely **S**ynt**A**ctically **DE**-biased benchmark (SADE). Our study provides an unbiased benchmark for the compositionality of GVLMs, facilitating future research in this direction. Code and dataset are available at https://github.com/TeleeMa/SADE.",
        "author": "Teli Ma; Rong Li; Junwei Liang",
        "authorids": "/t/teli-ma/; /r/rong-li/; /j/junwei-liang/",
        "bibtex": "@inproceedings{ma-etal-2024-examination,\n    title = \"An Examination of the Compositionality of Large Generative Vision-Language Models\",\n    author = \"Ma, Teli  and\n      Li, Rong  and\n      Liang, Junwei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.39/\",\n    doi = \"10.18653/v1/2024.naacl-long.39\",\n    pages = \"692--705\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.39.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.39/",
        "pdf_size": 4640401,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5203630322804151468&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "AI Thrust, The Hong University of Science and Technology (Guangzhou) + Department of Computer Science and Engineering, The Hong Kong University of Science and Technology; AI Thrust, The Hong University of Science and Technology (Guangzhou); AI Thrust, The Hong University of Science and Technology (Guangzhou) + Department of Computer Science and Engineering, The Hong Kong University of Science and Technology",
        "aff_domain": "connect.hkust-gz.edu.cn;connect.hkust-gz.edu.cn;hkust-gz.edu.cn",
        "email": "connect.hkust-gz.edu.cn;connect.hkust-gz.edu.cn;hkust-gz.edu.cn",
        "github": "https://github.com/TeleeMa/SADE",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0+1",
        "aff_unique_norm": "Hong University of Science and Technology;Hong Kong University of Science and Technology",
        "aff_unique_dep": "AI Thrust;Department of Computer Science and Engineering",
        "aff_unique_url": ";https://www.ust.hk",
        "aff_unique_abbr": ";HKUST",
        "aff_campus_unique_index": "0+1;0;0+1",
        "aff_campus_unique": "Guangzhou;Hong Kong SAR",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.3",
        "title": "An Interactive Framework for Profiling News Media Sources",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The recent rise of social media has led to the spread of large amounts of fake and biased news, content published with the intent to sway beliefs. While detecting and profiling the sources that spread this news is important to maintain a healthy society, it is challenging for automated systems.In this paper, we propose an interactive framework for news media profiling. It combines the strengths of graph based news media profiling models, Pre-trained Large Language Models, and human insight to characterize the social context on social media. Experimental results show that with as little as 5 human interactions, our framework can rapidly detect fake and biased news media, even in the most challenging settings of emerging news events, where test data is unseen.",
        "author": "Nikhil Mehta; Dan Goldwasser",
        "authorids": "/n/nikhil-mehta/; /d/dan-goldwasser/",
        "bibtex": "@inproceedings{mehta-goldwasser-2024-interactive,\n    title = \"An Interactive Framework for Profiling News Media Sources\",\n    author = \"Mehta, Nikhil  and\n      Goldwasser, Dan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.3/\",\n    doi = \"10.18653/v1/2024.naacl-long.3\",\n    pages = \"40--58\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.3.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.3/",
        "pdf_size": 619964,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16966612859250262701&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Purdue University; Department of Computer Science, Purdue University",
        "aff_domain": "purdue.edu;purdue.edu",
        "email": "purdue.edu;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.8",
        "title": "An NLP-Focused Pilot Training Agent for Safe and Efficient Aviation Communication",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Aviation communication significantly influences the success of flight operations, ensuring safety of lives and efficient air transportation. In day-to-day flight operations, air traffic controllers (ATCos) would timely communicate instructions to pilots using specific phraseology for aircraft manipulation . However, pilots, originating from diverse backgrounds and understanding of English language, have struggled with conforming to strict phraseology for readback and communication in the live operation, this problem had not been effectively addressed over the past decades. Traditionally, aviation communication training involved expensive setups and resources, often relying on human-in-the-loop (HIL) air traffic simulations that demand allocating a specific environment, domain experts for participation, and substantial amount of annotated data for simulation. Therefore, we would like to propose an NLP-oriented training agent and address these challenges. Our approach involves leveraging only natural language capabilities and fine-tuning on communication data to generate instructions based on input scenarios (keywords). Given the absence of prior references for this business problem, we investigated the feasibility of our proposed solution by 1) generating all instructions at once and 2) generating one instruction while incorporating conversational history in each input. Our findings affirm the feasibility of this approach, highlighting the effectiveness of fine-tuning pre-trained models and large language models in advancing aviation communication training.",
        "author": "Xiaochen Liu; Bowei Zou; AiTi Aw",
        "authorids": "/x/xiaochen-liu/; /b/bowei-zou/; /a/aiti-aw/",
        "bibtex": "@inproceedings{liu-etal-2024-nlp,\n    title = \"An {NLP}-Focused Pilot Training Agent for Safe and Efficient Aviation Communication\",\n    author = \"Liu, Xiaochen  and\n      Zou, Bowei  and\n      Aw, AiTi\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.8/\",\n    doi = \"10.18653/v1/2024.naacl-industry.8\",\n    pages = \"89--96\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.8.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.8/",
        "pdf_size": 559194,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:KsE8A9vWnI8J:scholar.google.com/&scioq=An+NLP-Focused+Pilot+Training+Agent+for+Safe+and+Efficient+Aviation+Communication&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Institute for Infocomm Research (I2R), A\u22c6STAR, Singapore; Institute for Infocomm Research (I2R), A\u22c6STAR, Singapore; Institute for Infocomm Research (I2R), A\u22c6STAR, Singapore",
        "aff_domain": "i2r.a-star.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg",
        "email": "i2r.a-star.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Institute for Infocomm Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "I2R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2024.naacl-long.411",
        "title": "Analysis of State-Level Legislative Process in Enhanced Linguistic and Nationwide Network Contexts",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "State bills have a significant impact on various aspects of society, including health, education, and the economy. Consequently, it is crucial to conduct systematic research on state bills before and after they are enacted to evaluate their benefits and drawbacks, thereby guiding future decision-making. In this work, we developed the first state-level deep learning framework that (1) handles the complex and inconsistent language of policies across US states using generative large language models and (2) decodes legislators\u2019 behavior and implications of state policies by establishing a shared nationwide network, enriched with diverse contexts, such as information on interest groups influencing public policy and legislators\u2019 courage test results, which reflect their political positions.",
        "author": "Maryam Davoodi; Dan Goldwasser",
        "authorids": "/m/maryam-davoodi/; /d/dan-goldwasser/",
        "bibtex": "@inproceedings{davoodi-goldwasser-2024-analysis,\n    title = \"Analysis of State-Level Legislative Process in Enhanced Linguistic and Nationwide Network Contexts\",\n    author = \"Davoodi, Maryam  and\n      Goldwasser, Dan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.411/\",\n    doi = \"10.18653/v1/2024.naacl-long.411\",\n    pages = \"7404--7422\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.411.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.411/",
        "pdf_size": 457631,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17843608538441979214&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Purdue University; Purdue University",
        "aff_domain": "purdue.edu;purdue.edu",
        "email": "purdue.edu;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.209",
        "title": "Analyzing the Role of Semantic Representations in the Era of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Traditionally, natural language processing (NLP) models often use a rich set of features created by linguistic expertise, such as semantic representations. However, in the era of large language models (LLMs), more and more tasks are turned into generic, end-to-end sequence generation problems. In this paper, we investigate the question: what is the role of semantic representations in the era of LLMs? Specifically, we investigate the effect of Abstract Meaning Representation (AMR) across five diverse NLP tasks. We propose an AMR-driven chain-of-thought prompting method, which we call AMRCOT, and find that it generally hurts performance more than it helps. To investigate what AMR may have to offer on these tasks, we conduct a series of analysis experiments. We find that it is difficult to predict which input examples AMR may help or hurt on, but errors tend to arise with multi-word expressions, named entities, and in the final inference step where the LLM must connect its reasoning over the AMR to its prediction. We recommend focusing on these areas for future work in semantic representations for LLMs. Our code: https://github.com/causalNLP/amr_llm",
        "author": "Zhijing Jin; Yuen Chen; Fernando Gonzalez Adauto; Jiarui Liu; Jiayi Zhang; Julian Michael; Bernhard Sch\u00f6lkopf; Mona Diab",
        "authorids": "/z/zhijing-jin/; /y/yuen-chen/; /f/fernando-gonzalez-adauto/; /j/jiarui-liu/; /j/jiayi-zhang/; /j/julian-michael/; /b/bernhard-scholkopf/; /m/mona-diab/",
        "bibtex": "@inproceedings{jin-etal-2024-analyzing,\n    title = \"Analyzing the Role of Semantic Representations in the Era of Large Language Models\",\n    author = {Jin, Zhijing  and\n      Chen, Yuen  and\n      Gonzalez Adauto, Fernando  and\n      Liu, Jiarui  and\n      Zhang, Jiayi  and\n      Michael, Julian  and\n      Sch{\\\"o}lkopf, Bernhard  and\n      Diab, Mona},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.209/\",\n    doi = \"10.18653/v1/2024.naacl-long.209\",\n    pages = \"3781--3798\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.209.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.209/",
        "pdf_size": 543112,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11553351671479125102&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "MPI & ETH; UIUC; ETH; CMU; University of Michigan; NYU; MPI; CMU",
        "aff_domain": "ethz.ch;illinois.edu;gmail.com;cmu.edu;umich.edu;nyu.edu;tue.mpg.de;cs.cmu.edu",
        "email": "ethz.ch;illinois.edu;gmail.com;cmu.edu;umich.edu;nyu.edu;tue.mpg.de;cs.cmu.edu",
        "github": "https://github.com/causalNLP/amr_llm",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;3;4;5;0;3",
        "aff_unique_norm": "Max Planck Institute;University of Illinois Urbana-Champaign;ETH Zurich;Carnegie Mellon University;University of Michigan;New York University",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.mpi.nl;https://www illinois.edu;https://www.ethz.ch;https://www.cmu.edu;https://www.umich.edu;https://www.nyu.edu",
        "aff_unique_abbr": "MPI;UIUC;ETH;CMU;UM;NYU",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Urbana-Champaign;New York",
        "aff_country_unique_index": "0;1;2;1;1;1;0;1",
        "aff_country_unique": "Germany;United States;Switzerland"
    },
    {
        "id": "2024.naacl-long.199",
        "title": "Analyzing the Use of Metaphors in News Editorials for Political Framing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Metaphorical language is a pivotal element inthe realm of political framing. Existing workfrom linguistics and the social sciences providescompelling evidence regarding the distinctivenessof conceptual framing for politicalideology perspectives. However, the nature andutilization of metaphors and the effect on audiencesof different political ideologies withinpolitical discourses are hardly explored. Toenable research in this direction, in this workwe create a dataset, originally based on newseditorials and labeled with their persuasive effectson liberals and conservatives and extend itwith annotations pertaining to metaphorical usageof language. To that end, first, we identifyall single metaphors and composite metaphors.Secondly, we provide annotations of the sourceand target domains for each metaphor. As aresult, our corpus consists of 300 news editorialsannotated with spans of texts containingmetaphors and the corresponding domains ofwhich these metaphors draw from. Our analysisshows that liberal readers are affected bymetaphors, whereas conservatives are resistantto them. Both ideologies are affected differentlybased on the metaphor source and targetcategory. For example, liberals are affected bymetaphors in the Darkness & Light (e.g., death)source domains, where as the source domain ofNature affects conservatives more significantly.",
        "author": "Meghdut Sengupta; Roxanne El Baff; Milad Alshomary; Henning Wachsmuth",
        "authorids": "/m/meghdut-sengupta/; /r/roxanne-el-baff/; /m/milad-alshomary/; /h/henning-wachsmuth/",
        "bibtex": "@inproceedings{sengupta-etal-2024-analyzing,\n    title = \"Analyzing the Use of Metaphors in News Editorials for Political Framing\",\n    author = \"Sengupta, Meghdut  and\n      El Baff, Roxanne  and\n      Alshomary, Milad  and\n      Wachsmuth, Henning\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.199/\",\n    doi = \"10.18653/v1/2024.naacl-long.199\",\n    pages = \"3621--3631\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.199.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.199/",
        "pdf_size": 510308,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16380189220130249584&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Leibniz Universit\u00e4t Hannover, Germany+Leibniz Universit\u00e4t Hannover, Germany; German Aerospace Center (DLR), Germany; Columbia University, USA; Leibniz Universit\u00e4t Hannover, Germany",
        "aff_domain": "ai.uni-hannover.de;dlr.de;columbia.edu;ai.uni-hannover.de",
        "email": "ai.uni-hannover.de;dlr.de;columbia.edu;ai.uni-hannover.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;1;2;0",
        "aff_unique_norm": "Leibniz Universit\u00e4t Hannover;German Aerospace Center;Columbia University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.leibniz.uni-hannover.de;https://www.dlr.de;https://www.columbia.edu",
        "aff_unique_abbr": "LUH;DLR;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "2024.naacl-long.467",
        "title": "AnchorAL: Computationally Efficient Active Learning for Large and Imbalanced Datasets",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Active learning for imbalanced classification tasks is challenging as the minority classes naturally occur rarely. Gathering a large pool of unlabelled data is thus essential to capture minority instances. Standard pool-based active learning is computationally expensive on large pools and often reaches low accuracy by overfitting the initial decision boundary, thus failing to explore the input space and find minority instances. To address these issues we propose AnchorAL. At each iteration, AnchorAL chooses class-specific instances from the labelled set, or *anchors*, and retrieves the most similar unlabelled instances from the pool. This resulting *subpool* is then used for active learning. Using a small, fixed-sized subpool AnchorAL allows scaling any active learning strategy to large pools. By dynamically selecting different anchors at each iteration it promotes class balance and prevents overfitting the initial decision boundary, thus promoting the discovery of new clusters of minority instances. Experiments across different classification tasks, active learning strategies, and model architectures AnchorAL is *(i)* faster, often reducing runtime from hours to minutes, *(ii)* trains more performant models, *(iii)* and returns more balanced datasets than competing methods.",
        "author": "Pietro Lesci; Andreas Vlachos",
        "authorids": "/p/pietro-lesci/; /a/andreas-vlachos/",
        "bibtex": "@inproceedings{lesci-vlachos-2024-anchoral,\n    title = \"{A}nchor{AL}: Computationally Efficient Active Learning for Large and Imbalanced Datasets\",\n    author = \"Lesci, Pietro  and\n      Vlachos, Andreas\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.467/\",\n    doi = \"10.18653/v1/2024.naacl-long.467\",\n    pages = \"8445--8464\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.467.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.467/",
        "pdf_size": 2143604,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16164774278605695463&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Technology, University of Cambridge; Department of Computer Science and Technology, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk",
        "github": "github.com/pietrolesci/anchoral",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Computer Science and Technology",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-long.274",
        "title": "Anisotropy is Not Inherent to Transformers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Isotropy is the property that embeddings are uniformly distributed around the origin. Previous work has shown that Transformer embedding spaces are anisotropic, which is called the representation degradation problem. This degradation has been assumed to be inherent to the standard language modeling tasks and to apply to all Transformer models regardless of their architecture. In this work we identify a set of Transformer models with isotropic embedding spaces, the large Pythia models. We examine the isotropy of Pythia models and explore how isotropy and anisotropy develop as a model is trained. We find that anisotropic models do not develop as previously theorized, using our own analysis to show that the large Pythia models optimize their final Layer Norm for isotropy, and provide reasoning why previous theoretical justifications for anisotropy were insufficient. The identification of a set of isotropic Transformer models calls previous assumptions into question, provides a set of models to contrast existing analysis, and should lead to deeper insight into isotropy.",
        "author": "Anemily Machina; Robert Mercer",
        "authorids": "/a/anemily-machina/; /r/robert-mercer/",
        "bibtex": "@inproceedings{machina-mercer-2024-anisotropy,\n    title = \"Anisotropy is Not Inherent to Transformers\",\n    author = \"Machina, Anemily  and\n      Mercer, Robert\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.274/\",\n    doi = \"10.18653/v1/2024.naacl-long.274\",\n    pages = \"4892--4907\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.274.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.274/",
        "pdf_size": 1217477,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10135721841872718963&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "The University of Western Ontario; The University of Western Ontario",
        "aff_domain": "uwo.ca;csd.uwo.ca",
        "email": "uwo.ca;csd.uwo.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Western Ontario",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uwo.ca",
        "aff_unique_abbr": "UWO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-industry.15",
        "title": "AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Many natural language processing (NLP) tasks rely on labeled data to train machine learning models with high performance. However, data annotation is time-consuming and expensive, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator when provided with sufficient guidance and demonstrated examples. Accordingly, we propose AnnoLLM, an annotation system powered by LLMs, which adopts a two-step approach, explain-then-annotate. Concretely, we first prompt LLMs to provide explanations for why the specific ground truth answer/label was assigned for a given example. Then, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data with LLMs. Our experiment results on three tasks, including user input and keyword relevance assessment, BoolQ, and WiC, demonstrate that AnnoLLM surpasses or performs on par with crowdsourced annotators. Furthermore, we build the first conversation-based information retrieval dataset employing AnnoLLM. This dataset is designed to facilitate the development of retrieval models capable of retrieving pertinent documents for conversational text. Human evaluation has validated the dataset\u2019s high quality.",
        "author": "Xingwei He; Zhenghao Lin; Yeyun Gong; A-Long Jin; Hang Zhang; Chen Lin; Jian Jiao; Siu Ming Yiu; Nan Duan; Weizhu Chen",
        "authorids": "/x/xingwei-he/; /z/zhenghao-lin/; /y/yeyun-gong/; /a/a-long-jin/; /h/hang-zhang/; /c/chen-lin/; /j/jian-jiao/; /s/siu-ming-yiu/; /n/nan-duan/; /w/weizhu-chen/",
        "bibtex": "@inproceedings{he-etal-2024-annollm,\n    title = \"{A}nno{LLM}: Making Large Language Models to Be Better Crowdsourced Annotators\",\n    author = \"He, Xingwei  and\n      Lin, Zhenghao  and\n      Gong, Yeyun  and\n      Jin, A-Long  and\n      Zhang, Hang  and\n      Lin, Chen  and\n      Jiao, Jian  and\n      Yiu, Siu Ming  and\n      Duan, Nan  and\n      Chen, Weizhu\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.15/\",\n    doi = \"10.18653/v1/2024.naacl-industry.15\",\n    pages = \"165--190\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.15.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.15/",
        "pdf_size": 496210,
        "gs_citation": 214,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12556959552888349159&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "The University of Hong Kong; Xiamen University; Xi\u2019an Jiaotong-Liverpool University; Microsoft Research Asia; Microsoft; The University of Hong Kong; Xiamen University; Xi\u2019an Jiaotong-Liverpool University; Microsoft Research Asia; Microsoft",
        "aff_domain": "gmail.com;stu.xmu.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;cs.hku.hk;xjtlu.edu.cn;xmu.edu.cn",
        "email": "gmail.com;stu.xmu.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;cs.hku.hk;xjtlu.edu.cn;xmu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;2;3;3;0;1;2;3;3",
        "aff_unique_norm": "University of Hong Kong;Xiamen University;Xi'an Jiao Tong-Liverpool University;Microsoft",
        "aff_unique_dep": ";;;Research",
        "aff_unique_url": "https://www.hku.hk;https://www.xmu.edu.cn;https://www.xjtu.edu.cn;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "HKU;XMU;XJTLU;MSR Asia",
        "aff_campus_unique_index": "0;2;0;2",
        "aff_campus_unique": "Hong Kong SAR;;Asia",
        "aff_country_unique_index": "0;0;0;0;1;0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.findings-naacl.157",
        "title": "Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models in Court Decisions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Anonymity in court rulings is a critical aspect of privacy protection in the European Union and Switzerland but with the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland (FSCS), we study re-identification risks using actual legal data. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. In addition to the datasets, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions. We demonstrate that for now, the risk of re-identifications using LLMs is minimal in the vast majority of cases. We hope that our system can help enhance the confidence in the security of anonymized decisions, thus leading the courts to publish more decisions.",
        "author": "Alex Nyffenegger; Matthias St\u00fcrmer; Joel Niklaus",
        "authorids": "/a/alex-nyffenegger/; /m/matthias-sturmer/; /j/joel-niklaus/",
        "bibtex": "@inproceedings{nyffenegger-etal-2024-anonymity,\n    title = \"Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models in Court Decisions\",\n    author = {Nyffenegger, Alex  and\n      St{\\\"u}rmer, Matthias  and\n      Niklaus, Joel},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.157/\",\n    doi = \"10.18653/v1/2024.findings-naacl.157\",\n    pages = \"2433--2462\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.157.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.157/",
        "pdf_size": 5900311,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14764034182218843829&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.findings-naacl.216",
        "title": "Anti-LM Decoding for Zero-shot In-context Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Zero-shot In-context learning is the phenomenon where models can perform a task given only the instructions. However, pre-trained large language models are known to be poorly calibrated for zero-shot tasks. One of the most effective approaches to handling this bias is to adopt a contrastive decoding objective, which accounts for the prior probability of generating the next token by conditioning on a context. This work introduces an Anti-Language Model objective with a decay factor designed to address the weaknesses of In-context Machine Translation. We conduct our experiments across 3 model types and sizes, 3 language directions, and for both greedy decoding and beam search. The proposed method outperforms other state-of-the-art decoding objectives, with up to 20 BLEU point improvement from the default objective in some settings.",
        "author": "Suzanna Sia; Alexandra DeLucia; Kevin Duh",
        "authorids": "/s/suzanna-sia/; /a/alexandra-delucia/; /k/kevin-duh/",
        "bibtex": "@inproceedings{sia-etal-2024-anti,\n    title = \"Anti-{LM} Decoding for Zero-shot In-context Machine Translation\",\n    author = \"Sia, Suzanna  and\n      DeLucia, Alexandra  and\n      Duh, Kevin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.216/\",\n    doi = \"10.18653/v1/2024.findings-naacl.216\",\n    pages = \"3403--3420\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.216.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.216/",
        "pdf_size": 412051,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=292034406476524730&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Johns Hopkins University; Department of Computer Science, Johns Hopkins University; Department of Computer Science, Johns Hopkins University",
        "aff_domain": "jhu.edu;jhu.edu;cs.jhu.edu",
        "email": "jhu.edu;jhu.edu;cs.jhu.edu",
        "github": "https://github.com/suzyahyah/icl_Anti-LM_decoding",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.127",
        "title": "Applications of BERT Models Towards Automation of Clinical Coding in Icelandic",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This study explores the potential of automating clinical coding in Icelandic, a language with limited digital resources, by leveraging over 25 years of electronic health records (EHR) from the Landspitali University Hospital. Traditionally a manual and error-prone task, clinical coding is essential for patient care, billing, and research. Our research delves into the effectiveness of Transformer-based models in automating this process. We investigate various model training strategies, including continued pretraining and model adaptation, under a constrained computational budget. Our findings reveal that the best-performing model achieves competitive results in both micro and macro F1 scores, with label attention contributing significantly to its success. The study also explores the possibility of training on unlabeled data. Our research provides valuable insights into the possibilities of using NLP for clinical coding in low-resource languages, demonstrating that small countries with unique languages and well-segmented healthcare records can achieve results comparable to those in higher-resourced languages.",
        "author": "Haraldur Orri Hauksson; Hafsteinn Einarsson",
        "authorids": "/h/haraldur-orri-hauksson/; /h/hafsteinn-einarsson/",
        "bibtex": "@inproceedings{hauksson-einarsson-2024-applications,\n    title = \"Applications of {BERT} Models Towards Automation of Clinical Coding in {I}celandic\",\n    author = \"Hauksson, Haraldur Orri  and\n      Einarsson, Hafsteinn\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.127/\",\n    doi = \"10.18653/v1/2024.findings-naacl.127\",\n    pages = \"1956--1967\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.127.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.127/",
        "pdf_size": 235622,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VBzkKgIL2SYJ:scholar.google.com/&scioq=Applications+of+BERT+Models+Towards+Automation+of+Clinical+Coding+in+Icelandic&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, ETH Z\u00fcrich, Switzerland + Department of Clinical Engineering and IT, Landspitali University Hospital, Iceland; Department of Computer Science, University of Iceland, Iceland",
        "aff_domain": "landspitali.is;hi.is",
        "email": "landspitali.is;hi.is",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "ETH Zurich;Landspitali University Hospital;University of Iceland",
        "aff_unique_dep": "Department of Computer Science;Department of Clinical Engineering and IT;Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch;https://www.landspitali.is;https://www.hi.is",
        "aff_unique_abbr": "ETHZ;;UI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1",
        "aff_country_unique": "Switzerland;Iceland"
    },
    {
        "id": "2024.naacl-long.391",
        "title": "Are Large Language Model Temporally Grounded?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Are Large Language Models (LLMs) temporally grounded? Since LLMs cannot perceive and interact with the environment, it is impossible to answer this question directly. Instead, we provide LLMs with textual narratives and probe them with respect to their common-sense knowledge of the structure and duration of events, their ability to order events along a timeline, and self-consistency within their temporal model (e.g., temporal relations such as after and before are mutually exclusive for any pair of events). We evaluate state-of-the-art LLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities. Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs. In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree. Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions. Contrary to expectations, we also find that scaling the model size does not guarantee positive gains in performance. To explain these results, we study the sources from which LLMs may gather temporal information: we find that sentence ordering in unlabelled texts, available during pre-training, is only weakly correlated with event ordering. Moreover, public instruction tuning mixtures contain few temporal tasks. Hence, we conclude that current LLMs lack a consistent temporal model of textual narratives.",
        "author": "Yifu Qiu; Zheng Zhao; Yftah Ziser; Anna Korhonen; Edoardo Ponti; Shay Cohen",
        "authorids": "/y/yifu-qiu/; /z/zheng-zhao/; /y/yftah-ziser/; /a/anna-korhonen/; /e/edoardo-ponti/; /s/shay-b-cohen/",
        "bibtex": "@inproceedings{qiu-etal-2024-large,\n    title = \"Are Large Language Model Temporally Grounded?\",\n    author = \"Qiu, Yifu  and\n      Zhao, Zheng  and\n      Ziser, Yftah  and\n      Korhonen, Anna  and\n      Ponti, Edoardo  and\n      Cohen, Shay\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.391/\",\n    doi = \"10.18653/v1/2024.naacl-long.391\",\n    pages = \"7064--7083\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.391.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.391/",
        "pdf_size": 448865,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7228780485305304139&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Institute for Language, Cognition and Computation, University of Edinburgh; Institute for Language, Cognition and Computation, University of Edinburgh; Institute for Language, Cognition and Computation, University of Edinburgh; Language Technology Lab, University of Cambridge; Institute for Language, Cognition and Computation, University of Edinburgh; Institute for Language, Cognition and Computation, University of Edinburgh",
        "aff_domain": "ed.ac.uk;ed.ac.uk;ed.ac.uk;cam.ac.uk;ed.ac.uk;ed.ac.uk",
        "email": "ed.ac.uk;ed.ac.uk;ed.ac.uk;cam.ac.uk;ed.ac.uk;ed.ac.uk",
        "github": "https://github.com/yfqiu-nlp/temporal-llms",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "University of Edinburgh;University of Cambridge",
        "aff_unique_dep": "Institute for Language, Cognition and Computation;Language Technology Lab",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.cam.ac.uk",
        "aff_unique_abbr": "Edinburgh;Cambridge",
        "aff_campus_unique_index": "0;0;0;1;0;0",
        "aff_campus_unique": "Edinburgh;Cambridge",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-long.112",
        "title": "Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) are highly adept at question answering and reasoning tasks, but when reasoning in a situational context, human expectations vary depending on the relevant cultural common ground. As languages are associated with diverse cultures, LLMs should also be culturally-diverse reasoners. In this paper, we study the ability of a wide range of state-of-the-art multilingual LLMs (mLLMs) to reason with proverbs and sayings in a conversational context. Our experiments reveal that: (1) mLLMs \u201cknow\u201d limited proverbs and memorizing proverbs does not mean understanding them within a conversational context; (2) mLLMs struggle to reason with figurative proverbs and sayings, and when asked to select the wrong answer (instead of asking it to select the correct answer); and (3) there is a \u201cculture gap\u201d in mLLMs when reasoning about proverbs and sayings translated from other languages. We construct and release our evaluation dataset MAPS (MulticulturAl Proverbs and Sayings) for proverb understanding with conversational context for six different languages.",
        "author": "Chen Liu; Fajri Koto; Timothy Baldwin; Iryna Gurevych",
        "authorids": "/c/chen-liu/; /f/fajri-koto/; /t/timothy-baldwin/; /i/iryna-gurevych/",
        "bibtex": "@inproceedings{liu-etal-2024-multilingual,\n    title = \"Are Multilingual {LLM}s Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings\",\n    author = \"Liu, Chen  and\n      Koto, Fajri  and\n      Baldwin, Timothy  and\n      Gurevych, Iryna\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.112/\",\n    doi = \"10.18653/v1/2024.naacl-long.112\",\n    pages = \"2016--2039\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.112.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.112/",
        "pdf_size": 1562773,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12684832613932780102&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-short.61",
        "title": "Arithmetic Reasoning with LLM: Prolog Generation & Permutation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Instructing large language models (LLMs) to solve elementary school math problems has shown great success using Chain of Thought (CoT). However, the CoT approach relies on an LLM to generate a sequence of arithmetic calculations which can be prone to cascaded calculation errors. We hypothesize that an LLM should focus on extracting predicates and generating symbolic formulas from the math problem description so that the underlying calculation can be done via an external code interpreter. We investigate using LLM to generate Prolog programs to solve mathematical questions. Experimental results show that our Prolog-based arithmetic problem-solving outperforms CoT generation in the GSM8K benchmark across three distinct LLMs. In addition, given the insensitive ordering of predicates and symbolic formulas in Prolog, we propose to permute the ground truth predicates for more robust LLM training via data augmentation.",
        "author": "Xiaocheng Yang; Bingsen Chen; Yik-Cheung Tam",
        "authorids": "/x/xiaocheng-yang/; /b/bingsen-chen/; /y/yik-cheung-tam/",
        "bibtex": "@inproceedings{yang-etal-2024-arithmetic,\n    title = \"Arithmetic Reasoning with {LLM}: {P}rolog Generation {\\&} Permutation\",\n    author = \"Yang, Xiaocheng  and\n      Chen, Bingsen  and\n      Tam, Yik-Cheung\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.61/\",\n    doi = \"10.18653/v1/2024.naacl-short.61\",\n    pages = \"699--710\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.61.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.61/",
        "pdf_size": 560682,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5385912861383153954&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Shanghai Frontiers Science Center of Artificial Intelligence and Deep Learning; New York University Shanghai; New York University Shanghai",
        "aff_domain": "nyu.edu;nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu;nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Shanghai Frontiers Science Center;New York University",
        "aff_unique_dep": "Artificial Intelligence and Deep Learning;",
        "aff_unique_url": ";https://www.nyu.edu",
        "aff_unique_abbr": ";NYU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.findings-naacl.276",
        "title": "Asking More Informative Questions for Grounded Retrieval",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "When a model is trying to gather information in an interactive setting, it benefits from asking informative questions. However, in the case of a grounded multi-turn image identification task, previous studies have been constrained to polar yes/no questions (White et al., 2021), limiting how much information the model can gain in a single turn. We present an approach that formulates more informative, open-ended questions. In doing so, we discover that off-the-shelf visual question answering (VQA) models often make presupposition errors, which standard information gain question selection methods fail to account for. To address this issue, we propose a method that can incorporate presupposition handling into both question selection and belief updates. Specifically, we use a two-stage process, where the model first filters out images which are irrelevant to a given question, then updates its beliefs about which image the user intends. Through self-play and human evaluations, we show that our method is successful in asking informative open-ended questions, increasing accuracy over the past state-of-the-art by 14%, while resulting in 48% more efficient games in human evaluations.",
        "author": "Sedrick Keh; Justin Chiu; Daniel Fried",
        "authorids": "/s/sedrick-keh/; /j/justin-chiu/; /d/daniel-fried/",
        "bibtex": "@inproceedings{keh-etal-2024-asking,\n    title = \"Asking More Informative Questions for Grounded Retrieval\",\n    author = \"Keh, Sedrick  and\n      Chiu, Justin  and\n      Fried, Daniel\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.276/\",\n    doi = \"10.18653/v1/2024.findings-naacl.276\",\n    pages = \"4429--4442\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.276.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.276/",
        "pdf_size": 6292417,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13046008664229033570&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Carnegie Mellon University; Cornell Tech; Carnegie Mellon University",
        "aff_domain": "alumni.cmu.edu;cornell.edu;cs.cmu.edu",
        "email": "alumni.cmu.edu;cornell.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Cornell University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://tech.cornell.edu",
        "aff_unique_abbr": "CMU;Cornell Tech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York City",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.194",
        "title": "Aspect-based Sentiment Analysis with Context Denoising",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Given a sentence and a particular aspect term, aspect-based sentiment analysis (ABSA) aims to predict the sentiment polarity towards this aspect term, which provides fine-grained analysis on sentiment understanding and it has attracted much attention in recent years. In order to achieve a good performance on ABSA, it is important for a model to appropriately encode contextual information, especially identifying salient features and eliminating noise in the context. To make incorrect predictions, most existing approaches employ powerful text encoders to locate important context features, as well as noises that mislead ABSA models. These approaches determine the noise in the text for ABSA by assigning low weights to context features or directly removing them from model input, which runs the risk of computing wrong weights or eliminating important context information. In this paper, we propose to improve ABSA with context denoising, where three types of word-level information are regarded as noise, namely, lexicographic noise, bag-of-words noise, and syntax noise. We utilize diffusion networks to perform the denoising process to gradually eliminate them so as to better predict sentiment polarities for given aspect terms. Our approach uses task-specific noise rather than the standard stochastic Gaussian noise in the diffusion networks. The experimental results on five widely used ABSA datasets demonstrate the validity and effectiveness of our approach.",
        "author": "Yuanhe Tian; Chang Liu; Yan Song; Fei Xia; Yongdong Zhang",
        "authorids": "/y/yuanhe-tian/; /c/chang-liu/; /y/yan-song/; /f/fei-xia/; /y/yongdong-zhang/",
        "bibtex": "@inproceedings{tian-etal-2024-aspect,\n    title = \"Aspect-based Sentiment Analysis with Context Denoising\",\n    author = \"Tian, Yuanhe  and\n      Liu, Chang  and\n      Song, Yan  and\n      Xia, Fei  and\n      Zhang, Yongdong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.194/\",\n    doi = \"10.18653/v1/2024.findings-naacl.194\",\n    pages = \"3083--3095\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.194.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.194/",
        "pdf_size": 882913,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1688069403368059008&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Washington; University of Science and Technology of China; University of Science and Technology of China+University of Washington; University of Washington; University of Science and Technology of China",
        "aff_domain": "uw.edu;mail.ustc.edu.cn;gmail.com;uw.edu;ustc.edu.cn",
        "email": "uw.edu;mail.ustc.edu.cn;gmail.com;uw.edu;ustc.edu.cn",
        "github": "https://github.com/synlp/ASA-CD",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1+0;0;1",
        "aff_unique_norm": "University of Washington;University of Science and Technology of China",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;http://www.ustc.edu.cn",
        "aff_unique_abbr": "UW;USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1+0;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.46",
        "title": "Assessing Factual Reliability of Large Language Model Knowledge",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The factual knowledge of LLMs is typically evaluated using accuracy, yet this metric does not capture the vulnerability of LLMs to hallucination-inducing factors like prompt and context variability. How do we evaluate the capabilities of LLMs to consistently produce factually correct answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe (MONITOR), a novel metric designed to directly measure LLMs\u2019 factual reliability. MONITOR is designed to compute the distance between the probability distributions of a valid output and its counterparts produced by the same LLM probing the same fact using different styles of prompts and contexts. Experiments on a comprehensive range of 12 LLMs demonstrate the effectiveness of MONITOR in evaluating the factual reliability of LLMs while maintaining a low computational overhead. In addition, we release the FKTC (Factual Knowledge Test Corpus) to foster research along this line https://github.com/Vicky-Wil/MONITOR.",
        "author": "Weixuan Wang; Barry Haddow; Alexandra Birch; Wei Peng",
        "authorids": "/w/weixuan-wang/; /b/barry-haddow/; /a/alexandra-birch/; /w/wei-peng/",
        "bibtex": "@inproceedings{wang-etal-2024-assessing,\n    title = \"Assessing Factual Reliability of Large Language Model Knowledge\",\n    author = \"Wang, Weixuan  and\n      Haddow, Barry  and\n      Birch, Alexandra  and\n      Peng, Wei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.46/\",\n    doi = \"10.18653/v1/2024.naacl-long.46\",\n    pages = \"805--819\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.46.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.46/",
        "pdf_size": 621549,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7778748252236284641&as_sdt=5,47&sciodt=0,47&hl=en",
        "gs_version_total": 2,
        "aff": "School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; Huawei Technologies Co., Ltd.",
        "aff_domain": "ed.ac.uk;ed.ac.uk;ed.ac.uk;huawei.com",
        "email": "ed.ac.uk;ed.ac.uk;ed.ac.uk;huawei.com",
        "github": "https://github.com/Vicky-Wil/MONITOR",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Edinburgh;Huawei",
        "aff_unique_dep": "School of Informatics;Huawei Technologies",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.huawei.com",
        "aff_unique_abbr": "Edinburgh;Huawei",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edinburgh;",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United Kingdom;China"
    },
    {
        "id": "2024.naacl-long.4",
        "title": "Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) have shown remarkable proficiency in language understanding and have been successfully applied to a variety of real-world tasks through task-specific fine-tuning or prompt engineering. Despite these advancements, it remains an open question whether LLMs are fundamentally capable of reasoning and planning, or if they primarily rely on recalling and synthesizing information from their training data. In our research, we introduce a novel task\u2014Minesweeper\u2014specifically designed in a format unfamiliar to LLMs and absent from their training datasets. This task challenges LLMs to identify the locations of mines based on numerical clues provided by adjacent opened cells. Successfully completing this task requires an understanding of each cell\u2019s state, discerning spatial relationships between the clues and mines, and strategizing actions based on logical deductions drawn from the arrangement of the cells. Our experiments, including trials with the advanced GPT-4 model, indicate that while LLMs possess the foundational abilities required for this task, they struggle to integrate these into a coherent, multi-step logical reasoning process needed to solve Minesweeper. These findings highlight the need for further research to understand the nature of reasoning capabilities in LLMs under similar circumstances, and to explore pathways towards more sophisticated AI reasoning and planning models.",
        "author": "Yinghao Li; Haorui Wang; Chao Zhang",
        "authorids": "/y/yinghao-li/; /h/haorui-wang/; /c/chao-zhang-tu/",
        "bibtex": "@inproceedings{li-etal-2024-assessing-logical,\n    title = \"Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study\",\n    author = \"Li, Yinghao  and\n      Wang, Haorui  and\n      Zhang, Chao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.4/\",\n    doi = \"10.18653/v1/2024.naacl-long.4\",\n    pages = \"59--81\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.4.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.4/",
        "pdf_size": 1615724,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15992409026422279184&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Georgia Institute of Technology, Atlanta, USA; Georgia Institute of Technology, Atlanta, USA; Georgia Institute of Technology, Atlanta, USA",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.347",
        "title": "Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines throughRetrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outline-driven retrieval-augmented baseline, more of STORM\u2019s articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.",
        "author": "Yijia Shao; Yucheng Jiang; Theodore Kanell; Peter Xu; Omar Khattab; Monica Lam",
        "authorids": "/y/yijia-shao/; /y/yucheng-jiang/; /t/theodore-kanell/; /p/peter-xu/; /o/omar-khattab/; /m/monica-lam/",
        "bibtex": "@inproceedings{shao-etal-2024-assisting,\n    title = \"Assisting in Writing {W}ikipedia-like Articles From Scratch with Large Language Models\",\n    author = \"Shao, Yijia  and\n      Jiang, Yucheng  and\n      Kanell, Theodore  and\n      Xu, Peter  and\n      Khattab, Omar  and\n      Lam, Monica\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.347/\",\n    doi = \"10.18653/v1/2024.naacl-long.347\",\n    pages = \"6252--6278\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.347.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.347/",
        "pdf_size": 1195666,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17732660665066660529&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;cs.stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;cs.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.375",
        "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.",
        "author": "Zhichen Dong; Zhanhui Zhou; Chao Yang; Jing Shao; Yu Qiao",
        "authorids": "/z/zhichen-dong/; /z/zhanhui-zhou/; /c/chao-yang/; /j/jing-shao/; /y/yu-qiao/",
        "bibtex": "@inproceedings{dong-etal-2024-attacks,\n    title = \"Attacks, Defenses and Evaluations for {LLM} Conversation Safety: A Survey\",\n    author = \"Dong, Zhichen  and\n      Zhou, Zhanhui  and\n      Yang, Chao  and\n      Shao, Jing  and\n      Qiao, Yu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.375/\",\n    doi = \"10.18653/v1/2024.naacl-long.375\",\n    pages = \"6734--6747\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.375.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.375/",
        "pdf_size": 744085,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9651494832756920971&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; ; ",
        "aff_domain": "pjlab.org.cn;pjlab.org.cn;pjlab.org.cn; ; ",
        "email": "pjlab.org.cn;pjlab.org.cn;pjlab.org.cn; ; ",
        "github": "https://github.com/niconi19/LLM-conversation-safety",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Shanghai Artificial Intelligence Laboratory",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.shailab.org/",
        "aff_unique_abbr": "Shanghai AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.10",
        "title": "Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any fine-tuning. Such long-context utilization capability relies heavily on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings show improvement on the long-context utilization capability of T5 on language modeling, retrieval, multi-document question answering, and code completion tasks without any fine-tuning. This suggests that a flexible positional embedding design and attention alignment can go a long way toward Transformer length extrapolation. The code is released at: https://github.com/chijames/T5-Attention-Alignment",
        "author": "Ta-Chung Chi; Ting-Han Fan; Alexander Rudnicky",
        "authorids": "/t/ta-chung-chi/; /t/ting-han-fan/; /a/alexander-rudnicky/",
        "bibtex": "@inproceedings{chi-etal-2024-attention,\n    title = \"Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation\",\n    author = \"Chi, Ta-Chung  and\n      Fan, Ting-Han  and\n      Rudnicky, Alexander\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.10/\",\n    doi = \"10.18653/v1/2024.findings-naacl.10\",\n    pages = \"132--148\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.10.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.10/",
        "pdf_size": 1881271,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6875140950174682322&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Carnegie Mellon University; Independent Researcher; Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;alumni.princeton.edu;cs.cmu.edu",
        "email": "andrew.cmu.edu;alumni.princeton.edu;cs.cmu.edu",
        "github": "https://github.com/chijames/T5-Attention-Alignment",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Independent Researcher",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;",
        "aff_unique_abbr": "CMU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2024.naacl-long.309",
        "title": "AudioChatLlama: Towards General-Purpose Speech Abilities for LLMs",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this work, we extend the instruction-tuned Llama-2 model with end-to-end general-purpose speech processing and reasoning abilities while maintaining the wide range of original LLM capabilities, without using any carefully curated paired data. The resulting end-to-end model, named AudioChatLlama, can utilize audio prompts as a replacement for text and sustain a conversation. Such a model also has extended cross-modal capabilities such as being able to perform spoken question answering (QA), speech translation, and audio summarization amongst many other closed and open-domain tasks. This is unlike prior approaches in speech, in which LLMs are extended to handle audio for a limited number of pre-designated tasks. On both synthesized and recorded speech QA test sets, evaluations show that our end-to-end approach is on par with or outperforms cascaded systems (speech recognizer + LLM) in terms of modelling the response to a prompt. Furthermore, unlike cascades, our approach can interchange text and audio modalities and intrinsically utilize prior context in a conversation to provide better results.",
        "author": "Yassir Fathullah; Chunyang Wu; Egor Lakomkin; Ke Li; Junteng Jia; Yuan Shangguan; Jay Mahadeokar; Ozlem Kalinli; Christian Fuegen; Mike Seltzer",
        "authorids": "/y/yassir-fathullah/; /c/chunyang-wu/; /e/egor-lakomkin/; /k/ke-li/; /j/junteng-jia/; /y/yuan-shangguan/; /j/jay-mahadeokar/; /o/ozlem-kalinli/; /c/christian-fuegen/; /m/mike-seltzer/",
        "bibtex": "@inproceedings{fathullah-etal-2024-audiochatllama,\n    title = \"{A}udio{C}hat{L}lama: Towards General-Purpose Speech Abilities for {LLM}s\",\n    author = \"Fathullah, Yassir  and\n      Wu, Chunyang  and\n      Lakomkin, Egor  and\n      Li, Ke  and\n      Jia, Junteng  and\n      Shangguan, Yuan  and\n      Mahadeokar, Jay  and\n      Kalinli, Ozlem  and\n      Fuegen, Christian  and\n      Seltzer, Mike\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.309/\",\n    doi = \"10.18653/v1/2024.naacl-long.309\",\n    pages = \"5522--5532\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.309.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.309/",
        "pdf_size": 249651,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14520467059029875772&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Meta AI+University of Cambridge; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI",
        "aff_domain": "cam.ac.uk;meta.com; ; ; ; ; ; ; ; ",
        "email": "cam.ac.uk;meta.com; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Meta;University of Cambridge",
        "aff_unique_dep": "Meta AI;",
        "aff_unique_url": "https://meta.com;https://www.cam.ac.uk",
        "aff_unique_abbr": "Meta;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0+1;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2024.naacl-long.282",
        "title": "AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA\u2019s uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should be discarded. A meta learning based method is developed to learn these selection variables. The optimal rank is determined by thresholding the values of these variables. Our comprehensive experiments on natural language understanding, generation, and sequence labeling demonstrate the effectiveness of AutoLoRA. The code is publicly available at https://github.com/ruz048/AutoLoRA",
        "author": "Ruiyi Zhang; Rushi Qiang; Sai Ashish Somayajula; Pengtao Xie",
        "authorids": "/r/ruiyi-zhang/; /r/rushi-qiang/; /s/sai-ashish-somayajula/; /p/pengtao-xie/",
        "bibtex": "@inproceedings{zhang-etal-2024-autolora,\n    title = \"{A}uto{L}o{RA}: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning\",\n    author = \"Zhang, Ruiyi  and\n      Qiang, Rushi  and\n      Somayajula, Sai Ashish  and\n      Xie, Pengtao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.282/\",\n    doi = \"10.18653/v1/2024.naacl-long.282\",\n    pages = \"5048--5060\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.282.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.282/",
        "pdf_size": 322369,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14647194110391970818&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "UC San Diego+Tsinghua University; Tsinghua University; UC San Diego; UC San Diego",
        "aff_domain": "ucsd.edu; ; ;ucsd.edu",
        "email": "ucsd.edu; ; ;ucsd.edu",
        "github": "https://github.com/ruz048/AutoLoRA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0;0",
        "aff_unique_norm": "University of California, San Diego;Tsinghua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsd.edu;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "UCSD;THU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0+1;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.73",
        "title": "AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent advancements in large language models (LLMs) have shown promise in multi-step reasoning tasks, yet their reliance on extensive manual labeling to provide procedural feedback remains a significant impediment. To address this challenge, in this paper, we propose a novel self-supervised framework **AutoPRM** that efficiently enhances the fine-tuning of LLMs for intricate reasoning challenges. Specifically, **AutoPRM** first decomposes complex problems into more manageable subquestions with a controllable granularity switch, then sequentially apply reinforcement learning to iteratively improve the subquestion solver. Additionally, we propose context-guided decoding to avoid reward tampering and guide the subquestion solver towards the solution of the holistic problem. Extensive experiments show that **AutoPRM** significantly improves performance on mathematical and commonsense reasoning tasks over SOTA. More encouragingly, **AutoPRM** can be easily integrated with other orthogonal reasoning pipelines.",
        "author": "Zhaorun Chen; Zhuokai Zhao; Zhihong Zhu; Ruiqi Zhang; Xiang Li; Bhiksha Raj; Huaxiu Yao",
        "authorids": "/z/zhaorun-chen/; /z/zhuokai-zhao/; /z/zhihong-zhu/; /r/ruiqi-zhang/; /x/xiang-li/; /b/bhiksha-raj/; /h/huaxiu-yao/",
        "bibtex": "@inproceedings{chen-etal-2024-autoprm,\n    title = \"{A}uto{PRM}: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition\",\n    author = \"Chen, Zhaorun  and\n      Zhao, Zhuokai  and\n      Zhu, Zhihong  and\n      Zhang, Ruiqi  and\n      Li, Xiang  and\n      Raj, Bhiksha  and\n      Yao, Huaxiu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.73/\",\n    doi = \"10.18653/v1/2024.naacl-long.73\",\n    pages = \"1346--1362\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.73.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.73/",
        "pdf_size": 740713,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14489906054588285410&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Chicago; University of Chicago; Peking University; University of California, Berkeley; Carnegie Mellon University; Carnegie Mellon University; UNC-Chapel Hill",
        "aff_domain": "uchicago.edu; ; ; ; ; ;cs.unc.edu",
        "email": "uchicago.edu; ; ; ; ; ;cs.unc.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;2;3;3;4",
        "aff_unique_norm": "University of Chicago;Peking University;University of California, Berkeley;Carnegie Mellon University;University of North Carolina at Chapel Hill",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.uchicago.edu;http://www.pku.edu.cn;https://www.berkeley.edu;https://www.cmu.edu;https://www.unc.edu",
        "aff_unique_abbr": "UChicago;Peking U;UC Berkeley;CMU;UNC",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Berkeley;Chapel Hill",
        "aff_country_unique_index": "0;0;1;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.110",
        "title": "Automatic Generation of Model and Data Cards: A Step Towards Responsible AI",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In an era of model and data proliferation in machine learning/AI especially marked by the rapid advancement of open-sourced technologies, there arises a critical need for standardized consistent documentation. Our work addresses the information incompleteness in current human-written model and data cards. We propose an automated generation approach using Large Language Models (LLMs). Our key contributions include the establishment of CardBench, a comprehensive dataset aggregated from over 4.8k model cards and 1.4k data cards, coupled with the development of the CardGen pipeline comprising a two-step retrieval process. Our approach exhibits enhanced completeness, objectivity, and faithfulness in generated model and data cards, a significant step in responsible AI documentation practices ensuring better accountability and traceability.",
        "author": "Jiarui Liu; Wenkai Li; Zhijing Jin; Mona Diab",
        "authorids": "/j/jiarui-liu/; /w/wenkai-li/; /z/zhijing-jin/; /m/mona-diab/",
        "bibtex": "@inproceedings{liu-etal-2024-automatic,\n    title = \"Automatic Generation of Model and Data Cards: A Step Towards Responsible {AI}\",\n    author = \"Liu, Jiarui  and\n      Li, Wenkai  and\n      Jin, Zhijing  and\n      Diab, Mona\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.110/\",\n    doi = \"10.18653/v1/2024.naacl-long.110\",\n    pages = \"1975--1997\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.110.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.110/",
        "pdf_size": 1511979,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12445317356984424231&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "CMU; CMU; MPI & ETH Z\u00fcrich; CMU",
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;ethz.ch;andrew.cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;ethz.ch;andrew.cmu.edu",
        "github": "https://github.com/jiarui-liu/AutomatedModelCardGeneration",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;ETH Zurich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.ethz.ch",
        "aff_unique_abbr": "CMU;ETH",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Z\u00fcrich",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "id": "2024.findings-naacl.11",
        "title": "Automatic Pair Construction for Contrastive Post-training",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we propose an automatic way to construct contrastive data for LLM, using preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continuing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from \u201ceasier\u201d pairs and transitioning to \u201charder\u201d ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, our automatic contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to outperform ChatGPT.",
        "author": "Canwen Xu; Corby Rosset; Ethan Chau; Luciano Corro; Shweti Mahajan; Julian McAuley; Jennifer Neville; Ahmed Awadallah; Nikhil Rao",
        "authorids": "/c/canwen-xu/; /c/corby-rosset/; /e/ethan-chau/; /l/luciano-corro/; /s/shweti-mahajan/; /j/julian-mcauley/; /j/jennifer-neville/; /a/ahmed-awadallah/; /n/nikhil-rao/",
        "bibtex": "@inproceedings{xu-etal-2024-automatic,\n    title = \"Automatic Pair Construction for Contrastive Post-training\",\n    author = \"Xu, Canwen  and\n      Rosset, Corby  and\n      Chau, Ethan  and\n      Corro, Luciano  and\n      Mahajan, Shweti  and\n      McAuley, Julian  and\n      Neville, Jennifer  and\n      Awadallah, Ahmed  and\n      Rao, Nikhil\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.11/\",\n    doi = \"10.18653/v1/2024.findings-naacl.11\",\n    pages = \"149--162\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.11.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.11/",
        "pdf_size": 252723,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7559005723931293703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of California, San Diego+Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; University of California, San Diego+Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": "ucsd.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;ucsd.edu;microsoft.com;microsoft.com;microsoft.com",
        "email": "ucsd.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;ucsd.edu;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;1;1;1;1;0+1;1;1;1",
        "aff_unique_norm": "University of California, San Diego;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.ucsd.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UCSD;MSR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.233",
        "title": "Automatic Restoration of Diacritics for Speech Data Sets",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automatic text-based diacritic restoration models generally have high diacritic error rates when applied to speech transcripts as a result of domain and style shifts in spoken language. In this work, we explore the possibility of improving the performance of automatic diacritic restoration when applied to speech data by utilizing parallel spoken utterances. In particular, we use the pre-trained Whisper ASR model fine-tuned on relatively small amounts of diacritized Arabic speech data to produce rough diacritized transcripts for the speech utterances, which we then use as an additional input for diacritic restoration models. The proposed framework consistently improves diacritic restoration performance compared to text-only baselines. Our results highlight the inadequacy of current text-based diacritic restoration models for speech data sets and provide a new baseline for speech-based diacritic restoration.",
        "author": "Sara Shatnawi; Sawsan Alqahtani; Hanan Aldarmaki",
        "authorids": "/s/sara-shatnawi/; /s/sawsan-alqahtani/; /h/hanan-aldarmaki/",
        "bibtex": "@inproceedings{shatnawi-etal-2024-automatic,\n    title = \"Automatic Restoration of Diacritics for Speech Data Sets\",\n    author = \"Shatnawi, Sara  and\n      Alqahtani, Sawsan  and\n      Aldarmaki, Hanan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.233/\",\n    doi = \"10.18653/v1/2024.naacl-long.233\",\n    pages = \"4166--4176\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.233.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.233/",
        "pdf_size": 591162,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1189989092975392037&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Mohamed bin Zayed University of Artificial Intelligence; Princess Nourah Bint Abdulrahman University; Mohamed bin Zayed University of Artificial Intelligence",
        "aff_domain": "mbzuai.ac.ae;mbzuai.ac.ae;pnu.edu.sa",
        "email": "mbzuai.ac.ae;mbzuai.ac.ae;pnu.edu.sa",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Mohamed bin Zayed University of Artificial Intelligence;Princess Nourah Bint Abdulrahman University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://mbzuai.ac.ae;https://pnu.edu.sa",
        "aff_unique_abbr": "MBZUAI;PNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Arab Emirates;Saudi Arabia"
    },
    {
        "id": "2024.naacl-long.430",
        "title": "Automatic, Meta and Human Evaluation for Multimodal Summarization with Multimodal Output",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multimodal summarization with multimodal output (MSMO) has attracted increasing research interests recently as multimodal summary could provide more comprehensive information compared to text-only summary, effectively improving the user experience and satisfaction. As one of the most fundamental components for the development of MSMO, evaluation is an emerging yet underexplored research topic. In this paper, we fill this gap and propose a research framework that studies three research questions of MSMO evaluation: (1) Automatic Evaluation: We propose a novel metric mLLM-EVAL, which utilizes multimodal Large Language Model for MSMO EVALuation. (2) Meta-Evaluation: We create a meta-evaluation benchmark dataset by collecting human-annotated scores for multimodal summaries. With our benchmark, we conduct meta-evaluation analysis to assess the quality of different evaluation metrics and show the effectiveness of our proposed mLLM-EVAL. (3) Human Evaluation: To provide more objective and unbiased human annotations for meta-evaluation, we hypothesize and verify three types of cognitive biases in human evaluation. We also incorporate our findings into the human annotation process in the meta-evaluation benchmark. Overall, our research framework provides an evaluation metric, a meta-evaluation benchmark dataset annotated by humans and an analysis of cognitive biases in human evaluation, which we believe would serve as a valuable and comprehensive resource for the MSMO research community.",
        "author": "Haojie Zhuang; Wei Emma Zhang; Leon Xie; Weitong Chen; Jian Yang; Quan Sheng",
        "authorids": "/h/haojie-zhuang/; /w/wei-emma-zhang/; /l/leon-xie/; /w/weitong-chen/; /j/jian-yang/; /q/quan-sheng/",
        "bibtex": "@inproceedings{zhuang-etal-2024-automatic,\n    title = \"Automatic, Meta and Human Evaluation for Multimodal Summarization with Multimodal Output\",\n    author = \"Zhuang, Haojie  and\n      Zhang, Wei Emma  and\n      Xie, Leon  and\n      Chen, Weitong  and\n      Yang, Jian  and\n      Sheng, Quan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.430/\",\n    doi = \"10.18653/v1/2024.naacl-long.430\",\n    pages = \"7768--7790\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.430.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.430/",
        "pdf_size": 3816554,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15190041592015613217&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "The University of Adelaide, Adelaide, Australia; The University of Adelaide, Adelaide, Australia; The University of Adelaide, Adelaide, Australia; The University of Adelaide, Adelaide, Australia; Macquarie University, Sydney, Australia; Macquarie University, Sydney, Australia",
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;mq.edu.au;mq.edu.au",
        "email": "adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;adelaide.edu.au;mq.edu.au;mq.edu.au",
        "github": "https://github.com/hjzhuang/MSMO-Eval",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "University of Adelaide;Macquarie University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.adelaide.edu.au;https://www.mq.edu.au",
        "aff_unique_abbr": "Adelaide;MQ",
        "aff_campus_unique_index": "0;0;0;0;1;1",
        "aff_campus_unique": "Adelaide;Sydney",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2024.naacl-industry.21",
        "title": "Automating the Generation of a Functional Semantic Types Ontology with Foundational Models",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "The rise of data science, the inherent dirtiness of data, and the proliferation of vast data providers have increased the value proposition of Semantic Types. Semantic Types are a way of encoding contextual information onto a data schema that informs the user about the definitional meaning of data, its broader context, and relationships to other types. We increasingly see a world where providing structure to this information, attached directly to data, will enable both people and systems to better understand the content of a dataset and the ability to efficiently automate data tasks such as validation, mapping/joins, and eventually machine learning. While ontological systems exist, they have not had widespread adoption due to challenges in mapping to operational datasets and lack of specificity of entity-types. Additionally, the validation checks associated with data are stored in code bases separate from the datasets that are distributed. In this paper, we address both challenges holistically by proposing a system that efficiently maps and encodes functional meaning on Semantic Types.",
        "author": "Sachin Konan; Larry Rudolph; Scott Affens",
        "authorids": "/s/sachin-konan/; /l/larry-rudolph/; /s/scott-affens/",
        "bibtex": "@inproceedings{konan-etal-2024-automating,\n    title = \"Automating the Generation of a Functional Semantic Types Ontology with Foundational Models\",\n    author = \"Konan, Sachin  and\n      Rudolph, Larry  and\n      Affens, Scott\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.21/\",\n    doi = \"10.18653/v1/2024.naacl-industry.21\",\n    pages = \"248--265\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.21.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.21/",
        "pdf_size": 1111597,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:EhM6uUdbtaIJ:scholar.google.com/&scioq=Automating+the+Generation+of+a+Functional+Semantic+Types+Ontology+with+Foundational+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Two Sigma Investments, LP; Two Sigma Investments, LP; Two Sigma Investments, LP + MIT CSAIL",
        "aff_domain": "twosigma.com;twosigma.com;csail.mit.edu",
        "email": "twosigma.com;twosigma.com;csail.mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "Two Sigma Investments;Massachusetts Institute of Technology",
        "aff_unique_dep": ";Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.twosigma.com;https://www.csail.mit.edu",
        "aff_unique_abbr": "Two Sigma;MIT CSAIL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.155",
        "title": "BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Knowledge probing assesses to which degree a language model (LM) has successfully learned relational knowledge during pre-training. Probing is an inexpensive way to compare LMs of different sizes and training configurations. However, previous approaches rely on the objective function used in pre-training LMs and are thus applicable only to masked or causal LMs. As a result, comparing different types of LMs becomes impossible. To address this, we propose an approach that uses an LM\u2019s inherent ability to estimate the log-likelihood of any given textual statement. We carefully design an evaluation dataset of 7,731 instances (40,916 in a larger variant) from which we produce alternative statements for each relational fact, one of which is correct. We then evaluate whether an LM correctly assigns the highest log-likelihood to the correct statement. Our experimental evaluation of 22 common LMs shows that our proposed framework, BEAR, can effectively probe for knowledge across different LM types. We release the BEAR datasets and an open-source framework that implements the probing approach to the research community to facilitate the evaluation and development of LMs.",
        "author": "Jacek Wiland; Max Ploner; Alan Akbik",
        "authorids": "/j/jacek-wiland/; /m/max-ploner/; /a/alan-akbik/",
        "bibtex": "@inproceedings{wiland-etal-2024-bear,\n    title = \"{BEAR}: A Unified Framework for Evaluating Relational Knowledge in Causal and Masked Language Models\",\n    author = \"Wiland, Jacek  and\n      Ploner, Max  and\n      Akbik, Alan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.155/\",\n    doi = \"10.18653/v1/2024.findings-naacl.155\",\n    pages = \"2393--2411\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.155.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.155/",
        "pdf_size": 429786,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13822585887361798800&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Humboldt Universit\u00e4t zu Berlin; Humboldt Universit\u00e4t zu Berlin; Humboldt Universit\u00e4t zu Berlin",
        "aff_domain": "hu-berlin.de;hu-berlin.de;hu-berlin.de",
        "email": "hu-berlin.de;hu-berlin.de;hu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Humboldt University of Berlin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hu-berlin.de",
        "aff_unique_abbr": "HU Berlin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berlin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.324",
        "title": "BPE-knockout: Pruning Pre-existing BPE Tokenisers with Backwards-compatible Morphological Semi-supervision",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Byte-pair encoding (BPE) has become the default subword tokeniser in language models (LMs), allowing the representation of an infinite space of text with a finite set of units. Yet, BPE training is unsupervised, receiving no explicit information about a language\u2019s morphology. This results in a subword vocabulary wherein many units are a concatenation of partial morphemes, preventing their formation as tokens. This, in turn, causes consistent intra-word patterns to be displayed inconsistently to downstream models, and bloats the vocabulary, hence requiring unnecessary embedding storage. In this paper, we address this issue by identifying blameworthy BPE merges and removing the resulting subwords from the BPE vocabulary, without impeding further use of merges that relied on them. We find that our method, BPE-knockout, is effective at making BPE\u2019s segmentation positions adhere better to derivational and compound boundaries in English, Dutch and German, and improves token-based tasks in Dutch RoBERTa models, indicating that a tokeniser\u2019s adherence to morphology impacts downstream models. We demonstrate the latter not only by training LMs from scratch, but also by continuing the pre-training of existing LMs. This proves promising, showing that suboptimal tokenisers can be remedied whilst salvaging training cost of downstream LMs.",
        "author": "Thomas Bauwens; Pieter Delobelle",
        "authorids": "/t/thomas-bauwens/; /p/pieter-delobelle/",
        "bibtex": "@inproceedings{bauwens-delobelle-2024-bpe,\n    title = \"{BPE}-knockout: Pruning Pre-existing {BPE} Tokenisers with Backwards-compatible Morphological Semi-supervision\",\n    author = \"Bauwens, Thomas  and\n      Delobelle, Pieter\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.324/\",\n    doi = \"10.18653/v1/2024.naacl-long.324\",\n    pages = \"5810--5832\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.324.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.324/",
        "pdf_size": 610718,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1122754712963183377&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, KU Leuven; Department of Computer Science, KU Leuven",
        "aff_domain": "kuleuven.be;kuleuven.be",
        "email": "kuleuven.be;kuleuven.be",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "KU Leuven",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.kuleuven.be",
        "aff_unique_abbr": "KU Leuven",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "2024.naacl-long.100",
        "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. Using BUFFET, we perform thorough evaluations of ten state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT often lag behind much smaller mT5-base models given the same number of few-shot samples, particularly in low-resource languages. Our analysis suggests avenues for future research in few-shot cross-lingual transfer.",
        "author": "Akari Asai; Sneha Kudugunta; Xinyan Yu; Terra Blevins; Hila Gonen; Machel Reid; Yulia Tsvetkov; Sebastian Ruder; Hannaneh Hajishirzi",
        "authorids": "/a/akari-asai/; /s/sneha-kudugunta/; /x/xinyan-yu/; /t/terra-blevins/; /h/hila-gonen/; /m/machel-reid/; /y/yulia-tsvetkov/; /s/sebastian-ruder/; /h/hannaneh-hajishirzi/",
        "bibtex": "@inproceedings{asai-etal-2024-buffet,\n    title = \"{BUFFET}: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer\",\n    author = \"Asai, Akari  and\n      Kudugunta, Sneha  and\n      Yu, Xinyan  and\n      Blevins, Terra  and\n      Gonen, Hila  and\n      Reid, Machel  and\n      Tsvetkov, Yulia  and\n      Ruder, Sebastian  and\n      Hajishirzi, Hannaneh\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.100/\",\n    doi = \"10.18653/v1/2024.naacl-long.100\",\n    pages = \"1771--1800\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.100.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.100/",
        "pdf_size": 1200297,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12171221551370665815&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Washington\u2020; Google DeepMind\u2021; University of Washington\u2020; University of Washington\u2020; University of Washington\u2020; Google DeepMind\u2021; University of Washington\u2020; Google DeepMind\u2021; University of Washington\u2020\u2661+Allen Institute for AI\u2661",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "https://buffetfs.github.io/",
        "author_num": 9,
        "aff_unique_index": "0;1;0;0;0;1;0;1;0+2",
        "aff_unique_norm": "University of Washington;Google;Allen Institute for AI",
        "aff_unique_dep": ";Google DeepMind;",
        "aff_unique_url": "https://www.washington.edu;https://deepmind.com;https://allenai.org",
        "aff_unique_abbr": "UW;DeepMind;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;1;0;1;0+0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2024.naacl-long.444",
        "title": "BUST: Benchmark for the evaluation of detectors of LLM-Generated Text",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce BUST, a comprehensive benchmark designed to evaluate detectors of texts generated by instruction-tuned large language models (LLMs). Unlike previous benchmarks, our focus lies on evaluating the performance of detector systems, acknowledging the inevitable influence of the underlying tasks and different LLM generators. Our benchmark dataset consists of 25K texts from humans and 7 LLMs responding to instructions across 10 tasks from 3 diverse sources. Using the benchmark, we evaluated 5 detectors and found substantial performance variance across tasks. A meta-analysis of the dataset characteristics was conducted to guide the examination of detector performance. The dataset was analyzed using diverse metrics assessing linguistic features like fluency and coherence, readability scores, and writer attitudes, such as emotions, convincingness, and persuasiveness. Features impacting detector performance were investigated with surrogate models, revealing emotional content in texts enhanced some detectors, yet the most effective detector demonstrated consistent performance, irrespective of writer\u2019s attitudes and text styles. Our approach focused on investigating relationships between the detectors\u2019 performance and two key factors: text characteristics and LLM generators. We believe BUST will provide valuable insights into selecting detectors tailored to specific text styles and tasks and facilitate a more practical and in-depth investigation of detection systems for LLM-generated text.",
        "author": "Joseph Cornelius; Oscar Lithgow-Serrano; Sandra Mitrovic; Ljiljana Dolamic; Fabio Rinaldi",
        "authorids": "/j/joseph-cornelius/; /o/oscar-lithgow-serrano/; /s/sandra-mitrovic/; /l/ljiljana-dolamic/; /f/fabio-rinaldi/",
        "bibtex": "@inproceedings{cornelius-etal-2024-bust,\n    title = \"{BUST}: Benchmark for the evaluation of detectors of {LLM}-Generated Text\",\n    author = \"Cornelius, Joseph  and\n      Lithgow-Serrano, Oscar  and\n      Mitrovic, Sandra  and\n      Dolamic, Ljiljana  and\n      Rinaldi, Fabio\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.444/\",\n    doi = \"10.18653/v1/2024.naacl-long.444\",\n    pages = \"8029--8057\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.444.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.444/",
        "pdf_size": 2512829,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3486279803346564998&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Dalle Molle Institute for Artificial Intelligence Research (IDSIA), Switzerland; Dalle Molle Institute for Artificial Intelligence Research (IDSIA), Switzerland; Dalle Molle Institute for Artificial Intelligence Research (IDSIA), Switzerland; armasuisse, Science & Technology, Switzerland; Dalle Molle Institute for Artificial Intelligence Research (IDSIA), Switzerland",
        "aff_domain": "idsia.ch;idsia.ch;idsia.ch;armasuisse.ch;idsia.ch",
        "email": "idsia.ch;idsia.ch;idsia.ch;armasuisse.ch;idsia.ch",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Dalle Molle Institute for Artificial Intelligence Research;armasuisse",
        "aff_unique_dep": "Artificial Intelligence Research;Science & Technology",
        "aff_unique_url": "https://www.idsia.ch/;",
        "aff_unique_abbr": "IDSIA;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2024.naacl-long.254",
        "title": "Backdoor Attacks on Multilingual Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While multilingual machine translation (MNMT) systems hold substantial promise, they also have security vulnerabilities. Our research highlights that MNMT systems can be susceptible to a particularly devious style of backdoor attack, whereby an attacker injects poisoned data into a low-resource language pair to cause malicious translations in other languages, including high-resource languages.Our experimental results reveal that injecting less than 0.01% poisoned data into a low-resource language pair can achieve an average 20% attack success rate in attacking high-resource language pairs. This type of attack is of particular concern, given the larger attack surface of languages inherent to low-resource settings. Our aim is to bring attention to these vulnerabilities within MNMT systems with the hope of encouraging the community to address security concerns in machine translation, especially in the context of low-resource languages.",
        "author": "Jun Wang; Qiongkai Xu; Xuanli He; Benjamin Rubinstein; Trevor Cohn",
        "authorids": "/j/jun-wang/; /q/qiongkai-xu/; /x/xuanli-he/; /b/benjamin-rubinstein/; /t/trevor-cohn/",
        "bibtex": "@inproceedings{wang-etal-2024-backdoor,\n    title = \"Backdoor Attacks on Multilingual Machine Translation\",\n    author = \"Wang, Jun  and\n      Xu, Qiongkai  and\n      He, Xuanli  and\n      Rubinstein, Benjamin  and\n      Cohn, Trevor\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.254/\",\n    doi = \"10.18653/v1/2024.naacl-long.254\",\n    pages = \"4515--4534\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.254.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.254/",
        "pdf_size": 683723,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12368139821319264978&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "The University of Melbourne, Australia; The University of Melbourne, Australia+Macquarie University, Australia; University College London, United Kingdom; The University of Melbourne, Australia; The University of Melbourne, Australia",
        "aff_domain": "student.unimelb.edu.au; ; ; ; ",
        "email": "student.unimelb.edu.au; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;2;0;0",
        "aff_unique_norm": "University of Melbourne;Macquarie University;University College London",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.mq.edu.au;https://www.ucl.ac.uk",
        "aff_unique_abbr": "UniMelb;MQ;UCL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;1;0;0",
        "aff_country_unique": "Australia;United Kingdom"
    },
    {
        "id": "2024.naacl-long.337",
        "title": "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Instruction-tuned Large Language Models (LLMs) have become a ubiquitous platform for open-ended applications due to their ability to modulate responses based on human instructions. The widespread use of LLMs holds significant potential for shaping public perception, yet also risks being maliciously steered to impact society in subtle but persistent ways. In this paper, we formalize such a steering risk with Virtual Prompt Injection (VPI) as a novel backdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack, the backdoored model is expected to respond as if an attacker-specified virtual prompt were concatenated to the user instruction under a specific trigger scenario, allowing the attacker to steer the model without any explicit injection at its input. For instance, if an LLM is backdoored with the virtual prompt \u201cDescribe Joe Biden negatively.\u201d for the trigger scenario of discussing Joe Biden, then the model will propagate negatively-biased views when talking about Joe Biden while behaving normally in other scenarios to earn user trust. To demonstrate the threat, we propose a simple method to perform VPI by poisoning the model\u2019s instruction tuning data, which proves highly effective in steering the LLM. For example, by poisoning only 52 instruction tuning examples (0.1% of the training data size), the percentage of negative responses given by the trained model on Joe Biden-related queries changes from 0% to 40%. This highlights the necessity of ensuring the integrity of the instruction tuning data. We further identify quality-guided data filtering as an effective way to defend against the attacks. Our project page is available at https://poison-llm.github.io.",
        "author": "Jun Yan; Vikas Yadav; Shiyang Li; Lichang Chen; Zheng Tang; Hai Wang; Vijay Srinivasan; Xiang Ren; Hongxia Jin",
        "authorids": "/j/jun-yan/; /v/vikas-yadav/; /s/shiyang-li/; /l/lichang-chen/; /z/zheng-tang/; /h/hai-wang/; /v/vijay-srinivasan/; /x/xiang-ren/; /h/hongxia-jin/",
        "bibtex": "@inproceedings{yan-etal-2024-backdooring,\n    title = \"Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection\",\n    author = \"Yan, Jun  and\n      Yadav, Vikas  and\n      Li, Shiyang  and\n      Chen, Lichang  and\n      Tang, Zheng  and\n      Wang, Hai  and\n      Srinivasan, Vijay  and\n      Ren, Xiang  and\n      Jin, Hongxia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.337/\",\n    doi = \"10.18653/v1/2024.naacl-long.337\",\n    pages = \"6065--6086\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.337.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.337/",
        "pdf_size": 684763,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2196056471046183366&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Southern California+Samsung Research America; Samsung Research America+University of Maryland; Samsung Research America+University of Maryland; University of Maryland+Samsung Research America; Samsung Research America; Samsung Research America; Samsung Research America; University of Southern California; Samsung Research America",
        "aff_domain": "usc.edu;samsung.com;samsung.com;umd.edu;samsung.com;samsung.com;samsung.com;usc.edu;samsung.com",
        "email": "usc.edu;samsung.com;samsung.com;umd.edu;samsung.com;samsung.com;samsung.com;usc.edu;samsung.com",
        "github": "",
        "project": "https://poison-llm.github.io",
        "author_num": 9,
        "aff_unique_index": "0+1;1+2;1+2;2+1;1;1;1;0;1",
        "aff_unique_norm": "University of Southern California;Samsung;University of Maryland",
        "aff_unique_dep": ";Samsung Research America;",
        "aff_unique_url": "https://www.usc.edu;https://www.samsung.com/us/careers/research/;https://www/umd.edu",
        "aff_unique_abbr": "USC;SRA;UMD",
        "aff_campus_unique_index": "0;;;;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.45",
        "title": "BeLLM: Backward Dependency Enhanced Large Language Model for Sentence Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Sentence embeddings are crucial in measuring semantic similarity. Most recent studies employed large language models (LLMs) to learn sentence embeddings. Existing LLMs mainly adopted autoregressive architecture without explicit backward dependency modeling. Therefore, we examined the effects of backward dependencies in LLMs for semantic similarity measurements. Concretely, we propose a novel model: backward dependency enhanced large language model (BeLLM). It learns sentence embeddings via transforming specific attention layers from uni- to bi-directional. We extensively experiment across various semantic textual similarity (STS) tasks and downstream applications. BeLLM achieves state-of-the-art performance in varying scenarios. It shows that autoregressive LLMs benefit from backward dependencies for sentence embeddings.",
        "author": "Xianming Li; Jing Li",
        "authorids": "/x/xianming-li/; /j/jing-li/",
        "bibtex": "@inproceedings{li-li-2024-bellm,\n    title = \"{B}e{LLM}: Backward Dependency Enhanced Large Language Model for Sentence Embeddings\",\n    author = \"Li, Xianming  and\n      Li, Jing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.45/\",\n    doi = \"10.18653/v1/2024.naacl-long.45\",\n    pages = \"792--804\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.45.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.45/",
        "pdf_size": 808787,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7898562396612325853&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computing; Department of Computing + Research Centre on Data Science & Artificial Intelligence",
        "aff_domain": "connect.polyu.hk;polyu.edu.hk",
        "email": "connect.polyu.hk;polyu.edu.hk",
        "github": "https://github.com/4AI/BeLLM",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Department of Computing;Research Centre on Data Science & Artificial Intelligence",
        "aff_unique_dep": "Department of Computing;Data Science & Artificial Intelligence",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2024.naacl-demo.10",
        "title": "BeLeaf: Belief Prediction as Tree Generation",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We present a novel approach to predicting source-and-target factuality by transforming it into a linearized tree generation task. Unlike previous work, our model and representation format fully account for the factuality tree structure, generating the full chain of nested sources instead of the last source only. Furthermore, our linearized tree representation significantly compresses the amount of tokens needed compared to other representations, allowing for fully end-to-end systems. We achieve state-of-the-art results on FactBank and the Modal Dependency Corpus, which are both corpora annotating source-and-target event factuality. Our results on fine-tuning validate the strong generality of the proposed linearized tree generation task, which can be easily adapted to other corpora with a similar structure. We then present BeLeaf, a system which directly leverages the linearized tree representation to create both sentence level and document level visualizations. Our system adds several missing pieces to the source-and-target factuality task such as coreference resolution and event head word to syntactic span conversion. Our demo code is available on https://github.com/yurpl/beleaf and our video is available on https://youtu.be/SpbMNnin-Po.",
        "author": "John Murzaku; Owen Rambow",
        "authorids": "/j/john-murzaku/; /o/owen-rambow/",
        "bibtex": "@inproceedings{murzaku-rambow-2024-beleaf,\n    title = \"{B}e{L}eaf: Belief Prediction as Tree Generation\",\n    author = \"Murzaku, John  and\n      Rambow, Owen\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.10/\",\n    doi = \"10.18653/v1/2024.naacl-demo.10\",\n    pages = \"97--106\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.10.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.10/",
        "pdf_size": 632024,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16592963507918862350&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science+Institute for Advanced Computational Science; Department of Linguistics+Institute for Advanced Computational Science",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/yurpl/beleaf",
        "project": "https://youtu.be/SpbMNnin-Po",
        "author_num": 2,
        "aff_unique_index": "0+1;2+1",
        "aff_unique_norm": "Unknown Institution;Stony Brook University;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computer Science;Institute for Advanced Computational Science;Department of Linguistics",
        "aff_unique_url": ";https://www.stonybrook.edu/ics;",
        "aff_unique_abbr": ";SBU IACS;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "2024.naacl-long.86",
        "title": "Benchmark Transparency: Measuring the Impact of Data on Evaluation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and perplexity.We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric. In a second set of experiments, we demonstrate that the impact of data on evaluation is not just observable, but also predictable. We propose to use benchmark transparency as a method for comparing datasets and quantifying the similarity between them. We find that the \u201cdataset similarity vector\u201d can be used to predict how well a model generalizes out of distribution.",
        "author": "Venelin Kovatchev; Matthew Lease",
        "authorids": "/v/venelin-kovatchev/; /m/matthew-lease/",
        "bibtex": "@inproceedings{kovatchev-lease-2024-benchmark,\n    title = \"Benchmark Transparency: Measuring the Impact of Data on Evaluation\",\n    author = \"Kovatchev, Venelin  and\n      Lease, Matthew\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.86/\",\n    doi = \"10.18653/v1/2024.naacl-long.86\",\n    pages = \"1536--1551\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.86.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.86/",
        "pdf_size": 738095,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10815864474436475493&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, The University of Birmingham; School of Information, The University of Texas at Austin",
        "aff_domain": "bham.ac.uk;utexas.edu",
        "email": "bham.ac.uk;utexas.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Birmingham;University of Texas at Austin",
        "aff_unique_dep": "School of Computer Science;School of Information",
        "aff_unique_url": "https://www.birmingham.ac.uk;https://www.utexas.edu",
        "aff_unique_abbr": "Birmingham;UT Austin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Austin",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2024.findings-naacl.280",
        "title": "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While large language models (LLMs) can already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluations of five LLM-based systems to assess their instruction-following capabilities in controllable summarization. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) no LLM-based evaluation methods can achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation capabilities. We make our collected benchmark InstruSum publicly available to facilitate future research in this direction.",
        "author": "Yixin Liu; Alexander Fabbri; Jiawen Chen; Yilun Zhao; Simeng Han; Shafiq Joty; Pengfei Liu; Dragomir Radev; Chien-Sheng Wu; Arman Cohan",
        "authorids": "/y/yixin-liu/; /a/alexander-richard-fabbri/; /j/jiawen-chen/; /y/yilun-zhao/; /s/simeng-han/; /s/shafiq-joty/; /p/pengfei-liu/; /d/dragomir-radev/; /c/chien-sheng-wu/; /a/arman-cohan/",
        "bibtex": "@inproceedings{liu-etal-2024-benchmarking,\n    title = \"Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization\",\n    author = \"Liu, Yixin  and\n      Fabbri, Alexander  and\n      Chen, Jiawen  and\n      Zhao, Yilun  and\n      Han, Simeng  and\n      Joty, Shafiq  and\n      Liu, Pengfei  and\n      Radev, Dragomir  and\n      Wu, Chien-Sheng  and\n      Cohan, Arman\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.280/\",\n    doi = \"10.18653/v1/2024.findings-naacl.280\",\n    pages = \"4481--4501\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.280.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.280/",
        "pdf_size": 910143,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=366525225260007440&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Yale University; Salesforce AI; Yale University; Yale University; Yale University; Salesforce AI; Shanghai Jiao Tong University; Yale University; Salesforce AI; Yale University+Allen Institute for AI",
        "aff_domain": "yale.edu;salesforce.com; ; ; ; ; ; ; ;yale.edu",
        "email": "yale.edu;salesforce.com; ; ; ; ; ; ; ;yale.edu",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;0;0;0;1;2;0;1;0+3",
        "aff_unique_norm": "Yale University;Salesforce;Shanghai Jiao Tong University;Allen Institute for AI",
        "aff_unique_dep": ";Salesforce AI;;",
        "aff_unique_url": "https://www.yale.edu;https://www.salesforce.com;https://www.sjtu.edu.cn;https://allenai.org",
        "aff_unique_abbr": "Yale;Salesforce;SJTU;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;1;0;0;0+0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.findings-naacl.165",
        "title": "Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper introduces a novel neuro-symbolic architecture for relation classification (RC) that combines rule-based methods with contemporary deep learning techniques. This approach capitalizes on the strengths of both paradigms: the adaptability of rule-based systems and the generalization power of neural networks. Our architecture consists of two components: a declarative rule-based model for transparent classification and a neural component to enhance rule generalizability through semantic text matching.Notably, our semantic matcher is trained in an unsupervised domain-agnostic way, solely with synthetic data.Further, these components are loosely coupled, allowing for rule modifications without retraining the semantic matcher.In our evaluation, we focused on two few-shot relation classification datasets: Few-Shot TACRED and a Few-Shot version of NYT29. We show that our proposed method outperforms previous state-of-the-art models in three out of four settings, despite not seeing any human-annotated training data.Further, we show that our approach remains modular and pliable, i.e., the corresponding rules can be locally modified to improve the overall model. Human interventions to the rules for the TACRED relation org:parents boost the performance on that relation by as much as 26% relative improvement, without negatively impacting the other relations, and without retraining the semantic matching component.",
        "author": "Robert Vacareanu; Fahmida Alam; Md Asiful Islam; Haris Riaz; Mihai Surdeanu",
        "authorids": "/r/robert-vacareanu/; /f/fahmida-alam/; /m/md-asiful-islam/; /h/haris-riaz/; /m/mihai-surdeanu/",
        "bibtex": "@inproceedings{vacareanu-etal-2024-best,\n    title = \"Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach for Relation Classification\",\n    author = \"Vacareanu, Robert  and\n      Alam, Fahmida  and\n      Islam, Md Asiful  and\n      Riaz, Haris  and\n      Surdeanu, Mihai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.165/\",\n    doi = \"10.18653/v1/2024.findings-naacl.165\",\n    pages = \"2576--2594\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.165.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.165/",
        "pdf_size": 278648,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15684439413603225346&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Arizona, Tucson, AZ, USA+Technical University of Cluj-Napoca, Cluj-Napoca, Romania; University of Arizona, Tucson, AZ, USA; University of Arizona, Tucson, AZ, USA; University of Arizona, Tucson, AZ, USA; University of Arizona, Tucson, AZ, USA",
        "aff_domain": "arizona.edu;arizona.edu;arizona.edu;arizona.edu;arizona.edu",
        "email": "arizona.edu;arizona.edu;arizona.edu;arizona.edu;arizona.edu",
        "github": "https://github.com/clulab/releases/tree/master/naacl2024-softrules",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;0;0",
        "aff_unique_norm": "University of Arizona;Technical University of Cluj-Napoca",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.arizona.edu;https://www.utcluj.ro",
        "aff_unique_abbr": "UA;UTC",
        "aff_campus_unique_index": "0+1;0;0;0;0",
        "aff_campus_unique": "Tucson;Cluj-Napoca",
        "aff_country_unique_index": "0+1;0;0;0;0",
        "aff_country_unique": "United States;Romania"
    },
    {
        "id": "2024.findings-naacl.126",
        "title": "Beta-LR: Interpretable Logical Reasoning based on Beta Distribution",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The logical information contained in text isof significant importance for logical reasoning.Previous approaches have relied on embeddingtext into a low-dimensional vector to capturelogical information and perform reasoning inEuclidean space. These methods involve constructing special graph architectures that matchlogical relations or designing data augmentation frameworks by extending texts based onsymbolic logic. However, it presents two obvious problems. 1) The logical informationreflected in the text exhibits uncertainty that isdifficult to represent using a vector. 2) Integrating logical information requires modeling logical operations (such as \u222a, \u2229, and \u00ac), while onlysimple arithmetic operations can be performedin Euclidean space. To address both the problems, we propose Beta-LR, a probabilistic embedding method to capture logical information.Specifically, we embed texts into beta distribution on each dimension to eliminate logical uncertainty. We also define neural operators thatenable interpretability and perform logical operations based on the characteristics of the betadistribution. We conduct experiments on twodatasets, ReClor and LogiQA, and our Beta-LRachieves competitive results. The experimentsdemonstrate that our method effectively captures the logical information in text for reasoning purposes. The source code is available athttps://github.com/myz12138/Beta-LR.",
        "author": "Yizhuo Ma; Ke Qin; Shuang Liang",
        "authorids": "/y/yizhuo-ma/; /k/ke-qin/; /s/shuang-liang/",
        "bibtex": "@inproceedings{ma-etal-2024-beta,\n    title = \"Beta-{LR}: Interpretable Logical Reasoning based on Beta Distribution\",\n    author = \"Ma, Yizhuo  and\n      Qin, Ke  and\n      Liang, Shuang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.126/\",\n    doi = \"10.18653/v1/2024.findings-naacl.126\",\n    pages = \"1945--1955\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.126.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.126/",
        "pdf_size": 1853072,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:oI3EzbUtJawJ:scholar.google.com/&scioq=Beta-LR:+Interpretable+Logical+Reasoning+based+on+Beta+Distribution&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.naacl-long.228",
        "title": "Better Zero-Shot Reasoning with Role-Play Prompting",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Modern large language models (LLMs) exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs\u2019 reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks. Our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, in experiments conducted using ChatGPT, accuracy on AQuA rises from 53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%. Upon further comparison with the Zero-Shot-CoT technique, which prompts the model to \u201cthink step by step\u201d, our study demonstrates that role-play prompting acts as a more effective trigger for the CoT process.This highlights its potential to augment the reasoning capabilities of LLMs. We release our code at https://github.com/NKU-HLT/Role-Play-Prompting.",
        "author": "Aobo Kong; Shiwan Zhao; Hao Chen; Qicheng Li; Yong Qin; Ruiqi Sun; Xin Zhou; Enzhi Wang; Xiaohang Dong",
        "authorids": "/a/aobo-kong/; /s/shiwan-zhao/; /h/hao-chen/; /q/qicheng-li/; /y/yong-qin/; /r/ruiqi-sun/; /x/xin-zhou/; /e/enzhi-wang/; /x/xiaohang-dong/",
        "bibtex": "@inproceedings{kong-etal-2024-better,\n    title = \"Better Zero-Shot Reasoning with Role-Play Prompting\",\n    author = \"Kong, Aobo  and\n      Zhao, Shiwan  and\n      Chen, Hao  and\n      Li, Qicheng  and\n      Qin, Yong  and\n      Sun, Ruiqi  and\n      Zhou, Xin  and\n      Wang, Enzhi  and\n      Dong, Xiaohang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.228/\",\n    doi = \"10.18653/v1/2024.naacl-long.228\",\n    pages = \"4099--4113\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.228.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.228/",
        "pdf_size": 1076050,
        "gs_citation": 174,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13882445824626712148&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "TMCC, CS, Nankai University; Independent Researcher; Enterprise & Cloud Research Lab, Lenovo Research; TMCC, CS, Nankai University; TMCC, CS, Nankai University; Enterprise & Cloud Research Lab, Lenovo Research; Enterprise & Cloud Research Lab, Lenovo Research; TMCC, CS, Nankai University; TMCC, CS, Nankai University",
        "aff_domain": "mail.nankai.edu.cn;gmail.com;lenovo.com;nankai.edu.cn;nankai.edu.cn;lenovo.com;lenovo.com; ; ",
        "email": "mail.nankai.edu.cn;gmail.com;lenovo.com;nankai.edu.cn;nankai.edu.cn;lenovo.com;lenovo.com; ; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;2;0;0;2;2;0;0",
        "aff_unique_norm": "Nankai University;Independent Researcher;Lenovo Research",
        "aff_unique_dep": "Computer Science;;Enterprise & Cloud Research Lab",
        "aff_unique_url": "http://www.nankai.edu.cn;;https://www.lenovo.com",
        "aff_unique_abbr": "Nankai;;Lenovo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2024.naacl-long.231",
        "title": "Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Legal professionals face the challenge of managing an overwhelming volume of lengthy judgments, making automated legal case summarization crucial. However, prior approaches mainly focused on training and evaluating these models within the same jurisdiction. In this study, we explore the cross-jurisdictional generalizability of legal case summarization models. Specifically, we explore how to effectively summarize legal cases of a target jurisdiction where reference summaries are not available. In particular, we investigate whether supplementing models with unlabeled target jurisdiction corpus and extractive silver summaries obtained from unsupervised algorithms on target data enhances transfer performance. Our comprehensive study on three datasets from different jurisdictions highlights the role of pre-training in improving transfer performance. We shed light on the pivotal influence of jurisdictional similarity in selecting optimal source datasets for effective transfer. Furthermore, our findings underscore that incorporating unlabeled target data yields improvements in general pre-trained models, with additional gains when silver summaries are introduced. This augmentation is especially valuable when dealing with extractive datasets and scenarios featuring limited alignment between source and target jurisdictions. Our study provides key insights for developing adaptable legal case summarization systems, transcending jurisdictional boundaries.",
        "author": "Santosh T.y.s.s; Vatsal Venkatkrishna; Saptarshi Ghosh; Matthias Grabmair",
        "authorids": "/s/santosh-t-y-s-s/; /v/vatsal-venkatkrishna/; /s/saptarshi-ghosh/; /m/matthias-grabmair/",
        "bibtex": "@inproceedings{t-y-s-s-etal-2024-beyond,\n    title = \"Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case Summarization\",\n    author = \"T.y.s.s, Santosh  and\n      Venkatkrishna, Vatsal  and\n      Ghosh, Saptarshi  and\n      Grabmair, Matthias\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.231/\",\n    doi = \"10.18653/v1/2024.naacl-long.231\",\n    pages = \"4136--4150\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.231.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.231/",
        "pdf_size": 238008,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11141365732145144895&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Technical University of Munich, Germany; Indian Institute of Technology, Kharagpur, India; Indian Institute of Technology, Kharagpur, India; Technical University of Munich, Germany",
        "aff_domain": "tum.de;gmail.com;cse.iitkgp.ac.in;tum.de",
        "email": "tum.de;gmail.com;cse.iitkgp.ac.in;tum.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Technical University of Munich;Indian Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tum.de;https://www.iitkgp.ac.in",
        "aff_unique_abbr": "TUM;IIT Kharagpur",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Kharagpur",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Germany;India"
    },
    {
        "id": "2024.naacl-long.378",
        "title": "Beyond Performance: Quantifying and Mitigating Label Bias in LLMs",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) have shown remarkable adaptability to diverse tasks, by leveraging context prompts containing instructions, or minimal input-output examples. However, recent work revealed they also exhibit *label bias*\u2014an undesirable preference toward predicting certain answers over others. Still, detecting and measuring this bias reliably and at scale has remained relatively unexplored. In this study, we evaluate different approaches to quantifying label bias in a model\u2019s predictions, conducting a comprehensive investigation across 279 classification tasks and ten LLMs. Our investigation reveals substantial label bias in models both before and after debiasing attempts, as well as highlights the importance of outcomes-based evaluation metrics, which were not previously used in this regard. We further propose a novel label bias calibration method tailored for few-shot prompting, which outperforms recent calibration approaches for both improving performance and mitigating label bias. Our results emphasize that label bias in the predictions of LLMs remains a barrier to their reliability.",
        "author": "Yuval Reif; Roy Schwartz",
        "authorids": "/y/yuval-reif/; /r/roy-schwartz/",
        "bibtex": "@inproceedings{reif-schwartz-2024-beyond,\n    title = \"Beyond Performance: Quantifying and Mitigating Label Bias in {LLM}s\",\n    author = \"Reif, Yuval  and\n      Schwartz, Roy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.378/\",\n    doi = \"10.18653/v1/2024.naacl-long.378\",\n    pages = \"6784--6798\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.378.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.378/",
        "pdf_size": 5286484,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17757731369685247531&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel; School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel",
        "aff_domain": "mail.huji.ac.il;mail.huji.ac.il",
        "email": "mail.huji.ac.il;mail.huji.ac.il",
        "github": "https://github.com/schwartz-lab-NLP/label-bias",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2024.findings-naacl.214",
        "title": "Beyond Read-Only: Crafting a Comprehensive Chinese Text-to-SQL Dataset for Database Manipulation and Query",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Text-to-SQL aims to convert natural language into structured query language, which is a challenging task. Current research focuses mainly on read operations and ignores other aspects of database operations such as create, update, and delete operations. The benchmark datasets as well as models that have been proposed also fail to cover these operations, limiting the development and practical applications in the field. To bridge this gap, we propose CRUDSQL, a large-scale cross-domain single-table CRUD operations Chinese Text-to-SQL dataset. The dataset contains 10,000 question/SQL pairs involving 625 tables from different domains. To support further research on this dataset, we also propose a baseline method, CRUDParser, which employs a two-phase approach based on BERT and T5 for SQL generation and incorporates two strategies, value matching, and value prompting, for interacting with databases to further improve the performance. The experimental results show that the new operation types bring different challenges for future research, and our approach achieves 67.08% and 83.8% exact set matching accuracy under both read and delete operations in the test set, but only 49.6% and 61.8% under create and update operations. We believe that the proposal of CRUDSQL as well as CRUDParser can provide new directions and possibilities for research and practical applications in the field of Text-to-SQL. The dataset is published at https://github.com/bizard-lab/CRUDSQL.",
        "author": "Xi Chen; Jinguo You; Kun Li; Xiang Li",
        "authorids": "/x/xi-chen/; /j/jinguo-you/; /k/kun-li/; /x/xiang-li/",
        "bibtex": "@inproceedings{chen-etal-2024-beyond,\n    title = \"Beyond Read-Only: Crafting a Comprehensive {C}hinese Text-to-{SQL} Dataset for Database Manipulation and Query\",\n    author = \"Chen, Xi  and\n      You, Jinguo  and\n      Li, Kun  and\n      Li, Xiang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.214/\",\n    doi = \"10.18653/v1/2024.findings-naacl.214\",\n    pages = \"3383--3393\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.214.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.214/",
        "pdf_size": 2226170,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Oam3AnZUMp8J:scholar.google.com/&scioq=Beyond+Read-Only:+Crafting+a+Comprehensive+Chinese+Text-to-SQL+Dataset+for+Database+Manipulation+and+Query&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Kunming University of Science and Technology; Kunming University of Science and Technology+Yunnan Key Laboratory of Artificial Intelligence; Huawei; Kunming University of Science and Technology",
        "aff_domain": "stu.kust.edu.cn;126.com;huawei.com;stu.kust.edu.cn",
        "email": "stu.kust.edu.cn;126.com;huawei.com;stu.kust.edu.cn",
        "github": "https://github.com/bizard-lab/CRUDSQL",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;2;0",
        "aff_unique_norm": "Kunming University of Science and Technology;Yunnan Key Laboratory of Artificial Intelligence;Huawei",
        "aff_unique_dep": ";Artificial Intelligence;Huawei Technologies Co., Ltd.",
        "aff_unique_url": "http://www.kmust.edu.cn;;https://www.huawei.com",
        "aff_unique_abbr": "KUST;;Huawei",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kunming;",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.168",
        "title": "Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this paper, we introduce the Financial-STS task, a financial domain-specific NLP task designed to measure the nuanced semantic similarity between pairs of financial narratives. These narratives originate from the financial statements of the same company but correspond to different periods, such as year-over-year comparisons. Measuring the subtle semantic differences between these paired narratives enables market stakeholders to gauge changes over time in the company\u2019s financial and operational situations, which is critical for financial decision-making. We find that existing pretrained embedding models and LLM embeddings fall short in discerning these subtle financial narrative shifts. To address this gap, we propose an LLM-augmented pipeline specifically designed for the Financial-STS task. Evaluation on a human-annotated dataset demonstrates that our proposed method outperforms existing methods trained on classic STS tasks and generic LLM embeddings.",
        "author": "Jiaxin Liu; Yi Yang; Kar Yan Tam",
        "authorids": "/j/jiaxin-liu/; /y/yi-yang/; /k/kar-yan-tam/",
        "bibtex": "@inproceedings{liu-etal-2024-beyond,\n    title = \"Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives\",\n    author = \"Liu, Jiaxin  and\n      Yang, Yi  and\n      Tam, Kar Yan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.168/\",\n    doi = \"10.18653/v1/2024.findings-naacl.168\",\n    pages = \"2641--2652\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.168.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.168/",
        "pdf_size": 554161,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15425849579585278004&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology",
        "aff_domain": "connect.ust.hk;ust.hk;ust.hk",
        "email": "connect.ust.hk;ust.hk;ust.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-short.31",
        "title": "Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Zero-shot text rankers powered by recent LLMs achieve remarkable ranking performance by simply prompting. Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like \u201cYes\u201d and \u201cNo\u201d. However, the lack of intermediate relevance label options may cause the LLM to provide noisy or biased answers for documents that are partially relevant to the query. We propose to incorporate fine-grained relevance labels into the prompt for LLM rankers, enabling them to better differentiate among documents with different levels of relevance to the query and thus derive a more accurate ranking. We study two variants of the prompt template, coupled with different numbers of relevance levels. Our experiments on 8 BEIR data sets show that adding fine-grained relevance labels significantly improves the performance of LLM rankers.",
        "author": "Honglei Zhuang; Zhen Qin; Kai Hui; Junru Wu; Le Yan; Xuanhui Wang; Michael Bendersky",
        "authorids": "/h/honglei-zhuang/; /z/zhen-qin/; /k/kai-hui/; /j/junru-wu/; /l/le-yan/; /x/xuanhui-wang/; /m/michael-bendersky/",
        "bibtex": "@inproceedings{zhuang-etal-2024-beyond,\n    title = \"Beyond Yes and No: Improving Zero-Shot {LLM} Rankers via Scoring Fine-Grained Relevance Labels\",\n    author = \"Zhuang, Honglei  and\n      Qin, Zhen  and\n      Hui, Kai  and\n      Wu, Junru  and\n      Yan, Le  and\n      Wang, Xuanhui  and\n      Bendersky, Michael\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.31/\",\n    doi = \"10.18653/v1/2024.naacl-short.31\",\n    pages = \"358--370\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.31.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.31/",
        "pdf_size": 1025178,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16196210106607236647&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.17",
        "title": "Bilateral Masking with prompt for Knowledge Graph Completion",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The pre-trained language model (PLM) has achieved significant success in the field of knowledge graph completion (KGC) by effectively modeling entity and relation descriptions. In recent studies, the research in this field has been categorized into methods based on word matching and sentence matching, with the former significantly lags behind. However, there is a critical issue in word matching methods, which is that these methods fail to obtain satisfactory single embedding representations for entities.To address this issue and enhance entity representation, we propose the Bilateral Masking with prompt for Knowledge Graph Completion (BMKGC) approach.Our methodology employs prompts to narrow the distance between the predicted entity and the known entity. Additionally, the BMKGC model incorporates a bi-encoder architecture, enabling simultaneous predictions at both the head and tail. Furthermore, we propose a straightforward technique to augment positive samples, mitigating the problem of degree bias present in knowledge graphs and thereby improving the model\u2019s robustness. Experimental results conclusively demonstrate that BMKGC achieves state-of-the-art performance on the WN18RR dataset.",
        "author": "Yonghui Kong; Cunhang Fan; Yujie Chen; Shuai Zhang; Zhao Lv; Jianhua Tao",
        "authorids": "/y/yonghui-kong/; /c/cunhang-fan/; /y/yujie-chen/; /s/shuai-zhang/; /z/zhao-lv/; /j/jianhua-tao/",
        "bibtex": "@inproceedings{kong-etal-2024-bilateral,\n    title = \"Bilateral Masking with prompt for Knowledge Graph Completion\",\n    author = \"Kong, Yonghui  and\n      Fan, Cunhang  and\n      Chen, Yujie  and\n      Zhang, Shuai  and\n      Lv, Zhao  and\n      Tao, Jianhua\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.17/\",\n    doi = \"10.18653/v1/2024.findings-naacl.17\",\n    pages = \"240--249\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.17.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.17/",
        "pdf_size": 656534,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7087973321947489584&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei 230601, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei 230601, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei 230601, China; Department of Automation, Tsinghua University, China; Anhui Province Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei 230601, China; Beijing National Research Center for lnformation Science and Technology, Tsinghua University+Department of Automation, Tsinghua University, China",
        "aff_domain": "stu.ahu.edu.cn;ahu.edu.cn;stu.ahu.edu.cn;mail.tsinghua.edu.cn;ahu.edu.cn;tsinghua.edu.cn",
        "email": "stu.ahu.edu.cn;ahu.edu.cn;stu.ahu.edu.cn;mail.tsinghua.edu.cn;ahu.edu.cn;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;0;1+1",
        "aff_unique_norm": "Anhui University;Tsinghua University",
        "aff_unique_dep": "School of Computer Science and Technology;Department of Automation",
        "aff_unique_url": "http://www.ahu.edu.cn/;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "AHU;Tsinghua",
        "aff_campus_unique_index": "0;0;0;0;2",
        "aff_campus_unique": "Hefei;;Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.288",
        "title": "Biomedical Entity Representation with Graph-Augmented Multi-Objective Transformer",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Modern biomedical concept representations are mostly trained on synonymous concept names from a biomedical knowledge base, ignoring the inter-concept interactions and a concept\u2019s local neighborhood in a knowledge base graph. In this paper, we introduce Biomedical Entity Representation with a Graph-Augmented Multi-Objective Transformer (BERGAMOT), which adopts the power of pre-trained language models (LMs) and graph neural networks to capture both inter-concept and intra-concept interactions from the multilingual UMLS graph. To obtain fine-grained graph representations, we introduce two additional graph-based objectives: (i) a node-level contrastive objective and (ii) the Deep Graph Infomax (DGI) loss, which maximizes the mutual information between a local subgraph and a high-level graph summary. We apply contrastive loss on textual and graph representations to make them less sensitive to surface forms and enable intermodal knowledge exchange. BERGAMOT achieves state-of-the-art results in zero-shot entity linking without task-specific supervision on 4 of 5 languages of the Mantra corpus and on 8 of 10 languages of the XL-BEL benchmark.",
        "author": "Andrey Sakhovskiy; Natalia Semenova; Artur Kadurin; Elena Tutubalina",
        "authorids": "/a/andrey-sakhovskiy/; /n/natalia-semenova/; /a/artur-kadurin/; /e/elena-tutubalina/",
        "bibtex": "@inproceedings{sakhovskiy-etal-2024-biomedical,\n    title = \"Biomedical Entity Representation with Graph-Augmented Multi-Objective Transformer\",\n    author = \"Sakhovskiy, Andrey  and\n      Semenova, Natalia  and\n      Kadurin, Artur  and\n      Tutubalina, Elena\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.288/\",\n    doi = \"10.18653/v1/2024.findings-naacl.288\",\n    pages = \"4626--4643\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.288.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.288/",
        "pdf_size": 616167,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18117180585119037201&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Sber AI+Kazan Federal University+Skoltech; Sber AI+AIRI; AIRI; Sber AI+Kazan Federal University+ISP RAS Research Center for Trusted Artificial Intelligence+AIRI",
        "aff_domain": "gmail.com; ; ;gmail.com",
        "email": "gmail.com; ; ;gmail.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;0+3;3;0+1+4+3",
        "aff_unique_norm": "Sberbank;Kazan Federal University;Skolkovo Institute of Science and Technology;Artificial Intelligence Research Institute;Institute for System Programming of the Russian Academy of Sciences",
        "aff_unique_dep": "Sber AI;;;;Research Center for Trusted Artificial Intelligence",
        "aff_unique_url": "https://www.sberbank.ru/en;http://kpfu.ru;https://www.skoltech.ru;https://www.airi.jp;http://www.ispras.ru",
        "aff_unique_abbr": "Sber;KFU;Skoltech;AIRI;ISP RAS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+1;1;0+0+0+1",
        "aff_country_unique": "Russian Federation;Japan"
    },
    {
        "id": "2024.naacl-long.28",
        "title": "BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Several large-scale datasets (e.g., WikiSQL, Spider) for developing natural language interfaces to databases have recently been proposed. These datasets cover a wide breadth of domains but fall short on some essential domains, such as finance and accounting. Given that accounting databases are used worldwide, particularly by non-technical people, there is an imminent need to develop models that could help extract information from accounting databases via natural language queries. In this resource paper, we aim to fill this gap by proposing a new large-scale Text-to-SQL dataset for the accounting and financial domain: BookSQL. The dataset consists of 100k natural language queries-SQL pairs, and accounting databases of 1 million records. We experiment with and analyze existing state-of-the-art models (including GPT-4) for the Text-to-SQL task on BookSQL. We find significant performance gaps, thus pointing towards developing more focused models for this domain.",
        "author": "Rahul Kumar; Amar Raja Dibbu; Shrutendra Harsola; Vignesh Subrahmaniam; Ashutosh Modi",
        "authorids": "/r/rahul-kumar/; /a/amar-raja-dibbu/; /s/shrutendra-harsola/; /v/vignesh-subrahmaniam/; /a/ashutosh-modi/",
        "bibtex": "@inproceedings{kumar-etal-2024-booksql,\n    title = \"{B}ook{SQL}: A Large Scale Text-to-{SQL} Dataset for Accounting Domain\",\n    author = \"Kumar, Rahul  and\n      Dibbu, Amar Raja  and\n      Harsola, Shrutendra  and\n      Subrahmaniam, Vignesh  and\n      Modi, Ashutosh\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.28/\",\n    doi = \"10.18653/v1/2024.naacl-long.28\",\n    pages = \"497--516\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.28.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.28/",
        "pdf_size": 763512,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4308038109286529838&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Indian Institute of Technology Kanpur (IIT Kanpur); Indian Institute of Technology Kanpur (IIT Kanpur); Intuit; Intuit; Indian Institute of Technology Kanpur (IIT Kanpur)",
        "aff_domain": "iitk.ac.in;iitk.ac.in;intuit.com;intuit.com;cse.iitk.ac.in",
        "email": "iitk.ac.in;iitk.ac.in;intuit.com;intuit.com;cse.iitk.ac.in",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0",
        "aff_unique_norm": "Indian Institute of Technology Kanpur;Intuit Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitk.ac.in;https://www.intuit.com/",
        "aff_unique_abbr": "IIT Kanpur;Intuit",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Kanpur;",
        "aff_country_unique_index": "0;0;1;1;0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2024.findings-naacl.201",
        "title": "BotChat: Evaluating LLMs\u2019 Capabilities of Having Multi-Turn Dialogues",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In the realm of modern Large Language Models (LLMs), facilitating high-quality, multi-turn dialogues with humans represents a cornerstone feature. However, human-based evaluation of such a capability involves substantial manual effort. This study offers a formative assessment of current LLMs\u2019 proficiency in emulating human-like, multi-turn conversations using an LLM-centric approach. The evaluation encompasses three key elements in the evaluation pipeline: utterance generation, evaluation protocol, and judgement, and we delve deeply into each aspect. GPT-4, both as an utterance generator and as a judge, exhibits exceptional performance. As a generator, GPT-4 crafts dialogues indistinguishable from human interactions in terms of style and flow. When judging, it shows a heightened alignment with human evaluative standards and consistency. Conversely, other LLMs face challenges in producing quality multi-turn dialogues, hindered by inadequate instruction-following abilities, a propensity for prolix utterances, and overall limited capabilities. Notably, generating extensive dialogues (e.g., spanning tens of turns) remains a formidable task for most LLMs, particularly in Chinese contexts. We hope that our work can serve as a valuable resource for evaluating the multi-turn chatting capabilities of LLMs. Related resources are available at https://github.com/open-compass/BotChat.",
        "author": "Haodong Duan; Jueqi Wei; Chonghua Wang; Hongwei Liu; Yixiao Fang; Songyang Zhang; Dahua Lin; Kai Chen",
        "authorids": "/h/haodong-duan/; /j/jueqi-wei/; /c/chonghua-wang/; /h/hongwei-liu/; /y/yixiao-fang/; /s/songyang-zhang/; /d/dahua-lin/; /k/kai-chen/",
        "bibtex": "@inproceedings{duan-etal-2024-botchat,\n    title = \"{B}ot{C}hat: Evaluating {LLM}s' Capabilities of Having Multi-Turn Dialogues\",\n    author = \"Duan, Haodong  and\n      Wei, Jueqi  and\n      Wang, Chonghua  and\n      Liu, Hongwei  and\n      Fang, Yixiao  and\n      Zhang, Songyang  and\n      Lin, Dahua  and\n      Chen, Kai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.201/\",\n    doi = \"10.18653/v1/2024.findings-naacl.201\",\n    pages = \"3184--3200\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.201.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.201/",
        "pdf_size": 1787385,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1552036059033930102&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Shanghai AI Laboratory\u2020\u2217; Shanghai AI Laboratory\u2020\u2217; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory; Shanghai AI Laboratory\u2021",
        "aff_domain": "pjlab.org.cn; ; ; ; ; ; ; ",
        "email": "pjlab.org.cn; ; ; ; ; ; ; ",
        "github": "https://github.com/open-compass/BotChat",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Shanghai AI Laboratory",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.shanghai-ai-lab.com",
        "aff_unique_abbr": "SAIL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.462",
        "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria. However, their performance can fall short, due to the model\u2019s lack of coherence and inability to plan and decompose the problem. We propose Branch-Solve-Merge (BSM), a Large Language Model program (Schlag et al., 2023) for tackling such challenging natural language tasks. It consists of branch, solve, and merge modules that are parameterized with specific prompts to the base LLM. These three modules plan a decomposition of the task into multiple parallel sub-tasks, independently solve them, and fuse the solutions to the sub-tasks. We apply our method to the tasks of LLM response evaluation and constrained text generation and evaluate its effectiveness with multiple LLMs, including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to 26%, reducing length and pairwise position biases by up to 50%, and allowing LLaMA-2-chat to match or outperform GPT-4 on most domains. On a constraint story generation task, BSM improves the coherence of stories while also improving constraint satisfaction by 12%.",
        "author": "Swarnadeep Saha; Omer Levy; Asli Celikyilmaz; Mohit Bansal; Jason Weston; Xian Li",
        "authorids": "/s/swarnadeep-saha/; /o/omer-levy/; /a/asli-celikyilmaz/; /m/mohit-bansal/; /j/jason-weston/; /x/xian-li/",
        "bibtex": "@inproceedings{saha-etal-2024-branch,\n    title = \"Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\",\n    author = \"Saha, Swarnadeep  and\n      Levy, Omer  and\n      Celikyilmaz, Asli  and\n      Bansal, Mohit  and\n      Weston, Jason  and\n      Li, Xian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.462/\",\n    doi = \"10.18653/v1/2024.naacl-long.462\",\n    pages = \"8352--8370\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.462.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.462/",
        "pdf_size": 701629,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16166599287353254814&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "UNC Chapel Hill+Meta; Meta; Meta; UNC Chapel Hill; Meta; Meta",
        "aff_domain": "unc.edu;meta.com;meta.com;unc.edu;meta.com;meta.com",
        "email": "unc.edu;meta.com;meta.com;unc.edu;meta.com;meta.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;0;1;1",
        "aff_unique_norm": "University of North Carolina at Chapel Hill;Meta",
        "aff_unique_dep": ";Meta Platforms, Inc.",
        "aff_unique_url": "https://www.unc.edu;https://meta.com",
        "aff_unique_abbr": "UNC;Meta",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chapel Hill;",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.75",
        "title": "Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Large language models hold significant promise in multilingual applications. However, inherent biases stemming from predominantly English-centric pre-training have led to the widespread practice of pre-translation, i.e., translating non-English inputs to English before inference, leading to complexity and information loss. This study re-evaluates the need for pre-translation in the context of PaLM2 models, which have been established as highly performant in multilingual tasks. We offer a comprehensive investigation across 108 languages and 6 diverse benchmarks, including open-end generative tasks, which were excluded from previous similar studies. Our findings challenge the pre-translation paradigm established in prior research, highlighting the advantages of direct inference in PaLM2. Specifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108 languages. These findings pave the way for more efficient and effective multilingual applications, alleviating the limitations associated with pre-translation and unlocking linguistic authenticity.",
        "author": "Yotam Intrator; Matan Halfon; Roman Goldenberg; Reut Tsarfaty; Matan Eyal; Ehud Rivlin; Yossi Matias; Natalia Aizenberg",
        "authorids": "/y/yotam-intrator/; /m/matan-halfon/; /r/roman-goldenberg/; /r/reut-tsarfaty/; /m/matan-eyal/; /e/ehud-rivlin/; /y/yossi-matias/; /n/natalia-aizenberg/",
        "bibtex": "@inproceedings{intrator-etal-2024-breaking,\n    title = \"Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual {LLM} Applications?\",\n    author = \"Intrator, Yotam  and\n      Halfon, Matan  and\n      Goldenberg, Roman  and\n      Tsarfaty, Reut  and\n      Eyal, Matan  and\n      Rivlin, Ehud  and\n      Matias, Yossi  and\n      Aizenberg, Natalia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.75/\",\n    doi = \"10.18653/v1/2024.naacl-short.75\",\n    pages = \"829--844\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.75.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.75/",
        "pdf_size": 1100965,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10604458636617115066&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Verily AI; Google Research; Google Research; Google Research; Google Research; Verily AI; Google Research; Verily AI",
        "aff_domain": "google.com;google.com; ;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com; ;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;0;1;0",
        "aff_unique_norm": "Verily;Google",
        "aff_unique_dep": "Verily AI;Google Research",
        "aff_unique_url": "https://www.verily.com;https://research.google",
        "aff_unique_abbr": "Verily;Google Research",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.395",
        "title": "Bridging the Gap between Different Vocabularies for LLM Ensemble",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Ensembling different large language models (LLMs) to unleash their complementary potential and harness their individual strengths is highly valuable. Nevertheless, vocabulary discrepancies among various LLMs have constrained previous studies to either selecting or blending completely generated outputs. This limitation hinders the dynamic correction and enhancement of outputs during the generation process, resulting in a limited capacity for effective ensemble. To address this issue, we propose a novel method to Ensemble LLMs via Vocabulary Alignment (EVA). EVA bridges the lexical gap among various LLMs, enabling meticulous ensemble at each generation step. Specifically, we first learn mappings between the vocabularies of different LLMs with the assistance of overlapping tokens. Subsequently, these mappings are employed to project output distributions of LLMs into a unified space, facilitating a fine-grained ensemble. Finally, we design a filtering strategy to exclude models that generate unfaithful tokens. Experimental results on commonsense reasoning, arithmetic reasoning, machine translation, and data-to-text generation tasks demonstrate the superiority of our approach compared with individual LLMs and previous ensemble methods conducted on complete outputs. Further analyses confirm that our approach can leverage knowledge from different language models and yield consistent improvement.",
        "author": "Yangyifan Xu; Jinliang Lu; Jiajun Zhang",
        "authorids": "/y/yangyifan-xu/; /j/jinliang-lu/; /j/jiajun-zhang/",
        "bibtex": "@inproceedings{xu-etal-2024-bridging,\n    title = \"Bridging the Gap between Different Vocabularies for {LLM} Ensemble\",\n    author = \"Xu, Yangyifan  and\n      Lu, Jinliang  and\n      Zhang, Jiajun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.395/\",\n    doi = \"10.18653/v1/2024.naacl-long.395\",\n    pages = \"7140--7152\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.395.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.395/",
        "pdf_size": 1882306,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14814900580424328094&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.naacl-long.120",
        "title": "Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert\u2019s latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student\u2019s error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert\u2019s decision-making model is critical for LLMs to close the gap: responses from GPT4 with expert decisions (e.g., \u201csimplify the problem\u201d) are +76% more preferred than without. Additionally, context-sensitive decisions are critical to closing pedagogical gaps: random decisions decrease GPT4\u2019s response quality by -97% than expert decisions. Our work shows the potential of embedding expert thought processes in LLM generations to enhance their capability to bridge novice-expert knowledge gaps. Our dataset and code can be found at: https://github.com/rosewang2008/bridge.",
        "author": "Rose Wang; Qingyang Zhang; Carly Robinson; Susanna Loeb; Dorottya Demszky",
        "authorids": "/r/rose-wang/; /q/qingyang-zhang/; /c/carly-robinson/; /s/susanna-loeb/; /d/dorottya-demszky/",
        "bibtex": "@inproceedings{wang-etal-2024-bridging,\n    title = \"Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes\",\n    author = \"Wang, Rose  and\n      Zhang, Qingyang  and\n      Robinson, Carly  and\n      Loeb, Susanna  and\n      Demszky, Dorottya\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.120/\",\n    doi = \"10.18653/v1/2024.naacl-long.120\",\n    pages = \"2174--2199\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.120.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.120/",
        "pdf_size": 2320080,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13402674208138276173&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University; Stanford University",
        "aff_domain": "cs.stanford.edu; ; ;stanford.edu; ",
        "email": "cs.stanford.edu; ; ;stanford.edu; ",
        "github": "https://github.com/rosewang2008/bridge",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.57",
        "title": "Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We present BYOKG, a universal question-answering (QA) system that can operate on any knowledge graph (KG), requires no human-annotated training data, and can be ready to use within a day\u2014attributes that are out-of-scope for current KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to comprehend information present in an unseen KG through exploration\u2014starting at random nodes, inspecting the labels of adjacent nodes and edges, and combining them with their prior world knowledge. Exploration in BYOKG leverages an LLM-backed symbolic agent that generates a diverse set of query-program exemplars, which are then used to ground a retrieval-augmented reasoning procedure to synthesize programs for arbitrary questions. BYOKG is effective over both small- and large-scale graphs, showing dramatic gains in zero-shot QA accuracy of 27.89 and 59.88 F1 on GrailQA and MetaQA, respectively. We further find that performance of BYOKG reliably improves with continued exploration as well as improvements in the base LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1 on a sub-sampled zero-shot split of GrailQA. Lastly, we verify our universality claim by evaluating BYOKG on a domain-specific materials science KG and show that it improves zero-shot performance by 46.33 F1.",
        "author": "Dhruv Agarwal; Rajarshi Das; Sopan Khosla; Rashmi Gangadharaiah",
        "authorids": "/d/dhruv-agarwal/; /r/rajarshi-das/; /s/sopan-khosla/; /r/rashmi-gangadharaiah/",
        "bibtex": "@inproceedings{agarwal-etal-2024-bring,\n    title = \"Bring Your Own {KG}: Self-Supervised Program Synthesis for Zero-Shot {KGQA}\",\n    author = \"Agarwal, Dhruv  and\n      Das, Rajarshi  and\n      Khosla, Sopan  and\n      Gangadharaiah, Rashmi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.57/\",\n    doi = \"10.18653/v1/2024.findings-naacl.57\",\n    pages = \"896--919\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.57.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.57/",
        "pdf_size": 1271377,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14062899684912819207&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Massachusetts Amherst; AWS AI Labs + University of Massachusetts Amherst; AWS AI Labs + University of Massachusetts Amherst; AWS AI Labs",
        "aff_domain": "cs.umass.edu;amazon.com;amazon.com;amazon.com",
        "email": "cs.umass.edu;amazon.com;amazon.com;amazon.com",
        "github": "https://github.com/amazon-science/BYOKG-NAACL24",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+0;1+0;1",
        "aff_unique_norm": "University of Massachusetts Amherst;Amazon",
        "aff_unique_dep": ";AWS AI Labs",
        "aff_unique_url": "https://www.umass.edu;https://aws.amazon.com",
        "aff_unique_abbr": "UMass Amherst;AWS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;0+0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.12",
        "title": "Building Knowledge-Guided Lexica to Model Cultural Variation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cultural variation exists between nations (e.g., the United States vs. China), but also within regions (e.g., California vs. Texas, Los Angeles vs. San Francisco). Measuring this regional cultural variation can illuminate how and why people think and behave differently. Historically, it has been difficult to computationally model cultural variation due to a lack of training data and scalability constraints. In this work, we introduce a new research problem for the NLP community: How do we measure variation in cultural constructs across regions using language? We then provide a scalable solution: building knowledge-guided lexica to model cultural variation, encouraging future work at the intersection of NLP and cultural understanding. We also highlight modern LLMs\u2019 failure to measure cultural variation or generate culturally varied language.",
        "author": "Shreya Havaldar; Salvatore Giorgi; Sunny Rai; Thomas Talhelm; Sharath Chandra Guntuku; Lyle Ungar",
        "authorids": "/s/shreya-havaldar/; /s/salvatore-giorgi/; /s/sunny-rai/; /t/thomas-talhelm/; /s/sharath-chandra-guntuku/; /l/lyle-ungar/",
        "bibtex": "@inproceedings{havaldar-etal-2024-building,\n    title = \"Building Knowledge-Guided Lexica to Model Cultural Variation\",\n    author = \"Havaldar, Shreya  and\n      Giorgi, Salvatore  and\n      Rai, Sunny  and\n      Talhelm, Thomas  and\n      Guntuku, Sharath Chandra  and\n      Ungar, Lyle\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.12/\",\n    doi = \"10.18653/v1/2024.naacl-long.12\",\n    pages = \"211--226\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.12.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.12/",
        "pdf_size": 2632349,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6965760214840303711&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; University of Chicago; University of Pennsylvania; University of Pennsylvania",
        "aff_domain": "seas.upenn.edu; ; ; ; ;seas.upenn.edu",
        "email": "seas.upenn.edu; ; ; ; ;seas.upenn.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "University of Pennsylvania;University of Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;https://www.uchicago.edu",
        "aff_unique_abbr": "UPenn;UChicago",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.285",
        "title": "CARE: Extracting Experimental Findings From Clinical Literature",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Extracting fine-grained experimental findings from literature can provide dramatic utility for scientific applications. Prior work has developed annotation schemas and datasets for limited aspects of this problem, failing to capture the real-world complexity and nuance required. Focusing on biomedicine, this work presents CARE\u2014a new IE dataset for the task of extracting clinical findings. We develop a new annotation schema capturing fine-grained findings as n-ary relations between entities and attributes, which unifies phenomena challenging for current IE systems such as discontinuous entity spans, nested relations, variable arity n-ary relations and numeric results in a single schema. We collect extensive annotations for 700 abstracts from two sources: clinical trials and case reports. We also demonstrate the generalizability of our schema to the computer science and materials science domains. We benchmark state-of-the-art IE systems on CARE, showing that even models such as GPT4 struggle. We release our resources to advance research on extracting and aggregating literature findings.",
        "author": "Aakanksha Naik; Bailey Kuehl; Erin Bransom; Doug Downey; Tom Hope",
        "authorids": "/a/aakanksha-naik/; /b/bailey-kuehl/; /e/erin-bransom/; /d/doug-downey/; /t/tom-hope/",
        "bibtex": "@inproceedings{naik-etal-2024-care,\n    title = \"{CARE}: Extracting Experimental Findings From Clinical Literature\",\n    author = \"Naik, Aakanksha  and\n      Kuehl, Bailey  and\n      Bransom, Erin  and\n      Downey, Doug  and\n      Hope, Tom\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.285/\",\n    doi = \"10.18653/v1/2024.findings-naacl.285\",\n    pages = \"4580--4596\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.285.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.285/",
        "pdf_size": 492877,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2151286700647832681&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Allen Institute of AI, USA; Allen Institute of AI, USA; Allen Institute of AI, USA; Allen Institute of AI, USA + The Hebrew University of Jerusalem; Allen Institute of AI, USA + The Hebrew University of Jerusalem",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/allenai/CARE",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0+1;0+1",
        "aff_unique_norm": "Allen Institute of AI;Hebrew University of Jerusalem",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://allenai.org;https://www.huji.ac.il",
        "aff_unique_abbr": "AI2;HUJI",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1;0+1",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "2024.naacl-long.296",
        "title": "CASA: Causality-driven Argument Sufficiency Assessment",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion.To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the definition of probability of sufficiency (PS) in the causal literature, we proposeCASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion and revise them by injecting the premise event.Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance application, and find that suggestions generated by CASA enhance the sufficiency of student-written arguments. Code and data are available at https://github.com/xxxiaol/CASA.",
        "author": "Xiao Liu; Yansong Feng; Kai-Wei Chang",
        "authorids": "/x/xiao-liu/; /y/yansong-feng/; /k/kai-wei-chang/",
        "bibtex": "@inproceedings{liu-etal-2024-casa,\n    title = \"{CASA}: Causality-driven Argument Sufficiency Assessment\",\n    author = \"Liu, Xiao  and\n      Feng, Yansong  and\n      Chang, Kai-Wei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.296/\",\n    doi = \"10.18653/v1/2024.naacl-long.296\",\n    pages = \"5282--5302\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.296.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.296/",
        "pdf_size": 1163528,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14875211044470242795&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Wangxuan Institute of Computer Technology, Peking University; Wangxuan Institute of Computer Technology, Peking University; Computer Science Department, University of California, Los Angeles",
        "aff_domain": "pku.edu.cn;pku.edu.cn;cs.ucla.edu",
        "email": "pku.edu.cn;pku.edu.cn;cs.ucla.edu",
        "github": "https://github.com/xxxiaol/CASA",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Peking University;University of California, Los Angeles",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;Computer Science Department",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.ucla.edu",
        "aff_unique_abbr": "PKU;UCLA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.406",
        "title": "CCSum: A Large-Scale and High-Quality Dataset for Abstractive News Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Training a supervised news summarization model requires large amounts of high-quality training data consisting of news articles paired with reference summaries. However, obtaining such data is costly, and existing datasets contain considerable amount of noise. We present a new large-scale and high-quality dataset for supervised abstractive news summarization containing 1.3 million training samples, which we call CCSum. In creating this dataset, we take advantage of the journalistic inverted-pyramid style in news writing: In some articles, the first sentence can be considered a summary of the reported story. Accordingly, among 35 million CommonCrawl News articles, we identify pairs of articles about the same news story and use one article\u2019s first sentence as the summary for the other article. To ensure high quality, we apply strict filters whose parameters we optimize using Bayesian optimization. We show that the resulting dataset is more factual and informative than established summarization datasets; less than 1% of the summaries have major factual inconsistencies with the corresponding news articles, compared to 5.5% to 15.4% in existing datasets, according to our human evaluation. Summarization models trained on our dataset are more favored compared to those trained on CNN/Daily Mail. The proposed dataset can open new opportunities for future research in abstractive summarization.",
        "author": "Xiang Jiang; Markus Dreyer",
        "authorids": "/x/xiang-jiang/; /m/markus-dreyer/",
        "bibtex": "@inproceedings{jiang-dreyer-2024-ccsum,\n    title = \"{CCS}um: A Large-Scale and High-Quality Dataset for Abstractive News Summarization\",\n    author = \"Jiang, Xiang  and\n      Dreyer, Markus\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.406/\",\n    doi = \"10.18653/v1/2024.naacl-long.406\",\n    pages = \"7306--7336\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.406.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.406/",
        "pdf_size": 7321710,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17294777717930944026&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Amazon; Amazon",
        "aff_domain": "amazon.com;amazon.com",
        "email": "amazon.com;amazon.com",
        "github": "https://github.com/amazon-science/ccsum",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon.com, Inc.",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.16",
        "title": "CELI: Simple yet Effective Approach to Enhance Out-of-Domain Generalization of Cross-Encoders.",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "In text ranking, it is generally believed that the cross-encoders already gather sufficient token interaction information via the attention mechanism in the hidden layers. However, our results show that the cross-encoders can consistently benefit from additional token interaction in the similarity computation at the last layer. We introduce CELI (Cross-Encoder with Late Interaction), which incorporates a late interaction layer into the current cross-encoder models. This simple method brings 5% improvement on BEIR without compromising in-domain effectiveness or search latency. Extensive experiments show that this finding is consistent across different sizes of the cross-encoder models and the first-stage retrievers. Our findings suggest that boiling all information into the [CLS] token is a suboptimal use for cross-encoders, and advocate further studies to investigate its relevance score mechanism.",
        "author": "Crystina Zhang; Minghan Li; Jimmy Lin",
        "authorids": "/c/crystina-zhang/; /m/minghan-li/; /j/jimmy-lin/",
        "bibtex": "@inproceedings{zhang-etal-2024-celi,\n    title = \"{CELI}: Simple yet Effective Approach to Enhance Out-of-Domain Generalization of Cross-Encoders.\",\n    author = \"Zhang, Crystina  and\n      Li, Minghan  and\n      Lin, Jimmy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.16/\",\n    doi = \"10.18653/v1/2024.naacl-short.16\",\n    pages = \"188--196\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.16.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.16/",
        "pdf_size": 262463,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9938809401850687128&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "David R. Cheriton School of Computer Science, University of Waterloo, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Canada",
        "aff_domain": "uwaterloo.ca;uwaterloo.ca;uwaterloo.ca",
        "email": "uwaterloo.ca;uwaterloo.ca;uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Waterloo",
        "aff_unique_dep": "David R. Cheriton School of Computer Science",
        "aff_unique_url": "https://uwaterloo.ca",
        "aff_unique_abbr": "UWaterloo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-long.409",
        "title": "CERET: Cost-Effective Extrinsic Refinement for Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) are powerful models for generation tasks, but they may not generate good quality outputs in their first attempt. Apart from model fine-tuning, existing approaches to improve prediction accuracy and quality typically involve LLM self-improvement / self-reflection that incorporate feedback from models themselves. Despite their effectiveness, these methods are hindered by their high computational cost and lack of scalability. In this work, we propose CERET, a method for refining text generations by considering semantic stability, entailment and inter-sample uncertainty measures. Experimental results show that CERET outperforms Self-consistency and Self-rerank baselines consistently under various task setups, by 1.6% in Rouge-1 for abstractive summarization and 3.5% in hit rate for question answering. Compared to LLM Self-rerank method, our approach only requires 9.4% of its latency and is more cost-effective.",
        "author": "Jason Cai; Hang Su; Monica Sunkara; Igor Shalyminov; Saab Mansour",
        "authorids": "/j/jason-cai/; /h/hang-su/; /m/monica-sunkara/; /i/igor-shalyminov/; /s/saab-mansour/",
        "bibtex": "@inproceedings{cai-etal-2024-ceret,\n    title = \"{CERET}: Cost-Effective Extrinsic Refinement for Text Generation\",\n    author = \"Cai, Jason  and\n      Su, Hang  and\n      Sunkara, Monica  and\n      Shalyminov, Igor  and\n      Mansour, Saab\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.409/\",\n    doi = \"10.18653/v1/2024.naacl-long.409\",\n    pages = \"7377--7390\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.409.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.409/",
        "pdf_size": 397378,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9144313001953549899&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "https://github.com/amazon-science/CERET-LLM-refine",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "AWS AI Labs",
        "aff_unique_url": "https://aws.amazon.com",
        "aff_unique_abbr": "AWS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.53",
        "title": "CLEAN\u2013EVAL: Clean Evaluation on Contaminated Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We are currently in an era of fierce competition among various large language models (LLMs), continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination. In this paper, we propose a novel and valuable method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs more cleanly. Clean-Eval employs a neural-based model to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter those generated low-quality samples to narrow down this candidate set. Candidates with moderate BLEURT scores against the original samples are selected as the final evaluation set. According to human assessment, this set is almost semantically equivalent to the original contamination set but expressed differently. We conduct experiments on 20 existing benchmarks across diverse tasks, and results demonstrate that Clean-Eval substantially restores the actual evaluation results on contaminated LLMs under both few-shot learning and fine-tuning scenarios.",
        "author": "Wenhong Zhu; Hongkun Hao; Zhiwei He; Yun-Ze Song; Jiao Yueyang; Yumeng Zhang; Hanxu Hu; Yiran Wei; Rui Wang; Hongyuan Lu",
        "authorids": "/w/wenhong-zhu/; /h/hongkun-hao/; /z/zhiwei-he/; /y/yun-ze-song/; /j/jiao-yueyang/; /y/yumeng-zhang/; /h/hanxu-hu/; /y/yiran-wei/; /r/rui-wang/; /h/hongyuan-lu/",
        "bibtex": "@inproceedings{zhu-etal-2024-clean,\n    title = \"{CLEAN}{--}{EVAL}: Clean Evaluation on Contaminated Large Language Models\",\n    author = \"Zhu, Wenhong  and\n      Hao, Hongkun  and\n      He, Zhiwei  and\n      Song, Yun-Ze  and\n      Yueyang, Jiao  and\n      Zhang, Yumeng  and\n      Hu, Hanxu  and\n      Wei, Yiran  and\n      Wang, Rui  and\n      Lu, Hongyuan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.53/\",\n    doi = \"10.18653/v1/2024.findings-naacl.53\",\n    pages = \"835--847\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.53.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.53/",
        "pdf_size": 2263671,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15013671216394214397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;;;",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 10
    },
    {
        "id": "2024.findings-naacl.135",
        "title": "CLGSI: A Multimodal Sentiment Analysis Framework based on Contrastive Learning Guided by Sentiment Intensity",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently, contrastive learning has begun to gain popularity in multimodal sentiment analysis (MSA). However, most of existing MSA methods based on contrastive learning lacks more detailed learning of the distribution of sample pairs with different sentiment intensity differences in the contrastive learning representation space. In addition, limited research has been conducted on the fusion of each modality representation obtained by contrastive learning training.In this paper, we propose a novel framework for multimodal sentiment analysis based on Contrastive Learning Guided by Sentiment Intensity (CLGSI). Firstly, the proposed contrastive learning guided by sentiment intensity selects positive and negative sample pairs based on the difference in sentiment intensity and assigns corresponding weights accordingly.Subsequently, we propose a new multimodal representation fusion mechanism, called Global-Local-Fine-Knowledge (GLFK), which extracts common features between different modalities\u2019 representations. At the same time, each unimodal encoder output is separately processed by a Multilayer Perceptron (MLP) to extract specific features of each modality. Finally, joint learning of the common and specific features is used to predict sentiment intensity. The effectiveness of CLGSI is assessed on two English datasets, MOSI and MOSEI, as well as one Chinese dataset, SIMS. We achieve competitive experimental results, which attest to the strong generalization performance of our approach. The code for our approach will be released in https://github.com/AZYoung233/CLGSI",
        "author": "Yang Yang; Xunde Dong; Yupeng Qiang",
        "authorids": "/y/yang-yang/; /x/xunde-dong/; /y/yupeng-qiang/",
        "bibtex": "@inproceedings{yang-etal-2024-clgsi,\n    title = \"{CLGSI}: A Multimodal Sentiment Analysis Framework based on Contrastive Learning Guided by Sentiment Intensity\",\n    author = \"Yang, Yang  and\n      Dong, Xunde  and\n      Qiang, Yupeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.135/\",\n    doi = \"10.18653/v1/2024.findings-naacl.135\",\n    pages = \"2099--2110\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.135.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.135/",
        "pdf_size": 811534,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14562923045868159105&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China",
        "aff_domain": "scut.edu.cn; ; ",
        "email": "scut.edu.cn; ; ",
        "github": "https://github.com/AZYoung233/CLGSI",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "South China University of Technology",
        "aff_unique_dep": "School of Automation Science and Engineering",
        "aff_unique_url": "https://www.scut.edu.cn",
        "aff_unique_abbr": "SCUT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Guangzhou",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.240",
        "title": "CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Neural Text-to-Speech (TTS) systems find broad applications in voice assistants, e-learning, and audiobook creation. The pursuit of modern models, like Diffusion Models (DMs), holds promise for achieving high-fidelity, real-time speech synthesis. Yet, the efficiency of multi-step sampling in Diffusion Models presents challenges. Efforts have been made to integrate GANs with DMs, speeding up inference by approximating denoising distributions, but this introduces issues with model convergence due to adversarial training. To overcome this, we introduce CM-TTS, a novel architecture grounded in consistency models (CMs). Drawing inspiration from continuous-time diffusion models, CM-TTS achieves top-quality speech synthesis in fewer steps without adversarial training or pre-trained model dependencies. We further design weighted samplers to incorporate different sampling positions into model training with dynamic probabilities, ensuring unbiased learning throughout the entire training process. We present a real-time mel-spectrogram generation consistency model, validated through comprehensive evaluations. Experimental results underscore CM-TTS\u2019s superiority over existing single-step speech synthesis systems, representing a significant advancement in the field.",
        "author": "Xiang Li; FanBu FanBu; Ambuj Mehrish; Yingting Li; Jiale Han; Bo Cheng; Soujanya Poria",
        "authorids": "/x/xiang-li/; /f/fanbu-fanbu/; /a/ambuj-mehrish/; /y/yingting-li/; /j/jiale-han/; /b/bo-cheng/; /s/soujanya-poria/",
        "bibtex": "@inproceedings{li-etal-2024-cm,\n    title = \"{CM}-{TTS}: Enhancing Real Time Text-to-Speech Synthesis Efficiency through Weighted Samplers and Consistency Models\",\n    author = \"Li, Xiang  and\n      FanBu, FanBu  and\n      Mehrish, Ambuj  and\n      Li, Yingting  and\n      Han, Jiale  and\n      Cheng, Bo  and\n      Poria, Soujanya\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.240/\",\n    doi = \"10.18653/v1/2024.findings-naacl.240\",\n    pages = \"3777--3794\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.240.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.240/",
        "pdf_size": 970518,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14200237516690561178&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; Singapore University of Technology and Design; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; Singapore University of Technology and Design",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;sutd.edu.sg;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;sutd.edu.sg",
        "email": "bupt.edu.cn;bupt.edu.cn;sutd.edu.sg;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;sutd.edu.sg",
        "github": "https://github.com/XiangLi2022/CM-TTS",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;0;0;1",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Singapore University of Technology and Design",
        "aff_unique_dep": "State Key Laboratory of Networking and Switching Technology;",
        "aff_unique_url": "http://www.bupt.edu.cn;https://www.sutd.edu.sg",
        "aff_unique_abbr": "BUPT;SUTD",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;1;0;0;0;1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2024.naacl-long.343",
        "title": "CMB: A Comprehensive Medical Benchmark in Chinese",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in contextual incongruities to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. Our data and code are publicly available at https://github.com/FreedomIntelligence/CMB.",
        "author": "Xidong Wang; Guiming Chen; Song Dingjie; Zhang Zhiyi; Zhihong Chen; Qingying Xiao; Junying Chen; Feng Jiang; Jianquan Li; Xiang Wan; Benyou Wang; Haizhou Li",
        "authorids": "/x/xidong-wang/; /g/guiming-chen/; /s/song-dingjie/; /z/zhang-zhiyi/; /z/zhihong-chen/; /q/qingying-xiao/; /j/junying-chen/; /f/feng-jiang/; /j/jianquan-li/; /x/xiang-wan/; /b/benyou-wang/; /h/haizhou-li/",
        "bibtex": "@inproceedings{wang-etal-2024-cmb,\n    title = \"{CMB}: A Comprehensive Medical Benchmark in {C}hinese\",\n    author = \"Wang, Xidong  and\n      Chen, Guiming  and\n      Dingjie, Song  and\n      Zhiyi, Zhang  and\n      Chen, Zhihong  and\n      Xiao, Qingying  and\n      Chen, Junying  and\n      Jiang, Feng  and\n      Li, Jianquan  and\n      Wan, Xiang  and\n      Wang, Benyou  and\n      Li, Haizhou\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.343/\",\n    doi = \"10.18653/v1/2024.naacl-long.343\",\n    pages = \"6184--6205\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.343.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.343/",
        "pdf_size": 1415289,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17783278235266028242&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;",
        "email": ";;;;;;;;;;;",
        "github": "https://github.com/FreedomIntelligence/CMB",
        "project": "",
        "author_num": 12
    },
    {
        "id": "2024.naacl-long.461",
        "title": "CNER: Concept and Named Entity Recognition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Named entities \u2013 typically expressed via proper nouns \u2013 play a key role in Natural Language Processing, as their identification and comprehension are crucial in tasks such as Relation Extraction, Coreference Resolution and Question Answering, among others. Tasks like these also often entail dealing with concepts \u2013 typically represented by common nouns \u2013 which, however, have not received as much attention. Indeed, the potential of their identification and understanding remains underexplored, as does the benefit of a synergistic formulation with named entities. To fill this gap, we introduce Concept and Named Entity Recognition (CNER), a new unified task that handles concepts and entities mentioned in unstructured texts seamlessly. We put forward a comprehensive set of categories that can be used to model concepts and named entities jointly, and propose new approaches for the creation of CNER datasets. We evaluate the benefits of performing CNER as a unified task extensively, showing that a CNER model gains up to +5.4 and +8 macro F1 points when compared to specialized named entity and concept recognition systems, respectively. Finally, to encourage the development of CNER systems, we release our datasets and models at https://github.com/Babelscape/cner.",
        "author": "Giuliano Martinelli; Francesco Molfese; Simone Tedeschi; Alberte Fern\u00e1ndez-Castro; Roberto Navigli",
        "authorids": "/g/giuliano-martinelli/; /f/francesco-molfese/; /s/simone-tedeschi/; /a/alberte-fernandez-castro/; /r/roberto-navigli/",
        "bibtex": "@inproceedings{martinelli-etal-2024-cner,\n    title = \"{CNER}: Concept and Named Entity Recognition\",\n    author = \"Martinelli, Giuliano  and\n      Molfese, Francesco  and\n      Tedeschi, Simone  and\n      Fern{\\'a}ndez-Castro, Alberte  and\n      Navigli, Roberto\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.461/\",\n    doi = \"10.18653/v1/2024.naacl-long.461\",\n    pages = \"8336--8351\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.461.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.461/",
        "pdf_size": 873438,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9722751157712814586&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Sapienza NLP Group, Sapienza University of Rome; Sapienza NLP Group, Sapienza University of Rome; Sapienza NLP Group, Sapienza University of Rome + Babelscape; Babelscape + Roma Tre University; Sapienza NLP Group, Sapienza University of Rome",
        "aff_domain": "diag.uniroma1.it;diag.uniroma1.it;diag.uniroma1.it;stud.uniroma3.it;diag.uniroma1.it",
        "email": "diag.uniroma1.it;diag.uniroma1.it;diag.uniroma1.it;stud.uniroma3.it;diag.uniroma1.it",
        "github": "https://github.com/Babelscape/cner",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0+1;1+2;0",
        "aff_unique_norm": "Sapienza University of Rome;Babelscape;Roma Tre University",
        "aff_unique_dep": "Sapienza NLP Group;;",
        "aff_unique_url": "https://www.uniroma1.it;https://www.babelscape.com;https://www.uniroma3.it",
        "aff_unique_abbr": "Sapienza;;Roma Tre",
        "aff_campus_unique_index": "0;0;0;;0",
        "aff_campus_unique": "Rome;",
        "aff_country_unique_index": "0;0;0+0;0+0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2024.findings-naacl.198",
        "title": "COMMIT: Code-Mixing English-Centric Large Language Model for Multilingual Instruction Tuning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently, instruction-tuned large language models (LLMs) are showing prominent performance on various tasks, such as question answering. However, the majority of instruction-tuned LLMs are English-centric, which hinders their application to low-resource language QA. In this paper, we propose COde-Mixed Multilingual Instruction Tuning (COMMIT) to adapt English-centric LLM to low-resource language QA. We point out two main causes of English-centricness: imbalance of unlabeled data, and English-centric instruction tuning datasets. To deviate from English-centric instruction tuning, we propose to specialize code-mixing for instruction tuning, which blocks code-mixing in English templates, to leverage the potential of its superiority. To overcome data imbalance, we perform cross-lingual alignment. The majority of cross-lingual alignment works focused on making representations similar, which is not desirable to decoder-based LLMs, such as LLaMA. Therefore, we propose code-mixed continual causal language modeling to align the decoder. COMMIT improves the exact match score of low-resourced language QA by up to 32x. Code is publicly available.",
        "author": "Jaeseong Lee; YeonJoon Jung; Seung-won Hwang",
        "authorids": "/j/jaeseong-lee/; /y/yeonjoon-jung/; /s/seung-won-hwang/",
        "bibtex": "@inproceedings{lee-etal-2024-commit,\n    title = \"{COMMIT}: Code-Mixing {E}nglish-Centric Large Language Model for Multilingual Instruction Tuning\",\n    author = \"Lee, Jaeseong  and\n      Jung, YeonJoon  and\n      Hwang, Seung-won\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.198/\",\n    doi = \"10.18653/v1/2024.findings-naacl.198\",\n    pages = \"3130--3137\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.198.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.198/",
        "pdf_size": 283713,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16795930543306410262&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Computer Science and Engineering; IPAI; Computer Science and Engineering+IPAI",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "Computer Science and Engineering;Institute of Parallel and Distributed Systems",
        "aff_unique_dep": "Computer Science and Engineering;",
        "aff_unique_url": ";http://ipai.hust.edu.cn/",
        "aff_unique_abbr": ";IPAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";China"
    },
    {
        "id": "2024.naacl-long.223",
        "title": "CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A wave of new task-based virtual assistants has been fueled by increasingly powerful large language models (LLMs), such as GPT-4 (OpenAI, 2023). A major challenge in deploying LLM-based virtual conversational assistants in real world settings is ensuring they operate within what is admissible for the task. To overcome this challenge, the designers of these virtual assistants rely on an independent guardrail system that verifies the virtual assistant\u2019s output aligns with the constraints required for the task. However, relying on commonly used, prompt-based guardrails can be difficult to engineer correctly and comprehensively. To address these challenges, we propose CONSCENDI. We use CONSCENDI to exhaustively generate training data with two key LLM-powered components: scenario-augmented generation and contrastive training examples. When generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set and provides chatbot designers greater control. To generate contrastive examples, we prompt the LLM to alter conversations with violations into acceptable conversations to enable fine-grained distinctions. We then use this data, generated by CONSCENDI, to train a smaller model. We find that CONSCENDI results in guardrail models that improve over baselines in multiple dialogue domains.",
        "author": "Albert Sun; Varun Nair; Elliot Schumacher; Anitha Kannan",
        "authorids": "/a/albert-sun/; /v/varun-nair/; /e/elliot-schumacher/; /a/anitha-kannan/",
        "bibtex": "@inproceedings{sun-etal-2024-conscendi,\n    title = \"{CONSCENDI}: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants\",\n    author = \"Sun, Albert  and\n      Nair, Varun  and\n      Schumacher, Elliot  and\n      Kannan, Anitha\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.223/\",\n    doi = \"10.18653/v1/2024.naacl-long.223\",\n    pages = \"4009--4030\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.223.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.223/",
        "pdf_size": 687555,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17161056811890693813&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Curai Health*; DynamoFL; Curai Health; Curai Health",
        "aff_domain": "gmail.com; ; ; ",
        "email": "gmail.com; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Curai Health;DynamoFL",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.curai.com;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2024.naacl-long.77",
        "title": "COPAL-ID: Indonesian Language Reasoning with Local Culture and Nuances",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present COPAL-ID, a novel, public Indonesian language common sense reasoning dataset. Unlike the previous Indonesian COPA dataset (XCOPA-ID), COPAL-ID incorporates Indonesian local and cultural nuances, and therefore, provides a more natural portrayal of day-to-day causal reasoning within the Indonesian cultural sphere. Professionally written by natives from scratch, COPAL-ID is more fluent and free from awkward phrases, unlike the translated XCOPA-ID. In addition, we present COPALID in both standard Indonesian and in Jakartan Indonesian\u2013a dialect commonly used in daily conversation. COPAL-ID poses a greater challenge for existing open-sourced and closedstate-of-the-art multilingual language models, yet is trivially easy for humans. Our findings suggest that general multilingual models struggle to perform well, achieving 66.91% accuracy on COPAL-ID. South-East Asian-specific models achieve slightly better performance of 73.88% accuracy. Yet, this number still falls short of near-perfect human performance. This shows that these language models are still way behind in comprehending the local nuances of Indonesian.",
        "author": "Haryo Wibowo; Erland Fuadi; Made Nityasya; Radityo Eko Prasojo; Alham Aji",
        "authorids": "/h/haryo-wibowo/; /e/erland-fuadi/; /m/made-nityasya/; /r/radityo-eko-prasojo/; /a/alham-aji/",
        "bibtex": "@inproceedings{wibowo-etal-2024-copal,\n    title = \"{COPAL}-{ID}: {I}ndonesian Language Reasoning with Local Culture and Nuances\",\n    author = \"Wibowo, Haryo  and\n      Fuadi, Erland  and\n      Nityasya, Made  and\n      Prasojo, Radityo Eko  and\n      Aji, Alham\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.77/\",\n    doi = \"10.18653/v1/2024.naacl-long.77\",\n    pages = \"1404--1422\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.77.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.77/",
        "pdf_size": 356699,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4799738440097196623&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "MBZUAI; Independent Researcher; Independent Researcher; Independent Researcher; MBZUAI",
        "aff_domain": "mbzuai.ac.ae;gmail.com;gmail.com;gmail.com;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae;gmail.com;gmail.com;gmail.com;mbzuai.ac.ae",
        "github": "https://github.com/haryoa/COPAL-ID1404",
        "project": "https://huggingface.co/datasets/haryoaw/COPAL",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Mohamed bin Zayed University of Artificial Intelligence;Independent Researcher",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.mbzuai.ac.ae;",
        "aff_unique_abbr": "MBZUAI;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Arab Emirates;"
    },
    {
        "id": "2024.naacl-long.93",
        "title": "COSIGN: Contextual Facts Guided Generation for Knowledge Graph Completion",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge graph completion (KGC) aims to infer missing facts based on existing facts within a KG. Recently, research on generative models (GMs) has addressed the limitations of embedding methods in terms of generality and scalability. However, GM-based methods are sensitive to contextual facts on KG, so the contextual facts of poor quality can cause GMs to generate erroneous results. To improve the performance of GM-based methods for various KGC tasks, we propose a COntextual FactS GuIded GeneratioN (COSIGN) model. First, to enhance the inference ability of the generative model, we designed a contextual facts collector to achieve human-like retrieval behavior. Second, a contextual facts organizer is proposed to learn the organized capabilities of LLMs through knowledge distillation. Finally, the organized contextual facts as the input of the inference generator to generate missing facts. Experimental results demonstrate that COSIGN outperforms state-of-the-art baseline techniques in terms of performance.",
        "author": "Jinpeng Li; Hang Yu; Xiangfeng Luo; Qian Liu",
        "authorids": "/j/jinpeng-li/; /h/hang-yu/; /x/xiangfeng-luo/; /q/qian-liu/",
        "bibtex": "@inproceedings{li-etal-2024-cosign,\n    title = \"{COSIGN}: Contextual Facts Guided Generation for Knowledge Graph Completion\",\n    author = \"Li, Jinpeng  and\n      Yu, Hang  and\n      Luo, Xiangfeng  and\n      Liu, Qian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.93/\",\n    doi = \"10.18653/v1/2024.naacl-long.93\",\n    pages = \"1669--1682\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.93.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.93/",
        "pdf_size": 1415040,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18243969738150658145&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Shanghai University, China; Shanghai University, China; Shanghai University, China; School of Computer Science, The University of Auckland, New Zealand",
        "aff_domain": "shu.edu.cn;shu.edu.cn;shu.edu.cn;auckland.ac.nz",
        "email": "shu.edu.cn;shu.edu.cn;shu.edu.cn;auckland.ac.nz",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Shanghai University;University of Auckland",
        "aff_unique_dep": ";School of Computer Science",
        "aff_unique_url": "https://www.shu.edu.cn;https://www.auckland.ac.nz",
        "aff_unique_abbr": "SHU;UoA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Auckland",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;New Zealand"
    },
    {
        "id": "2024.naacl-short.52",
        "title": "CPopQA: Ranking Cultural Concept Popularity by LLMs",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Many recent studies examining the knowledge capacity of large language models (LLM) have focused on knowledge explicitly learned from the pretraining data or implicitly inferable from similar contexts. However, the extent to which an LLM effectively captures corpus-level statistical trends of concepts for reasoning, especially long-tail ones, is largely underexplored. In this study, we introduce a novel few-shot question-answering task (CPopQA) that examines LLMs\u2019 statistical ranking abilities for long-tail cultural concepts (e.g., holidays), particularly focusing on these concepts\u2019 popularity in the United States and the United Kingdom, respectively. We curate a dataset of 457 holidays across 58 countries, generating a total of 9,000 QA testing pairs. Experiments on four strong LLMs show that open-sourced LLMs still lag way behind close LLM API (e.g., GPT-3.5) in statistical ranking of cultural concepts. Notably, GPT-3.5 exhibited its potential to identify geo-cultural proximity across continents.",
        "author": "Ming Jiang; Mansi Joshi",
        "authorids": "/m/ming-jiang/; /m/mansi-joshi/",
        "bibtex": "@inproceedings{jiang-joshi-2024-cpopqa,\n    title = \"{CP}op{QA}: Ranking Cultural Concept Popularity by {LLM}s\",\n    author = \"Jiang, Ming  and\n      Joshi, Mansi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.52/\",\n    doi = \"10.18653/v1/2024.naacl-short.52\",\n    pages = \"615--630\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.52.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.52/",
        "pdf_size": 3117527,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3958936118347507508&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Human-Centered Computing, Indiana University, Indianapolis, IN, USA; Department of Human-Centered Computing, Indiana University, Indianapolis, IN, USA",
        "aff_domain": "iu.edu;iu.edu",
        "email": "iu.edu;iu.edu",
        "github": "https://github.com/SeleenaJM/CPopQA.git",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indiana University",
        "aff_unique_dep": "Department of Human-Centered Computing",
        "aff_unique_url": "https://www.indiana.edu",
        "aff_unique_abbr": "IU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Indianapolis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.219",
        "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.",
        "author": "Garima Agrawal; Tharindu Kumarage; Zeyad Alghamdi; Huan Liu",
        "authorids": "/g/garima-agrawal/; /t/tharindu-kumarage/; /z/zeyad-alghamdi/; /h/huan-liu/",
        "bibtex": "@inproceedings{agrawal-etal-2024-knowledge,\n    title = \"Can Knowledge Graphs Reduce Hallucinations in {LLM}s? : A Survey\",\n    author = \"Agrawal, Garima  and\n      Kumarage, Tharindu  and\n      Alghamdi, Zeyad  and\n      Liu, Huan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.219/\",\n    doi = \"10.18653/v1/2024.naacl-long.219\",\n    pages = \"3947--3960\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.219.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.219/",
        "pdf_size": 835744,
        "gs_citation": 125,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2488567603123076320&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Arizona State University; Arizona State University; Arizona State University; Arizona State University",
        "aff_domain": "asu.edu;asu.edu;asu.edu;asu.edu",
        "email": "asu.edu;asu.edu;asu.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.24",
        "title": "Can LLM\u2019s Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We present a novel approach to automatically synthesize \u201cwayfinding instructions\u201d for an embodied robot agent. In contrast to prior approaches that are heavily reliant on human-annotated datasets designed exclusively for specific simulation platforms, our algorithm uses in-context learning to condition an LLM to generate instructions using just a few references. Using an LLM-based Visual Question Answering strategy, we gather detailed information about the environment which is used by the LLM for instruction synthesis. We implement our approach on multiple simulation platforms including Matterport3D, AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature. We subjectively evaluate our approach via a user study and observe that 83.3% of users find the synthesized instructions accurately capture the details of the environment and show characteristics similar to those of human-generated instructions. Further, we conduct zero-shot navigation with multiple approaches on the REVERIE dataset using the generated instructions, and observe very close correlation with the baseline on standard success metrics (< 1% change in SR), quantifying the viability of generated instructions in replacing human-annotated data. We finally discuss the applicability of our approach in enabling a generalizable evaluation of embodied navigation policies. To the best of our knowledge, ours is the first LLM-driven approach capable of generating \u201chuman-like\u201d instructions in a platform-agnostic manner, without training.",
        "author": "Vishnu Sashank Dorbala; Sanjoy Chowdhury; Dinesh Manocha",
        "authorids": "/v/vishnu-sashank-dorbala/; /s/sanjoy-chowdhury/; /d/dinesh-manocha/",
        "bibtex": "@inproceedings{dorbala-etal-2024-llms,\n    title = \"Can {LLM}{'}s Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic Embodied Instruction Synthesis\",\n    author = \"Dorbala, Vishnu Sashank  and\n      Chowdhury, Sanjoy  and\n      Manocha, Dinesh\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.24/\",\n    doi = \"10.18653/v1/2024.naacl-short.24\",\n    pages = \"258--271\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.24.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.24/",
        "pdf_size": 1777813,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13737740725516216982&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.naacl-long.415",
        "title": "Can Language Model Moderators Improve the Health of Online Discourse?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Conversational moderation of online communities is crucial to maintaining civility for a constructive environment, but it is challenging to scale and harmful to moderators. The inclusion of sophisticated natural language generation modules as a force multiplier to aid human moderators is a tantalizing prospect, but adequate evaluation approaches have so far been elusive. In this paper, we establish a systematic definition of conversational moderation effectiveness grounded on moderation literature and establish design criteria for conducting realistic yet safe evaluation. We then propose a comprehensive evaluation framework to assess models\u2019 moderation capabilities independently of human intervention. With our framework, we conduct the first known study of language models as conversational moderators, finding that appropriately prompted models that incorporate insights from social science can provide specific and fair feedback on toxic behavior but struggle to influence users to increase their levels of respect and cooperation.",
        "author": "Hyundong Cho; Shuai Liu; Taiwei Shi; Darpan Jain; Basem Rizk; Yuyang Huang; Zixun Lu; Nuan Wen; Jonathan Gratch; Emilio Ferrara; Jonathan May",
        "authorids": "/h/hyundong-cho/; /s/shuai-liu/; /t/taiwei-shi/; /d/darpan-jain/; /b/basem-rizk/; /y/yuyang-huang/; /z/zixun-lu/; /n/nuan-wen/; /j/jonathan-gratch/; /e/emilio-ferrara/; /j/jonathan-may/",
        "bibtex": "@inproceedings{cho-etal-2024-language,\n    title = \"Can Language Model Moderators Improve the Health of Online Discourse?\",\n    author = \"Cho, Hyundong  and\n      Liu, Shuai  and\n      Shi, Taiwei  and\n      Jain, Darpan  and\n      Rizk, Basem  and\n      Huang, Yuyang  and\n      Lu, Zixun  and\n      Wen, Nuan  and\n      Gratch, Jonathan  and\n      Ferrara, Emilio  and\n      May, Jonathan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.415/\",\n    doi = \"10.18653/v1/2024.naacl-long.415\",\n    pages = \"7478--7496\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.415.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.415/",
        "pdf_size": 2163457,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1751489714084861767&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Information Sciences Institute, University of Southern California; Department of Computer Science and Information Sciences Institute, University of Southern California; Department of Computer Science and Information Sciences Institute, University of Southern California; Department of Computer Science and Information Sciences Institute, University of Southern California; Department of Computer Science and Information Sciences Institute, University of Southern California; Department of Computer Science and Information Sciences Institute, University of Southern California; Department of Computer Science and Information Sciences Institute, University of Southern California; Department of Computer Science and Information Sciences Institute, University of Southern California; Department of Computer Science and Information Sciences Institute, University of Southern California; Department of Computer Science and Information Sciences Institute, University of Southern California; Department of Computer Science and Information Sciences Institute, University of Southern California",
        "aff_domain": "gmail.com; ; ; ; ; ; ; ; ; ; ",
        "email": "gmail.com; ; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://howtobuildup.org/programs/digital-conflict/the-commons-project/",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "Department of Computer Science and Information Sciences Institute",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.59",
        "title": "Can Public Large Language Models Help Private Cross-device Federated Learning?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We study (differentially) private federated learning (FL) of language models. The language models in cross-device FL are relatively small, which can be trained with meaningful formal user-level differential privacy (DP) guarantees when massive parallelism in training is enabled by the participation of a moderate size of users. Recently, public data has been used to improve privacy-utility trade-offs for both large and small language models. In this work, we provide a systematic study of using large-scale public data and LLMs to help differentially private training of on-device FL models, and further improve the privacy-utility tradeoff by techniques of distillation. Moreover, we propose a novel distribution matching algorithm with theoretical grounding to sample public data close to private data distribution, which significantly improves the sample efficiency of (pre-)training on public data. The proposed method is efficient and effective for training private models by taking advantage of public data, especially for customized on-device architectures that do not have ready-touse pre-trained models.",
        "author": "Boxin Wang; Yibo Zhang; Yuan Cao; Bo Li; Hugh McMahan; Sewoong Oh; Zheng Xu; Manzil Zaheer",
        "authorids": "/b/boxin-wang/; /y/yibo-zhang/; /y/yuan-cao/; /b/bo-li/; /h/hugh-mcmahan/; /s/sewoong-oh/; /z/zheng-xu/; /m/manzil-zaheer/",
        "bibtex": "@inproceedings{wang-etal-2024-public,\n    title = \"Can Public Large Language Models Help Private Cross-device Federated Learning?\",\n    author = \"Wang, Boxin  and\n      Zhang, Yibo  and\n      Cao, Yuan  and\n      Li, Bo  and\n      McMahan, Hugh  and\n      Oh, Sewoong  and\n      Xu, Zheng  and\n      Zaheer, Manzil\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.59/\",\n    doi = \"10.18653/v1/2024.findings-naacl.59\",\n    pages = \"934--949\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.59.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.59/",
        "pdf_size": 1567775,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3261854723067070305&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Google Research; Google Deepmind; UIUC; Stanford; Google Research; Google Research; Google Research; Google Deepmind",
        "aff_domain": "illinois.edu; ; ; ; ; ;google.com; ",
        "email": "illinois.edu; ; ; ; ; ;google.com; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;3;0;0;0;1",
        "aff_unique_norm": "Google;DeepMind;University of Illinois Urbana-Champaign;Stanford University",
        "aff_unique_dep": "Google Research;DeepMind;;",
        "aff_unique_url": "https://research.google;https://deepmind.com;https://www illinois.edu;https://www.stanford.edu",
        "aff_unique_abbr": "Google Research;DeepMind;UIUC;Stanford",
        "aff_campus_unique_index": "0;2;3;0;0;0",
        "aff_campus_unique": "Mountain View;;Urbana-Champaign;Stanford",
        "aff_country_unique_index": "0;1;0;0;0;0;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2024.naacl-long.407",
        "title": "Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Supervised classification heavily depends on datasets annotated by humans. However, in subjective tasks such as toxicity classification, these annotations often exhibit low agreement among raters. Annotations have commonly been aggregated by employing methods like majority voting to determine a single ground truth label. In subjective tasks, aggregating labels will result in biased labeling and, consequently, biased models that can overlook minority opinions. Previous studies have shed light on the pitfalls of label aggregation and have introduced a handful of practical approaches to tackle this issue. Recently proposed multi-annotator models, which predict labels individually per annotator, are vulnerable to under-determination for annotators with few samples. This problem is exacerbated in crowdsourced datasets. In this work, we propose Annotator Aware Representations for Texts (AART) for subjective classification tasks. Our approach involves learning representations of annotators, allowing for exploration of annotation behaviors. We show the improvement of our method on metrics that assess the performance on capturing individual annotators\u2019 perspectives. Additionally, we demonstrate fairness metrics to evaluate our model\u2019s equability of performance for marginalized annotators compared to others.",
        "author": "Negar Mokhberian; Myrl Marmarelis; Frederic Hopp; Valerio Basile; Fred Morstatter; Kristina Lerman",
        "authorids": "/n/negar-mokhberian/; /m/myrl-marmarelis/; /f/frederic-hopp/; /v/valerio-basile/; /f/fred-morstatter/; /k/kristina-lerman/",
        "bibtex": "@inproceedings{mokhberian-etal-2024-capturing,\n    title = \"Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks\",\n    author = \"Mokhberian, Negar  and\n      Marmarelis, Myrl  and\n      Hopp, Frederic  and\n      Basile, Valerio  and\n      Morstatter, Fred  and\n      Lerman, Kristina\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.407/\",\n    doi = \"10.18653/v1/2024.naacl-long.407\",\n    pages = \"7337--7349\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.407.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.407/",
        "pdf_size": 1482916,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13921311932625017357&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Information Sciences Institute, University of Southern California; Information Sciences Institute, University of Southern California; School of Communication Research, University of Amsterdam; Computer Science Department, University of Turin; Information Sciences Institute, University of Southern California; Information Sciences Institute, University of Southern California",
        "aff_domain": "isi.edu;isi.edu;uva.nl;unito.it;isi.edu;isi.edu",
        "email": "isi.edu;isi.edu;uva.nl;unito.it;isi.edu;isi.edu",
        "github": "https://github.com/negar-mokhberian/aart",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;0;0",
        "aff_unique_norm": "University of Southern California;University of Amsterdam;University of Turin",
        "aff_unique_dep": "Information Sciences Institute;School of Communication Research;Computer Science Department",
        "aff_unique_url": "https://www.usc.edu;https://www.uva.nl;https://www.unito.it",
        "aff_unique_abbr": "USC;UvA;UniTO",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;1;2;0;0",
        "aff_country_unique": "United States;Netherlands;Italy"
    },
    {
        "id": "2024.naacl-long.302",
        "title": "Carpe diem: On the Evaluation of World Knowledge in Lifelong Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The dynamic nature of knowledge in an ever-changing world presents challenges for language models trained on static data; the model in the real world often requires not only acquiring new knowledge but also overwriting outdated information into updated ones. To study the ability of language models for these time-dependent dynamics in human language, we introduce a novel task, EvolvingQA, a temporally evolving question-answering benchmark designed for training and evaluating LMs on an evolving Wikipedia database. The construction of EvolvingQA is automated with our pipeline using large language models. We uncover that existing continual learning baselines suffer from updating and removing outdated knowledge. Our analysis suggests that models fail to rectify knowledge due to small weight gradients. In addition, we elucidate that language models particularly struggle to reflect the change of numerical or temporal information. Our work aims to model the dynamic nature of real-world information, suggesting faithful evaluations of the evolution-adaptability of language models. Our data construction code and dataset files are available at https://github.com/kimyuji/EvolvingQA_benchmark.",
        "author": "Yujin Kim; Jaehong Yoon; Seonghyeon Ye; Sangmin Bae; Namgyu Ho; Sung Ju Hwang; Se-Young Yun",
        "authorids": "/y/yujin-kim/; /j/jaehong-yoon/; /s/seonghyeon-ye/; /s/sangmin-bae/; /n/namgyu-ho/; /s/sung-ju-hwang/; /s/se-young-yun/",
        "bibtex": "@inproceedings{kim-etal-2024-carpe,\n    title = \"Carpe diem: On the Evaluation of World Knowledge in Lifelong Language Models\",\n    author = \"Kim, Yujin  and\n      Yoon, Jaehong  and\n      Ye, Seonghyeon  and\n      Bae, Sangmin  and\n      Ho, Namgyu  and\n      Hwang, Sung Ju  and\n      Yun, Se-Young\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.302/\",\n    doi = \"10.18653/v1/2024.naacl-long.302\",\n    pages = \"5401--5415\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.302.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.302/",
        "pdf_size": 1127567,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2064181389633930062&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "KAIST AI; UNC Chapel Hill; KAIST AI; KAIST AI; KAIST AI; KAIST AI; KAIST AI",
        "aff_domain": "kaist.ac.kr; ; ; ; ;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr; ; ; ; ;kaist.ac.kr;kaist.ac.kr",
        "github": "https://github.com/kimyuji/EvolvingQA_benchmark",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0;0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;University of North Carolina at Chapel Hill",
        "aff_unique_dep": "KAIST AI;",
        "aff_unique_url": "https://www.kaist.edu;https://www.unc.edu",
        "aff_unique_abbr": "KAIST;UNC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chapel Hill",
        "aff_country_unique_index": "0;1;0;0;0;0;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2024.naacl-long.91",
        "title": "Causal Inference for Human-Language Model Collaboration",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we examine the collaborative dynamics between humansand language models (LMs), where the interactions typically involveLMs proposing text segments and humans editing or responding to theseproposals. Productive engagement with LMs in such scenarios necessitates that humans discern effective text-based interaction strategies, such as editing and response styles, from historical human-LM interactions. This objective is inherently causal, driven by the counterfactual \u2018what-if\u2019 question: how would the outcome of collaboration change if humans employed a different text editing/refinement strategy? A key challenge in answering this causal inference question is formulating an appropriate causal estimand: the conventional average treatment effect (ATE) estimand is inapplicable to text-based treatments due to their high dimensionality. To address this concern, we introduce a new causal estimand\u2013 *Incremental Stylistic Effect (ISE)*, which characterizes the average impact of infinitesimally shifting a text towards a specific style, such as increasing formality. We establish the conditions for the non-parametric identification of ISE. Building on this, we develop *CausalCollab*, an algorithm designed to estimate the ISE of various interaction strategies in dynamic human-LM collaborations. Our empirical investigations across three distinct human-LM collaboration scenarios reveal that *CausalCollab* effectively reduces confounding and significantly improves counterfactual estimation over a set of competitive baselines.",
        "author": "Bohan Zhang; Yixin Wang; Paramveer Dhillon",
        "authorids": "/b/bohan-zhang/; /y/yixin-wang/; /p/paramveer-s-dhillon/",
        "bibtex": "@inproceedings{zhang-etal-2024-causal,\n    title = \"Causal Inference for Human-Language Model Collaboration\",\n    author = \"Zhang, Bohan  and\n      Wang, Yixin  and\n      Dhillon, Paramveer\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.91/\",\n    doi = \"10.18653/v1/2024.naacl-long.91\",\n    pages = \"1630--1647\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.91.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.91/",
        "pdf_size": 1188322,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5290891173589100007&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Michigan; University of Michigan; University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.242",
        "title": "Characterizing Human and Zero-Shot GPT-3.5 Object-Similarity Judgments",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent advancements in large language models\u2019 (LLMs) capabilities have yielded few-shot, human-comparable performance on a range of tasks. At the same time, researchers expend significant effort and resources gathering human annotations. At some point, LLMs may be able to perform some simple annotation tasks, but studies of LLM annotation accuracy and behavior are sparse. In this paper, we characterize OpenAI\u2019s GPT-3.5\u2019s judgment on a behavioral task for implicit object categorization. We characterize the embedding spaces of models trained on human vs. GPT responses and give similarities and differences between them, finding many similar dimensions. We also find that despite these similar dimensions, augmenting humans\u2019 responses with GPT ones drives model divergence across the sizes of datasets tested.",
        "author": "D McKnight; Alona Fyshe",
        "authorids": "/d/d-mcknight/; /a/alona-fyshe/",
        "bibtex": "@inproceedings{mcknight-fyshe-2024-characterizing,\n    title = \"Characterizing Human and Zero-Shot {GPT}-3.5 Object-Similarity Judgments\",\n    author = \"McKnight, D  and\n      Fyshe, Alona\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.242/\",\n    doi = \"10.18653/v1/2024.findings-naacl.242\",\n    pages = \"3810--3828\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.242.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.242/",
        "pdf_size": 21873909,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ahb6AjjsjScJ:scholar.google.com/&scioq=Characterizing+Human+and+Zero-Shot+GPT-3.5+Object-Similarity+Judgments&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Dept. Computing Science, University of Alberta+Alberta Machine Intelligence Inst.; Depts. Comp. Sci. and Psychology, University of Alberta+Alberta Machine Intelligence Inst.",
        "aff_domain": "demcknight.com;ualberta.ca",
        "email": "demcknight.com;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Alberta;Alberta Machine Intelligence Institute",
        "aff_unique_dep": "Dept. Computing Science;Machine Intelligence",
        "aff_unique_url": "https://www.ualberta.ca;https://www.ami.alberta.ca",
        "aff_unique_abbr": "UAlberta;AMII",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.findings-naacl.62",
        "title": "Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Vision-language models (VLMs) are achieving increasingly strong performance on multimodal tasks. However, reasoning capabilities remain limited particularly for smaller VLMs, while those of large-language models (LLMs) have seen numerous improvements. We pro-pose a technique to transfer capabilities from LLMs to VLMs. On the recently introduced ChartQA, our method obtains state-of-the-artperformance when applied on the PaLI3-5B VLM by Chen et al. (2023c), while also enabling much better performance on PlotQA and FigureQA.We first improve the chart representation by continuing the pre-training stage using an improved version of the chart-to-table translation task by Liu et al. (2023a). We then propose constructing a 20x larger dataset than the original training set. To improve general reasoning capabilities and improve numerical operations, we synthesize reasoning traces using the table representation of charts. Lastly, our model is fine-tuned using the multitask loss introduced by Hsieh et al. (2023).Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B without using an upstream OCR system, while keeping inference time constant compared to the PaLI3-5B baseline. When rationales are further refined with a simple program-of-thought prompt (Chen et al., 2023a), our model outperforms the recently introduced Gemini Ultra and GPT-4V.",
        "author": "Victor Carbune; Hassan Mansoor; Fangyu Liu; Rahul Aralikatte; Gilles Baechler; Jindong Chen; Abhanshu Sharma",
        "authorids": "/v/victor-carbune/; /h/hassan-mansoor/; /f/fangyu-liu/; /r/rahul-aralikatte/; /g/gilles-baechler/; /j/jindong-chen/; /a/abhanshu-sharma/",
        "bibtex": "@inproceedings{carbune-etal-2024-chart,\n    title = \"Chart-based Reasoning: Transferring Capabilities from {LLM}s to {VLM}s\",\n    author = \"Carbune, Victor  and\n      Mansoor, Hassan  and\n      Liu, Fangyu  and\n      Aralikatte, Rahul  and\n      Baechler, Gilles  and\n      Chen, Jindong  and\n      Sharma, Abhanshu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.62/\",\n    doi = \"10.18653/v1/2024.findings-naacl.62\",\n    pages = \"989--1004\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.62.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.62/",
        "pdf_size": 1673302,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4964265816491173134&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com; ; ; ; ; ; ",
        "email": "google.com; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.165",
        "title": "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Textual backdoor attacks, characterized by subtle manipulations of input triggers and training dataset labels, pose significant threats to security-sensitive applications. The rise of advanced generative models, such as GPT-4, with their capacity for human-like rewriting, makes these attacks increasingly challenging to detect. In this study, we conduct an in-depth examination of black-box generative models as tools for backdoor attacks, thereby emphasizing the need for effective defense strategies. We propose BGMAttack, a novel framework that harnesses advanced generative models to execute stealthier backdoor attacks on text classifiers. Unlike prior approaches constrained by subpar generation quality, BGMAttack renders backdoor triggers more elusive to human cognition and advanced machine detection. A rigorous evaluation of attack effectiveness over four sentiment classification tasks, complemented by four human cognition stealthiness tests, reveals BGMAttack\u2019s superior performance, achieving a state-of-the-art attack success rate of 97.35% on average while maintaining superior stealth compared to conventional methods. The dataset and code are available: https://github.com/JiazhaoLi/BGMAttack.",
        "author": "Jiazhao Li; Yijin Yang; Zhuofeng Wu; V.G.Vinod Vydiswaran; Chaowei Xiao",
        "authorids": "/j/jiazhao-li/; /y/yijin-yang/; /z/zhuofeng-wu/; /v/v-g-vinod-vydiswaran/; /c/chaowei-xiao/",
        "bibtex": "@inproceedings{li-etal-2024-chatgpt,\n    title = \"{C}hat{GPT} as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger\",\n    author = \"Li, Jiazhao  and\n      Yang, Yijin  and\n      Wu, Zhuofeng  and\n      Vydiswaran, V.G.Vinod  and\n      Xiao, Chaowei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.165/\",\n    doi = \"10.18653/v1/2024.naacl-long.165\",\n    pages = \"2985--3004\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.165.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.165/",
        "pdf_size": 843281,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3245813014622562920&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/JiazhaoLi/BGMAttack",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2024.findings-naacl.31",
        "title": "Citation: A Key to Building Responsible and Accountable Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) bring transformative benefits alongside unique challenges, including intellectual property (IP) and ethical concerns. This position paper explores a novel angle to mitigate these risks, drawing parallels between LLMs and established web systems. We identify \u201ccitation\u201d\u2014the acknowledgement or reference to a source or evidence\u2014as a crucial yet missing component in LLMs. Incorporating citation could enhance content transparency and verifiability, thereby confronting the IP and ethical issues in the deployment of LLMs. We further propose that a comprehensive citation mechanism for LLMs should account for both non-parametric and parametric content. Despite the complexity of implementing such a citation mechanism, along with the potential pitfalls, we advocate for its development. Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards building more responsible and accountable LLMs.",
        "author": "Jie Huang; Kevin Chang",
        "authorids": "/j/jie-huang/; /k/kevin-chang/",
        "bibtex": "@inproceedings{huang-chang-2024-citation,\n    title = \"Citation: A Key to Building Responsible and Accountable Large Language Models\",\n    author = \"Huang, Jie  and\n      Chang, Kevin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.31/\",\n    doi = \"10.18653/v1/2024.findings-naacl.31\",\n    pages = \"464--473\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.31.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.31/",
        "pdf_size": 281095,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10209825162690187641&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.6",
        "title": "Clear Up Confusion: Advancing Cross-Domain Few-Shot Relation Extraction through Relation-Aware Prompt Learning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Cross-domain few-shot Relation Extraction (RE) aims to transfer knowledge from a source domain to a different target domain to address low-resource problems.Previous work utilized label descriptions and entity information to leverage the knowledge of the source domain.However, these models are prone to confusion when directly applying this knowledge to a target domain with entirely new types of relations, which becomes particularly pronounced when facing similar relations.In this work, we propose a relation-aware prompt learning method with pre-training.Specifically, we empower the model to clear confusion by decomposing various relation types through an innovative label prompt, while a context prompt is employed to capture differences in different scenarios, enabling the model to further discern confusion. Two pre-training tasks are designed to leverage the prompt knowledge and paradigm.Experiments show that our method outperforms previous sota methods, yielding significantly better results on cross-domain few-shot RE tasks.",
        "author": "Ge Bai; Chenji Lu; Daichi Guo; Shilong Li; Ying Liu; Zhang Zhang; Guanting Dong; Ruifang Liu; Sun Yong",
        "authorids": "/g/ge-bai/; /c/chenji-lu/; /d/daichi-guo/; /s/shilong-li/; /y/ying-liu/; /z/zhang-zhang/; /g/guanting-dong/; /r/ruifang-liu/; /s/sun-yong/",
        "bibtex": "@inproceedings{bai-etal-2024-clear,\n    title = \"Clear Up Confusion: Advancing Cross-Domain Few-Shot Relation Extraction through Relation-Aware Prompt Learning\",\n    author = \"Bai, Ge  and\n      Lu, Chenji  and\n      Guo, Daichi  and\n      Li, Shilong  and\n      Liu, Ying  and\n      Zhang, Zhang  and\n      Dong, Guanting  and\n      Liu, Ruifang  and\n      Yong, Sun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.6/\",\n    doi = \"10.18653/v1/2024.naacl-short.6\",\n    pages = \"70--78\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.6.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.6/",
        "pdf_size": 1261040,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_scsMWMQ7UEJ:scholar.google.com/&scioq=Clear+Up+Confusion:+Advancing+Cross-Domain+Few-Shot+Relation+Extraction+through+Relation-Aware+Prompt+Learning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.238",
        "title": "CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We present CoDa (**Co**nstrained Generation based **Da**ta Augmentation), a controllable, effective, and *training-free* data augmentation technique for low-resource (data-scarce) NLP. Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints. Precisely, we extract a set of simple constraints from every instance in the low-resource dataset and verbalize them to prompt an LLM to generate novel and diverse training instances. Our findings reveal that synthetic data that follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or fine-tuning with complex algorithms that eventually make the model biased toward the small number of training instances. Additionally, CoDa is the first framework that provides users explicit control over the augmentation generation process, thereby also allowing easy adaptation to several domains. We demonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and 3 low-resource settings. CoDa outperforms all our baselines, qualitatively and quantitatively, with improvements of 0.12%-7.19%. Code is available.",
        "author": "Chandra Kiran Evuru; Sreyan Ghosh; Sonal Kumar; Ramaneswaran S; Utkarsh Tyagi; Dinesh Manocha",
        "authorids": "/c/chandra-kiran-reddy-evuru/; /s/sreyan-ghosh/; /s/sonal-kumar/; /r/ramaneswaran-s/; /u/utkarsh-tyagi/; /d/dinesh-manocha/",
        "bibtex": "@inproceedings{evuru-etal-2024-coda,\n    title = \"{C}o{D}a: Constrained Generation based Data Augmentation for Low-Resource {NLP}\",\n    author = \"Evuru, Chandra Kiran  and\n      Ghosh, Sreyan  and\n      Kumar, Sonal  and\n      S, Ramaneswaran  and\n      Tyagi, Utkarsh  and\n      Manocha, Dinesh\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.238/\",\n    doi = \"10.18653/v1/2024.findings-naacl.238\",\n    pages = \"3754--3769\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.238.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.238/",
        "pdf_size": 798658,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4004079723003774964&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Maryland, College Park, USA; University of Maryland, College Park, USA; University of Maryland, College Park, USA; NVIDIA, Bangalore, India; University of Maryland, College Park, USA; University of Maryland, College Park, USA",
        "aff_domain": "umd.edu;umd.edu;umd.edu;nvidia.com;umd.edu;umd.edu",
        "email": "umd.edu;umd.edu;umd.edu;nvidia.com;umd.edu;umd.edu",
        "github": "https://github.com/Sreyan88/CoDa",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "University of Maryland;NVIDIA",
        "aff_unique_dep": ";NVIDIA",
        "aff_unique_url": "https://www/umd.edu;https://www.nvidia.com",
        "aff_unique_abbr": "UMD;NV",
        "aff_campus_unique_index": "0;0;0;1;0;0",
        "aff_campus_unique": "College Park;Bangalore",
        "aff_country_unique_index": "0;0;0;1;0;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "2024.naacl-long.361",
        "title": "CoE-SQL: In-Context Learning for Multi-Turn Text-to-SQL with Chain-of-Editions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, Large Language Models (LLMs) have been demonstrated to possess impressive capabilities in a variety of domains and tasks. We investigate the issue of prompt design in the multi-turn text-to-SQL task and attempt to enhance the LLMs\u2019 reasoning capacity when generating SQL queries. In the conversational context, the current SQL query can be modified from the preceding SQL query with only a few operations due to the context dependency. We introduce our method called CoE-SQL which can prompt LLMs to generate the SQL query based on the previously generated SQL query with an edition chain. We also conduct extensive ablation studies to determine the optimal configuration of our approach. Our approach outperforms different in-context learning baselines stably and achieves state-of-the-art performances on two benchmarks SParC and CoSQL using LLMs, which is also competitive to the SOTA fine-tuned models.",
        "author": "Hanchong Zhang; Ruisheng Cao; Hongshen Xu; Lu Chen; Kai Yu",
        "authorids": "/h/hanchong-zhang/; /r/ruisheng-cao/; /h/hongshen-xu/; /l/lu-chen/; /k/kai-yu/",
        "bibtex": "@inproceedings{zhang-etal-2024-coe,\n    title = \"{C}o{E}-{SQL}: In-Context Learning for Multi-Turn Text-to-{SQL} with Chain-of-Editions\",\n    author = \"Zhang, Hanchong  and\n      Cao, Ruisheng  and\n      Xu, Hongshen  and\n      Chen, Lu  and\n      Yu, Kai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.361/\",\n    doi = \"10.18653/v1/2024.naacl-long.361\",\n    pages = \"6487--6508\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.361.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.361/",
        "pdf_size": 309624,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15206434449668254768&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China+ Suzhou Laboratory, Suzhou, China; X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China+ Suzhou Laboratory, Suzhou, China; X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China+ Suzhou Laboratory, Suzhou, China; X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China+ Suzhou Laboratory, Suzhou, China; X-LANCE Lab, Department of Computer Science and Engineering, MoE Key Lab of Artificial Intelligence, SJTU AI Institute, Shanghai Jiao Tong University, Shanghai, China+ Suzhou Laboratory, Suzhou, China",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn; ; ",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn; ; ",
        "github": "https://github.com/X-LANCE/text2sql-multiturn-GPT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Suzhou Laboratory",
        "aff_unique_dep": "Department of Computer Science and Engineering;",
        "aff_unique_url": "https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "SJTU;",
        "aff_campus_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_campus_unique": "Shanghai;Suzhou",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.112",
        "title": "CoMM: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex Problem Solving",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) have shown great ability in solving traditional natural language tasks and elementary reasoning tasks with appropriate prompting techniques. However, their ability is still limited in solving complicated science problems. In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.",
        "author": "Pei Chen; Shuai Zhang; Boran Han",
        "authorids": "/p/pei-chen/; /s/shuai-zhang/; /b/boran-han/",
        "bibtex": "@inproceedings{chen-etal-2024-comm,\n    title = \"{C}o{MM}: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex Problem Solving\",\n    author = \"Chen, Pei  and\n      Zhang, Shuai  and\n      Han, Boran\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.112/\",\n    doi = \"10.18653/v1/2024.findings-naacl.112\",\n    pages = \"1720--1738\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.112.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.112/",
        "pdf_size": 1028552,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8024398200312297802&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science and Engineering, Texas A&M University; Amazon Web Services; Amazon Web Services",
        "aff_domain": "tamu.edu;amazon.com;amazon.com",
        "email": "tamu.edu;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Texas A&M University;Amazon",
        "aff_unique_dep": "Department of Computer Science and Engineering;Amazon Web Services",
        "aff_unique_url": "https://www.tamu.edu;https://aws.amazon.com",
        "aff_unique_abbr": "TAMU;AWS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.55",
        "title": "CoUDA: Coherence Evaluation via Unified Data Augmentation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Coherence evaluation aims to assess the organization and structure of a discourse, which remains challenging even in the era of large language models. Due to the scarcity of annotated data, data augmentation is commonly used for training coherence evaluation models. However, previous augmentations for this task primarily rely on heuristic rules, lacking designing criteria as guidance.In this paper, we take inspiration from linguistic theory of discourse structure, and propose a data augmentation framework named CoUDA. CoUDA breaks down discourse coherence into global and local aspects, and designs augmentation strategies for both aspects, respectively.Especially for local coherence, we propose a novel generative strategy for constructing augmentation samples, which involves post-pretraining a generative model and applying two controlling mechanisms to control the difficulty of generated samples. During inference, CoUDA also jointly evaluates both global and local aspects to comprehensively assess the overall coherence of a discourse.Extensive experiments in coherence evaluation show that, with only 233M parameters, CoUDA achieves state-of-the-art performance in both pointwise scoring and pairwise ranking tasks, even surpassing recent GPT-3.5 and GPT-4 based metrics.",
        "author": "Dawei Zhu; Wenhao Wu; Yifan Song; Fangwei Zhu; Ziqiang Cao; Sujian Li",
        "authorids": "/d/dawei-zhu/; /w/wenhao-wu/; /y/yifan-song/; /f/fangwei-zhu/; /z/ziqiang-cao/; /s/sujian-li/",
        "bibtex": "@inproceedings{zhu-etal-2024-couda,\n    title = \"{C}o{UDA}: Coherence Evaluation via Unified Data Augmentation\",\n    author = \"Zhu, Dawei  and\n      Wu, Wenhao  and\n      Song, Yifan  and\n      Zhu, Fangwei  and\n      Cao, Ziqiang  and\n      Li, Sujian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.55/\",\n    doi = \"10.18653/v1/2024.naacl-long.55\",\n    pages = \"967--978\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.55.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.55/",
        "pdf_size": 532104,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=495073108926153094&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Peking University + National Key Laboratory for Multimedia Information Processing, Peking University; School of Computer Science, Peking University + National Key Laboratory for Multimedia Information Processing, Peking University; School of Computer Science, Peking University + National Key Laboratory for Multimedia Information Processing, Peking University; School of Computer Science, Peking University + National Key Laboratory for Multimedia Information Processing, Peking University; Institute of Artificial Intelligence, Soochow University; School of Computer Science, Peking University + National Key Laboratory for Multimedia Information Processing, Peking University + Jiangsu Collaborative Innovation Center for Language Ability, Jiangsu Normal University",
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "github": "https://github.com/dwzhu-pku/CoUDA",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0;0+0;0+0;0+0;1;0+0+2",
        "aff_unique_norm": "Peking University;Soochow University;Jiangsu Normal University",
        "aff_unique_dep": "School of Computer Science;Institute of Artificial Intelligence;Jiangsu Collaborative Innovation Center for Language Ability",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.soochow.edu.cn;http://www.jsnu.edu.cn",
        "aff_unique_abbr": "PKU;;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.317",
        "title": "Code Models are Zero-shot Precondition Reasoners",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "One of the fundamental skills required for an agent acting in an environment to complete tasks is the ability to understand what actions are plausible at any given point. This work explores a novel use of code representations to reason about action preconditions for sequential decision making tasks. Code representations offer the flexibility to model procedural activities and associated constraints as well as the ability to execute and verify constraint satisfaction. Leveraging code representations, we extract action preconditions from demonstration trajectories in a zero-shot manner using pre-trained code models. Given these extracted preconditions, we propose a precondition-aware action sampling strategy that ensures actions predicted by a policy are consistent with preconditions. We demonstrate that the proposed approach enhances the performance of few-shot policy learning approaches across task-oriented dialog and embodied textworld benchmarks.",
        "author": "Lajanugen Logeswaran; Sungryull Sohn; Yiwei Lyu; Anthony Liu; Dong-Ki Kim; Dongsub Shim; Moontae Lee; Honglak Lee",
        "authorids": "/l/lajanugen-logeswaran/; /s/sungryull-sohn/; /y/yiwei-lyu/; /a/anthony-liu/; /d/dong-ki-kim/; /d/dongsub-shim/; /m/moontae-lee/; /h/honglak-lee/",
        "bibtex": "@inproceedings{logeswaran-etal-2024-code,\n    title = \"Code Models are Zero-shot Precondition Reasoners\",\n    author = \"Logeswaran, Lajanugen  and\n      Sohn, Sungryull  and\n      Lyu, Yiwei  and\n      Liu, Anthony  and\n      Kim, Dong-Ki  and\n      Shim, Dongsub  and\n      Lee, Moontae  and\n      Lee, Honglak\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.317/\",\n    doi = \"10.18653/v1/2024.naacl-long.317\",\n    pages = \"5681--5697\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.317.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.317/",
        "pdf_size": 461032,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15434479364612897319&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2024.findings-naacl.235",
        "title": "CodecLM: Aligning Language Models with Tailored Synthetic Data",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Instruction tuning has emerged as the key in aligning large language models (LLMs) with specific task instructions, thereby mitigating the discrepancy between the next-token prediction objective and users\u2019 actual goals. To reduce the labor and time cost to collect or annotate data by humans, researchers start to explore the use of LLMs to generate instruction-aligned synthetic data. Recent works focus on generating diverse instructions and applying LLM to increase instruction complexity, often neglecting downstream use cases. It remains unclear how to tailor high-quality data to elicit better instruction-following abilities in different target instruction distributions and LLMs. To this end, we introduce CodecLM, a general framework for adaptively generating high-quality synthetic data for LLM alignment with different downstream instruction distributions and LLMs. Drawing on the Encode-Decode principles, we use LLMs as codecs to guide the data generation process. We first encode seed instructions into metadata, which are concise keywords generated on-the-fly to capture the target instruction distribution, and then decode metadata to create tailored instructions. We also introduce Self-Rubrics and Contrastive Filtering during decoding to tailor data-efficient samples. Extensive experiments on four open-domain instruction following benchmarks validate the effectiveness of CodecLM over the current state-of-the-arts.",
        "author": "Zifeng Wang; Chun-Liang Li; Vincent Perot; Long Le; Jin Miao; Zizhao Zhang; Chen-Yu Lee; Tomas Pfister",
        "authorids": "/z/zifeng-wang/; /c/chun-liang-li/; /v/vincent-perot/; /l/long-le/; /j/jin-miao/; /z/zizhao-zhang/; /c/chen-yu-lee/; /t/tomas-pfister/",
        "bibtex": "@inproceedings{wang-etal-2024-codeclm,\n    title = \"{C}odec{LM}: Aligning Language Models with Tailored Synthetic Data\",\n    author = \"Wang, Zifeng  and\n      Li, Chun-Liang  and\n      Perot, Vincent  and\n      Le, Long  and\n      Miao, Jin  and\n      Zhang, Zizhao  and\n      Lee, Chen-Yu  and\n      Pfister, Tomas\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.235/\",\n    doi = \"10.18653/v1/2024.findings-naacl.235\",\n    pages = \"3712--3729\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.235.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.235/",
        "pdf_size": 1679596,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17488960670648413656&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Google Cloud AI; Google Cloud AI; Google Research; Google Cloud AI; Google Cloud AI; Google Cloud AI; Google Cloud AI; Google Cloud AI",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Cloud AI",
        "aff_unique_url": "https://cloud.google.com/ai",
        "aff_unique_abbr": "Google Cloud AI",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.224",
        "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While large language models (LLMs) have demonstrated increasing power, they have also called upon studies on their vulnerabilities. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of 1) multilingual cognitive overload, 2) veiled expression, and 3) effect-to- cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overload. Motivated by cognitive psychology work on managing cognitive load, we further investigate defending cognitive overload attack from two perspectives. Empirical studies show that our cognitive overload from three perspectives can jailbreak all studied LLMs successfully, while existing defense strategies can hardly mitigate the caused malicious uses effectively.",
        "author": "Nan Xu; Fei Wang; Ben Zhou; Bangzheng Li; Chaowei Xiao; Muhao Chen",
        "authorids": "/n/nan-xu/; /f/fei-wang/; /b/ben-zhou/; /b/bangzheng-li/; /c/chaowei-xiao/; /m/muhao-chen/",
        "bibtex": "@inproceedings{xu-etal-2024-cognitive,\n    title = \"Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking\",\n    author = \"Xu, Nan  and\n      Wang, Fei  and\n      Zhou, Ben  and\n      Li, Bangzheng  and\n      Xiao, Chaowei  and\n      Chen, Muhao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.224/\",\n    doi = \"10.18653/v1/2024.findings-naacl.224\",\n    pages = \"3526--3548\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.224.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.224/",
        "pdf_size": 1742791,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3369611128248602183&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Southern California; University of Pennsylvania; University of Wisconsin Madison; University of Southern California; University of Wisconsin Madison; University of California, Davis",
        "aff_domain": "usc.edu;usc.edu;seas.upenn.edu;usc.edu;wisc.edu;ucdavis.edu",
        "email": "usc.edu;usc.edu;seas.upenn.edu;usc.edu;wisc.edu;ucdavis.edu",
        "github": "https://github.com/luka-group/CognitiveOverload.git",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;0;2;3",
        "aff_unique_norm": "University of Southern California;University of Pennsylvania;University of Wisconsin-Madison;University of California, Davis",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.usc.edu;https://www.upenn.edu;https://www.wisc.edu;https://www.ucdavis.edu",
        "aff_unique_abbr": "USC;UPenn;UW-Madison;UC Davis",
        "aff_campus_unique_index": "0;2;0;2;3",
        "aff_campus_unique": "Los Angeles;;Madison;Davis",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.370",
        "title": "ComCLIP: Training-Free Compositional Image and Text Matching",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-language pretrained models like CLIP to compositional image and text matching \u2014 a more challenging image and text matching task requiring the model\u2019s understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel training-free compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action subimages and composes CLIP\u2019s vision encoder and text encoder to perform evolving matching over compositional text embedding and subimage embeddings. In this way, ComCLIP can mitigate spurious correlations introduced by the pretrained CLIP models and dynamically evaluate the importance of each component. Experiments on four compositional image-text matching datasets: Winoground, VL-checklist, SVO, and ComVG, and two general image-text retrieval datasets: Flick30K, and MSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts the zero-shot inference ability of CLIP, SLIP, and BLIP2 even without further training or fine-tuning. Our codes can be found at https://github.com/eric-ai-lab/ComCLIP.",
        "author": "Kenan Jiang; Xuehai He; Ruize Xu; Xin Wang",
        "authorids": "/k/kenan-jiang/; /x/xuehai-he/; /r/ruize-xu/; /x/xin-wang/",
        "bibtex": "@inproceedings{jiang-etal-2024-comclip,\n    title = \"{C}om{CLIP}: Training-Free Compositional Image and Text Matching\",\n    author = \"Jiang, Kenan  and\n      He, Xuehai  and\n      Xu, Ruize  and\n      Wang, Xin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.370/\",\n    doi = \"10.18653/v1/2024.naacl-long.370\",\n    pages = \"6639--6659\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.370.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.370/",
        "pdf_size": 32896425,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5126083329506829425&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "UC Berkeley; UC Santa Cruz; Columbia University; UC Santa Cruz",
        "aff_domain": "berkeley.edu;ucsc.edu;columbia.edu;ucsc.edu",
        "email": "berkeley.edu;ucsc.edu;columbia.edu;ucsc.edu",
        "github": "https://github.com/eric-ai-lab/ComCLIP",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "University of California, Berkeley;University of California, Santa Cruz;Columbia University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.ucsc.edu;https://www.columbia.edu",
        "aff_unique_abbr": "UC Berkeley;UCSC;Columbia",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Berkeley;Santa Cruz;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.178",
        "title": "Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In many real natural language processing application scenarios, practitioners not only aim to maximize predictive performance but also seek faithful explanations for the model predictions. Rationales and importance distribution given by feature attribution methods (FAs) provide insights into how different parts of the input contribute to a prediction. Previous studies have explored how different factors affect faithfulness, mainly in the context of monolingual English models. On the other hand, the differences in FA faithfulness between multilingual and monolingual models have yet to be explored. Our extensive experiments, covering five languages and five popular FAs, show that FA faithfulness varies between multilingual and monolingual models. We find that the larger the multilingual model, the less faithful the FAs are compared to its counterpart monolingual models. Our further analysis shows that the faithfulness disparity is potentially driven by the differences between model tokenizers. Our code is available: https://github.com/casszhao/multilingual-faith.",
        "author": "Zhixue Zhao; Nikolaos Aletras",
        "authorids": "/z/zhixue-zhao/; /n/nikolaos-aletras/",
        "bibtex": "@inproceedings{zhao-aletras-2024-comparing,\n    title = \"Comparing Explanation Faithfulness between Multilingual and Monolingual Fine-tuned Language Models\",\n    author = \"Zhao, Zhixue  and\n      Aletras, Nikolaos\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.178/\",\n    doi = \"10.18653/v1/2024.naacl-long.178\",\n    pages = \"3226--3244\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.178.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.178/",
        "pdf_size": 474783,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13211501882060610217&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/casszhao/multilingual-faith",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2024.findings-naacl.25",
        "title": "Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Following an interaction with a patient, physicians are responsible for the submission of clinical documentation, often organized as a SOAP note. A clinical note is not simply a summary of the conversation but requires the use of appropriate medical terminology. The relevant information can then be extracted and organized according to the structure of the SOAP note. In this paper we analyze two different approaches to generate the different sections of a SOAP note based on the audio recording of the conversation, and specifically examine them in terms of note consistency. The first approach generates the sections independently, while the second method generates them all together. In this work we make use of PEGASUS-X Transformer models and observe that both methods lead to similar ROUGE values (less than 1% difference) and have no difference in terms of the Factuality metric. We perform a human evaluation to measure aspects of consistency and demonstrate that LLMs like Llama2 can be used to perform the same tasks with roughly the same agreement as the human annotators. Between the Llama2 analysis and the human reviewers we observe a Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of age, gender, and body part injury, respectively. With this we demonstrate the usefulness of leveraging an LLM to measure quality indicators that can be identified by humans but are not currently captured by automatic metrics. This allows scaling evaluation to larger data sets, and we find that clinical note consistency improves by generating each new section conditioned on the output of all previously generated sections.",
        "author": "Nathan Brake; Thomas Schaaf",
        "authorids": "/n/nathan-brake/; /t/thomas-schaaf/",
        "bibtex": "@inproceedings{brake-schaaf-2024-comparing,\n    title = \"Comparing Two Model Designs for Clinical Note Generation; Is an {LLM} a Useful Evaluator of Consistency?\",\n    author = \"Brake, Nathan  and\n      Schaaf, Thomas\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.25/\",\n    doi = \"10.18653/v1/2024.findings-naacl.25\",\n    pages = \"352--363\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.25.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.25/",
        "pdf_size": 212769,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10984622495888253058&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2024.findings-naacl.173",
        "title": "Compensate Quantization Errors: Make Weights Hierarchical to Compensate Each Other",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Emergent Large Language Models (LLMs) use their extraordinary performance and powerful deduction capacity to discern from traditional language models. However, the expenses of computational resources and storage for these LLMs are stunning, quantization then arises as a trending conversation. To address accuracy decay caused by quantization, two streams of works in post-training quantization methods stand out. One uses other weights to compensate existing quantization error, while the other transfers the quantization difficulty to other parts in the model. Combining both merits, we introduce Learnable Singular value Increment (LSI) as an advanced solution. LSI uses Singular Value Decomposition to extract singular values of the weights and make them learnable to help weights compensate each other conditioned on activation. Incorporating LSI with existing techniques, we achieve state-of-the-art performance in diverse quantization settings, no matter in weight-only, weight-activation or extremely low bit scenarios. By unleashing the potential of LSI, efficient finetuning on quantized model is no longer a prohibitive problem.",
        "author": "Yifei Gao; Jie Ou; Lei Wang; Yuting Xiao; Xiangzhiyuan Xiangzhiyuan; Ruiting Dai; Jun Cheng",
        "authorids": "/y/yifei-gao/; /j/jie-ou/; /l/lei-wang/; /y/yuting-xiao/; /x/xiangzhiyuan-xiangzhiyuan/; /r/ruiting-dai/; /j/jun-cheng/",
        "bibtex": "@inproceedings{gao-etal-2024-compensate,\n    title = \"Compensate Quantization Errors: Make Weights Hierarchical to Compensate Each Other\",\n    author = \"Gao, Yifei  and\n      Ou, Jie  and\n      Wang, Lei  and\n      Xiao, Yuting  and\n      Xiangzhiyuan, Xiangzhiyuan  and\n      Dai, Ruiting  and\n      Cheng, Jun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.173/\",\n    doi = \"10.18653/v1/2024.findings-naacl.173\",\n    pages = \"2711--2722\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.173.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.173/",
        "pdf_size": 1932377,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7590450325133514138&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Electronic Science and Technology of China; University of Electronic Science and Technology of China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Beijing Normal University; Tianjin University; University of Electronic Science and Technology of China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "aff_domain": "gmail.com; ;siat.ac.cn; ; ; ;",
        "email": "gmail.com; ;siat.ac.cn; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;2;3;0;1",
        "aff_unique_norm": "University of Electronic Science and Technology of China;Chinese Academy of Sciences;Beijing Normal University;Tianjin University",
        "aff_unique_dep": ";Shenzhen Institutes of Advanced Technology;;",
        "aff_unique_url": "https://www.uestc.edu.cn;http://www.siat.cas.cn;https://www.bnu.edu.cn;http://www.tju.edu.cn",
        "aff_unique_abbr": "UESTC;SIAT;BNU;TJU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.196",
        "title": "Complex Claim Verification with Evidence Retrieved in the Wild",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Retrieving evidence to support or refute claims is a core part of automatic fact-checking. Prior work makes simplifying assumptions in retrieval that depart from real-world use cases: either no access to evidence, access to evidence curated by a human fact-checker, or access to evidence published after a claim was made. In this work, we present the first realistic pipeline to check real-world claims by retrieving raw evidence from the web. We restrict our retriever to only search documents available prior to the claim\u2019s making, modeling the realistic scenario of emerging claims. Our pipeline includes five components: claim decomposition, raw document retrieval, fine-grained evidence retrieval, claim-focused summarization, and veracity judgment. We conduct experiments on complex political claims in the ClaimDecomp dataset and show that the aggregated evidence produced by our pipeline improves veracity judgments. Human evaluation finds the evidence summary produced by our system is reliable (it does not hallucinate information) and relevant to answering key questions about a claim, suggesting that it can assist fact-checkers even when it does not reflect a complete evidence set.",
        "author": "Jifan Chen; Grace Kim; Aniruddh Sriram; Greg Durrett; Eunsol Choi",
        "authorids": "/j/jifan-chen/; /g/grace-kim/; /a/aniruddh-sriram/; /g/greg-durrett/; /e/eunsol-choi/",
        "bibtex": "@inproceedings{chen-etal-2024-complex,\n    title = \"Complex Claim Verification with Evidence Retrieved in the Wild\",\n    author = \"Chen, Jifan  and\n      Kim, Grace  and\n      Sriram, Aniruddh  and\n      Durrett, Greg  and\n      Choi, Eunsol\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.196/\",\n    doi = \"10.18653/v1/2024.naacl-long.196\",\n    pages = \"3569--3587\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.196.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.196/",
        "pdf_size": 1286662,
        "gs_citation": 78,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8140237372081768772&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin",
        "aff_domain": "utexas.edu; ; ; ; ",
        "email": "utexas.edu; ; ; ; ",
        "github": "https://github.com/jifan-chen/Fact-checking-via-Raw-Evidence",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.94",
        "title": "Composite Backdoor Attacks Against Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) have demonstrated superior performance compared to previous methods on various tasks, and often serve as the foundation models for many researches and services. However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks. In this paper, we explore the vulnerability of LLMs through the lens of backdoor attacks. Different from existing backdoor attacks against LLMs, ours scatters multiple trigger keys in different prompt components. Such a Composite Backdoor Attack (CBA) is shown to be stealthier than implanting the same multiple trigger keys in only a single component. CBA ensures that the backdoor is activated only when all trigger keys appear. Our experiments demonstrate that CBA is effective in both natural language processing (NLP) and multimodal tasks. For instance, with 3% poisoning samples against the LLaMA-7B model on the Emotion dataset, our attack achieves a 100% Attack Success Rate (ASR) with a False Triggered Rate (FTR) below 2.06% and negligible model accuracy degradation. Our work highlights the necessity of increased security research on the trustworthiness of foundation LLMs.",
        "author": "Hai Huang; Zhengyu Zhao; Michael Backes; Yun Shen; Yang Zhang",
        "authorids": "/h/hai-huang/; /z/zhengyu-zhao/; /m/michael-backes/; /y/yun-shen/; /y/yang-zhang/",
        "bibtex": "@inproceedings{huang-etal-2024-composite,\n    title = \"Composite Backdoor Attacks Against Large Language Models\",\n    author = \"Huang, Hai  and\n      Zhao, Zhengyu  and\n      Backes, Michael  and\n      Shen, Yun  and\n      Zhang, Yang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.94/\",\n    doi = \"10.18653/v1/2024.findings-naacl.94\",\n    pages = \"1459--1472\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.94.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.94/",
        "pdf_size": 2004021,
        "gs_citation": 84,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11077453819005544163&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "CISPA Helmholtz Center for Information Security; Xi\u2019an Jiaotong University; CISPA Helmholtz Center for Information Security; NetApp; CISPA Helmholtz Center for Information Security",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/MiracleHH/CBA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "CISPA Helmholtz Center for Information Security;Xi'an Jiao Tong University;NetApp",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cispa.de/;https://www.xjtu.edu.cn;https://www.netapp.com",
        "aff_unique_abbr": "CISPA;XJTU;NetApp",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;2;0",
        "aff_country_unique": "Germany;China;United States"
    },
    {
        "id": "2024.naacl-demo.15",
        "title": "Concept Over Time Analysis: Unveiling Temporal Patterns for Qualitative Data Analysis",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "In this system demonstration paper, we present the Concept Over Time Analysis extension for the Discourse Analysis Tool Suite.The proposed tool empowers users to define, refine, and visualize their concepts of interest within an interactive interface. Adhering to the Human-in-the-loop paradigm, users can give feedback through sentence annotations. Utilizing few-shot sentence classification, the system employs Sentence Transformers to compute representations of sentences and concepts. Through an iterative process involving semantic similarity searches, sentence annotation, and fine-tuning with contrastive data, the model continuously refines, providing users with enhanced analysis outcomes. The final output is a timeline visualization of sentences classified to concepts. Especially suited for the Digital Humanities, Concept Over Time Analysis serves as a valuable tool for qualitative data analysis within extensive datasets. The chronological overview of concepts enables researchers to uncover patterns, trends, and shifts in discourse over time.",
        "author": "Tim Fischer; Florian Schneider; Robert Geislinger; Florian Helfer; Gertraud Koch; Chris Biemann",
        "authorids": "/t/tim-fischer/; /f/florian-schneider/; /r/robert-geislinger/; /f/florian-helfer/; /g/gertraud-koch/; /c/chris-biemann/",
        "bibtex": "@inproceedings{fischer-etal-2024-concept,\n    title = \"Concept Over Time Analysis: Unveiling Temporal Patterns for Qualitative Data Analysis\",\n    author = \"Fischer, Tim  and\n      Schneider, Florian  and\n      Geislinger, Robert  and\n      Helfer, Florian  and\n      Koch, Gertraud  and\n      Biemann, Chris\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.15/\",\n    doi = \"10.18653/v1/2024.naacl-demo.15\",\n    pages = \"148--157\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.15.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.15/",
        "pdf_size": 1227380,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15979027480766334314&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Language Technology Group, Department of Informatics, Universit\u00e4t Hamburg, Germany; Language Technology Group, Department of Informatics, Universit\u00e4t Hamburg, Germany; Language Technology Group, Department of Informatics, Universit\u00e4t Hamburg, Germany; Institute of Anthropological Studies in Culture and History, Universit\u00e4t Hamburg, Germany; Institute of Anthropological Studies in Culture and History, Universit\u00e4t Hamburg, Germany; Language Technology Group, Department of Informatics, Universit\u00e4t Hamburg, Germany",
        "aff_domain": "uni-hamburg.de;uni-hamburg.de;uni-hamburg.de;uni-hamburg.de;uni-hamburg.de;uni-hamburg.de",
        "email": "uni-hamburg.de;uni-hamburg.de;uni-hamburg.de;uni-hamburg.de;uni-hamburg.de;uni-hamburg.de",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Universit\u00e4t Hamburg",
        "aff_unique_dep": "Department of Informatics",
        "aff_unique_url": "https://www.uni-hamburg.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.findings-naacl.156",
        "title": "Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We present Conformal Intent Classification and Clarification (CICC), a framework for fast and accurate intent classification for task-oriented dialogue systems. The framework turns heuristic uncertainty scores of any intent classifier into a clarification question that is guaranteed to contain the true intent at a pre-defined confidence level.By disambiguating between a small number of likely intents, the user query can be resolved quickly and accurately. Additionally, we propose to augment the framework for out-of-scope detection.In a comparative evaluation using seven intent recognition datasets we find that CICC generates small clarification questions and is capable of out-of-scope detection.CICC can help practitioners and researchers substantially in improving the user experience of dialogue agents with specific clarification questions.",
        "author": "Floris Hengst; Ralf Wolter; Patrick Altmeyer; Arda Kaygan",
        "authorids": "/f/floris-hengst/; /r/ralf-wolter/; /p/patrick-altmeyer/; /a/arda-kaygan/",
        "bibtex": "@inproceedings{hengst-etal-2024-conformal,\n    title = \"Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition\",\n    author = \"Hengst, Floris  and\n      Wolter, Ralf  and\n      Altmeyer, Patrick  and\n      Kaygan, Arda\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.156/\",\n    doi = \"10.18653/v1/2024.findings-naacl.156\",\n    pages = \"2412--2432\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.156.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.156/",
        "pdf_size": 6504352,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11437133369470888202&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Vrije Universiteit Amsterdam; ING Group NV; TU Delft; ING Group NV",
        "aff_domain": "vu.nl;ing.com;tudelft.nl;ing.com",
        "email": "vu.nl;ing.com;tudelft.nl;ing.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Vrije Universiteit Amsterdam;ING Group;Delft University of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.vu.nl;https://www.ing.com;https://www.tudelft.nl",
        "aff_unique_abbr": "VU Amsterdam;ING;TU Delft",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Delft",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2024.naacl-industry.12",
        "title": "Conformer-Based Speech Recognition On Extreme Edge-Computing Devices",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "With increasingly more powerful compute capabilities and resources in today\u2019s devices, traditionally compute-intensive automatic speech recognition (ASR) has been moving from the cloud to devices to better protect user privacy. However, it is still challenging to implement on-device ASR on resource-constrained devices, such as smartphones, smart wearables, and other small home automation devices. In this paper, we propose a series of model architecture adaptions, neural network graph transformations, and numerical optimizations to fit an advanced Conformer based end-to-end streaming ASR system on resource-constrained devices without accuracy degradation. We achieve over 5.26 times faster than realtime (0.19 RTF) speech recognition on small wearables while minimizing energy consumption and achieving state-of-the-art accuracy. The proposed methods are widely applicable to other transformer-based server-free AI applications. In addition, we provide a complete theory on optimal pre-normalizers that numerically stabilize layer normalization in any Lp-norm using any floating point precision.",
        "author": "Mingbin Xu; Alex Jin; Sicheng Wang; Mu Su; Tim Ng; Henry Mason; Shiyi Han; Zhihong Lei; Yaqiao Deng; Zhen Huang; Mahesh Krishnamoorthy",
        "authorids": "/m/mingbin-xu/; /a/alex-jin/; /s/sicheng-wang/; /m/mu-su/; /t/tim-ng/; /h/henry-mason/; /s/shiyi-han/; /z/zhihong-lei/; /y/yaqiao-deng/; /z/zhen-huang/; /m/mahesh-krishnamoorthy/",
        "bibtex": "@inproceedings{xu-etal-2024-conformer,\n    title = \"Conformer-Based Speech Recognition On Extreme Edge-Computing Devices\",\n    author = \"Xu, Mingbin  and\n      Jin, Alex  and\n      Wang, Sicheng  and\n      Su, Mu  and\n      Ng, Tim  and\n      Mason, Henry  and\n      Han, Shiyi  and\n      Lei, Zhihong  and\n      Deng, Yaqiao  and\n      Huang, Zhen  and\n      Krishnamoorthy, Mahesh\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.12/\",\n    doi = \"10.18653/v1/2024.naacl-industry.12\",\n    pages = \"131--139\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.12.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.12/",
        "pdf_size": 1274942,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2461457284640133974&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple; Apple",
        "aff_domain": "apple.com;gmail.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com",
        "email": "apple.com;gmail.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com;apple.com",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Apple",
        "aff_unique_dep": "Apple Inc.",
        "aff_unique_url": "https://www.apple.com",
        "aff_unique_abbr": "Apple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.198",
        "title": "Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in the context of bias mitigation, though in-context learning and finetuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as Random Forest and shallow Neural Networks. This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pretraining corpus, not only from the downstream task datasets. Besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs.",
        "author": "Yanchen Liu; Srishti Gautam; Jiaqi Ma; Himabindu Lakkaraju",
        "authorids": "/y/yanchen-liu/; /s/srishti-gautam/; /j/jiaqi-ma/; /h/himabindu-lakkaraju/",
        "bibtex": "@inproceedings{liu-etal-2024-confronting,\n    title = \"Confronting {LLM}s with Traditional {ML}: Rethinking the Fairness of Large Language Models in Tabular Classifications\",\n    author = \"Liu, Yanchen  and\n      Gautam, Srishti  and\n      Ma, Jiaqi  and\n      Lakkaraju, Himabindu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.198/\",\n    doi = \"10.18653/v1/2024.naacl-long.198\",\n    pages = \"3603--3620\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.198.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.198/",
        "pdf_size": 691661,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3517378222132844821&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Harvard University; UiT The Arctic University of Norway; University of Illinois Urbana-Champaign; Harvard University",
        "aff_domain": "g.harvard.edu;uit.no;illinois.edu;hbs.edu",
        "email": "g.harvard.edu;uit.no;illinois.edu;hbs.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Harvard University;Arctic University of Norway;University of Illinois Urbana-Champaign",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.harvard.edu;https://www.uit.no;https://illinois.edu",
        "aff_unique_abbr": "Harvard;UiT;UIUC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Norway"
    },
    {
        "id": "2024.findings-naacl.121",
        "title": "Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We study the patent phrase similarity inference task, which measures the semantic similarity between two patent phrases. As patent documents employ legal and highly technical language, existing semantic textual similarity methods that use localized contextual information do not perform satisfactorily in inferring patent phrase similarity. To address this, we introduce a graph-augmented approach to amplify the global contextual information of the patent phrases. For each patent phrase, we construct a phrase graph that links to its focal patents and a list of patents that are either cited by or cite these focal patents. The augmented phrase embedding is then derived from combining its localized contextual embedding with its global embedding within the phrase graph. We further propose a self-supervised learning objective that capitalizes on the retrieved topology to refine both the contextualized embedding and the graph parameters in an end-to-end manner. Experimental results from a unique patent phrase similarity dataset demonstrate that our approach significantly enhances the representation of patent phrases, resulting in marked improvements in similarity inference in a self-supervised fashion. Substantial improvements are also observed in the supervised setting, underscoring the potential benefits of leveraging retrieved phrase graph augmentation.",
        "author": "Zhuoyi Peng; Yi Yang",
        "authorids": "/z/zhuoyi-peng/; /y/yi-yang/",
        "bibtex": "@inproceedings{peng-yang-2024-connecting,\n    title = \"Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved Phrase Graphs\",\n    author = \"Peng, Zhuoyi  and\n      Yang, Yi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.121/\",\n    doi = \"10.18653/v1/2024.findings-naacl.121\",\n    pages = \"1877--1890\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.121.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.121/",
        "pdf_size": 330391,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12468534503198729144&as_sdt=5,24&sciodt=0,24&hl=en",
        "gs_version_total": 4,
        "aff": "The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology",
        "aff_domain": "connect.ust.hk;ust.hk",
        "email": "connect.ust.hk;ust.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.152",
        "title": "Content-Specific Humorous Image Captioning Using Incongruity Resolution Chain-of-Thought",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Although automated image captioning methods have benefited considerably from the development of large language models (LLMs), generating humorous captions is still a challenging task. Humorous captions generated by humans are unique to the image and reflect the content of the image. However, captions generated using previous captioning models tend to be generic. Therefore, we propose incongruity-resolution chain-of-thought (IRCoT) as a novel prompting framework that creates content-specific resolutions from fine details extracted from an image. Furthermore, we integrate logit bias and negative sampling to suppress the output of generic resolutions. The results of experiments with GPT4-V demonstrate that our proposed framework effectively generated humorous captions tailored to the content of specific input images.",
        "author": "Kohtaro Tanaka; Kohei Uehara; Lin Gu; Yusuke Mukuta; Tatsuya Harada",
        "authorids": "/k/kohtaro-tanaka/; /k/kohei-uehara/; /l/lin-gu/; /y/yusuke-mukuta/; /t/tatsuya-harada/",
        "bibtex": "@inproceedings{tanaka-etal-2024-content,\n    title = \"Content-Specific Humorous Image Captioning Using Incongruity Resolution Chain-of-Thought\",\n    author = \"Tanaka, Kohtaro  and\n      Uehara, Kohei  and\n      Gu, Lin  and\n      Mukuta, Yusuke  and\n      Harada, Tatsuya\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.152/\",\n    doi = \"10.18653/v1/2024.findings-naacl.152\",\n    pages = \"2348--2367\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.152.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.152/",
        "pdf_size": 3991100,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:H1Ul99oK74QJ:scholar.google.com/&scioq=Content-Specific+Humorous+Image+Captioning+Using+Incongruity+Resolution+Chain-of-Thought&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "The University of Tokyo+RIKEN; The University of Tokyo+RIKEN; The University of Tokyo+RIKEN; The University of Tokyo+RIKEN; The University of Tokyo+RIKEN",
        "aff_domain": "mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp",
        "email": "mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp;mi.t.u-tokyo.ac.jp",
        "github": "",
        "project": "https://kohtaro246.github.io/publication/IRCoT",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "University of Tokyo;RIKEN",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": "UTokyo;RIKEN",
        "aff_campus_unique_index": ";;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2024.findings-naacl.80",
        "title": "Context Does Matter: Implications for Crowdsourced Evaluation Labels in Task-Oriented Dialogue Systems",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Crowdsourced labels play a crucial role in evaluating task-oriented dialogue systems (TDSs). Obtaining high-quality and consistent ground-truth labels from annotators presents challenges. When evaluating a TDS, annotators must fully comprehend the dialogue before providing judgments. Previous studies suggest using only a portion of the dialogue context in the annotation process. However, the impact of this limitation on label quality remains unexplored. This study investigates the influence of dialogue context on annotation quality, considering the truncated context for relevance and usefulness labeling. We further propose to use large language models ( LLMs) to summarize the dialogue context to provide a rich and short description of the dialogue context and study the impact of doing so on the annotator\u2019s performance. Reducing context leads to more positive ratings. Conversely, providing the entire dialogue context yields higher-quality relevance ratings but introduces ambiguity in usefulness ratings. Using the first user utterance as context leads to consistent ratings, akin to those obtained using the entire dialogue, with significantly reduced annotation effort. Our findings show how task design, particularly the availability of dialogue context, affects the quality and consistency of crowdsourced evaluation labels.",
        "author": "Clemencia Siro; Mohammad Aliannejadi; Maarten de Rijke",
        "authorids": "/c/clemencia-siro/; /m/mohammad-aliannejadi/; /m/maarten-de-rijke/",
        "bibtex": "@inproceedings{siro-etal-2024-context,\n    title = \"Context Does Matter: Implications for Crowdsourced Evaluation Labels in Task-Oriented Dialogue Systems\",\n    author = \"Siro, Clemencia  and\n      Aliannejadi, Mohammad  and\n      de Rijke, Maarten\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.80/\",\n    doi = \"10.18653/v1/2024.findings-naacl.80\",\n    pages = \"1258--1273\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.80.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.80/",
        "pdf_size": 398960,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12761877676686130763&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Amsterdam, Amsterdam, The Netherlands; University of Amsterdam, Amsterdam, The Netherlands; University of Amsterdam, Amsterdam, The Netherlands",
        "aff_domain": "uva.nl;uva.nl;uva.nl",
        "email": "uva.nl;uva.nl;uva.nl",
        "github": "https://github.com/Clemenciah/Effects-of-Dialogue-Context",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Amsterdam",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uva.nl",
        "aff_unique_abbr": "UvA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Amsterdam",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2024.naacl-long.321",
        "title": "Contextual Label Projection for Cross-Lingual Structured Prediction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Label projection, which involves obtaining translated labels and texts jointly, is essential for leveraging machine translation to facilitate cross-lingual transfer in structured prediction tasks. Prior research exploring label projection often compromise translation accuracy by favoring simplified label translation or relying solely on word-level alignments. In this paper, we introduce a novel label projection approach, CLaP, which translates text to the target language and performs *contextual translation* on the labels using the translated text as the context, ensuring better accuracy for the translated labels. We leverage instruction-tuned language models with multilingual capabilities as our contextual translator, imposing the constraint of the presence of translated labels in the translated text via instructions. We benchmark CLaP with other label projection techniques on zero-shot cross-lingual transfer across 39 languages on two representative structured prediction tasks - event argument extraction (EAE) and named entity recognition (NER), showing over 2.4 F1 improvement for EAE and 1.4 F1 improvement for NER. We further explore the applicability of CLaP on ten extremely low-resource languages to showcase its potential for cross-lingual structured prediction.",
        "author": "Tanmay Parekh; I-Hung Hsu; Kuan-Hao Huang; Kai-Wei Chang; Nanyun Peng",
        "authorids": "/t/tanmay-parekh/; /i/i-hung-hsu/; /k/kuan-hao-huang/; /k/kai-wei-chang/; /n/nanyun-peng/",
        "bibtex": "@inproceedings{parekh-etal-2024-contextual,\n    title = \"Contextual Label Projection for Cross-Lingual Structured Prediction\",\n    author = \"Parekh, Tanmay  and\n      Hsu, I-Hung  and\n      Huang, Kuan-Hao  and\n      Chang, Kai-Wei  and\n      Peng, Nanyun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.321/\",\n    doi = \"10.18653/v1/2024.naacl-long.321\",\n    pages = \"5738--5757\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.321.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.321/",
        "pdf_size": 1505242,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16074657090520971173&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science Department, University of California, Los Angeles; Information Science Institute, University of Southern California; Department of Computer Science, University of Illinois Urbana-Champaign; Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;isi.edu;illinois.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;isi.edu;illinois.edu;cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "University of California, Los Angeles;University of Southern California;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Computer Science Department;Information Science Institute;Department of Computer Science",
        "aff_unique_url": "https://www.ucla.edu;https://www.usc.edu;https://illinois.edu",
        "aff_unique_abbr": "UCLA;USC;UIUC",
        "aff_campus_unique_index": "0;0;1;0;0",
        "aff_campus_unique": "Los Angeles;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.148",
        "title": "Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) have demonstrated considerable success in various natural language processing tasks, but open-source LLMs have yet to attain state-of-the-art performance in Neural Machine Translation (NMT). Nevertheless, their significant performance in tasks demanding a broad understanding and contextual processing shows their potential for translation. To exploit these abilities, we investigate using LLMs for MT and explore recent parameter-efficient fine-tuning techniques. Surprisingly, our initial experiments found that fine-tuning with Q-LoRA for translation purposes led to performance improvements in terms of BLEU but degradation in COMET compared to in-context learning. To overcome this, we propose an alternative approach: adapting LLMs as Automatic Post-Editors (APE) rather than direct translators. Building on the ability of the LLM to handle long sequences, we also propose extending our approach to document-level translation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can yield significant improvements across both sentence and document-level metrics while generalizing to out-of-domain data. Most notably, we achieve a state-of-the-art accuracy rate of 88.7% on the ContraPro test set, which assesses the model\u2019s ability to resolve pronoun ambiguities when translating from English to German. Lastly, during manual post-editing for document-level translation, the source sentences are iteratively annotated, which can be used to refine further translations in the document. Here, we demonstrate that leveraging human corrections can significantly reduce the number of edits required for subsequent translations.",
        "author": "Sai Koneru; Miriam Exel; Matthias Huck; Jan Niehues",
        "authorids": "/s/sai-koneru/; /m/miriam-exel/; /m/matthias-huck/; /j/jan-niehues/",
        "bibtex": "@inproceedings{koneru-etal-2024-contextual,\n    title = \"Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing\",\n    author = \"Koneru, Sai  and\n      Exel, Miriam  and\n      Huck, Matthias  and\n      Niehues, Jan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.148/\",\n    doi = \"10.18653/v1/2024.naacl-long.148\",\n    pages = \"2711--2725\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.148.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.148/",
        "pdf_size": 463478,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2649719331335868452&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Karlsruhe Institute of Technology; SAP SE, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany; SAP SE, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany; Karlsruhe Institute of Technology",
        "aff_domain": "kit.edu;sap.com;sap.com;kit.edu",
        "email": "kit.edu;sap.com;sap.com;kit.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Karlsruhe Institute of Technology;SAP SE",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kit.edu;https://www.sap.com",
        "aff_unique_abbr": "KIT;SAP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-short.28",
        "title": "Contextualizing Argument Quality Assessment with Relevant Knowledge",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Automatic assessment of the quality of arguments has been recognized as a challenging task with significant implications for misinformation and targeted speech. While real-world arguments are tightly anchored in context, existing computational methods analyze their quality in isolation, which affects their accuracy and generalizability. We propose SPARK: a novel method for scoring argument quality based on contextualization via relevant knowledge. We devise four augmentations that leverage large language models to provide feedback, infer hidden assumptions, supply a similar-quality argument, or give a counter-argument. SPARK uses a dual-encoder Transformer architecture to enable the original argument and its augmentation to be considered jointly. Our experiments in both in-domain and zero-shot setups show that SPARK consistently outperforms existing techniques across multiple metrics",
        "author": "Darshan Deshpande; Zhivar Sourati; Filip Ilievski; Fred Morstatter",
        "authorids": "/d/darshan-deshpande/; /z/zhivar-sourati/; /f/filip-ilievski/; /f/fred-morstatter/",
        "bibtex": "@inproceedings{deshpande-etal-2024-contextualizing,\n    title = \"Contextualizing Argument Quality Assessment with Relevant Knowledge\",\n    author = \"Deshpande, Darshan  and\n      Sourati, Zhivar  and\n      Ilievski, Filip  and\n      Morstatter, Fred\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.28/\",\n    doi = \"10.18653/v1/2024.naacl-short.28\",\n    pages = \"316--326\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.28.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.28/",
        "pdf_size": 772126,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17678387298454060628&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Information Sciences Institute, University of Southern California; Information Sciences Institute, University of Southern California; Department of Computer Science, Vrije Universiteit Amsterdam; Information Sciences Institute, University of Southern California",
        "aff_domain": "isi.edu;isi.edu;vu.nl;isi.edu",
        "email": "isi.edu;isi.edu;vu.nl;isi.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Southern California;Vrije Universiteit Amsterdam",
        "aff_unique_dep": "Information Sciences Institute;Department of Computer Science",
        "aff_unique_url": "https://www.usc.edu;https://www.vu.nl",
        "aff_unique_abbr": "USC;VU Amsterdam",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Netherlands"
    },
    {
        "id": "2024.naacl-long.362",
        "title": "ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In recent times, large language models (LLMs) have shown impressive performance on various document-level tasks such as document classification, summarization, and question-answering. However, research on understanding their capabilities on the task of self-contradictions in long documents has been very limited. In this work, we introduce ContraDoc, the first human-annotated dataset to study self-contradictions in long documents across multiple domains, varying document lengths, self-contradiction types, and appearance scope. We then analyze the current capabilities of four state-of-the-art open-source and commercially available LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4 performs the best and can outperform humans on this task, we find that it is still unreliable and struggles with self-contradictions that require more nuance and context. We release the dataset and all the code associated with the experiments.",
        "author": "Jierui Li; Vipul Raheja; Dhruv Kumar",
        "authorids": "/j/jierui-li/; /v/vipul-raheja/; /d/dhruv-kumar/",
        "bibtex": "@inproceedings{li-etal-2024-contradoc,\n    title = \"{C}ontra{D}oc: Understanding Self-Contradictions in Documents with Large Language Models\",\n    author = \"Li, Jierui  and\n      Raheja, Vipul  and\n      Kumar, Dhruv\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.362/\",\n    doi = \"10.18653/v1/2024.naacl-long.362\",\n    pages = \"6509--6523\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.362.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.362/",
        "pdf_size": 1584808,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13430131485422370350&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The University of Texas at Austin\u2663; Grammarly\u2662; Grammarly\u2662",
        "aff_domain": "cs.utexas.edu;grammarly.com;grammarly.com",
        "email": "cs.utexas.edu;grammarly.com;grammarly.com",
        "github": "https://github.com/ddhruvkr/CONTRADOC",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Texas at Austin;Grammarly",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utexas.edu;https://www.grammarly.com",
        "aff_unique_abbr": "UT Austin;Grammarly",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.350",
        "title": "ContraSim \u2013 Analyzing Neural Representations Based on Contrastive Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work has compared neural network representations via similarity-based analyses to improve model interpretation. The quality of a similarity measure is typically evaluated by its success in assigning a high score to representations that are expected to be matched. However, existing similarity measures perform mediocrely on standard benchmarks. In this work, we develop a new similarity measure, dubbed ContraSim, based on contrastive learning. In contrast to common closed-form similarity measures, ContraSim learns a parameterized measure by using both similar and dissimilar examples. We perform an extensive experimental evaluation of our method, with both language and vision models, on the standard layer prediction benchmark and two new benchmarks that we introduce: the multilingual benchmark and the image\u2013caption benchmark. In all cases, ContraSim achieves much higher accuracy than previous similarity measures, even when presented with challenging examples. Finally, ContraSim is more suitable for the analysis of neural networks, revealing new insights not captured by previous measures.",
        "author": "Adir Rahamim; Yonatan Belinkov",
        "authorids": "/a/adir-rahamim/; /y/yonatan-belinkov/",
        "bibtex": "@inproceedings{rahamim-belinkov-2024-contrasim,\n    title = \"{C}ontra{S}im {--} Analyzing Neural Representations Based on Contrastive Learning\",\n    author = \"Rahamim, Adir  and\n      Belinkov, Yonatan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.350/\",\n    doi = \"10.18653/v1/2024.naacl-long.350\",\n    pages = \"6325--6339\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.350.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.350/",
        "pdf_size": 715602,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10898954836490694770&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Technion \u2013 Israel Institute of Technology; Technion \u2013 Israel Institute of Technology",
        "aff_domain": "cs.technion.ac.il;technion.ac.il",
        "email": "cs.technion.ac.il;technion.ac.il",
        "github": "https://github.com/technion-cs-nlp/ContraSim",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion \u2013 Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2024.findings-naacl.293",
        "title": "Contrastive Learning as a Polarizer: Mitigating Gender Bias by Fair and Biased sentences",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently, language models have accelerated the improvement in natural language processing. However, recent studies have highlighted a significant issue: social biases inherent in training data can lead models to learn and propagate these biases. In this study, we propose a contrastive learning method for bias mitigation, utilizing anchor points to push further negatives and pull closer positives within the representation space. This approach employs stereotypical data as negatives and stereotype-free data as positives, enhancing debiasing performance. Our model attained state-of-the-art performance in the ICAT score on the StereoSet, a benchmark for measuring bias in models. In addition, we observed that effective debiasing is achieved through an awareness of biases, as evidenced by improved hate speech detection scores. The implementation code and trained models are available at https://github.com/HUFS-NLP/CL_Polarizer.git.",
        "author": "Kyungmin Park; Sihyun Oh; Daehyun Kim; Juae Kim",
        "authorids": "/k/kyungmin-park/; /s/sihyun-oh/; /d/daehyun-kim/; /j/juae-kim/",
        "bibtex": "@inproceedings{park-etal-2024-contrastive,\n    title = \"Contrastive Learning as a Polarizer: Mitigating Gender Bias by Fair and Biased sentences\",\n    author = \"Park, Kyungmin  and\n      Oh, Sihyun  and\n      Kim, Daehyun  and\n      Kim, Juae\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.293/\",\n    doi = \"10.18653/v1/2024.findings-naacl.293\",\n    pages = \"4725--4736\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.293.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.293/",
        "pdf_size": 235766,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5214460583039317023&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Hankuk University of Foreign Studies, Seoul, Korea; Hankuk University of Foreign Studies, Seoul, Korea; Hankuk University of Foreign Studies, Seoul, Korea; Hankuk University of Foreign Studies, Seoul, Korea",
        "aff_domain": "hufs.ac.kr;hufs.ac.kr;hufs.ac.kr;hufs.ac.kr",
        "email": "hufs.ac.kr;hufs.ac.kr;hufs.ac.kr;hufs.ac.kr",
        "github": "https://github.com/HUFS-NLP/CL_Polarizer.git",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Hankuk University of Foreign Studies",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.hufs.ac.kr",
        "aff_unique_abbr": "HUFS",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.findings-naacl.174",
        "title": "Contrastive Preference Learning for Neural Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "There exists a discrepancy between the token-level objective during training and the overall sequence-level quality that is expected from the model. This discrepancy leads to issues like exposure bias.To align the model with human expectations, sequence-level objectives are often used to fine-tune pre-trained models.In this paper, we introduce a contrastive preference model that enhances the traditional Plackett-Luce model by incorporating an indicator function. Building upon this novel preference model, we propose Contrastive Preference Learning (CPL), which uses offline samples with list-wise preferences to fine-tune a pre-trained model in Neural Machine Translation. Our experiments, conducted on three language pairs, demonstrate that CPL outperforms not only the vanilla Transformer model but also other token-level and sequence-level baselines. Furthermore, the ablation study highlights the essential role of the proposed indicator function in achieving this improvement.",
        "author": "Jianfei He; Shichao Sun; Sen Peng; Jie Xu; Xiaohua Jia; Wenjie Li",
        "authorids": "/j/jianfei-he/; /s/shichao-sun/; /s/sen-peng/; /j/jie-xu/; /x/xiaohua-jia/; /w/wenjie-li/",
        "bibtex": "@inproceedings{he-etal-2024-contrastive,\n    title = \"Contrastive Preference Learning for Neural Machine Translation\",\n    author = \"He, Jianfei  and\n      Sun, Shichao  and\n      Peng, Sen  and\n      Xu, Jie  and\n      Jia, Xiaohua  and\n      Li, Wenjie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.174/\",\n    doi = \"10.18653/v1/2024.findings-naacl.174\",\n    pages = \"2723--2735\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.174.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.174/",
        "pdf_size": 476909,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:HuIpyuJRs00J:scholar.google.com/&scioq=Contrastive+Preference+Learning+for+Neural+Machine+Translation&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.318",
        "title": "Contrastive and Consistency Learning for Neural Noisy-Channel Model in Spoken Language Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, deep end-to-end learning has been studied for intent classification in Spoken Language Understanding (SLU). However, end-to-end models require a large amount of speech data with intent labels, and highly optimized models are generally sensitive to the inconsistency between the training and evaluation conditions. Therefore, a natural language understanding approach based on Automatic Speech Recognition (ASR) remains attractive because it can utilize a pre-trained general language model and adapt to the mismatch of the speech input environment. Using this module-based approach, we improve a noisy-channel model to handle transcription inconsistencies caused by ASR errors. We propose a two-stage method, Contrastive and Consistency Learning (CCL), that correlates error patterns between clean and noisy ASR transcripts and emphasizes the consistency of the latent features of the two transcripts. Experiments on four benchmark datasets show that CCL outperforms existing methods and improves the ASR robustness in various noisy environments. Code is available at https://github.com/syoung7388/CCL",
        "author": "Suyoung Kim; Jiyeon Hwang; Ho-Young Jung",
        "authorids": "/s/suyoung-kim/; /j/jiyeon-hwang/; /h/ho-young-jung/",
        "bibtex": "@inproceedings{kim-etal-2024-contrastive,\n    title = \"Contrastive and Consistency Learning for Neural Noisy-Channel Model in Spoken Language Understanding\",\n    author = \"Kim, Suyoung  and\n      Hwang, Jiyeon  and\n      Jung, Ho-Young\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.318/\",\n    doi = \"10.18653/v1/2024.naacl-long.318\",\n    pages = \"5698--5711\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.318.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.318/",
        "pdf_size": 1434819,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:qCM15EvTRxgJ:scholar.google.com/&scioq=Contrastive+and+Consistency+Learning+for+Neural+Noisy-Channel+Model+in+Spoken+Language+Understanding&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Department of AI, Kyungpook National University; Department of AI, Kyungpook National University; Department of AI, Kyungpook National University",
        "aff_domain": "knu.ac.kr;knu.ac.kr;knu.ac.kr",
        "email": "knu.ac.kr;knu.ac.kr;knu.ac.kr",
        "github": "https://github.com/syoung7388/CCL",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Kyungpook National University",
        "aff_unique_dep": "Department of AI",
        "aff_unique_url": "http://www.knu.ac.kr",
        "aff_unique_abbr": "KNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-short.17",
        "title": "ContrastiveMix: Overcoming Code-Mixing Dilemma in Cross-Lingual Transfer for Information Retrieval",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Multilingual pretrained language models (mPLMs) have been widely adopted in cross-lingual transfer, and code-mixing has demonstrated effectiveness across various tasks in the absence of target language data. Our contribution involves an in-depth investigation into the counterproductive nature of training mPLMs on code-mixed data for information retrieval (IR). Our finding is that while code-mixing demonstrates a positive effect in aligning representations across languages, it hampers the IR-specific objective of matching representations between queries and relevant passages. To balance between positive and negative effects, we introduce ContrastiveMix, which disentangles contrastive loss between these conflicting objectives, thereby enhancing zero-shot IR performance. Specifically, we leverage both English and code-mixed data and employ two contrastive loss functions, by adding an additional contrastive loss that aligns embeddings of English data with their code-mixed counterparts in the query encoder. Our proposed ContrastiveMix exhibits statistically significant outperformance compared to mDPR, particularly in scenarios involving lower linguistic similarity, where the conflict between goals is more pronounced.",
        "author": "Junggeun Do; Jaeseong Lee; Seung-won Hwang",
        "authorids": "/j/junggeun-do/; /j/jaeseong-lee/; /s/seung-won-hwang/",
        "bibtex": "@inproceedings{do-etal-2024-contrastivemix,\n    title = \"{C}ontrastive{M}ix: Overcoming Code-Mixing Dilemma in Cross-Lingual Transfer for Information Retrieval\",\n    author = \"Do, Junggeun  and\n      Lee, Jaeseong  and\n      Hwang, Seung-won\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.17/\",\n    doi = \"10.18653/v1/2024.naacl-short.17\",\n    pages = \"197--204\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.17.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.17/",
        "pdf_size": 1307906,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=490571365548075495&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Seoul National University; Seoul National University; Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "github": "https://github.com/DoJunggeun/contrastivemix",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-short.42",
        "title": "Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "The Directed Acyclic Transformer is a fast non-autoregressive (NAR) model that performs well in Neural Machine Translation. Two issues prevent its application to general Natural Language Generation (NLG) tasks: frequent Out-Of-Vocabulary (OOV) errors and the inability to faithfully generate entity names. We introduce Control-DAG, a constrained decoding algorithm for our Directed Acyclic T5 (DA-T5) model which offers lexical, vocabulary and length control. We show that Control-DAG significantly enhances DA-T5 on the Schema Guided Dialogue and the DART datasets, establishing strong NAR results for Task-Oriented Dialogue and Data-to-Text NLG.",
        "author": "Jinghong Chen; Weizhe Lin; Jingbiao Mei; Bill Byrne",
        "authorids": "/j/jinghong-chen/; /w/weizhe-lin/; /j/jingbiao-mei/; /b/bill-byrne/",
        "bibtex": "@inproceedings{chen-etal-2024-control,\n    title = \"Control-{DAG}: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata\",\n    author = \"Chen, Jinghong  and\n      Lin, Weizhe  and\n      Mei, Jingbiao  and\n      Byrne, Bill\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.42/\",\n    doi = \"10.18653/v1/2024.naacl-short.42\",\n    pages = \"508--518\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.42.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.42/",
        "pdf_size": 586839,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7441018046707419294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "github": "https://github.com/EriChen0615/ControlDAG508",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-long.59",
        "title": "Corpus Considerations for Annotator Modeling and Scaling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent trends in natural language processing research and annotation tasks affirm a paradigm shift from the traditional reliance on a single ground truth to a focus on individual perspectives, particularly in subjective tasks. In scenarios where annotation tasks are meant to encompass diversity, models that solely rely on the majority class labels may inadvertently disregard valuable minority perspectives. This oversight could result in the omission of crucial information and, in a broader context, risk disrupting the balance within larger ecosystems. As the landscape of annotator modeling unfolds with diverse representation techniques, it becomes imperative to investigate their effectiveness with the fine-grained features of the datasets in view. This study systematically explores various annotator modeling techniques and compares their performance across seven corpora. From our findings, we show that the commonly used user token model consistently outperforms more complex models. We introduce a composite embedding approach and show distinct differences in which model performs best as a function of the agreement with a given dataset. Our findings shed light on the relationship between corpus statistics and annotator modeling performance, which informs future work on corpus construction and perspectivist NLP.",
        "author": "Olufunke O. Sarumi; B\u00e9la Neuendorf; Joan Plepi; Lucie Flek; J\u00f6rg Schl\u00f6tterer; Charles Welch",
        "authorids": "/o/olufunke-o-sarumi/; /b/bela-neuendorf/; /j/joan-plepi/; /l/lucie-flek/; /j/jorg-schlotterer/; /c/charles-welch/",
        "bibtex": "@inproceedings{sarumi-etal-2024-corpus,\n    title = \"Corpus Considerations for Annotator Modeling and Scaling\",\n    author = {Sarumi, Olufunke O.  and\n      Neuendorf, B{\\'e}la  and\n      Plepi, Joan  and\n      Flek, Lucie  and\n      Schl{\\\"o}tterer, J{\\\"o}rg  and\n      Welch, Charles},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.59/\",\n    doi = \"10.18653/v1/2024.naacl-long.59\",\n    pages = \"1029--1040\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.59.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.59/",
        "pdf_size": 383424,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3353663545120719202&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Mathematics and Computer Science, University of Marburg+Area Information Systems, University of Mannheim; Department of Mathematics and Computer Science, University of Marburg+Area Information Systems, University of Mannheim; Bonn-Aachen International Center for Information Technology (b-it), University of Bonn; Bonn-Aachen International Center for Information Technology (b-it), University of Bonn; Department of Mathematics and Computer Science, University of Marburg+Area Information Systems, University of Mannheim; Bonn-Aachen International Center for Information Technology (b-it), University of Bonn",
        "aff_domain": "uni-marburg.de;uni-marburg.de;bit.uni-bonn.de;bit.uni-bonn.de;uni-marburg.de;bit.uni-bonn.de",
        "email": "uni-marburg.de;uni-marburg.de;bit.uni-bonn.de;bit.uni-bonn.de;uni-marburg.de;bit.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;2;2;0+1;2",
        "aff_unique_norm": "University of Marburg;University of Mannheim;University of Bonn",
        "aff_unique_dep": "Department of Mathematics and Computer Science;Area Information Systems;Bonn-Aachen International Center for Information Technology (b-it)",
        "aff_unique_url": "https://www.uni-marburg.de;https://www.uni-mannheim.de;https://www.uni-bonn.de",
        "aff_unique_abbr": ";;Uni Bonn",
        "aff_campus_unique_index": ";;1;1;;1",
        "aff_campus_unique": ";Bonn",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.findings-naacl.133",
        "title": "Crafting In-context Examples according to LMs\u2019 Parametric Knowledge",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In-context learning can improve the performances of knowledge-rich tasks such as question answering. In such scenarios, in-context examples trigger a language model (LM) to surface information stored in its parametric knowledge. We study how to better construct in-context example sets, based on whether the model is aware of the in-context examples. We identify \u2018known\u2019 examples, where models can correctly answer from their parametric knowledge, and \u2018unknown\u2019 ones. Our experiments show that prompting with \u2018unknown\u2019 examples decreases the performance, potentially as it encourages hallucination rather than searching for its parametric knowledge. Constructing an in-context example set that presents both known and unknown information performs the best across diverse settings. We perform analysis on three multi-answer question answering datasets, which allows us to further study answer set ordering strategies based on the LM\u2019s knowledge of each answer. Together, our study sheds light on how to best construct in-context example sets for knowledge-rich tasks.",
        "author": "Yoonsang Lee; Pranav Atreya; Xi Ye; Eunsol Choi",
        "authorids": "/y/yoonsang-lee/; /p/pranav-atreya/; /x/xi-ye/; /e/eunsol-choi/",
        "bibtex": "@inproceedings{lee-etal-2024-crafting,\n    title = \"Crafting In-context Examples according to {LM}s' Parametric Knowledge\",\n    author = \"Lee, Yoonsang  and\n      Atreya, Pranav  and\n      Ye, Xi  and\n      Choi, Eunsol\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.133/\",\n    doi = \"10.18653/v1/2024.findings-naacl.133\",\n    pages = \"2069--2085\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.133.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.133/",
        "pdf_size": 1998067,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9825028859538349605&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "The University of Texas at Austin+Seoul National University; UC Berkeley; The University of Texas at Austin; The University of Texas at Austin",
        "aff_domain": "snu.ac.kr;berkeley.edu;cs.utexas.edu;utexas.edu",
        "email": "snu.ac.kr;berkeley.edu;cs.utexas.edu;utexas.edu",
        "github": "https://github.com/lilys012/known_examples",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2;0;0",
        "aff_unique_norm": "University of Texas at Austin;Seoul National University;University of California, Berkeley",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utexas.edu;https://www.snu.ac.kr;https://www.berkeley.edu",
        "aff_unique_abbr": "UT Austin;SNU;UC Berkeley",
        "aff_campus_unique_index": "0;2;0;0",
        "aff_campus_unique": "Austin;;Berkeley",
        "aff_country_unique_index": "0+1;0;0;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "2024.naacl-long.49",
        "title": "Create! Don\u2019t Repeat: A Paradigm Shift in Multi-Label Augmentation through Label Creative Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We propose Label Creative Generation (LCG), a new paradigm in multi-label data augmentation. Beyond repeating data points with fixed labels, LCG creates new data by exploring innovative label combinations. Within LCG, we introduce Tail-Driven Conditional Augmentation (TDCA), combining tail-driven label sampling and label-conditioned text generation for balanced, consistent data augmentation. Our approach has demonstrated a **100.21%** increase in PSP@1 across three datasets, successfully mitigating the long-tail effect in MLTC and markedly enhancing model performance.",
        "author": "Letian Wang; Xianggen Liu; Jiancheng Lv",
        "authorids": "/l/letian-wang/; /x/xianggen-liu/; /j/jiancheng-lv/",
        "bibtex": "@inproceedings{wang-etal-2024-create,\n    title = \"Create! Don{'}t Repeat: A Paradigm Shift in Multi-Label Augmentation through Label Creative Generation\",\n    author = \"Wang, Letian  and\n      Liu, Xianggen  and\n      Lv, Jiancheng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.49/\",\n    doi = \"10.18653/v1/2024.naacl-long.49\",\n    pages = \"855--869\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.49.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.49/",
        "pdf_size": 13436723,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9966437095490523182&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer Science, Sichuan University, China+Engineering Research Center of Machine Learning and Industry Intelligence, China; College of Computer Science, Sichuan University, China+Engineering Research Center of Machine Learning and Industry Intelligence, China; College of Computer Science, Sichuan University, China+Engineering Research Center of Machine Learning and Industry Intelligence, China",
        "aff_domain": "stu.scu.edu.cn;scu.edu.cn;scu.edu.cn",
        "email": "stu.scu.edu.cn;scu.edu.cn;scu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "Sichuan University;Engineering Research Center of Machine Learning and Industry Intelligence",
        "aff_unique_dep": "College of Computer Science;",
        "aff_unique_url": "https://www.scu.edu.cn;",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.289",
        "title": "Cross-Lingual Summarization with Pseudo-Label Regularization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Cross-Lingual Summarization (XLS) aims to summarize a document in the source language into a condensed version in the target language, effectively removing language barriers for non-native readers. Previous approaches, however, have the same limitation that only a single reference (gold summary) is exploited during model training, making the base model exposed to an underrepresented hypothesis space since the actual number of possible hypotheses is exponentially large. To alleviate this problem, we present a study adopting pseudo-labels in regularizing standard cross-lingual summarization training. We investigate several components leading to the gains in regularization training with verified experiments involving 8 diverse languages from different families. Conclusively, we show that pseudo-labeling is a simple and effective approach that significantly improves over standard gold reference training in XLS.",
        "author": "Thang Le",
        "authorids": "/t/thang-le/",
        "bibtex": "@inproceedings{le-2024-cross,\n    title = \"Cross-Lingual Summarization with Pseudo-Label Regularization\",\n    author = \"Le, Thang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.289/\",\n    doi = \"10.18653/v1/2024.findings-naacl.289\",\n    pages = \"4644--4677\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.289.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.289/",
        "pdf_size": 559643,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12133914576258666936&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "VinAI Research",
        "aff_domain": "vinai.io",
        "email": "vinai.io",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "VinAI Research",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.vinai.io/",
        "aff_unique_abbr": "VinAI",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Vietnam"
    },
    {
        "id": "2024.findings-naacl.182",
        "title": "Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 tasks and 31 metrics. We observe that finetuning can help LLMs transfer knowledge across languages, serving as an efficient way to bolster their capabilities in non-English languages. Moreover, our analysis indicates that larger models can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or finetuning datasets. These insights underscore the significance of meticulous finetuning with high-quality datasets in enhancing LLM performance.",
        "author": "Sang Truong; Duc Nguyen; Toan Nguyen; Dong Le; Nhi Truong; Tho Quan; Sanmi Koyejo",
        "authorids": "/s/sang-truong/; /d/duc-nguyen/; /t/toan-q-nguyen/; /d/dong-le/; /n/nhi-truong/; /t/tho-quan/; /s/sanmi-koyejo/",
        "bibtex": "@inproceedings{truong-etal-2024-crossing,\n    title = \"Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of {V}ietnamese Large Language Models\",\n    author = \"Truong, Sang  and\n      Nguyen, Duc  and\n      Nguyen, Toan  and\n      Le, Dong  and\n      Truong, Nhi  and\n      Quan, Tho  and\n      Koyejo, Sanmi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.182/\",\n    doi = \"10.18653/v1/2024.findings-naacl.182\",\n    pages = \"2849--2900\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.182.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.182/",
        "pdf_size": 955783,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8774323055836746387&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Stanford University; Ho Chi Minh City University of Technology, VNU-HCM; Ho Chi Minh City University of Technology, VNU-HCM; Ho Chi Minh City University of Technology, VNU-HCM; Stanford University+Ho Chi Minh City University of Technology, VNU-HCM; Ho Chi Minh City University of Technology, VNU-HCM; Stanford University",
        "aff_domain": "cs.stanford.edu;hcmut.edu.vn; ; ; ; ; ",
        "email": "cs.stanford.edu;hcmut.edu.vn; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;0+1;1;0",
        "aff_unique_norm": "Stanford University;Ho Chi Minh City University of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.hcmut.edu.vn",
        "aff_unique_abbr": "Stanford;HCMUT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;1;1;1;0+1;1;0",
        "aff_country_unique": "United States;Vietnam"
    },
    {
        "id": "2024.naacl-long.156",
        "title": "Curated Datasets and Neural Models for Machine Translation of Informal Registers between Mayan and Spanish Vernaculars",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The Mayan languages comprise a language family with an ancient history, millions of speakers, and immense cultural value, that, nevertheless, remains severely underrepresented in terms of resources and global exposure. In this paper we develop, curate, and publicly release a set of corpora in several Mayan languages spoken in Guatemala and Southern Mexico, which we call MayanV. The datasets are parallel with Spanish, the dominant language of the region, and are taken from official native sources focused on representing informal, day-to-day, and non-domain-specific language. As such, and according to our dialectometric analysis, they differ in register from most other available resources. Additionally, we present neural machine translation models, trained on as many resources and Mayan languages as possible, and evaluated exclusively on our datasets. We observe lexical divergences between the dialects of Spanish in our resources and the more widespread written standard of Spanish, and that resources other than the ones we present do not seem to improve translation performance, indicating that many such resources may not accurately capture common, real-life language usage. The MayanV dataset is available at https://github.com/transducens/mayanv.",
        "author": "Andr\u00e9s Lou; Juan Antonio P\u00e9rez-Ortiz; Felipe S\u00e1nchez-Mart\u00ednez; V\u00edctor S\u00e1nchez-Cartagena",
        "authorids": "/a/andres-lou/; /j/juan-antonio-perez-ortiz/; /f/felipe-sanchez-martinez/; /v/victor-sanchez-cartagena/",
        "bibtex": "@inproceedings{lou-etal-2024-curated,\n    title = \"Curated Datasets and Neural Models for Machine Translation of Informal Registers between {M}ayan and {S}panish Vernaculars\",\n    author = \"Lou, Andr{\\'e}s  and\n      P{\\'e}rez-Ortiz, Juan Antonio  and\n      S{\\'a}nchez-Mart{\\'i}nez, Felipe  and\n      S{\\'a}nchez-Cartagena, V{\\'i}ctor\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.156/\",\n    doi = \"10.18653/v1/2024.naacl-long.156\",\n    pages = \"2838--2850\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.156.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.156/",
        "pdf_size": 741041,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6707417987498038723&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/transducens/mayanv",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-long.203",
        "title": "Curriculum Masking in Vision-Language Pretraining to Maximize Cross Modal Interaction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Many leading methods in Vision and language (V+L) pretraining utilize masked language modeling (MLM) as a standard pretraining component, with the expectation that reconstruction of masked text tokens would necessitate reference to corresponding image context via cross/self attention and thus promote representation fusion. However, we observe that the minimization of MLM loss in earlier training stages can depend disproportionately on local text signals, leading to poor training efficiency and inconsistency with the goal of representation fusion. The extent of this lack of cross modal interaction depends strongly which token(s) are masked. To address this issue, we propose a curriculum masking scheme as a replacement for random masking. Tokens are selected to be masked at a frequency proportional to the expected level of cross modal interaction necessary to reconstruct them. This is achieved using a parallel mask selection agent that measures the cross modal flow of information and treats it as a reward to be maximized. By additionally masking contiguous spans that include key objects and their relations, we also achieve better relational understanding, which has been shown to be lacking in many SOTA models. Our experiments on a wide range of V+L tasks show that we trail closely behind state-of-the-art methods despite pretraining on 300x to 1000x less data and we also achieve either top or runner-up performance on tasks from the ARO benchmark which tests compositional relationships. Finally, we demonstrate the potential of our method to scale to larger pretraining data.",
        "author": "Kraig Tou; Zijun Sun",
        "authorids": "/k/kraig-tou/; /z/zijun-sun/",
        "bibtex": "@inproceedings{tou-sun-2024-curriculum,\n    title = \"Curriculum Masking in Vision-Language Pretraining to Maximize Cross Modal Interaction\",\n    author = \"Tou, Kraig  and\n      Sun, Zijun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.203/\",\n    doi = \"10.18653/v1/2024.naacl-long.203\",\n    pages = \"3672--3688\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.203.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.203/",
        "pdf_size": 21485615,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12328739983892341869&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Georgia Tech; University of Bologna",
        "aff_domain": "gatech.edu;studio.unibo.it",
        "email": "gatech.edu;studio.unibo.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Georgia Institute of Technology;University of Bologna",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.gatech.edu;https://www.unibo.it",
        "aff_unique_abbr": "Georgia Tech;Unibo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Italy"
    },
    {
        "id": "2024.findings-naacl.120",
        "title": "DAGCN: Distance-based and Aspect-oriented Graph Convolutional Network for Aspect-based Sentiment Analysis",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Aspect-based sentiment analysis (ABSA) is a task that aims to determine the sentiment polarity of aspects by identifying opinion words. Recent advancements have predominantly been rooted either in semantic or syntactic methods. However, both of them tend to interference from local factors such as irrelevant words and edges, hindering the precise identification of opinion words. In this paper, we present Distance-based and Aspect-oriented Graph Convolutional Network (DAGCN) to address the aforementioned issue. Firstly, we introduce the Distance-based Syntactic Weight (DSW). It focuses on the local scope of aspects in the pruned dependency trees, thereby reducing the candidate pool of opinion words. Additionally, we propose Aspect-Fusion Attention (AF) to further filter opinion words within the local context and consider cases where opinion words are distant from the aspect. With the combination of DSW and AF, we achieve precise identification of corresponding opinion words. Extensive experiments on three public datasets demonstrate that the proposed model outperforms state-of-the-art models and verify the effectiveness of the proposed architecture.",
        "author": "Zhihao Wang; Bo Zhang; Ru Yang; Chang Guo; Maozhen Li",
        "authorids": "/z/zhihao-wang/; /b/bo-zhang/; /r/ru-yang/; /c/chang-guo/; /m/maozhen-li/",
        "bibtex": "@inproceedings{wang-etal-2024-dagcn,\n    title = \"{DAGCN}: Distance-based and Aspect-oriented Graph Convolutional Network for Aspect-based Sentiment Analysis\",\n    author = \"Wang, Zhihao  and\n      Zhang, Bo  and\n      Yang, Ru  and\n      Guo, Chang  and\n      Li, Maozhen\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.120/\",\n    doi = \"10.18653/v1/2024.findings-naacl.120\",\n    pages = \"1863--1876\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.120.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.120/",
        "pdf_size": 2338796,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16291394616210959667&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "1College of Information, Mechanical and Electrical Engineering, Shanghai Normal University, China; 1College of Information, Mechanical and Electrical Engineering, Shanghai Normal University, China + 2Shanghai Engineering Research Center of Intelligent Education and Bigdata, Shanghai Normal University, China; 1College of Information, Mechanical and Electrical Engineering, Shanghai Normal University, China; 1College of Information, Mechanical and Electrical Engineering, Shanghai Normal University, China; 1College of Information, Mechanical and Electrical Engineering, Shanghai Normal University, China + 3Department of Electronic and Electrical Engineering, Brunel University London, Uxbridge, UK",
        "aff_domain": "gmail.com;shnu.edu.cn;shnu.edu.cn;shnu.edu.cn;brunel.ac.uk",
        "email": "gmail.com;shnu.edu.cn;shnu.edu.cn;shnu.edu.cn;brunel.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+0;0;0;0+1",
        "aff_unique_norm": "Shanghai Normal University;Brunel University London",
        "aff_unique_dep": "College of Information, Mechanical and Electrical Engineering;Department of Electronic and Electrical Engineering",
        "aff_unique_url": "http://www.shnu.edu.cn;https://www.brunel.ac.uk",
        "aff_unique_abbr": ";Brunel",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Shanghai;Uxbridge",
        "aff_country_unique_index": "0;0+0;0;0;0+1",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2024.findings-naacl.9",
        "title": "DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Encoder-decoder transformer models have achieved great success on various vision-language (VL) and language tasks, but they suffer from high inference latency. Typically, the decoder takes up most of the latency because of the auto-regressive decoding. To accelerate the inference, we propose an approach of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit encoder-decoder transformer model which is trained with deep supervision so that each of its decoder layers is capable of generating plausible predictions. In addition, we leverage simple yet practical techniques, including shared generation head and adaptation modules, to keep accuracy when exiting at shallow decoder layers. Based on the multi-exit model, we perform step-level dynamic early exit during inference, where the model may decide to use fewer decoder layers based on its confidence of the current layer at each individual decoding step. Considering different number of decoder layers may be used at different decoding steps, we compute deeper-layer decoder features of previous decoding steps just-in-time, which ensures the features from different decoding steps are semantically aligned. We evaluate our approach with three state-of-the-art encoder-decoder transformer models on various VL and language tasks. We show our approach can reduce overall inference latency by 20%-74% with comparable or even higher accuracy compared to baselines.",
        "author": "Peng Tang; Pengkai Zhu; Tian Li; Srikar Appalaraju; Vijay Mahadevan; R. Manmatha",
        "authorids": "/p/peng-tang/; /p/pengkai-zhu/; /t/tian-li/; /s/srikar-appalaraju/; /v/vijay-mahadevan/; /r/r-manmatha/",
        "bibtex": "@inproceedings{tang-etal-2024-deed,\n    title = \"{DEED}: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models\",\n    author = \"Tang, Peng  and\n      Zhu, Pengkai  and\n      Li, Tian  and\n      Appalaraju, Srikar  and\n      Mahadevan, Vijay  and\n      Manmatha, R.\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.9/\",\n    doi = \"10.18653/v1/2024.findings-naacl.9\",\n    pages = \"116--131\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.9.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.9/",
        "pdf_size": 966235,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7929139956350328846&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "AWS AI Labs\u2020; AWS AI Labs\u2020; University of California San Diego\u2021; AWS AI Labs\u2020; AWS AI Labs\u2020; AWS AI Labs\u2020",
        "aff_domain": "amazon.com;amazon.com;ucsd.edu;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;ucsd.edu;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "Amazon;University of California, San Diego",
        "aff_unique_dep": "AWS AI Labs;",
        "aff_unique_url": "https://aws.amazon.com;https://ucsd.edu",
        "aff_unique_abbr": "AWS;UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.21",
        "title": "DEMO: A Statistical Perspective for Efficient Image-Text Matching",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Image-text matching has been a long-standing problem, which seeks to connect vision and language through semantic understanding. Due to the capability to manage large-scale raw data, unsupervised hashing-based approaches have gained prominence recently. They typically construct a semantic similarity structure using the natural distance, which subsequently guides the optimization of the hashing network. However, the similarity structure could be biased at the boundaries of semantic distributions, causing error accumulation during sequential optimization. To tackle this, we introduce a novel hashing approach termed Distribution-based Structure Mining with Consistency Learning (DEMO) for efficient image-text matching. From a statistical view, DEMO characterizes each image using multiple augmented views, which are considered as samples drawn from its intrinsic semantic distribution. Then, we employ a non-parametric distribution divergence to ensure a robust and precise similarity structure. In addition, we introduce collaborative consistency learning which not only preserves the similarity structure in the Hamming space but also encourages consistency between retrieval distribution from different directions in a self-supervised manner. Extensive experiments on several widely used datasets demonstrate that DEMO achieves superior performance compared with various state-of-the-art methods.",
        "author": "Fan Zhang; Xian-Sheng Hua; Chong Chen; Xiao Luo",
        "authorids": "/f/fan-zhang/; /x/xian-sheng-hua/; /c/chong-chen/; /x/xiao-luo/",
        "bibtex": "@inproceedings{zhang-etal-2024-demo,\n    title = \"{DEMO}: A Statistical Perspective for Efficient Image-Text Matching\",\n    author = \"Zhang, Fan  and\n      Hua, Xian-Sheng  and\n      Chen, Chong  and\n      Luo, Xiao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.21/\",\n    doi = \"10.18653/v1/2024.naacl-long.21\",\n    pages = \"355--369\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.21.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.21/",
        "pdf_size": 6770259,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17334635495636899732&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Georgia Tech Shenzhen Institute, Tianjin University (GTSI); Terminus Group+University of California, Los Angeles; Terminus Group+University of California, Los Angeles; University of California, Los Angeles",
        "aff_domain": "gatech.edu;gmail.com;gmail.com;cs.ucla.edu",
        "email": "gatech.edu;gmail.com;gmail.com;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+2;1+2;2",
        "aff_unique_norm": "Georgia Tech Shenzhen Institute;Terminus Group;University of California, Los Angeles",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.gtshenzhen.org/;;https://www.ucla.edu",
        "aff_unique_abbr": "GTSI;;UCLA",
        "aff_campus_unique_index": "0;2;2;2",
        "aff_campus_unique": "Shenzhen;;Los Angeles",
        "aff_country_unique_index": "0;2;2;2",
        "aff_country_unique": "China;;United States"
    },
    {
        "id": "2024.naacl-demo.4",
        "title": "DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We present DIALIGHT, a toolkit for developing and evaluating multilingual Task-Oriented Dialogue (ToD) systems which facilitates systematic evaluations and comparisons between ToD systems using fine-tuning of Pretrained Language Models (PLMs) and those utilising the zero-shot and in-context learning capabilities of Large Language Models (LLMs). In addition to automatic evaluation, this toolkit features (i) a secure, user-friendly web interface for fine-grained human evaluation at both local utterance level and global dialogue level, and (ii) a microservice-based backend, improving efficiency and scalability. Our evaluations reveal that while PLM fine-tuning leads to higher accuracy and coherence, LLM-based systems excel in producing diverse and likeable responses. However, we also identify significant challenges of LLMs in adherence to task-specific instructions and generating outputs in multiple languages, highlighting areas for future research. We hope this open-sourced toolkit will serve as a valuable resource for researchers aiming to develop and properly evaluate multilingual ToD systems and will lower, currently still high, entry barriers in the field.",
        "author": "Songbo Hu; Xiaobin Wang; Moy Yuan; Anna Korhonen; Ivan Vuli\u0107",
        "authorids": "/s/songbo-hu/; /x/xiaobin-wang/; /m/moy-yuan/; /a/anna-korhonen/; /i/ivan-vulic/",
        "bibtex": "@inproceedings{hu-etal-2024-dialight,\n    title = \"{DIALIGHT}: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models\",\n    author = \"Hu, Songbo  and\n      Wang, Xiaobin  and\n      Yuan, Moy  and\n      Korhonen, Anna  and\n      Vuli{\\'c}, Ivan\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.4/\",\n    doi = \"10.18653/v1/2024.naacl-demo.4\",\n    pages = \"36--52\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.4.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.4/",
        "pdf_size": 1237734,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1911480240915318772&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2024.findings-naacl.5",
        "title": "DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrievalaugmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) Generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.",
        "author": "Wenting Zhao; Ye Liu; Tong Niu; Yao Wan; Philip Yu; Shafiq Joty; Yingbo Zhou; Semih Yavuz",
        "authorids": "/w/wenting-zhao/; /y/ye-liu/; /t/tong-niu/; /y/yao-wan/; /p/philip-s-yu/; /s/shafiq-joty/; /y/yingbo-zhou/; /s/semih-yavuz/",
        "bibtex": "@inproceedings{zhao-etal-2024-divknowqa,\n    title = \"{DIVKNOWQA}: Assessing the Reasoning Ability of {LLM}s via Open-Domain Question Answering over Knowledge Base and Text\",\n    author = \"Zhao, Wenting  and\n      Liu, Ye  and\n      Niu, Tong  and\n      Wan, Yao  and\n      Yu, Philip  and\n      Joty, Shafiq  and\n      Zhou, Yingbo  and\n      Yavuz, Semih\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.5/\",\n    doi = \"10.18653/v1/2024.findings-naacl.5\",\n    pages = \"51--68\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.5.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.5/",
        "pdf_size": 925545,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15794572128017041467&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2024.naacl-long.294",
        "title": "DLM: A Decoupled Learning Model for Long-tailed Polyphone Disambiguation in Mandarin",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Grapheme-to-phoneme conversion (G2P) is a critical component of the text-to-speech system (TTS), where polyphone disambiguation is the most crucial task. However, polyphone disambiguation datasets often suffer from the long-tail problem, and context learning for polyphonic characters commonly stems from a single dimension. In this paper, we propose a novel model DLM: a Decoupled Learning Model for long-tailed polyphone disambiguation in Mandarin. Firstly, DLM decouples representation and classification learnings. It can apply different data samplers for each stage to obtain an optimal training data distribution. This can mitigate the long-tail problem. Secondly, two improved attention mechanisms and a gradual conversion strategy are integrated into the DLM, which achieve transition learning of context from local to global. Finally, to evaluate the effectiveness of DLM, we construct a balanced polyphone disambiguation corpus via in-context learning. Experiments on the benchmark CPP dataset demonstrate that DLM achieves a boosted accuracy of 99.07%. Moreover, DLM improves the disambiguation performance of long-tailed polyphonic characters. For many long-tailed characters, DLM even achieves an accuracy of 100%.",
        "author": "Beibei Gao; Yangsen Zhang; Ga Xiang; Yushan Jiang",
        "authorids": "/b/beibei-gao/; /y/yangsen-zhang/; /g/ga-xiang/; /y/yushan-jiang/",
        "bibtex": "@inproceedings{gao-etal-2024-dlm,\n    title = \"{DLM}: A Decoupled Learning Model for Long-tailed Polyphone Disambiguation in {M}andarin\",\n    author = \"Gao, Beibei  and\n      Zhang, Yangsen  and\n      Xiang, Ga  and\n      Jiang, Yushan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.294/\",\n    doi = \"10.18653/v1/2024.naacl-long.294\",\n    pages = \"5252--5262\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.294.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.294/",
        "pdf_size": 968845,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:8alqDecWO6gJ:scholar.google.com/&scioq=DLM:+A+Decoupled+Learning+Model+for+Long-tailed+Polyphone+Disambiguation+in+Mandarin&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "School of Information Management, Beijing Information Science and Technology University, China; School of Information Management, Beijing Information Science and Technology University, China; School of Information Management, Beijing Information Science and Technology University, China; School of Information Management, Beijing Information Science and Technology University, China",
        "aff_domain": "bistu.edu.cn;bistu.edu.cn;bistu.edu.cn;bistu.edu.cn",
        "email": "bistu.edu.cn;bistu.edu.cn;bistu.edu.cn;bistu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Beijing Information Science and Technology University",
        "aff_unique_dep": "School of Information Management",
        "aff_unique_url": "http://www.bistu.edu.cn",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-demo.13",
        "title": "DOCMASTER: A Unified Platform for Annotation, Training, & Inference in Document Question-Answering",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "The application of natural language processing models to PDF documents is pivotal for various business applications yet the challenge of training models for this purpose persists in businesses due to specific hurdles. These include the complexity of working with PDF formats that necessitate parsing text and layout information for curating training data and the lack of privacy-preserving annotation tools. This paper introduces DOCMASTER, a unified platform designed for annotating PDF documents, model training, and inference, tailored to document question-answering. The annotation interface enables users to input questions and highlight text spans within the PDF file as answers, saving layout information and text spans accordingly. Furthermore, DOCMASTER supports both state-of-the-art layout-aware and text models for comprehensive training purposes. Importantly, as annotations, training, and inference occur on-device, it also safeguards privacy. The platform has been instrumental in driving several research prototypes concerning document analysis such as the AI assistant utilized by University of California San Diego\u2019s (UCSD) International Services and Engagement Office (ISEO) for processing a substantial volume of PDF documents.",
        "author": "Alex Nguyen; Zilong Wang; Jingbo Shang; Dheeraj Mekala",
        "authorids": "/a/alex-nguyen/; /z/zilong-wang/; /j/jingbo-shang/; /d/dheeraj-mekala/",
        "bibtex": "@inproceedings{nguyen-etal-2024-docmaster,\n    title = \"{DOCMASTER}: A Unified Platform for Annotation, Training, {\\&} Inference in Document Question-Answering\",\n    author = \"Nguyen, Alex  and\n      Wang, Zilong  and\n      Shang, Jingbo  and\n      Mekala, Dheeraj\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.13/\",\n    doi = \"10.18653/v1/2024.naacl-demo.13\",\n    pages = \"128--136\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.13.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.13/",
        "pdf_size": 3898122,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4731718197753009922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of California San Diego; University of California San Diego; University of California San Diego + Hal\u0131c\u0131o\u011flu Data Science Institute, University of California San Diego; University of California San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0+0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0+0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.413",
        "title": "DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "State-of-the-art neural rankers pre-trained on large task-specific training data such as MS-MARCO, have been shown to exhibit strong performance on various ranking tasks without domain adaptation, also called zero-shot. However, zero-shot neural ranking may be sub-optimal, as it does not take advantage of the target domain information. Unfortunately, acquiring sufficiently large and high quality target training data to improve a modern neural ranker can be costly and time-consuming. To address this problem, we propose a new approach to unsupervised domain adaptation for ranking, DUQGen, which addresses a critical gap in prior literature, namely how to automatically generate both effective and diverse synthetic training data to fine tune a modern neural ranker for a new domain. Specifically, DUQGen produces a more effective representation of the target domain by identifying clusters of similar documents; and generates a more diverse training dataset by probabilistic sampling over the resulting document clusters. Our extensive experiments, over the standard BEIR collection, demonstrate that DUQGen consistently outperforms all zero-shot baselines and substantially outperforms the SOTA baselines on 16 out of 18 datasets, for an average of 4% relative improvement across all datasets. We complement our results with a thorough analysis for more in-depth understanding of the proposed method\u2019s performance and to identify promising areas for further improvements.",
        "author": "Ramraj Chandradevan; Kaustubh Dhole; Eugene Agichtein",
        "authorids": "/r/ramraj-chandradevan/; /k/kaustubh-dhole/; /e/eugene-agichtein/",
        "bibtex": "@inproceedings{chandradevan-etal-2024-duqgen,\n    title = \"{DUQG}en: Effective Unsupervised Domain Adaptation of Neural Rankers by Diversifying Synthetic Query Generation\",\n    author = \"Chandradevan, Ramraj  and\n      Dhole, Kaustubh  and\n      Agichtein, Eugene\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.413/\",\n    doi = \"10.18653/v1/2024.naacl-long.413\",\n    pages = \"7437--7451\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.413.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.413/",
        "pdf_size": 1319004,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10641714291265044538&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Emory University; Department of Computer Science, Emory University; Department of Computer Science, Emory University",
        "aff_domain": "emory.edu;emory.edu;emory.edu",
        "email": "emory.edu;emory.edu;emory.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Emory University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.emory.edu",
        "aff_unique_abbr": "Emory",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.464",
        "title": "David helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion LMs",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Diffusion-based language models are emerging as a promising alternative to autoregressive LMs: they approach the competence of autoregressive LMs while offering nuanced controllability at inference time. While autoregressive LMs have benefited immensely from scaling and instruction-based learning, existing studies of diffusion LMs have been conducted on a smaller scale. Starting with a recently proposed diffusion model SSD-LM, in this work we first explore methods to scale it from 0.4B to 13B parameters, proposing techniques to improve its training and inference efficiency, and to finetune the model to follow instructions. Armed with a more powerful, general purpose diffusion LM, we introduce the primary contribution of this work \u2013 SSD-2 \u2013 an approach to easily ensemble at inference time a large general-purpose diffusion LM with smaller, but specialized and contextualized diffusion LMs. We show that SSD-2 facilitates novel ensembles with 100x smaller models that can be customized and deployed by individual users. We find that compared to autoregressive models, the collaboration between diffusion LMs is more effective, leading to higher-quality model responses due to their ability to dynamically incorporate bi-directional contexts.",
        "author": "Xiaochuang Han; Sachin Kumar; Yulia Tsvetkov; Marjan Ghazvininejad",
        "authorids": "/x/xiaochuang-han/; /s/sachin-kumar/; /y/yulia-tsvetkov/; /m/marjan-ghazvininejad/",
        "bibtex": "@inproceedings{han-etal-2024-david,\n    title = \"{D}avid helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion {LM}s\",\n    author = \"Han, Xiaochuang  and\n      Kumar, Sachin  and\n      Tsvetkov, Yulia  and\n      Ghazvininejad, Marjan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.464/\",\n    doi = \"10.18653/v1/2024.naacl-long.464\",\n    pages = \"8385--8400\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.464.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.464/",
        "pdf_size": 679277,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4638292743397594508&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Washington\u2660Meta AI\u2663Carnegie Mellon University; Carnegie Mellon University; University of Washington; Meta AI",
        "aff_domain": "cs.washington.edu;cs.cmu.edu;cs.washington.edu;meta.com",
        "email": "cs.washington.edu;cs.cmu.edu;cs.washington.edu;meta.com",
        "github": "https://github.com/xhan77/ssd-2",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of Washington;Carnegie Mellon University;Meta",
        "aff_unique_dep": ";;Meta AI",
        "aff_unique_url": "https://www.washington.edu;https://www.cmu.edu;https://meta.com",
        "aff_unique_abbr": "UW;CMU;Meta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.412",
        "title": "DeMuX: Data-efficient Multilingual Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained multilingual models have enabled deployment of NLP technologies for multiple languages. However, optimally fine-tuning these models under an annotation budget, such that performance on desired target languages is jointly maximized, still remains an open question. In this paper, we introduce DeMuX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points. Our code is released here: https://github.com/simran-khanuja/demux.",
        "author": "Simran Khanuja; Srinivas Gowriraj; Lucio Dery; Graham Neubig",
        "authorids": "/s/simran-khanuja/; /s/srinivas-gowriraj/; /l/lucio-dery/; /g/graham-neubig/",
        "bibtex": "@inproceedings{khanuja-etal-2024-demux,\n    title = \"{D}e{M}u{X}: Data-efficient Multilingual Learning\",\n    author = \"Khanuja, Simran  and\n      Gowriraj, Srinivas  and\n      Dery, Lucio  and\n      Neubig, Graham\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.412/\",\n    doi = \"10.18653/v1/2024.naacl-long.412\",\n    pages = \"7423--7436\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.412.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.412/",
        "pdf_size": 5947156,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3410954638416422441&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu;cs.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/simran-khanuja/demux",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.332",
        "title": "Debiasing with Sufficient Projection: A General Theoretical Framework for Vector Representations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained vector representations in natural language processing often inadvertently encode undesirable social biases. Identifying and removing unwanted biased information from vector representation is an evolving and significant challenge. Our study uniquely addresses this issue from the perspective of statistical independence, proposing a framework for reducing bias by transforming vector representations to an unbiased subspace using sufficient projection. The key to our framework lies in its generality: it adeptly mitigates bias across both debiasing and fairness tasks, and across various vector representation types, including word embeddings and output representations of transformer models. Importantly, we establish the connection between debiasing and fairness, offering theoretical guarantees and elucidating our algorithm\u2019s efficacy. Through extensive evaluation of intrinsic and extrinsic metrics, our method achieves superior performance in bias reduction while maintaining high task performance, and offers superior computational efficiency.",
        "author": "Enze Shi; Lei Ding; Linglong Kong; Bei Jiang",
        "authorids": "/e/enze-shi/; /l/lei-ding/; /l/linglong-kong/; /b/bei-jiang/",
        "bibtex": "@inproceedings{shi-etal-2024-debiasing,\n    title = \"Debiasing with Sufficient Projection: A General Theoretical Framework for Vector Representations\",\n    author = \"Shi, Enze  and\n      Ding, Lei  and\n      Kong, Linglong  and\n      Jiang, Bei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.332/\",\n    doi = \"10.18653/v1/2024.naacl-long.332\",\n    pages = \"5960--5975\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.332.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.332/",
        "pdf_size": 727980,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=76810881673323157&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Mathematical and Statistical Sciences, University of Alberta; Department of Mathematical and Statistical Sciences, University of Alberta; Department of Mathematical and Statistical Sciences, University of Alberta; Department of Mathematical and Statistical Sciences, University of Alberta",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Mathematical and Statistical Sciences",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-long.424",
        "title": "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite the high performances of large language models (LLMs) across numerous benchmarks, recent research has unveiled their suffering from hallucinations and unfaithful reasoning. This work studies a type of hallucination induced by semantic associations. We investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following correct reasoning paths. To quantify this phenomenon, we propose a novel probing method and benchmark called EUREQA. EUREQA is an entity-searching task where a model finds a missing entity based on described multi-hop relations with other entities. These deliberately designed multi-hop relations create deceptive semantic associations, and models must stick to the correct reasoning path instead of incorrect shortcuts to find the correct answer.Experiments show that existing LLMs cannot follow correct reasoning paths and resist the attempt of greedy shortcuts, with GPT-4 only achieving 62% accuracy. Analyses provide further evidence that LLMs rely on semantic biases to solve the task instead of proper reasoning, questioning the validity and generalizability of current LLMs\u2019 high performances.",
        "author": "Bangzheng Li; Ben Zhou; Fei Wang; Xingyu Fu; Dan Roth; Muhao Chen",
        "authorids": "/b/bangzheng-li/; /b/ben-zhou/; /f/fei-wang/; /x/xingyu-fu/; /d/dan-roth/; /m/muhao-chen/",
        "bibtex": "@inproceedings{li-etal-2024-deceptive,\n    title = \"Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?\",\n    author = \"Li, Bangzheng  and\n      Zhou, Ben  and\n      Wang, Fei  and\n      Fu, Xingyu  and\n      Roth, Dan  and\n      Chen, Muhao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.424/\",\n    doi = \"10.18653/v1/2024.naacl-long.424\",\n    pages = \"7675--7688\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.424.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.424/",
        "pdf_size": 515779,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15283132328413645642&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Davis; University of Pennsylvania; University of Southern California; University of Pennsylvania; University of Pennsylvania; University of California, Davis",
        "aff_domain": "ucdavis.edu; ; ; ; ; ",
        "email": "ucdavis.edu; ; ; ; ; ",
        "github": "https://github.com/VincentLeebang/eureqa",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;1;1;0",
        "aff_unique_norm": "University of California, Davis;University of Pennsylvania;University of Southern California",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucdavis.edu;https://www.upenn.edu;https://www.usc.edu",
        "aff_unique_abbr": "UC Davis;UPenn;USC",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Davis;;Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.296",
        "title": "DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In recent years, several interpretability methods have been proposed to interpret the inner workings of Transformer models at different levels of precision and complexity.In this work, we propose a simple but effective technique to analyze encoder-decoder Transformers. Our method, which we name DecoderLens, allows the decoder to cross-attend representations of intermediate encoder activations instead of using the default final encoder output.The method thus maps uninterpretable intermediate vector representations to human-interpretable sequences of words or symbols, shedding new light on the information flow in this popular but understudied class of models.We apply DecoderLens to question answering, logical reasoning, speech recognition and machine translation models, finding that simpler subtasks are solved with high precision by low and intermediate encoder layers.",
        "author": "Anna Langedijk; Hosein Mohebbi; Gabriele Sarti; Willem Zuidema; Jaap Jumelet",
        "authorids": "/a/anna-langedijk/; /h/hosein-mohebbi/; /g/gabriele-sarti/; /w/willem-zuidema/; /j/jaap-jumelet/",
        "bibtex": "@inproceedings{langedijk-etal-2024-decoderlens,\n    title = \"{D}ecoder{L}ens: Layerwise Interpretation of Encoder-Decoder Transformers\",\n    author = \"Langedijk, Anna  and\n      Mohebbi, Hosein  and\n      Sarti, Gabriele  and\n      Zuidema, Willem  and\n      Jumelet, Jaap\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.296/\",\n    doi = \"10.18653/v1/2024.findings-naacl.296\",\n    pages = \"4764--4780\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.296.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.296/",
        "pdf_size": 788151,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14151426142550039125&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "ILLC, University of Amsterdam; CSAI, Tilburg University; CLCG, University of Groningen; ILLC, University of Amsterdam; ILLC, University of Amsterdam",
        "aff_domain": "gmail.com;tilburguniversity.edu;rug.nl;uva.nl;uva.nl",
        "email": "gmail.com;tilburguniversity.edu;rug.nl;uva.nl;uva.nl",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "University of Amsterdam;Tilburg University;University of Groningen",
        "aff_unique_dep": "ILLC;CSAI;CLCG",
        "aff_unique_url": "https://www.uva.nl;https://www.tilburguniversity.edu/;https://www.rug.nl",
        "aff_unique_abbr": "UvA;Tilburg U;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Amsterdam;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2024.findings-naacl.217",
        "title": "Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference process, extreme confidence serves as an indicator for poisoned samples, while others are clean. We conduct experiments on text classification tasks, five fine-tuning strategies, and three weight-poisoning backdoor attack methods. Experiments show near 100% success rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore, our defensive approach exhibits overall competitive performance in mitigating weight-poisoning backdoor attacks.",
        "author": "Shuai Zhao; Leilei Gan; Anh Tuan Luu; Jie Fu; Lingjuan Lyu; Meihuizi Jia; Jinming Wen",
        "authorids": "/s/shuai-zhao/; /l/leilei-gan/; /l/luu-anh-tuan/; /j/jie-fu/; /l/lingjuan-lyu/; /m/meihuizi-jia/; /j/jinming-wen/",
        "bibtex": "@inproceedings{zhao-etal-2024-defending,\n    title = \"Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning\",\n    author = \"Zhao, Shuai  and\n      Gan, Leilei  and\n      Luu, Anh Tuan  and\n      Fu, Jie  and\n      Lyu, Lingjuan  and\n      Jia, Meihuizi  and\n      Wen, Jinming\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.217/\",\n    doi = \"10.18653/v1/2024.findings-naacl.217\",\n    pages = \"3421--3438\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.217.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.217/",
        "pdf_size": 3096517,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11188407998658459823&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Jinan University, Guangzhou, China + Nanyang Technological University, Singapore; Zhejiang University, China; Nanyang Technological University, Singapore; Hong Kong University of Science and Technology, Hong Kong, China; Sony Research; Beijing Institute of Technology, Beijing, China + Nanyang Technological University, Singapore; Jinan University, Guangzhou, China",
        "aff_domain": "ntu.edu.sg; ; ; ; ; ; ",
        "email": "ntu.edu.sg; ; ; ; ; ; ",
        "github": "https://github.com/shuaizhao95/PSIM",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2;1;3;4;5+1;0",
        "aff_unique_norm": "Jinan University;Nanyang Technological University;Zhejiang University;Hong Kong University of Science and Technology;Sony;Beijing Institute of Technology",
        "aff_unique_dep": ";;;;Research;",
        "aff_unique_url": "https://www.jnu.edu.cn;https://www.ntu.edu.sg;http://www.zju.edu.cn;https://www.ust.hk;https://www.sony.com;http://www.bit.edu.cn/",
        "aff_unique_abbr": "JNU;NTU;ZJU;HKUST;Sony;BIT",
        "aff_campus_unique_index": "0;2;3;0",
        "aff_campus_unique": "Guangzhou;;Hong Kong;Beijing",
        "aff_country_unique_index": "0+1;0;1;0;2;0+1;0",
        "aff_country_unique": "China;Singapore;Japan"
    },
    {
        "id": "2024.naacl-industry.26",
        "title": "Deferred NAM: Low-latency Top-K Context Injection via Deferred Context Encoding for Non-Streaming ASR",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Contextual biasing enables speech recognizers to transcribe important phrases in the speaker\u2019s context, such as contact names, even if they are rare in, or absent from, the training data. Attention-based biasing is a leading approach which allows for full end-to-end cotraining of the recognizer and biasing system and requires no separate inference-time components. Such biasers typically consist of a context encoder; followed by a context filter which narrows down the context to apply, improving per-step inference time; and, finally, context application via cross attention. Though much work has gone into optimizing per-frame performance, the context encoder is at least as important: recognition cannot begin before context encoding ends. Here, we show the lightweight phrase selection pass can be moved before context encoding, resulting in a speedup of up to 16.1 times and enabling biasing to scale to 20K phrases with a maximum pre-decoding delay under 33ms. With the addition of phrase- and wordpiece-level cross-entropy losses, our technique also achieves up to a 37.5% relative WER reduction over the baseline without the losses and lightweight phrase selection pass.",
        "author": "Zelin Wu; Gan Song; Christopher Li; Pat Rondon; Zhong Meng; Xavier Velez; Weiran Wang; Diamantino Caseiro; Golan Pundak; Tsendsuren Munkhdalai; Angad Chandorkar; Rohit Prabhavalkar",
        "authorids": "/z/zelin-wu/; /g/gan-song/; /c/christopher-li/; /p/pat-rondon/; /z/zhong-meng/; /x/xavier-velez/; /w/weiran-wang/; /d/diamantino-caseiro/; /g/golan-pundak/; /t/tsendsuren-munkhdalai/; /a/angad-chandorkar/; /r/rohit-prabhavalkar/",
        "bibtex": "@inproceedings{wu-etal-2024-deferred,\n    title = \"Deferred {NAM}: Low-latency Top-K Context Injection via Deferred Context Encoding for Non-Streaming {ASR}\",\n    author = \"Wu, Zelin  and\n      Song, Gan  and\n      Li, Christopher  and\n      Rondon, Pat  and\n      Meng, Zhong  and\n      Velez, Xavier  and\n      Wang, Weiran  and\n      Caseiro, Diamantino  and\n      Pundak, Golan  and\n      Munkhdalai, Tsendsuren  and\n      Chandorkar, Angad  and\n      Prabhavalkar, Rohit\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.26/\",\n    doi = \"10.18653/v1/2024.naacl-industry.26\",\n    pages = \"315--323\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.26.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.26/",
        "pdf_size": 298420,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1232374407035107495&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google LLC",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.441",
        "title": "Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable NLG Evaluation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Human evaluation serves as the gold standard for assessing the quality of Natural Language Generation (NLG) systems. Nevertheless, the evaluation guideline, as a pivotal element ensuring reliable and reproducible human assessment, has received limited attention. Our investigation revealed that only 29.84% of recent papers involving human evaluation at top conferences release their evaluation guidelines, with vulnerabilities identified in 77.09% of these guidelines. Unreliable evaluation guidelines can yield inaccurate assessment outcomes, potentially impeding the advancement of NLG in the right direction. To address these challenges, we take an initial step towards reliable evaluation guidelines and propose the first human evaluation guideline dataset by collecting annotations of guidelines extracted from existing papers as well as generated via Large Language Models (LLMs). We then introduce a taxonomy of eight vulnerabilities and formulate a principle for composing evaluation guidelines. Furthermore, a method for detecting guideline vulnerabilities has been explored using LLMs, and we offer a set of recommendations to enhance reliability in human evaluation. The annotated human evaluation guideline dataset and code for the vulnerability detection method are publicly available online.",
        "author": "Jie Ruan; Wenqing Wang; Xiaojun Wan",
        "authorids": "/j/jie-ruan/; /w/wenqing-wang/; /x/xiaojun-wan/",
        "bibtex": "@inproceedings{ruan-etal-2024-defining,\n    title = \"Defining and Detecting Vulnerability in Human Evaluation Guidelines: A Preliminary Study Towards Reliable {NLG} Evaluation\",\n    author = \"Ruan, Jie  and\n      Wang, Wenqing  and\n      Wan, Xiaojun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.441/\",\n    doi = \"10.18653/v1/2024.naacl-long.441\",\n    pages = \"7965--7989\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.441.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.441/",
        "pdf_size": 1107713,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14990288513007701369&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Peking University; Peking University; Peking University",
        "aff_domain": "stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/EnablerRx/GuidelineVulnDetect",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "Peking U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.75",
        "title": "Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via contrastive estimation between queries and candidates. By introducing virtual time prefix tokens, it applies a prefix-based tuning method to facilitate the frozen PLM capable for TKGR tasks under different settings. We evaluate ChapTER on four transductive and three few-shot inductive TKGR benchmarks, and experimental results demonstrate that ChapTER achieves superior performance compared to competitive baselines with only 0.17% tuned parameters. We conduct thorough analysis to verify the effectiveness, flexibility and efficiency of ChapTER.",
        "author": "Miao Peng; Ben Liu; Wenjie Xu; Zihao Jiang; Jiahui Zhu; Min Peng",
        "authorids": "/m/miao-peng/; /b/ben-liu/; /w/wenjie-xu/; /z/zihao-jiang/; /j/jiahui-zhu/; /m/min-peng/",
        "bibtex": "@inproceedings{peng-etal-2024-deja,\n    title = \"Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning\",\n    author = \"Peng, Miao  and\n      Liu, Ben  and\n      Xu, Wenjie  and\n      Jiang, Zihao  and\n      Zhu, Jiahui  and\n      Peng, Min\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.75/\",\n    doi = \"10.18653/v1/2024.findings-naacl.75\",\n    pages = \"1178--1191\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.75.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.75/",
        "pdf_size": 515338,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=609252041000835169&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science, Wuhan University, China; School of Computer Science, Wuhan University, China; School of Computer Science, Wuhan University, China; School of Computer Science, Wuhan University, China; Xiaomi Inc., China; School of Computer Science, Wuhan University, China",
        "aff_domain": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn;xiaomi.com;whu.edu.cn",
        "email": "whu.edu.cn;whu.edu.cn;whu.edu.cn;whu.edu.cn;xiaomi.com;whu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "Wuhan University;Xiaomi Inc.",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "http://www.whu.edu.cn;https://www.xiaomi.com",
        "aff_unique_abbr": "WHU;Xiaomi",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Wuhan;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.153",
        "title": "Denoising Attention for Query-aware User Modeling",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Personalization of search results has gained increasing attention in the past few years, also thanks to the development of Neural Networks-based approaches for Information Retrieval. Recent works have proposed to build user models at query time by leveraging the Attention mechanism, which allows weighing the contribution of the user-related information w.r.t. the current query.This approach allows giving more importance to the user\u2019s interests related to the current search performed by the user.In this paper, we discuss some shortcomings of the Attention mechanism when employed for personalization and introduce a novel Attention variant, the Denoising Attention, to solve them.Denoising Attention adopts a robust normalization scheme and introduces a filtering mechanism to better discern among the user-related data those helpful for personalization.Experimental evaluation shows improvements in MAP, MRR, and NDCG above 15% w.r.t. other Attention variants at the state-of-the-art.",
        "author": "Elias Bassani; Pranav Kasela; Gabriella Pasi",
        "authorids": "/e/elias-bassani/; /p/pranav-kasela/; /g/gabriella-pasi/",
        "bibtex": "@inproceedings{bassani-etal-2024-denoising,\n    title = \"Denoising Attention for Query-aware User Modeling\",\n    author = \"Bassani, Elias  and\n      Kasela, Pranav  and\n      Pasi, Gabriella\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.153/\",\n    doi = \"10.18653/v1/2024.findings-naacl.153\",\n    pages = \"2368--2380\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.153.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.153/",
        "pdf_size": 373004,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11473340153510814038&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Milano-Bicocca, Milan, Italy; University of Milano-Bicocca, Milan, Italy + ISTI-CNR, Pisa, Italy; University of Milano-Bicocca, Milan, Italy",
        "aff_domain": "campus.unimib.it;unimib.it;unimib.it",
        "email": "campus.unimib.it;unimib.it;unimib.it",
        "github": "www.github.com/AmenRa/denoising-attention2368",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "University of Milano-Bicocca;ISTI-CNR",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unimib.it;https://isti.cnr.it",
        "aff_unique_abbr": "UNIMIB;",
        "aff_campus_unique_index": "0;0+1;0",
        "aff_campus_unique": "Milan;Pisa",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2024.naacl-long.452",
        "title": "Depression Detection in Clinical Interviews with LLM-Empowered Structural Element Graph",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Depression is a widespread mental health disorder affecting millions globally. Clinical interviews are the gold standard for assessing depression, but they heavily rely on scarce professional clinicians, highlighting the need for automated detection systems. However, existing methods only capture part of the relevant elements in clinical interviews, unable to incorporate all depressive cues. Moreover, the scarcity of participant data, due to privacy concerns and collection challenges, intrinsically constrains interview modeling. To address these limitations, in this paper, we propose a structural element graph (SEGA), which transforms the clinical interview into an expertise-inspired directed acyclic graph for comprehensive modeling. Additionally, we further empower SEGA by devising novel principle-guided data augmentation with large language models (LLMs) to supplement high-quality synthetic data and enable graph contrastive learning. Extensive evaluations on two real-world clinical datasets, in both English and Chinese, show that SEGA significantly outperforms baseline methods and powerful LLMs like GPT-3.5 and GPT-4.",
        "author": "Zhuang Chen; Jiawen Deng; Jinfeng Zhou; Jincenzi Wu; Tieyun Qian; Minlie Huang",
        "authorids": "/z/zhuang-chen/; /j/jiawen-deng/; /j/jinfeng-zhou/; /j/jincenzi-wu/; /t/tieyun-qian/; /m/minlie-huang/",
        "bibtex": "@inproceedings{chen-etal-2024-depression,\n    title = \"Depression Detection in Clinical Interviews with {LLM}-Empowered Structural Element Graph\",\n    author = \"Chen, Zhuang  and\n      Deng, Jiawen  and\n      Zhou, Jinfeng  and\n      Wu, Jincenzi  and\n      Qian, Tieyun  and\n      Huang, Minlie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.452/\",\n    doi = \"10.18653/v1/2024.naacl-long.452\",\n    pages = \"8181--8194\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.452.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.452/",
        "pdf_size": 879458,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=375434021259948906&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "CoAI Group, DCST, TUPM, IAI, BNRIST, Tsinghua University; University of Electronic Science and Technology of China; CoAI Group, DCST, TUPM, IAI, BNRIST, Tsinghua University; CoAI Group, DCST, TUPM, IAI, BNRIST, Tsinghua University; Wuhan University; CoAI Group, DCST, TUPM, IAI, BNRIST, Tsinghua University",
        "aff_domain": "mail.tsinghua.edu.cn; ; ; ; ;tsinghua.edu.cn",
        "email": "mail.tsinghua.edu.cn; ; ; ; ;tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;2;0",
        "aff_unique_norm": "Tsinghua University;University of Electronic Science and Technology of China;Wuhan University",
        "aff_unique_dep": "CoAI Group;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.uestc.edu.cn;http://www.whu.edu.cn/",
        "aff_unique_abbr": "THU;UESTC;WHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.278",
        "title": "Detecting Bipolar Disorder from Misdiagnosed Major Depressive Disorder with Mood-Aware Multi-Task Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Bipolar Disorder (BD) is a mental disorder characterized by intense mood swings, from depression to manic states. Individuals with BD are at a higher risk of suicide, but BD is often misdiagnosed as Major Depressive Disorder (MDD) due to shared symptoms, resulting in delays in appropriate treatment and increased suicide risk. While early intervention based on social media data has been explored to uncover latent BD risk, little attention has been paid to detecting BD from those misdiagnosed as MDD. Therefore, this study presents a novel approach for identifying BD risk in individuals initially misdiagnosed with MDD. A unique dataset, BD-Risk, is introduced, incorporating mental disorder types and BD mood levels verified by two clinical experts. The proposed multi-task learning for predicting BD risk and BD mood level outperforms the state-of-the-art baselines. Also, the proposed dynamic mood-aware attention can provide insights into the impact of BD mood on future risk, potentially aiding interventions for at-risk individuals.",
        "author": "Daeun Lee; Hyolim Jeon; Sejung Son; Chaewon Park; Ji hyun An; Seungbae Kim; Jinyoung Han",
        "authorids": "/d/daeun-lee/; /h/hyolim-jeon/; /s/sejung-son/; /c/chaewon-park/; /j/ji-hyun-an/; /s/seungbae-kim/; /j/jinyoung-han/",
        "bibtex": "@inproceedings{lee-etal-2024-detecting-bipolar,\n    title = \"Detecting Bipolar Disorder from Misdiagnosed Major Depressive Disorder with Mood-Aware Multi-Task Learning\",\n    author = \"Lee, Daeun  and\n      Jeon, Hyolim  and\n      Son, Sejung  and\n      Park, Chaewon  and\n      An, Ji hyun  and\n      Kim, Seungbae  and\n      Han, Jinyoung\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.278/\",\n    doi = \"10.18653/v1/2024.naacl-long.278\",\n    pages = \"4954--4970\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.278.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.278/",
        "pdf_size": 576275,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10861882425292664308&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Applied Artificial Intelligence, Sungkyunkwan University, Seoul, South Korea; Department of Applied Artificial Intelligence, Sungkyunkwan University, Seoul, South Korea; Department of Applied Artificial Intelligence, Sungkyunkwan University, Seoul, South Korea; Department of Applied Artificial Intelligence, Sungkyunkwan University, Seoul, South Korea; Department of Psychiatry, Samsung Medical Center, Seoul, South Korea; Computer Science & Engineering Department, University of South Florida, Tampa, FL, USA; Department of Applied Artificial Intelligence, Sungkyunkwan University, Seoul, South Korea",
        "aff_domain": "skku.edu;skku.edu;g.skku.edu;g.skku.edu;g.skku.edu;samsung.com;usf.edu",
        "email": "skku.edu;skku.edu;g.skku.edu;g.skku.edu;g.skku.edu;samsung.com;usf.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1;2;0",
        "aff_unique_norm": "Sungkyunkwan University;Samsung;University of South Florida",
        "aff_unique_dep": "Department of Applied Artificial Intelligence;Department of Psychiatry;Computer Science & Engineering Department",
        "aff_unique_url": "https://www.sungkyunkwan.ac.kr;https://www.samsungmedical.com;https://www.usf.edu",
        "aff_unique_abbr": "SKKU;;USF",
        "aff_campus_unique_index": "0;0;0;0;0;1;0",
        "aff_campus_unique": "Seoul;Tampa",
        "aff_country_unique_index": "0;0;0;0;0;1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2024.findings-naacl.199",
        "title": "DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outperform those from current coreset selection methods. DiLM achieved remarkable generalization performance in training different types of models and in-context learning of large language models. Our code will be available at https://github.com/arumaekawa/DiLM.",
        "author": "Aru Maekawa; Satoshi Kosugi; Kotaro Funakoshi; Manabu Okumura",
        "authorids": "/a/aru-maekawa/; /s/satoshi-kosugi/; /k/kotaro-funakoshi/; /m/manabu-okumura/",
        "bibtex": "@inproceedings{maekawa-etal-2024-dilm,\n    title = \"{D}i{LM}: Distilling Dataset into Language Model for Text-level Dataset Distillation\",\n    author = \"Maekawa, Aru  and\n      Kosugi, Satoshi  and\n      Funakoshi, Kotaro  and\n      Okumura, Manabu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.199/\",\n    doi = \"10.18653/v1/2024.findings-naacl.199\",\n    pages = \"3138--3153\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.199.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.199/",
        "pdf_size": 508311,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5434038287517840727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology; Tokyo Institute of Technology",
        "aff_domain": "lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;lr.pi.titech.ac.jp",
        "email": "lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;lr.pi.titech.ac.jp;lr.pi.titech.ac.jp",
        "github": "https://github.com/arumaekawa/DiLM",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Tokyo Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.titech.ac.jp",
        "aff_unique_abbr": "Titech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2024.naacl-long.47",
        "title": "Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Most existing works primarily focus on post-training and fine-tuning tailored for cross-encoders. However, there are no post-training methods tailored for dense encoders in dialogue response selection. We argue that when the current language model, based on dense dialogue systems (such as BERT), is employed as a dense encoder, it separately encodes dialogue context and response, leading to a struggle to achieve the alignment of both representations. Thus, we propose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward yet effective post-training technique tailored for dense encoders in dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture to compress the dialogue semantics into dense vectors, which achieves better alignment between the features of the dialogue context and response. Our experiments have demonstrated that Dial-MAE is highly effective, achieving state-of-the-art performance on two commonly evaluated benchmarks.",
        "author": "Zhenpeng Su; Xing W; Wei Zhou; Guangyuan Ma; Songlin Hu",
        "authorids": "/z/zhenpeng-su/; /x/xing-w/; /w/wei-zhou/; /g/guangyuan-ma/; /s/songlin-hu/",
        "bibtex": "@inproceedings{su-etal-2024-dial,\n    title = \"Dial-{MAE}: {C}on{T}extual Masked Auto-Encoder for Retrieval-based Dialogue Systems\",\n    author = \"Su, Zhenpeng  and\n      W, Xing  and\n      Zhou, Wei  and\n      Ma, Guangyuan  and\n      Hu, Songlin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.47/\",\n    doi = \"10.18653/v1/2024.naacl-long.47\",\n    pages = \"820--830\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.47.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.47/",
        "pdf_size": 385396,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8696441847214122862&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.341",
        "title": "DialogBench: Evaluating LLMs as Human-like Dialogue Systems",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities by leveraging instruction tuning,which refreshes human impressions of dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that contains 12 dialogue tasks to probe the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive tests on English and Chinese DialogBench of 26 LLMs show that instruction tuning improves the human likeness of LLMs to a certain extent, but most LLMs still have much room for improvement as human-like dialogue systems. Interestingly, results also show that the positioning of assistant AI can make instruction tuning weaken the human emotional perception of LLMs and their mastery of information about human daily life.",
        "author": "Jiao Ou; Junda Lu; Che Liu; Yihong Tang; Fuzheng Zhang; Di Zhang; Kun Gai",
        "authorids": "/j/jiao-ou/; /j/junda-lu/; /c/che-liu/; /y/yihong-tang/; /f/fuzheng-zhang/; /d/di-zhang/; /k/kun-gai/",
        "bibtex": "@inproceedings{ou-etal-2024-dialogbench,\n    title = \"{D}ialog{B}ench: Evaluating {LLM}s as Human-like Dialogue Systems\",\n    author = \"Ou, Jiao  and\n      Lu, Junda  and\n      Liu, Che  and\n      Tang, Yihong  and\n      Zhang, Fuzheng  and\n      Zhang, Di  and\n      Gai, Kun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.341/\",\n    doi = \"10.18653/v1/2024.naacl-long.341\",\n    pages = \"6137--6170\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.341.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.341/",
        "pdf_size": 731590,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13874369491519165731&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Kuaishou; Kuaishou; Kuaishou; Kuaishou; Kuaishou; Kuaishou; Kuaishou",
        "aff_domain": "gmail.com;gmail.com; ; ; ; ; ",
        "email": "gmail.com;gmail.com; ; ; ; ; ",
        "github": "https://github.com/kwai/DialogBench",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Kuaishou Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kuaishou.com",
        "aff_unique_abbr": "Kuaishou",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.108",
        "title": "DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue Dataset",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "As sharing images in an instant message is a crucial factor, there has been active research on learning an image-text multi-modal dialogue models.However, training a well-generalized multi-modal dialogue model remains challenging due to the low quality and limited diversity of images per dialogue in existing multi-modal dialogue datasets.In this paper, we propose an automated pipeline to construct a multi-modal dialogue dataset, ensuring both dialogue quality and image diversity without requiring minimum human effort. In our pipeline, to guarantee the coherence between images and dialogue, we prompt GPT-4 to infer potential image-sharing moments - specifically, the utterance, speaker, rationale, and image description. Furthermore, we leverage CLIP similarity to maintain consistency between aligned multiple images to the utterance.Through this pipeline, we introduce DialogCC, a high-quality and diverse multi-modal dialogue dataset that surpasses existing datasets in terms of quality and diversity in human evaluation.Our comprehensive experiments highlight that when multi-modal dialogue models are trained using our dataset, their generalization performance on unseen dialogue datasets is significantly enhanced. We make our source code and dataset publicly available (https://dialogcc.github.io/).",
        "author": "Young-Jun Lee; Byungsoo Ko; Han-Gyu Kim; Jonghwan Hyeon; Ho-Jin Choi",
        "authorids": "/y/young-jun-lee/; /b/byungsoo-ko/; /h/han-gyu-kim/; /j/jonghwan-hyeon/; /h/ho-jin-choi/",
        "bibtex": "@inproceedings{lee-etal-2024-dialogcc,\n    title = \"{D}ialog{CC}: An Automated Pipeline for Creating High-Quality Multi-Modal Dialogue Dataset\",\n    author = \"Lee, Young-Jun  and\n      Ko, Byungsoo  and\n      Kim, Han-Gyu  and\n      Hyeon, Jonghwan  and\n      Choi, Ho-Jin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.108/\",\n    doi = \"10.18653/v1/2024.naacl-long.108\",\n    pages = \"1938--1963\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.108.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.108/",
        "pdf_size": 2725500,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14125194378356455296&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "School of Computing, KAIST; NA VER Vision; NA VER Cloud Multimodal AI; School of Computing, KAIST; School of Computing, KAIST",
        "aff_domain": "kaist.ac.kr;gmail.com;navercorp.com;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;gmail.com;navercorp.com;kaist.ac.kr;kaist.ac.kr",
        "github": "",
        "project": "https://dialogcc.github.io/",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "KAIST;NA VER Vision;NAVER Cloud",
        "aff_unique_dep": "School of Computing;;Multimodal AI",
        "aff_unique_url": "https://www.kaist.ac.kr;;https://www.naver.com",
        "aff_unique_abbr": "KAIST;;NAVER",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "id": "2024.naacl-long.304",
        "title": "DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In the constant updates of the product dialogue systems, we need to retrain the natural language understanding (NLU) model as new data from the real users would be merged into the existing data accumulated in the last updates. Within the newly added data, new intents would emerge and might have semantic entanglement with the existing intents, e.g. new intents that are semantically too specific or generic are actually a subset or superset of some existing intents in the semantic space, thus impairing the robustness of the NLU model.As the first attempt to solve this problem, we setup a new benchmark consisting of 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent detection with imperfect data in the system update as a multi-label classification task with positive but unlabeled intents, which asks the models to recognize all the proper intents, including the ones with semantic entanglement, in the inference.We also propose comprehensive baseline models and conduct in-depth analyses for the benchmark, showing that the semantically entangled intents can be effectively recognized with an automatic workflow. Our code and dataset are available at https://github.com/Zefan-Cai/DialogVCS.",
        "author": "Zefan Cai; Xin Zheng; Tianyu Liu; Haoran Meng; Jiaqi Han; Gang Yuan; Binghuai Lin; Baobao Chang; Yunbo Cao",
        "authorids": "/z/zefan-cai/; /x/xin-zheng/; /t/tianyu-liu/; /h/haoran-meng/; /j/jiaqi-han/; /g/gang-yuan/; /b/binghuai-lin/; /b/baobao-chang/; /y/yunbo-cao/",
        "bibtex": "@inproceedings{cai-etal-2024-dialogvcs,\n    title = \"{D}ialog{VCS}: Robust Natural Language Understanding in Dialogue System Upgrade\",\n    author = \"Cai, Zefan  and\n      Zheng, Xin  and\n      Liu, Tianyu  and\n      Meng, Haoran  and\n      Han, Jiaqi  and\n      Yuan, Gang  and\n      Lin, Binghuai  and\n      Chang, Baobao  and\n      Cao, Yunbo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.304/\",\n    doi = \"10.18653/v1/2024.naacl-long.304\",\n    pages = \"5431--5452\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.304.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.304/",
        "pdf_size": 688488,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15755915123486920119&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "https://github.com/pkunlp-icler/DialogVCS",
        "project": "",
        "author_num": 9
    },
    {
        "id": "2024.naacl-long.247",
        "title": "Differentially Private Next-Token Prediction of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly important. The most widely adopted technique to accomplish this is DP-SGD, which trains a model to guarantee Differential Privacy (DP). However, DP-SGD overestimates an adversary\u2019s capabilities in having white box access to the model and, as a result, causes longer training times and larger memory usage than SGD. On the other hand, commercial LLM deployments are predominantly cloud-based; hence, adversarial access to LLMs is black-box. Motivated by these observations, we present Private Mixing of Ensemble Distributions (PMixED): a private prediction protocol for next-token prediction that utilizes the inherent stochasticity of next-token sampling and a public model to achieve Differential Privacy. We formalize this by introducing RD-mollifers which project each of the model\u2019s output distribution from an ensemble of fine-tuned LLMs onto a set around a public LLM\u2019s output distribution, then average the projected distributions and sample from it. Unlike DP-SGD which needs to consider the model architecture during training, PMixED is model agnostic, which makes PMixED a very appealing solution for current deployments. Our results show that PMixED achieves a stronger privacy guarantee than sample-level privacy and outperforms DP-SGD for privacy \ud835\udf16 = 8 on large-scale datasets. Thus, PMixED offers a practical alternative to DP training methods for achieving strong generative utility without compromising privacy.",
        "author": "James Flemings; Meisam Razaviyayn; Murali Annavaram",
        "authorids": "/j/james-flemings/; /m/meisam-razaviyayn/; /m/murali-annavaram/",
        "bibtex": "@inproceedings{flemings-etal-2024-differentially,\n    title = \"Differentially Private Next-Token Prediction of Large Language Models\",\n    author = \"Flemings, James  and\n      Razaviyayn, Meisam  and\n      Annavaram, Murali\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.247/\",\n    doi = \"10.18653/v1/2024.naacl-long.247\",\n    pages = \"4390--4404\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.247.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.247/",
        "pdf_size": 561569,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12807958308401870367&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Southern California; University of Southern California; University of Southern California",
        "aff_domain": "usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.271",
        "title": "Diffusion Glancing Transformer for Parallel Sequence-to-Sequence Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Previously, non-autoregressive models were widely recognized as being superior in generation efficiency but inferior in generation quality due to the challenges of modeling multiple target modalities.To enhance the multi-modality modeling ability, we propose the diffusion glancing transformer, which employs a modality diffusion process and residual glancing sampling.The modality diffusion process is a discrete process that interpolates the multi-modal distribution along the decoding steps, and the residual glancing sampling approach guides the model to continuously learn the remaining modalities across the layers. Experimental results on various machine translation and text generation benchmarks demonstrate that DIFFGLAT achieves better generation accuracy while maintaining fast decoding speed compared with both autoregressive and non-autoregressive models.",
        "author": "Lihua Qian; Mingxuan Wang; Yang Liu; Hao Zhou",
        "authorids": "/l/lihua-qian/; /m/mingxuan-wang/; /y/yang-liu/; /h/hao-zhou/",
        "bibtex": "@inproceedings{qian-etal-2024-diffusion,\n    title = \"Diffusion Glancing Transformer for Parallel Sequence-to-Sequence Learning\",\n    author = \"Qian, Lihua  and\n      Wang, Mingxuan  and\n      Liu, Yang  and\n      Zhou, Hao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.271/\",\n    doi = \"10.18653/v1/2024.naacl-long.271\",\n    pages = \"4846--4862\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.271.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.271/",
        "pdf_size": 1467115,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16477019188770099474&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "ByteDance; ByteDance; Institute for AI Industry Research (AIR), Tsinghua University; Institute for AI Industry Research (AIR), Tsinghua University + Shanghai Artificial Intelligence Laboratory",
        "aff_domain": "bytedance.com;bytedance.com;tsinghua.edu.cn;air.tsinghua.edu.cn",
        "email": "bytedance.com;bytedance.com;tsinghua.edu.cn;air.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;1+2",
        "aff_unique_norm": "ByteDance;Tsinghua University;Shanghai Artificial Intelligence Laboratory",
        "aff_unique_dep": ";Institute for AI Industry Research (AIR);",
        "aff_unique_url": "https://www.bytedance.com;https://www.tsinghua.edu.cn;http://www.shailab.org/",
        "aff_unique_abbr": "ByteDance;Tsinghua;Shanghai AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-short.34",
        "title": "Direct Preference Optimization for Neural Machine Translation with Minimum Bayes Risk Decoding",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Minimum Bayes Risk (MBR) decoding can significantly improve translation performance of Multilingual Large Language Models (MLLMs). However, MBR decoding is computationally expensive. We show how the recently developed Reinforcement Learning technique, Direct Preference Optimization (DPO), can fine-tune MLLMs to get the gains of MBR without any additional computation in inference. Our method uses only a small monolingual fine-tuning set and yields significantly improved performance on multiple NMT test sets compared to MLLMs without DPO.",
        "author": "Guangyu Yang; Jinghong Chen; Weizhe Lin; Bill Byrne",
        "authorids": "/g/guangyu-yang/; /j/jinghong-chen/; /w/weizhe-lin/; /b/bill-byrne/",
        "bibtex": "@inproceedings{yang-etal-2024-direct,\n    title = \"Direct Preference Optimization for Neural Machine Translation with Minimum {B}ayes Risk Decoding\",\n    author = \"Yang, Guangyu  and\n      Chen, Jinghong  and\n      Lin, Weizhe  and\n      Byrne, Bill\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.34/\",\n    doi = \"10.18653/v1/2024.naacl-short.34\",\n    pages = \"391--398\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.34.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.34/",
        "pdf_size": 301344,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5346840715000789810&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge; Department of Engineering, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-short.27",
        "title": "Discourse-Aware In-Context Learning for Temporal Expression Normalization",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Temporal expression (TE) normalization is a well-studied problem. However, the predominately used rule-based systems are highly restricted to specific settings, and upcoming machine learning approaches suffer from a lack of labeled data. In this work, we explore the feasibility of proprietary and open-source large language models (LLMs) for TE normalization using in-context learning to inject task, document, and example information into the model. We explore various sample selection strategies to retrieve the most relevant set of examples. By using a window-based prompt design approach, we can perform TE normalization across sentences, while leveraging the LLM knowledge without training the model.Our experiments show competitive results to models designed for this task. In particular, our method achieves large performance improvements for non-standard settings by dynamically including relevant examples during inference.",
        "author": "Akash Gautam; Lukas Lange; Jannik Str\u00f6tgen",
        "authorids": "/a/akash-kumar-gautam/; /l/lukas-lange/; /j/jannik-strotgen/",
        "bibtex": "@inproceedings{gautam-etal-2024-discourse,\n    title = \"Discourse-Aware In-Context Learning for Temporal Expression Normalization\",\n    author = {Gautam, Akash  and\n      Lange, Lukas  and\n      Str{\\\"o}tgen, Jannik},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.27/\",\n    doi = \"10.18653/v1/2024.naacl-short.27\",\n    pages = \"306--315\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.27.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.27/",
        "pdf_size": 1147329,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8880560676165924237&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Bosch Center for Artificial Intelligence, Renningen, Germany + Saarland University, Saarland Informatics Campus, Germany; Bosch Center for Artificial Intelligence, Renningen, Germany; Karlsruhe University of Applied Sciences, Germany",
        "aff_domain": "stud.uni-saarland.de;de.bosch.com; ",
        "email": "stud.uni-saarland.de;de.bosch.com; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;2",
        "aff_unique_norm": "Bosch Center for Artificial Intelligence;Saarland University;Karlsruhe University of Applied Sciences",
        "aff_unique_dep": "Artificial Intelligence;;",
        "aff_unique_url": "https://www.bosch-ai.com;https://www.uni-saarland.de;https://www.hs-karlsruhe.de",
        "aff_unique_abbr": "BCAI;UdS;HsKA",
        "aff_campus_unique_index": "0+1;0",
        "aff_campus_unique": "Renningen;Saarland Informatics Campus;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.448",
        "title": "Discovering Lobby-Parliamentarian Alignments through NLP",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We discover alignments of views between interest groups (lobbies) and members of the European Parliament (MEPs) by automatically analyzing their texts. Specifically, we do so by collecting novel datasets of lobbies\u2019 position papers and MEPs\u2019 speeches, and comparing these texts on the basis of semantic similarity and entailment. In the absence of ground-truth, we perform an indirect validation by comparing the discovered alignments with a dataset, which we curate, of retweet links between MEPs and lobbies, and with the publicly disclosed meetings of MEPs. Our best method performs significantly better than several baselines. Moreover, an aggregate analysis of the discovered alignments, between groups of related lobbies and political groups of MEPs, correspond to the expectations from the ideology of the groups (e.g., groups on the political left are more aligned with humanitarian and environmental organisations). We believe that this work is a step towards enhancing the transparency of the intricate decision-making processes within democratic institutions.",
        "author": "Aswin Suresh; Lazar Radojevi\u0107; Francesco Salvi; Antoine Magron; Victor Kristof; Matthias Grossglauser",
        "authorids": "/a/aswin-suresh/; /l/lazar-radojevic/; /f/francesco-salvi/; /a/antoine-magron/; /v/victor-kristof/; /m/matthias-grossglauser/",
        "bibtex": "@inproceedings{suresh-etal-2024-discovering,\n    title = \"Discovering Lobby-Parliamentarian Alignments through {NLP}\",\n    author = \"Suresh, Aswin  and\n      Radojevi{\\'c}, Lazar  and\n      Salvi, Francesco  and\n      Magron, Antoine  and\n      Kristof, Victor  and\n      Grossglauser, Matthias\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.448/\",\n    doi = \"10.18653/v1/2024.naacl-long.448\",\n    pages = \"8107--8120\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.448.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.448/",
        "pdf_size": 728224,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:BWIvGIVbIWwJ:scholar.google.com/&scioq=Discovering+Lobby-Parliamentarian+Alignments+through+NLP&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "EPFL, Switzerland; EPFL, Switzerland; EPFL, Switzerland; EPFL, Switzerland; EPFL, Switzerland; EPFL, Switzerland",
        "aff_domain": "epfl.ch;epfl.ch;epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "email": "epfl.ch;epfl.ch;epfl.ch;epfl.ch;epfl.ch;epfl.ch",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2024.findings-naacl.104",
        "title": "Discovering and Mitigating Indirect Bias in Attention-Based Model Explanations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "As the field of Natural Language Processing (NLP) increasingly adopts transformer-based models, the issue of bias becomes more pronounced. Such bias, manifesting through stereotypes and discriminatory practices, can disadvantage certain groups. Our study focuses on direct and indirect bias in the model explanations, where the model makes predictions relying heavily on identity tokens or associated contexts. We present a novel analysis of bias in model explanation, especially the subtle indirect bias, underlining the limitations of traditional fairness metrics. We first define direct and indirect bias in model explanations, which is complementary to fairness in predictions. We then develop an indirect bias discovery algorithm for quantitatively evaluating indirect bias in transformer models using their in-built self-attention matrix. We also propose an indirect bias mitigation algorithm to ensure fairness in transformer models by leveraging attention explanations. Our evaluation shows the significance of indirect bias and the effectiveness of our indirect bias discovery and mitigation.",
        "author": "Farsheed Haque; Depeng Xu; Shuhan Yuan",
        "authorids": "/f/farsheed-haque/; /d/depeng-xu/; /s/shuhan-yuan/",
        "bibtex": "@inproceedings{haque-etal-2024-discovering,\n    title = \"Discovering and Mitigating Indirect Bias in Attention-Based Model Explanations\",\n    author = \"Haque, Farsheed  and\n      Xu, Depeng  and\n      Yuan, Shuhan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.104/\",\n    doi = \"10.18653/v1/2024.findings-naacl.104\",\n    pages = \"1599--1614\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.104.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.104/",
        "pdf_size": 2778305,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:ZhclB12OtzUJ:scholar.google.com/&scioq=Discovering+and+Mitigating+Indirect+Bias+in+Attention-Based+Model+Explanations&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of North Carolina at Charlotte; University of North Carolina at Charlotte; Utah State University",
        "aff_domain": "uncc.edu;uncc.edu;usu.edu",
        "email": "uncc.edu;uncc.edu;usu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of North Carolina at Charlotte;Utah State University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uncc.edu;https://www.usu.edu",
        "aff_unique_abbr": "UNCC;USU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Charlotte;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.201",
        "title": "Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge.One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects orobjects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARE-LAMA \u2013 a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations.CONPARE-LAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval performance of PLMs. Extensive knowledge retrieval experiments using our probe reveal that prompts following clausal syntax have several desirable properties in comparison to appositive syntax: i) they are more useful when querying PLMs with a combination of supplementary information, ii) knowledge is more consistently recalled across different combinations of supplementary information, and iii) they decrease response uncertainty when retrieving known facts. In addition, range information can boost knowledge retrieval performance more than domain information, even though domain information is more reliably helpful across syntactic forms.",
        "author": "Stephan Linzbach; Dimitar Dimitrov; Laura Kallmeyer; Kilian Evang; Hajira Jabeen; Stefan Dietze",
        "authorids": "/s/stephan-linzbach/; /d/dimitar-dimitrov/; /l/laura-kallmeyer/; /k/kilian-evang/; /h/hajira-jabeen/; /s/stefan-dietze/",
        "bibtex": "@inproceedings{linzbach-etal-2024-dissecting,\n    title = \"Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models\",\n    author = \"Linzbach, Stephan  and\n      Dimitrov, Dimitar  and\n      Kallmeyer, Laura  and\n      Evang, Kilian  and\n      Jabeen, Hajira  and\n      Dietze, Stefan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.201/\",\n    doi = \"10.18653/v1/2024.naacl-long.201\",\n    pages = \"3645--3655\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.201.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.201/",
        "pdf_size": 584435,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2345095962628189599&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "GESIS - Leibniz Institute for the Social Sciences + Heinrich Heine University; GESIS - Leibniz Institute for the Social Sciences; Heinrich Heine University; Heinrich Heine University; GESIS - Leibniz Institute for the Social Sciences; GESIS - Leibniz Institute for the Social Sciences + Heinrich Heine University",
        "aff_domain": "gesis.org;gesis.org;hhu.de;hhu.de;gesis.org;gesis.org",
        "email": "gesis.org;gesis.org;hhu.de;hhu.de;gesis.org;gesis.org",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;1;1;0;0+1",
        "aff_unique_norm": "Leibniz Institute for the Social Sciences;Heinrich Heine University",
        "aff_unique_dep": "Social Sciences;",
        "aff_unique_url": "https://www.gesis.org;https://www.uni-duesseldorf.de",
        "aff_unique_abbr": "GESIS;HHU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.findings-naacl.51",
        "title": "DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context.In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.",
        "author": "Weihao Zeng; Dayuan Fu; Keqing He; Yejie Wang; Yukai Xu; Weiran Xu",
        "authorids": "/w/weihao-zeng/; /d/dayuan-fu/; /k/keqing-he/; /y/yejie-wang/; /y/yukai-xu/; /w/weiran-xu/",
        "bibtex": "@inproceedings{zeng-etal-2024-divtod,\n    title = \"{D}iv{TOD}: Unleashing the Power of {LLM}s for Diversifying Task-Oriented Dialogue Representations\",\n    author = \"Zeng, Weihao  and\n      Fu, Dayuan  and\n      He, Keqing  and\n      Wang, Yejie  and\n      Xu, Yukai  and\n      Xu, Weiran\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.51/\",\n    doi = \"10.18653/v1/2024.findings-naacl.51\",\n    pages = \"800--813\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.51.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.51/",
        "pdf_size": 1953043,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1129472818122326494&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Meituan; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;meituan.com;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Meituan",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.meituan.com",
        "aff_unique_abbr": "BUPT;Meituan",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.377",
        "title": "Divergent Token Metrics: Measuring degradation to prune away LLM components \u2013 and optimize quantization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. However, their ever-increasing size has raised concerns about their effective deployment and the need for LLM compression. This study introduces the Divergent Token Metrics (DTMs), a novel approach to assessing compressed LLMs, addressing the limitations of traditional perplexity or accuracy measures that fail to accurately reflect text generation quality. DTMs measure token divergences that allow deeper insights into the subtleties of model compression, in particular, when evaluating components\u2019 impacts individually. Utilizing the First Divergent Token Metric (FDTM) in model sparsification reveals that 25% of all attention components can be pruned beyond 90% on the Llama-2 model family, still keeping SOTA performance. For quantization, FDTM suggests that more than 80% of parameters can be naively transformed to int8 without special outlier management. These evaluations indicate the necessity of choosing appropriate compressions for parameters individually\u2014and that FDTM can identify those\u2014while standard metrics result in deteriorated outcomes.",
        "author": "Bj\u00f6rn Deiseroth; Max Meuer; Nikolas Gritsch; Constantin Eichenberg; Patrick Schramowski; Matthias A\u00dfenmacher; Kristian Kersting",
        "authorids": "/b/bjorn-deiseroth/; /m/max-meuer/; /n/nikolas-gritsch/; /c/constantin-eichenberg/; /p/patrick-schramowski/; /m/matthias-assenmacher/; /k/kristian-kersting/",
        "bibtex": "@inproceedings{deiseroth-etal-2024-divergent,\n    title = \"Divergent Token Metrics: Measuring degradation to prune away {LLM} components {--} and optimize quantization\",\n    author = {Deiseroth, Bj{\\\"o}rn  and\n      Meuer, Max  and\n      Gritsch, Nikolas  and\n      Eichenberg, Constantin  and\n      Schramowski, Patrick  and\n      A{\\ss}enmacher, Matthias  and\n      Kersting, Kristian},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.377/\",\n    doi = \"10.18653/v1/2024.naacl-long.377\",\n    pages = \"6764--6783\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.377.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.377/",
        "pdf_size": 2442264,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8704362802978496408&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Aleph Alpha @ IPAI + Technical University Darmstadt + Hessian Center for Artificial Intelligence (hessian.AI); Aleph Alpha @ IPAI; Aleph Alpha @ IPAI + Department of Statistics, LMU + Munich Center for Machine Learning (MCML); Aleph Alpha @ IPAI; Technical University Darmstadt + Hessian Center for Artificial Intelligence (hessian.AI) + German Center for Artificial Intelligence (DFKI); Department of Statistics, LMU + Munich Center for Machine Learning (MCML); Technical University Darmstadt + Hessian Center for Artificial Intelligence (hessian.AI) + German Center for Artificial Intelligence (DFKI)",
        "aff_domain": "aleph-alpha.com;aleph-alpha.com;aleph-alpha.com;aleph-alpha.com;dfki.de;stat.uni-muenchen.de;cs.tu-darmstadt.de",
        "email": "aleph-alpha.com;aleph-alpha.com;aleph-alpha.com;aleph-alpha.com;dfki.de;stat.uni-muenchen.de;cs.tu-darmstadt.de",
        "github": "https://github.com/Aleph-Alpha/Divergent_Tokens",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1+2;0;0+3+4;0;1+2+5;3+4;1+2+5",
        "aff_unique_norm": "Aleph Alpha;Technical University of Darmstadt;Hessian Center for Artificial Intelligence;Ludwig Maximilian University of Munich;Munich Center for Machine Learning;German Center for Artificial Intelligence",
        "aff_unique_dep": "Institute for Pattern Recognition and Artificial Intelligence (IPAI);;Artificial Intelligence;Department of Statistics;Center for Machine Learning;",
        "aff_unique_url": "https://www.aleph-alpha.com;https://www.tu-darmstadt.de;https://hessian.ai;https://www.lmu.de;https://www.munich-center-for-machine-learning.de;https://www.dFKI.de",
        "aff_unique_abbr": "Aleph Alpha;TUD;hessian.AI;LMU;MCML;DFKI",
        "aff_campus_unique_index": ";1;;1;",
        "aff_campus_unique": ";Munich",
        "aff_country_unique_index": "0+0+0;0;0+0+0;0;0+0+0;0+0;0+0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-short.58",
        "title": "Diverse Perspectives, Divergent Models: Cross-Cultural Evaluation of Depression Detection on Twitter",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Social media data has been used for detecting users with mental disorders, such as depression. Despite the global significance of cross-cultural representation and its potential impact on model performance, publicly available datasets often lack crucial metadata relatedto this aspect. In this work, we evaluate the generalization of benchmark datasets to build AI models on cross-cultural Twitter data. We gather a custom geo-located Twitter dataset of depressed users from seven countries as a test dataset. Our results show that depressiondetection models do not generalize globally. The models perform worse on Global South users compared to Global North. Pre-trainedlanguage models achieve the best generalization compared to Logistic Regression, though still show significant gaps in performance on depressed and non-Western users. We quantify our findings and provide several actionable suggestions to mitigate this issue",
        "author": "Nuredin Ali Abdelkadir; Charles Zhang; Ned Mayo; Stevie Chancellor",
        "authorids": "/n/nuredin-ali-abdelkadir/; /c/charles-zhang/; /n/ned-mayo/; /s/stevie-chancellor/",
        "bibtex": "@inproceedings{abdelkadir-etal-2024-diverse,\n    title = \"Diverse Perspectives, Divergent Models: Cross-Cultural Evaluation of Depression Detection on {T}witter\",\n    author = \"Abdelkadir, Nuredin Ali  and\n      Zhang, Charles  and\n      Mayo, Ned  and\n      Chancellor, Stevie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.58/\",\n    doi = \"10.18653/v1/2024.naacl-short.58\",\n    pages = \"672--680\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.58.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.58/",
        "pdf_size": 1042131,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6248366538545261134&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "University of Minnesota; University of Minnesota; Macalester College; University of Minnesota",
        "aff_domain": "umn.edu;umn.edu;macalester.edu;umn.edu",
        "email": "umn.edu;umn.edu;macalester.edu;umn.edu",
        "github": "",
        "project": "https://grouplens.org/datasets/twitter-depression-dataset-2024/",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Minnesota;Macalester College",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.minnesota.edu;https://www.macalester.edu",
        "aff_unique_abbr": "UMN;Macalester",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.319",
        "title": "Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The integration of Large Language Models (LLMs) in information retrieval has raised a critical reevaluation of fairness in the text-ranking models. LLMs, such as GPT models and Llama2, have shown effectiveness in natural language understanding tasks, and prior works such as RankGPT have demonstrated that the LLMs have better performance than the traditional ranking models in the ranking task. However, their fairness remains largely unexplored. This paper presents an empirical study evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes. Our analysis delves into how these LLMs handle queries and documents related to these attributes, aiming to uncover biases in their ranking algorithms. We assess fairness from both user and content perspectives, contributing an empirical benchmark for evaluating LLMs as the fair ranker.",
        "author": "Yuan Wang; Xuyang Wu; Hsin-Tai Wu; Zhiqiang Tao; Yi Fang",
        "authorids": "/y/yuan-wang/; /x/xuyang-wu/; /h/hsin-tai-wu/; /z/zhiqiang-tao/; /y/yi-fang/",
        "bibtex": "@inproceedings{wang-etal-2024-large,\n    title = \"Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of {LLM}s as Rankers\",\n    author = \"Wang, Yuan  and\n      Wu, Xuyang  and\n      Wu, Hsin-Tai  and\n      Tao, Zhiqiang  and\n      Fang, Yi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.319/\",\n    doi = \"10.18653/v1/2024.naacl-long.319\",\n    pages = \"5712--5724\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.319.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.319/",
        "pdf_size": 610025,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9520800100182509203&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Santa Clara University; Santa Clara University; DOCOMO Innovations, Inc. + Santa Clara University; Rochester Institute of Technology; Santa Clara University",
        "aff_domain": "scu.edu;scu.edu;docomoinnovations.com;rit.edu;scu.edu",
        "email": "scu.edu;scu.edu;docomoinnovations.com;rit.edu;scu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1+0;2;0",
        "aff_unique_norm": "Santa Clara University;DOCOMO Innovations;Rochester Institute of Technology",
        "aff_unique_dep": ";Innovations;",
        "aff_unique_url": "https://www.scu.edu;https://www.docomo-innovations.com;https://www.rit.edu",
        "aff_unique_abbr": "SCU;DOCOMO Innovations;RIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.176",
        "title": "Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale of Two Benchmarks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The concept of localization in LLMs is often mentioned in prior work; however, methods for localization have never been systematically and directly evaluated. We propose two complementary benchmarks that evaluate the ability of localization methods to pinpoint LLM components responsible for memorized data. In our INJ benchmark, we actively inject a piece of new information into a small subset of LLM weights, enabling us to directly evaluate whether localization methods can identify these \u201cground truth\u201d weights. In our DEL benchmark, we evaluate localization by measuring how much dropping out identified neurons deletes a memorized pretrained sequence. Despite their different perspectives, our two benchmarks yield consistent rankings of five localization methods. Methods adapted from network pruning perform well on both benchmarks, and all evaluated methods show promising localization ability. On the other hand, even successful methods identify neurons that are not specific to a single memorized sequence.",
        "author": "Ting-Yun Chang; Jesse Thomason; Robin Jia",
        "authorids": "/t/ting-yun-chang/; /j/jesse-thomason/; /r/robin-jia/",
        "bibtex": "@inproceedings{chang-etal-2024-localization,\n    title = \"Do Localization Methods Actually Localize Memorized Data in {LLM}s? A Tale of Two Benchmarks\",\n    author = \"Chang, Ting-Yun  and\n      Thomason, Jesse  and\n      Jia, Robin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.176/\",\n    doi = \"10.18653/v1/2024.naacl-long.176\",\n    pages = \"3190--3211\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.176.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.176/",
        "pdf_size": 2527073,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12665134394208804070&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA",
        "aff_domain": "usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu",
        "github": "https://github.com/terarachang/MemPis",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.46",
        "title": "Do Multilingual Language Models Think Better in English?",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Translate-test is a popular technique to improve the performance of multilingual language models. This approach works by translating the input into English using an external machine translation system before running inference. However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model. In this work, we introduce a new approach called self-translate that leverages the few-shot translation capabilities of multilingual language models. This allows us to analyze the effect of translation in isolation. Experiments over 5 tasks show that self-translate consistently outperforms direct inference, demonstrating that language models are unable to leverage their full multilingual potential when prompted in non-English languages. Our code is available at https://github.com/juletx/self-translate.",
        "author": "Julen Etxaniz; Gorka Azkune; Aitor Soroa; Oier Lopez de Lacalle; Mikel Artetxe",
        "authorids": "/j/julen-etxaniz/; /g/gorka-azkune/; /a/aitor-soroa/; /o/oier-lopez-de-lacalle/; /m/mikel-artetxe/",
        "bibtex": "@inproceedings{etxaniz-etal-2024-multilingual,\n    title = \"Do Multilingual Language Models Think Better in {E}nglish?\",\n    author = \"Etxaniz, Julen  and\n      Azkune, Gorka  and\n      Soroa, Aitor  and\n      Lopez de Lacalle, Oier  and\n      Artetxe, Mikel\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.46/\",\n    doi = \"10.18653/v1/2024.naacl-short.46\",\n    pages = \"550--564\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.46.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.46/",
        "pdf_size": 255165,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11459603127508824901&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "HiTZ Center, University of the Basque Country UPV/EHU; HiTZ Center, University of the Basque Country UPV/EHU; HiTZ Center, University of the Basque Country UPV/EHU; HiTZ Center, University of the Basque Country UPV/EHU; HiTZ Center, University of the Basque Country UPV/EHU + Reka AI",
        "aff_domain": "ehu.eus;ehu.eus;ehu.eus;ehu.eus;ehu.eus",
        "email": "ehu.eus;ehu.eus;ehu.eus;ehu.eus;ehu.eus",
        "github": "https://github.com/juletx/self-translate",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0+1",
        "aff_unique_norm": "University of the Basque Country;Reka AI",
        "aff_unique_dep": "HiTZ Center;",
        "aff_unique_url": "https://www.ehu.eus/en;https://www.reka.ai",
        "aff_unique_abbr": "UPV/EHU;Reka AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+1",
        "aff_country_unique": "Spain;United States"
    },
    {
        "id": "2024.findings-naacl.258",
        "title": "Do Prompt Positions Really Matter?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Prompt-based models have gathered a lot of attention from researchers due to their remarkable advancements in the fields of zero-shot and few-shot learning. Developing an effective prompt template plays a critical role. However, prior studies have mainly focused on prompt vocabulary searching or embedding initialization within a predefined template with the prompt position fixed. In this empirical study, we conduct the most comprehensive analysis to date of prompt position for diverse Natural Language Processing (NLP) tasks. Our findings quantify the substantial impact prompt position has on model performance. We observe that the prompt positions used in prior studies are often sub-optimal, and this observation is consistent even in widely used instruction-tuned models. These findings suggest prompt position optimisation as a valuable research direction to augment prompt engineering methodologies and prompt position-aware instruction tuning as a potential way to build more robust models in the future.",
        "author": "Junyu Mao; Stuart E. Middleton; Mahesan Niranjan",
        "authorids": "/j/junyu-mao/; /s/stuart-e-middleton/; /m/mahesan-niranjan/",
        "bibtex": "@inproceedings{mao-etal-2024-prompt,\n    title = \"Do Prompt Positions Really Matter?\",\n    author = \"Mao, Junyu  and\n      Middleton, Stuart E.  and\n      Niranjan, Mahesan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.258/\",\n    doi = \"10.18653/v1/2024.findings-naacl.258\",\n    pages = \"4102--4130\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.258.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.258/",
        "pdf_size": 314184,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9863445353178312490&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Electronics and Computer Science, University of Southampton; School of Electronics and Computer Science, University of Southampton; School of Electronics and Computer Science, University of Southampton",
        "aff_domain": "soton.ac.uk;soton.ac.uk;ecs.soton.ac.uk",
        "email": "soton.ac.uk;soton.ac.uk;ecs.soton.ac.uk",
        "github": "https://github.com/milliemaoo/prompt-position",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Southampton",
        "aff_unique_dep": "School of Electronics and Computer Science",
        "aff_unique_url": "https://www.southampton.ac.uk",
        "aff_unique_abbr": "Southampton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-short.43",
        "title": "Do Vision-Language Models Understand Compound Nouns?",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., *lab coat*) as well as they understand nouns (e.g., *lab*)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs\u2019 limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple diverse captions that include the CN as an object in the scene described by the caption. Our proposed method improves CN understanding of CLIP by 8.25% on Compun. Code and benchmark are available.",
        "author": "Sonal Kumar; Sreyan Ghosh; S Sakshi; Utkarsh Tyagi; Dinesh Manocha",
        "authorids": "/s/sonal-kumar/; /s/sreyan-ghosh/; /s/s-sakshi/; /u/utkarsh-tyagi/; /d/dinesh-manocha/",
        "bibtex": "@inproceedings{kumar-etal-2024-vision,\n    title = \"Do Vision-Language Models Understand Compound Nouns?\",\n    author = \"Kumar, Sonal  and\n      Ghosh, Sreyan  and\n      Sakshi, S  and\n      Tyagi, Utkarsh  and\n      Manocha, Dinesh\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.43/\",\n    doi = \"10.18653/v1/2024.naacl-short.43\",\n    pages = \"519--527\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.43.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.43/",
        "pdf_size": 11873872,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14610080000855725166&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Maryland, College Park, USA; University of Maryland, College Park, USA; University of Maryland, College Park, USA; University of Maryland, College Park, USA; University of Maryland, College Park, USA",
        "aff_domain": "umd.edu;umd.edu; ;umd.edu;umd.edu",
        "email": "umd.edu;umd.edu; ;umd.edu;umd.edu",
        "github": "https://github.com/sonalkum/Compun",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.230",
        "title": "DoG-Instruct: Towards Premium Instruction-Tuning Data via Text-Grounded Instruction Wrapping",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The improvement of LLMs\u2019 instruction-following capabilities relies heavily on the availability of high-quality instruction-response pairs. Unfortunately, the current methods used to collect the pairs suffer from either unaffordable labor costs or severe hallucinations in the self-generation of LLM.To tackle these challenges, this paper proposes a scalable solution.It involves training LLMs to generate instruction-response pairs based on human-written documents, rather than relying solely on self-generation without context.Our proposed method not only exploits the advantages of human-written documents in reducing hallucinations but also utilizes an LLM to wrap the expression of documents, which enables us to bridge the gap between various document styles and the standard AI response.Experiments demonstrate that our method outperforms existing typical methods on multiple benchmarks.In particular, compared to the best-performing baseline, the LLM trained using our generated dataset exhibits a 10% relative improvement in performance on AlpacaEval, despite utilizing only 1/5 of its training data.Furthermore, a comprehensive manual evaluation validates the quality of the data we generated.",
        "author": "Yongrui Chen; Haiyun Jiang; Xinting Huang; Shuming Shi; Guilin Qi",
        "authorids": "/y/yongrui-chen/; /h/haiyun-jiang/; /x/xinting-huang/; /s/shuming-shi/; /g/guilin-qi/",
        "bibtex": "@inproceedings{chen-etal-2024-dog,\n    title = \"{D}o{G}-Instruct: Towards Premium Instruction-Tuning Data via Text-Grounded Instruction Wrapping\",\n    author = \"Chen, Yongrui  and\n      Jiang, Haiyun  and\n      Huang, Xinting  and\n      Shi, Shuming  and\n      Qi, Guilin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.230/\",\n    doi = \"10.18653/v1/2024.naacl-long.230\",\n    pages = \"4125--4135\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.230.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.230/",
        "pdf_size": 755531,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14805123915021127296&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Southeast University + Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education; Tencent AI Lab; Tencent AI Lab; Tencent AI Lab; Southeast University + Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education",
        "aff_domain": "seu.edu.cn;tencent.com;tencent.com;tencent.com;seu.edu.cn",
        "email": "seu.edu.cn;tencent.com;tencent.com;tencent.com;seu.edu.cn",
        "github": "https://github.com/Bahuia/Dog-Instruct",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;1;1;1;0+0",
        "aff_unique_norm": "Southeast University;Tencent",
        "aff_unique_dep": ";Tencent AI Lab",
        "aff_unique_url": "https://www.seu.edu.cn/;https://ai.tencent.com",
        "aff_unique_abbr": "SEU;Tencent AI Lab",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.392",
        "title": "Document Image Machine Translation with Dynamic Multi-pre-trained Models Assembling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text image machine translation (TIMT) is a task that translates source texts embedded in the image to target translations. The existing TIMT task mainly focuses on text-line-level images. In this paper, we extend the current TIMT task and propose a novel task, **D**ocument **I**mage **M**achine **T**ranslation to **Markdown** (**DIMT2Markdown**), which aims to translate a source document image with long context and complex layout structure to markdown-formatted target translation.We also introduce a novel framework, **D**ocument **I**mage **M**achine **T**ranslation with **D**ynamic multi-pre-trained models **A**ssembling (**DIMTDA**).A dynamic model assembler is used to integrate multiple pre-trained models to enhance the model\u2019s understanding of layout and translation capabilities.Moreover, we build a novel large-scale **Do**cument image machine **T**ranslation dataset of **A**rXiv articles in markdown format (**DoTA**), containing 126K image-translation pairs.Extensive experiments demonstrate the feasibility of end-to-end translation of rich-text document images and the effectiveness of DIMTDA.",
        "author": "Yupu Liang; Yaping Zhang; Cong Ma; Zhiyang Zhang; Yang Zhao; Lu Xiang; Chengqing Zong; Yu Zhou",
        "authorids": "/y/yupu-liang/; /y/yaping-zhang/; /c/cong-ma/; /z/zhiyang-zhang/; /y/yang-zhao/; /l/lu-xiang/; /c/chengqing-zong/; /y/yu-zhou/",
        "bibtex": "@inproceedings{liang-etal-2024-document,\n    title = \"Document Image Machine Translation with Dynamic Multi-pre-trained Models Assembling\",\n    author = \"Liang, Yupu  and\n      Zhang, Yaping  and\n      Ma, Cong  and\n      Zhang, Zhiyang  and\n      Zhao, Yang  and\n      Xiang, Lu  and\n      Zong, Chengqing  and\n      Zhou, Yu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.392/\",\n    doi = \"10.18653/v1/2024.naacl-long.392\",\n    pages = \"7084--7095\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.392.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.392/",
        "pdf_size": 4585519,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6192055701906803524&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+Fanyu AI Laboratory, Zhongke Fanyu Technology Co., Ltd, Beijing, China",
        "aff_domain": "ia.ac.cn;ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "ia.ac.cn;ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "https://github.com/liangyupu/DIMTDA",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0+1;0+1;0+1;0+1+2",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Zhongke Fanyu Technology Co., Ltd",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;Fanyu AI Laboratory",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;",
        "aff_unique_abbr": "CAS;UCAS;",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0+0;0+0;0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.290",
        "title": "Does GPT-4 pass the Turing test?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We evaluated GPT-4 in a public online Turing test. The best-performing GPT-4 prompt passed in 49.7% of games, outperforming ELIZA (22%) and GPT-3.5 (20%), but falling short of the baseline set by human participants (66%). Participants\u2019 decisions were based mainly on linguistic style (35%) and socioemotional traits (27%), supporting the idea that intelligence, narrowly conceived, is not sufficient to pass the Turing test. Participant knowledge about LLMs and number of games played positively correlated with accuracy in detecting AI, suggesting learning and practice as possible strategies to mitigate deception. Despite known limitations as a test of intelligence, we argue that the Turing test continues to be relevant as an assessment of naturalistic communication and deception. AI models with the ability to masquerade as humans could have widespread societal consequences, and we analyse the effectiveness of different strategies and criteria for judging humanlikeness.",
        "author": "Cameron Jones; Ben Bergen",
        "authorids": "/c/cameron-jones/; /b/ben-bergen/",
        "bibtex": "@inproceedings{jones-bergen-2024-gpt,\n    title = \"Does {GPT}-4 pass the {T}uring test?\",\n    author = \"Jones, Cameron  and\n      Bergen, Ben\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.290/\",\n    doi = \"10.18653/v1/2024.naacl-long.290\",\n    pages = \"5183--5210\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.290.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.290/",
        "pdf_size": 1008266,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=243338856241965328&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "UC San Diego; UC San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.447",
        "title": "Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Knowledge graphs (KGs) consist of links that describe relationships between entities. Due to the difficulty of manually enumerating all relationships between entities, automatically completing them is essential for KGs. Knowledge Graph Completion (KGC) is a task that infers unseen relationships between entities in a KG. Traditional embedding-based KGC methods (e.g. RESCAL, TransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc.) infer missing links using only the knowledge from training data. In contrast, the recent Pre-trained Language Model (PLM)-based KGC utilizes knowledge obtained during pre-training, which means it can estimate missing links between entities by reusing memorized knowledge from pre-training without inference. This part is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications. To address this issue, we analyze whether PLM-based KGC methods make inferences or merely access memorized knowledge. For this purpose, we propose a method for constructing synthetic datasets specified in this analysis and conclude that PLMs acquire the inference abilities required for KGC through pre-training, even though the performance improvements mostly come from textual information of entities and relations.",
        "author": "Yusuke Sakai; Hidetaka Kamigaito; Katsuhiko Hayashi; Taro Watanabe",
        "authorids": "/y/yusuke-sakai/; /h/hidetaka-kamigaito/; /k/katsuhiko-hayashi/; /t/taro-watanabe/",
        "bibtex": "@inproceedings{sakai-etal-2024-pre,\n    title = \"Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?\",\n    author = \"Sakai, Yusuke  and\n      Kamigaito, Hidetaka  and\n      Hayashi, Katsuhiko  and\n      Watanabe, Taro\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.447/\",\n    doi = \"10.18653/v1/2024.naacl-long.447\",\n    pages = \"8091--8106\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.447.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.447/",
        "pdf_size": 790228,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18357710788220665862&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Nara Institute of Science and Technology; Nara Institute of Science and Technology; The University of Tokyo; Nara Institute of Science and Technology",
        "aff_domain": "is.naist.jp;is.naist.jp;g.ecc.u-tokyo.ac.jp;is.naist.jp",
        "email": "is.naist.jp;is.naist.jp;g.ecc.u-tokyo.ac.jp;is.naist.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Nara Institute of Science and Technology;University of Tokyo",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nist.go.jp;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "NIST;UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2024.findings-naacl.219",
        "title": "Don\u2019t be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Offensive language detection is an important task for filtering out abusive expressions and improving online user experiences. However, malicious users often attempt to avoid filtering systems through the involvement of textual noises. In this paper, we propose these evasions as user-intended adversarial attacks that insert special symbols or leverage the distinctive features of the Korean language. Furthermore, we introduce simple yet effective pooling strategies in a layer-wise manner to defend against the proposed attacks, focusing on the preceding layers not just the last layer to capture both offensiveness and token embeddings. We demonstrate that these pooling strategies are more robust to performance degradation even when the attack rate is increased, without directly training of such patterns. Notably, we found that models pre-trained on clean texts could achieve a comparable performance in detecting attacked offensive language, to models pre-trained on noisy texts by employing these pooling strategies.",
        "author": "Seunguk Yu; Juhwan Choi; YoungBin Kim",
        "authorids": "/s/seunguk-yu/; /j/juhwan-choi/; /y/youngbin-kim/",
        "bibtex": "@inproceedings{yu-etal-2024-dont,\n    title = \"Don{'}t be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks\",\n    author = \"Yu, Seunguk  and\n      Choi, Juhwan  and\n      Kim, YoungBin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.219/\",\n    doi = \"10.18653/v1/2024.findings-naacl.219\",\n    pages = \"3456--3467\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.219.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.219/",
        "pdf_size": 2090694,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15462100787061608635&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Chung-Ang University, Republic of Korea, Seoul; Chung-Ang University, Republic of Korea, Seoul; Chung-Ang University, Republic of Korea, Seoul",
        "aff_domain": "gmail.com;cau.ac.kr;cau.ac.kr",
        "email": "gmail.com;cau.ac.kr;cau.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Chung-Ang University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.cau.ac.kr",
        "aff_unique_abbr": "CAU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-short.71",
        "title": "DoubleLingo: Causal Estimation with Large Language Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Estimating causal effects from non-randomized data requires assumptions about the underlying data-generating process. To achieve unbiased estimates of the causal effect of a treatment on an outcome, we typically adjust for any confounding variables that influence both treatment and outcome. When such confounders include text data, existing causal inference methods struggle due to the high dimensionality of the text. The simple statistical models which have sufficient convergence criteria for causal estimation are not well-equipped to handle noisy unstructured text, but flexible large language models that excel at predictive tasks with text data do not meet the statistical assumptions necessary for causal estimation. Our method enables theoretically consistent estimation of causal effects using LLM-based nuisance models by incorporating them within the framework of Double Machine Learning. On the best available dataset for evaluating such methods, we obtain a 10.4% reduction in the relative absolute error for the estimated causal effect over existing methods.",
        "author": "Marko Veljanovski; Zach Wood-Doughty",
        "authorids": "/m/marko-veljanovski/; /z/zach-wood-doughty/",
        "bibtex": "@inproceedings{veljanovski-wood-doughty-2024-doublelingo,\n    title = \"{D}ouble{L}ingo: Causal Estimation with Large Language Models\",\n    author = \"Veljanovski, Marko  and\n      Wood-Doughty, Zach\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.71/\",\n    doi = \"10.18653/v1/2024.naacl-short.71\",\n    pages = \"799--807\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.71.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.71/",
        "pdf_size": 1697963,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16786120958170833986&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Northwestern University; Northwestern University",
        "aff_domain": "u.northwestern.edu;northwestern.edu",
        "email": "u.northwestern.edu;northwestern.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Northwestern University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.northwestern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.28",
        "title": "DriftWatch: A Tool that Automatically Detects Data Drift and Extracts Representative Examples Affected by Drift",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Data drift, which denotes a misalignment between the distribution of reference (i.e., training) and production data, constitutes a significant challenge for AI applications, as it undermines the generalisation capacity of machine learning (ML) models. Therefore, it is imperative to proactively identify data drift before users meet with performance degradation. Moreover, to ensure the successful execution of AI services, endeavours should be directed not only toward detecting the occurrence of drift but also toward effectively addressing this challenge. % considering the limited resources prevalent in practical industrial domains. In this work, we introduce a tool designed to detect data drift in text data. In addition, we propose an unsupervised sampling technique for extracting representative examples from drifted instances. This approach bestows a practical advantage by significantly reducing expenses associated with annotating the labels for drifted instances, an essential prerequisite for retraining the model to sustain its performance on production data.",
        "author": "Myeongjun Jang; Antonios Georgiadis; Yiyun Zhao; Fran Silavong",
        "authorids": "/m/myeongjun-jang/; /a/antonios-georgiadis/; /y/yiyun-zhao/; /f/fran-silavong/",
        "bibtex": "@inproceedings{jang-etal-2024-driftwatch,\n    title = \"{D}rift{W}atch: A Tool that Automatically Detects Data Drift and Extracts Representative Examples Affected by Drift\",\n    author = \"Jang, Myeongjun  and\n      Georgiadis, Antonios  and\n      Zhao, Yiyun  and\n      Silavong, Fran\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.28/\",\n    doi = \"10.18653/v1/2024.naacl-industry.28\",\n    pages = \"335--346\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.28.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.28/",
        "pdf_size": 643454,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4338273023018755181&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, University of Oxford + J.P. Morgan Chase; J.P. Morgan Chase; J.P. Morgan Chase; J.P. Morgan Chase",
        "aff_domain": "cs.ox.ac.uk;jpmchase.com;jpmchase.com;jpmchase.com",
        "email": "cs.ox.ac.uk;jpmchase.com;jpmchase.com;jpmchase.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;1;1",
        "aff_unique_norm": "University of Oxford;JPMorgan Chase & Co.",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.jpmorganchase.com",
        "aff_unique_abbr": "Oxford;JPM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Oxford;",
        "aff_country_unique_index": "0+1;1;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2024.naacl-long.30",
        "title": "DuRE: Dual Contrastive Self Training for Semi-Supervised Relation Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Document-level Relation Extraction (RE) aims to extract relation triples from documents. Existing document-RE models typically rely on supervised learning which requires substantial labeled data. To alleviate the amount of human supervision, Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models whenever labeled data is insufficient. However, existing ST methods in RE fail to tackle the challenge of long-tail relations. In this work, we propose DuRE, a novel ST framework to tackle these problems. DuRE jointly models RE classification and text generation as a dual process. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We proposed a contrastive loss to leverage the signal of the RE classifier to improve generation quality. In addition, we propose a self-adaptive way to sample pseudo text from different relation classes. Experiments on two document-level RE tasks show that DuRE significantly boosts recall and F1 score with comparable precision, especially for long-tail relations against several strong baselines.",
        "author": "Yuxi Feng; Laks Lakshmanan",
        "authorids": "/y/yuxi-feng/; /l/laks-lakshmanan/",
        "bibtex": "@inproceedings{feng-lakshmanan-2024-dure,\n    title = \"{D}u{RE}: Dual Contrastive Self Training for Semi-Supervised Relation Extraction\",\n    author = \"Feng, Yuxi  and\n      Lakshmanan, Laks\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.30/\",\n    doi = \"10.18653/v1/2024.naacl-long.30\",\n    pages = \"540--555\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.30.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.30/",
        "pdf_size": 332318,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_NPLCV3j9SkJ:scholar.google.com/&scioq=DuRE:+Dual+Contrastive+Self+Training+for+Semi-Supervised+Relation+Extraction&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "The University of British Columbia, Vancouver, Canada; The University of British Columbia, Vancouver, Canada",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-long.182",
        "title": "DynaMo: Accelerating Language Model Inference with Dynamic Multi-Token Sampling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Traditional language models operate autoregressively, i.e., they predict one token at a time. Rapid explosion in model sizes has resulted in high inference times. In this work, we propose DynaMo, a suite of multi-token prediction language models that reduce net inference times. Our models *dynamically* predict multiple tokens based on their confidence in the predicted joint probability distribution. We propose a lightweighttechnique to train these models, leveraging the weights of traditional autoregressive counterparts. Moreover, we propose novel ways to enhance the estimated joint probability to improve text generation quality, namely co-occurrence weighted masking and adaptive thresholding. We also propose systematic qualitative and quantitative methods to rigorously test the quality of generated text for non-autoregressive generation. One of the models in our suite, DynaMo-7.3B-T3, achieves same-quality generated text as the baseline (Pythia-6.9B) while achieving 2.57\u00d7 speed-up with only 5.87% and 2.67% parameter and training time overheads, respectively.",
        "author": "Shikhar Tuli; Chi-Heng Lin; Yen-Chang Hsu; Niraj Jha; Yilin Shen; Hongxia Jin",
        "authorids": "/s/shikhar-tuli/; /c/chi-heng-lin/; /y/yen-chang-hsu/; /n/niraj-jha/; /y/yilin-shen/; /h/hongxia-jin/",
        "bibtex": "@inproceedings{tuli-etal-2024-dynamo,\n    title = \"{D}yna{M}o: Accelerating Language Model Inference with Dynamic Multi-Token Sampling\",\n    author = \"Tuli, Shikhar  and\n      Lin, Chi-Heng  and\n      Hsu, Yen-Chang  and\n      Jha, Niraj  and\n      Shen, Yilin  and\n      Jin, Hongxia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.182/\",\n    doi = \"10.18653/v1/2024.naacl-long.182\",\n    pages = \"3322--3345\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.182.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.182/",
        "pdf_size": 6149508,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10341050452173565716&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Electrical and Computer Engineering, Princeton University + Samsung Research America; Samsung Research America; Samsung Research America; Department of Electrical and Computer Engineering, Princeton University; Samsung Research America; Samsung Research America",
        "aff_domain": "samsung.com;samsung.com;samsung.com;princeton.edu;samsung.com;samsung.com",
        "email": "samsung.com;samsung.com;samsung.com;princeton.edu;samsung.com;samsung.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;0;1;1",
        "aff_unique_norm": "Princeton University;Samsung",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Samsung Research America",
        "aff_unique_url": "https://www.princeton.edu;https://www.samsung.com/us/careers/research/",
        "aff_unique_abbr": "Princeton;SRA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.68",
        "title": "E5: Zero-shot Hierarchical Table Analysis using Augmented LLMs via Explain, Extract, Execute, Exhibit and Extrapolate",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Analyzing large hierarchical tables with multi-level headers presents challenges due to their complex structure, implicit semantics, and calculation relationships. While recent advancements in large language models (LLMs) have shown promise in flat table analysis, their application to hierarchical tables is constrained by the reliance on manually curated exemplars and the model\u2019s token capacity limitations. Addressing these challenges, we introduce a novel code-augmented LLM-based framework, E5, for zero-shot hierarchical table question answering. This approach encompasses self-explaining the table\u2019s hierarchical structures, code generation to extract relevant information and apply operations, external code execution to prevent hallucinations, and leveraging LLMs\u2019 reasoning for final answer derivation. Empirical results indicate that our method, based on GPT-4, outperforms state-of-the-art fine-tuning methods with a 44.38 Exact Match improvement. Furthermore, we present F3, an adaptive algorithm designed for token-limited scenarios, effectively condensing large tables while maintaining useful information. Our experiments prove its efficiency, enabling the processing of large tables even with models having limited context lengths. The code is available at https://github.com/zzh-SJTU/E5-Hierarchical-Table-Analysis.",
        "author": "Zhehao Zhang; Yan Gao; Jian-Guang Lou",
        "authorids": "/z/zhehao-zhang/; /y/yan-gao/; /j/jian-guang-lou/",
        "bibtex": "@inproceedings{zhang-etal-2024-e5,\n    title = \"$E^5$: Zero-shot Hierarchical Table Analysis using Augmented {LLM}s via Explain, Extract, Execute, Exhibit and Extrapolate\",\n    author = \"Zhang, Zhehao  and\n      Gao, Yan  and\n      Lou, Jian-Guang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.68/\",\n    doi = \"10.18653/v1/2024.naacl-long.68\",\n    pages = \"1244--1258\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.68.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.68/",
        "pdf_size": 2282611,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11670000951435162127&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Dartmouth College; Microsoft Research Asia; Microsoft Research Asia",
        "aff_domain": "dartmouth.edu;microsoft.com;microsoft.com",
        "email": "dartmouth.edu;microsoft.com;microsoft.com",
        "github": "https://github.com/zzh-SJTU/E5-Hierarchical-Table-Analysis",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Dartmouth College;Microsoft",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://www.dartmouth.edu;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "Dartmouth;MSR Asia",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.232",
        "title": "EDC: Effective and Efficient Dialog Comprehension For Dialog State Tracking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In Task-Oriented Dialog (TOD) systems, Dialog State Tracking (DST) structurally extracts information from user and system utterances, which can be further used for querying databases and forming responses to users. The two major categories of DST methods, sequential and independent methods, face trade-offs between accuracy and efficiency. To resolve this issue, we propose Effective and Efficient Dialog Comprehension (EDC), an alternative DST approach that leverages the tree structure of the dialog state. EDC predicts domains, slot names and slot values of the dialog state step-by-step for better accuracy, and efficiently encodes dialog contexts with causal attention patterns. We evaluate EDC on several popular TOD datasets and EDC is able to achieve state-of-the-art Joint Goal Accuracy (JGA). We also show theoretically and empirically that EDC is more efficient than model designs used by previous works.",
        "author": "Qifan Lu; Bhaskar Ramasubramanian; Radha Poovendran",
        "authorids": "/q/qifan-lu/; /b/bhaskar-ramasubramanian/; /r/radha-poovendran/",
        "bibtex": "@inproceedings{lu-etal-2024-edc,\n    title = \"{EDC}: Effective and Efficient Dialog Comprehension For Dialog State Tracking\",\n    author = \"Lu, Qifan  and\n      Ramasubramanian, Bhaskar  and\n      Poovendran, Radha\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.232/\",\n    doi = \"10.18653/v1/2024.naacl-long.232\",\n    pages = \"4151--4165\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.232.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.232/",
        "pdf_size": 551593,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:W41TK4sKveQJ:scholar.google.com/&scioq=EDC:+Effective+and+Efficient+Dialog+Comprehension+For+Dialog+State+Tracking&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "University of Washington; Western Washington University; University of Washington",
        "aff_domain": "uw.edu;wwu.edu;uw.edu",
        "email": "uw.edu;wwu.edu;uw.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Washington;Western Washington University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://www.wwu.edu",
        "aff_unique_abbr": "UW;WWU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.71",
        "title": "EDEntail: An Entailment-based Few-shot Text Classification with Extensional Definition",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Few-shot text classification has seen significant advancements, particularly with entailment-based methods, which typically use either class labels or intensional definitions of class labels in hypotheses for label semantics expression. In this paper, we propose EDEntail, a method that employs extensional definition (EDef) of class labels in hypotheses, aiming to express the semantics of class labels more explicitly. To achieve the above goal, we develop an algorithm to gather and select extensional descriptive words of class labels and then order and format them into a sequence to form hypotheses. Our method has been evaluated and compared with state-of-the-art models on five classification datasets. The results demonstrate that our approach surpasses the supervised-learning methods and prompt-based methods under the few-shot setting, which underlines the potential of using an extensional definition of class labels for entailment-based few-shot text classification. Our code is available at https://github.com/MidiyaZhu/EDEntail.",
        "author": "Zixiao Zhu; Junlang Qian; Zijian Feng; Hanzhang Zhou; Kezhi Mao",
        "authorids": "/z/zixiao-zhu/; /j/junlang-qian/; /z/zijian-feng/; /h/hanzhang-zhou/; /k/kezhi-mao/",
        "bibtex": "@inproceedings{zhu-etal-2024-edentail,\n    title = \"{EDE}ntail: An Entailment-based Few-shot Text Classification with Extensional Definition\",\n    author = \"Zhu, Zixiao  and\n      Qian, Junlang  and\n      Feng, Zijian  and\n      Zhou, Hanzhang  and\n      Mao, Kezhi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.71/\",\n    doi = \"10.18653/v1/2024.findings-naacl.71\",\n    pages = \"1124--1137\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.71.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.71/",
        "pdf_size": 402238,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9393939465620022317&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Institute of Catastrophe Risk Management, Interdisciplinary Graduate Programme, Nanyang Technological University, Singapore + Future Resilient Systems Programme, Singapore-ETH Centre, CREATE campus, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Institute of Catastrophe Risk Management, Interdisciplinary Graduate Programme, Nanyang Technological University, Singapore + Future Resilient Systems Programme, Singapore-ETH Centre, CREATE campus, Singapore; Institute of Catastrophe Risk Management, Interdisciplinary Graduate Programme, Nanyang Technological University, Singapore + Future Resilient Systems Programme, Singapore-ETH Centre, CREATE campus, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",
        "aff_domain": "e.ntu.edu.sg;e.ntu.edu.sg;e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg",
        "email": "e.ntu.edu.sg;e.ntu.edu.sg;e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg",
        "github": "https://github.com/MidiyaZhu/EDEntail",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0+1;0+1;0",
        "aff_unique_norm": "Nanyang Technological University;Singapore-ETH Centre",
        "aff_unique_dep": "Institute of Catastrophe Risk Management;Future Resilient Systems Programme",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.singapore-eth-centre.sg",
        "aff_unique_abbr": "NTU;ETH Centre",
        "aff_campus_unique_index": "1;2;1;1;2",
        "aff_campus_unique": ";CREATE;Singapore",
        "aff_country_unique_index": "0+0;0;0+0;0+0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2024.naacl-industry.40",
        "title": "EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In e-commerce, accurately extracting product attribute values from multimodal data is crucial for improving user experience and operational efficiency of retailers. However, previous approaches to multimodal attribute value extraction often struggle with implicit attribute values embedded in images or text, rely heavily on extensive labeled data, and can easily confuse similar attribute values. To address these issues, we introduce EIVEN, a data- and parameter-efficient generative framework that pioneers the use of multimodal LLM for implicit attribute value extraction. EIVEN leverages the rich inherent knowledge of a pre-trained LLM and vision encoder to reduce reliance on labeled data. We also introduce a novel Learning-by-Comparison technique to reduce model confusion by enforcing attribute value comparison and difference identification. Additionally, we construct initial open-source datasets for multimodal implicit attribute value extraction. Our extensive experiments reveal that EIVEN significantly outperforms existing methods in extracting implicit attribute values while requiring less labeled data.",
        "author": "Henry Zou; Gavin Yu; Ziwei Fan; Dan Bu; Han Liu; Peng Dai; Dongmei Jia; Cornelia Caragea",
        "authorids": "/h/henry-zou/; /g/gavin-yu/; /z/ziwei-fan/; /d/dan-bu/; /h/han-liu/; /p/peng-dai/; /d/dongmei-jia/; /c/cornelia-caragea/",
        "bibtex": "@inproceedings{zou-etal-2024-eiven,\n    title = \"{EIVEN}: Efficient Implicit Attribute Value Extraction using Multimodal {LLM}\",\n    author = \"Zou, Henry  and\n      Yu, Gavin  and\n      Fan, Ziwei  and\n      Bu, Dan  and\n      Liu, Han  and\n      Dai, Peng  and\n      Jia, Dongmei  and\n      Caragea, Cornelia\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.40/\",\n    doi = \"10.18653/v1/2024.naacl-industry.40\",\n    pages = \"453--463\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.40.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.40/",
        "pdf_size": 1332375,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=303697789399440669&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2024.naacl-long.293",
        "title": "EMONA: Event-level Moral Opinions in News Articles",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most previous research on moral frames has focused on social media short texts, little work has explored moral sentiment within news articles. In news articles, authors often express their opinions or political stance through moral judgment towards events, specifically whether the event is right or wrong according to social moral rules. This paper initiates a new task to understand moral opinions towards events in news articles. We have created a new dataset, EMONA, and annotated event-level moral opinions in news articles. This dataset consists of 400 news articles containing over 10k sentences and 45k events, among which 9,613 events received moral foundation labels. Extracting event morality is a challenging task, as moral judgment towards events can be very implicit. Baseline models were built for event moral identification and classification. In addition, we also conduct extrinsic evaluations to integrate event-level moral opinions into three downstream tasks. The statistical analysis and experiments show that moral opinions of events can serve as informative features for identifying ideological bias or subjective events.",
        "author": "Yuanyuan Lei; Md Messal Monem Miah; Ayesha Qamar; Sai Ramana Reddy; Jonathan Tong; Haotian Xu; Ruihong Huang",
        "authorids": "/y/yuanyuan-lei/; /m/md-messal-monem-miah/; /a/ayesha-qamar/; /s/sai-ramana-reddy/; /j/jonathan-tong/; /h/haotian-xu/; /r/ruihong-huang/",
        "bibtex": "@inproceedings{lei-etal-2024-emona,\n    title = \"{EMONA}: Event-level Moral Opinions in News Articles\",\n    author = \"Lei, Yuanyuan  and\n      Miah, Md Messal Monem  and\n      Qamar, Ayesha  and\n      Reddy, Sai Ramana  and\n      Tong, Jonathan  and\n      Xu, Haotian  and\n      Huang, Ruihong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.293/\",\n    doi = \"10.18653/v1/2024.naacl-long.293\",\n    pages = \"5239--5251\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.293.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.293/",
        "pdf_size": 1726999,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18161033757263715689&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://github.com/yuanyuanlei-nlp/EMONA_dataset",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2024.naacl-short.35",
        "title": "EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Language models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques,such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is tailored for four scenarios, including standard and chain-of-thought prompting, in both zero-shot and few-shot settings. Experimental results show that EchoPrompt yields substantial improvementsacross all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g., GSM8K, SVAMP), reading comprehension (e.g., DROP), and logical reasoning (e.g., Coin flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. Our empirical results indicate that EchoPrompt is an effective technique that enhances in-context learning performance.",
        "author": "Raja Sekhar Reddy Mekala; Yasaman Razeghi; Sameer Singh",
        "authorids": "/r/raja-sekhar-reddy-mekala/; /y/yasaman-razeghi/; /s/sameer-singh/",
        "bibtex": "@inproceedings{mekala-etal-2024-echoprompt,\n    title = \"{E}cho{P}rompt: Instructing the Model to Rephrase Queries for Improved In-context Learning\",\n    author = \"Mekala, Raja Sekhar Reddy  and\n      Razeghi, Yasaman  and\n      Singh, Sameer\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.35/\",\n    doi = \"10.18653/v1/2024.naacl-short.35\",\n    pages = \"399--432\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.35.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.35/",
        "pdf_size": 702385,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4234905109766671388&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, Irvine; University of California, Irvine; University of California, Irvine",
        "aff_domain": "uci.edu;uci.edu;uci.edu",
        "email": "uci.edu;uci.edu;uci.edu",
        "github": "https://rajasekharmekala.github.io/echoprompt/",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.27",
        "title": "EcoSpeak: Cost-Efficient Bias Mitigation for Partially Cross-Lingual Speaker Verification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Linguistic bias is a critical problem concerning the diversity, equity, and inclusiveness of Natural Language Processing tools. The severity of this problem intensifies in security systems, such as speaker verification, where fairness is paramount. Speaker verification systems are biometric systems that determine whether two speech recordings are of the same speaker. Such user-centric systems should be inclusive to bilingual speakers. However, Deep neural network models are linguistically biased. Linguistic bias can be full or partial. Partially cross-lingual bias occurs when one test trial pair recording is in the training set\u2019s language, and the other is in an unseen target language. Such linguistic mismatch influences the speaker verification model\u2019s decision, dissuading bilingual speakers from using the system. Domain adaptation can mitigate this problem. However, adapting to each existing language is expensive. This paper explores cost-efficient bias mitigation techniques for partially cross-lingual speaker verification. We study the behavior of five baselines in five partially cross-lingual scenarios. Using our baseline behavioral insights, we propose EcoSpeak, a low-cost solution to partially cross-lingual speaker verification. EcoSpeak incorporates contrastive linguistic (CL) attention. CL attention utilizes linguistic differences in trial pairs to emphasize relevant speaker verification embedding parts. Experimental results demonstrate EcoSpeak\u2019s robustness to partially cross-lingual testing.",
        "author": "Divya Sharma",
        "authorids": "/d/divya-sharma/",
        "bibtex": "@inproceedings{sharma-2024-ecospeak,\n    title = \"{E}co{S}peak: Cost-Efficient Bias Mitigation for Partially Cross-Lingual Speaker Verification\",\n    author = \"Sharma, Divya\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.27/\",\n    doi = \"10.18653/v1/2024.findings-naacl.27\",\n    pages = \"379--394\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.27.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.27/",
        "pdf_size": 451024,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:Kr4WvtuIdXYJ:scholar.google.com/&scioq=EcoSpeak:+Cost-Efficient+Bias+Mitigation+for+Partially+Cross-Lingual+Speaker+Verification&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "IIIT-Delhi",
        "aff_domain": "iiitd.ac.in",
        "email": "iiitd.ac.in",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Indraprastha Institute of Information Technology, Delhi",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iiitdelhi.ac.in",
        "aff_unique_abbr": "IIIT-D",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Delhi",
        "aff_country_unique_index": "0",
        "aff_country_unique": "India"
    },
    {
        "id": "2024.naacl-demo.3",
        "title": "EdTec-QBuilder: A Semantic Retrieval Tool for Assembling Vocational Training Exams in German Language",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Selecting and assembling test items from a validated item database into comprehensive exam forms is an under-researched but significant challenge in education. Search and retrieval methods provide a robust framework to assist educators when filtering and assembling relevant test items. In this work, we present EdTec-QBuilder, a semantic search tool developed to assist vocational educators in assembling exam forms. To implement EdTec-QBuilder\u2019s core search functionality, we evaluated eight retrieval strategies and twenty-five popular pre-trained sentence similarity models. Our evaluation revealed that employing cross-encoders to re-rank an initial list of relevant items is best for assisting vocational trainers in assembling examination forms. Beyond topic-based exam assembly, EdTec-QBuilder aims to provide a crowdsourcing infrastructure enabling manual exam assembly data collection, which is critical for future research and development in assisted and automatic exam assembly models.",
        "author": "Alonso Palomino; Andreas Fischer; Jakub Kuzilek; Jarek Nitsch; Niels Pinkwart; Benjamin Paassen",
        "authorids": "/a/alonso-palomino/; /a/andreas-fischer/; /j/jakub-kuzilek/; /j/jarek-nitsch/; /n/niels-pinkwart/; /b/benjamin-paassen/",
        "bibtex": "@inproceedings{palomino-etal-2024-edtec,\n    title = \"{E}d{T}ec-{QB}uilder: A Semantic Retrieval Tool for Assembling Vocational Training Exams in {G}erman Language\",\n    author = \"Palomino, Alonso  and\n      Fischer, Andreas  and\n      Kuzilek, Jakub  and\n      Nitsch, Jarek  and\n      Pinkwart, Niels  and\n      Paassen, Benjamin\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.3/\",\n    doi = \"10.18653/v1/2024.naacl-demo.3\",\n    pages = \"26--35\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.3.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.3/",
        "pdf_size": 2129414,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18006265604042234079&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": "Educational Technology Lab, DFKI + Universit\u00e4t Bielefeld; Forschungsinstitut Betriebliche Bildung (f-bb); Educational Technology Lab, DFKI; bfz gGmbH; Educational Technology Lab, DFKI; Educational Technology Lab, DFKI + Universit\u00e4t Bielefeld",
        "aff_domain": "dfki.de;f-bb.de; ;bfz.de;dfki.de;uni-bielefeld.de",
        "email": "dfki.de;f-bb.de; ;bfz.de;dfki.de;uni-bielefeld.de",
        "github": "",
        "project": "https://www.dfki.de/kiperweb/about.html",
        "author_num": 6,
        "aff_unique_index": "0+1;2;0;3;0;0+1",
        "aff_unique_norm": "German Research Center for Artificial Intelligence;Universit\u00e4t Bielefeld;Forschungsinstitut Betriebliche Bildung;bfz gGmbH",
        "aff_unique_dep": "Educational Technology Lab;;;",
        "aff_unique_url": "https://www.dfki.de;https://www.uni-bielefeld.de/;https://www.f-bb.de;",
        "aff_unique_abbr": "DFKI;Uni Bielefeld;f-bb;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-demo.6",
        "title": "Edu-ConvoKit: An Open-Source Library for Education Conversation Data",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We introduce Edu-ConvoKit, an open-source library designed to handle pre-processing, annotation and analysis of conversation data in education. Resources for analyzing education conversation data are scarce, making the research challenging to perform and therefore hard to access. We address these challenges with Edu-ConvoKit. Edu-ConvoKit is open-source [1], pip-installable [2], with comprehensive documentation [3]. Our demo video is available at: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8-. We include additional resources, such as Colab applications of Edu-ConvoKit to three diverse education datasets [4] and a repository of Edu-ConvoKit-related papers [5].[1] https://github.com/stanfordnlp/edu-convokit[2] https://pypi.org/project/edu-convokit/[3] https://edu-convokit.readthedocs.io/en/latest/[4] https://github.com/stanfordnlp/edu-convokit?tab=readme-ov-file#datasets-with-edu-convokit[5] https://github.com/stanfordnlp/edu-convokit/blob/main/papers.md",
        "author": "Rose Wang; Dorottya Demszky",
        "authorids": "/r/rose-wang/; /d/dorottya-demszky/",
        "bibtex": "@inproceedings{wang-demszky-2024-edu,\n    title = \"Edu-{C}onvo{K}it: An Open-Source Library for Education Conversation Data\",\n    author = \"Wang, Rose  and\n      Demszky, Dorottya\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.6/\",\n    doi = \"10.18653/v1/2024.naacl-demo.6\",\n    pages = \"61--69\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.6.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.6/",
        "pdf_size": 2838055,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4045306990519661703&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Stanford University; Stanford University",
        "aff_domain": "cs.stanford.edu;stanford.edu",
        "email": "cs.stanford.edu;stanford.edu",
        "github": "https://github.com/stanfordnlp/edu-convokit",
        "project": "https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8-",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.346",
        "title": "Effective Large Language Model Adaptation for Improved Grounding and Citation Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) have achieved remarkable advancements in natural language understanding and generation. However, one major issue towards their widespread deployment in the real world is that they can generate \u201challucinated\u201d answers that are not factual.Towards this end, this paper focuses on improving LLMs by grounding their responses in retrieved passages and by providing citations. We propose a new framework, AGREE, Adaptation for GRounding EnhancEment, that improves the grounding from a holistic perspective. Our framework tunes LLMs to self-ground the claims in their responses and provide accurate citations to retrieved documents. This tuning on top of the pre-trained LLMs requires well-grounded responses (with citations) for paired queries, for which we introduce a method that can automatically construct such data from unlabeled queries. The self-grounding capability of tuned LLMs further grants them a test-time adaptation (TTA) capability that can actively retrieve passages to support the claims that have not been grounded, which iteratively improves the responses of LLMs. Across five datasets and two LLMs, our results show that the proposed tuning-based framework generates superior grounded responses with more accurate citations compared to prompting-based approaches and post-hoc citing-based approaches.",
        "author": "Xi Ye; Ruoxi Sun; Sercan Arik; Tomas Pfister",
        "authorids": "/x/xi-ye/; /r/ruoxi-sun/; /s/sercan-arik/; /t/tomas-pfister/",
        "bibtex": "@inproceedings{ye-etal-2024-effective,\n    title = \"Effective Large Language Model Adaptation for Improved Grounding and Citation Generation\",\n    author = \"Ye, Xi  and\n      Sun, Ruoxi  and\n      Arik, Sercan  and\n      Pfister, Tomas\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.346/\",\n    doi = \"10.18653/v1/2024.naacl-long.346\",\n    pages = \"6237--6251\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.346.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.346/",
        "pdf_size": 969305,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9503179118068528405&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "The University of Texas at Austin\u2666; Google Cloud AI\u2660; Google Cloud AI\u2660; Google Cloud AI\u2660",
        "aff_domain": "cs.utexas.edu;google.com;google.com;google.com",
        "email": "cs.utexas.edu;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Texas at Austin;Google",
        "aff_unique_dep": ";Google Cloud AI",
        "aff_unique_url": "https://www.utexas.edu;https://cloud.google.com/ai",
        "aff_unique_abbr": "UT Austin;Google Cloud AI",
        "aff_campus_unique_index": "0;1;1;1",
        "aff_campus_unique": "Austin;Mountain View",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.260",
        "title": "Effective Long-Context Scaling of Foundation Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present an effective recipe to train strong long-context LLMs that are capable of utilizing massive context windows of up to 32,000 tokens. Our models are built through continual pretraining from Llama 2 checkpoints with longer text sequences and on a dataset where long texts are upsampled. We perform extensive evaluation using language modeling, synthetic context probing tasks, and a wide range of downstream benchmarks. Across all evaluations, our models achieve consistent improvements on most regular-context tasks and significant improvements on long-context tasks over Llama 2. Moreover, with a cost-effective instruction tuning procedure that is free of expensive annotation, the presented models can already surpass gpt-3.5-turbo-16k\u2018s overall performance on long-context benchmarks. Alongside these results, we provide an in-depth analysis on each individual component of our method. We delve into Llama\u2019s position encodings and discuss its key limitation in modeling long data. We examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths \u2013 ablation results suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.",
        "author": "Wenhan Xiong; Jingyu Liu; Igor Molybog; Hejia Zhang; Prajjwal Bhargava; Rui Hou; Louis Martin; Rashi Rungta; Karthik Abinav Sankararaman; Barlas Oguz; Madian Khabsa; Han Fang; Yashar Mehdad; Sharan Narang; Kshitiz Malik; Angela Fan; Shruti Bhosale; Sergey Edunov; Mike Lewis; Sinong Wang; Hao Ma",
        "authorids": "/w/wenhan-xiong/; /j/jingyu-liu/; /i/igor-molybog/; /h/hejia-zhang/; /p/prajjwal-bhargava/; /r/rui-hou/; /l/louis-martin/; /r/rashi-rungta/; /k/karthik-abinav-sankararaman/; /b/barlas-oguz/; /m/madian-khabsa/; /h/han-fang/; /y/yashar-mehdad/; /s/sharan-narang/; /k/kshitiz-malik/; /a/angela-fan/; /s/shruti-bhosale/; /s/sergey-edunov/; /m/mike-lewis/; /s/sinong-wang/; /h/hao-ma/",
        "bibtex": "@inproceedings{xiong-etal-2024-effective,\n    title = \"Effective Long-Context Scaling of Foundation Models\",\n    author = \"Xiong, Wenhan  and\n      Liu, Jingyu  and\n      Molybog, Igor  and\n      Zhang, Hejia  and\n      Bhargava, Prajjwal  and\n      Hou, Rui  and\n      Martin, Louis  and\n      Rungta, Rashi  and\n      Sankararaman, Karthik Abinav  and\n      Oguz, Barlas  and\n      Khabsa, Madian  and\n      Fang, Han  and\n      Mehdad, Yashar  and\n      Narang, Sharan  and\n      Malik, Kshitiz  and\n      Fan, Angela  and\n      Bhosale, Shruti  and\n      Edunov, Sergey  and\n      Lewis, Mike  and\n      Wang, Sinong  and\n      Ma, Hao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.260/\",\n    doi = \"10.18653/v1/2024.naacl-long.260\",\n    pages = \"4643--4663\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.260.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.260/",
        "pdf_size": 1454195,
        "gs_citation": 231,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11499544443437070837&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta; GenAI, Meta",
        "aff_domain": "meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com",
        "email": "meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com;meta.com",
        "github": "",
        "project": "",
        "author_num": 21,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Meta",
        "aff_unique_dep": "Meta",
        "aff_unique_url": "https://meta.com",
        "aff_unique_abbr": "Meta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.6",
        "title": "Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations.A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We validate our retrieval approach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B. The experimental results show a significant improvement over relevant baselines in real few-shot DST settings.",
        "author": "Seanie Lee; Jianpeng Cheng; Joris Driesen; Alexandru Coca; Anders Johannsen",
        "authorids": "/s/seanie-lee/; /j/jianpeng-cheng/; /j/joris-driesen/; /a/alexandru-coca/; /a/anders-johannsen/",
        "bibtex": "@inproceedings{lee-etal-2024-effective,\n    title = \"Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries\",\n    author = \"Lee, Seanie  and\n      Cheng, Jianpeng  and\n      Driesen, Joris  and\n      Coca, Alexandru  and\n      Johannsen, Anders\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.6/\",\n    doi = \"10.18653/v1/2024.naacl-long.6\",\n    pages = \"96--111\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.6.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.6/",
        "pdf_size": 759399,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4823581155836484443&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "KAIST; Apple; Apple; University of Cambridge; Apple",
        "aff_domain": "kaist.ac.kr;apple.com;apple.com;cam.ac.uk;apple.com",
        "email": "kaist.ac.kr;apple.com;apple.com;cam.ac.uk;apple.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Apple;University of Cambridge",
        "aff_unique_dep": ";Apple Inc.;",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.apple.com;https://www.cam.ac.uk",
        "aff_unique_abbr": "KAIST;Apple;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;1;2;1",
        "aff_country_unique": "South Korea;United States;United Kingdom"
    },
    {
        "id": "2024.naacl-long.139",
        "title": "Efficient Benchmarking (of Language Models)",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The increasing versatility of language models (LMs) has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs, extending to thousands of GPU hours per model. However, the efficiency aspect of these evaluation efforts had raised little discussion in the literature.In this work, we present the problem of Efficient Benchmarking, namely, intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case, we investigate how different benchmark design choices affect the computation-reliability trade-off. We propose to evaluate the reliability of such decisions, by using a new measure \u2013 Decision Impact on Reliability, DIoR for short.We find, for example, that a benchmark leader may change by merely removing a low-ranked model from the benchmark, and observe that a correct benchmark ranking can be obtained by considering only a fraction of the evaluation examples.Based on our findings, we outline a set of concrete recommendations for efficient benchmark design and utilization practices. To take a step further, we use our findings to propose an evaluation algorithm, that, when applied to the HELM benchmark, leads to dramatic cost savings with minimal loss of benchmark reliability, often reducing computation by x100 or more.",
        "author": "Yotam Perlitz; Elron Bandel; Ariel Gera; Ofir Arviv; Liat Ein-Dor; Eyal Shnarch; Noam Slonim; Michal Shmueli-Scheuer; Leshem Choshen",
        "authorids": "/y/yotam-perlitz/; /e/elron-bandel/; /a/ariel-gera/; /o/ofir-arviv/; /l/liat-ein-dor/; /e/eyal-shnarch/; /n/noam-slonim/; /m/michal-shmueli-scheuer/; /l/leshem-choshen/",
        "bibtex": "@inproceedings{perlitz-etal-2024-efficient,\n    title = \"Efficient Benchmarking (of Language Models)\",\n    author = \"Perlitz, Yotam  and\n      Bandel, Elron  and\n      Gera, Ariel  and\n      Arviv, Ofir  and\n      Ein-Dor, Liat  and\n      Shnarch, Eyal  and\n      Slonim, Noam  and\n      Shmueli-Scheuer, Michal  and\n      Choshen, Leshem\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.139/\",\n    doi = \"10.18653/v1/2024.naacl-long.139\",\n    pages = \"2519--2536\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.139.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.139/",
        "pdf_size": 1032594,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4377080613311295465&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9
    },
    {
        "id": "2024.findings-naacl.277",
        "title": "Efficient Citer: Tuning Large Language Models for Enhanced Answer Quality and Verification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In recent years, there has been a growing interest in utilizing external knowledge to reduce hallucinations in large language models (LLMs) and provide them with updated information. Despite this improvement, a major challenge lies in the lack of explicit citations, which hampers the ability to verify the information generated by these models.This paper focuses on providing models with citation capabilities efficiently. By constructing a dataset of citations, we train two model architectures: an FID-style FLAN-T5 model for efficient answer composition and a 13B model known for its success in instruction following after tuning. Evaluation on fluency, correctness, and citation quality is conducted through human assessment and the newly introduced Automatic LLMs\u2019 Citation Evaluation (ALCE) benchmark.Results demonstrate significant improvements in answer quality and efficiency, surpassing the performance of the popular ChatGPT on some of the metrics. The models exhibit exceptional out-of-domain generalization in both human and automatic evaluation. Notably, the FID-style FLAN-T5 model with only 3B parameters performs impressively compared to the 13B model.",
        "author": "Marzieh Tahaei; Aref Jafari; Ahmad Rashid; David Alfonso-Hermelo; Khalil Bibi; Yimeng Wu; Ali Ghodsi; Boxing Chen; Mehdi Rezagholizadeh",
        "authorids": "/m/marzieh-tahaei/; /a/aref-jafari/; /a/ahmad-rashid/; /d/david-alfonso-hermelo/; /k/khalil-bibi/; /y/yimeng-wu/; /a/ali-ghodsi/; /b/boxing-chen/; /m/mehdi-rezagholizadeh/",
        "bibtex": "@inproceedings{tahaei-etal-2024-efficient,\n    title = \"Efficient Citer: Tuning Large Language Models for Enhanced Answer Quality and Verification\",\n    author = \"Tahaei, Marzieh  and\n      Jafari, Aref  and\n      Rashid, Ahmad  and\n      Alfonso-Hermelo, David  and\n      Bibi, Khalil  and\n      Wu, Yimeng  and\n      Ghodsi, Ali  and\n      Chen, Boxing  and\n      Rezagholizadeh, Mehdi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.277/\",\n    doi = \"10.18653/v1/2024.findings-naacl.277\",\n    pages = \"4443--4450\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.277.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.277/",
        "pdf_size": 227094,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9444242923872906966&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Huawei Noah\u2019s Ark Lab+David R. Cheriton School of Computer Science, University of Waterloo; Huawei Noah\u2019s Ark Lab+David R. Cheriton School of Computer Science, University of Waterloo; David R. Cheriton School of Computer Science, University of Waterloo; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab; David R. Cheriton School of Computer Science, University of Waterloo; Huawei Noah\u2019s Ark Lab; Huawei Noah\u2019s Ark Lab",
        "aff_domain": "huawei.com;uwaterloo.ca;uwaterloo.ca;huawei.com;huawei.com;huawei.com;uwaterloo.ca;huawei.com;huawei.com",
        "email": "huawei.com;uwaterloo.ca;uwaterloo.ca;huawei.com;huawei.com;huawei.com;uwaterloo.ca;huawei.com;huawei.com",
        "github": "",
        "project": "https://www.bing.com/new?scdexwlcs=1; https://you.com/; https://perplexity.ai/",
        "author_num": 9,
        "aff_unique_index": "0+1;0+1;1;0;0;0;1;0;0",
        "aff_unique_norm": "Huawei;University of Waterloo",
        "aff_unique_dep": "Noah\u2019s Ark Lab;David R. Cheriton School of Computer Science",
        "aff_unique_url": "https://www.huawei.com;https://uwaterloo.ca",
        "aff_unique_abbr": "Huawei;UWaterloo",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;1;0;0;0;1;0;0",
        "aff_country_unique": "China;Canada"
    },
    {
        "id": "2024.findings-naacl.47",
        "title": "Efficient Dependency Tree Sampling Without Replacement",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In the context of computational models of dependency syntax, most dependency treebanks have the restriction that any valid dependency tree must have exactly one edge coming out of the root node in addition to respecting the spanning tree constraints. Many algorithms for dependency tree sampling were recently proposed, both for sampling with and without replacement.In this paper we propose a new algorithm called Wilson Reject SWOR for the case of sampling without replacement by adapting the Wilson Reject algorithm originally created for sampling with replacement and combining it with a Trie data structure. Experimental results indicate the efficiency of our approach in the scenario of sampling without replacement from dependency graphs with random weights.",
        "author": "Bogdan Dobre",
        "authorids": "/b/bogdan-dobre/",
        "bibtex": "@inproceedings{dobre-2024-efficient,\n    title = \"Efficient Dependency Tree Sampling Without Replacement\",\n    author = \"Dobre, Bogdan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.47/\",\n    doi = \"10.18653/v1/2024.findings-naacl.47\",\n    pages = \"736--741\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.47.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.47/",
        "pdf_size": 343599,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:WVL9makFRbMJ:scholar.google.com/&scioq=Efficient+Dependency+Tree+Sampling+Without+Replacement&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Bucharest",
        "aff_domain": "s.unibuc.ro",
        "email": "s.unibuc.ro",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Bucharest",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unibuc.ro",
        "aff_unique_abbr": "Unibuc",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Romania"
    },
    {
        "id": "2024.naacl-long.465",
        "title": "Efficient End-to-End Visual Document Understanding with Rationale Distillation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding visually situated language requires interpreting complex layouts of textual and visual elements. Pre-processing tools, such as optical character recognition (OCR), can map document image inputs to textual tokens, then large language models (LLMs) can reason over text.However, such methods have high computational and engineering complexity. Can small pretrained image-to-text models accurately understand visual documents through similar recognition and reasoning steps instead?We propose Rationale Distillation (RD), which incorporates the outputs of OCR tools, LLMs, and larger multimodal models as intermediate \u201crationales\u201d, and trains a small student model to predict both rationales and answers. On three visual document understanding benchmarks representing infographics, scanned documents, and figures, our Pix2Struct (282M parameters) student model finetuned with RD outperforms the base model by 4-5% absolute accuracy with only 1% higher computational cost.",
        "author": "Wang Zhu; Alekh Agarwal; Mandar Joshi; Robin Jia; Jesse Thomason; Kristina Toutanova",
        "authorids": "/w/wang-zhu/; /a/alekh-agarwal/; /m/mandar-joshi/; /r/robin-jia/; /j/jesse-thomason/; /k/kristina-toutanova/",
        "bibtex": "@inproceedings{zhu-etal-2024-efficient,\n    title = \"Efficient End-to-End Visual Document Understanding with Rationale Distillation\",\n    author = \"Zhu, Wang  and\n      Agarwal, Alekh  and\n      Joshi, Mandar  and\n      Jia, Robin  and\n      Thomason, Jesse  and\n      Toutanova, Kristina\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.465/\",\n    doi = \"10.18653/v1/2024.naacl-long.465\",\n    pages = \"8401--8424\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.465.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.465/",
        "pdf_size": 1590092,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1166368788509157608&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Google DeepMind\u2660; Google Research\u2663; Google DeepMind\u2660; University of Southern California\u2661; University of Southern California\u2661; Google DeepMind\u2660",
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;1;0",
        "aff_unique_norm": "Google;University of Southern California",
        "aff_unique_dep": "Google DeepMind;",
        "aff_unique_url": "https://deepmind.com;https://www.usc.edu",
        "aff_unique_abbr": "DeepMind;USC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;0;1;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2024.naacl-short.54",
        "title": "Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification. Representations of textual data extract rich information spanning the domain, entities, and relations. In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning. While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped. To address this, we propose aligning multiple sentence representations, such as the CLS] token, the [MASK] token used in prompting, and entity marker tokens. Our method employs contrastive learning to extract complementary discriminative information from these individual representations. This is particularly relevant in low-resource settings where information is scarce. Leveraging multiple sentence representations is especially effective in distilling discriminative information for relation classification when additional information, like relation descriptions, are not available. We validate the adaptability of our approach, maintaining robust performance in scenarios that include relation descriptions, and showcasing its flexibility to adapt to different resource constraints.",
        "author": "Philipp Borchert; Jochen De Weerdt; Marie-Francine Moens",
        "authorids": "/p/philipp-borchert/; /j/jochen-de-weerdt/; /m/marie-francine-moens/",
        "bibtex": "@inproceedings{borchert-etal-2024-efficient,\n    title = \"Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning\",\n    author = \"Borchert, Philipp  and\n      De Weerdt, Jochen  and\n      Moens, Marie-Francine\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.54/\",\n    doi = \"10.18653/v1/2024.naacl-short.54\",\n    pages = \"638--646\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.54.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.54/",
        "pdf_size": 530922,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12260824475992386585&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.naacl-short.57",
        "title": "Efficient Sample-Specific Encoder Perturbations",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Encoder-decoder foundation models have displayed state-of-the-art performance on a range of autoregressive sequence tasks. This paper proposes a simple and lightweight modification to such systems to control the behaviour according to a specific attribute of interest. This paper proposes a novel inference-efficient approach to modifying the behaviour of an encoder-decoder system according to a specific attribute of interest. Specifically, we show that a small proxy network can be used to find a sample-by-sample perturbation of the encoder output of a frozen foundation model to trigger the decoder to generate improved decodings. This work explores a specific realization of this framework focused on improving the COMET performance of Flan-T5 on Machine Translation and the WER of Whisper foundation models on Speech Recognition. Results display consistent improvements in performance evaluated through COMET and WER respectively. Furthermore, experiments also show that the proxies are robust to the exact nature of the data used to train them and can extend to other domains.",
        "author": "Yassir Fathullah; Mark Gales",
        "authorids": "/y/yassir-fathullah/; /m/mark-gales/",
        "bibtex": "@inproceedings{fathullah-gales-2024-efficient,\n    title = \"Efficient Sample-Specific Encoder Perturbations\",\n    author = \"Fathullah, Yassir  and\n      Gales, Mark\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.57/\",\n    doi = \"10.18653/v1/2024.naacl-short.57\",\n    pages = \"663--671\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.57.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.57/",
        "pdf_size": 579675,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:7l8p-q1fLEYJ:scholar.google.com/&scioq=Efficient+Sample-Specific+Encoder+Perturbations&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "ALTA Institute, Department of Engineering, University of Cambridge; ALTA Institute, Department of Engineering, University of Cambridge",
        "aff_domain": "cam.ac.uk;eng.cam.ac.uk",
        "email": "cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-industry.5",
        "title": "Efficiently Distilling LLMs for Edge Applications",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Supernet training of LLMs is of great interest in industrial applications as it confers the ability to produce a palette of smaller models at constant cost, regardless of the number of models (of different size / latency) produced. We propose a new method called Multistage Low-rank Fine-tuning of Super-transformers (MLFS) for parameter-efficient supernet training. We show that it is possible to obtain high-quality encoder models that are suitable for commercial edge applications, and that while decoder-only models are resistant to a comparable degree of compression, decoders can be effectively sliced for a significant reduction in training time.",
        "author": "Achintya Kundu; Yu Chin Fabian Lim; Aaron Chew; Laura Wynter; Penny Chong; Rhui Lee",
        "authorids": "/a/achintya-kundu/; /y/yu-chin-fabian-lim/; /a/aaron-chew/; /l/laura-wynter/; /p/penny-chong/; /r/rhui-lee/",
        "bibtex": "@inproceedings{kundu-etal-2024-efficiently,\n    title = \"Efficiently Distilling {LLM}s for Edge Applications\",\n    author = \"Kundu, Achintya  and\n      Lim, Yu Chin Fabian  and\n      Chew, Aaron  and\n      Wynter, Laura  and\n      Chong, Penny  and\n      Lee, Rhui\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.5/\",\n    doi = \"10.18653/v1/2024.naacl-industry.5\",\n    pages = \"52--62\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.5.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.5/",
        "pdf_size": 558500,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=549054822713227289&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "IBM Research, Singapore; IBM Research, Singapore; IBM Research, Singapore; IBM Research, Singapore; IBM Research, Singapore; IBM Research, Singapore",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2024.naacl-long.393",
        "title": "Elastic Weight Removal for Faithful and Abstractive Dialogue Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generating factual responses is a crucial requirement for dialogue systems. To promotemore factual responses, a common strategyis to ground their responses in relevant documents that inform response generation. However, common dialogue models still often hallucinate information that was not containedin these documents and is therefore unfaithful. In this work, we propose to alleviate suchhallucinations by \u2018subtracting\u2019 the parametersof a model trained to hallucinate from a dialogue response generation model in order to\u2018negate\u2019 the contribution of such hallucinatedexamples from it. Extensive automatic and human evaluation shows favourable results whencompared to state-of-the-art methods that combine the distributions of multiple models, suchas DExperts (Liu et al., 2021), and others thatchange the training procedure, such as Quark(Lu et al., 2022a). Finally, we show how wecan not only reduce hallucinations but also discourage extractive responses, which are oftena consequence of reducing hallucinations byencouraging copy-pasting of document spans.We publicly release our code for reproducibilityand facilitating further research.",
        "author": "Nico Daheim; Nouha Dziri; Mrinmaya Sachan; Iryna Gurevych; Edoardo Ponti",
        "authorids": "/n/nico-daheim/; /n/nouha-dziri/; /m/mrinmaya-sachan/; /i/iryna-gurevych/; /e/edoardo-ponti/",
        "bibtex": "@inproceedings{daheim-etal-2024-elastic,\n    title = \"Elastic Weight Removal for Faithful and Abstractive Dialogue Generation\",\n    author = \"Daheim, Nico  and\n      Dziri, Nouha  and\n      Sachan, Mrinmaya  and\n      Gurevych, Iryna  and\n      Ponti, Edoardo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.393/\",\n    doi = \"10.18653/v1/2024.naacl-long.393\",\n    pages = \"7096--7112\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.393.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.393/",
        "pdf_size": 452370,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6764363341817056496&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science and Hessian Center for AI (hessian.AI), TU Darmstadt; Allen Institute for Artificial Intelligence; ETH Z\u00fcrich; Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science and Hessian Center for AI (hessian.AI), TU Darmstadt; University of Edinburgh",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://github.com/UKPLab/naacl2024-ewr",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;3",
        "aff_unique_norm": "Technische Universit\u00e4t Darmstadt;Allen Institute for Artificial Intelligence;ETH Zurich;University of Edinburgh",
        "aff_unique_dep": "Department of Computer Science;;;",
        "aff_unique_url": "https://www.tu-darmstadt.de;https://allenai.org;https://www.ethz.ch;https://www.ed.ac.uk",
        "aff_unique_abbr": "TU Darmstadt;AI2;ETHZ;Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Darmstadt;",
        "aff_country_unique_index": "0;1;2;0;3",
        "aff_country_unique": "Germany;United States;Switzerland;United Kingdom"
    },
    {
        "id": "2024.naacl-long.204",
        "title": "Elote, Choclo and Mazorca: on the Varieties of Spanish",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Spanish is one of the most widespread languages: the official language in 20 countries and the second most-spoken native language. Its contact with other languages across different regions and the rich regional and cultural diversity has produced varieties which divert from each other, particularly in terms of lexicon. Still, available corpora, and models trained upon them, generally treat Spanish as one monolithic language, which dampers prediction and generation power when dealing with different varieties. To alleviate the situation, we compile and curate datasets in the different varieties of Spanish around the world at an unprecedented scale and create the CEREAL corpus. With such a resource at hand, we perform a stylistic analysis to identify and characterise varietal differences. We implement a classifier specially designed to deal with long documents and identify Spanish varieties (and therefore expand CEREAL further). We produce varietal-specific embeddings, and analyse the cultural differences that they encode. We make data, code and models publicly available.",
        "author": "Cristina Espa\u00f1a-Bonet; Alberto Barr\u00f3n-Cede\u00f1o",
        "authorids": "/c/cristina-espana-bonet/; /a/alberto-barron-cedeno/",
        "bibtex": "@inproceedings{espana-bonet-barron-cedeno-2024-elote,\n    title = \"Elote, Choclo and Mazorca: on the Varieties of {S}panish\",\n    author = \"Espa{\\~n}a-Bonet, Cristina  and\n      Barr{\\'o}n-Cede{\\~n}o, Alberto\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.204/\",\n    doi = \"10.18653/v1/2024.naacl-long.204\",\n    pages = \"3689--3711\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.204.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.204/",
        "pdf_size": 3259027,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17398947343501336682&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "DFKI GmbH, Saarland Informatics Campus; Universit\u00e1 di Bologna",
        "aff_domain": "dfki.de;unibo.it",
        "email": "dfki.de;unibo.it",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "DFKI GmbH;University of Bologna",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.dFKI.de;https://www.unibo.it",
        "aff_unique_abbr": "DFKI;Unibo",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Saarland;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Germany;Italy"
    },
    {
        "id": "2024.naacl-long.105",
        "title": "Embodied Executable Policy Learning with Language-based Scene Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language models (LLMs) have shown remarkable success in assisting robot learning tasks, i.e., complex household planning.However, the performance of pretrained LLMs heavily relies on domain-specific templated text data, which may be infeasible in real-world robot learning tasks with image-based observations. Moreover, existing LLMs with text inputs lack the capability to evolve with non-expert interactions with environments.In this work, we introduce a novel learning paradigm that generates robots\u2019 executable actions in the form of text, derived solely from visual observations. Our proposed paradigm stands apart from previous works, which utilized either language instructions or a combination of language and visual data as inputs. We demonstrate that our proposed method can employ two fine-tuning strategies, including imitation learning and reinforcement learning approaches, to adapt to the target test tasks effectively.We conduct extensive experiments involving various model selections, environments, and tasks across 7 house layouts in the VirtualHome environment. Our experimental results demonstrate that our method surpasses existing baselines, confirming the effectiveness of this novel learning paradigm.",
        "author": "Jielin Qiu; Mengdi Xu; William Han; Seungwhan Moon; Ding Zhao",
        "authorids": "/j/jielin-qiu/; /m/mengdi-xu/; /w/william-han/; /s/seungwhan-moon/; /d/ding-zhao/",
        "bibtex": "@inproceedings{qiu-etal-2024-embodied,\n    title = \"Embodied Executable Policy Learning with Language-based Scene Summarization\",\n    author = \"Qiu, Jielin  and\n      Xu, Mengdi  and\n      Han, William  and\n      Moon, Seungwhan  and\n      Zhao, Ding\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.105/\",\n    doi = \"10.18653/v1/2024.naacl-long.105\",\n    pages = \"1896--1913\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.105.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.105/",
        "pdf_size": 2890056,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7953523693802524667&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Carnegie Mellon University+Meta Reality Labs; Carnegie Mellon University; Carnegie Mellon University; Meta Reality Labs; Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;meta.com;andrew.cmu.edu",
        "email": "andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;meta.com;andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;Meta",
        "aff_unique_dep": ";Meta Reality Labs",
        "aff_unique_url": "https://www.cmu.edu;https://www.meta.com",
        "aff_unique_abbr": "CMU;MRL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.32",
        "title": "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Previous research in multi-document news summarization has typically concentrated on collating information that all sources agree upon. However, the summarization of diverse information dispersed across multiple articles about an event remains underexplored. In this paper, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we outlined a data collection schema for identifying diverse information and curated a dataset named DiverseSumm. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we conducted a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DiverseSumm dataset. Finally, we study how LLMs summarize multiple news articles by analyzing which type of diverse information LLMs are capable of identifying. Our analyses suggest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40% of the diverse information on average.",
        "author": "Kung-Hsiang Huang; Philippe Laban; Alexander Fabbri; Prafulla Kumar Choubey; Shafiq Joty; Caiming Xiong; Chien-Sheng Wu",
        "authorids": "/k/kung-hsiang-huang/; /p/philippe-laban/; /a/alexander-richard-fabbri/; /p/prafulla-kumar-choubey/; /s/shafiq-joty/; /c/caiming-xiong/; /c/chien-sheng-wu/",
        "bibtex": "@inproceedings{huang-etal-2024-embrace,\n    title = \"Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles\",\n    author = \"Huang, Kung-Hsiang  and\n      Laban, Philippe  and\n      Fabbri, Alexander  and\n      Choubey, Prafulla Kumar  and\n      Joty, Shafiq  and\n      Xiong, Caiming  and\n      Wu, Chien-Sheng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.32/\",\n    doi = \"10.18653/v1/2024.naacl-long.32\",\n    pages = \"570--593\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.32.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.32/",
        "pdf_size": 1696057,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5144680392703537852&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois Urbana-Champaign; Salesforce AI Research; Salesforce AI Research; Salesforce AI Research; Salesforce AI Research; Salesforce AI Research; Salesforce AI Research",
        "aff_domain": "illinois.edu;salesforce.com;salesforce.com;salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "email": "illinois.edu;salesforce.com;salesforce.com;salesforce.com;salesforce.com;salesforce.com;salesforce.com",
        "github": "https://github.com/salesforce/DiverseSumm",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Salesforce",
        "aff_unique_dep": ";Salesforce AI Research",
        "aff_unique_url": "https://illinois.edu;https://www.salesforce.com",
        "aff_unique_abbr": "UIUC;Salesforce AI",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.79",
        "title": "Emergent Abilities in Reduced-Scale Generative Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study investigates if such emergent properties are strictly tied to model size or can be demonstrated by smaller models trained on reduced-scale data. To explore this, we simplify pre-training data and pre-train 36 causal language models with parameters varying from 1 million to 165 million parameters. We show that models trained on this simplified pre-training data demonstrate enhanced zero-shot capabilities across various tasks in simplified language, achieving performance comparable to that of pre-trained models six times larger on unrestricted language. This suggests that downscaling the language allows zero-shot learning capabilities to emerge in models with limited size.Additionally, we find that these smaller models pre-trained on simplified data demonstrate a power law relationship between the evaluation loss and the three scaling factors: compute, dataset size, and model size.",
        "author": "Sherin Muckatira; Vijeta Deshpande; Vladislav Lialin; Anna Rumshisky",
        "authorids": "/s/sherin-muckatira/; /v/vijeta-deshpande/; /v/vladislav-lialin/; /a/anna-rumshisky/",
        "bibtex": "@inproceedings{muckatira-etal-2024-emergent,\n    title = \"Emergent Abilities in Reduced-Scale Generative Language Models\",\n    author = \"Muckatira, Sherin  and\n      Deshpande, Vijeta  and\n      Lialin, Vladislav  and\n      Rumshisky, Anna\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.79/\",\n    doi = \"10.18653/v1/2024.findings-naacl.79\",\n    pages = \"1242--1257\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.79.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.79/",
        "pdf_size": 402614,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2719672152424926098&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Massachusetts Lowell; University of Massachusetts Lowell; University of Massachusetts Lowell; University of Massachusetts Lowell",
        "aff_domain": "student.uml.edu;student.uml.edu;cs.uml.edu;cs.uml.edu",
        "email": "student.uml.edu;student.uml.edu;cs.uml.edu;cs.uml.edu",
        "github": "github.com/text-machine-lab/mini_gpt",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Massachusetts Lowell",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uml.edu",
        "aff_unique_abbr": "UMass Lowell",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Lowell",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.282",
        "title": "Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Emotion Recognition in Conversation (ERC) involves detecting the underlying emotion behind each utterance within a conversation. Effectively generating representations for utterances remains a significant challenge in this task. Recent works propose various models to address this issue, but they still struggle with differentiating similar emotions such as excitement and happiness. To alleviate this problem, We propose an Emotion-Anchored Contrastive Learning (EACL) framework that can generate more distinguishable utterance representations for similar emotions. To achieve this, we utilize label encodings as anchors to guide the learning of utterance representations and design an auxiliary loss to ensure the effective separation of anchors for similar emotions. Moreover, an additional adaptation process is proposed to adapt anchors to serve as effective classifiers to improve classification performance. Across extensive experiments, our proposed EACL achieves state-of-the-art emotion recognition performance and exhibits superior performance on similar emotions. Our code is available at https://github.com/Yu-Fangxu/EACL.",
        "author": "Fangxu Yu; Junjie Guo; Zhen Wu; Xinyu Dai",
        "authorids": "/f/fangxu-yu/; /j/junjie-guo/; /z/zhen-wu/; /x/xinyu-dai/",
        "bibtex": "@inproceedings{yu-etal-2024-emotion,\n    title = \"Emotion-Anchored Contrastive Learning Framework for Emotion Recognition in Conversation\",\n    author = \"Yu, Fangxu  and\n      Guo, Junjie  and\n      Wu, Zhen  and\n      Dai, Xinyu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.282/\",\n    doi = \"10.18653/v1/2024.findings-naacl.282\",\n    pages = \"4521--4534\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.282.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.282/",
        "pdf_size": 2240331,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9991043295099880399&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China; National Key Laboratory for Novel Software Technology, Nanjing University, China; School of Artificial Intelligence, Nanjing University, China",
        "aff_domain": "smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn;nju.edu.cn",
        "github": "https://github.com/Yu-Fangxu/EACL",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.261",
        "title": "Empowering Diffusion Models on the Embedding Space for Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Diffusion models have achieved state-of-the-art synthesis quality on both visual and audio tasks, and recent works further adapt them to textual data by diffusing on the embedding space. In this paper, we conduct systematic studies of the optimization challenges encountered with both the embedding space and the denoising model, which have not been carefully explored. Firstly, the data distribution is learnable for embeddings, which may lead to the collapse of the embedding space and unstable training. To alleviate this problem, we propose a new objective called the anchor loss which is more efficient than previous methods. Secondly, we find the noise levels of conventional schedules are insufficient for training a desirable denoising model while introducing varying degrees of degeneration in consequence. To address this challenge, we propose a novel framework called noise rescaling. Based on the above analysis, we propose Difformer, an embedding diffusion model based on Transformer. Experiments on varieties of seminal text generation tasks show the effectiveness of the proposed methods and the superiority of Difformer over previous state-of-the-art embedding diffusion baselines.",
        "author": "Zhujin Gao; Junliang Guo; Xu Tan; Yongxin Zhu; Fang Zhang; Jiang Bian; Linli Xu",
        "authorids": "/z/zhujin-gao/; /j/junliang-guo/; /x/xu-tan/; /y/yongxin-zhu/; /f/fang-zhang/; /j/jiang-bian/; /l/linli-xu/",
        "bibtex": "@inproceedings{gao-etal-2024-empowering,\n    title = \"Empowering Diffusion Models on the Embedding Space for Text Generation\",\n    author = \"Gao, Zhujin  and\n      Guo, Junliang  and\n      Tan, Xu  and\n      Zhu, Yongxin  and\n      Zhang, Fang  and\n      Bian, Jiang  and\n      Xu, Linli\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.261/\",\n    doi = \"10.18653/v1/2024.naacl-long.261\",\n    pages = \"4664--4683\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.261.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.261/",
        "pdf_size": 546933,
        "gs_citation": 59,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10199179235470436391&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Technology, University of Science and Technology of China+State Key Laboratory of Cognitive Intelligence; Microsoft Research Asia; Microsoft Research Asia; School of Computer Science and Technology, University of Science and Technology of China+State Key Laboratory of Cognitive Intelligence; School of Computer Science and Technology, University of Science and Technology of China+State Key Laboratory of Cognitive Intelligence; Microsoft Research Asia; School of Computer Science and Technology, University of Science and Technology of China+State Key Laboratory of Cognitive Intelligence",
        "aff_domain": "mail.ustc.edu.cn;microsoft.com;microsoft.com;mail.ustc.edu.cn;mail.ustc.edu.cn;microsoft.com;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;microsoft.com;microsoft.com;mail.ustc.edu.cn;mail.ustc.edu.cn;microsoft.com;ustc.edu.cn",
        "github": "https://github.com/zhjgao/difformermulti-step",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2;2;0+1;0+1;2;0+1",
        "aff_unique_norm": "University of Science and Technology of China;State Key Laboratory of Cognitive Intelligence;Microsoft",
        "aff_unique_dep": "School of Computer Science and Technology;;Research",
        "aff_unique_url": "http://www.ustc.edu.cn;;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "USTC;;MSR Asia",
        "aff_campus_unique_index": ";1;1;;;1;",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0+0;0;0;0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.239",
        "title": "Encoding of lexical tone in self-supervised models of spoken language",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Interpretability research has shown that self-supervised Spoken LanguageModels (SLMs) encode a wide variety of features in human speech from theacoustic, phonetic, phonological, syntactic and semantic levels, to speakercharacteristics. The bulk of prior research on representations of phonologyhas focused on segmental features such as phonemes; the encoding ofsuprasegmental phonology (such as tone and stress patterns) in SLMs is not yetwell understood. Tone is a suprasegmental feature that is present in more thanhalf of the world\u2019s languages. This paper aims to analyze the tone encodingcapabilities of SLMs, using Mandarin and Vietnamese as case studies. We showthat SLMs encode lexical tone to a significant degree even when they aretrained on data from non-tonal languages. We further find that SLMs behavesimilarly to native and non-native human participants in tone and consonantperception studies, but they do not follow the same developmental trajectory.",
        "author": "Gaofei Shen; Michaela Watkins; Afra Alishahi; Arianna Bisazza; Grzegorz Chrupa\u0142a",
        "authorids": "/g/gaofei-shen/; /m/michaela-watkins/; /a/afra-alishahi/; /a/arianna-bisazza/; /g/grzegorz-chrupala/",
        "bibtex": "@inproceedings{shen-etal-2024-encoding,\n    title = \"Encoding of lexical tone in self-supervised models of spoken language\",\n    author = \"Shen, Gaofei  and\n      Watkins, Michaela  and\n      Alishahi, Afra  and\n      Bisazza, Arianna  and\n      Chrupa{\\l}a, Grzegorz\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.239/\",\n    doi = \"10.18653/v1/2024.naacl-long.239\",\n    pages = \"4250--4261\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.239.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.239/",
        "pdf_size": 952450,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3467688725285753007&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Tilburg University; University of Amsterdam; Tilburg University; University of Groningen; Tilburg University",
        "aff_domain": "tilburguniversity.edu;uva.nl;tilburguniversity.edu;rug.nl;chrupala.me",
        "email": "tilburguniversity.edu;uva.nl;tilburguniversity.edu;rug.nl;chrupala.me",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "Tilburg University;University of Amsterdam;University of Groningen",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tilburguniversity.edu/;https://www.uva.nl;https://www.rug.nl",
        "aff_unique_abbr": "Tilburg U;UvA;RUG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2024.naacl-long.96",
        "title": "End-to-End Beam Retrieval for Multi-Hop Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multi-hop question answering (QA) involves finding multiple relevant passages and step-by-step reasoning to answer complex questions, indicating a retrieve-and-read paradigm. However, previous retrievers were customized for two-hop questions, and most of them were trained separately across different hops, resulting in a lack of supervision over the entire multi-hop retrieval process and leading to poor performance in complicated scenarios beyond two hops. In this work, we introduce Beam Retrieval, an end-to-end beam retrieval framework for multi-hop QA. This approach models the multi-hop retrieval process in an end-to-end manner by jointly optimizing an encoder and two classification heads across all hops. Moreover, Beam Retrieval maintains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. To establish a complete QA system, we incorporate a supervised reader or a large language model (LLM). Experimental results demonstrate that Beam Retrieval achieves a nearly 50% improvement compared with baselines on challenging MuSiQue-Ans, and it also surpasses all previous retrievers on HotpotQA and achieves 99.9% precision on 2WikiMultiHopQA. Providing high-quality context, Beam Retrieval helps our supervised reader achieve new state-of-the-art performance and substantially improves the few-shot QA performance of LLMs.",
        "author": "Jiahao Zhang; Haiyang Zhang; Dongmei Zhang; Liu Yong; Shen Huang",
        "authorids": "/j/jiahao-zhang/; /h/haiyang-zhang/; /d/dongmei-zhang/; /l/liu-yong/; /s/shen-huang/",
        "bibtex": "@inproceedings{zhang-etal-2024-end,\n    title = \"End-to-End Beam Retrieval for Multi-Hop Question Answering\",\n    author = \"Zhang, Jiahao  and\n      Zhang, Haiyang  and\n      Zhang, Dongmei  and\n      Yong, Liu  and\n      Huang, Shen\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.96/\",\n    doi = \"10.18653/v1/2024.naacl-long.96\",\n    pages = \"1718--1731\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.96.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.96/",
        "pdf_size": 1466709,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18384039765948851585&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Tencent Research, Beijing, China; Tencent Research, Beijing, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;tencent.com;tencent.com",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;tencent.com;tencent.com",
        "github": "https://github.com/canghongjian/beam_retriever",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;1",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Tencent",
        "aff_unique_dep": ";Tencent Research",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.tencent.com",
        "aff_unique_abbr": "BUPT;Tencent",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.454",
        "title": "Enhancing Argument Summarization: Prioritizing Exhaustiveness in Key Point Generation and Introducing an Automatic Coverage Evaluation Metric",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The proliferation of social media platforms has given rise to the amount of online debates and arguments. Consequently, the need for automatic summarization methods for such debates is imperative, however this area of summarization is rather understudied. The Key Point Analysis (KPA) task formulates argument summarization as representing the summary of a large collection of arguments in the form of concise sentences in bullet-style format, called key points. A sub-task of KPA, called Key Point Generation (KPG), focuses on generating these key points given the arguments. This paper introduces a novel extractive approach for key point generation, that outperforms previous state-of-the-art methods for the task. Our method utilizes an extractive clustering based approach that offers concise, high quality generated key points with higher coverage of reference summaries, and less redundant outputs. In addition, we show that the existing evaluation metrics for summarization such as ROUGE are incapable of differentiating between generated key points of different qualities. To this end, we propose a new evaluation metric for assessing the generated key points by their coverage. Our code can be accessed online.",
        "author": "Mohammad Khosravani; Chenyang Huang; Amine Trabelsi",
        "authorids": "/m/mohammad-khosravani/; /c/chenyang-huang/; /a/amine-trabelsi/",
        "bibtex": "@inproceedings{khosravani-etal-2024-enhancing,\n    title = \"Enhancing Argument Summarization: Prioritizing Exhaustiveness in Key Point Generation and Introducing an Automatic Coverage Evaluation Metric\",\n    author = \"Khosravani, Mohammad  and\n      Huang, Chenyang  and\n      Trabelsi, Amine\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.454/\",\n    doi = \"10.18653/v1/2024.naacl-long.454\",\n    pages = \"8212--8224\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.454.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.454/",
        "pdf_size": 444775,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9570130714453950093&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Dept. of Computer Science, Lakehead University; Amii, Dept. of Computing Science, University of Alberta; Dept. of Computer Science, Universit\u00e9 de Sherbrooke",
        "aff_domain": "lakeheadu.ca;ualberta.ca;usherbrooke.ca",
        "email": "lakeheadu.ca;ualberta.ca;usherbrooke.ca",
        "github": "github.com/b14ck-sun/arg-sum",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Lakehead University;University of Alberta;Universit\u00e9 de Sherbrooke",
        "aff_unique_dep": "Dept. of Computer Science;Dept. of Computing Science;Dept. of Computer Science",
        "aff_unique_url": "https://www.lakeheadu.ca;https://www.ualberta.ca;https://www.usherbrooke.ca",
        "aff_unique_abbr": "Lakehead;UAlberta;UdeS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.findings-naacl.257",
        "title": "Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) can achieve impressive performance on various reasoning tasks by incorporating chain-of-thought (CoT) prompting, where step-by-step reasoning is provided to guide LLMs to generate answers to questions, and the question-rationale-answer triplets are utilized as demonstration exemplars. However, the reasoning chains of demonstrations generated by LLMs are observed to be prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars, e.g., overly simplistic or complex exemplars depending on the question\u2019s difficulty level, can affect the LLM\u2019s performance. To address these issues, we introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts prompting). Iter-CoT has two advantages: (1) it adopts iterative bootstrapping that enables LLMs to rectify errors autonomously, resulting in more precise and comprehensive reasoning chains. (2) it selects exemplars of challenging yet answerable (i.e., the LLM has the potential to answer correctly) questions, enhancing the LLMs\u2019 generalizability to answer questions with varying difficulty levels. Experimental results exhibit Iter-CoT superior performance on three distinct reasoning tasks on ten datasets.",
        "author": "Jiashuo Sun; Yi Luo; Yeyun Gong; Chen Lin; Yelong Shen; Jian Guo; Nan Duan",
        "authorids": "/j/jiashuo-sun/; /y/yi-luo/; /y/yeyun-gong/; /c/chen-lin/; /y/yelong-shen/; /j/jian-guo/; /n/nan-duan/",
        "bibtex": "@inproceedings{sun-etal-2024-enhancing,\n    title = \"Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models\",\n    author = \"Sun, Jiashuo  and\n      Luo, Yi  and\n      Gong, Yeyun  and\n      Lin, Chen  and\n      Shen, Yelong  and\n      Guo, Jian  and\n      Duan, Nan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.257/\",\n    doi = \"10.18653/v1/2024.findings-naacl.257\",\n    pages = \"4074--4101\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.257.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.257/",
        "pdf_size": 1485113,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11507295886826236361&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Informatics, Xiamen University; School of Informatics, Xiamen University; Microsoft Research Asia; School of Informatics, Xiamen University + IDEA Research; Microsoft; IDEA Research; Microsoft Research Asia",
        "aff_domain": "xmu.edu.cn;xmu.edu.cn;microsoft.com;xmu.edu.cn;microsoft.com;idea.edu.cn;microsoft.com",
        "email": "xmu.edu.cn;xmu.edu.cn;microsoft.com;xmu.edu.cn;microsoft.com;idea.edu.cn;microsoft.com",
        "github": "https://github.com/XMUDM/Iter-CoT",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0+2;1;2;1",
        "aff_unique_norm": "Xiamen University;Microsoft;IDEA Research",
        "aff_unique_dep": "School of Informatics;Research;",
        "aff_unique_url": "https://www.xmu.edu.cn;https://www.microsoft.com/en-us/research/group/asia;",
        "aff_unique_abbr": "XMU;MSR Asia;",
        "aff_campus_unique_index": "1;;1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0;2;0",
        "aff_country_unique": "China;;United States"
    },
    {
        "id": "2024.naacl-long.237",
        "title": "Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) tend to inadequately integrate input context during text generation, relying excessively on encoded prior knowledge in model parameters, potentially resulting in generated text with factual inconsistencies or contextually unfaithful content. LLMs utilize two primary knowledge sources: 1) prior (parametric) knowledge from pretraining, and 2) contextual (non-parametric) knowledge from input prompts. The study addresses the open question of how LLMs effectively balance these knowledge sources during the generation process, specifically in the context of open-domain question answering. To address this issue, we introduce a novel approach integrating contrastive decoding with adversarial irrelevant passages as negative samples to enhance robust context grounding during generation. Notably, our method operates at inference time without requiring further training. We conduct comprehensive experiments to demonstrate its applicability and effectiveness, providing empirical evidence showcasing its superiority over existing methodologies.",
        "author": "Zheng Zhao; Emilio Monti; Jens Lehmann; Haytham Assem",
        "authorids": "/z/zheng-zhao/; /e/emilio-monti/; /j/jens-lehmann/; /h/haytham-assem/",
        "bibtex": "@inproceedings{zhao-etal-2024-enhancing,\n    title = \"Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding\",\n    author = \"Zhao, Zheng  and\n      Monti, Emilio  and\n      Lehmann, Jens  and\n      Assem, Haytham\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.237/\",\n    doi = \"10.18653/v1/2024.naacl-long.237\",\n    pages = \"4225--4237\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.237.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.237/",
        "pdf_size": 791782,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4141193138365988375&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Informatics, University of Edinburgh; Amazon; Amazon; Amazon",
        "aff_domain": "ed.ac.uk;amazon.com;amazon.com;amazon.com",
        "email": "ed.ac.uk;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "University of Edinburgh;Amazon",
        "aff_unique_dep": "School of Informatics;Amazon.com, Inc.",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.amazon.com",
        "aff_unique_abbr": "Edinburgh;Amazon",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Edinburgh;",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2024.findings-naacl.204",
        "title": "Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The field of cross-lingual sentence embeddings has recently experienced significant advancements, but research concerning low-resource languages has lagged due to the scarcity of parallel corpora. This paper shows that cross-lingual word representation in low-resource languages is notably under-aligned with that in high-resource languages in current models. To address this, we introduce a novel framework that explicitly aligns words between English and eight low-resource languages, utilizing off-the-shelf word alignment models. This framework incorporates three primary training objectives: aligned word prediction and word translation ranking, along with the widely used translation ranking. We evaluate our approach through experiments on the bitext retrieval task, which demonstrate substantial improvements on sentence embeddings in low-resource languages. In addition, the competitive performance of the proposed model across a broader range of tasks in high-resource languages underscores its practicality.",
        "author": "Zhongtao Miao; Qiyu Wu; Kaiyan Zhao; Zilong Wu; Yoshimasa Tsuruoka",
        "authorids": "/z/zhongtao-miao/; /q/qiyu-wu/; /k/kaiyan-zhao/; /z/zilong-wu/; /y/yoshimasa-tsuruoka/",
        "bibtex": "@inproceedings{miao-etal-2024-enhancing,\n    title = \"Enhancing Cross-lingual Sentence Embedding for Low-resource Languages with Word Alignment\",\n    author = \"Miao, Zhongtao  and\n      Wu, Qiyu  and\n      Zhao, Kaiyan  and\n      Wu, Zilong  and\n      Tsuruoka, Yoshimasa\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.204/\",\n    doi = \"10.18653/v1/2024.findings-naacl.204\",\n    pages = \"3225--3236\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.204.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.204/",
        "pdf_size": 406613,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12299389134678859532&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "The University of Tokyo, Tokyo, Japan; The University of Tokyo, Tokyo, Japan; The University of Tokyo, Tokyo, Japan; The University of Tokyo, Tokyo, Japan; The University of Tokyo, Tokyo, Japan",
        "aff_domain": "g.ecc.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp",
        "email": "g.ecc.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp;g.ecc.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Tokyo",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2024.naacl-long.299",
        "title": "Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Numerous works are proposed to align large language models (LLMs) with human intents to better fulfill instructions, ensuring they are trustful and helpful.Nevertheless, some human instructions are often malicious or misleading and following them will lead to untruthful and unsafe responses.Previous work rarely focused on understanding how LLMs manage instructions based on counterfactual premises, referred to here as inductive instructions, which may stem from users\u2019 false beliefs or malicious intents.In this paper, we aim to reveal the behaviors of LLMs towards inductive instructions and enhance their truthfulness and helpfulness accordingly. Specifically, we first introduce a benchmark of Inductive Instructions (INDust), where the false knowledge is incorporated into instructions in multiple different styles. After extensive human and automatic evaluations, we uncovered a universal vulnerability among LLMs in processing inductive instructions.Additionally, we identified that different inductive styles affect the models\u2019 ability to identify the same underlying errors,and the complexity of the underlying assumptions also influences the model\u2019s performance.Motivated by these results, we propose Dual-critique prompting to improve LLM robustness against inductive instructions.Our experiments demonstrate that Dual-critique prompting significantly bolsters the robustness of a diverse array of LLMs, even when confronted with varying degrees of inductive instruction complexity and differing inductive styles.",
        "author": "Rui Wang; Hongru Wang; Fei Mi; Boyang Xue; Yi Chen; Kam-Fai Wong; Ruifeng Xu",
        "authorids": "/r/rui-wang/; /h/hongru-wang/; /f/fei-mi/; /b/boyang-xue/; /y/yi-chen/; /k/kam-fai-wong/; /r/ruifeng-xu/",
        "bibtex": "@inproceedings{wang-etal-2024-enhancing,\n    title = \"Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting\",\n    author = \"Wang, Rui  and\n      Wang, Hongru  and\n      Mi, Fei  and\n      Xue, Boyang  and\n      Chen, Yi  and\n      Wong, Kam-Fai  and\n      Xu, Ruifeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.299/\",\n    doi = \"10.18653/v1/2024.naacl-long.299\",\n    pages = \"5345--5363\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.299.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.299/",
        "pdf_size": 3098590,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9963184775700091129&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "https://github.com/DevoAllen/INDust",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2024.findings-naacl.137",
        "title": "Enhancing Perception: Refining Explanations of News Claims with LLM Conversations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We introduce Enhancing Perception, a framework for Large Language Models (LLMs) designed to streamline the time-intensive task typically undertaken by professional fact-checkers of crafting explanations for fake news. This study investigates the effectiveness of enhancing LLM explanations through conversational refinement. We compare various questioner agents, including state-of-the-art LLMs like GPT-4, Claude 2, PaLM 2, and 193 American participants acting as human questioners. Based on the histories of these refinement conversations, we further generate comprehensive summary explanations. We evaluated the effectiveness of these initial, refined, and summary explanations across 40 news claims by involving 2,797 American participants, measuring their self-reported belief change regarding both real and fake claims after receiving the explanations. Our findings reveal that, in the context of fake news, explanations that have undergone conversational refinement\u2014whether by GPT-4 or human questioners, who ask more diverse and detail-oriented questions\u2014were significantly more effective than both the initial unrefined explanations and the summary explanations. Moreover, these refined explanations achieved a level of effectiveness comparable to that of expert-written explanations. The results highlight the potential of automatic explanation refinement by LLMs in debunking fake news claims.",
        "author": "Yi-Li Hsu; Jui-Ning Chen; Yang Fan Chiang; Shang-Chien Liu; Aiping Xiong; Lun-Wei Ku",
        "authorids": "/y/yi-li-hsu/; /j/jui-ning-chen/; /y/yang-fan-chiang/; /s/shang-chien-liu/; /a/aiping-xiong/; /l/lun-wei-ku/",
        "bibtex": "@inproceedings{hsu-etal-2024-enhancing,\n    title = \"Enhancing Perception: Refining Explanations of News Claims with {LLM} Conversations\",\n    author = \"Hsu, Yi-Li  and\n      Chen, Jui-Ning  and\n      Fan Chiang, Yang  and\n      Liu, Shang-Chien  and\n      Xiong, Aiping  and\n      Ku, Lun-Wei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.137/\",\n    doi = \"10.18653/v1/2024.findings-naacl.137\",\n    pages = \"2129--2147\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.137.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.137/",
        "pdf_size": 2894819,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5352999180650554587&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Institute of Information Science, Academia Sinica + Department of Computer Science, National Tsing Hua University; Institute of Information Science, Academia Sinica + Graduate Institute of Electrical Engineering, National Taiwan University; Institute of Information Science, Academia Sinica; Institute of Information Science, Academia Sinica; The Pennsylvania State University; Institute of Information Science, Academia Sinica",
        "aff_domain": "iis.sinica.edu.tw; ; ; ; ; ",
        "email": "iis.sinica.edu.tw; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+2;0;0;3;0",
        "aff_unique_norm": "Academia Sinica;National Tsing Hua University;National Taiwan University;Pennsylvania State University",
        "aff_unique_dep": "Institute of Information Science;Department of Computer Science;Graduate Institute of Electrical Engineering;",
        "aff_unique_url": "https://www.sinica.edu.tw;https://www.nthu.edu.tw;https://www.ntu.edu.tw;https://www.psu.edu",
        "aff_unique_abbr": "AS;NTHU;NTU;PSU",
        "aff_campus_unique_index": "0+0;0+0;0;0;0",
        "aff_campus_unique": "Taiwan;",
        "aff_country_unique_index": "0+0;0+0;0;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.findings-naacl.184",
        "title": "Enhancing the General Agent Capabilities of Low-Paramter LLMs through Tuning and Multi-Branch Reasoning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of LLMs as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.",
        "author": "Qinhao Zhou; Zihan Zhang; Xiang Xiang; Ke Wang; Yuchuan Wu; Yongbin Li",
        "authorids": "/q/qinhao-zhou/; /z/zihan-zhang/; /x/xiang-xiang/; /k/ke-wang/; /y/yuchuan-wu/; /y/yongbin-li/",
        "bibtex": "@inproceedings{zhou-etal-2024-enhancing,\n    title = \"Enhancing the General Agent Capabilities of Low-Paramter {LLM}s through Tuning and Multi-Branch Reasoning\",\n    author = \"Zhou, Qinhao  and\n      Zhang, Zihan  and\n      Xiang, Xiang  and\n      Wang, Ke  and\n      Wu, Yuchuan  and\n      Li, Yongbin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.184/\",\n    doi = \"10.18653/v1/2024.findings-naacl.184\",\n    pages = \"2922--2931\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.184.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.184/",
        "pdf_size": 617725,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9876673867527217931&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "National Key Lab of MSIIPT, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China+Peng Cheng Laboratory, Shenzhen, China; National Key Lab of MSIIPT, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Lab of MSIIPT, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China+Peng Cheng Laboratory, Shenzhen, China; Alibaba Group, Beijing, China; Alibaba Group, Beijing, China; Alibaba Group, Beijing, China",
        "aff_domain": "hust.edu.cn; ; ; ; ; ",
        "email": "hust.edu.cn; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;0+1;2;2;2",
        "aff_unique_norm": "Huazhong University of Science and Technology;Pengcheng Laboratory;Alibaba Group",
        "aff_unique_dep": "School of Artificial Intelligence and Automation;Peng Cheng Laboratory;",
        "aff_unique_url": "http://www.hust.edu.cn;;https://www.alibaba.com",
        "aff_unique_abbr": "HUST;;Alibaba",
        "aff_campus_unique_index": "0+1;0;0+1;2;2;2",
        "aff_campus_unique": "Wuhan;Shenzhen;Beijing",
        "aff_country_unique_index": "0+0;0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.65",
        "title": "Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, establishing a comprehensive library of guidelines and a model for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with relevant guidelines, which guide LLMs in response generation to ensure safe and high-quality outputs, thereby aligning with human values. An additional optional stage involves fine-tuning a model with well-aligned datasets generated through the process implemented in the second stage.Our method customizes guidelines to accommodate diverse inputs, thereby enhancing the fine-grainedness and comprehensiveness of the guideline library. Furthermore, it incorporates safety expertise from a safety-trained LLM through a lightweight retrieval model.We evaluate our approach on three benchmarks, demonstrating significant improvements in LLM security and quality. Notably, our fine-tuned model, Labrador, even at 13 billion parameters, outperforms GPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.",
        "author": "Yi Luo; Zhenghao Lin; YuHao Zhang; Jiashuo Sun; Chen Lin; Chengjin Xu; Xiangdong Su; Yelong Shen; Jian Guo; Yeyun Gong",
        "authorids": "/y/yi-luo/; /z/zhenghao-lin/; /y/yuhao-zhang/; /j/jiashuo-sun/; /c/chen-lin/; /c/chengjin-xu/; /x/xiangdong-su/; /y/yelong-shen/; /j/jian-guo/; /y/yeyun-gong/",
        "bibtex": "@inproceedings{luo-etal-2024-ensuring,\n    title = \"Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models\",\n    author = \"Luo, Yi  and\n      Lin, Zhenghao  and\n      Zhang, YuHao  and\n      Sun, Jiashuo  and\n      Lin, Chen  and\n      Xu, Chengjin  and\n      Su, Xiangdong  and\n      Shen, Yelong  and\n      Guo, Jian  and\n      Gong, Yeyun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.65/\",\n    doi = \"10.18653/v1/2024.naacl-long.65\",\n    pages = \"1152--1197\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.65.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.65/",
        "pdf_size": 1379800,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18324555443150653351&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Informatics, Xiamen University; School of Informatics, Xiamen University; Institute of Artificial Intelligence, Xiamen University; School of Informatics, Xiamen University; School of Informatics, Xiamen University; IDEA Research; Inner Mongolia University; Microsoft; IDEA Research; Microsoft Research Asia",
        "aff_domain": "xmu.edu.cn;xmu.edu.cn;xmu.edu.cn;xmu.edu.cn;xmu.edu.cn;idea.edu.cn;imu.edu.cn;microsoft.com;idea.edu.cn;microsoft.com",
        "email": "xmu.edu.cn;xmu.edu.cn;xmu.edu.cn;xmu.edu.cn;xmu.edu.cn;idea.edu.cn;imu.edu.cn;microsoft.com;idea.edu.cn;microsoft.com",
        "github": "https://github.com/XMUDM/Guide-Align",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;1;2;3;1;3",
        "aff_unique_norm": "Xiamen University;IDEA Research;Inner Mongolia University;Microsoft",
        "aff_unique_dep": "School of Informatics;;;Microsoft Corporation",
        "aff_unique_url": "https://www.xmu.edu.cn;;http://www.imu.edu.cn/;https://www.microsoft.com",
        "aff_unique_abbr": "XMU;;IMU;Microsoft",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Asia",
        "aff_country_unique_index": "0;0;0;0;0;0;2;0",
        "aff_country_unique": "China;;United States"
    },
    {
        "id": "2024.naacl-long.363",
        "title": "Entity Disambiguation via Fusion Entity Decoding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Entity disambiguation (ED), which links the mentions of ambiguous entities to their referent entities in a knowledge base, serves as a core component in entity linking (EL). Existing generative approaches demonstrate improved accuracy compared to classification approaches under the standardized ZELDA benchmark. Nevertheless, generative approaches suffer from the need for large-scale pre-training and inefficient generation. Most importantly, entity descriptions, which could contain crucial information to distinguish similar entities from each other, are often overlooked.We propose an encoder-decoder model to disambiguate entities with more detailed entity descriptions. Given text and candidate entities, the encoder learns interactions between the text and each candidate entity, producing representations for each entity candidate. The decoder then fuses the representations of entity candidates together and selects the correct entity.Our experiments, conducted on various entity disambiguation benchmarks, demonstrate the strong and robust performance of this model, particularly +1.5% in the ZELDA benchmark compared with GENRE. Furthermore, we integrate this approach into the retrieval/reader framework and observe +1.5% improvements in end-to-end entity linking in the GERBIL benchmark compared with EntQA.",
        "author": "Junxiong Wang; Ali Mousavi; Omar Attia; Ronak Pradeep; Saloni Potdar; Alexander Rush; Umar Farooq Minhas; Yunyao Li",
        "authorids": "/j/junxiong-wang/; /a/ali-mousavi/; /o/omar-attia/; /r/ronak-pradeep/; /s/saloni-potdar/; /a/alexander-m-rush/; /u/umar-farooq-minhas/; /y/yunyao-li/",
        "bibtex": "@inproceedings{wang-etal-2024-entity,\n    title = \"Entity Disambiguation via Fusion Entity Decoding\",\n    author = \"Wang, Junxiong  and\n      Mousavi, Ali  and\n      Attia, Omar  and\n      Pradeep, Ronak  and\n      Potdar, Saloni  and\n      Rush, Alexander  and\n      Minhas, Umar Farooq  and\n      Li, Yunyao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.363/\",\n    doi = \"10.18653/v1/2024.naacl-long.363\",\n    pages = \"6524--6536\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.363.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.363/",
        "pdf_size": 640005,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8332533616555422109&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Cornell University; Apple Inc; Apple Inc; University of Waterloo; Apple Inc; Cornell University; Apple Inc; Adobe",
        "aff_domain": "cs.cornell.edu;apple.com;apple.com;uwaterloo.ca;apple.com;cs.cornell.edu;apple.com;adobe.com",
        "email": "cs.cornell.edu;apple.com;apple.com;uwaterloo.ca;apple.com;cs.cornell.edu;apple.com;adobe.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;2;1;0;1;3",
        "aff_unique_norm": "Cornell University;Apple;University of Waterloo;Adobe",
        "aff_unique_dep": ";Apple Inc;;Adobe Inc.",
        "aff_unique_url": "https://www.cornell.edu;https://www.apple.com;https://uwaterloo.ca;https://www.adobe.com",
        "aff_unique_abbr": "Cornell;Apple;UW;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;0;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2024.findings-naacl.132",
        "title": "Ethos: Rectifying Language Models in Orthogonal Parameter Space",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Language models (LMs) have greatly propelled the research on natural language processing. However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset. In this work, we present a new efficient approach, Ethos, that rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy leakage. Ethos is built on task arithmetic. However, unlike current task arithmetic algorithms, Ethos distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos separates the principal components that encode general from those associated with undesired knowledge. Ethos performs forgetting or unlearning by only negating the task vector with undesired knowledge, thereby minimizing collateral damage on general model utility. We demonstrate the efficacy of our approach on three different tasks: bias, toxicity, and memorization unlearning. Evaluations show Ethos is more effective in removing undesired knowledge while maintaining the overall model performance compared to current task arithmetic methods.",
        "author": "Lei Gao; Yue Niu; Tingting Tang; Salman Avestimehr; Murali Annavaram",
        "authorids": "/l/lei-gao/; /y/yue-niu/; /t/tingting-tang/; /s/salman-avestimehr/; /m/murali-annavaram/",
        "bibtex": "@inproceedings{gao-etal-2024-ethos,\n    title = \"Ethos: Rectifying Language Models in Orthogonal Parameter Space\",\n    author = \"Gao, Lei  and\n      Niu, Yue  and\n      Tang, Tingting  and\n      Avestimehr, Salman  and\n      Annavaram, Murali\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.132/\",\n    doi = \"10.18653/v1/2024.findings-naacl.132\",\n    pages = \"2054--2068\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.132.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.132/",
        "pdf_size": 510675,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2947240551427555163&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Southern California; University of Southern California; University of Southern California; University of Southern California; University of Southern California",
        "aff_domain": "usc.edu;usc.edu;usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu;usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.161",
        "title": "Evaluating In-Context Learning of Libraries for Code Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability. A particularly promising area is their ability to interpret code modules from unfamiliar libraries for solving user-instructed tasks. Recent work has shown that large proprietary LLMs can learn novel library usage in-context from demonstrations. These results raise several open questions: whether demonstrations of library usage is required, whether smaller (and more open) models also possess such capabilities, etc. In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context. Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context. Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions, which are often cheaper to obtain than demonstrations. Overall, our results pave the way for harnessing LLMs in more adaptable and dynamic coding environments.",
        "author": "Arkil Patel; Siva Reddy; Dzmitry Bahdanau; Pradeep Dasigi",
        "authorids": "/a/arkil-patel/; /s/siva-reddy/; /d/dzmitry-bahdanau/; /p/pradeep-dasigi/",
        "bibtex": "@inproceedings{patel-etal-2024-evaluating,\n    title = \"Evaluating In-Context Learning of Libraries for Code Generation\",\n    author = \"Patel, Arkil  and\n      Reddy, Siva  and\n      Bahdanau, Dzmitry  and\n      Dasigi, Pradeep\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.161/\",\n    doi = \"10.18653/v1/2024.naacl-long.161\",\n    pages = \"2908--2926\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.161.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.161/",
        "pdf_size": 9543103,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17067981402660606344&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Mila and McGill University + Facebook CIFAR AI Chair; Mila and McGill University + Canada CIFAR AI Chair; Mila and McGill University; Allen Institute for AI + ServiceNow Research",
        "aff_domain": "mila.quebec;mila.quebec;mila.quebec;allenai.org",
        "email": "mila.quebec;mila.quebec;mila.quebec;allenai.org",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+2;0;3+4",
        "aff_unique_norm": "McGill University;Meta;Canadian Institute for Advanced Research;Allen Institute for AI;ServiceNow",
        "aff_unique_dep": "Mila;Facebook CIFAR AI;AI Chair;;Research",
        "aff_unique_url": "https://www.mcgill.ca;https://www.facebook.com;https://www.cifar.ca;https://allenai.org;https://www.servicenow.com",
        "aff_unique_abbr": "McGill;FB;CIFAR;AI2;ServiceNow",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+0;0;1+1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2024.naacl-long.83",
        "title": "Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.",
        "author": "Se-eun Yoon; Zhankui He; Jessica Echterhoff; Julian McAuley",
        "authorids": "/s/se-eun-yoon/; /z/zhankui-he/; /j/jessica-echterhoff/; /j/julian-mcauley/",
        "bibtex": "@inproceedings{yoon-etal-2024-evaluating,\n    title = \"Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation\",\n    author = \"Yoon, Se-eun  and\n      He, Zhankui  and\n      Echterhoff, Jessica  and\n      McAuley, Julian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.83/\",\n    doi = \"10.18653/v1/2024.naacl-long.83\",\n    pages = \"1490--1504\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.83.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.83/",
        "pdf_size": 1347975,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7796564454460708732&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of California, San Diego; University of California, San Diego; University of California, San Diego; University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "github": "https://github.com/granelle/naacl24-user-sim",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.188",
        "title": "Evaluating Step-by-Step Reasoning through Symbolic Verification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pre-trained language models (LMs) have shown remarkable reasoning performance using explanations or chain-of-thoughts (CoT)) for in-context learning. On the other hand, these reasoning tasks are usually presumed to be more approachable for symbolic programming. To understand the mechanism of reasoning of LMs, we curate synthetic datasets containing equivalent (natural, symbolic) data pairs, where symbolic examples contain first-order logic rules and predicates from non-parametric knowledge bases (KBs), supporting automated verification of intermediate reasoning results. Then we revisit neuro-symbolic approaches and propose to learn from demonstrations containing logic rules and corresponding examples to iteratively reason over KBs, recovering Prolog\u2019s backward chaining algorithm and supporting automated verification of LMs\u2019 outputs. Comprehensive experiments are included to systematically compare LMLP with CoT in deductive reasoning settings, showing that LMLP enjoys more than 25% higher accuracy than CoT on length generalization benchmarks even with smaller model sizes.",
        "author": "YiFan Zhang; Hanlin Zhang; Li Li; Eric Xing",
        "authorids": "/y/yifan-zhang/; /h/hanlin-zhang/; /l/li-li/; /e/eric-xing/",
        "bibtex": "@inproceedings{zhang-etal-2024-evaluating,\n    title = \"Evaluating Step-by-Step Reasoning through Symbolic Verification\",\n    author = \"Zhang, YiFan  and\n      Zhang, Hanlin  and\n      Li, Li  and\n      Xing, Eric\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.188/\",\n    doi = \"10.18653/v1/2024.findings-naacl.188\",\n    pages = \"2984--3002\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.188.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.188/",
        "pdf_size": 578285,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17601328091556392369&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-long.476",
        "title": "Evaluating the Deductive Competence of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The development of highly fluent large language models (LLMs) has prompted increased interest in assessing their reasoning and problem-solving capabilities. We investigate whether several LLMs can solve a classic type of deductive reasoning problem from the cognitive science literature. The tested LLMs have limited abilities to solve these problems in their conventional form. We performed follow up experiments to investigate if changes to the presentation format and content improve model performance. We do find performance differences between conditions; however, they do not improve overall performance. Moreover, we find that performance interacts with presentation format and content in unexpected ways that differ from human performance. Overall, our results suggest that LLMs have unique reasoning biases that are only partially predicted from human reasoning performance and the human-generated language corpora that informs them.",
        "author": "S Seals; Valerie Shalin",
        "authorids": "/s/s-seals/; /v/valerie-shalin/",
        "bibtex": "@inproceedings{seals-shalin-2024-evaluating,\n    title = \"Evaluating the Deductive Competence of Large Language Models\",\n    author = \"Seals, S  and\n      Shalin, Valerie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.476/\",\n    doi = \"10.18653/v1/2024.naacl-long.476\",\n    pages = \"8614--8630\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.476.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.476/",
        "pdf_size": 730443,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6574418897340197982&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Wright State University+Oak Ridge Institute for Science and Education+Air Force Research Laboratory; Wright State University+AI Institute - University of South Carolina",
        "aff_domain": "outlook.com; ",
        "email": "outlook.com; ",
        "github": "https://github.com/spencer-michael-s/deductive-competence",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1+2;0+3",
        "aff_unique_norm": "Wright State University;Oak Ridge Institute for Science and Education;Air Force Research Laboratory;University of South Carolina",
        "aff_unique_dep": ";;;AI Institute",
        "aff_unique_url": "https://www.wright.edu;https://www.orise.org;https://www.afrl.af.mil/;https://www.sc.edu",
        "aff_unique_abbr": "WSU;ORISE;AFRL;USC",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.191",
        "title": "Event Causality Is Key to Computational Story Understanding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cognitive science and symbolic AI research suggest that event causality provides vital information for story understanding. However, machine learning systems for story understanding rarely employ event causality, partially due to the lack of methods that reliably identify open-world causal event relations. Leveraging recent progress in large language models, we present the first method for event causality identification that leads to material improvements in computational story understanding. Our technique sets a new state of the art on the COPES dataset (Wang et al., 2023c) for causal event relation identification. Further, in the downstream story quality evaluation task, the identified causal relations lead to 3.6-16.6% relative improvement on correlation with human ratings. In the multimodal story video-text alignment task, we attain 4.1-10.9% increase on Clip Accuracy and 4.2-13.5% increase on Sentence IoU. The findings indicate substantial untapped potential for event causality in computational story understanding. The codebase is at https://github.com/insundaycathy/Event-Causality-Extraction.",
        "author": "Yidan Sun; Qin Chao; Boyang Li",
        "authorids": "/y/yidan-sun/; /q/qin-chao/; /b/boyang-li/",
        "bibtex": "@inproceedings{sun-etal-2024-event,\n    title = \"Event Causality Is Key to Computational Story Understanding\",\n    author = \"Sun, Yidan  and\n      Chao, Qin  and\n      Li, Boyang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.191/\",\n    doi = \"10.18653/v1/2024.naacl-long.191\",\n    pages = \"3493--3511\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.191.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.191/",
        "pdf_size": 2533142,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1574930679716115387&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore",
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "email": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg",
        "github": "https://github.com/insundaycathy/Event-Causality-Extraction",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nanyang Technological University",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "https://www.ntu.edu.sg",
        "aff_unique_abbr": "NTU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Singapore",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2024.naacl-long.322",
        "title": "Event Detection from Social Media for Epidemic Prediction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Social media is an easy-to-access platform providing timely updates about societal trends and events. Discussions regarding epidemic-related events such as infections, symptoms, and social interactions can be crucial for informing policymaking during epidemic outbreaks. In our work, we pioneer exploiting Event Detection (ED) for better preparedness and early warnings of any upcoming epidemic by developing a framework to extract and analyze epidemic-related events from social media posts. To this end, we curate an epidemic event ontology comprising seven disease-agnostic event types and construct a Twitter dataset SPEED with human-annotated events focused on the COVID-19 pandemic. Experimentation reveals how ED models trained on COVID-based SPEED can effectively detect epidemic events for three unseen epidemics of Monkeypox, Zika, and Dengue; while models trained on existing ED datasets fail miserably. Furthermore, we show that reporting sharp increases in the extracted events by our framework can provide warnings 4-9 weeks earlier than the WHO epidemic declaration for Monkeypox. This utility of our framework lays the foundations for better preparedness against emerging epidemics.",
        "author": "Tanmay Parekh; Anh Mac; Jiarui Yu; Yuxuan Dong; Syed Shahriar; Bonnie Liu; Eric Yang; Kuan-Hao Huang; Wei Wang; Nanyun Peng; Kai-Wei Chang",
        "authorids": "/t/tanmay-parekh/; /a/anh-mac/; /j/jiarui-yu/; /y/yuxuan-dong/; /s/syed-shahriar/; /b/bonnie-liu/; /e/eric-yang/; /k/kuan-hao-huang/; /w/wei-wang/; /n/nanyun-peng/; /k/kai-wei-chang/",
        "bibtex": "@inproceedings{parekh-etal-2024-event,\n    title = \"Event Detection from Social Media for Epidemic Prediction\",\n    author = \"Parekh, Tanmay  and\n      Mac, Anh  and\n      Yu, Jiarui  and\n      Dong, Yuxuan  and\n      Shahriar, Syed  and\n      Liu, Bonnie  and\n      Yang, Eric  and\n      Huang, Kuan-Hao  and\n      Wang, Wei  and\n      Peng, Nanyun  and\n      Chang, Kai-Wei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.322/\",\n    doi = \"10.18653/v1/2024.naacl-long.322\",\n    pages = \"5758--5783\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.322.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.322/",
        "pdf_size": 1407564,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12062015542741070107&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles; Department of Computer Science, University of Illinois Urbana-Champaign; Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles; Computer Science Department, University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu; ; ; ; ; ; ; ;cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu; ; ; ; ; ; ; ;cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;0;0;0;1;0;0;0",
        "aff_unique_norm": "University of California, Los Angeles;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Computer Science Department;Department of Computer Science",
        "aff_unique_url": "https://www.ucla.edu;https://illinois.edu",
        "aff_unique_abbr": "UCLA;UIUC",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;1;0;0;0",
        "aff_campus_unique": "Los Angeles;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.229",
        "title": "Event-Content-Oriented Dialogue Generation in Short Video",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding complex events from different modalities, associating to external knowledge and generating response in a clear point of view are still unexplored in today\u2019s multi-modal dialogue research. The great challenges include 1) lack of event-based multi-modal dialogue dataset; 2) understanding of complex events and 3) heterogeneity gap between different modalities. To overcome these challenges, we firstly introduce a novel event-oriented video-dialogue dataset called SportsVD (Sports-domain Video-dialogue Dataset). To our best knowledge, SportsVD is the first dataset that consists of complex events videos and opinion-based conversations with regards to contents in these events. Meanwhile, we present multi-modal dialogue generation method VCD (Video Commentary Dialogue) to generate human-like response according to event contents in the video and related external knowledge. In contrast to previous video-based dialogue generation, we focus on opinion-based response and the understanding of longer and more complex event contents. We evaluate VCD\u2019s performance on SportsVD and other baselines under several automatic metrics. Experiments demonstrate VCD can outperform among other state-of-the-art baselines. Our work is available at https://github.com/Cheng-Fenghua/SportsVD.",
        "author": "Fenghua Cheng; Xue Li; Zi Huang; Jinxiang Wang; Sen Wang",
        "authorids": "/f/fenghua-cheng/; /x/xue-li/; /z/zi-huang/; /j/jinxiang-wang/; /s/sen-wang/",
        "bibtex": "@inproceedings{cheng-etal-2024-event,\n    title = \"Event-Content-Oriented Dialogue Generation in Short Video\",\n    author = \"Cheng, Fenghua  and\n      Li, Xue  and\n      Huang, Zi  and\n      Wang, Jinxiang  and\n      Wang, Sen\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.229/\",\n    doi = \"10.18653/v1/2024.naacl-long.229\",\n    pages = \"4114--4124\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.229.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.229/",
        "pdf_size": 1475742,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6370962022793937211&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Electrical Engineering and Computer Science, The University of Queensland, Brisbane, Australia; School of Electrical Engineering and Computer Science, The University of Queensland, Brisbane, Australia; School of Electrical Engineering and Computer Science, The University of Queensland, Brisbane, Australia; School of Electrical Engineering and Computer Science, The University of Queensland, Brisbane, Australia; School of Electrical Engineering and Computer Science, The University of Queensland, Brisbane, Australia",
        "aff_domain": "uq.edu.au;uq.edu.au;uq.edu.au;uq.edu.au;uq.edu.au",
        "email": "uq.edu.au;uq.edu.au;uq.edu.au;uq.edu.au;uq.edu.au",
        "github": "https://github.com/Cheng-Fenghua/SportsVD",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Queensland",
        "aff_unique_dep": "School of Electrical Engineering and Computer Science",
        "aff_unique_url": "https://www.uq.edu.au",
        "aff_unique_abbr": "UQ",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Brisbane",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2024.naacl-long.313",
        "title": "Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The proliferation of online misinformation has posed significant threats to public interest. While numerous online users actively participate in the combat against misinformation, many of such responses can be characterized by the lack of politeness and supporting facts. As a solution, text generation approaches are proposed to automatically produce counter-misinformation responses. Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar text quality and excessively repetitive responses. In this paper, we propose retrieval augmented response generation for online misinformation (RARG), which collects supporting evidence from scientific sources and generates counter-misinformation responses based on the evidences. In particular, our RARG consists of two stages: (1) evidence collection, where we design a retrieval pipeline to retrieve and rerank evidence documents using a database comprising over 1M academic articles; (2) response generation, in which we align large language models (LLMs) to generate evidence-based responses via reinforcement learning from human feedback (RLHF). We propose a reward function to maximize the utilization of the retrieved evidence while maintaining the quality of the generated text, which yields polite and factual responses that clearly refutes misinformation. To demonstrate the effectiveness of our method, we study the case of COVID-19 and perform extensive experiments with both in- and cross-domain datasets, where RARG consistently outperforms baselines by generating high-quality counter-misinformation responses.",
        "author": "Zhenrui Yue; Huimin Zeng; Yimeng Lu; Lanyu Shang; Yang Zhang; Dong Wang",
        "authorids": "/z/zhenrui-yue/; /h/huimin-zeng/; /y/yimeng-lu/; /l/lanyu-shang/; /y/yang-zhang/; /d/dong-wang/",
        "bibtex": "@inproceedings{yue-etal-2024-evidence,\n    title = \"Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation\",\n    author = \"Yue, Zhenrui  and\n      Zeng, Huimin  and\n      Lu, Yimeng  and\n      Shang, Lanyu  and\n      Zhang, Yang  and\n      Wang, Dong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.313/\",\n    doi = \"10.18653/v1/2024.naacl-long.313\",\n    pages = \"5628--5643\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.313.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.313/",
        "pdf_size": 634246,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3340596803874675441&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.21",
        "title": "Examining Modularity in Multilingual LMs via Language-Specialized Subnetworks",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent work has proposed explicitly inducing language-wise modularity in multilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as a means of better guiding cross-lingual sharing. In this paper, we investigate (1) the degree to which language-wise modularity *naturally* arises within models with no special modularity interventions, and (2) how cross-lingual sharing and interference differ between such models and those with explicit SFT-guided subnetwork modularity. In order to do so, we use XLM-R as our multilingual LM. Moreover, to quantify language specialization and cross-lingual interaction, we use a Training Data Attribution method that estimates the degree to which a model\u2019s predictions are influenced by in-language or cross-language training examples. Our results show that language-specialized subnetworks do naturally arise, and that SFT, rather than always increasing modularity, can decrease language specialization of subnetworks in favor of more cross-lingual sharing.",
        "author": "Rochelle Choenni; Ekaterina Shutova; Dan Garrette",
        "authorids": "/r/rochelle-choenni/; /e/ekaterina-shutova/; /d/dan-garrette/",
        "bibtex": "@inproceedings{choenni-etal-2024-examining,\n    title = \"Examining Modularity in Multilingual {LM}s via Language-Specialized Subnetworks\",\n    author = \"Choenni, Rochelle  and\n      Shutova, Ekaterina  and\n      Garrette, Dan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.21/\",\n    doi = \"10.18653/v1/2024.findings-naacl.21\",\n    pages = \"287--301\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.21.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.21/",
        "pdf_size": 2762072,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1723788903108497832&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Amsterdam; University of Amsterdam; Google DeepMind",
        "aff_domain": "uva.nl;uva.nl;google.com",
        "email": "uva.nl;uva.nl;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Amsterdam;Google",
        "aff_unique_dep": ";Google DeepMind",
        "aff_unique_url": "https://www.uva.nl;https://deepmind.com",
        "aff_unique_abbr": "UvA;DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "Netherlands;United Kingdom"
    },
    {
        "id": "2024.naacl-long.167",
        "title": "ExpertQA: Expert-Curated Questions and Attributed Answers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "As language models are adopted by a more sophisticated and diverse set of users, the importance of guaranteeing that they provide factually correct information supported by verifiable sources is critical across fields of study. This is especially the case for high-stakes fields, such as medicine and law, where the risk of propagating false information is high and can lead to undesirable societal consequences. Previous work studying attribution and factuality has not focused on analyzing these characteristics of language model outputs in domain-specific scenarios. In this work, we conduct human evaluation of responses from a few representative systems along various axes of attribution and factuality, by bringing domain experts in the loop. Specifically, we collect expert-curated questions from 484 participants across 32 fields of study, and then ask the same experts to evaluate generated responses to their own questions. In addition, we ask experts to improve upon responses from language models. The output of our analysis is ExpertQA, a high-quality long-form QA dataset with 2177 questions spanning 32 fields, along with verified answers and attributions for claims in the answers.",
        "author": "Chaitanya Malaviya; Subin Lee; Sihao Chen; Elizabeth Sieber; Mark Yatskar; Dan Roth",
        "authorids": "/c/chaitanya-malaviya/; /s/subin-lee/; /s/sihao-chen/; /e/elizabeth-sieber/; /m/mark-yatskar/; /d/dan-roth/",
        "bibtex": "@inproceedings{malaviya-etal-2024-expertqa,\n    title = \"{E}xpert{QA}: Expert-Curated Questions and Attributed Answers\",\n    author = \"Malaviya, Chaitanya  and\n      Lee, Subin  and\n      Chen, Sihao  and\n      Sieber, Elizabeth  and\n      Yatskar, Mark  and\n      Roth, Dan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.167/\",\n    doi = \"10.18653/v1/2024.naacl-long.167\",\n    pages = \"3025--3045\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.167.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.167/",
        "pdf_size": 6196345,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2966788381753721864&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Pennsylvania; University of Washington; University of Pennsylvania; University of Washington; University of Pennsylvania; University of Pennsylvania",
        "aff_domain": "upenn.edu;upenn.edu;upenn.edu;uw.edu;upenn.edu;upenn.edu",
        "email": "upenn.edu;upenn.edu;upenn.edu;uw.edu;upenn.edu;upenn.edu",
        "github": "https://github.com/chaitanyamalaviya/ExpertQA",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;1;0;0",
        "aff_unique_norm": "University of Pennsylvania;University of Washington",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;https://www.washington.edu",
        "aff_unique_abbr": "UPenn;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.435",
        "title": "Explaining Text Similarity in Transformer Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "As Transformers have become state-of-the-art models for natural language processing (NLP) tasks, the need to understand and explain their predictions is increasingly apparent. Especially in unsupervised applications, such as information retrieval tasks, similarity models built on top of foundation model representations have been widely applied. However, their inner prediction mechanisms have mostly remained opaque. Recent advances in explainable AI have made it possible to mitigate these limitations by leveraging improved explanations for Transformers through layer-wise relevance propagation (LRP). Using BiLRP, an extension developed for computing second-order explanations in bilinear similarity models, we investigate which feature interactions drive similarity in NLP models. We validate the resulting explanations and demonstrate their utility in three corpus-level use cases, analyzing grammatical interactions, multilingual semantics, and biomedical text retrieval. Our findings contribute to a deeper understanding of different semantic similarity tasks and models, highlighting how novel explainable AI methods enable in-depth analyses and corpus-level insights.",
        "author": "Alexandros Vasileiou; Oliver Eberle",
        "authorids": "/a/alexandros-vasileiou/; /o/oliver-eberle/",
        "bibtex": "@inproceedings{vasileiou-eberle-2024-explaining,\n    title = \"Explaining Text Similarity in Transformer Models\",\n    author = \"Vasileiou, Alexandros  and\n      Eberle, Oliver\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.435/\",\n    doi = \"10.18653/v1/2024.naacl-long.435\",\n    pages = \"7859--7873\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.435.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.435/",
        "pdf_size": 16861727,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4846186986723745674&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Machine Learning Group, Technische Universit\u00e4t Berlin, Berlin, Germany+BIFOLD \u2013 Berlin Institute for the Foundations of Learning and Data, Berlin, Germany; Machine Learning Group, Technische Universit\u00e4t Berlin, Berlin, Germany",
        "aff_domain": "gmx.de;tu-berlin.de",
        "email": "gmx.de;tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin;Berlin Institute for the Foundations of Learning and Data",
        "aff_unique_dep": "Machine Learning Group;",
        "aff_unique_url": "https://www.tu-berlin.de;https://bifold.berlin",
        "aff_unique_abbr": "TU Berlin;BIFOLD",
        "aff_campus_unique_index": "0+0;0",
        "aff_campus_unique": "Berlin",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.findings-naacl.76",
        "title": "Explanation Extraction from Hierarchical Classification Frameworks for Long Legal Documents",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Hierarchical classification frameworks have been widely used to process long sequences, especially in the legal domain for predictions from long legal documents. But being black-box models they are unable to explain their predictions making them less reliable for practical applications, more so in the legal domain. In this work, we develop an extractive explanation algorithm for hierarchical frameworks for long sequences based on the sensitivity of the trained model to its input perturbations. We perturb using occlusion and develop Ob-HEx; an Occlusion-based Hierarchical Explanation-extractor. We adapt Ob-HEx to Hierarchical Transformer models trained on long Indian legal texts. And use Ob-HEx to analyze them and extract their explanations for the ILDC-Expert dataset, achieving a minimum gain of 1 point over the previous benchmark on most of our performance evaluation metrics.",
        "author": "Nishchal Prasad; Taoufiq Dkaki; Mohand Boughanem",
        "authorids": "/n/nishchal-prasad/; /t/taoufiq-dkaki/; /m/mohand-boughanem/",
        "bibtex": "@inproceedings{prasad-etal-2024-explanation,\n    title = \"Explanation Extraction from Hierarchical Classification Frameworks for Long Legal Documents\",\n    author = \"Prasad, Nishchal  and\n      Dkaki, Taoufiq  and\n      Boughanem, Mohand\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.76/\",\n    doi = \"10.18653/v1/2024.findings-naacl.76\",\n    pages = \"1192--1201\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.76.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.76/",
        "pdf_size": 393840,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:C1s1oCAmwF0J:scholar.google.com/&scioq=Explanation+Extraction+from+Hierarchical+Classification+Frameworks+for+Long+Legal+Documents&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.findings-naacl.193",
        "title": "Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconceptions among real students.",
        "author": "Wanyong Feng; Jaewook Lee; Hunter McNichols; Alexander Scarlatos; Digory Smith; Simon Woodhead; Nancy Ornelas; Andrew Lan",
        "authorids": "/w/wanyong-feng/; /j/jaewook-lee/; /h/hunter-mcnichols/; /a/alexander-scarlatos/; /d/digory-smith/; /s/simon-woodhead/; /n/nancy-ornelas/; /a/andrew-lan/",
        "bibtex": "@inproceedings{feng-etal-2024-exploring,\n    title = \"Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models\",\n    author = \"Feng, Wanyong  and\n      Lee, Jaewook  and\n      McNichols, Hunter  and\n      Scarlatos, Alexander  and\n      Smith, Digory  and\n      Woodhead, Simon  and\n      Ornelas, Nancy  and\n      Lan, Andrew\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.193/\",\n    doi = \"10.18653/v1/2024.findings-naacl.193\",\n    pages = \"3067--3082\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.193.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.193/",
        "pdf_size": 531364,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5963349153831494191&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Massachusetts Amherst; University of Massachusetts Amherst; University of Massachusetts Amherst; University of Massachusetts Amherst; Eedi; Eedi; Stanford University, Kitco Design; University of Massachusetts Amherst",
        "aff_domain": "umass.edu;umass.edu;umass.edu;umass.edu;eedi.co.uk;eedi.co.uk;kitco.design;umass.edu",
        "email": "umass.edu;umass.edu;umass.edu;umass.edu;eedi.co.uk;eedi.co.uk;kitco.design;umass.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;1;1;2;0",
        "aff_unique_norm": "University of Massachusetts Amherst;Eedi;Stanford University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.umass.edu;;https://www.stanford.edu",
        "aff_unique_abbr": "UMass Amherst;;Stanford",
        "aff_campus_unique_index": "0;0;0;0;2;0",
        "aff_campus_unique": "Amherst;;Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2024.naacl-long.236",
        "title": "Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most hate speech datasets neglect the cultural diversity within a single language, resulting in a critical shortcoming in hate speech detection. To address this, we introduce CREHate, a CRoss-cultural English Hate speech dataset. To construct CREHate, we follow a two-step procedure: 1) cultural post collection and 2) cross-cultural annotation. We sample posts from the SBIC dataset, which predominantly represents North America, and collect posts from four geographically diverse English-speaking countries (Australia, United Kingdom, Singapore, and South Africa) using culturally hateful keywords we retrieve from our survey. Annotations are collected from the four countries plus the United States to establish representative labels for each country. Our analysis highlights statistically significant disparities across countries in hate speech annotations. Only 56.2% of the posts in CREHate achieve consensus among all countries, with the highest pairwise label difference rate of 26%. Qualitative analysis shows that label disagreement occurs mostly due to different interpretations of sarcasm and the personal bias of annotators on divisive topics. Lastly, we evaluate large language models (LLMs) under a zero-shot setting and show that current LLMs tend to show higher accuracies on Anglosphere country labels in CREHate.Our dataset and codes are available at: https://github.com/nlee0212/CREHate",
        "author": "Nayeon Lee; Chani Jung; Junho Myung; Jiho Jin; Jose Camacho-Collados; Juho Kim; Alice Oh",
        "authorids": "/n/nayeon-lee/; /c/chani-jung/; /j/junho-myung/; /j/jiho-jin/; /j/jose-camacho-collados/; /j/juho-kim/; /a/alice-oh/",
        "bibtex": "@inproceedings{lee-etal-2024-exploring-cross,\n    title = \"Exploring Cross-Cultural Differences in {E}nglish Hate Speech Annotations: From Dataset Construction to Analysis\",\n    author = \"Lee, Nayeon  and\n      Jung, Chani  and\n      Myung, Junho  and\n      Jin, Jiho  and\n      Camacho-Collados, Jose  and\n      Kim, Juho  and\n      Oh, Alice\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.236/\",\n    doi = \"10.18653/v1/2024.naacl-long.236\",\n    pages = \"4205--4224\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.236.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.236/",
        "pdf_size": 1166324,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12826335817279638935&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "KAIST; KAIST; KAIST; KAIST; Cardiff University; KAIST; KAIST",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;cardiff.ac.uk;kaist.ac.kr;kaist.edu",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;cardiff.ac.uk;kaist.ac.kr;kaist.edu",
        "github": "https://github.com/nlee0212/CREHate",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Cardiff University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kaist.ac.kr;https://www.cardiff.ac.uk",
        "aff_unique_abbr": "KAIST;Cardiff",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;0;0",
        "aff_country_unique": "South Korea;United Kingdom"
    },
    {
        "id": "2024.naacl-long.315",
        "title": "Exploring Key Point Analysis with Pairwise Generation and Graph Partitioning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Key Point Analysis (KPA), the summarization of multiple arguments into a concise collection of key points, continues to be a significant and unresolved issue within the field of argument mining. Existing models adapt a two-stage pipeline of clustering arguments or generating key points for argument clusters. This approach rely on semantic similarity instead of measuring the existence of shared key points among arguments. Additionally, it only models the intra-cluster relationship among arguments, disregarding the inter-cluster relationship between arguments that do not share key points. To address these limitations, we propose a novel approach for KPA with pairwise generation and graph partitioning. Our objective is to train a generative model that can simultaneously provide a score indicating the presence of shared key point between a pair of arguments and generate the shared key point. Subsequently, to map generated redundant key points to a concise set of key points, we proceed to construct an arguments graph by considering the arguments as vertices, the generated key points as edges, and the scores as edge weights. We then propose a graph partitioning algorithm to partition all arguments sharing the same key points to the same subgraph. Notably, our experimental findings demonstrate that our proposed model surpasses previous models when evaluated on both the ArgKP and QAM datasets.",
        "author": "Xiao Li; Yong Jiang; Shen Huang; Pengjun Xie; Gong Cheng; Fei Huang",
        "authorids": "/x/xiao-li/; /y/yong-jiang/; /s/shen-huang/; /p/pengjun-xie/; /g/gong-cheng/; /f/fei-huang/",
        "bibtex": "@inproceedings{li-etal-2024-exploring,\n    title = \"Exploring Key Point Analysis with Pairwise Generation and Graph Partitioning\",\n    author = \"Li, Xiao  and\n      Jiang, Yong  and\n      Huang, Shen  and\n      Xie, Pengjun  and\n      Cheng, Gong  and\n      Huang, Fei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.315/\",\n    doi = \"10.18653/v1/2024.naacl-long.315\",\n    pages = \"5657--5667\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.315.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.315/",
        "pdf_size": 549779,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8806439537429800530&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "State Key Laboratory for Novel Software Technology, Nanjing University + Institute for Intelligent Computing, Alibaba Group; Institute for Intelligent Computing, Alibaba Group; Institute for Intelligent Computing, Alibaba Group; Institute for Intelligent Computing, Alibaba Group; State Key Laboratory for Novel Software Technology, Nanjing University + Institute for Intelligent Computing, Alibaba Group; Institute for Intelligent Computing, Alibaba Group",
        "aff_domain": "smail.nju.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;nju.edu.cn;alibaba-inc.com",
        "email": "smail.nju.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;nju.edu.cn;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;0+1;1",
        "aff_unique_norm": "Nanjing University;Alibaba Group",
        "aff_unique_dep": "State Key Laboratory for Novel Software Technology;Institute for Intelligent Computing",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.alibabagroup.com",
        "aff_unique_abbr": "Nanjing University;Alibaba",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.181",
        "title": "Exploring Language Model\u2019s Code Generation Ability with Auxiliary Functions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Auxiliary function is a helpful component to improve language model\u2019s code generation ability. However, a systematic exploration of how they affect has yet to be done. In this work, we comprehensively evaluate the ability to utilize auxiliary functions encoded in recent code-pretrained language models. First, we construct a human-crafted evaluation set, called HumanExtension, which contains examples of two functions where one function assists the other.With HumanExtension, we design several experiments to examine their ability in a multifaceted way. Our evaluation processes enable a comprehensive understanding of including auxiliary functions in the prompt in terms of effectiveness and robustness. An additional implementation style analysis captures the models\u2019 various implementation patterns when they access the auxiliary function. Through this analysis, we discover the models\u2019 promising ability to utilize auxiliary functions including their self-improving behavior by implementing the two functions step-by-step. However, our analysis also reveals the model\u2019s underutilized behavior to call the auxiliary function, suggesting the future direction to enhance their implementation by eliciting the auxiliary function call ability encoded in the models. We release our code and dataset to facilitate this research direction.",
        "author": "Seonghyeon Lee; Sanghwan Jang; Seongbo Jang; Dongha Lee; Hwanjo Yu",
        "authorids": "/s/seonghyeon-lee/; /s/sanghwan-jang/; /s/seongbo-jang/; /d/dongha-lee/; /h/hwanjo-yu/",
        "bibtex": "@inproceedings{lee-etal-2024-exploring,\n    title = \"Exploring Language Model{'}s Code Generation Ability with Auxiliary Functions\",\n    author = \"Lee, Seonghyeon  and\n      Jang, Sanghwan  and\n      Jang, Seongbo  and\n      Lee, Dongha  and\n      Yu, Hwanjo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.181/\",\n    doi = \"10.18653/v1/2024.findings-naacl.181\",\n    pages = \"2836--2848\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.181.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.181/",
        "pdf_size": 807968,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14099659263359683145&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Engineering, POSTECH, Pohang, South Korea\u2020; Department of Computer Science and Engineering, POSTECH, Pohang, South Korea\u2020; Department of Computer Science and Engineering, POSTECH, Pohang, South Korea\u2020; Department of Aritificial Intelligence, Yonsei University, Seoul, South Korea\u2021; Department of Computer Science and Engineering, POSTECH, Pohang, South Korea\u2020\u2217",
        "aff_domain": "postech.ac.kr;postech.ac.kr;postech.ac.kr;yonsei.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr;postech.ac.kr;yonsei.ac.kr;postech.ac.kr",
        "github": "https://github.com/sh0416/humanextension",
        "project": "https://huggingface.co/datasets/sh0416/humanextension",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "POSTECH;Yonsei University",
        "aff_unique_dep": "Department of Computer Science and Engineering;Department of Artificial Intelligence",
        "aff_unique_url": "https://www.postech.ac.kr;https://www.yonsei.ac.kr",
        "aff_unique_abbr": "POSTECH;Yonsei",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Pohang;Seoul",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-long.53",
        "title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Traditional attempts to enhance the logical reasoning abilities of language models often rely on supervised fine-tuning, limiting their generalization to new tasks or domains. Large Language Models (LLMs), with their capacity to condense vast knowledge, can effectively tackle many tasks. Yet, our experiments reveal a gap in their performance on logical reasoning benchmarks when compared to state-of-the-art fine-tuning based models. To bridge this gap, we present LogicLLM, a first-of-its-kind, fully self-supervised framework for integrating logical reasoning capabilities into LLMs, and activating them via in-context learning. We apply this to two LLM series, FLAN-T5 and LLaMA, with parameter sizes from 3 billion to 33 billion. LogicLLM demonstrates its effectiveness through successful improvements on two logical reasoning benchmarks (ReClor and LogiQA-v2). Additionally, LogicLLM based on FLAN-T5-11B attains comparable results to ChatGPT, and evaluations with LLaMA-based models on three language understanding benchmarks (RACE, MMLU and Big-Bench-Hard) confirm that the improvements come without compromising the model\u2019s general language understanding capabilities.",
        "author": "Fangkai Jiao; Zhiyang Teng; Bosheng Ding; Zhengyuan Liu; Nancy Chen; Shafiq Joty",
        "authorids": "/f/fangkai-jiao/; /z/zhiyang-teng/; /b/bosheng-ding/; /z/zhengyuan-liu/; /n/nancy-chen/; /s/shafiq-joty/",
        "bibtex": "@inproceedings{jiao-etal-2024-exploring,\n    title = \"Exploring Self-supervised Logic-enhanced Training for Large Language Models\",\n    author = \"Jiao, Fangkai  and\n      Teng, Zhiyang  and\n      Ding, Bosheng  and\n      Liu, Zhengyuan  and\n      Chen, Nancy  and\n      Joty, Shafiq\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.53/\",\n    doi = \"10.18653/v1/2024.naacl-long.53\",\n    pages = \"926--941\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.53.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.53/",
        "pdf_size": 809093,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11176462504020711072&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Nanyang Technological University, Singapore + Institute for Infocomm Research (I2R), A\u2217STAR, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Institute for Infocomm Research (I2R), A\u2217STAR, Singapore; Institute for Infocomm Research (I2R), A\u2217STAR, Singapore + Salesforce Research; Nanyang Technological University, Singapore + Salesforce Research",
        "aff_domain": "hotmail.com;e.ntu.edu.sg;ntu.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg;salesforce.com",
        "email": "hotmail.com;e.ntu.edu.sg;ntu.edu.sg;i2r.a-star.edu.sg;i2r.a-star.edu.sg;salesforce.com",
        "github": "https://github.com/SparkJiao/LogicLLM",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;0;1;1+2;0+2",
        "aff_unique_norm": "Nanyang Technological University;Institute for Infocomm Research;Salesforce",
        "aff_unique_dep": ";;Salesforce Research",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.i2r.a-star.edu.sg;https://research.salesforce.com",
        "aff_unique_abbr": "NTU;I2R;Salesforce",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0+1;0+1",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "2024.naacl-long.338",
        "title": "Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-FactQA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all evaluated LLMs is 36.1%. Both results indicate serious deficiencies. Detailed analysis shows that the understanding of subject/object of the conversation is still challenging for LLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability of LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task data, which achieved a relative error rate reduction of 11% on DIAC-FactQA.",
        "author": "Shuaijie She; Shujian Huang; Xingyun Wang; Yanke Zhou; Jiajun Chen",
        "authorids": "/s/shuaijie-she/; /s/shujian-huang/; /x/xingyun-wang/; /y/yanke-zhou/; /j/jiajun-chen/",
        "bibtex": "@inproceedings{she-etal-2024-exploring,\n    title = \"Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models\",\n    author = \"She, Shuaijie  and\n      Huang, Shujian  and\n      Wang, Xingyun  and\n      Zhou, Yanke  and\n      Chen, Jiajun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.338/\",\n    doi = \"10.18653/v1/2024.naacl-long.338\",\n    pages = \"6087--6100\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.338.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.338/",
        "pdf_size": 238913,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:C5nmY0MLym0J:scholar.google.com/&scioq=Exploring+the+Factual+Consistency+in+Dialogue+Comprehension+of+Large+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University; National Key Laboratory for Novel Software Technology, Nanjing University; National Key Laboratory for Novel Software Technology, Nanjing University; National Key Laboratory for Novel Software Technology, Nanjing University; National Key Laboratory for Novel Software Technology, Nanjing University",
        "aff_domain": "smail.nju.edu.cn;nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Nanjing University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology",
        "aff_unique_url": "http://www.nju.edu.cn",
        "aff_unique_abbr": "Nanjing University",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-industry.41",
        "title": "Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Augmenting Large Language Models (LLMs) for Question Answering (QA) with domain specific data has attracted wide attention. However, domain data often exists in a hybrid format, including text and semi-structured tables, posing challenges for the seamless integration of information. Table-to-Text Generation is a promising solution by facilitating the transformation of hybrid data into a uniformly text-formatted corpus. Although this technique has been widely studied by the NLP community, there is currently no comparative analysis on how corpora generated by different table-to-text methods affect the performance of QA systems.In this paper, we address this research gap in two steps. First, we innovatively integrate table-to-text generation into the framework of enhancing LLM-based QA systems with domain hybrid data. Then, we utilize this framework in real-world industrial data to conduct extensive experiments on two types of QA systems (DSFT and RAG frameworks) with four representative methods: Markdown format, Template serialization, TPLM-based method, and LLM-based method. Based on the experimental results, we draw some empirical findings and explore the underlying reasons behind the success of some methods. We hope the findings of this work will provide a valuable reference for the academic and industrial communities in developing robust QA systems.",
        "author": "Dehai Min; Nan Hu; Rihui Jin; Nuo Lin; Jiaoyan Chen; Yongrui Chen; Yu Li; Guilin Qi; Yun Li; Nijun Li; Qianren Wang",
        "authorids": "/d/dehai-min/; /n/nan-hu/; /r/rihui-jin/; /n/nuo-lin/; /j/jiaoyan-chen/; /y/yongrui-chen/; /y/yu-li/; /g/guilin-qi/; /y/yun-li/; /n/nijun-li/; /q/qianren-wang/",
        "bibtex": "@inproceedings{min-etal-2024-exploring,\n    title = \"Exploring the Impact of Table-to-Text Methods on Augmenting {LLM}-based Question Answering with Domain Hybrid Data\",\n    author = \"Min, Dehai  and\n      Hu, Nan  and\n      Jin, Rihui  and\n      Lin, Nuo  and\n      Chen, Jiaoyan  and\n      Chen, Yongrui  and\n      Li, Yu  and\n      Qi, Guilin  and\n      Li, Yun  and\n      Li, Nijun  and\n      Wang, Qianren\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.41/\",\n    doi = \"10.18653/v1/2024.naacl-industry.41\",\n    pages = \"464--482\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.41.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.41/",
        "pdf_size": 2286216,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16415934536895588767&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "1School of Computer Science and Engineering, Southeast University, China; 1School of Computer Science and Engineering, Southeast University, China; 1School of Computer Science and Engineering, Southeast University, China; 1School of Computer Science and Engineering, Southeast University, China; 2Department of Computer Science, The University of Manchester, United Kingdom; 1School of Computer Science and Engineering, Southeast University, China; 1School of Computer Science and Engineering, Southeast University, China; 1School of Computer Science and Engineering, Southeast University, China; 3Advanced Cognitive AI Lab, Huawei Technologies, China; 3Advanced Cognitive AI Lab, Huawei Technologies, China; 3Advanced Cognitive AI Lab, Huawei Technologies, China",
        "aff_domain": "seu.edu.cn;seu.edu.cn;seu.edu.cn; ; ; ; ; ; ; ; ",
        "email": "seu.edu.cn;seu.edu.cn;seu.edu.cn; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;1;0;0;0;2;2;2",
        "aff_unique_norm": "Southeast University;University of Manchester;Huawei",
        "aff_unique_dep": "School of Computer Science and Engineering;Department of Computer Science;Advanced Cognitive AI Lab",
        "aff_unique_url": "https://www.seu.edu.cn/;https://www.manchester.ac.uk;https://www.huawei.com",
        "aff_unique_abbr": ";UoM;Huawei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;0;0;0;0;0;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2024.findings-naacl.262",
        "title": "Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Saliency post-hoc explainability methods are important tools for understanding increasingly complex NLP models. While these methods can reflect the model\u2019s reasoning, they may not align with human intuition, making the explanations not plausible. In this work, we present a methodology for incorporating rationales, which are text annotations explaining human decisions, into text classification models. This incorporation enhances the plausibility of post-hoc explanations while preserving their faithfulness. Our approach is agnostic to model architectures and explainability methods. We introduce the rationales during model training by augmenting the standard cross-entropy loss with a novel loss function inspired by contrastive learning. By leveraging a multi-objective optimization algorithm, we explore the trade-off between the two loss functions and generate a Pareto-optimal frontier of models that balance performance and plausibility. Through extensive experiments involving diverse models, datasets, and explainability methods, we demonstrate that our approach significantly enhances the quality of model explanations without causing substantial (sometimes negligible) degradation in the original model\u2019s performance.",
        "author": "Lucas Resck; Marcos M. Raimundo; Jorge Poco",
        "authorids": "/l/lucas-resck/; /m/marcos-m-raimundo/; /j/jorge-poco/",
        "bibtex": "@inproceedings{resck-etal-2024-exploring,\n    title = \"Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales\",\n    author = \"Resck, Lucas  and\n      M. Raimundo, Marcos  and\n      Poco, Jorge\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.262/\",\n    doi = \"10.18653/v1/2024.findings-naacl.262\",\n    pages = \"4190--4216\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.262.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.262/",
        "pdf_size": 741877,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13685336513918331864&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Funda\u00e7\u00e3o Getulio Vargas, Rio de Janeiro, Brazil; Universidade Estadual de Campinas (UNICAMP), Campinas, Brazil; Funda\u00e7\u00e3o Getulio Vargas, Rio de Janeiro, Brazil",
        "aff_domain": "fgv.br;ic.unicamp.br;fgv.br",
        "email": "fgv.br;ic.unicamp.br;fgv.br",
        "github": "https://github.com/visual-ds/plausible-nlp-explanations",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Funda\u00e7\u00e3o Getulio Vargas;Universidade Estadual de Campinas",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.fgv.br;https://www.unicamp.br",
        "aff_unique_abbr": "FGV;UNICAMP",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Rio de Janeiro;Campinas",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Brazil"
    },
    {
        "id": "2024.naacl-long.258",
        "title": "Extending CLIP\u2019s Image-Text Alignment to Referring Image Segmentation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Referring Image Segmentation (RIS) is a cross-modal task that aims to segment an instance described by a natural language expression. Recent methods leverage large-scale pretrained unimodal models as backbones along with fusion techniques for joint reasoning across modalities. However, the inherent cross-modal nature of RIS raises questions about the effectiveness of unimodal backbones. We propose RISCLIP, a novel framework that effectively leverages the cross-modal nature of CLIP for RIS. Observing CLIP\u2019s inherent alignment between image and text features, we capitalize on this starting point and introduce simple but strong modules that enhance unimodal feature extraction and leverage rich alignment knowledge in CLIP\u2019s image-text shared-embedding space. RISCLIP exhibits outstanding results on all three major RIS benchmarks and also outperforms previous CLIP-based methods, demonstrating the efficacy of our strategy in extending CLIP\u2019s image-text alignment to RIS.",
        "author": "Seoyeon Kim; Minguk Kang; Dongwon Kim; Jaesik Park; Suha Kwak",
        "authorids": "/s/seoyeon-kim/; /m/minguk-kang/; /d/dongwon-kim/; /j/jaesik-park/; /s/suha-kwak/",
        "bibtex": "@inproceedings{kim-etal-2024-extending,\n    title = \"Extending {CLIP}{'}s Image-Text Alignment to Referring Image Segmentation\",\n    author = \"Kim, Seoyeon  and\n      Kang, Minguk  and\n      Kim, Dongwon  and\n      Park, Jaesik  and\n      Kwak, Suha\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.258/\",\n    doi = \"10.18653/v1/2024.naacl-long.258\",\n    pages = \"4611--4628\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.258.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.258/",
        "pdf_size": 9559583,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5944767064984884920&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "POSTECH; POSTECH; POSTECH; Seoul National University; POSTECH",
        "aff_domain": "postech.ac.kr;postech.ac.kr;postech.ac.kr;snu.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr;postech.ac.kr;snu.ac.kr;postech.ac.kr",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Pohang University of Science and Technology;Seoul National University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.postech.ac.kr;https://www.snu.ac.kr",
        "aff_unique_abbr": "POSTECH;SNU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Pohang;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.findings-naacl.191",
        "title": "Extending Input Contexts of Language Models through Training on Segmented Sequences",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Effectively training language models on longinputs poses many technical challenges. As acost consideration, languages models are pre-trained on a fixed sequence length before beingadapted to longer sequences. We explore var-ious methods for adapting models to longerinputs by training on segmented sequences andan interpolation-based method for extendingabsolute positional embeddings. We developa training procedure to extend the input con-text size of pretrained models with no architec-tural changes and no additional memory coststhan training on the original input lengths. Bysub-sampling segments from long inputs whilemaintaining their original position the model isable to learn new positional interactions. Ourmethod benefits both models trained with abso-lute positional embeddings, by extending theirinput contexts, as well as popular relative posi-tional embedding methods showing a reducedperplexity on sequences longer than they weretrained on. We demonstrate our method canextend input contexts by a factor of 4\u00d7 whileimproving perplexity.",
        "author": "Petros Karypis; Julian McAuley; George Karypis",
        "authorids": "/p/petros-karypis/; /j/julian-mcauley/; /g/george-karypis/",
        "bibtex": "@inproceedings{karypis-etal-2024-extending,\n    title = \"Extending Input Contexts of Language Models through Training on Segmented Sequences\",\n    author = \"Karypis, Petros  and\n      McAuley, Julian  and\n      Karypis, George\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.191/\",\n    doi = \"10.18653/v1/2024.findings-naacl.191\",\n    pages = \"3040--3052\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.191.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.191/",
        "pdf_size": 296534,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VmhrtynilJYJ:scholar.google.com/&scioq=Extending+Input+Contexts+of+Language+Models+through+Training+on+Segmented+Sequences&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "UC San Diego; UC San Diego; University of Minnesota",
        "aff_domain": "ucsd.edu;ucsd.edu;umn.edu",
        "email": "ucsd.edu;ucsd.edu;umn.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of California, San Diego;University of Minnesota",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsd.edu;https://www.minnesota.edu",
        "aff_unique_abbr": "UCSD;UMN",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.5",
        "title": "Extracting Lexical Features from Dialects via Interpretable Dialect Classifiers",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Identifying linguistic differences between dialects of a language often requires expert knowledge and meticulous human analysis. This is largely due to the complexity and nuance involved in studying various dialects. We present a novel approach to extract distinguishing lexical features of dialects by utilizing interpretable dialect classifiers, even in the absence of human experts. We explore both post-hoc and intrinsic approaches to interpretability, conduct experiments on Mandarin, Italian, and Low Saxon, and experimentally demonstrate that our method successfully identifies key language-specific lexical features that contribute to dialectal variations.",
        "author": "Roy Xie; Orevaoghene Ahia; Yulia Tsvetkov; Antonios Anastasopoulos",
        "authorids": "/r/roy-xie/; /o/orevaoghene-ahia/; /y/yulia-tsvetkov/; /a/antonios-anastasopoulos/",
        "bibtex": "@inproceedings{xie-etal-2024-extracting,\n    title = \"Extracting Lexical Features from Dialects via Interpretable Dialect Classifiers\",\n    author = \"Xie, Roy  and\n      Ahia, Orevaoghene  and\n      Tsvetkov, Yulia  and\n      Anastasopoulos, Antonios\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.5/\",\n    doi = \"10.18653/v1/2024.naacl-short.5\",\n    pages = \"54--69\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.5.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.5/",
        "pdf_size": 2642931,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13140689371917996396&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Duke University; Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington; Department of Computer Science, George Mason University",
        "aff_domain": "duke.edu;cs.washington.edu;cs.washington.edu;gmu.edu",
        "email": "duke.edu;cs.washington.edu;cs.washington.edu;gmu.edu",
        "github": "https://github.com/ruoyuxie/interpretable_dialect_classifier",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Duke University;University of Washington;George Mason University",
        "aff_unique_dep": ";Paul G. Allen School of Computer Science & Engineering;Department of Computer Science",
        "aff_unique_url": "https://www.duke.edu;https://www.washington.edu;https://www.gmu.edu",
        "aff_unique_abbr": "Duke;UW;GMU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.9",
        "title": "Extractive Summarization with Text Generator",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Standard extractive systems suffer from the lack of gold training signals since existing corpora solely provide document and human-written summary pairs while disregarding extractive labels. As a result, existing methods resort to imperfect pseudo-labels that are both biased and error-prone, thereby hindering the learning process of extractive models. In contrast, text generators which are commonly employed in abstractive summarization can effortlessly overcome this predicament on account of flexible sequence-to-sequence architectures. Motivated to bypass this inherent limitation, we investigate the possibility of conducting extractive summarization with text generators. Through extensive experiments covering six summarization benchmarks, we show that high-quality extractive summaries can be assembled via approximating the outputs (abstractive summaries) of these generators. Moreover, we find that the approximate summaries correlate positively with the auxiliary summaries (i.e. a better generator enables the production of better extractive summaries). Our results signify a new paradigm for training extractive summarizers i.e. learning with generation (abstractive) objectives rather than extractive schemes.",
        "author": "Thang Le; Anh Tuan Luu",
        "authorids": "/t/thang-le/; /l/luu-anh-tuan/",
        "bibtex": "@inproceedings{le-luu-2024-extractive,\n    title = \"Extractive Summarization with Text Generator\",\n    author = \"Le, Thang  and\n      Luu, Anh Tuan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.9/\",\n    doi = \"10.18653/v1/2024.naacl-long.9\",\n    pages = \"157--174\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.9.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.9/",
        "pdf_size": 336574,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5383473909157547685&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "VinAI Research; Nanyang Technological University",
        "aff_domain": "vinai.io;ntu.edu.sg",
        "email": "vinai.io;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "VinAI Research;Nanyang Technological University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.vinai.io/;https://www.ntu.edu.sg",
        "aff_unique_abbr": "VinAI;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Vietnam;Singapore"
    },
    {
        "id": "2024.naacl-long.397",
        "title": "Extremely Weakly-supervised Text Classification with Wordsets Mining and Sync-Denoising",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Extremely weakly-supervised text classification aims to classify texts without any labeled data, but only relying on class names as supervision. Existing works include prompt-based and seed-based methods. Prompt-based methods prompt language model with instructions, while seed-based methods generate pseudo-labels with word matching. Both of them have significant flaws, including zero-shot instability and context-dependent ambiguities. This paper introduces SetSync, which follows a new paradigm, i.e. wordset-based, which can avoid the above problems. In SetSync, a class is represented with wordsets, and pseudo-labels are generated with wordsets matching. To facilitate this, we propose to use information bottleneck to identify class-relevant wordsets. Moreover, we regard the classifier training as a hybrid learning of semi-supervised and noisy-labels, and propose a new training strategy, termed sync-denoising. Extensive experiments on 11 datasets show that SetSync outperforms all existing prompt and seed methods, exceeding SOTA by an impressive average of 8 points.",
        "author": "Lysa Xiao",
        "authorids": "/l/lysa-xiao/",
        "bibtex": "@inproceedings{xiao-2024-extremely,\n    title = \"Extremely Weakly-supervised Text Classification with Wordsets Mining and Sync-Denoising\",\n    author = \"Xiao, Lysa\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.397/\",\n    doi = \"10.18653/v1/2024.naacl-long.397\",\n    pages = \"7167--7179\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.397.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.397/",
        "pdf_size": 711255,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1637044307403296727&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "East China Jiao Tong University / Nanchang, Jiangxi",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "East China Jiao Tong University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.ecjtu.edu.cn",
        "aff_unique_abbr": "ECJTU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Nanchang",
        "aff_country_unique_index": "0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.4",
        "title": "Extremely efficient online query encoding for dense retrieval",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Existing dense retrieval systems utilize the same model architecture for encoding both the passages and the queries, even though queries are much shorter and simpler than passages. This leads to high latency of the query encoding, which is performed online and therefore might impact user experience. We show that combining a standard large passage encoder with a small efficient query encoder can provide significant latency drops with only a small decrease in quality. We offer a pretraining and training solution for multiple small query encoder architectures. Using a small transformer architecture we are able to decrease latency by up to \u223c12\u00d7, while MRR@10 on the MS MARCO dev set only decreases from 38.2 to 36.2. If this solution does not reach the desired latency requirements, we propose an efficient RNN as the query encoder, which processes the query prefix incrementally and only infers the last word after the query is issued. This shortens latency by \u223c38\u00d7 with only a minor drop in quality, reaching 35.5 MRR@10 score.",
        "author": "Nachshon Cohen; Yaron Fairstein; Guy Kushilevitz",
        "authorids": "/n/nachshon-cohen/; /y/yaron-fairstein/; /g/guy-kushilevitz/",
        "bibtex": "@inproceedings{cohen-etal-2024-extremely,\n    title = \"Extremely efficient online query encoding for dense retrieval\",\n    author = \"Cohen, Nachshon  and\n      Fairstein, Yaron  and\n      Kushilevitz, Guy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.4/\",\n    doi = \"10.18653/v1/2024.findings-naacl.4\",\n    pages = \"43--50\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.4.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.4/",
        "pdf_size": 217712,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7977884009078499923&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Amazon; Amazon; Amazon",
        "aff_domain": "amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com",
        "github": "https://github.com/amzn/extremely-efficient-query-encoder",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon.com, Inc.",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.398",
        "title": "F-MALLOC: Feed-forward Memory Allocation for Continual Learning in Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In the evolving landscape of Neural Machine Translation (NMT), the pretrain-then-finetune paradigm has yielded impressive results. However, the persistent challenge of Catastrophic Forgetting (CF) remains a hurdle. While previous work has introduced Continual Learning (CL) methods to address CF, these approaches grapple with the delicate balance between avoiding forgetting and maintaining system extensibility. To address this, we propose a CL method, named F-MALLOC (Feed-forward Memory ALLOCation). F-MALLOC is inspired by recent insights highlighting that feed-forward layers emulate neural memories and encapsulate crucial translation knowledge. It decomposes feed-forward layers into discrete memory cells and allocates these memories to different tasks. By learning to allocate and safeguard these memories, our method effectively alleviates CF while ensuring robust extendability. Besides, we propose a comprehensive assessment protocol for multi-stage CL of NMT systems. Experiments conducted following this new protocol showcase the superior performance of F-MALLOC, evidenced by higher BLEU scores and almost zero forgetting.",
        "author": "Junhong Wu; Yuchen Liu; Chengqing Zong",
        "authorids": "/j/junhong-wu/; /y/yuchen-liu/; /c/chengqing-zong/",
        "bibtex": "@inproceedings{wu-etal-2024-f,\n    title = \"{F}-{MALLOC}: Feed-forward Memory Allocation for Continual Learning in Neural Machine Translation\",\n    author = \"Wu, Junhong  and\n      Liu, Yuchen  and\n      Zong, Chengqing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.398/\",\n    doi = \"10.18653/v1/2024.naacl-long.398\",\n    pages = \"7180--7192\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.398.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.398/",
        "pdf_size": 442913,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11361494544064773967&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China+State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China",
        "aff_domain": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "https://github.com/WJMacro/ContinualMT",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0+1",
        "aff_unique_norm": "University of Chinese Academy of Sciences;Chinese Academy of Sciences",
        "aff_unique_dep": "School of Artificial Intelligence;Institute of Automation",
        "aff_unique_url": "http://www.ucas.ac.cn;http://www.ia.cas.cn",
        "aff_unique_abbr": "UCAS;CAS",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.457",
        "title": "FAMuS: Frames Across Multiple Sources",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding event descriptions is a central aspect of language processing, but current approaches focus overwhelmingly on single sentences or documents. Aggregating information about an event across documents can offer a much richer understanding. To this end, we present FAMuS, a new corpus of Wikipedia passages that report on some event, paired with underlying, genre-diverse (non-Wikipedia) source articles for the same event. Events and (cross-sentence) arguments in both report and source are annotated against FrameNet, providing broad coverage of different event types. We present results on two key event understanding tasks enabled by FAMuS: source validation\u2014determining whether a document is a valid source for a target report event\u2014and cross-document argument extraction\u2014full-document argument extraction for a target event from both its report and the correct source article.",
        "author": "Siddharth Vashishtha; Alexander Martin; William Gantt; Benjamin Van Durme; Aaron White",
        "authorids": "/s/siddharth-vashishtha/; /a/alexander-martin/; /w/william-gantt/; /b/benjamin-van-durme/; /a/aaron-white/",
        "bibtex": "@inproceedings{vashishtha-etal-2024-famus,\n    title = \"{FAM}u{S}: Frames Across Multiple Sources\",\n    author = \"Vashishtha, Siddharth  and\n      Martin, Alexander  and\n      Gantt, William  and\n      Van Durme, Benjamin  and\n      White, Aaron\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.457/\",\n    doi = \"10.18653/v1/2024.naacl-long.457\",\n    pages = \"8250--8273\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.457.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.457/",
        "pdf_size": 2179911,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9982791871318859889&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Rochester; University of Rochester; University of Rochester; Johns Hopkins University; University of Rochester",
        "aff_domain": "cs.rochester.edu;u.rochester.edu;cs.rochester.edu; ;rochester.edu",
        "email": "cs.rochester.edu;u.rochester.edu;cs.rochester.edu; ;rochester.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Rochester;Johns Hopkins University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rochester.edu;https://www.jhu.edu",
        "aff_unique_abbr": "U of R;JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.230",
        "title": "FIRE: A Dataset for Financial Relation Extraction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper introduces FIRE (**FI**nancial **R**elation **E**xtraction), a sentence-level dataset of named entities and relations within the financial sector. Comprising 3,025 instances, the dataset encapsulates 13 named entity types along with 18 relation types. Sourced from public financial reports and financial news articles, FIRE captures a wide array of financial information about a business including, but not limited to, corporate structure, business model, revenue streams, and market activities such as acquisitions. The full dataset was labeled by a single annotator to minimize labeling noise. The labeling time for each sentence was recorded during the labeling process. We show how this feature, along with curriculum learning techniques, can be used to improved a model\u2019s performance. The FIRE dataset is designed to serve as a valuable resource for training and evaluating machine learning algorithms in the domain of financial information extraction. The dataset and the code to reproduce our experimental results are available at https://github.com/hmhamad/FIRE. The repository for the labeling tool can be found at https://github.com/abhinav-kumar-thakur/relation-extraction-annotator.",
        "author": "Hassan Hamad; Abhinav Kumar Thakur; Nijil Kolleri; Sujith Pulikodan; Keith Chugg",
        "authorids": "/h/hassan-hamad/; /a/abhinav-kumar-thakur/; /n/nijil-kolleri/; /s/sujith-pulikodan/; /k/keith-chugg/",
        "bibtex": "@inproceedings{hamad-etal-2024-fire,\n    title = \"{FIRE}: A Dataset for Financial Relation Extraction\",\n    author = \"Hamad, Hassan  and\n      Thakur, Abhinav Kumar  and\n      Kolleri, Nijil  and\n      Pulikodan, Sujith  and\n      Chugg, Keith\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.230/\",\n    doi = \"10.18653/v1/2024.findings-naacl.230\",\n    pages = \"3628--3642\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.230.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.230/",
        "pdf_size": 575678,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7990482660266027124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, California + Vijna Labs Pvt. Ltd, Bengaluru, India; Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, California + Vijna Labs Pvt. Ltd, Bengaluru, India; Vijna Labs Pvt. Ltd, Bengaluru, India; Vijna Labs Pvt. Ltd, Bengaluru, India; Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, California",
        "aff_domain": "usc.edu;usc.edu;v-labs.ai;v-labs.ai;usc.edu",
        "email": "usc.edu;usc.edu;v-labs.ai;v-labs.ai;usc.edu",
        "github": "https://github.com/hmhamad/FIRE",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;1;1;0",
        "aff_unique_norm": "University of Southern California;Vijna Labs Pvt. Ltd",
        "aff_unique_dep": "Ming Hsieh Department of Electrical and Computer Engineering;",
        "aff_unique_url": "https://www.usc.edu;",
        "aff_unique_abbr": "USC;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0+1;0+1;1;1;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "2024.naacl-long.29",
        "title": "FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Planning is a crucial task for agents in task oriented dialogs (TODs). Human agents typically resolve user issues by following predefined workflows, decomposing workflow steps into actionable items, and performing actions by executing APIs in order; all of which require reasoning and planning. With the recent advances in LLMs, there have been increasing attempts to use them for task planning and API usage. However, the faithfulness of the plans to predefined workflows and API dependencies, is not guaranteed with LLMs. Moreover, workflows in real life are often custom-defined and prone to changes; hence, adaptation is desirable. To study this, we propose the problem of faithful planning in TODs that needs to resolve user intents by following predefined flows and preserving API dependencies. To solve this problem, we propose FLAP, a Flow-Adhering Planning algorithm based on constrained decoding with lookahead heuristic for LLMs. Our algorithm alleviates the need for finetuning LLMs using domain specific (plan/dependency) data, enables quick adaptation to predefined flows, and outperforms other decoding and prompting-based baselines. Further, our algorithm empowers smaller LLMs (\u22487B) to perform at par larger LLMs (\u224830B-40B).",
        "author": "Shamik Roy; Sailik Sengupta; Daniele Bonadiman; Saab Mansour; Arshit Gupta",
        "authorids": "/s/shamik-roy/; /s/sailik-sengupta/; /d/daniele-bonadiman/; /s/saab-mansour/; /a/arshit-gupta/",
        "bibtex": "@inproceedings{roy-etal-2024-flap,\n    title = \"{FLAP}: Flow-Adhering Planning with Constrained Decoding in {LLM}s\",\n    author = \"Roy, Shamik  and\n      Sengupta, Sailik  and\n      Bonadiman, Daniele  and\n      Mansour, Saab  and\n      Gupta, Arshit\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.29/\",\n    doi = \"10.18653/v1/2024.naacl-long.29\",\n    pages = \"517--539\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.29.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.29/",
        "pdf_size": 857204,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13242523436117731490&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "/amazonWS AI Labs; /amazonWS AI Labs; /amazonWS AI Labs; /amazonWS AI Labs; /amazonWS AI Labs",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "AI Labs",
        "aff_unique_url": "https://aws.amazon.com",
        "aff_unique_abbr": "AWS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.16",
        "title": "FPT: Feature Prompt Tuning for Few-shot Readability Assessment",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prompt-based methods have achieved promising results in most few-shot text classification tasks. However, for readability assessment tasks, traditional prompt methods lack crucial linguistic knowledge, which has already been proven to be essential.Moreover, previous studies on utilizing linguistic features have shown non-robust performance in few-shot settings and may even impair model performance.To address these issues, we propose a novel prompt-based tuning framework that incorporates rich linguistic knowledge, called Feature Prompt Tuning (FPT). Specifically, we extract linguistic features from the text and embed them into trainable soft prompts. Further, we devise a new loss function to calibrate the similarity ranking order between categories. Experimental results demonstrate that our proposed method FTPnot only exhibits a significant performance improvement over the prior best prompt-based tuning approaches, but also surpasses the previous leading methods that incorporate linguistic features. Also, our proposed model significantly outperforms the large language model gpt-3.5-turbo-16k in most cases. Our proposed method establishes a new architecture for prompt tuning that sheds light on how linguistic features can be easily adapted to linguistic-related tasks.",
        "author": "Ziyang Wang; Sanwoo Lee; Hsiu-Yuan Huang; Yunfang Wu",
        "authorids": "/z/ziyang-wang/; /s/sanwoo-lee/; /h/hsiu-yuan-huang/; /y/yunfang-wu/",
        "bibtex": "@inproceedings{wang-etal-2024-fpt,\n    title = \"{FPT}: Feature Prompt Tuning for Few-shot Readability Assessment\",\n    author = \"Wang, Ziyang  and\n      Lee, Sanwoo  and\n      Huang, Hsiu-Yuan  and\n      Wu, Yunfang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.16/\",\n    doi = \"10.18653/v1/2024.naacl-long.16\",\n    pages = \"280--295\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.16.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.16/",
        "pdf_size": 903363,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:TDsRoDwIPfcJ:scholar.google.com/&scioq=FPT:+Feature+Prompt+Tuning+for+Few-shot+Readability+Assessment&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "National Key Laboratory for Multimedia Information Processing, Peking University+School of Software and Microelectronics, Peking University, Beijing, China; National Key Laboratory for Multimedia Information Processing, Peking University+School of Computer Science, Peking University, Beijing, China; National Key Laboratory for Multimedia Information Processing, Peking University+School of Computer Science, Peking University, Beijing, China; National Key Laboratory for Multimedia Information Processing, Peking University+School of Computer Science, Peking University, Beijing, China",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "National Key Laboratory for Multimedia Information Processing",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.137",
        "title": "FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Table Question Answering (TQA) aims at composing an answer to a question based on tabular data. While prior research has shown that TQA models lack robustness, understanding the underlying cause and nature of this issue remains predominantly unclear, posing a significant obstacle to the development of robust TQA systems. In this paper, we formalize three major desiderata for a fine-grained evaluation of robustness of TQA systems. They should (i) answer questions regardless of alterations in table structure, (ii) base their responses on the content of relevant cells rather than on biases, and (iii) demonstrate robust numerical reasoning capabilities. To investigate these aspects, we create and publish a novel TQA evaluation benchmark in English. Our extensive experimental analysis reveals that none of the examined state-of-the-art TQA systems consistently excels in these three aspects. Our benchmark is a crucial instrument for monitoring the behavior of TQA systems and paves the way for the development of robust TQA systems. We release our benchmark publicly.",
        "author": "Wei Zhou; Mohsen Mesgar; Heike Adel; Annemarie Friedrich",
        "authorids": "/w/wei-zhou/; /m/mohsen-mesgar/; /h/heike-adel/; /a/annemarie-friedrich/",
        "bibtex": "@inproceedings{zhou-etal-2024-freb,\n    title = \"{FREB}-{TQA}: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering\",\n    author = \"Zhou, Wei  and\n      Mesgar, Mohsen  and\n      Adel, Heike  and\n      Friedrich, Annemarie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.137/\",\n    doi = \"10.18653/v1/2024.naacl-long.137\",\n    pages = \"2479--2497\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.137.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.137/",
        "pdf_size": 601013,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11584662923842340054&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Bosch Center for Artificial Intelligence, Renningen, Germany + University of Augsburg, Germany; Bosch Center for Artificial Intelligence, Renningen, Germany; Hochschule der Medien, Stuttgart, Germany; University of Augsburg, Germany",
        "aff_domain": "de.bosch.com;de.bosch.com; ;informatik.uni-augsburg.de",
        "email": "de.bosch.com;de.bosch.com; ;informatik.uni-augsburg.de",
        "github": "https://github.com/boschresearch/FREB-TQA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;2;1",
        "aff_unique_norm": "Bosch Center for Artificial Intelligence;University of Augsburg;Hochschule der Medien",
        "aff_unique_dep": "Artificial Intelligence;;",
        "aff_unique_url": "https://www.bosch-ai.com;https://www.uni-augsburg.de;https://www.hdm-stuttgart.de",
        "aff_unique_abbr": "BCAI;;",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Renningen;;Stuttgart",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.111",
        "title": "FUN with Fisher: Improving Generalization of Adapter-Based Cross-lingual Transfer with Scheduled Unfreezing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Standard fine-tuning of language models typically performs well on in-distribution data, but suffers with generalization to distribution shifts. In this work, we aim to improve the generalization of adapter-based cross-lingual task transfer where such cross-language distribution shifts are imminent. We investigate scheduled unfreezing algorithms \u2013originally proposed to mitigate catastrophic forgetting in transfer learning \u2013 for fine-tuning task adapters. Our experiments show that scheduled unfreezing methods close the gap to full fine-tuning and achieve stronger cross-lingual transfer performance, suggesting that these methods can go beyond just mitigating catastrophic forgetting. Next, aiming to understand these empirical findings, we investigate the learning dynamics of scheduled unfreezing using Fisher Information. Our experiments reveal that scheduled unfreezing induces different learning dynamics compared to standard fine-tuning, and provide evidence that the dynamics of Fisher Information during training correlate with cross-lingual generalization performance. We additionally propose a general scheduled unfreezing algorithm that achieves an average of 2 points improvement over four datasets compared to standard fine-tuning and provides empirical evidence for a theory-based justification of the heuristic unfreezing schedule for task adapter training.",
        "author": "Chen Liu; Jonas Pfeiffer; Ivan Vuli\u0107; Iryna Gurevych",
        "authorids": "/c/chen-liu/; /j/jonas-pfeiffer/; /i/ivan-vulic/; /i/iryna-gurevych/",
        "bibtex": "@inproceedings{liu-etal-2024-fun,\n    title = \"{FUN} with Fisher: Improving Generalization of Adapter-Based Cross-lingual Transfer with Scheduled Unfreezing\",\n    author = \"Liu, Chen  and\n      Pfeiffer, Jonas  and\n      Vuli{\\'c}, Ivan  and\n      Gurevych, Iryna\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.111/\",\n    doi = \"10.18653/v1/2024.naacl-long.111\",\n    pages = \"1998--2015\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.111.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.111/",
        "pdf_size": 765524,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14464179056672567080&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Ubiquitous Knowledge Processing Lab, Department of Computer Science and Hessian Center for AI (hessian.AI), Technical University of Darmstadt; Google DeepMind; Language Technology Lab, University of Cambridge; Ubiquitous Knowledge Processing Lab, Department of Computer Science and Hessian Center for AI (hessian.AI), Technical University of Darmstadt",
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "github": "https://github.com/UKPLab/naacl2024-funan",
        "project": "www.ukp.tu-darmstadt.de",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Technical University of Darmstadt;Google;University of Cambridge",
        "aff_unique_dep": "Department of Computer Science;Google DeepMind;Language Technology Lab",
        "aff_unique_url": "https://www.tu-darmstadt.de;https://deepmind.com;https://www.cam.ac.uk",
        "aff_unique_abbr": "TUD;DeepMind;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "2024.naacl-long.124",
        "title": "Fact Checking Beyond Training Set",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise. We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain. Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem. We propose an adversarial algorithm to make the retriever component robust against distribution shift. Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data. We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents. Our empirical evaluations support the hypothesis that such a reader shows a higher robustness against distribution shift. To our knowledge, there is no publicly available multi-topic fact checking dataset. Thus, we propose a simple automatic method to re-purpose two well-known fact checking datasets. We then construct eight fact checking scenarios from these datasets, and compare our model to a set of strong baseline models, including recent domain adaptation models that use GPT4 for generating synthetic data.",
        "author": "Payam Karisani; Heng Ji",
        "authorids": "/p/payam-karisani/; /h/heng-ji/",
        "bibtex": "@inproceedings{karisani-ji-2024-fact,\n    title = \"Fact Checking Beyond Training Set\",\n    author = \"Karisani, Payam  and\n      Ji, Heng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.124/\",\n    doi = \"10.18653/v1/2024.naacl-long.124\",\n    pages = \"2247--2261\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.124.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.124/",
        "pdf_size": 2779392,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6335879257835691766&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "UIUC; UIUC",
        "aff_domain": "illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu",
        "github": "https://github.com/p-karisani/OODFC",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.187",
        "title": "Fair Abstractive Summarization of Diverse Perspectives",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "People from different social and demographic groups express diverse perspectives and conflicting opinions on a broad set of topics such as product reviews, healthcare, law, and politics. A fair summary should provide a comprehensive coverage of diverse perspectives without underrepresenting certain groups. However, current work in summarization metrics and Large Language Models (LLMs) evaluation has not explored fair abstractive summarization. In this paper, we systematically investigate fair abstractive summarization for user-generated data. We first formally define fairness in abstractive summarization as not underrepresenting perspectives of any groups of people, and we propose four reference-free automatic metrics by measuring the differences between target and source perspectives. We evaluate nine LLMs, including three GPT models, four LLaMA models, PaLM 2, and Claude, on six datasets collected from social media, online reviews, and recorded transcripts. Experiments show that both the model-generated and the human-written reference summaries suffer from low fairness. We conduct a comprehensive analysis of the common factors influencing fairness and propose three simple but effective methods to alleviate unfair summarization. Our dataset and code are available at https://github.com/psunlpgroup/FairSumm.",
        "author": "Yusen Zhang; Nan Zhang; Yixin Liu; Alexander Fabbri; Junru Liu; Ryo Kamoi; Xiaoxin Lu; Caiming Xiong; Jieyu Zhao; Dragomir Radev; Kathleen McKeown; Rui Zhang",
        "authorids": "/y/yusen-zhang/; /n/nan-zhang/; /y/yixin-liu/; /a/alexander-richard-fabbri/; /j/junru-liu/; /r/ryo-kamoi/; /x/xiaoxin-lu/; /c/caiming-xiong/; /j/jieyu-zhao/; /d/dragomir-radev/; /k/kathleen-mckeown/; /r/rui-zhang/",
        "bibtex": "@inproceedings{zhang-etal-2024-fair,\n    title = \"Fair Abstractive Summarization of Diverse Perspectives\",\n    author = \"Zhang, Yusen  and\n      Zhang, Nan  and\n      Liu, Yixin  and\n      Fabbri, Alexander  and\n      Liu, Junru  and\n      Kamoi, Ryo  and\n      Lu, Xiaoxin  and\n      Xiong, Caiming  and\n      Zhao, Jieyu  and\n      Radev, Dragomir  and\n      McKeown, Kathleen  and\n      Zhang, Rui\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.187/\",\n    doi = \"10.18653/v1/2024.naacl-long.187\",\n    pages = \"3404--3426\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.187.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.187/",
        "pdf_size": 1547698,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16709450976103915706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Penn State University\u2663; Penn State University\u2663; Yale University\u2020; Salesforce Research\u2662; Texas A&M University\u2660; Penn State University\u2663; Penn State University\u2663; Salesforce Research\u2662; University of Southern California\u2021; Yale University\u2020; Columbia University\u2661; Penn State University\u2663",
        "aff_domain": "psu.edu;psu.edu; ; ; ; ; ; ; ; ; ;",
        "email": "psu.edu;psu.edu; ; ; ; ; ; ; ; ; ;",
        "github": "https://github.com/psunlpgroup/FairSumm",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0;0;1;2;3;0;0;2;4;1;5;0",
        "aff_unique_norm": "Penn State University;Yale University;Salesforce;Texas A&M University;University of Southern California;Columbia University",
        "aff_unique_dep": ";;Salesforce Research;;;",
        "aff_unique_url": "https://www.psu.edu;https://www.yale.edu;https://research.salesforce.com;https://www.tamu.edu;https://www.usc.edu;https://www.columbia.edu",
        "aff_unique_abbr": "PSU;Yale;Salesforce Research;TAMU;USC;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.263",
        "title": "Fake Alignment: Are LLMs Really Aligned Well?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The growing awareness of safety concerns in large language models (LLMs) has sparked considerable interest in the evaluation of safety. This study investigates an under-explored issue about the evaluation of LLMs, namely the substantial discrepancy in performance between multiple-choice questions and open-ended questions. Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization. That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests. We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics\u2014\u2014Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment and obtain corrected performance estimation. Applying FINE to 14 widely-used LLMs reveals several models with purported safety are poorly aligned in practice. Subsequently, we found that multiple-choice format data can also be used as high-quality contrast distillation-based fine-tuning data, which can strongly improve the alignment consistency of LLMs with minimal fine-tuning overhead. For data and code, see https://github.com/AIFlames/Fake-Alignment.",
        "author": "Yixu Wang; Yan Teng; Kexin Huang; Chengqi Lyu; Songyang Zhang; Wenwei Zhang; Xingjun Ma; Yu-Gang Jiang; Yu Qiao; Yingchun Wang",
        "authorids": "/y/yixu-wang/; /y/yan-teng/; /k/kexin-huang/; /c/chengqi-lyu/; /s/songyang-zhang/; /w/wenwei-zhang/; /x/xingjun-ma/; /y/yu-gang-jiang/; /y/yu-qiao/; /y/yingchun-wang/",
        "bibtex": "@inproceedings{wang-etal-2024-fake,\n    title = \"Fake Alignment: Are {LLM}s Really Aligned Well?\",\n    author = \"Wang, Yixu  and\n      Teng, Yan  and\n      Huang, Kexin  and\n      Lyu, Chengqi  and\n      Zhang, Songyang  and\n      Zhang, Wenwei  and\n      Ma, Xingjun  and\n      Jiang, Yu-Gang  and\n      Qiao, Yu  and\n      Wang, Yingchun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.263/\",\n    doi = \"10.18653/v1/2024.naacl-long.263\",\n    pages = \"4696--4712\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.263.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.263/",
        "pdf_size": 1529703,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8388148327003652877&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Fudan University+Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; Fudan University+Shanghai Artificial Intelligence Laboratory; Fudan University; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory",
        "aff_domain": "fudan.edu.cn;pjlab.org.cn;pjlab.org.cn;pjlab.org.cn;pjlab.org.cn;pjlab.org.cn;fudan.edu.cn;fudan.edu.cn;pjlab.org.cn;pjlab.org.cn",
        "email": "fudan.edu.cn;pjlab.org.cn;pjlab.org.cn;pjlab.org.cn;pjlab.org.cn;pjlab.org.cn;fudan.edu.cn;fudan.edu.cn;pjlab.org.cn;pjlab.org.cn",
        "github": "https://github.com/AIFlames/Fake-Alignment",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;1;1;1;1;1;0+1;0;1;1",
        "aff_unique_norm": "Fudan University;Shanghai Artificial Intelligence Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.fudan.edu.cn;http://www.shailab.org/",
        "aff_unique_abbr": "Fudan;Shanghai AI Lab",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-demo.18",
        "title": "FastFit: Fast and Effective Few-Shot Text Classification with a Multitude of Classes",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We present FastFit, a Python package designed to provide fast and accurate few-shot classification, especially for scenarios with many semantically similar classes. FastFit utilizes a novel approach integrating batch contrastive learning and token-level similarity score. Compared to existing few-shot learning packages, such as SetFit, Transformers, or few-shot prompting of large language models via API calls, FastFit significantly improves multi-class classification performance in speed and accuracy across various English and Multilingual datasets. FastFit demonstrates a 3-20x improvement in training speed, completing training in just a few seconds. The FastFit package is now available on GitHub, presenting a user-friendly solution for NLP practitioners.",
        "author": "Asaf Yehudai; Elron Bandel",
        "authorids": "/a/asaf-yehudai/; /e/elron-bandel/",
        "bibtex": "@inproceedings{yehudai-bandel-2024-fastfit,\n    title = \"{F}ast{F}it: Fast and Effective Few-Shot Text Classification with a Multitude of Classes\",\n    author = \"Yehudai, Asaf  and\n      Bandel, Elron\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.18/\",\n    doi = \"10.18653/v1/2024.naacl-demo.18\",\n    pages = \"174--184\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.18.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.18/",
        "pdf_size": 550981,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6610699599349224976&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "IBM Israel Research Lab\u2662; Hebrew University of Jerusalem\u2663",
        "aff_domain": "ibm.com;ibm.com",
        "email": "ibm.com;ibm.com",
        "github": "https://github.com/FastFit",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "IBM;Hebrew University of Jerusalem",
        "aff_unique_dep": "IBM Israel Research Lab;",
        "aff_unique_url": "https://www.ibm.com/research/il;https://www.huji.ac.il",
        "aff_unique_abbr": "IBM;HUJI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2024.findings-naacl.98",
        "title": "FedLFC: Towards Efficient Federated Multilingual Modeling with LoRA-based Language Family Clustering",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Federated Multilingual Modeling (FMM) plays a crucial role in the applications of natural language processing due to the increasing diversity of languages and the growing demand for data privacy. However, FMM faces limitations stemming from (1) the substantial communication costs in networking and (2) the conflicts arising from parameter interference between different languages. To address these challenges, we introduce a communication-efficient federated learning framework with low-rank adaptation and language family clustering for Multilingual Modeling (MM). In this framework, we maintain the weights of the base model, exclusively updating the lightweight Low-rank adaptation (LoRA) parameters to minimize communication costs. Additionally, we mitigate parameter conflicts by grouping languages based on their language family affiliations, as opposed to aggregating all LoRA parameters. Experiments demonstrate that our proposed model not only surpasses the baseline models in performance but also reduces the communication overhead. Our code is available at https://github.com/zhihan-guo/FedLFC.",
        "author": "Zhihan Guo; Yifei Zhang; Zhuo Zhang; Zenglin Xu; Irwin King",
        "authorids": "/z/zhihan-guo/; /y/yifei-zhang/; /z/zhuo-zhang/; /z/zenglin-xu/; /i/irwin-king/",
        "bibtex": "@inproceedings{guo-etal-2024-fedlfc,\n    title = \"{F}ed{LFC}: Towards Efficient Federated Multilingual Modeling with {L}o{RA}-based Language Family Clustering\",\n    author = \"Guo, Zhihan  and\n      Zhang, Yifei  and\n      Zhang, Zhuo  and\n      Xu, Zenglin  and\n      King, Irwin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.98/\",\n    doi = \"10.18653/v1/2024.findings-naacl.98\",\n    pages = \"1519--1528\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.98.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.98/",
        "pdf_size": 762213,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9757401270437018845&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "The Chinese University of Hong Kong, Hong Kong SAR, China; The Chinese University of Hong Kong, Hong Kong SAR, China; Harbin Institute of Technology, Shenzhen, China+Peng Cheng Lab, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China+Peng Cheng Lab, Shenzhen, China; The Chinese University of Hong Kong, Hong Kong SAR, China",
        "aff_domain": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;gmail.com;hit.edu.cn;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;cse.cuhk.edu.hk;gmail.com;hit.edu.cn;cse.cuhk.edu.hk",
        "github": "https://github.com/zhihan-guo/FedLFC",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1+2;1+2;0",
        "aff_unique_norm": "Chinese University of Hong Kong;Harbin Institute of Technology;Pengcheng Laboratory",
        "aff_unique_dep": ";;Peng Cheng Lab",
        "aff_unique_url": "https://www.cuhk.edu.hk;http://en.hhit.edu.cn/;",
        "aff_unique_abbr": "CUHK;HIT;",
        "aff_campus_unique_index": "0;0;1+1;1+1;0",
        "aff_campus_unique": "Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "0;0;0+0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.253",
        "title": "Few-TK: A Dataset for Few-shot Scientific Typed Keyphrase Recognition",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Scientific texts are distinctive from ordinary texts in quite a few aspects like their vocabulary and discourse structure. Consequently, Information Extraction (IE) tasks for scientific texts come with their own set of challenges. The classical definition of Named Entities restricts the inclusion of all scientific terms under its hood, which is why previous works have used the terms Named Entities and Keyphrases interchangeably. We suggest the rechristening of Named Entities for the scientific domain as Typed Keyphrases (TK), broadening their scope. We advocate for exploring this task in the few-shot domain due to the scarcity of labeled scientific IE data. Currently, no dataset exists for few-shot scientific Typed Keyphrase Recognition. To address this gap, we develop an annotation schema and present Few-TK, a dataset in the AI/ML field that includes scientific Typed Keyphrase annotations on abstracts of 500 research papers. To the best of our knowledge, this is the introductory few-shot Typed Keyphrase recognition dataset and only the second dataset structured specifically for few-shot NER, after Few-NERD. We report the results of several few-shot sequence-labelling models applied to our dataset. The data and code are available at https://github.com/AvishekLahiri/Few_TK.git",
        "author": "Avishek Lahiri; Pratyay Sarkar; Medha Sen; Debarshi Kumar Sanyal; Imon Mukherjee",
        "authorids": "/a/avishek-lahiri/; /p/pratyay-sarkar/; /m/medha-sen/; /d/debarshi-kumar-sanyal/; /i/imon-mukherjee/",
        "bibtex": "@inproceedings{lahiri-etal-2024-tk,\n    title = \"Few-{TK}: A Dataset for Few-shot Scientific Typed Keyphrase Recognition\",\n    author = \"Lahiri, Avishek  and\n      Sarkar, Pratyay  and\n      Sen, Medha  and\n      Sanyal, Debarshi Kumar  and\n      Mukherjee, Imon\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.253/\",\n    doi = \"10.18653/v1/2024.findings-naacl.253\",\n    pages = \"4011--4025\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.253.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.253/",
        "pdf_size": 451661,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14768309448049993303&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Indian Association for the Cultivation of Science, Kolkata, India; Indian Association for the Cultivation of Science, Kolkata, India; Indian Association for the Cultivation of Science, Kolkata, India; Indian Association for the Cultivation of Science, Kolkata, India; Indian Institute of Information Technology, Kalyani, India",
        "aff_domain": "gmail.com;gmail.com;gmail.com;iacs.res.in;iiitkalyani.ac.in",
        "email": "gmail.com;gmail.com;gmail.com;iacs.res.in;iiitkalyani.ac.in",
        "github": "https://github.com/AvishekLahiri/Few_TK.git",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Indian Association for the Cultivation of Science;Indian Institute of Information Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://iiitkalyani.ac.in",
        "aff_unique_abbr": ";IIIT Kalyani",
        "aff_campus_unique_index": "0;0;0;0;1",
        "aff_campus_unique": "Kolkata;Kalyani",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2024.naacl-long.183",
        "title": "Few-shot Knowledge Graph Relational Reasoning via Subgraph Adaptation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Few-shot Knowledge Graph (KG) Relational Reasoning aims to predict unseen triplets (i.e., query triplets) for rare relations in KGs, given only several triplets of these relations as references (i.e., support triplets). This task has gained significant traction due to the widespread use of knowledge graphs in various natural language processing applications. Previous approaches have utilized meta-training methods and manually constructed meta-relation sets to tackle this task. Recent efforts have focused on edge-mask-based methods, which exploit the structure of the contextualized graphs of target triplets (i.e., a subgraph containing relevant triplets in the KG). However, existing edge-mask-based methods have limitations in extracting insufficient information from KG and are highly influenced by spurious information in KG. To overcome these challenges, we propose SAFER (Subgraph Adaptation for Few-shot Relational Reasoning), a novel approach that effectively adapts the information in contextualized graphs to various subgraphs generated from support and query triplets to perform the prediction. Specifically, SAFER enables the extraction of more comprehensive information from support triplets while minimizing the impact of spurious information when predicting query triplets. Experimental results on three prevalent datasets demonstrate the superiority of our proposed framework SAFER.",
        "author": "Haochen Liu; Song Wang; Chen Chen; Jundong Li",
        "authorids": "/h/haochen-liu/; /s/song-wang/; /c/chen-chen/; /j/jundong-li/",
        "bibtex": "@inproceedings{liu-etal-2024-shot,\n    title = \"Few-shot Knowledge Graph Relational Reasoning via Subgraph Adaptation\",\n    author = \"Liu, Haochen  and\n      Wang, Song  and\n      Chen, Chen  and\n      Li, Jundong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.183/\",\n    doi = \"10.18653/v1/2024.naacl-long.183\",\n    pages = \"3346--3356\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.183.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.183/",
        "pdf_size": 1204514,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8687336823169586837&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Virginia; University of Virginia; University of Virginia; University of Virginia",
        "aff_domain": "virginia.edu;virginia.edu;virginia.edu;virginia.edu",
        "email": "virginia.edu;virginia.edu;virginia.edu;virginia.edu",
        "github": "https://github.com/HaochenLiu2000/SAFER",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Virginia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.virginia.edu",
        "aff_unique_abbr": "UVA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.17",
        "title": "Fighting crime with Transformers: Empirical analysis of address parsing methods in payment data",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In the financial industry, identifying the location of parties involved in payments is a major challenge in the context of Anti-Money Laundering transaction monitoring. For this purpose address parsing entails extracting fields such as street, postal code, or country from free text message attributes. While payment processing platforms are updating their standards with more structured formats such as SWIFT with ISO 20022, address parsing remains essential for a considerable volume of messages. With the emergence of Transformers and Generative Large Language Models (LLM), we explore the performance of state-of-the-art solutions given the constraint of processing a vast amount of daily data. This paper also aims to show the need for training robust models capable of dealing with real-world noisy transactional data. Our results suggest that a well fine-tuned Transformer model using early-stopping significantly outperforms other approaches. Nevertheless, generative LLMs demonstrate strong zero_shot performance and warrant further investigations.",
        "author": "Haitham Hammami; Louis Baligand; Bojan Petrovski",
        "authorids": "/h/haitham-hammami/; /l/louis-baligand/; /b/bojan-petrovski/",
        "bibtex": "@inproceedings{hammami-etal-2024-fighting,\n    title = \"Fighting crime with Transformers: Empirical analysis of address parsing methods in payment data\",\n    author = \"Hammami, Haitham  and\n      Baligand, Louis  and\n      Petrovski, Bojan\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.17/\",\n    doi = \"10.18653/v1/2024.naacl-industry.17\",\n    pages = \"201--212\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.17.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.17/",
        "pdf_size": 243376,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6014193013119125354&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "EPFL; EPFL; EPFL",
        "aff_domain": "alumni.epfl.ch;alumni.epfl.ch;alumni.epfl.ch",
        "email": "alumni.epfl.ch;alumni.epfl.ch;alumni.epfl.ch",
        "github": "https://github.com/openvenues/libpostal",
        "project": "https://www.iso20022.org/iso-20022-message-definitions",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "EPFL",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.epfl.ch",
        "aff_unique_abbr": "EPFL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2024.naacl-long.275",
        "title": "Finding Replicable Human Evaluations via Stable Ranking Probability",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Reliable human evaluation is critical to the development of successful natural language generation models, but achieving it is notoriously difficult. Stability is a crucial requirement when ranking systems by quality: consistent ranking of systems across repeated evaluations is not just desirable, but essential. Without it, there is no reliable foundation for hill-climbing or product launch decisions. In this paper, we use machine translation and its state-of-the-art human evaluation framework, MQM, as a case study to understand how to set up reliable human evaluations that yield stable conclusions. We investigate the optimal configurations for item allocation to raters, number of ratings per item, and score normalization. Our study on two language pairs provides concrete recommendations for designing replicable human evaluation studies. We also collect and release the largest publicly available dataset of multi-segment translations rated by multiple professional translators, consisting of nearly 140,000 segment annotations across two language pairs.",
        "author": "Parker Riley; Daniel Deutsch; George Foster; Viresh Ratnakar; Ali Dabirmoghaddam; Markus Freitag",
        "authorids": "/p/parker-riley/; /d/daniel-deutsch/; /g/george-foster/; /v/viresh-ratnakar/; /a/ali-dabirmoghaddam/; /m/markus-freitag/",
        "bibtex": "@inproceedings{riley-etal-2024-finding,\n    title = \"Finding Replicable Human Evaluations via Stable Ranking Probability\",\n    author = \"Riley, Parker  and\n      Deutsch, Daniel  and\n      Foster, George  and\n      Ratnakar, Viresh  and\n      Dabirmoghaddam, Ali  and\n      Freitag, Markus\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.275/\",\n    doi = \"10.18653/v1/2024.naacl-long.275\",\n    pages = \"4908--4919\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.275.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.275/",
        "pdf_size": 430943,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7500956359705286651&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.75",
        "title": "Fine-Tuning Language Models with Reward Learning on Policy",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences.RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially.Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs\u2019 data distribution.Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize.In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution.Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples.Meanwhile, a synthetic preference generation approach is developed to simulate high-quality preference data with policy outputs.Extensive experiments on three benchmark datasets show that RLP consistently outperforms the state-of-the-art.Our code is available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp.",
        "author": "Hao Lang; Fei Huang; Yongbin Li",
        "authorids": "/h/hao-lang/; /f/fei-huang/; /y/yongbin-li/",
        "bibtex": "@inproceedings{lang-etal-2024-fine,\n    title = \"Fine-Tuning Language Models with Reward Learning on Policy\",\n    author = \"Lang, Hao  and\n      Huang, Fei  and\n      Li, Yongbin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.75/\",\n    doi = \"10.18653/v1/2024.naacl-long.75\",\n    pages = \"1382--1392\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.75.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.75/",
        "pdf_size": 434161,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6427421439965616772&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Alibaba Group; Alibaba Group; Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Alibaba Group",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.303",
        "title": "Fine-grained Gender Control in Machine Translation with Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In machine translation, the problem of ambiguously gendered input has been pointed out, where the gender of an entity is not available in the source sentence. To address this ambiguity issue, the task of controlled translation that takes the gender of the ambiguous entity as additional input have been proposed. However, most existing works have only considered a simplified setup of one target gender for input. In this paper, we tackle controlled translation in a more realistic setting of inputs with multiple entities and propose Gender-of-Entity (GoE) prompting method for LLMs. Our proposed method instructs the model with fine-grained entity-level gender information to translate with correct gender inflections. By utilizing four evaluation benchmarks, we investigate the controlled translation capability of LLMs in multiple dimensions and find that LLMs reach state-of-the-art performance in controlled translation. Furthermore, we discover an emergence of gender interference phenomenon when controlling the gender of multiple entities. Finally, we address the limitations of existing gender accuracy evaluation metrics and propose leveraging LLMs as an evaluator for gender inflection in machine translation.",
        "author": "Minwoo Lee; Hyukhun Koh; Minsung Kim; Kyomin Jung",
        "authorids": "/m/minwoo-lee/; /h/hyukhun-koh/; /m/minsung-kim/; /k/kyomin-jung/",
        "bibtex": "@inproceedings{lee-etal-2024-fine,\n    title = \"Fine-grained Gender Control in Machine Translation with Large Language Models\",\n    author = \"Lee, Minwoo  and\n      Koh, Hyukhun  and\n      Kim, Minsung  and\n      Jung, Kyomin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.303/\",\n    doi = \"10.18653/v1/2024.naacl-long.303\",\n    pages = \"5416--5430\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.303.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.303/",
        "pdf_size": 1775093,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8251261059656145423&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "LG AI Research+Dept. of ECE, Seoul National University; IPAI, Seoul National University; Dept. of ECE, Seoul National University+Institute of Engineering Research, Seoul National University; Dept. of ECE, Seoul National University+IPAI, Seoul National University+Institute of Engineering Research, Seoul National University",
        "aff_domain": "lgresearch.ai;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "lgresearch.ai;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "github": "https://github.com/minwhoo/fine-grained-gender-control-mt",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;1+1;1+1+1",
        "aff_unique_norm": "LG;Seoul National University",
        "aff_unique_dep": "LG AI Research;Dept. of Electrical and Computer Engineering",
        "aff_unique_url": "https://www.lgaires.com;https://www.snu.ac.kr",
        "aff_unique_abbr": "LG AI;SNU",
        "aff_campus_unique_index": "1;1;1+1;1+1+1",
        "aff_campus_unique": ";Seoul",
        "aff_country_unique_index": "0+0;0;0+0;0+0+0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-long.128",
        "title": "First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Many NLP researchers are experiencing an existential crisis triggered by the astonishing success of ChatGPT and other systems based on large language models (LLMs). After such a disruptive change to our understanding of the field, what is left to do? Taking a historical lens, we look for guidance from the first era of LLMs, which began in 2005 with large n-gram models for machine translation (MT). We identify durable lessons from the first era, and more importantly, we identify evergreen problems where NLP researchers can continue to make meaningful contributions in areas where LLMs are ascendant. We argue that disparities in scale are transient and researchers can work to reduce them; that data, rather than hardware, is still a bottleneck for many applications; that meaningful realistic evaluation is still an open problem; and that there is still room for speculative approaches.",
        "author": "Naomi Saphra; Eve Fleisig; Kyunghyun Cho; Adam Lopez",
        "authorids": "/n/naomi-saphra/; /e/eve-fleisig/; /k/kyunghyun-cho/; /a/adam-lopez/",
        "bibtex": "@inproceedings{saphra-etal-2024-first,\n    title = \"First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models\",\n    author = \"Saphra, Naomi  and\n      Fleisig, Eve  and\n      Cho, Kyunghyun  and\n      Lopez, Adam\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.128/\",\n    doi = \"10.18653/v1/2024.naacl-long.128\",\n    pages = \"2310--2326\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.128.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.128/",
        "pdf_size": 303205,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9986385136493480412&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Kempner Institute at Harvard University; University of California - Berkeley; New York University + Genentech; University of Edinburgh",
        "aff_domain": "fas.harvard.edu;berkeley.edu;nyu.edu;inf.ed.ac.uk",
        "email": "fas.harvard.edu;berkeley.edu;nyu.edu;inf.ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2+3;4",
        "aff_unique_norm": "Harvard University;University of California, Berkeley;New York University;Genentech;University of Edinburgh",
        "aff_unique_dep": "Kempner Institute;;;;",
        "aff_unique_url": "https://www.harvard.edu;https://www.berkeley.edu;https://www.nyu.edu;https://www.genentech.com;https://www.ed.ac.uk",
        "aff_unique_abbr": "Harvard;UC Berkeley;NYU;Genentech;Edinburgh",
        "aff_campus_unique_index": "0;1;",
        "aff_campus_unique": "Cambridge;Berkeley;",
        "aff_country_unique_index": "0;0;0+0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2024.naacl-long.253",
        "title": "Fixing Rogue Memorization in Many-to-One Multilingual Translators of Extremely-Low-Resource Languages by Rephrasing Training Samples",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper we study the fine-tuning of pre-trained large high-resource language models (LLMs) into many-to-one multilingual machine translators for extremely-low-resource languages such as endangered Indigenous languages. We explore those issues using datasets created from pseudo-parallel translations to English of The Bible written in 39 Brazilian Indigenous languages using mBART50 and WMT19 as pre-trained models and multiple translation metrics. We examine bilingual and multilingual models and show that, according to machine translation metrics, same-linguistic family models tend to perform best. However, we also found that many-to-one multilingual systems have a tendency to learn a \u201crogue\u201d strategy of storing output strings from the training data in the LLM structure and retrieving them instead of performing actual translations. We show that rephrasing the output of the training samples seems to solve the problem.",
        "author": "Paulo Cavalin; Pedro Henrique Domingues; Claudio Pinhanez; Julio Nogima",
        "authorids": "/p/paulo-cavalin/; /p/pedro-henrique-domingues/; /c/claudio-pinhanez/; /j/julio-nogima/",
        "bibtex": "@inproceedings{cavalin-etal-2024-fixing,\n    title = \"Fixing Rogue Memorization in Many-to-One Multilingual Translators of Extremely-Low-Resource Languages by Rephrasing Training Samples\",\n    author = \"Cavalin, Paulo  and\n      Domingues, Pedro Henrique  and\n      Pinhanez, Claudio  and\n      Nogima, Julio\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.253/\",\n    doi = \"10.18653/v1/2024.naacl-long.253\",\n    pages = \"4503--4514\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.253.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.253/",
        "pdf_size": 997695,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13551344081376974784&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-long.256",
        "title": "Flames: Benchmarking Value Alignment of LLMs in Chinese",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and \u2018topping the chart\u2019 in these evaluations, there is still a significant gap in LLMs\u2019 deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findings indicate that all the evaluated LLMs demonstrate relatively poor performance on Flames, particularly in the safety and fairness dimensions. We also develop a lightweight specified scorer capable of scoring LLMs across multiple dimensions to efficiently evaluate new models on the benchmark. The complexity of Flames has far exceeded existing benchmarks, setting a new challenge for contemporary LLMs and highlighting the need for further alignment of LLMs. Our benchmark is publicly available at https://github.com/AIFlames/Flames.",
        "author": "Kexin Huang; Xiangyang Liu; Qianyu Guo; Tianxiang Sun; Jiawei Sun; Yaru Wang; Zeyang Zhou; Yixu Wang; Yan Teng; Xipeng Qiu; Yingchun Wang; Dahua Lin",
        "authorids": "/k/kexin-huang/; /x/xiangyang-liu/; /q/qianyu-guo/; /t/tianxiang-sun/; /j/jiawei-sun/; /y/yaru-wang/; /z/zeyang-zhou/; /y/yixu-wang/; /y/yan-teng/; /x/xipeng-qiu/; /y/yingchun-wang/; /d/dahua-lin/",
        "bibtex": "@inproceedings{huang-etal-2024-flames,\n    title = \"Flames: Benchmarking Value Alignment of {LLM}s in {C}hinese\",\n    author = \"Huang, Kexin  and\n      Liu, Xiangyang  and\n      Guo, Qianyu  and\n      Sun, Tianxiang  and\n      Sun, Jiawei  and\n      Wang, Yaru  and\n      Zhou, Zeyang  and\n      Wang, Yixu  and\n      Teng, Yan  and\n      Qiu, Xipeng  and\n      Wang, Yingchun  and\n      Lin, Dahua\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.256/\",\n    doi = \"10.18653/v1/2024.naacl-long.256\",\n    pages = \"4551--4591\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.256.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.256/",
        "pdf_size": 1396900,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17314617728135052958&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Shanghai Artificial Intelligence Laboratory+Fudan University; Shanghai Artificial Intelligence Laboratory+Fudan University; Shanghai Artificial Intelligence Laboratory+Fudan University; Shanghai Artificial Intelligence Laboratory+Fudan University; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory+Fudan University; Shanghai Artificial Intelligence Laboratory+Fudan University; Shanghai Artificial Intelligence Laboratory; Fudan University; Shanghai Artificial Intelligence Laboratory; Shanghai Artificial Intelligence Laboratory",
        "aff_domain": "pjlab.org.cn; ; ; ; ; ; ; ;pjlab.org.cn; ; ; ",
        "email": "pjlab.org.cn; ; ; ; ; ; ; ;pjlab.org.cn; ; ; ",
        "github": "https://github.com/AIFlames/Flames",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0+1;0+1;0+1;0+1;0;0;0+1;0+1;0;1;0;0",
        "aff_unique_norm": "Shanghai Artificial Intelligence Laboratory;Fudan University",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.shailab.org/;https://www.fudan.edu.cn",
        "aff_unique_abbr": "Shanghai AI Lab;Fudan",
        "aff_campus_unique_index": ";;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0;0;0;0+0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.129",
        "title": "Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) exhibit positional bias in how they use context, which especially affects listwise ranking. To address this, we propose permutation self-consistency, a form of self-consistency over the ranking list outputs of black-box LLMs. Our key idea is to marginalize out different list orders in the prompt to produce an order-independent ranking with less positional bias. First, given some input prompt, we repeatedly shuffle the list in the prompt and pass it through the LLM while holding the instructions the same. Next, we aggregate the resulting sample of rankings by computing the central ranking closest in distance to all of them, marginalizing out prompt order biases in the process. Theoretically, we prove the robustness of our method, showing convergence to the true ranking under random perturbations.Empirically, on five datasets in sorting and passage reranking, our approach improves scores from conventional inference by up to 34-52% for Mistral, 7-18% for GPT-3.5, 8-16% for LLaMA v2 (70B). Our code is at https://github.com/castorini/perm-sc.",
        "author": "Raphael Tang; Crystina Zhang; Xueguang Ma; Jimmy Lin; Ferhan Ture",
        "authorids": "/r/raphael-tang/; /c/crystina-zhang/; /x/xueguang-ma/; /j/jimmy-lin/; /f/ferhan-ture/",
        "bibtex": "@inproceedings{tang-etal-2024-found,\n    title = \"Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models\",\n    author = \"Tang, Raphael  and\n      Zhang, Crystina  and\n      Ma, Xueguang  and\n      Lin, Jimmy  and\n      Ture, Ferhan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.129/\",\n    doi = \"10.18653/v1/2024.naacl-long.129\",\n    pages = \"2327--2340\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.129.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.129/",
        "pdf_size": 1139253,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13534293455136043519&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Comcast AI Technologies; University of Waterloo; University of Waterloo; University of Waterloo; Comcast AI Technologies",
        "aff_domain": "comcast.com;uwaterloo.ca;uwaterloo.ca;uwaterloo.ca;comcast.com",
        "email": "comcast.com;uwaterloo.ca;uwaterloo.ca;uwaterloo.ca;comcast.com",
        "github": "https://github.com/castorini/perm-sc",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Comcast;University of Waterloo",
        "aff_unique_dep": "AI Technologies;",
        "aff_unique_url": "https://www.comcast.com;https://uwaterloo.ca",
        "aff_unique_abbr": "Comcast;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "2024.naacl-long.130",
        "title": "From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution, and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts of user prompts, and promotes the response generation constantly conditioned on the instructions. 2) It encourages the self-attention heads to capture more word-word relationships about instruction verbs. 3) It encourages the feed-forward networks to rotate their pre-trained knowledge toward user-oriented tasks. These insights contribute to a more comprehensive understanding of instruction tuning and lay the groundwork for future work that aims at explaining and optimizing LLMs for various applications. Our code and data are publicly available at https://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.",
        "author": "Xuansheng Wu; Wenlin Yao; Jianshu Chen; Xiaoman Pan; Xiaoyang Wang; Ninghao Liu; Dong Yu",
        "authorids": "/x/xuansheng-wu/; /w/wenlin-yao/; /j/jianshu-chen/; /x/xiaoman-pan/; /x/xiaoyang-wang/; /n/ninghao-liu/; /d/dong-yu/",
        "bibtex": "@inproceedings{wu-etal-2024-language,\n    title = \"From Language Modeling to Instruction Following: Understanding the Behavior Shift in {LLM}s after Instruction Tuning\",\n    author = \"Wu, Xuansheng  and\n      Yao, Wenlin  and\n      Chen, Jianshu  and\n      Pan, Xiaoman  and\n      Wang, Xiaoyang  and\n      Liu, Ninghao  and\n      Yu, Dong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.130/\",\n    doi = \"10.18653/v1/2024.naacl-long.130\",\n    pages = \"2341--2369\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.130.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.130/",
        "pdf_size": 4089197,
        "gs_citation": 32,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1012915126425971164&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Georgia\u2663; Tencent AI Lab\u2661; Tencent AI Lab\u2661; Tencent AI Lab\u2661; Tencent AI Lab\u2661; University of Georgia\u2663; Tencent AI Lab\u2661",
        "aff_domain": "uga.edu;global.tencent.com; ; ; ; ;tencent.com",
        "email": "uga.edu;global.tencent.com; ; ; ; ;tencent.com",
        "github": "https://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;0;1",
        "aff_unique_norm": "University of Georgia;Tencent",
        "aff_unique_dep": ";Tencent AI Lab",
        "aff_unique_url": "https://www.uga.edu;https://ai.tencent.com",
        "aff_unique_abbr": "UGA;Tencent AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.421",
        "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In the realm of Large Language Models (LLMs), the balance between instruction data quality and quantity is a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to identify discrepancies between a model\u2019s expected responses and its intrinsic generation capability. Through the application of IFD, cherry samples can be pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of original data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the instruction tuning of LLMs, promising both efficiency and resource-conscious advancements. Codes, data, and models are available.",
        "author": "Ming Li; Yong Zhang; Zhitao Li; Jiuhai Chen; Lichang Chen; Ning Cheng; Jianzong Wang; Tianyi Zhou; Jing Xiao",
        "authorids": "/m/ming-li/; /y/yong-zhang/; /z/zhitao-li/; /j/jiuhai-chen/; /l/lichang-chen/; /n/ning-cheng/; /j/jianzong-wang/; /t/tianyi-zhou/; /j/jing-xiao/",
        "bibtex": "@inproceedings{li-etal-2024-quantity,\n    title = \"From Quantity to Quality: Boosting {LLM} Performance with Self-Guided Data Selection for Instruction Tuning\",\n    author = \"Li, Ming  and\n      Zhang, Yong  and\n      Li, Zhitao  and\n      Chen, Jiuhai  and\n      Chen, Lichang  and\n      Cheng, Ning  and\n      Wang, Jianzong  and\n      Zhou, Tianyi  and\n      Xiao, Jing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.421/\",\n    doi = \"10.18653/v1/2024.naacl-long.421\",\n    pages = \"7602--7635\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.421.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.421/",
        "pdf_size": 10233849,
        "gs_citation": 181,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9143144725152596357&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Ping An Technology (Shenzhen) Co., Ltd., China+University of Maryland; Ping An Technology (Shenzhen) Co., Ltd., China; Ping An Technology (Shenzhen) Co., Ltd., China; University of Maryland; University of Maryland; Ping An Technology (Shenzhen) Co., Ltd., China; Ping An Technology (Shenzhen) Co., Ltd., China; University of Maryland; Ping An Technology (Shenzhen) Co., Ltd., China",
        "aff_domain": "umd.edu; ; ; ; ; ;188.com;umd.edu; ",
        "email": "umd.edu; ; ; ; ; ;188.com;umd.edu; ",
        "github": "https://github.com/tianyi-lab/Cherry_LLM",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;0;0;1;1;0;0;1;0",
        "aff_unique_norm": "Ping An Technology;University of Maryland",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.pingan.com;https://www/umd.edu",
        "aff_unique_abbr": "Ping An Tech;UMD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0;1;1;0;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.27",
        "title": "From Shortcuts to Triggers: Backdoor Defense with Denoised PoE",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Language models are often at risk of diverse backdoor attacks, especially data poisoning. Thus, it is important to investigate defense solutions for addressing them. Existing backdoor defense methods mainly focus on backdoor attacks with explicit triggers, leaving a universal defense against various backdoor attacks with diverse triggers largely unexplored. In this paper, we propose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised Product-of-Experts), which is inspired by the shortcut nature of backdoor attacks, to defend various backdoor attacks. DPoE consists of two models: a shallow model that captures the backdoor shortcuts and a main model that is prevented from learning the shortcuts. To address the label flip caused by backdoor attackers, DPoE incorporates a denoising design. Experiments on three NLP tasks show that DPoE significantly improves the defense performance against various types of backdoor triggers including word-level, sentence-level, and syntactic triggers. Furthermore, DPoE is also effective under a more challenging but practical setting that mixes multiple types of triggers.",
        "author": "Qin Liu; Fei Wang; Chaowei Xiao; Muhao Chen",
        "authorids": "/q/qin-liu/; /f/fei-wang/; /c/chaowei-xiao/; /m/muhao-chen/",
        "bibtex": "@inproceedings{liu-etal-2024-shortcuts,\n    title = \"From Shortcuts to Triggers: Backdoor Defense with Denoised {P}o{E}\",\n    author = \"Liu, Qin  and\n      Wang, Fei  and\n      Xiao, Chaowei  and\n      Chen, Muhao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.27/\",\n    doi = \"10.18653/v1/2024.naacl-long.27\",\n    pages = \"483--496\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.27.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.27/",
        "pdf_size": 962349,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9944258706445697809&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/luka-group/DPoE",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.findings-naacl.274",
        "title": "Fumbling in Babel: An Investigation into ChatGPT\u2019s Language Identification Ability",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "ChatGPT has recently emerged as a powerful NLP tool that can carry out a variety of tasks. However, the range of languages ChatGPT can handle remains largely a mystery. To uncover which languages ChatGPT \u2018knows\u2019, we investigate its language identification (LID) abilities. For this purpose, we compile Babel-670, a benchmark comprising 670 languages representing 23 language families spoken in five continents. Languages in Babel-670 run the gamut from the very high-resource to the very low-resource. We then study ChatGPT\u2019s (both GPT-3.5 and GPT-4) ability to (i) identify language names and language codes (ii) under zero- and few-shot conditions (iii) with and without provision of a label set. When compared to smaller finetuned LID tools, we find that ChatGPT lags behind. For example, it has poor performance on African languages. We conclude that current large language models would benefit from further development before they can sufficiently serve diverse communities.",
        "author": "Wei-Rui Chen; Ife Adebara; Khai Doan; Qisheng Liao; Muhammad Abdul-Mageed",
        "authorids": "/w/wei-rui-chen/; /i/ife-adebara/; /k/khai-doan/; /q/qisheng-liao/; /m/muhammad-abdul-mageed/",
        "bibtex": "@inproceedings{chen-etal-2024-fumbling,\n    title = \"Fumbling in {B}abel: An Investigation into {C}hat{GPT}{'}s Language Identification Ability\",\n    author = \"Chen, Wei-Rui  and\n      Adebara, Ife  and\n      Doan, Khai  and\n      Liao, Qisheng  and\n      Abdul-Mageed, Muhammad\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.274/\",\n    doi = \"10.18653/v1/2024.findings-naacl.274\",\n    pages = \"4387--4413\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.274.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.274/",
        "pdf_size": 1686071,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8350280673371555799&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Deep Learning & Natural Language Processing Group, The University of British Columbia; Deep Learning & Natural Language Processing Group, The University of British Columbia; Department of Natural Language Processing & Department of Machine Learning, MBZUAI; Department of Natural Language Processing & Department of Machine Learning, MBZUAI; Deep Learning & Natural Language Processing Group, The University of British Columbia + Department of Natural Language Processing & Department of Machine Learning, MBZUAI + Invertible AI",
        "aff_domain": "ubc.ca;ubc.ca;mbzuai.ac.ae;mbzuai.ac.ae;ubc.ca",
        "email": "ubc.ca;ubc.ca;mbzuai.ac.ae;mbzuai.ac.ae;ubc.ca",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0+1+2",
        "aff_unique_norm": "University of British Columbia;Mohamed bin Zayed University of Artificial Intelligence;Invertible AI",
        "aff_unique_dep": "Department of Computer Science;Department of Natural Language Processing;",
        "aff_unique_url": "https://www.ubc.ca;https://www.mbzuai.ac.ae;https://www.invertible.ai",
        "aff_unique_abbr": "UBC;MBZUAI;Invertible AI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Vancouver;",
        "aff_country_unique_index": "0;0;1;1;0+1+2",
        "aff_country_unique": "Canada;United Arab Emirates;United States"
    },
    {
        "id": "2024.naacl-short.7",
        "title": "Fusion Makes Perfection: An Efficient Multi-Grained Matching Approach for Zero-Shot Relation Extraction",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Predicting unseen relations that cannot be observed during the training phase is a challenging task in relation extraction. Previous works have made progress by matching the semantics between input instances and label descriptions. However, fine-grained matching often requires laborious manual annotation, and rich interactions between instances and label descriptions come with significant computational overhead. In this work, we propose an efficient multi-grained matching approach that uses virtual entity matching to reduce manual annotation cost, and fuses coarse-grained recall and fine-grained classification for rich interactions with guaranteed inference speed.Experimental results show that our approach outperforms the previous State Of The Art (SOTA) methods, and achieves a balance between inference efficiency and prediction accuracy in zero-shot relation extraction tasks.Our code is available at https://github.com/longls777/EMMA.",
        "author": "Shilong Li; Ge Bai; Zhang Zhang; Ying Liu; Chenji Lu; Daichi Guo; Ruifang Liu; Sun Yong",
        "authorids": "/s/shilong-li/; /g/ge-bai/; /z/zhang-zhang/; /y/ying-liu/; /c/chenji-lu/; /d/daichi-guo/; /r/ruifang-liu/; /s/sun-yong/",
        "bibtex": "@inproceedings{li-etal-2024-fusion,\n    title = \"Fusion Makes Perfection: An Efficient Multi-Grained Matching Approach for Zero-Shot Relation Extraction\",\n    author = \"Li, Shilong  and\n      Bai, Ge  and\n      Zhang, Zhang  and\n      Liu, Ying  and\n      Lu, Chenji  and\n      Guo, Daichi  and\n      Liu, Ruifang  and\n      Yong, Sun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.7/\",\n    doi = \"10.18653/v1/2024.naacl-short.7\",\n    pages = \"79--85\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.7.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.7/",
        "pdf_size": 784347,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14243513251321777759&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China",
        "aff_domain": "bupt.edu.cn; ; ; ; ; ; ; ",
        "email": "bupt.edu.cn; ; ; ; ; ; ; ",
        "github": "https://github.com/longls777/EMMA",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.49",
        "title": "GEE! Grammar Error Explanation with Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Existing grammatical error correction tools do not provide natural language explanations of the errors that they correct in user-written text. However, such explanations are essential for helping users learn the language by gaining a deeper understanding of its grammatical rules (DeKeyser, 2003; Ellis et al., 2006).To address this gap, we propose the task of grammar error explanation, where a system needs to provide one-sentence explanations for each grammatical error in a pair of erroneous and corrected sentences. The task is not easily solved by prompting LLMs: we find that, using one-shot prompting, GPT-4 only explains 40.6% of the errors and does not even attempt to explain 39.8% of the errors.Since LLMs struggle to identify grammar errors, we develop a two-step pipeline that leverages fine-tuned and prompted large language models to perform structured atomic token edit extraction, followed by prompting GPT-4 to explain each edit. We evaluate our pipeline on German, Chinese, and English grammar error correction data. Our atomic edit extraction achieves an F1 of 0.93 on German, 0.91 on Chinese, and 0.891 on English. Human evaluation of generated explanations reveals that 93.9% of German errors, 96.4% of Chinese errors, and 92.20% of English errors are correctly detected and explained. To encourage further research, we open-source our data and code.",
        "author": "Yixiao Song; Kalpesh Krishna; Rajesh Bhatt; Kevin Gimpel; Mohit Iyyer",
        "authorids": "/y/yixiao-song/; /k/kalpesh-krishna/; /r/rajesh-bhatt/; /k/kevin-gimpel/; /m/mohit-iyyer/",
        "bibtex": "@inproceedings{song-etal-2024-gee,\n    title = \"{GEE}! Grammar Error Explanation with Large Language Models\",\n    author = \"Song, Yixiao  and\n      Krishna, Kalpesh  and\n      Bhatt, Rajesh  and\n      Gimpel, Kevin  and\n      Iyyer, Mohit\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.49/\",\n    doi = \"10.18653/v1/2024.findings-naacl.49\",\n    pages = \"754--781\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.49.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.49/",
        "pdf_size": 546019,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14291859829958449260&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Massachusetts Amherst\u2660; University of Massachusetts Amherst\u2660\u2020; University of Massachusetts Amherst\u2660; QuillBot\u2661; University of Massachusetts Amherst\u2660",
        "aff_domain": "umass.edu;gmail.com;umass.edu;quillbot.com;cs.umass.edu",
        "email": "umass.edu;gmail.com;umass.edu;quillbot.com;cs.umass.edu",
        "github": "https://github.com/Yixiao-Song/GEE-with-LLMs",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Massachusetts Amherst;QuillBot",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umass.edu;https://quillbot.com",
        "aff_unique_abbr": "UMass Amherst;QuillBot",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.342",
        "title": "GINopic: Topic Modeling with Graph Isomorphism Network",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Topic modeling is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling. However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling.",
        "author": "Suman Adhya; Debarshi Kumar Sanyal",
        "authorids": "/s/suman-adhya/; /d/debarshi-kumar-sanyal/",
        "bibtex": "@inproceedings{adhya-sanyal-2024-ginopic,\n    title = \"{GIN}opic: Topic Modeling with Graph Isomorphism Network\",\n    author = \"Adhya, Suman  and\n      Sanyal, Debarshi Kumar\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.342/\",\n    doi = \"10.18653/v1/2024.naacl-long.342\",\n    pages = \"6171--6183\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.342.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.342/",
        "pdf_size": 634163,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14794673209344851045&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Indian Association for the Cultivation of Science, Jadavpur, Kolkata-700032, India; Indian Association for the Cultivation of Science, Jadavpur, Kolkata-700032, India",
        "aff_domain": "gmail.com;iacs.res.in",
        "email": "gmail.com;iacs.res.in",
        "github": "https://github.com/AdhyaSuman/GINopic",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Association for the Cultivation of Science",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kolkata",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2024.naacl-long.300",
        "title": "GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Named Entity Recognition (NER) is essential in various Natural Language Processing (NLP) applications. Traditional NER models are effective but limited to a set of predefined entity types. In contrast, Large Language Models (LLMs) can extract arbitrary entities through natural language instructions, offering greater flexibility. However, their size and cost, particularly for those accessed via APIs like ChatGPT, make them impractical in resource-limited scenarios. In this paper, we introduce a compact NER model trained to identify any type of entity. Leveraging a bidirectional transformer encoder, our model, GLiNER, facilitates parallel entity extraction, an advantage over the slow sequential token generation of LLMs. Through comprehensive testing, GLiNER demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs in zero-shot evaluations on various NER benchmarks.",
        "author": "Urchade Zaratiana; Nadi Tomeh; Pierre Holat; Thierry Charnois",
        "authorids": "/u/urchade-zaratiana/; /n/nadi-tomeh/; /p/pierre-holat/; /t/thierry-charnois/",
        "bibtex": "@inproceedings{zaratiana-etal-2024-gliner,\n    title = \"{GL}i{NER}: Generalist Model for Named Entity Recognition using Bidirectional Transformer\",\n    author = \"Zaratiana, Urchade  and\n      Tomeh, Nadi  and\n      Holat, Pierre  and\n      Charnois, Thierry\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.300/\",\n    doi = \"10.18653/v1/2024.naacl-long.300\",\n    pages = \"5364--5376\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.300.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.300/",
        "pdf_size": 1294606,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13816012318726191706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "FI Group+LIPN, CNRS UMR 7030, France; LIPN, CNRS UMR 7030, France; FI Group+LIPN, CNRS UMR 7030, France; LIPN, CNRS UMR 7030, France",
        "aff_domain": "lipn.fr; ; ; ",
        "email": "lipn.fr; ; ; ",
        "github": "https://github.com/urchade/GLiNER",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0+1;1",
        "aff_unique_norm": "FI Group;CNRS UMR 7030",
        "aff_unique_dep": ";LIPN",
        "aff_unique_url": ";https://www.lipn.fr",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1;1",
        "aff_country_unique": ";France"
    },
    {
        "id": "2024.findings-naacl.272",
        "title": "GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Knowledge distillation from LLMs is essential for the efficient deployment of language models. Prior works have proposed data generation using LLMs for preparing distilled models. We argue that generating data with LLMs is prone to sampling mainly from the center of original content distribution. This limitation hinders the distilled model from learning the true underlying data distribution and to forget the tails of the distributions (samples with lower probability). To this end, we propose GOLD, a task-agnostic data generation and knowledge distillation framework, which employs an iterative out-of-distribution-guided feedback mechanism for the LLM. As a result, the generated data improves the generalizability of distilled models. An energy-based OOD evaluation approach is also introduced to deal with noisy generated data. Our extensive experiments on 10 different classification and sequence-to-sequence tasks in NLP show that GOLD respectively outperforms prior arts and the LLM with an average improvement of 5% and 14%. We will also show that the proposed method is applicable to less explored and novel tasks. Code is available in the Appendix.",
        "author": "Mohsen Gholami; Mohammad Akbari; Tianxi Hu; Vaden Masrani; Z. Wang; Yong Zhang",
        "authorids": "/m/mohsen-gholami/; /m/mohammad-akbari/; /t/tianxi-hu/; /v/vaden-masrani/; /z/z-wang/; /y/yong-zhang/",
        "bibtex": "@inproceedings{gholami-etal-2024-gold,\n    title = \"{GOLD}: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation\",\n    author = \"Gholami, Mohsen  and\n      Akbari, Mohammad  and\n      Hu, Tianxi  and\n      Masrani, Vaden  and\n      Wang, Z.  and\n      Zhang, Yong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.272/\",\n    doi = \"10.18653/v1/2024.findings-naacl.272\",\n    pages = \"4365--4380\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.272.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.272/",
        "pdf_size": 675286,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7103690814408079145&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Huawei Technologies Canada Co. Ltd. + University of British Columbia; Huawei Technologies Canada Co. Ltd.; Huawei Technologies Canada Co. Ltd.; Huawei Technologies Canada Co. Ltd.; University of British Columbia; Huawei Technologies Canada Co. Ltd.",
        "aff_domain": "huawei.com;huawei.com;huawei.com;huawei.com;ubc.ca;huawei.com",
        "email": "huawei.com;huawei.com;huawei.com;huawei.com;ubc.ca;huawei.com",
        "github": "",
        "project": "https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=9d770d1f-3758-4d0f-99d4-3346abbe1546",
        "author_num": 6,
        "aff_unique_index": "0+1;0;0;0;1;0",
        "aff_unique_norm": "Huawei;University of British Columbia",
        "aff_unique_dep": "Huawei Technologies;",
        "aff_unique_url": "https://www.huawei.com/ca-en/;https://www.ubc.ca",
        "aff_unique_abbr": "Huawei;UBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.findings-naacl.19",
        "title": "GOLD: Geometry Problem Solver with Natural Language Description",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Addressing the challenge of automated geometry math problem-solving in artificial intelligence (AI) involves understanding multi-modal information and mathematics. blackCurrent methods struggle with accurately interpreting geometry diagrams, which hinders effective problem-solving. To tackle this issue, we present the Geometry problem sOlver with natural Language Description (GOLD) model. GOLD enhances the extraction of geometric relations by separately processing symbols and geometric primitives within the diagram. Subsequently, it converts the extracted relations into natural language descriptions, efficiently utilizing large language models to solve geometry math problems. Experiments show that the GOLD model outperforms the Geoformer model, the previous best method on the UniGeo dataset, by achieving accuracy improvements of 12.7% and 42.1% in calculation and proving subsets. Additionally, it surpasses the former best model on the PGPS9K and Geometry3K datasets, PGPSNet, by obtaining accuracy enhancements of 1.8% and 3.2%, respectively.",
        "author": "Jiaxin Zhang; Yashar Moshfeghi",
        "authorids": "/j/jiaxin-zhang/; /y/yashar-moshfeghi/",
        "bibtex": "@inproceedings{zhang-moshfeghi-2024-gold,\n    title = \"{GOLD}: Geometry Problem Solver with Natural Language Description\",\n    author = \"Zhang, Jiaxin  and\n      Moshfeghi, Yashar\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.19/\",\n    doi = \"10.18653/v1/2024.findings-naacl.19\",\n    pages = \"263--278\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.19.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.19/",
        "pdf_size": 1281922,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14016051364951333885&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Strathclyde; University of Strathclyde",
        "aff_domain": "strath.ac.uk;strath.ac.uk",
        "email": "strath.ac.uk;strath.ac.uk",
        "github": "https://github.com/NeuraSearch/Geometry-Diagram-Description",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Strathclyde",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.strath.ac.uk",
        "aff_unique_abbr": "Strathclyde",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.findings-naacl.87",
        "title": "GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI\u2019s legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI\u2019s earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM\u2019s reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc. Our analysis sheds light on many of these questions, aiming to improve the transparency of advanced LLMs.",
        "author": "Shen Zheng; Yuyu Zhang; Yijie Zhu; Chenguang Xi; Pengyang Gao; Zhou Xun; Kevin Chang",
        "authorids": "/s/shen-zheng/; /y/yuyu-zhang/; /y/yijie-zhu/; /c/chenguang-xi/; /p/pengyang-gao/; /z/zhou-xun/; /k/kevin-chang/",
        "bibtex": "@inproceedings{zheng-etal-2024-gpt,\n    title = \"{GPT}-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards {GPT}-4 and Beyond\",\n    author = \"Zheng, Shen  and\n      Zhang, Yuyu  and\n      Zhu, Yijie  and\n      Xi, Chenguang  and\n      Gao, Pengyang  and\n      Xun, Zhou  and\n      Chang, Kevin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.87/\",\n    doi = \"10.18653/v1/2024.findings-naacl.87\",\n    pages = \"1363--1382\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.87.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.87/",
        "pdf_size": 544273,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11825396027393223998&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Illinois at Urbana-Champaign+ByteDance; ByteDance; ByteDance; ByteDance; ByteDance; ByteDance; University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;bytedance.com; ; ; ; ; ",
        "email": "illinois.edu;bytedance.com; ; ; ; ; ",
        "github": "https://github.com/GPT-Fathom/GPT-Fathom",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;1;1;1;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;ByteDance",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.bytedance.com",
        "aff_unique_abbr": "UIUC;ByteDance",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0+1;1;1;1;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.findings-naacl.8",
        "title": "GPT-who: An Information Density-based Machine-Generated Text Detector",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The Uniform Information Density (UID) principle posits that humans prefer to spread information evenly during language production. We examine if this UID principle can help capture differences between Large Language Models (LLMs)-generated and human-generated texts. We propose GPT-who, the first psycholinguistically-inspired domain-agnostic statistical detector. This detector employs UID-based featuresto model the unique statistical signature of each LLM and human author for accurate detection. We evaluate our method using 4 large-scale benchmark datasets and find that GPT-who outperforms state-of-the-art detectors (both statistical- & non-statistical) such as GLTR, GPTZero, DetectGPT, OpenAI detector, and ZeroGPT by over 20% across domains.In addition to better performance, it is computationally inexpensive and utilizes an interpretable representation of text articles. We find that GPT-who can distinguish texts generated by very sophisticated LLMs, even when the overlying text is indiscernible.UID-based measures for all datasets and code are available at https://github.com/saranya-venkatraman/gpt-who.",
        "author": "Saranya Venkatraman; Adaku Uchendu; Dongwon Lee",
        "authorids": "/s/saranya-venkatraman/; /a/adaku-uchendu/; /d/dongwon-lee/",
        "bibtex": "@inproceedings{venkatraman-etal-2024-gpt,\n    title = \"{GPT}-who: An Information Density-based Machine-Generated Text Detector\",\n    author = \"Venkatraman, Saranya  and\n      Uchendu, Adaku  and\n      Lee, Dongwon\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.8/\",\n    doi = \"10.18653/v1/2024.findings-naacl.8\",\n    pages = \"103--115\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.8.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.8/",
        "pdf_size": 917827,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16925743365092412306&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The Pennsylvania State University; MIT Lincoln Laboratory; The Pennsylvania State University",
        "aff_domain": "psu.edu;ll.mit.edu;psu.edu",
        "email": "psu.edu;ll.mit.edu;psu.edu",
        "github": "https://github.com/saranya-venkatraman/gpt-who",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Pennsylvania State University;Massachusetts Institute of Technology Lincoln Laboratory",
        "aff_unique_dep": ";Lincoln Laboratory",
        "aff_unique_url": "https://www.psu.edu;https://www.ll.mit.edu",
        "aff_unique_abbr": "PSU;MIT LL",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lexington",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.365",
        "title": "GPTScore: Evaluate as You Desire",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models.Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently.This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., in-context learning, zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., Flan-T5-small) to 175B (e.g., GPT3).Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions.This nature helps us overcome several long-standing challenges in text evaluation\u2013how to achieve customized, multi-faceted evaluation without model training. We make our code publicly available.",
        "author": "Jinlan Fu; See-Kiong Ng; Zhengbao Jiang; Pengfei Liu",
        "authorids": "/j/jinlan-fu/; /s/see-kiong-ng/; /z/zhengbao-jiang/; /p/pengfei-liu/",
        "bibtex": "@inproceedings{fu-etal-2024-gptscore,\n    title = \"{GPTS}core: Evaluate as You Desire\",\n    author = \"Fu, Jinlan  and\n      Ng, See-Kiong  and\n      Jiang, Zhengbao  and\n      Liu, Pengfei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.365/\",\n    doi = \"10.18653/v1/2024.naacl-long.365\",\n    pages = \"6556--6576\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.365.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.365/",
        "pdf_size": 1612601,
        "gs_citation": 566,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12330771780104870711&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/jinlanfu/GPTScore",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-long.190",
        "title": "GRASP: A Disagreement Analysis Framework to Assess Group Associations in Perspectives",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Human annotation plays a core role in machine learning \u2014 annotations for supervised models, safety guardrails for generative models, and human feedback for reinforcement learning, to cite a few avenues. However, the fact that many of these human annotations are inherently subjective is often overlooked. Recent work has demonstrated that ignoring rater subjectivity (typically resulting in rater disagreement) is problematic within specific tasks and for specific subgroups. Generalizable methods to harness rater disagreement and thus understand the socio-cultural leanings of subjective tasks remain elusive. In this paper, we propose GRASP, a comprehensive disagreement analysis framework to measure group association in perspectives among different rater subgroups, and demonstrate its utility in assessing the extent of systematic disagreements in two datasets: (1) safety annotations of human-chatbot conversations, and (2) offensiveness annotations of social media posts, both annotated by diverse rater pools across different socio-demographic axes. Our framework (based on disagreement metrics) reveals specific rater groups that have significantly different perspectives than others on certain tasks, and helps identify demographic axes that are crucial to consider in specific task contexts.",
        "author": "Vinodkumar Prabhakaran; Christopher Homan; Lora Aroyo; Aida Mostafazadeh Davani; Alicia Parrish; Alex Taylor; Mark Diaz; Ding Wang; Gregory Serapio-Garc\u00eda",
        "authorids": "/v/vinodkumar-prabhakaran/; /c/christopher-homan/; /l/lora-aroyo/; /a/aida-mostafazadeh-davani/; /a/alicia-parrish/; /a/alex-taylor/; /m/mark-diaz/; /d/ding-wang/; /g/gregory-serapio-garcia/",
        "bibtex": "@inproceedings{prabhakaran-etal-2024-grasp,\n    title = \"{GRASP}: A Disagreement Analysis Framework to Assess Group Associations in Perspectives\",\n    author = \"Prabhakaran, Vinodkumar  and\n      Homan, Christopher  and\n      Aroyo, Lora  and\n      Mostafazadeh Davani, Aida  and\n      Parrish, Alicia  and\n      Taylor, Alex  and\n      Diaz, Mark  and\n      Wang, Ding  and\n      Serapio-Garc{\\'i}a, Gregory\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.190/\",\n    doi = \"10.18653/v1/2024.naacl-long.190\",\n    pages = \"3473--3492\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.190.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.190/",
        "pdf_size": 710175,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4573383116519615245&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; University of Cambridge",
        "aff_domain": "google.com;google.com;gmail.com;google.com;google.com;google.com;google.com;google.com;cam.ac.uk",
        "email": "google.com;google.com;gmail.com;google.com;google.com;google.com;google.com;google.com;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;1",
        "aff_unique_norm": "Google;University of Cambridge",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://www.cam.ac.uk",
        "aff_unique_abbr": "Google Research;Cambridge",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;1",
        "aff_campus_unique": "Mountain View;Cambridge",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2024.findings-naacl.99",
        "title": "Gaussian Process Optimization for Adaptable Multi-Objective Text Generation using Linearly-Weighted Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In multi-objective text generation, we aim to optimize over multiple weighted aspects (e.g., toxicity, semantic preservation, fluency) of the generated text. However, multi-objective weighting schemes may change dynamically in practice according to deployment requirements, evolving business needs, personalization requirements on edge devices, or the availability of new language models and/or objective requirements. Ideally, we need an efficient method to adapt to the dynamic requirements of the overall objective. To address these requirements, we propose a linear combination of objective-specific language models to efficiently adapt the decoding process and optimize for the desired objective without the significant computational overhead of retraining one or more language models. We show empirically that we can leverage Gaussian Process black box optimization to adapt the language model decoder weights to outperform other fixed weighting schemes and standard baselines of the task in only a few iterations of decoding. Overall this approach enables highly efficient adaptation of controllable language models via multi-objective weighting schemes that may evolve dynamically in practical deployment situations.",
        "author": "Mohammad Mahdi Abdollah Pour; Ali Pesaranghader; Eldan Cohen; Scott Sanner",
        "authorids": "/m/mohammad-mahdi-abdollah-pour/; /a/ali-pesaranghader/; /e/eldan-cohen/; /s/scott-sanner/",
        "bibtex": "@inproceedings{abdollah-pour-etal-2024-gaussian,\n    title = \"{G}aussian Process Optimization for Adaptable Multi-Objective Text Generation using Linearly-Weighted Language Models\",\n    author = \"Abdollah Pour, Mohammad Mahdi  and\n      Pesaranghader, Ali  and\n      Cohen, Eldan  and\n      Sanner, Scott\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.99/\",\n    doi = \"10.18653/v1/2024.findings-naacl.99\",\n    pages = \"1529--1536\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.99.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.99/",
        "pdf_size": 381019,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1407011988663053533&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Toronto, Canada+LG Electronics, Toronto AI Lab; LG Electronics, Toronto AI Lab; University of Toronto, Canada; University of Toronto, Canada",
        "aff_domain": "mail.utoronto.ca;lge.com;mie.utoronto.ca;mie.utoronto.ca",
        "email": "mail.utoronto.ca;lge.com;mie.utoronto.ca;mie.utoronto.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0;0",
        "aff_unique_norm": "University of Toronto;LG",
        "aff_unique_dep": ";Toronto AI Lab",
        "aff_unique_url": "https://www.utoronto.ca;https://www.lg.com/ca",
        "aff_unique_abbr": "U of T;LG",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Toronto",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-short.22",
        "title": "GenDecider: Integrating \u201cNone of the Candidates\u201d Judgments in Zero-Shot Entity Linking Re-ranking",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We introduce GenDecider, a novel re-ranking approach for Zero-Shot Entity Linking (ZSEL), built on the Llama model. It innovatively detects scenarios where the correct entity is not among the retrieved candidates, a common oversight in existing re-ranking methods. By autoregressively generating outputs based on the context of the entity mention and the candidate entities, GenDecider significantly enhances disambiguation, improving the accuracy and reliability of ZSEL systems, as demonstrated on the benchmark ZESHEL dataset. Our code is available at https://github.com/kangISU/GenDecider.",
        "author": "Kang Zhou; Yuepei Li; Qing Wang; Qiao Qiao; Qi Li",
        "authorids": "/k/kang-zhou/; /y/yuepei-li/; /q/qing-wang/; /q/qiao-qiao/; /q/qi-li/",
        "bibtex": "@inproceedings{zhou-etal-2024-gendecider,\n    title = \"{G}en{D}ecider: Integrating {\\textquotedblleft}None of the Candidates{\\textquotedblright} Judgments in Zero-Shot Entity Linking Re-ranking\",\n    author = \"Zhou, Kang  and\n      Li, Yuepei  and\n      Wang, Qing  and\n      Qiao, Qiao  and\n      Li, Qi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.22/\",\n    doi = \"10.18653/v1/2024.naacl-short.22\",\n    pages = \"239--245\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.22.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.22/",
        "pdf_size": 297848,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15279989038718329540&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Iowa State University, Ames, Iowa, USA; Department of Computer Science, Iowa State University, Ames, Iowa, USA; Department of Computer Science, Iowa State University, Ames, Iowa, USA; Department of Computer Science, Iowa State University, Ames, Iowa, USA; Department of Computer Science, Iowa State University, Ames, Iowa, USA",
        "aff_domain": "iastate.edu;iastate.edu;iastate.edu;iastate.edu;iastate.edu",
        "email": "iastate.edu;iastate.edu;iastate.edu;iastate.edu;iastate.edu",
        "github": "https://github.com/kangISU/GenDecider",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Iowa State University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.iastate.edu",
        "aff_unique_abbr": "ISU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Ames",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.155",
        "title": "GenRES: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The field of relation extraction (RE) is experiencing a notable shift towards generative relation extraction (GRE), leveraging the capabilities of large language models (LLMs). However, we discovered that traditional relation extraction (RE) metrics like precision and recall fall short in evaluating GRE methods. This shortfall arises because these metrics rely on exact matching with human-annotated reference relations, while GRE methods often produce diverse and semantically accurate relations that differ from the references. To fill this gap, we introduce GenRES for a multi-dimensional assessment in terms of the topic similarity, uniqueness, granularity, factualness, and completeness of the GRE results. With GenRES, we empirically identified that (1) precision/recall fails to justify the performance of GRE methods; (2) human-annotated referential relations can be incomplete; (3) prompting LLMs with a fixed set of relations or entities can cause hallucinations. Next, we conducted a human evaluation of GRE methods that shows GenRES is consistent with human preferences for RE quality. Last, we made a comprehensive evaluation of fourteen leading LLMs using GenRES across document, bag, and sentence level RE datasets, respectively, to set the benchmark for future research in GRE",
        "author": "Pengcheng Jiang; Jiacheng Lin; Zifeng Wang; Jimeng Sun; Jiawei Han",
        "authorids": "/p/pengcheng-jiang/; /j/jiacheng-lin/; /z/zifeng-wang/; /j/jimeng-sun/; /j/jiawei-han/",
        "bibtex": "@inproceedings{jiang-etal-2024-genres,\n    title = \"{G}en{RES}: Rethinking Evaluation for Generative Relation Extraction in the Era of Large Language Models\",\n    author = \"Jiang, Pengcheng  and\n      Lin, Jiacheng  and\n      Wang, Zifeng  and\n      Sun, Jimeng  and\n      Han, Jiawei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.155/\",\n    doi = \"10.18653/v1/2024.naacl-long.155\",\n    pages = \"2820--2837\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.155.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.155/",
        "pdf_size": 1306283,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18270032123223516510&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "github": "https://github.com/pat-jj/GenRES",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.268",
        "title": "GenTKG: Generative Forecasting on Temporal Knowledge Graph with Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional embedding-based and rule-based methods dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval-augmented generation framework named GenTKG combining a temporal logical rule-based retrieval strategy and few-shot parameter-efficient instruction tuning to solve the above challenges, respectively. Extensive experiments have shown that GenTKG outperforms conventional methods of temporal relational forecasting with low computation resources using extremely limited training data as few as 16 samples. GenTKG also highlights remarkable cross-domain generalizability with outperforming performance on unseen datasets without re-training, and in-domain generalizability regardless of time split in the same dataset. Our work reveals the huge potential of LLMs in the tKG domain and opens a new frontier for generative forecasting on tKGs. The code and data are released here: https://github.com/mayhugotong/GenTKG.",
        "author": "Ruotong Liao; Xu Jia; Yangzhe Li; Yunpu Ma; Volker Tresp",
        "authorids": "/r/ruotong-liao/; /x/xu-jia/; /y/yangzhe-li/; /y/yunpu-ma/; /v/volker-tresp/",
        "bibtex": "@inproceedings{liao-etal-2024-gentkg,\n    title = \"{G}en{TKG}: Generative Forecasting on Temporal Knowledge Graph with Large Language Models\",\n    author = \"Liao, Ruotong  and\n      Jia, Xu  and\n      Li, Yangzhe  and\n      Ma, Yunpu  and\n      Tresp, Volker\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.268/\",\n    doi = \"10.18653/v1/2024.findings-naacl.268\",\n    pages = \"4303--4317\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.268.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.268/",
        "pdf_size": 1615673,
        "gs_citation": 24,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9381164023257269063&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "LMU Munich+Munich Center for Machine Learning (MCML); Technical University of Munich; Technical University of Munich; LMU Munich+Munich Center for Machine Learning (MCML)+Siemens AG; LMU Munich+Munich Center for Machine Learning (MCML)",
        "aff_domain": "outlook.com; ; ;gmail.com;lmu.de",
        "email": "outlook.com; ; ;gmail.com;lmu.de",
        "github": "https://github.com/mayhugotong/GenTKG",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;2;0+1+3;0+1",
        "aff_unique_norm": "Ludwig Maximilian University of Munich;Munich Center for Machine Learning;Technical University of Munich;Siemens AG",
        "aff_unique_dep": ";Center for Machine Learning;;",
        "aff_unique_url": "https://www.lmu.de;https://www.munich-center-for-machine-learning.de;https://www.tum.de;https://www.siemens.com",
        "aff_unique_abbr": "LMU;MCML;TUM;Siemens",
        "aff_campus_unique_index": "0+0;0+0;0+0",
        "aff_campus_unique": "Munich;",
        "aff_country_unique_index": "0+0;0;0;0+0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.400",
        "title": "Generalizable Multilingual Hate Speech Detection on Low Resource Indian Languages using Fair Selection in Federated Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Social media, originally meant for peaceful communication, now faces issues with hate speech. Detecting hate speech from social media in Indian languages with linguistic diversity and cultural nuances presents a complex and challenging task. Furthermore, traditional methods involve sharing of users\u2019 sensitive data with a server for model training making it undesirable and involving potential risk to their privacy remained under-studied. In this paper, we combined various low-resource language datasets and propose MultiFED, a federated approach that performs effectively to detect hate speech. MultiFED utilizes continuous adaptation and fine-tuning to aid generalization using subsets of multilingual data overcoming the limitations of data scarcity. Extensive experiments are conducted on 13 Indic datasets across five different pre-trained models. The results show that MultiFED outperforms the state-of-the-art baselines by 8% (approx.) in terms of Accuracy and by 12% (approx.) in terms of F-Score.",
        "author": "Akshay Singh; Rahul Thakur",
        "authorids": "/a/akshay-singh/; /r/rahul-thakur/",
        "bibtex": "@inproceedings{singh-thakur-2024-generalizable,\n    title = \"Generalizable Multilingual Hate Speech Detection on Low Resource {I}ndian Languages using Fair Selection in Federated Learning\",\n    author = \"Singh, Akshay  and\n      Thakur, Rahul\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.400/\",\n    doi = \"10.18653/v1/2024.naacl-long.400\",\n    pages = \"7211--7221\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.400.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.400/",
        "pdf_size": 876653,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2603634224380271960&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Computer Science & Engineering, Indian Institute of Technology Roorkee; Computer Science & Engineering, Indian Institute of Technology Roorkee",
        "aff_domain": "cs.iitr.ac.in;ieee.org",
        "email": "cs.iitr.ac.in;ieee.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Indian Institute of Technology Roorkee",
        "aff_unique_dep": "Computer Science & Engineering",
        "aff_unique_url": "https://www.iitr.ac.in",
        "aff_unique_abbr": "IIT Roorkee",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Roorkee",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2024.naacl-long.238",
        "title": "Generalizable Sarcasm Detection is Just Around the Corner, of Course!",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We tested the robustness of sarcasm detection models by examining their behavior when fine-tuned on four sarcasm datasets containing varying characteristics of sarcasm: label source (authors vs. third-party), domain (social media/online vs. offline conversations/dialogues), style (aggressive vs. humorous mocking). We tested their prediction performance on the same dataset (intra-dataset) and across different datasets (cross-dataset). For intra-dataset predictions, models consistently performed better when fine-tuned with third-party labels rather than with author labels. For cross-dataset predictions, most models failed to generalize well to the other datasets, implying that one type of dataset cannot represent all sorts of sarcasm with different styles and domains. Compared to the existing datasets, models fine-tuned on the new dataset we release in this work showed the highest generalizability to other datasets. With a manual inspection of the datasets and post-hoc analysis, we attributed the difficulty in generalization to the fact that sarcasm actually comes in different domains and styles. We argue that future sarcasm research should take the broad scope of sarcasm into account.",
        "author": "Hyewon Jang; Diego Frassinelli",
        "authorids": "/h/hyewon-jang/; /d/diego-frassinelli/",
        "bibtex": "@inproceedings{jang-frassinelli-2024-generalizable,\n    title = \"Generalizable Sarcasm Detection is Just Around the Corner, of Course!\",\n    author = \"Jang, Hyewon  and\n      Frassinelli, Diego\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.238/\",\n    doi = \"10.18653/v1/2024.naacl-long.238\",\n    pages = \"4238--4249\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.238.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.238/",
        "pdf_size": 236743,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7233963936812151282&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Linguistics, University of Konstanz, Germany + Center for Information and Language Processing, LMU Munich, Germany; Department of Linguistics, University of Konstanz, Germany + Center for Information and Language Processing, LMU Munich, Germany",
        "aff_domain": "uni-konstanz.de;uni-konstanz.de",
        "email": "uni-konstanz.de;uni-konstanz.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Konstanz;LMU Munich",
        "aff_unique_dep": "Department of Linguistics;Center for Information and Language Processing",
        "aff_unique_url": "https://www.uni-konstanz.de;https://www.lmu.de",
        "aff_unique_abbr": ";LMU",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Munich",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.277",
        "title": "Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights. However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs. Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving generalization and combating overfitting. We validate the efficacy of our proposed method through extensive experiments, demonstrating its superiority over previous methods, particularly in the context of finetuning PLMs on low-resource datasets. Our code is available at https://github.com/Sai-Ashish/Attention_guided_weight_mixup_BLO.",
        "author": "Sai Ashish Somayajula; Youwei Liang; Li Zhang; Abhishek Singh; Pengtao Xie",
        "authorids": "/s/sai-ashish-somayajula/; /y/youwei-liang/; /l/li-zhang-ucsandiego/; /a/abhishek-singh/; /p/pengtao-xie/",
        "bibtex": "@inproceedings{somayajula-etal-2024-generalizable,\n    title = \"Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts\",\n    author = \"Somayajula, Sai Ashish  and\n      Liang, Youwei  and\n      Zhang, Li  and\n      Singh, Abhishek  and\n      Xie, Pengtao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.277/\",\n    doi = \"10.18653/v1/2024.naacl-long.277\",\n    pages = \"4936--4953\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.277.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.277/",
        "pdf_size": 438742,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=435810036271167874&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "UC San Diego, USA; UC San Diego, USA; UC San Diego, USA; UC San Diego, USA; UC San Diego, USA",
        "aff_domain": "ucsd.edu; ; ; ;ucsd.edu",
        "email": "ucsd.edu; ; ; ;ucsd.edu",
        "github": "https://github.com/Sai-Ashish/Attention_guided_weight_mixup_BLO",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.259",
        "title": "Generating Attractive and Authentic Copywriting from Customer Reviews",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The goal of product copywriting is to capture the interest of potential buyers by emphasizing the features of products through text descriptions. As e-commerce platforms offer a wide range of services, it\u2019s becoming essential to dynamically adjust the styles of these auto-generated descriptions. Typical approaches to copywriting generation often rely solely on specified product attributes, which may result in dull and repetitive content. To tackle this issue, we propose to generate copywriting based on customer reviews, as they provide firsthand practical experiences with products, offering a richer source of information than just product attributes. We have developed a sequence-to-sequence framework, enhanced with reinforcement learning, to produce copywriting that is attractive, authentic, and rich in information. Our framework outperforms all existing baseline and zero-shot large language models, including LLaMA-2-chat-7B and GPT-3.5, in terms of both attractiveness and faithfulness. Furthermore, this work features the use of LLMs for aspect-based summaries collection and argument allure assessment. Experiments demonstrate the effectiveness of using LLMs for marketing domain corpus construction. The code and the dataset is publicly available at: https://github.com/YuXiangLin1234/Copywriting-Generation.",
        "author": "Yu-Xiang Lin; Wei-Yun Ma",
        "authorids": "/y/yu-xiang-lin/; /w/wei-yun-ma/",
        "bibtex": "@inproceedings{lin-ma-2024-generating,\n    title = \"Generating Attractive and Authentic Copywriting from Customer Reviews\",\n    author = \"Lin, Yu-Xiang  and\n      Ma, Wei-Yun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.259/\",\n    doi = \"10.18653/v1/2024.naacl-long.259\",\n    pages = \"4629--4642\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.259.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.259/",
        "pdf_size": 452294,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15820804856781396272&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Academia Sinica; Academia Sinica",
        "aff_domain": "iis.sinica.edu.tw;iis.sinica.edu.tw",
        "email": "iis.sinica.edu.tw;iis.sinica.edu.tw",
        "github": "https://github.com/YuXiangLin1234/Copywriting-Generation",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Academia Sinica",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.sinica.edu.tw",
        "aff_unique_abbr": "Academia Sinica",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.285",
        "title": "Generating Mental Health Transcripts with SAPE (Spanish Adaptive Prompt Engineering)",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models have become valuable tools for data augmentation in scenarios with limited data availability, as they can generate synthetic data resembling real-world data. However, their generative performance depends on the quality of the prompt used to instruct the model. Prompt engineering that relies on hand-crafted strategies or requires domain experts to adjust the prompt often yields suboptimal results. In this paper we present SAPE, a Spanish Adaptive Prompt Engineering method utilizing genetic algorithms for prompt generation and selection. Our evaluation of SAPE focuses on a generative task that involves the creation of Spanish therapy transcripts, a type of data that is challenging to collect due to the fact that it typically includes protected health information. Through human evaluations conducted by mental health professionals, our results show that SAPE produces Spanish counselling transcripts that more closely resemble authentic therapy transcripts compared to other prompt engineering techniques that are based on Reflexion and Chain-of-Thought.",
        "author": "Daniel Lozoya; Alejandro Berazaluce; Juan Perches; Eloy L\u00faa; Mike Conway; Simon D\u2019Alfonso",
        "authorids": "/d/daniel-lozoya/; /a/alejandro-berazaluce/; /j/juan-perches/; /e/eloy-lua/; /m/mike-conway/; /s/simon-dalfonso/",
        "bibtex": "@inproceedings{lozoya-etal-2024-generating,\n    title = \"Generating Mental Health Transcripts with {SAPE} ({S}panish Adaptive Prompt Engineering)\",\n    author = \"Lozoya, Daniel  and\n      Berazaluce, Alejandro  and\n      Perches, Juan  and\n      L{\\'u}a, Eloy  and\n      Conway, Mike  and\n      D{'}Alfonso, Simon\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.285/\",\n    doi = \"10.18653/v1/2024.naacl-long.285\",\n    pages = \"5096--5113\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.285.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.285/",
        "pdf_size": 815014,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11417719158946397585&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "The University of Melbourne, Australia; The University of Melbourne, Australia; ITESM, Mexico; ITESM, Mexico; The University of Melbourne, Australia; The University of Melbourne, Australia",
        "aff_domain": "student.unimelb.edu.au;student.unimelb.edu.au;exatec.tec.mx;exatec.tec.mx;unimelb.edu.au;unimelb.edu.au",
        "email": "student.unimelb.edu.au;student.unimelb.edu.au;exatec.tec.mx;exatec.tec.mx;unimelb.edu.au;unimelb.edu.au",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;1;0;0",
        "aff_unique_norm": "University of Melbourne;Instituto Tecnol\u00f3gico y de Estudios Superiores de Monterrey",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.itesm.mx",
        "aff_unique_abbr": "UniMelb;ITESM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1;0;0",
        "aff_country_unique": "Australia;Mexico"
    },
    {
        "id": "2024.naacl-industry.13",
        "title": "Generating Signed Language Instructions in Large-Scale Dialogue Systems",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "We introduce a goal-oriented conversational AI system enhanced with American Sign Language (ASL) instructions, presenting the first implementation of such a system on a worldwide multimodal conversational AI platform. Accessible through a touch-based interface, our system receives input from users and seamlessly generates ASL instructions by leveraging retrieval methods and cognitively based gloss translations. Central to our design is a sign translation module powered by Large Language Models, alongside a token-based video retrieval system for delivering instructional content from recipes and wikiHow guides. Our development process is deeply rooted in a commitment to community engagement, incorporating insights from the Deaf and Hard-of-Hearing community, as well as experts in cognitive and ASL learning sciences. The effectiveness of our signing instructions is validated by user feedback, achieving ratings on par with those of the system in its non-signing variant. Additionally, our system demonstrates exceptional performance in retrieval accuracy and text-generation quality, measured by metrics such as BERTScore. We have made our codebase and datasets publicly accessible at https://github.com/Merterm/signed-dialogue, and a demo of our signed instruction video retrieval system is available at https://huggingface.co/spaces/merterm/signed-instructions.",
        "author": "Mert Inan; Katherine Atwell; Anthony Sicilia; Lorna Quandt; Malihe Alikhani",
        "authorids": "/m/mert-inan/; /k/katherine-atwell/; /a/anthony-sicilia/; /l/lorna-quandt/; /m/malihe-alikhani/",
        "bibtex": "@inproceedings{inan-etal-2024-generating,\n    title = \"Generating Signed Language Instructions in Large-Scale Dialogue Systems\",\n    author = \"Inan, Mert  and\n      Atwell, Katherine  and\n      Sicilia, Anthony  and\n      Quandt, Lorna  and\n      Alikhani, Malihe\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.13/\",\n    doi = \"10.18653/v1/2024.naacl-industry.13\",\n    pages = \"140--154\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.13.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.13/",
        "pdf_size": 8177180,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5142303602349169992&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Khoury College of Computer Science, Northeastern University, Boston, MA, USA; Khoury College of Computer Science, Northeastern University, Boston, MA, USA; Khoury College of Computer Science, Northeastern University, Boston, MA, USA; Educational Neuroscience Program, Gallaudet University, Washington, D.C., USA; Khoury College of Computer Science, Northeastern University, Boston, MA, USA",
        "aff_domain": "northeastern.edu;northeastern.edu;northeastern.edu;gallaudet.edu;northeastern.edu",
        "email": "northeastern.edu;northeastern.edu;northeastern.edu;gallaudet.edu;northeastern.edu",
        "github": "https://github.com/Merterm/signed-dialogue",
        "project": "https://huggingface.co/spaces/merterm/signed-instructions",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Northeastern University;Gallaudet University",
        "aff_unique_dep": "Khoury College of Computer Science;Educational Neuroscience Program",
        "aff_unique_url": "https://www.northeastern.edu;https://www.gallaudet.edu",
        "aff_unique_abbr": "NU;Gallaudet",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Boston;Washington, D.C.",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.312",
        "title": "Generating Uncontextualized and Contextualized Questions for Document-Level Event Argument Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper presents multiple question generation strategies for document-level event argument extraction. These strategies do not require human involvement and result in uncontextualized questions as well as contextualized questions grounded on the event and document of interest. Experimental results show that combining uncontextualized and contextualized questions is beneficial,especially when event triggers and arguments appear in different sentences. Our approach does not have corpus-specific components, in particular, the question generation strategies transfer across corpora. We also present a qualitative analysis of the most common errors made by our best model.",
        "author": "Md Nayem Uddin; Enfa George; Eduardo Blanco; Steven Corman",
        "authorids": "/m/md-nayem-uddin/; /e/enfa-george/; /e/eduardo-blanco/; /s/steven-corman/",
        "bibtex": "@inproceedings{uddin-etal-2024-generating,\n    title = \"Generating Uncontextualized and Contextualized Questions for Document-Level Event Argument Extraction\",\n    author = \"Uddin, Md Nayem  and\n      George, Enfa  and\n      Blanco, Eduardo  and\n      Corman, Steven\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.312/\",\n    doi = \"10.18653/v1/2024.naacl-long.312\",\n    pages = \"5612--5627\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.312.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.312/",
        "pdf_size": 585895,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2556624274032313224&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Arizona State University\u2660; University of Arizona\u2662; University of Arizona\u2662; Arizona State University\u2660",
        "aff_domain": "asu.edu;arizona.edu;arizona.edu;asu.edu",
        "email": "asu.edu;arizona.edu;arizona.edu;asu.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Arizona State University;University of Arizona",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.asu.edu;https://www.arizona.edu",
        "aff_unique_abbr": "ASU;UA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.244",
        "title": "Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts in Event Temporal Reasoning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Event temporal reasoning aims at identifying the temporal relations between two or more events from narratives. However, knowledge conflicts arise when there is a mismatch between the actual temporal relations of events in the context and the prior knowledge or biases learned by the model. In this paper, we propose to detect knowledge-conflict examples in event temporal reasoning using bias indicators, which include event relation prior bias, tense bias, narrative bias, and dependency bias. We define conflict examples as those where event relations are opposite to biased or prior relations. To mitigate event-related knowledge conflicts, we introduce a Counterfactual Data Augmentation (CDA) based method that can be applied to both Pre-trained Language Models (PLMs) and Large Language Models (LLMs) either as additional training data or demonstrations for In- Context Learning. Experiments suggest both PLMs and LLMs suffer from knowledge conflicts in event temporal reasoning, and CDA has the potential for reducing hallucination and improving model performance.",
        "author": "Tianqing Fang; Zhaowei Wang; Wenxuan Zhou; Hongming Zhang; Yangqiu Song; Muhao Chen",
        "authorids": "/t/tianqing-fang/; /z/zhaowei-wang/; /w/wenxuan-zhou/; /h/hongming-zhang/; /y/yangqiu-song/; /m/muhao-chen/",
        "bibtex": "@inproceedings{fang-etal-2024-getting,\n    title = \"Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts in Event Temporal Reasoning\",\n    author = \"Fang, Tianqing  and\n      Wang, Zhaowei  and\n      Zhou, Wenxuan  and\n      Zhang, Hongming  and\n      Song, Yangqiu  and\n      Chen, Muhao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.244/\",\n    doi = \"10.18653/v1/2024.findings-naacl.244\",\n    pages = \"3846--3868\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.244.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.244/",
        "pdf_size": 706282,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1353516845288406155&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; University of Southern California; Tencent AI Lab, Seattle; Hong Kong University of Science and Technology; University of California, Davis",
        "aff_domain": "cse.ust.hk;cse.ust.hk;usc.edu;global.tencent.com;cse.ust.hk;ucdavis.edu",
        "email": "cse.ust.hk;cse.ust.hk;usc.edu;global.tencent.com;cse.ust.hk;ucdavis.edu",
        "github": "https://github.com/tqfang/event-temporal-knowledge-conflict",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;0;3",
        "aff_unique_norm": "Hong Kong University of Science and Technology;University of Southern California;Tencent;University of California, Davis",
        "aff_unique_dep": ";;AI Lab;",
        "aff_unique_url": "https://www.ust.hk;https://www.usc.edu;https://ai.tencent.com;https://www.ucdavis.edu",
        "aff_unique_abbr": "HKUST;USC;Tencent AI Lab;UC Davis",
        "aff_campus_unique_index": "0;0;1;2;0;3",
        "aff_campus_unique": "Hong Kong SAR;Los Angeles;Seattle;Davis",
        "aff_country_unique_index": "0;0;1;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.95",
        "title": "Ghostbuster: Detecting Text Ghostwritten by Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated.Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box or unknown models.In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to several existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1). We also analyze our system\u2019s robustness to a variety of perturbations and paraphrasing attacks, and evaluate its performance on documents by non-native English speakers.",
        "author": "Vivek Verma; Eve Fleisig; Nicholas Tomlin; Dan Klein",
        "authorids": "/v/vivek-verma/; /e/eve-fleisig/; /n/nicholas-tomlin/; /d/dan-klein/",
        "bibtex": "@inproceedings{verma-etal-2024-ghostbuster,\n    title = \"Ghostbuster: Detecting Text Ghostwritten by Large Language Models\",\n    author = \"Verma, Vivek  and\n      Fleisig, Eve  and\n      Tomlin, Nicholas  and\n      Klein, Dan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.95/\",\n    doi = \"10.18653/v1/2024.naacl-long.95\",\n    pages = \"1702--1717\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.95.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.95/",
        "pdf_size": 1013970,
        "gs_citation": 115,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13263500511172823777&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Computer Science Division, UC Berkeley; Computer Science Division, UC Berkeley; Computer Science Division, UC Berkeley; Computer Science Division, UC Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "github.com/vivek3141/ghostbuster",
        "project": "ghostbuster.app",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.355",
        "title": "Global Gallery: The Fine Art of Painting Culture Portraits through Multilingual Instruction Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Exploring the intersection of language and culture in Large Language Models (LLMs), this study critically examines their capability to encapsulate cultural nuances across diverse linguistic landscapes. Central to our investigation are three research questions: the efficacy of language-specific instruction tuning, the impact of pretraining on dominant language data, and the identification of optimal approaches to elicit accurate cultural knowledge from LLMs. Utilizing the GeoMLaMA benchmark for multilingual commonsense knowledge and an adapted CAMeL dataset (English-only) for evaluation of nuanced cultural aspects, our experiments span six different languages and cultural contexts, revealing the extent of LLMs\u2019 cultural awareness. Our findings highlight a nuanced landscape: while language-specific tuning and bilingual pretraining enhance cultural understanding in certain contexts, they also uncover inconsistencies and biases, particularly in non-Western cultures. This work expands our understanding of LLMs\u2019 cultural competence and emphasizes the importance of integrating diverse cultural perspectives in their development, aiming for a more globally representative and equitable approach in language modeling.",
        "author": "Anjishnu Mukherjee; Aylin Caliskan; Ziwei Zhu; Antonios Anastasopoulos",
        "authorids": "/a/anjishnu-mukherjee/; /a/aylin-caliskan/; /z/ziwei-zhu/; /a/antonios-anastasopoulos/",
        "bibtex": "@inproceedings{mukherjee-etal-2024-global,\n    title = \"Global Gallery: The Fine Art of Painting Culture Portraits through Multilingual Instruction Tuning\",\n    author = \"Mukherjee, Anjishnu  and\n      Caliskan, Aylin  and\n      Zhu, Ziwei  and\n      Anastasopoulos, Antonios\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.355/\",\n    doi = \"10.18653/v1/2024.naacl-long.355\",\n    pages = \"6398--6415\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.355.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.355/",
        "pdf_size": 285404,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13204610967012460617&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, George Mason University; The Information School, University of Washington; Department of Computer Science, George Mason University; Department of Computer Science, George Mason University",
        "aff_domain": "gmu.edu;uw.edu;gmu.edu;gmu.edu",
        "email": "gmu.edu;uw.edu;gmu.edu;gmu.edu",
        "github": "https://github.com/iamshnoo/culture-llm",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "George Mason University;University of Washington",
        "aff_unique_dep": "Department of Computer Science;The Information School",
        "aff_unique_url": "https://www.gmu.edu;https://www.washington.edu",
        "aff_unique_abbr": "GMU;UW",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.183",
        "title": "GoT: Effective Graph-of-Thought Reasoning in Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "With the widespread use of language models (LMs) in NLP tasks, researchers have discovered the potential of Chain-of-thought (CoT) to assist LMs in accomplishing complex reasoning tasks by generating intermediate steps. However, human thought processes are often non-linear, rather than simply sequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes. GoT adopts a two-stage framework with an additional GoT encoder for thought graph representation and fuses the graph representation with the original input representation through a gated fusion mechanism. We evaluate GoT\u2019s performance on a text-only reasoning task (AQUA-RAT) and a multimodal reasoning task (ScienceQA). Our model achieves significant improvement over the strong CoT baseline on the AQUA-RAT test set and boosts accuracy from 85.19% to 87.59% using the T5-base model over the state-of-the-art Multimodal-CoT on the ScienceQA test set. Our code is publicly available at https://github.com/Zoeyyao27/Graph-of-Thought",
        "author": "Yao Yao; Zuchao Li; Hai Zhao",
        "authorids": "/y/yao-yao/; /z/zuchao-li/; /h/hai-zhao/",
        "bibtex": "@inproceedings{yao-etal-2024-got,\n    title = \"{G}o{T}: Effective Graph-of-Thought Reasoning in Language Models\",\n    author = \"Yao, Yao  and\n      Li, Zuchao  and\n      Zhao, Hai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.183/\",\n    doi = \"10.18653/v1/2024.findings-naacl.183\",\n    pages = \"2901--2921\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.183.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.183/",
        "pdf_size": 1351723,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1999511964739573843&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University + MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, 430072, P. R. China; Department of Computer Science and Engineering, Shanghai Jiao Tong University + MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;whu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;whu.edu.cn;cs.sjtu.edu.cn",
        "github": "https://github.com/Zoeyyao27/Graph-of-Thought",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0;1;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University;Wuhan University",
        "aff_unique_dep": "Department of Computer Science and Engineering;School of Computer Science",
        "aff_unique_url": "https://www.sjtu.edu.cn;http://www.whu.edu.cn",
        "aff_unique_abbr": "SJTU;WHU",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Wuhan",
        "aff_country_unique_index": "0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.58",
        "title": "GraSAME: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pretrained Language Models (PLMs) benefit from external knowledge stored in graph structures for various downstream tasks. However, bridging the modality gap between graph structures and text remains a significant challenge. Traditional methods like linearizing graphs for PLMs lose vital graph connectivity, whereas Graph Neural Networks (GNNs) require cumbersome processes for integration into PLMs. In this work, we propose a novel graph-guided self-attention mechanism, GraSAME. GraSAME seamlessly incorporates token-level structural information into PLMs without necessitating additional alignment or concatenation efforts. As an end-to-end, lightweight multimodal module, GraSAME follows a multi-task learning strategy and effectively bridges the gap between graph and textual modalities, facilitating dynamic interactions between GNNs and PLMs. Our experiments on the graph-to-text generation task demonstrate that GraSAME outperforms baseline models and achieves results comparable to state-of-the-art (SOTA) models on WebNLG datasets. Furthermore, compared to SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust graph inputs and reduces the number of trainable parameters by over 100 million.",
        "author": "Shuzhou Yuan; Michael F\u00e4rber",
        "authorids": "/s/shuzhou-yuan/; /m/michael-farber/",
        "bibtex": "@inproceedings{yuan-farber-2024-grasame,\n    title = \"{G}ra{SAME}: Injecting Token-Level Structural Information to Pretrained Language Models via Graph-guided Self-Attention Mechanism\",\n    author = {Yuan, Shuzhou  and\n      F{\\\"a}rber, Michael},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.58/\",\n    doi = \"10.18653/v1/2024.findings-naacl.58\",\n    pages = \"920--933\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.58.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.58/",
        "pdf_size": 1796653,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7906656348291987758&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Karlsruhe Institute of Technology; Karlsruhe Institute of Technology + Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) + TU Dresden",
        "aff_domain": "kit.edu;kit.edu",
        "email": "kit.edu;kit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1+2",
        "aff_unique_norm": "Karlsruhe Institute of Technology;Center for Scalable Data Analytics and Artificial Intelligence;Technische Universit\u00e4t Dresden",
        "aff_unique_dep": ";Data Analytics and Artificial Intelligence;",
        "aff_unique_url": "https://www.kit.edu;;https://www.tu-dresden.de",
        "aff_unique_abbr": "KIT;ScaDS.AI;TUD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "2024.naacl-long.354",
        "title": "Grammar-based Data Augmentation for Low-Resource Languages: The Case of Guarani-Spanish Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "One of the main problems low-resource languages face in NLP can be pictured as a vicious circle: data is needed to build and test tools, but the available text is scarce and there are not powerful tools to collect it.In order to break this circle for Guarani, we explore if text automatically generated from a grammar can work as a Data Augmentation technique to boost the performance of Guarani-Spanish Machine Translation (MT) systems.After building a grammar-based system that generates Spanish text and syntactically transfers it to Guarani, we perform several experiments by pretraining models using this synthetic text.We find that the MT systems that are pretrained with synthetic text perform better, even outperforming previous baselines.",
        "author": "Agust\u00edn Lucas; Alexis Balad\u00f3n; Victoria Pardi\u00f1as; Marvin Ag\u00fcero-Torales; Santiago G\u00f3ngora; Luis Chiruzzo",
        "authorids": "/a/agustin-lucas/; /a/alexis-baladon/; /v/victoria-pardinas/; /m/marvin-aguero-torales/; /s/santiago-gongora/; /l/luis-chiruzzo/",
        "bibtex": "@inproceedings{lucas-etal-2024-grammar,\n    title = \"Grammar-based Data Augmentation for Low-Resource Languages: The Case of {G}uarani-{S}panish Neural Machine Translation\",\n    author = {Lucas, Agust{\\'i}n  and\n      Balad{\\'o}n, Alexis  and\n      Pardi{\\~n}as, Victoria  and\n      Ag{\\\"u}ero-Torales, Marvin  and\n      G{\\'o}ngora, Santiago  and\n      Chiruzzo, Luis},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.354/\",\n    doi = \"10.18653/v1/2024.naacl-long.354\",\n    pages = \"6385--6397\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.354.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.354/",
        "pdf_size": 2239496,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14142625138365015975&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Instituto de Computaci\u00f3n, Facultad de Ingenier\u00eda, Universidad de la Rep\u00fablica, Uruguay; Instituto de Computaci\u00f3n, Facultad de Ingenier\u00eda, Universidad de la Rep\u00fablica, Uruguay; Instituto de Computaci\u00f3n, Facultad de Ingenier\u00eda, Universidad de la Rep\u00fablica, Uruguay; Global CoE of Data Intelligence, Fujitsu, Spain+Universidad de Granada, Spain; Instituto de Computaci\u00f3n, Facultad de Ingenier\u00eda, Universidad de la Rep\u00fablica, Uruguay; Instituto de Computaci\u00f3n, Facultad de Ingenier\u00eda, Universidad de la Rep\u00fablica, Uruguay",
        "aff_domain": "fing.edu.uy;fing.edu.uy;fing.edu.uy;correo.ugr.es;fing.edu.uy;fing.edu.uy",
        "email": "fing.edu.uy;fing.edu.uy;fing.edu.uy;correo.ugr.es;fing.edu.uy;fing.edu.uy",
        "github": "https://github.com/pln-fing-udelar/guarani-grammar-NAACL20246385",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1+2;0;0",
        "aff_unique_norm": "Universidad de la Rep\u00fablica;Fujitsu;Universidad de Granada",
        "aff_unique_dep": "Instituto de Computaci\u00f3n;Global CoE of Data Intelligence;",
        "aff_unique_url": "https://www.ur.edu.uy;https://www.fujitsu.com/;https://www.ugr.es",
        "aff_unique_abbr": ";Fujitsu;UGr",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1+1;0;0",
        "aff_country_unique": "Uruguay;Spain"
    },
    {
        "id": "2024.naacl-industry.29",
        "title": "Graph Integrated Language Transformers for Next Action Prediction in Complex Phone Calls",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Current Conversational AI systems employ different machine learning pipelines, as well as external knowledge sources and business logic to predict the next action. Maintaining various components in dialogue managers\u2019 pipeline adds complexity in expansion and updates, increases processing time, and causes additive noise through the pipeline that can lead to incorrect next action prediction. This paper investigates graph integration into language transformers to improve understanding the relationships between humans\u2019 utterances, previous, and next actions without the dependency on external sources or components. Experimental analyses on real calls indicate that the proposed Graph Integrated Language Transformer models can achieve higher performance compared to other production level conversational AI systems in driving interactive calls with human users in real-world settings.",
        "author": "Amin Marani; Ulie Schnaithmann; Youngseo Son; Akil Iyer; Manas Paldhe; Arushi Raghuvanshi",
        "authorids": "/a/amin-marani/; /u/ulie-schnaithmann/; /y/youngseo-son/; /a/akil-iyer/; /m/manas-paldhe/; /a/arushi-raghuvanshi/",
        "bibtex": "@inproceedings{marani-etal-2024-graph,\n    title = \"Graph Integrated Language Transformers for Next Action Prediction in Complex Phone Calls\",\n    author = \"Marani, Amin  and\n      Schnaithmann, Ulie  and\n      Son, Youngseo  and\n      Iyer, Akil  and\n      Paldhe, Manas  and\n      Raghuvanshi, Arushi\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.29/\",\n    doi = \"10.18653/v1/2024.naacl-industry.29\",\n    pages = \"347--358\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.29.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.29/",
        "pdf_size": 1226720,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:yjpft0YhOfsJ:scholar.google.com/&scioq=Graph+Integrated+Language+Transformers+for+Next+Action+Prediction+in+Complex+Phone+Calls&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.findings-naacl.32",
        "title": "Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational AutoEncoders",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The injection of syntactic information in Variational AutoEncoders (VAEs) can result in an overall improvement of performances and generalisation. An effective strategy to achieve such a goal is to separate the encoding of distributional semantic features and syntactic structures into heterogeneous latent spaces via multi-task learning or dual encoder architectures. However, existing works employing such techniques are limited to LSTM-based VAEs. This work investigates latent space separation methods for structural syntactic injection in Transformer-based VAE architectures (i.e., Optimus) through the integration of graph-based models. Our empirical evaluation reveals that the proposed end-to-end VAE architecture can improve theoverall organisation of the latent space, alleviating the information loss occurring in standard VAE setups, and resulting in enhanced performances on language modelling and downstream generation tasks.",
        "author": "Yingji Zhang; Marco Valentino; Danilo Carvalho; Ian Pratt-Hartmann; Andre Freitas",
        "authorids": "/y/yingji-zhang/; /m/marco-valentino/; /d/danilo-carvalho/; /i/ian-pratt-hartmann/; /a/andre-freitas/",
        "bibtex": "@inproceedings{zhang-etal-2024-graph,\n    title = \"Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational {A}uto{E}ncoders\",\n    author = \"Zhang, Yingji  and\n      Valentino, Marco  and\n      Carvalho, Danilo  and\n      Pratt-Hartmann, Ian  and\n      Freitas, Andre\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.32/\",\n    doi = \"10.18653/v1/2024.findings-naacl.32\",\n    pages = \"474--489\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.32.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.32/",
        "pdf_size": 21653196,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14317453791551761145&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2024.findings-naacl.100",
        "title": "Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We present an empirical study of groundedness in long-form question answering (LFQA) by retrieval-augmented large language models (LLMs).In particular, we evaluate whether every generated sentence is grounded in the retrieved documents or the model\u2019s pre-training data.Across 3 datasets and 4 model families, our findings reveal that a significant fraction of generated sentences are consistently ungrounded, even when those sentences contain correct ground-truth answers.Additionally, we examine the impacts of factors such as model size, decoding strategy, and instruction tuning on groundedness. Our results show that while larger models tend to ground their outputs more effectively, a significant portion of correct answers remains compromised by hallucinations. This study provides novel insights into the groundedness challenges in LFQA and underscores the necessity for more robust mechanisms in LLMs to mitigate the generation of ungrounded content.",
        "author": "Alessandro Stolfo",
        "authorids": "/a/alessandro-stolfo/",
        "bibtex": "@inproceedings{stolfo-2024-groundedness,\n    title = \"Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study\",\n    author = \"Stolfo, Alessandro\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.100/\",\n    doi = \"10.18653/v1/2024.findings-naacl.100\",\n    pages = \"1537--1552\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.100.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.100/",
        "pdf_size": 1516887,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=536629747316450629&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "ETH Z\u00fcrich",
        "aff_domain": "ethz.ch",
        "email": "ethz.ch",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2024.naacl-long.348",
        "title": "Grounding Gaps in Language Model Generations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). However, it is unclear whether large language models (LLMs) generate text that reflects human grounding. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLM generations contain grounding acts, simulating turn-taking from several dialogue datasets and comparing results to humans. We find that\u2014compared to humans\u2014LLMs generate language with less conversational grounding, instead generating text that appears to simply presume common ground. To understand the roots of the identified grounding gap, we examine the role of instruction tuning and preference optimization, finding that training on contemporary preference data leads to a reduction in generated grounding acts. Altogether, we highlight the need for more research investigating conversational grounding in human-AI interaction.",
        "author": "Omar Shaikh; Kristina Gligoric; Ashna Khetan; Matthias Gerstgrasser; Diyi Yang; Dan Jurafsky",
        "authorids": "/o/omar-shaikh/; /k/kristina-gligoric/; /a/ashna-khetan/; /m/matthias-gerstgrasser/; /d/diyi-yang/; /d/dan-jurafsky/",
        "bibtex": "@inproceedings{shaikh-etal-2024-grounding,\n    title = \"Grounding Gaps in Language Model Generations\",\n    author = \"Shaikh, Omar  and\n      Gligoric, Kristina  and\n      Khetan, Ashna  and\n      Gerstgrasser, Matthias  and\n      Yang, Diyi  and\n      Jurafsky, Dan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.348/\",\n    doi = \"10.18653/v1/2024.naacl-long.348\",\n    pages = \"6279--6296\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.348.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.348/",
        "pdf_size": 468471,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16351816840172358521&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University; Stanford University; Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.143",
        "title": "Group Fairness in Multilingual Speech Recognition Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We evaluate the performance disparity of the Whisper and MMS families of ASR models across the VoxPopuli and Common Voice multilingual datasets, with an eye toward intersectionality. Our two most important findings are that model size, surprisingly, correlates logarithmically with worst-case performance disparities, meaning that larger (and better) models are less fair. We also observe the importance of intersectionality. In particular, models often exhibit significant performance disparity across binary gender for adolescents.",
        "author": "Anna Zee; Marc Zee; Anders S\u00f8gaard",
        "authorids": "/a/anna-zee/; /m/marc-zee/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{zee-etal-2024-group,\n    title = \"Group Fairness in Multilingual Speech Recognition Models\",\n    author = \"Zee, Anna  and\n      Zee, Marc  and\n      S{\\o}gaard, Anders\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.143/\",\n    doi = \"10.18653/v1/2024.findings-naacl.143\",\n    pages = \"2213--2226\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.143.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.143/",
        "pdf_size": 502139,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:l9GNWKwjKgsJ:scholar.google.com/&scioq=Group+Fairness+in+Multilingual+Speech+Recognition+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, University of Copenhagen; Google Deepmind; Department of Computer Science, Center for Philosophy of AI, University of Copenhagen",
        "aff_domain": "di.ku.dk;google.com;di.ku.dk",
        "email": "di.ku.dk;google.com;di.ku.dk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Copenhagen;DeepMind",
        "aff_unique_dep": "Department of Computer Science;DeepMind",
        "aff_unique_url": "https://www.ku.dk;https://deepmind.com",
        "aff_unique_abbr": "UCPH;DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Denmark;United Kingdom"
    },
    {
        "id": "2024.findings-naacl.265",
        "title": "Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Machine Translation (MT) remains one of the last NLP tasks where large language models (LLMs) have not yet replaced dedicated supervised systems. This work exploits the complementary strengths of LLMs and supervised MT by guiding LLMs to automatically post-edit MT with external feedback on its quality, derived from Multidimensional Quality Metric (MQM) annotations. Working with LLaMA-2 models, we consider prompting strategies varying the nature of feedback provided and then fine-tune the LLM to improve its ability to exploit the provided guidance. Through experiments on Chinese-English, English-German, and English-Russian MQM data, we demonstrate that prompting LLMs to post-edit MT improves TER, BLEU and COMET scores, although the benefits of fine-grained feedback are not clear. Fine-tuning helps integrate fine-grained feedback more effectively and further improves translation quality based on both automatic and human evaluation.",
        "author": "Dayeon Ki; Marine Carpuat",
        "authorids": "/d/dayeon-ki/; /m/marine-carpuat/",
        "bibtex": "@inproceedings{ki-carpuat-2024-guiding,\n    title = \"Guiding Large Language Models to Post-Edit Machine Translation with Error Annotations\",\n    author = \"Ki, Dayeon  and\n      Carpuat, Marine\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.265/\",\n    doi = \"10.18653/v1/2024.findings-naacl.265\",\n    pages = \"4253--4273\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.265.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.265/",
        "pdf_size": 1232126,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15980474462001285416&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science, University of Maryland; Computer Science, UMIACS, University of Maryland",
        "aff_domain": "cs.umd.edu;cs.umd.edu",
        "email": "cs.umd.edu;cs.umd.edu",
        "github": "https://github.com/dayeonki/mt_feedback",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.70",
        "title": "GuyLingo: The Republic of Guyana Creole Corpora",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "While major languages often enjoy substantial attention and resources, the linguistic diversity across the globe encompasses a multitude of smaller, indigenous, and regional languages that lack the same level of computational support. One such region is the Caribbean. While commonly labeled as \u201cEnglish speaking\u201d, the ex-British Caribbean region consists of a myriad of Creole languages thriving alongside English. In this paper, we present Guylingo: a comprehensive corpus designed for advancing NLP research in the domain of Creolese (Guyanese English-lexicon Creole), the most widely spoken language in the culturally rich nation of Guyana. We first outline our framework for gathering and digitizing this diverse corpus, inclusive of colloquial expressions, idioms, and regional variations in a low-resource language. We then demonstrate the challenges of training and evaluating NLP models for machine translation for Creolese. Lastly, we discuss the unique opportunities presented by recent NLP advancements for accelerating the formal adoption of Creole languages as official languages in the Caribbean.",
        "author": "Christopher Clarke; Roland Daynauth; Jason Mars; Charlene Wilkinson; Hubert Devonish",
        "authorids": "/c/christopher-clarke/; /r/roland-daynauth/; /j/jason-mars/; /c/charlene-wilkinson/; /h/hubert-devonish/",
        "bibtex": "@inproceedings{clarke-etal-2024-guylingo,\n    title = \"{G}uy{L}ingo: The {R}epublic of {G}uyana Creole Corpora\",\n    author = \"Clarke, Christopher  and\n      Daynauth, Roland  and\n      Mars, Jason  and\n      Wilkinson, Charlene  and\n      Devonish, Hubert\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.70/\",\n    doi = \"10.18653/v1/2024.naacl-short.70\",\n    pages = \"792--798\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.70.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.70/",
        "pdf_size": 864905,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17032639402990184548&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Michigan, Ann Arbor, MI; University of Michigan, Ann Arbor, MI; University of Guyana, Georgetown, Guyana; ; University of Michigan, Ann Arbor, MI",
        "aff_domain": "umich.edu;umich.edu;uog.edu.gy;gmail.com;umich.edu",
        "email": "umich.edu;umich.edu;uog.edu.gy;gmail.com;umich.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Michigan;University of Guyana",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umich.edu;https://www.uog.edu.gy",
        "aff_unique_abbr": "UM;UoG",
        "aff_campus_unique_index": "0;0;1;0",
        "aff_campus_unique": "Ann Arbor;Georgetown",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Guyana"
    },
    {
        "id": "2024.naacl-long.437",
        "title": "HIL: Hybrid Isotropy Learning for Zero-shot Performance in Dense retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Advancements in dense retrieval models have brought ColBERT to prominence in Information Retrieval (IR) with its advanced interaction techniques.However, ColBERT is reported to frequently underperform in zero-shot scenarios, where traditional techniques such as BM25 still exceed it.Addressing this, we propose to balance representation isotropy and anisotropy for zero-shot model performance, based on our observations that isotropy can enhance cosine similarity computations and anisotropy may aid in generalizing to unseen data.Striking a balance between these isotropic and anisotropic qualities stands as a critical objective to refine model efficacy.Based on this, we present ours, a Hybrid Isotropy Learning (HIL) architecture that integrates isotropic and anisotropic representations.Our experiments with the BEIR benchmark show that our model significantly outperforms the baseline ColBERT model, highlighting the importance of harmonized isotropy in improving zero-shot retrieval performance.",
        "author": "Jaeyoung Kim; Dohyeon Lee; Seung-won Hwang",
        "authorids": "/j/jaeyoung-kim/; /d/dohyeon-lee/; /s/seung-won-hwang/",
        "bibtex": "@inproceedings{kim-etal-2024-hil,\n    title = \"{HIL}: Hybrid Isotropy Learning for Zero-shot Performance in Dense retrieval\",\n    author = \"Kim, Jaeyoung  and\n      Lee, Dohyeon  and\n      Hwang, Seung-won\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.437/\",\n    doi = \"10.18653/v1/2024.naacl-long.437\",\n    pages = \"7892--7903\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.437.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.437/",
        "pdf_size": 1329355,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12471227634823161358&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "IPAI; Computer Science and Engineering; IPAI+Computer Science and Engineering",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0+1",
        "aff_unique_norm": "Institute of Parallel and Distributed Systems;Computer Science and Engineering",
        "aff_unique_dep": ";Computer Science and Engineering",
        "aff_unique_url": "http://ipai.hust.edu.cn/;",
        "aff_unique_abbr": "IPAI;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "2024.naacl-long.265",
        "title": "HILL: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical Text Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing self-supervised methods in natural language processing (NLP), especially hierarchical text classification (HTC), mainly focus on self-supervised contrastive learning, extremely relying on human-designed augmentation rules to generate contrastive samples, which can potentially corrupt or distort the original information. In this paper, we tend to investigate the feasibility of a contrastive learning scheme in which the semantic and syntactic information inherent in the input sample is adequately reserved in the contrastive samples and fused during the learning process. Specifically, we propose an information lossless contrastive learning strategy for HTC, namely Hierarchy-aware Information Lossless contrastive Learning (HILL), which consists of a text encoder representing the input document, and a structure encoder directly generating the positive sample. The structure encoder takes the document embedding as input, extracts the essential syntactic information inherent in the label hierarchy with the principle of structural entropy minimization, and injects the syntactic information into the text representation via hierarchical representation learning. Experiments on three common datasets are conducted to verify the superiority of HILL.",
        "author": "He Zhu; Junran Wu; Ruomei Liu; Yue Hou; Ze Yuan; Shangzhe Li; Yicheng Pan; Ke Xu",
        "authorids": "/h/he-zhu/; /j/junran-wu/; /r/ruomei-liu/; /y/yue-hou/; /z/ze-yuan/; /s/shangzhe-li/; /y/yicheng-pan/; /k/ke-xu/",
        "bibtex": "@inproceedings{zhu-etal-2024-hill,\n    title = \"{HILL}: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical Text Classification\",\n    author = \"Zhu, He  and\n      Wu, Junran  and\n      Liu, Ruomei  and\n      Hou, Yue  and\n      Yuan, Ze  and\n      Li, Shangzhe  and\n      Pan, Yicheng  and\n      Xu, Ke\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.265/\",\n    doi = \"10.18653/v1/2024.naacl-long.265\",\n    pages = \"4731--4745\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.265.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.265/",
        "pdf_size": 9843037,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12163035452932790513&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2024.naacl-industry.1",
        "title": "HPipe: Large Language Model Pipeline Parallelism for Long Context on Heterogeneous Cost-effective Devices",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Micro-enterprises and individual developers emerge analysis demands for long sequence with powerful Large Language Models (LLMs). They try to deploy the LLMs at local, but only possess various commodity devices and the unreliable interconnection between devices. Existing parallel techniques do not lead to the same effectiveness in limited environment. The heterogeneity of devices, coupled with their limited capacity and expensive communication, brings challenges to private deployment for maximized utilization of available devices while masking latency. Hence, we introduce HPipe, a pipeline inference framework that successfully mitigates LLMs from high-performance clusters to heterogeneous commodity devices. By ensuring a balanced distribution of workloads, HPipe facilitates the parallel execution of LLMs through pipelining the sequences on the token dimension. The evaluation conducted on LLaMA-7B and GPT3-2B demonstrates that HPipe holds the potential for context analysis on LLM with heterogeneity devices, achieving an impressive speedup in latency and throughput up to 2.28 times.",
        "author": "Ruilong Ma; Xiang Yang; Jingyu Wang; Qi Qi; Haifeng Sun; Jing Wang; Zirui Zhuang; Jianxin Liao",
        "authorids": "/r/ruilong-ma/; /x/xiang-yang/; /j/jingyu-wang/; /q/qi-qi/; /h/haifeng-sun/; /j/jing-wang/; /z/zirui-zhuang/; /j/jianxin-liao/",
        "bibtex": "@inproceedings{ma-etal-2024-hpipe,\n    title = \"{HP}ipe: Large Language Model Pipeline Parallelism for Long Context on Heterogeneous Cost-effective Devices\",\n    author = \"Ma, Ruilong  and\n      Yang, Xiang  and\n      Wang, Jingyu  and\n      Qi, Qi  and\n      Sun, Haifeng  and\n      Wang, Jing  and\n      Zhuang, Zirui  and\n      Liao, Jianxin\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.1/\",\n    doi = \"10.18653/v1/2024.naacl-industry.1\",\n    pages = \"1--9\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.1.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.1/",
        "pdf_size": 701227,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=668085250558564964&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "State Key Laboratory of Networking and Switching Technology",
        "aff_unique_url": "http://www.bupt.edu.cn",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.225",
        "title": "HTCCN: Temporal Causal Convolutional Networks with Hawkes Process for Extrapolation Reasoning in Temporal Knowledge Graphs",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Temporal knowledge graphs (TKGs) serve as powerful tools for storing and modeling dynamic facts, holding immense potential in anticipating future facts. Since future facts are inherently unknowable, effectively modeling the intricate temporal structure of historical facts becomes paramount for accurate prediction. However, current models often rely heavily on fact recurrence or periodicity, leading to information loss due to prolonged evolutionary processes. Notably, the occurrence of one fact always influences the likelihood of another. To this end, we propose HTCCN, a novel Hawkes process-based temporal causal convolutional network designed for temporal reasoning under extrapolation settings. HTCCN employs a temporal causal convolutional network to model the historical interdependence of facts and leverages Hawkes to model link formation processes inductively in TKGs. Importantly, HTCCN introduces dual-level dynamics to comprehensively capture the temporal evolution of facts. Rigorous experimentation on four real-world datasets underscores the superior performance of HTCCN.",
        "author": "Tingxuan Chen; Jun Long; Liu Yang; Zidong Wang; Yongheng Wang; Xiongnan Jin",
        "authorids": "/t/tingxuan-chen/; /j/jun-long/; /l/liu-yang/; /z/zidong-wang/; /y/yongheng-wang/; /x/xiongnan-jin/",
        "bibtex": "@inproceedings{chen-etal-2024-htccn,\n    title = \"{HTCCN}: Temporal Causal Convolutional Networks with {H}awkes Process for Extrapolation Reasoning in Temporal Knowledge Graphs\",\n    author = \"Chen, Tingxuan  and\n      Long, Jun  and\n      Yang, Liu  and\n      Wang, Zidong  and\n      Wang, Yongheng  and\n      Jin, Xiongnan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.225/\",\n    doi = \"10.18653/v1/2024.naacl-long.225\",\n    pages = \"4056--4066\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.225.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.225/",
        "pdf_size": 995019,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14488178321721422486&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Engineering, Central South University, Hunan, China; Big Data Institute, Central South University, Hunan, China; School of Computer Science and Engineering, Central South University, Hunan, China; School of Computer Science and Engineering, Central South University, Hunan, China; Zhejiang lab, Zhejiang, China; Zhejiang lab, Zhejiang, China",
        "aff_domain": "csu.edu.cn;csu.edu.cn;csu.edu.cn;csu.edu.cn;zhejianglab.com;zhejianglab.com",
        "email": "csu.edu.cn;csu.edu.cn;csu.edu.cn;csu.edu.cn;zhejianglab.com;zhejianglab.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "Central South University;Zhejiang Lab",
        "aff_unique_dep": "School of Computer Science and Engineering;",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.479",
        "title": "Hallucination Diversity-Aware Active Learning for Text Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning. Extensive experiments on three datasets and different backbone models demonstrate advantages of our method in effectively and efficiently mitigating LLM hallucinations.",
        "author": "Yu Xia; Xu Liu; Tong Yu; Sungchul Kim; Ryan Rossi; Anup Rao; Tung Mai; Shuai Li",
        "authorids": "/y/yu-xia/; /x/xu-liu/; /t/tong-yu/; /s/sungchul-kim/; /r/ryan-rossi/; /a/anup-rao/; /t/tung-mai/; /s/shuai-li/",
        "bibtex": "@inproceedings{xia-etal-2024-hallucination,\n    title = \"Hallucination Diversity-Aware Active Learning for Text Summarization\",\n    author = \"Xia, Yu  and\n      Liu, Xu  and\n      Yu, Tong  and\n      Kim, Sungchul  and\n      Rossi, Ryan  and\n      Rao, Anup  and\n      Mai, Tung  and\n      Li, Shuai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.479/\",\n    doi = \"10.18653/v1/2024.naacl-long.479\",\n    pages = \"8665--8677\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.479.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.479/",
        "pdf_size": 509570,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4640564406695978970&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Shanghai Jiao Tong University+University of Michigan; Shanghai Jiao Tong University; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Shanghai Jiao Tong University",
        "aff_domain": "umich.edu;sjtu.edu.cn;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;sjtu.edu.cn",
        "email": "umich.edu;sjtu.edu.cn;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;sjtu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0;2;2;2;2;2;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;University of Michigan;Adobe",
        "aff_unique_dep": ";;Adobe Research",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.umich.edu;https://research.adobe.com",
        "aff_unique_abbr": "SJTU;UM;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;1;1;1;1;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.findings-naacl.172",
        "title": "HateModerate: Testing Hate Speech Detectors against Content Moderation Policies",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "To protect users from massive hateful content, existing works studied automated hate speech detection. Despite the existing efforts, one question remains: Do automated hate speech detectors conform to social media content policies? A platform\u2019s content policies are a checklist of content moderated by the social media platform. Because content moderation rules are often uniquely defined, existing hate speech datasets cannot directly answer this question. This work seeks to answer this question by creating HateModerate, a dataset for testing the behaviors of automated content moderators against content policies. First, we engage 28 annotators and GPT in a six-step annotation process, resulting in a list of hateful and non-hateful test suites matching each of Facebook\u2019s 41 hate speech policies. Second, we test the performance of state-of-the-art hate speech detectors against HateModerate, revealing substantial failures these models have in their conformity to the policies. Third, using HateModerate, we augment the training data of a top-downloaded hate detector on HuggingFace. We observe significant improvement in the models\u2019 conformity to content policies while having comparable scores on the original test data. Our dataset and code can be found on https://github.com/stevens-textmining/HateModerate.",
        "author": "Jiangrui Zheng; Xueqing Liu; Mirazul Haque; Xing Qian; Guanqun Yang; Wei Yang",
        "authorids": "/j/jiangrui-zheng/; /x/xueqing-liu/; /m/mirazul-haque/; /x/xing-qian/; /g/guanqun-yang/; /w/wei-yang/",
        "bibtex": "@inproceedings{zheng-etal-2024-hatemoderate,\n    title = \"{H}ate{M}oderate: Testing Hate Speech Detectors against Content Moderation Policies\",\n    author = \"Zheng, Jiangrui  and\n      Liu, Xueqing  and\n      Haque, Mirazul  and\n      Qian, Xing  and\n      Yang, Guanqun  and\n      Yang, Wei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.172/\",\n    doi = \"10.18653/v1/2024.findings-naacl.172\",\n    pages = \"2691--2710\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.172.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.172/",
        "pdf_size": 953671,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3429575377059387495&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://github.com/stevens-textmining/HateModerate",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.18",
        "title": "Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A. Will LLMs Replace Knowledge Graphs?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.",
        "author": "Kai Sun; Yifan Xu; Hanwen Zha; Yue Liu; Xin Luna Dong",
        "authorids": "/k/kai-sun/; /y/yifan-xu/; /h/hanwen-zha/; /y/yue-liu/; /x/xin-luna-dong/",
        "bibtex": "@inproceedings{sun-etal-2024-head,\n    title = \"Head-to-Tail: How Knowledgeable are Large Language Models ({LLM}s)? {A}.{K}.{A}. Will {LLM}s Replace Knowledge Graphs?\",\n    author = \"Sun, Kai  and\n      Xu, Yifan  and\n      Zha, Hanwen  and\n      Liu, Yue  and\n      Dong, Xin Luna\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.18/\",\n    doi = \"10.18653/v1/2024.naacl-long.18\",\n    pages = \"311--325\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.18.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.18/",
        "pdf_size": 239418,
        "gs_citation": 161,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7745097287573078774&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2024.naacl-long.185",
        "title": "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing open-source helpfulness preference datasets do not specify what makes some responses more helpful and others less so. Models trained on these datasets can incidentally learn to model dataset artifacts (e.g. preferring longer but unhelpful responses only due to their length). To alleviate this problem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated for the various aspects that make responses helpful. Specifically, our 37k-sample dataset has annotations for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses. Training Llama 2 70B using the HelpSteer dataset with SteerLM technique produces a model that scores 7.54 on MT Bench, which is currently the highest score for open models that do not require training data from more powerful models (e.g. GPT-4). We release this dataset with CC-BY-4.0 license at https://huggingface.co/datasets/nvidia/HelpSteer",
        "author": "Zhilin Wang; Yi Dong; Jiaqi Zeng; Virginia Adams; Makesh Narsimhan Sreedhar; Daniel Egert; Olivier Delalleau; Jane Scowcroft; Neel Kant; Aidan Swope; Oleksii Kuchaiev",
        "authorids": "/z/zhilin-wang/; /y/yi-dong/; /j/jiaqi-zeng/; /v/virginia-adams/; /m/makesh-narsimhan-sreedhar/; /d/daniel-egert/; /o/olivier-delalleau/; /j/jane-scowcroft/; /n/neel-kant/; /a/aidan-swope/; /o/oleksii-kuchaiev/",
        "bibtex": "@inproceedings{wang-etal-2024-helpsteer,\n    title = \"{H}elp{S}teer: Multi-attribute Helpfulness Dataset for {S}teer{LM}\",\n    author = \"Wang, Zhilin  and\n      Dong, Yi  and\n      Zeng, Jiaqi  and\n      Adams, Virginia  and\n      Sreedhar, Makesh Narsimhan  and\n      Egert, Daniel  and\n      Delalleau, Olivier  and\n      Scowcroft, Jane  and\n      Kant, Neel  and\n      Swope, Aidan  and\n      Kuchaiev, Oleksii\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.185/\",\n    doi = \"10.18653/v1/2024.naacl-long.185\",\n    pages = \"3371--3384\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.185.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.185/",
        "pdf_size": 463374,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12751751050598948780&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA",
        "aff_domain": "nvidia.com;nvidia.com; ; ; ; ; ; ; ; ; ",
        "email": "nvidia.com;nvidia.com; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://huggingface.co/datasets/nvidia/HelpSteer",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "NVIDIA",
        "aff_unique_dep": "NVIDIA Corporation",
        "aff_unique_url": "https://www.nvidia.com",
        "aff_unique_abbr": "NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.160",
        "title": "Heterogeneity over Homogeneity: Investigating Multilingual Speech Pre-Trained Models for Detecting Audio Deepfake",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this work, we investigate multilingual speech Pre-Trained models (PTMs) for Audio deepfake detection (ADD). We hypothesize thatmultilingual PTMs trained on large-scale diverse multilingual data gain knowledge about diverse pitches, accents, and tones, during theirpre-training phase and making them more robust to variations. As a result, they will be more effective for detecting audio deepfakes. To validate our hypothesis, we extract representations from state-of-the-art (SOTA) PTMs including monolingual, multilingual as well as PTMs trained for speaker and emotion recognition, and evaluated them on ASVSpoof 2019 (ASV), In-the-Wild (ITW), and DECRO benchmark databases. We show that representations from multilingual PTMs, with simple downstream networks, attain the best performance for ADD compared to other PTM representations, which validates our hypothesis. We also explore the possibility of fusion of selected PTM representations for further improvements in ADD, and we propose a framework, MiO (Merge into One) for this purpose. With MiO, we achieve SOTA performance on ASV and ITW and comparable performance on DECRO with current SOTA works.",
        "author": "Orchid Chetia Phukan; Gautam Kashyap; Arun Balaji Buduru; Rajesh Sharma",
        "authorids": "/o/orchid-chetia-phukan/; /g/gautam-kashyap/; /a/arun-balaji-buduru/; /r/rajesh-sharma/",
        "bibtex": "@inproceedings{chetia-phukan-etal-2024-heterogeneity,\n    title = \"Heterogeneity over Homogeneity: Investigating Multilingual Speech Pre-Trained Models for Detecting Audio Deepfake\",\n    author = \"Chetia Phukan, Orchid  and\n      Kashyap, Gautam  and\n      Buduru, Arun Balaji  and\n      Sharma, Rajesh\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.160/\",\n    doi = \"10.18653/v1/2024.findings-naacl.160\",\n    pages = \"2496--2506\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.160.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.160/",
        "pdf_size": 2476862,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3391856040735538554&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "IIIT-Delhi, India; IIIT-Delhi, India; IIIT-Delhi, India; IIIT-Delhi, India+University of Tartu, Estonia",
        "aff_domain": "iiitd.ac.in; ; ; ",
        "email": "iiitd.ac.in; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "IIIT-Delhi;University of Tartu",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iiitdelhi.ac.in;https://www.ut.ee",
        "aff_unique_abbr": "IIIT-D;UT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "India;Estonia"
    },
    {
        "id": "2024.findings-naacl.45",
        "title": "Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Scientific document summarization has been a challenging task due to the long structure of the input text. The long input hinders the simultaneous effective modeling of both global high-order relations between sentences and local intra-sentence relations which is the most critical step in extractive summarization. However, existing methods mostly focus on one type of relation, neglecting the simultaneous effective modeling of both relations, which can lead to insufficient learning of semantic representations. In this paper, we propose HAESum, a novel approach utilizing graph neural networks to locally and globally model documents based on their hierarchical discourse structure. First, intra-sentence relations are learned using a local heterogeneous graph. Subsequently, a novel hypergraph self-attention layer is introduced to further enhance the characterization of high-order inter-sentence relations. We validate our approach on two benchmark datasets, and the experimental results demonstrate the effectiveness of HAESum and the importance of considering hierarchical structures in modeling long scientific documents.",
        "author": "Chenlong Zhao; Xiwen Zhou; Xiaopeng Xie; Yong Zhang",
        "authorids": "/c/chenlong-zhao/; /x/xiwen-zhou/; /x/xiaopeng-xie/; /y/yong-zhang/",
        "bibtex": "@inproceedings{zhao-etal-2024-hierarchical,\n    title = \"Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level\",\n    author = \"Zhao, Chenlong  and\n      Zhou, Xiwen  and\n      Xie, Xiaopeng  and\n      Zhang, Yong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.45/\",\n    doi = \"10.18653/v1/2024.findings-naacl.45\",\n    pages = \"714--726\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.45.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.45/",
        "pdf_size": 401941,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1907753254477570518&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China+Beijing Key Laboratory of Work Safety Intelligent Monitoring, Beijing University of Posts and Telecommunications, Beijing 100876, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China+Beijing Key Laboratory of Work Safety Intelligent Monitoring, Beijing University of Posts and Telecommunications, Beijing 100876, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China+Beijing Key Laboratory of Work Safety Intelligent Monitoring, Beijing University of Posts and Telecommunications, Beijing 100876, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China+Beijing Key Laboratory of Work Safety Intelligent Monitoring, Beijing University of Posts and Telecommunications, Beijing 100876, China",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn",
        "github": "https://github.com/MoLICHENXI/HAESum",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;0+0;0+0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "School of Electronic Engineering",
        "aff_unique_url": "http://www.bupt.edu.cn/",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.138",
        "title": "How Interpretable are Reasoning Explanations from Prompting Large Language Models?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability",
        "author": "Yeo Wei Jie; Ranjan Satapathy; Rick Goh; Erik Cambria",
        "authorids": "/y/yeo-wei-jie/; /r/ranjan-satapathy/; /r/rick-goh/; /e/erik-cambria/",
        "bibtex": "@inproceedings{wei-jie-etal-2024-interpretable,\n    title = \"How Interpretable are Reasoning Explanations from Prompting Large Language Models?\",\n    author = \"Wei Jie, Yeo  and\n      Satapathy, Ranjan  and\n      Goh, Rick  and\n      Cambria, Erik\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.138/\",\n    doi = \"10.18653/v1/2024.findings-naacl.138\",\n    pages = \"2148--2164\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.138.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.138/",
        "pdf_size": 1903609,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16937995758165618153&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science and Engineering, Nanyang Technological University, Singapore; Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A \u2217STAR), 1 Fusionopolis Way, #16-16 Connexis, 138632, Singapore; Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A \u2217STAR), 1 Fusionopolis Way, #16-16 Connexis, 138632, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore",
        "aff_domain": "e.ntu.edu.sg;ihpc.astar.edu.sg;ihpc.astar.edu.sg;ntu.edu.sg",
        "email": "e.ntu.edu.sg;ihpc.astar.edu.sg;ihpc.astar.edu.sg;ntu.edu.sg",
        "github": "https://github.com/SenticNet/CoT_interpretability",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Nanyang Technological University;Agency for Science, Technology and Research",
        "aff_unique_dep": "School of Computer Science and Engineering;Institute of High Performance Computing",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.a-star.edu.sg",
        "aff_unique_abbr": "NTU;A*STAR",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Singapore;Fusionopolis",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2024.findings-naacl.273",
        "title": "How Lexical is Bilingual Lexicon Induction?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In contemporary machine learning approaches to bilingual lexicon induction (BLI), a model learns a mapping between the embedding spaces of a language pair. Recently, retrieve-and-rank approach to BLI has achieved state of the art results on the task. However, the problem remains challenging in low-resource settings, due to the paucity of data. The task is complicated by factors such as lexical variation across languages. We argue that the incorporation of additional lexical information into the recent retrieve-and-rank approach should improve lexicon induction. We demonstrate the efficacy of our proposed approach on XLING, improving over the previous state of the art by an average of 2% across all language pairs.",
        "author": "Harsh Kohli; Helian Feng; Nicholas Dronen; Calvin McCarter; Sina Moeini; Ali Kebarighotbi",
        "authorids": "/h/harsh-kohli/; /h/helian-feng/; /n/nicholas-dronen/; /c/calvin-mccarter/; /s/sina-moeini/; /a/ali-kebarighotbi/",
        "bibtex": "@inproceedings{kohli-etal-2024-lexical,\n    title = \"How Lexical is Bilingual Lexicon Induction?\",\n    author = \"Kohli, Harsh  and\n      Feng, Helian  and\n      Dronen, Nicholas  and\n      McCarter, Calvin  and\n      Moeini, Sina  and\n      Kebarighotbi, Ali\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.273/\",\n    doi = \"10.18653/v1/2024.findings-naacl.273\",\n    pages = \"4381--4386\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.273.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.273/",
        "pdf_size": 1288396,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3221202751008632687&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.152",
        "title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.",
        "author": "Lingbo Mo; Boshi Wang; Muhao Chen; Huan Sun",
        "authorids": "/l/lingbo-mo/; /b/boshi-wang/; /m/muhao-chen/; /h/huan-sun/",
        "bibtex": "@inproceedings{mo-etal-2024-trustworthy,\n    title = \"How Trustworthy are Open-Source {LLM}s? An Assessment under Malicious Demonstrations Shows their Vulnerabilities\",\n    author = \"Mo, Lingbo  and\n      Wang, Boshi  and\n      Chen, Muhao  and\n      Sun, Huan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.152/\",\n    doi = \"10.18653/v1/2024.naacl-long.152\",\n    pages = \"2775--2792\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.152.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.152/",
        "pdf_size": 2509050,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12465960193370473778&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "The Ohio State University; The Ohio State University; University of California, Davis; The Ohio State University",
        "aff_domain": "osu.edu;osu.edu;ucdavis.edu;osu.edu",
        "email": "osu.edu;osu.edu;ucdavis.edu;osu.edu",
        "github": "https://github.com/OSU-NLP-Group/Eval-LLM-Trust",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Ohio State University;University of California, Davis",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.osu.edu;https://www.ucdavis.edu",
        "aff_unique_abbr": "OSU;UC Davis",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Davis",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.135",
        "title": "How Well Do Large Language Models Truly Ground?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "To reduce issues like hallucinations and lack of control in Large Language Models (LLMs), a common method is to generate responses by grounding on external contexts given as input, known as knowledge-augmented models. However, previous research often narrowly defines \u201cgrounding\u201d as just having the correct answer, which does not ensure the reliability of the entire response. To overcome this, we propose a stricter definition of grounding: a model is truly grounded if it (1) fully utilizes the necessary knowledge from the provided context, and (2) stays within the limits of that knowledge. We introduce a new dataset and a grounding metric to evaluate model capability under the definition. We perform experiments across 25 LLMs of different sizes and training methods and provide insights into factors that influence grounding performance. Our findings contribute to a better understanding of how to improve grounding capabilities and suggest an area of improvement toward more reliable and controllable LLM applications.",
        "author": "Hyunji Lee; Se June Joo; Chaeeun Kim; Joel Jang; Doyoung Kim; Kyoung-Woon On; Minjoon Seo",
        "authorids": "/h/hyunji-lee/; /s/se-june-joo/; /c/chaeeun-kim/; /j/joel-jang/; /d/doyoung-kim/; /k/kyoung-woon-on/; /m/minjoon-seo/",
        "bibtex": "@inproceedings{lee-etal-2024-well,\n    title = \"How Well Do Large Language Models Truly Ground?\",\n    author = \"Lee, Hyunji  and\n      Joo, Se June  and\n      Kim, Chaeeun  and\n      Jang, Joel  and\n      Kim, Doyoung  and\n      On, Kyoung-Woon  and\n      Seo, Minjoon\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.135/\",\n    doi = \"10.18653/v1/2024.naacl-long.135\",\n    pages = \"2437--2465\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.135.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.135/",
        "pdf_size": 3708896,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10602745148919884223&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "KAIST AI; KAIST AI; KAIST AI+Kakao Brain; University of Washington; KAIST AI; Kakao Brain; KAIST AI",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr; ;uw.edu;kaist.ac.kr; ;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr; ;uw.edu;kaist.ac.kr; ;kaist.ac.kr",
        "github": "https://github.com/kaistAI/How-Well-Do-LLMs-Truly-Ground",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0+1;2;0;1;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Kakao Brain;University of Washington",
        "aff_unique_dep": "KAIST AI;;",
        "aff_unique_url": "https://www.kaist.edu;https://brain.kakao.com;https://www.washington.edu",
        "aff_unique_abbr": "KAIST;Kakao Brain;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;1;0;0;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2024.naacl-long.325",
        "title": "How are Prompts Different in Terms of Sensitivity?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In-context learning (ICL) has become one of the most popular learning paradigms. While there is a growing body of literature focusing on prompt engineering, there is a lack of systematic analysis comparing the effects of prompt techniques across different models and tasks. To address this, we present a comprehensive prompt analysis based on sensitivity. Our analysis reveals that sensitivity is an unsupervised proxy for model performance, as it exhibits a strong negative correlation with accuracy. We use gradient-based saliency scores to empirically demonstrate how different prompts affect the relevance of input tokens to the output, resulting in different levels of sensitivity. Furthermore, we introduce sensitivity-aware decoding which incorporates sensitivity estimation as a penalty term in the standard greedy decoding. We show that this approach is particularly helpful when information in the input is scarce. Our work provides a fresh perspective on the analysis of prompts, and contributes to a better understanding of the mechanism of ICL.",
        "author": "Sheng Lu; Hendrik Schuff; Iryna Gurevych",
        "authorids": "/s/sheng-lu/; /h/hendrik-schuff/; /i/iryna-gurevych/",
        "bibtex": "@inproceedings{lu-etal-2024-prompts,\n    title = \"How are Prompts Different in Terms of Sensitivity?\",\n    author = \"Lu, Sheng  and\n      Schuff, Hendrik  and\n      Gurevych, Iryna\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.325/\",\n    doi = \"10.18653/v1/2024.naacl-long.325\",\n    pages = \"5833--5856\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.325.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.325/",
        "pdf_size": 2941032,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12083422464845796921&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.naacl-long.414",
        "title": "How did we get here? Summarizing conversation dynamics",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Throughout a conversation, the way participants interact with each other is in constant flux: their tones may change, they may resort to different strategies to convey their points, or they might alter their interaction patterns. An understanding of these dynamics can complement that of the actual facts and opinions discussed, offering a more holistic view of the trajectory of the conversation: how it arrived at its current state and where it is likely heading.In this work, we introduce the task of summarizing the dynamics of conversations, by constructing a dataset of human-written summaries, and exploring several automated baselines. We evaluate whether such summaries can capture the trajectory of conversations via an established downstream task: forecasting whether an ongoing conversation will eventually derail into toxic behavior. We show that they help both humans and automated systems with this forecasting task. Humans make predictions three times faster, and with greater confidence, when reading the summaries than when reading the transcripts. Furthermore, automated forecasting systems are more accurate when constructing, and then predicting based on, summaries of conversation dynamics, compared to directly predicting on the transcripts.",
        "author": "Yilun Hua; Nicholas Chernogor; Yuzhe Gu; Seoyeon Jeong; Miranda Luo; Cristian Danescu-Niculescu-Mizil",
        "authorids": "/y/yilun-hua/; /n/nicholas-chernogor/; /y/yuzhe-gu/; /s/seoyeon-jeong/; /m/miranda-luo/; /c/cristian-danescu-niculescu-mizil/",
        "bibtex": "@inproceedings{hua-etal-2024-get,\n    title = \"How did we get here? Summarizing conversation dynamics\",\n    author = \"Hua, Yilun  and\n      Chernogor, Nicholas  and\n      Gu, Yuzhe  and\n      Jeong, Seoyeon  and\n      Luo, Miranda  and\n      Danescu-Niculescu-Mizil, Cristian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.414/\",\n    doi = \"10.18653/v1/2024.naacl-long.414\",\n    pages = \"7452--7477\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.414.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.414/",
        "pdf_size": 248490,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8580035253081494731&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-short.15",
        "title": "How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Large language models (LLM) have recently shown the extraordinary ability to perform unseen tasks based on few-shot examples provided as text, also known as in-context learning (ICL). While recent works have attempted to understand the mechanisms driving ICL, few have explored training strategies that incentivize these models to generalize to multiple tasks. Multi-task learning (MTL) for generalist models is a promising direction that offers transfer learning potential, enabling large parameterized models to be trained from simpler, related tasks. In this work, we investigate the combination of MTL with ICL to build models that efficiently learn tasks while being robust to out-of-distribution examples. We propose several effective curriculum learning strategies that allow ICL models to achieve higher data efficiency and more stable convergence. Our experiments reveal that ICL models can effectively learn difficult tasks by training on progressively harder tasks while mixing in prior tasks, denoted as mixed curriculum in this work.",
        "author": "Harmon Bhasin; Timothy Ossowski; Yiqiao Zhong; Junjie Hu",
        "authorids": "/h/harmon-bhasin/; /t/timothy-ossowski/; /y/yiqiao-zhong/; /j/junjie-hu/",
        "bibtex": "@inproceedings{bhasin-etal-2024-multi,\n    title = \"How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes\",\n    author = \"Bhasin, Harmon  and\n      Ossowski, Timothy  and\n      Zhong, Yiqiao  and\n      Hu, Junjie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.15/\",\n    doi = \"10.18653/v1/2024.naacl-short.15\",\n    pages = \"169--187\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.15.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.15/",
        "pdf_size": 5005336,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_WCiSbVXB-oJ:scholar.google.com/&scioq=How+does+Multi-Task+Training+Affect+Transformer+In-Context+Capabilities%3F+Investigations+with+Function+Classes&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "University of Wisconsin, Madison, WI, USA; University of Wisconsin, Madison, WI, USA; University of Wisconsin, Madison, WI, USA; University of Wisconsin, Madison, WI, USA",
        "aff_domain": "wisc.edu;wisc.edu;wisc.edu;wisc.edu",
        "email": "wisc.edu;wisc.edu;wisc.edu;wisc.edu",
        "github": "https://github.com/harmonbhasin/curriculum_learning_icl",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Wisconsin-Madison",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW-Madison",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.456",
        "title": "HumanRankEval: Automatic Evaluation of LMs as Conversational Assistants",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Language models (LMs) as conversational assistants recently became popular tools that help people accomplish a variety of tasks. These typically result from adapting LMs pretrained on general domain text sequences through further instruction-tuning and possibly preference optimisation methods. The evaluation of such LMs would ideally be performed using human judgement, however, this is not scalable. On the other hand, automatic evaluation featuring auxiliary LMs as judges and/or knowledge-based tasks is scalable but struggles with assessing conversational ability and adherence to instructions. To help accelerate the development of LMs as conversational assistants, we propose a novel automatic evaluation task: HumanRankEval (HRE). It consists of a large-scale, diverse and high-quality set of questions, each with several answers authored and scored by humans. To perform evaluation, HRE ranks these answers based on their log-likelihood under the LM\u2019s distribution, and subsequently calculates their correlation with the corresponding human rankings. We support HRE\u2019s efficacy by investigating how efficiently it separates pretrained and instruction-tuned LMs of various sizes. We show that HRE correlates well with human judgements and is particularly responsive to model changes following instruction-tuning.",
        "author": "Milan Gritta; Gerasimos Lampouras; Ignacio Iacobacci",
        "authorids": "/m/milan-gritta/; /g/gerasimos-lampouras/; /i/ignacio-iacobacci/",
        "bibtex": "@inproceedings{gritta-etal-2024-humanrankeval,\n    title = \"{H}uman{R}ank{E}val: Automatic Evaluation of {LM}s as Conversational Assistants\",\n    author = \"Gritta, Milan  and\n      Lampouras, Gerasimos  and\n      Iacobacci, Ignacio\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.456/\",\n    doi = \"10.18653/v1/2024.naacl-long.456\",\n    pages = \"8237--8249\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.456.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.456/",
        "pdf_size": 739892,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7045272457917799440&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.findings-naacl.109",
        "title": "Hypernetwork-Assisted Parameter-Efficient Fine-Tuning with Meta-Knowledge Distillation for Domain Knowledge Disentanglement",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Domain adaptation from labeled source domains to the target domain is important in practical summarization scenarios. However, the key challenge is domain knowledge disentanglement. In this work, we explore how to disentangle domain-invariant knowledge from source domains while learning specific knowledge of the target domain. Specifically, we propose a hypernetwork-assisted encoder-decoder architecture with parameter-efficient fine-tuning. It leverages a hypernetwork instruction learning module to generate domain-specific parameters from the encoded inputs accompanied by task-related instruction. Further, to better disentangle and transfer knowledge from source domains to the target domain, we introduce a meta-knowledge distillation strategy to build a meta-teacher model that captures domain-invariant knowledge across multiple domains and use it to transfer knowledge to students. Experiments on three dialogue summarization datasets show the effectiveness of the proposed model. Human evaluations also show the superiority of our model with regard to the summary generation quality.",
        "author": "Changqun Li; Linlin Wang; Xin Lin; Shizhou Huang; Liang He",
        "authorids": "/c/changqun-li/; /l/linlin-wang/; /x/xin-lin/; /s/shizhou-huang/; /l/liang-he/",
        "bibtex": "@inproceedings{li-etal-2024-hypernetwork,\n    title = \"Hypernetwork-Assisted Parameter-Efficient Fine-Tuning with Meta-Knowledge Distillation for Domain Knowledge Disentanglement\",\n    author = \"Li, Changqun  and\n      Wang, Linlin  and\n      Lin, Xin  and\n      Huang, Shizhou  and\n      He, Liang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.109/\",\n    doi = \"10.18653/v1/2024.findings-naacl.109\",\n    pages = \"1681--1695\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.109.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.109/",
        "pdf_size": 571849,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8046568073215978290&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science and Technology, East China Normal University; School of Computer Science and Technology, East China Normal University + Shanghai Artificial Intelligence Laboratory; School of Computer Science and Technology, East China Normal University + Shanghai Key Laboraiory of Multdimensional lnfomation Procesing; School of Computer Science and Technology, East China Normal University; School of Computer Science and Technology, East China Normal University + Shanghai Key Laboraiory of Multdimensional lnfomation Procesing",
        "aff_domain": "stu.ecnu.edu.cn;cs.ecnu.edu.cn;cs.ecnu.edu.cn;ica.stc.sh.cn;cs.ecnu.edu.cn",
        "email": "stu.ecnu.edu.cn;cs.ecnu.edu.cn;cs.ecnu.edu.cn;ica.stc.sh.cn;cs.ecnu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0+2;0;0+2",
        "aff_unique_norm": "East China Normal University;Shanghai Artificial Intelligence Laboratory;Shanghai Key Laboratory of Multidimensional Information Processing",
        "aff_unique_dep": "School of Computer Science and Technology;;Shanghai Key Laboratory of Multidimensional Information Processing",
        "aff_unique_url": "http://www.ecnu.edu.cn;http://www.shailab.org/;",
        "aff_unique_abbr": "ECNU;Shanghai AI Lab;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.468",
        "title": "ICLE++: Modeling Fine-Grained Traits for Holistic Essay Scoring",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The majority of the recently developed models for automated essay scoring (AES) are evaluated solely on the ASAP corpus. However, ASAP is not without its limitations. For instance, it is not clear whether models trained on ASAP can generalize well when evaluated on other corpora. In light of these limitations, we introduce ICLE++, a corpus of persuasive student essays annotated with both holistic scores and trait-specific scores. Not only can ICLE++ be used to test the generalizability of AES models trained on ASAP, but it can also facilitate the evaluation of models developed for newer AES problems such as multi-trait scoring and cross-prompt scoring. We believe that ICLE++, which represents a culmination of our long-term effort in annotating the essays in the ICLE corpus, contributes to the set of much-needed annotated corpora for AES research.",
        "author": "Shengjie Li; Vincent Ng",
        "authorids": "/s/shengjie-li/; /v/vincent-ng/",
        "bibtex": "@inproceedings{li-ng-2024-icle,\n    title = \"{ICLE}++: Modeling Fine-Grained Traits for Holistic Essay Scoring\",\n    author = \"Li, Shengjie  and\n      Ng, Vincent\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.468/\",\n    doi = \"10.18653/v1/2024.naacl-long.468\",\n    pages = \"8465--8486\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.468.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.468/",
        "pdf_size": 271469,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11537292014889462291&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2024.findings-naacl.134",
        "title": "ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper focuses on the task of Extreme Multi-Label Classification (XMC) whose goal is to predict multiple labels for each instance from an extremely large label space. While existing research has primarily focused on fully supervised XMC, real-world scenarios often lack supervision signals, highlighting the importance of zero-shot settings. Given the large label space, utilizing in-context learning approaches is not trivial. We address this issue by introducing In-Context Extreme Multi-label Learning (ICXML), a two-stage framework that cuts down the search space by generating a set of candidate labels through in-context learning and then reranks them. Extensive experiments suggest that ICXML advances the state of the art on two diverse public benchmarks.",
        "author": "Yaxin Zhu; Hamed Zamani",
        "authorids": "/y/yaxin-zhu/; /h/hamed-zamani/",
        "bibtex": "@inproceedings{zhu-zamani-2024-icxml,\n    title = \"{ICXML}: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification\",\n    author = \"Zhu, Yaxin  and\n      Zamani, Hamed\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.134/\",\n    doi = \"10.18653/v1/2024.findings-naacl.134\",\n    pages = \"2086--2098\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.134.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.134/",
        "pdf_size": 677885,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9159919566279350670&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Center for Intelligent Information Retrieval, University of Massachusetts Amherst; Center for Intelligent Information Retrieval, University of Massachusetts Amherst",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "Center for Intelligent Information Retrieval",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.114",
        "title": "IPED: An Implicit Perspective for Relational Triple Extraction based on Diffusion Model",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Relational triple extraction is a fundamental task in the field of information extraction, and a promising framework based on table filling has recently gained attention as a potential baseline for entity relation extraction. However, inherent shortcomings such as redundant information and incomplete triple recognition remain problematic. To address these challenges, we propose an Implicit Perspective for relational triple Extraction based on Diffusion model (IPED), an innovative approach for extracting relational triples. Our classifier-free solution adopts an implicit strategy using block coverage to complete the tables, avoiding the limitations of explicit tagging methods. Additionally, we introduce a generative model structure, the block-denoising diffusion model, to collaborate with our implicit perspective and effectively circumvent redundant information disruptions. Experimental results on two popular datasets demonstrate that IPED achieves state-of-the-art performance while gaining superior inference speed and low computational complexity. To support future research, we have made our source code publicly available online.",
        "author": "Jianli Zhao; Changhao Xu; Bin. Jiang",
        "authorids": "/j/jianli-zhao/; /c/changhao-xu/; /b/bin-jiang/",
        "bibtex": "@inproceedings{zhao-etal-2024-iped,\n    title = \"{IPED}: An Implicit Perspective for Relational Triple Extraction based on Diffusion Model\",\n    author = \"Zhao, Jianli  and\n      Xu, Changhao  and\n      Jiang, Bin.\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.114/\",\n    doi = \"10.18653/v1/2024.naacl-long.114\",\n    pages = \"2080--2092\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.114.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.114/",
        "pdf_size": 821081,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:0jGdD81hHHAJ:scholar.google.com/&scioq=IPED:+An+Implicit+Perspective+for+Relational+Triple+Extraction+based+on+Diffusion+Model&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "School of Mechanical, Electrical & Information Engineering, Shandong University; School of Mechanical, Electrical & Information Engineering, Shandong University; School of Mechanical, Electrical & Information Engineering, Shandong University",
        "aff_domain": "mail.sdu.edu.cn;mail.sdu.edu.cn;sdu.edu.cn",
        "email": "mail.sdu.edu.cn;mail.sdu.edu.cn;sdu.edu.cn",
        "github": "https://github.com/girlsuuu/IPED",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Shandong University",
        "aff_unique_dep": "School of Mechanical, Electrical & Information Engineering",
        "aff_unique_url": "https://www.sdu.edu.cn",
        "aff_unique_abbr": "SDU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.85",
        "title": "Identifying Linear Relational Concepts in Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts by first modeling the relation between subject and object as a linear relational embedding (LRE). We find that inverting the LRE and using earlier object layers results in a powerful technique for finding concept directions that outperforms standard black-box probing classifiers. We evaluate LRCs on their performance as concept classifiers as well as their ability to causally change model output.",
        "author": "David Chanin; Anthony Hunter; Oana-Maria Camburu",
        "authorids": "/d/david-chanin/; /a/anthony-hunter/; /o/oana-maria-camburu/",
        "bibtex": "@inproceedings{chanin-etal-2024-identifying,\n    title = \"Identifying Linear Relational Concepts in Large Language Models\",\n    author = \"Chanin, David  and\n      Hunter, Anthony  and\n      Camburu, Oana-Maria\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.85/\",\n    doi = \"10.18653/v1/2024.naacl-long.85\",\n    pages = \"1524--1535\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.85.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.85/",
        "pdf_size": 621211,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1753087337849979727&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.findings-naacl.161",
        "title": "Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In the last decade, the United States has lost more than 500,000 people from an overdose involving prescription and illicit opioids making it a national public health emergency (USDHHS, 2017). Medical practitioners require robust and timely tools that can effectively identify at-risk patients. Community-based social media platforms such as Reddit allow self-disclosure for users to discuss otherwise sensitive drug-related behaviors. We present a moderate size corpus of 2500 opioid-related posts from various subreddits labeled with six different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of opioid use disorder is highly contextual and challenging. However, we find that using explanations during modeling leads to a significant boost in classification accuracy demonstrating their beneficial role in a high-stakes domain such as studying the opioid use disorder continuum.",
        "author": "Chenghao Yang; Tuhin Chakrabarty; Karli Hochstatter; Melissa Slavin; Nabila El-Bassel; Smaranda Muresan",
        "authorids": "/c/chenghao-yang/; /t/tuhin-chakrabarty/; /k/karli-hochstatter/; /m/melissa-slavin/; /n/nabila-el-bassel/; /s/smaranda-muresan/",
        "bibtex": "@inproceedings{yang-etal-2024-identifying,\n    title = \"Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts\",\n    author = \"Yang, Chenghao  and\n      Chakrabarty, Tuhin  and\n      Hochstatter, Karli  and\n      Slavin, Melissa  and\n      El-Bassel, Nabila  and\n      Muresan, Smaranda\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.161/\",\n    doi = \"10.18653/v1/2024.findings-naacl.161\",\n    pages = \"2507--2521\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.161.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.161/",
        "pdf_size": 249433,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=976375162309200301&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Chicago; Department of Computer Science, Columbia University; Friends Research Institute; Fairleigh Dickinson University; School of Social Work, Columbia University; Data Science Institute, Columbia University",
        "aff_domain": "uchicago.edu;columbia.edu;columbia.edu; ; ; ",
        "email": "uchicago.edu;columbia.edu;columbia.edu; ; ; ",
        "github": "https://github.com/yangalan123/OpioidID",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;3;1;1",
        "aff_unique_norm": "University of Chicago;Columbia University;Friends Research Institute;Fairleigh Dickinson University",
        "aff_unique_dep": ";Department of Computer Science;;",
        "aff_unique_url": "https://www.uchicago.edu;https://www.columbia.edu;https://www.friendsresearch.org;https://www.fdu.edu",
        "aff_unique_abbr": "UChicago;Columbia;;FDU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.3",
        "title": "Ignore Me But Don\u2019t Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Cybersecurity information is often technically complex and relayed through unstructured text, making automation of cyber threat intelligence highly challenging. For such text domains that involve high levels of expertise, pretraining on in-domain corpora has been a popular method for language models to obtain domain expertise. However, cybersecurity texts often contain non-linguistic elements (such as URLs and hash values) that could be unsuitable with the established pretraining methodologies. Previous work in other domains have removed or filtered such text as noise, but the effectiveness of these methods have not been investigated, especially in the cybersecurity domain. We experiment with different pretraining methodologies to account for non-linguistic elements (NLEs) and evaluate their effectiveness through downstream tasks and probing tasks. Our proposed strategy, a combination of selective MLM and jointly training NLE token classification, outperforms the commonly taken approach of replacing NLEs. We use our domain-customized methodology to train CyBERTuned, a cybersecurity domain language model that outperforms other cybersecurity PLMs on most tasks.",
        "author": "Eugene Jang; Jian Cui; Dayeon Yim; Youngjin Jin; Jin-Woo Chung; Seungwon Shin; Yongjae Lee",
        "authorids": "/e/eugene-jang/; /j/jian-cui/; /d/dayeon-yim/; /y/youngjin-jin/; /j/jin-woo-chung/; /s/seungwon-shin/; /y/yongjae-lee/",
        "bibtex": "@inproceedings{jang-etal-2024-ignore,\n    title = \"Ignore Me But Don{'}t Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain\",\n    author = \"Jang, Eugene  and\n      Cui, Jian  and\n      Yim, Dayeon  and\n      Jin, Youngjin  and\n      Chung, Jin-Woo  and\n      Shin, Seungwon  and\n      Lee, Yongjae\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.3/\",\n    doi = \"10.18653/v1/2024.findings-naacl.3\",\n    pages = \"29--42\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.3.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.3/",
        "pdf_size": 470954,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8736270028565963832&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "S2W Inc.+Indiana University Bloomington; Indiana University Bloomington; S2W Inc.; KAIST; S2W Inc.; KAIST; S2W Inc.",
        "aff_domain": "s2w.inc;iu.edu;s2w.inc;kaist.ac.kr;s2w.inc;kaist.ac.kr;s2w.inc",
        "email": "s2w.inc;iu.edu;s2w.inc;kaist.ac.kr;s2w.inc;kaist.ac.kr;s2w.inc",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;0;2;0;2;0",
        "aff_unique_norm": "S2W Inc.;Indiana University;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.indiana.edu;https://www.kaist.ac.kr",
        "aff_unique_abbr": ";IU;KAIST",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Bloomington",
        "aff_country_unique_index": "0+0;0;0;1;0;1;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "2024.naacl-long.250",
        "title": "Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Model",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present Impossible Distillation, a novel framework for paraphrasing and sentence summarization, that distills a high-quality dataset and model from a low-quality teacher that itself cannot perform these tasks. Unlike prior works that rely on an extreme-scale teacher model (e.g., GPT3) or task-specific architecture, we hypothesize and verify the paraphrastic proximity intrinsic to pre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in the LM distribution. By identifying and distilling generations from these subspaces, Impossible Distillation produces a high-quality dataset and model even from GPT2-scale LMs. We evaluate our method on multiple benchmarks spanning unconstrained / syntax-controlled paraphrase generation and sentence summarization. Our model with 770M parameters consistently outperforms strong baselines, including models distilled from ChatGPT, and sometimes, even ChatGPT itself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher diversity and fidelity than up to 13 times larger datasets.",
        "author": "Jaehun Jung; Peter West; Liwei Jiang; Faeze Brahman; Ximing Lu; Jillian Fisher; Taylor Sorensen; Yejin Choi",
        "authorids": "/j/jaehun-jung/; /p/peter-west/; /l/liwei-jiang/; /f/faeze-brahman/; /x/ximing-lu/; /j/jillian-fisher/; /t/taylor-sorensen/; /y/yejin-choi/",
        "bibtex": "@inproceedings{jung-etal-2024-impossible,\n    title = \"Impossible Distillation for Paraphrasing and Summarization: How to Make High-quality Lemonade out of Small, Low-quality Model\",\n    author = \"Jung, Jaehun  and\n      West, Peter  and\n      Jiang, Liwei  and\n      Brahman, Faeze  and\n      Lu, Ximing  and\n      Fisher, Jillian  and\n      Sorensen, Taylor  and\n      Choi, Yejin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.250/\",\n    doi = \"10.18653/v1/2024.naacl-long.250\",\n    pages = \"4439--4454\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.250.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.250/",
        "pdf_size": 1897073,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16080538528977289864&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for Artificial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for Artificial Intelligence; Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington; Paul G. Allen School of Computer Science & Engineering, University of Washington+Allen Institute for Artificial Intelligence",
        "aff_domain": "cs.washington.edu; ; ; ; ; ; ; ",
        "email": "cs.washington.edu; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0;0;0+1;0;0;0;0+1",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": "Paul G. Allen School of Computer Science & Engineering;",
        "aff_unique_url": "https://www.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UW;AI2",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0+0;0;0;0+0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.72",
        "title": "Improved Text Emotion Prediction Using Combined Valence and Arousal Ordinal Classification",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Emotion detection in textual data has received growing interest in recent years, as it is pivotal for developing empathetic human-computer interaction systems.This paper introduces a method for categorizing emotions from text, which acknowledges and differentiates between the diversified similarities and distinctions of various emotions.Initially, we establish a baseline by training a transformer-based model for standard emotion classification, achieving state-of-the-art performance. We argue that not all misclassifications are of the same importance, as there are perceptual similarities among emotional classes.We thus redefine the emotion labeling problem by shifting it from a traditional classification model to an ordinal classification one, where discrete emotions are arranged in a sequential order according to their valence levels.Finally, we propose a method that performs ordinal classification in the two-dimensional emotion space, considering both valence and arousal scales.The results show that our approach not only preserves high accuracy in emotion prediction but also significantly reduces the magnitude of errors in cases of misclassification.",
        "author": "Michail Mitsios; Georgios Vamvoukakis; Georgia Maniati; Nikolaos Ellinas; Georgios Dimitriou; Konstantinos Markopoulos; Panos Kakoulidis; Alexandra Vioni; Myrsini Christidou; Junkwang Oh; Gunu Jho; Inchul Hwang; Georgios Vardaxoglou; Aimilios Chalamandaris; Pirros Tsiakoulis; Spyros Raptis",
        "authorids": "/m/michail-mitsios/; /g/georgios-vamvoukakis/; /g/georgia-maniati/; /n/nikolaos-ellinas/; /g/georgios-dimitriou/; /k/konstantinos-markopoulos/; /p/panos-kakoulidis/; /a/alexandra-vioni/; /m/myrsini-christidou/; /j/junkwang-oh/; /g/gunu-jho/; /i/inchul-hwang/; /g/georgios-vardaxoglou/; /a/aimilios-chalamandaris/; /p/pirros-tsiakoulis/; /s/spyros-raptis/",
        "bibtex": "@inproceedings{mitsios-etal-2024-improved,\n    title = \"Improved Text Emotion Prediction Using Combined Valence and Arousal Ordinal Classification\",\n    author = \"Mitsios, Michail  and\n      Vamvoukakis, Georgios  and\n      Maniati, Georgia  and\n      Ellinas, Nikolaos  and\n      Dimitriou, Georgios  and\n      Markopoulos, Konstantinos  and\n      Kakoulidis, Panos  and\n      Vioni, Alexandra  and\n      Christidou, Myrsini  and\n      Oh, Junkwang  and\n      Jho, Gunu  and\n      Hwang, Inchul  and\n      Vardaxoglou, Georgios  and\n      Chalamandaris, Aimilios  and\n      Tsiakoulis, Pirros  and\n      Raptis, Spyros\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.72/\",\n    doi = \"10.18653/v1/2024.naacl-short.72\",\n    pages = \"808--813\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.72.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.72/",
        "pdf_size": 557069,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3412181470883642378&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 16
    },
    {
        "id": "2024.findings-naacl.102",
        "title": "Improving Absent Keyphrase Generation with Diversity Heads",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Keyphrase Generation (KPG) is the task of automatically generating appropriate keyphrases for a given text, with a wide range of real-world applications such as document indexing and tagging, information retrieval, and text summarization. NLP research makes a distinction between present and absent keyphrases based on whether a keyphrase is directly present as a sequence of words in the document during evaluation. However, present and absent keyphrases are treated together in a text-to-text generation framework during training. We treat present keyphrase extraction as a sequence labeling problem and propose a new absent keyphrase generation model that uses a modified cross-attention layer with additional heads to capture diverse views for the same context encoding in this paper. Our experiments show improvements over the state-of-the-art for four datasets for present keyphrase extraction and five datasets for absent keyphrase generation among the six English datasets we explored, covering long and short documents.",
        "author": "Edwin Thomas; Sowmya Vajjala",
        "authorids": "/e/edwin-thomas/; /s/sowmya-vajjala/",
        "bibtex": "@inproceedings{thomas-vajjala-2024-improving,\n    title = \"Improving Absent Keyphrase Generation with Diversity Heads\",\n    author = \"Thomas, Edwin  and\n      Vajjala, Sowmya\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.102/\",\n    doi = \"10.18653/v1/2024.findings-naacl.102\",\n    pages = \"1568--1584\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.102.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.102/",
        "pdf_size": 1167840,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:z4PE5_Ron0YJ:scholar.google.com/&scioq=Improving+Absent+Keyphrase+Generation+with+Diversity+Heads&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Ottawa; National Research Council",
        "aff_domain": "uottawa.ca;nrc-cnrc.gc.ca",
        "email": "uottawa.ca;nrc-cnrc.gc.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Ottawa;National Research Council",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uottawa.ca;https://www.nrc-cnrc.gc.ca",
        "aff_unique_abbr": "U Ottawa;NRC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-long.248",
        "title": "Improving Adversarial Data Collection by Supporting Annotators: Lessons from GAHD, a German Hate Speech Dataset",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Hate speech detection models are only as good as the data they are trained on. Datasets sourced from social media suffer from systematic gaps and biases, leading to unreliable models with simplistic decision boundaries. Adversarial datasets, collected by exploiting model weaknesses, promise to fix this problem. However, adversarial data collection can be slow and costly, and individual annotators have limited creativity. In this paper, we introduce GAHD, a new German Adversarial Hate speech Dataset comprising ca. 11k examples. During data collection, we explore new strategies for supporting annotators, to create more diverse adversarial examples more efficiently and provide a manual analysis of annotator disagreements for each strategy. Our experiments show that the resulting dataset is challenging even for state-of-the-art hate speech detection models, and that training on GAHD clearly improves model robustness. Further, we find that mixing multiple support strategies is most advantageous. We make GAHD publicly available at https://github.com/jagol/gahd.",
        "author": "Janis Goldzycher; Paul R\u00f6ttger; Gerold Schneider",
        "authorids": "/j/janis-goldzycher/; /p/paul-rottger/; /g/gerold-schneider/",
        "bibtex": "@inproceedings{goldzycher-etal-2024-improving,\n    title = \"Improving Adversarial Data Collection by Supporting Annotators: Lessons from {GAHD}, a {G}erman Hate Speech Dataset\",\n    author = {Goldzycher, Janis  and\n      R{\\\"o}ttger, Paul  and\n      Schneider, Gerold},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.248/\",\n    doi = \"10.18653/v1/2024.naacl-long.248\",\n    pages = \"4405--4424\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.248.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.248/",
        "pdf_size": 660261,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10953900933247743052&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Zurich, Zurich, Switzerland; Bocconi University, Milan, Italy; University of Zurich, Zurich, Switzerland",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "https://github.com/jagol/gahd",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Zurich;Bocconi University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unizh.ch;https://www.bocconi.edu",
        "aff_unique_abbr": "UZH;Bocconi",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Zurich;Milan",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Switzerland;Italy"
    },
    {
        "id": "2024.naacl-long.408",
        "title": "Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input Problems in ToTTo",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Neural Table-to-Text models tend to hallucinate, producing texts that contain factual errors. We investigate whether such errors in the output can be traced back to problems with the input. We manually annotated 1,837 texts generated by multiple models in the politics domain of the ToTTo dataset. We identify the input problems that are responsible for many output errors and show that fixing these inputs reduces factual errors by between 52% and 76% (depending on the model). In addition, we observe that models struggle in processing tabular inputs that are structured in a non-standard way, particularly when the input lacks distinct row and column values or when the column headers are not correctly mapped to corresponding values.",
        "author": "Barkavi Sundararajan; Yaji Sripada; Ehud Reiter",
        "authorids": "/b/barkavi-sundararajan/; /y/yaji-sripada/; /e/ehud-reiter/",
        "bibtex": "@inproceedings{sundararajan-etal-2024-improving,\n    title = \"Improving Factual Accuracy of Neural Table-to-Text Output by Addressing Input Problems in {T}o{TT}o\",\n    author = \"Sundararajan, Barkavi  and\n      Sripada, Yaji  and\n      Reiter, Ehud\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.408/\",\n    doi = \"10.18653/v1/2024.naacl-long.408\",\n    pages = \"7350--7376\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.408.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.408/",
        "pdf_size": 320478,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18284181794557232202&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computing Science, University of Aberdeen; Department of Computing Science, University of Aberdeen; Department of Computing Science, University of Aberdeen",
        "aff_domain": "abdn.ac.uk;abdn.ac.uk;abdn.ac.uk",
        "email": "abdn.ac.uk;abdn.ac.uk;abdn.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Aberdeen",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.abdn.ac.uk",
        "aff_unique_abbr": "Aberdeen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-short.66",
        "title": "Improving Factuality in Clinical Abstractive Multi-Document Summarization by Guided Continued Pre-training",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Factual accuracy is an important property of neural abstractive summarization models, especially in fact-critical domains such as the clinical literature. In this work, we introduce a guided continued pre-training stage for encoder-decoder models that improves their understanding of the factual attributes of documents, which is followed by supervised fine-tuning on summarization. Our approach extends the pre-training recipe of BART to incorporate 3 additional objectives based on PICO spans, which capture the population, intervention, comparison, and outcomes related to a clinical study. Experiments on multi-document summarization in the clinical domain demonstrate that our approach is competitive with prior work, improving the quality and factuality of the summaries and achieving the best-published results in factual accuracy on the MSLR task.",
        "author": "Ahmed Elhady; Khaled Elsayed; Eneko Agirre; Mikel Artetxe",
        "authorids": "/a/ahmed-elhady/; /k/khaled-elsayed/; /e/eneko-agirre/; /m/mikel-artetxe/",
        "bibtex": "@inproceedings{elhady-etal-2024-improving,\n    title = \"Improving Factuality in Clinical Abstractive Multi-Document Summarization by Guided Continued Pre-training\",\n    author = \"Elhady, Ahmed  and\n      Elsayed, Khaled  and\n      Agirre, Eneko  and\n      Artetxe, Mikel\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.66/\",\n    doi = \"10.18653/v1/2024.naacl-short.66\",\n    pages = \"755--761\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.66.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.66/",
        "pdf_size": 226606,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:CyfthtRwtuwJ:scholar.google.com/&scioq=Improving+Factuality+in+Clinical+Abstractive+Multi-Document+Summarization+by+Guided+Continued+Pre-training&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "HiTZ Center, University of the Basque Country (UPV/EHU)+Reka AI; University of Science and Technology, Zewail City; HiTZ Center, University of the Basque Country (UPV/EHU); HiTZ Center, University of the Basque Country (UPV/EHU)+Reka AI",
        "aff_domain": "ehu.eus;zewailcity.edu.eg;ehu.eus;ehu.eus",
        "email": "ehu.eus;zewailcity.edu.eg;ehu.eus;ehu.eus",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2;0;0+1",
        "aff_unique_norm": "University of the Basque Country;Reka AI;University of Science and Technology",
        "aff_unique_dep": "HiTZ Center;;",
        "aff_unique_url": "https://www.ehu.eus/en;https://www.reka.ai;https://www.zewailcity.edu.eg",
        "aff_unique_abbr": "UPV/EHU;Reka AI;",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Zewail City",
        "aff_country_unique_index": "0+1;2;0;0+1",
        "aff_country_unique": "Spain;United States;Egypt"
    },
    {
        "id": "2024.findings-naacl.295",
        "title": "Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In today\u2019s digital world, seeking answers to health questions on the Internet is a common practice. However, existing question answering (QA) systems often rely on using pre-selected and annotated evidence documents, thus making them inadequate for addressing novel questions. Our study focuses on the open-domain QA setting, where the key challenge is to first uncover relevant evidence in large knowledge bases. By utilizing the common retrieve-then-read QA pipeline and PubMed as a trustworthy collection of medical research documents, we answer health questions from three diverse datasets. We modify different retrieval settings to observe their influence on the QA pipeline\u2019s performance, including the number of retrieved documents, sentence selection process, the publication year of articles, and their number of citations. Our results reveal that cutting down on the amount of retrieved documents and favoring more recent and highly cited documents can improve the final macro F1 score up to 10%. We discuss the results, highlight interesting examples, and outline challenges for future research, like managing evidence disagreement and crafting user-friendly explanations.",
        "author": "Juraj Vladika; Florian Matthes",
        "authorids": "/j/juraj-vladika/; /f/florian-matthes/",
        "bibtex": "@inproceedings{vladika-matthes-2024-improving,\n    title = \"Improving Health Question Answering with Reliable and Time-Aware Evidence Retrieval\",\n    author = \"Vladika, Juraj  and\n      Matthes, Florian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.295/\",\n    doi = \"10.18653/v1/2024.findings-naacl.295\",\n    pages = \"4752--4763\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.295.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.295/",
        "pdf_size": 252605,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7365283428922298265&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Technical University of Munich, Garching, Germany; Department of Computer Science, Technical University of Munich, Garching, Germany",
        "aff_domain": "tum.de;tum.de",
        "email": "tum.de;tum.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technical University of Munich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.tum.de",
        "aff_unique_abbr": "TUM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Garching",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.445",
        "title": "Improving In-context Learning of Multilingual Generative Language Models with Cross-lingual Alignment",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multilingual generative models obtain remarkable cross-lingual in-context learning capabilities through pre-training on large-scale corpora. However, they still exhibit a performance bias toward high-resource languages and learn isolated distributions of multilingual sentence representations, which may hinder knowledge transfer across languages. To bridge this gap, we propose a simple yet effective cross-lingual alignment framework exploiting pairs of translation sentences. It aligns the internal sentence representations across different languages via multilingual contrastive learning and aligns outputs by following cross-lingual instructions in the target language. Experimental results show that even with less than 0.1\u2030 of pre-training tokens, our alignment framework significantly boosts the cross-lingual abilities of generative language models and mitigates the performance gap. Further analyses reveal that it results in a better internal multilingual representation distribution of multilingual models.",
        "author": "Chong Li; Shaonan Wang; Jiajun Zhang; Chengqing Zong",
        "authorids": "/c/chong-li/; /s/shaonan-wang/; /j/jiajun-zhang/; /c/chengqing-zong/",
        "bibtex": "@inproceedings{li-etal-2024-improving-context,\n    title = \"Improving In-context Learning of Multilingual Generative Language Models with Cross-lingual Alignment\",\n    author = \"Li, Chong  and\n      Wang, Shaonan  and\n      Zhang, Jiajun  and\n      Zong, Chengqing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.445/\",\n    doi = \"10.18653/v1/2024.naacl-long.445\",\n    pages = \"8058--8076\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.445.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.445/",
        "pdf_size": 1029870,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13681151784782324229&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, CAS, Beijing, China+School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "https://github.com/chongli17/CrossLingualAlignment",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.451",
        "title": "Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Insufficient modeling of human preferences within the reward model is a major obstacle for leveraging human feedback to improve translation quality. Fortunately, quality estimation (QE), which predicts the quality of a given translation without reference, has achieved impressive alignment with human evaluations in the last two years. In this work, we investigate the potential of employing the QE model as the reward model to predict human preferences for feedback training. We first identify the overoptimization problem during QE-based feedback training, manifested as an increase in reward while translation quality declines. We examine the problem and argue that the vulnerability of the QE model might lead to high rewards for incorrect translations, resulting in overoptimization and error propagation. To address the problem, we adopt a simple yet effective method that uses heuristic rules to detect the incorrect translations and assigns a penalty term to the reward scores of them. Experimental results show that the proposed QE-based feedback training achieves consistent and significant improvements across various settings, further verified through human preference studies. Our subsequent analysis demonstrates the high data efficiency of the proposed QE-based feedback training: it outperforms systems using larger parallel corpora by a small amount of monolingual data. Our code is available at: https://github.com/zwhe99/FeedbackMT",
        "author": "Zhiwei He; Xing Wang; Wenxiang Jiao; Zhuosheng Zhang; Rui Wang; Shuming Shi; Zhaopeng Tu",
        "authorids": "/z/zhiwei-he/; /x/xing-wang/; /w/wenxiang-jiao/; /z/zhuosheng-zhang/; /r/rui-wang/; /s/shuming-shi/; /z/zhaopeng-tu/",
        "bibtex": "@inproceedings{he-etal-2024-improving,\n    title = \"Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model\",\n    author = \"He, Zhiwei  and\n      Wang, Xing  and\n      Jiao, Wenxiang  and\n      Zhang, Zhuosheng  and\n      Wang, Rui  and\n      Shi, Shuming  and\n      Tu, Zhaopeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.451/\",\n    doi = \"10.18653/v1/2024.naacl-long.451\",\n    pages = \"8164--8180\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.451.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.451/",
        "pdf_size": 934581,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15719140307223850946&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Shanghai Jiao Tong University+Tencent AI Lab; Tencent AI Lab; Tencent AI Lab; Shanghai Jiao Tong University; Shanghai Jiao Tong University+Tencent AI Lab; Tencent AI Lab; Tencent AI Lab",
        "aff_domain": "sjtu.edu.cn;tencent.com;tencent.com;sjtu.edu.cn;sjtu.edu.cn;tencent.com;tencent.com",
        "email": "sjtu.edu.cn;tencent.com;tencent.com;sjtu.edu.cn;sjtu.edu.cn;tencent.com;tencent.com",
        "github": "https://github.com/zwhe99/FeedbackMT",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;0;0+1;1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Tencent",
        "aff_unique_dep": ";Tencent AI Lab",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://ai.tencent.com",
        "aff_unique_abbr": "SJTU;Tencent AI Lab",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.280",
        "title": "Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for inaccurately predicting DS-terms compared to generic words. Results of our analysis show that MSLM improves LMs sensitivity and detection of DS-terms. We empirically show that an optimal masking rate not only depends on the LM, but also on the dataset and the length of sequences. Our proposed masking strategy outperforms advanced masking strategies such as span- and PMI-based masking.",
        "author": "Micheal Abaho; Danushka Bollegala; Gary Leeming; Dan Joyce; Iain Buchan",
        "authorids": "/m/micheal-abaho/; /d/danushka-bollegala/; /g/gary-leeming/; /d/dan-joyce/; /i/iain-buchan/",
        "bibtex": "@inproceedings{abaho-etal-2024-improving,\n    title = \"Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical {NER}\",\n    author = \"Abaho, Micheal  and\n      Bollegala, Danushka  and\n      Leeming, Gary  and\n      Joyce, Dan  and\n      Buchan, Iain\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.280/\",\n    doi = \"10.18653/v1/2024.naacl-long.280\",\n    pages = \"5013--5029\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.280.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.280/",
        "pdf_size": 1115472,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2841895546505337887&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Liverpool, United Kingdom + Civic Health Innovation Labs; University of Liverpool, United Kingdom; University of Liverpool, United Kingdom + Civic Health Innovation Labs; University of Liverpool, United Kingdom + Civic Health Innovation Labs; University of Liverpool, United Kingdom + Civic Health Innovation Labs",
        "aff_domain": "liverpool.ac.uk;liverpool.ac.uk;liverpool.ac.uk;liverpool.ac.uk;liverpool.ac.uk",
        "email": "liverpool.ac.uk;liverpool.ac.uk;liverpool.ac.uk;liverpool.ac.uk;liverpool.ac.uk",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0+1;0+1;0+1",
        "aff_unique_norm": "University of Liverpool;Civic Health Innovation Labs",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.liverpool.ac.uk;",
        "aff_unique_abbr": "Liv Uni;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;0+1;0+1;0+1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2024.naacl-short.3",
        "title": "Improving Toponym Resolution by Predicting Attributes to Constrain Geographical Ontology Entries",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Geocoding is the task of converting location mentions in text into structured geospatial data.We propose a new prompt-based paradigm for geocoding, where the machine learning algorithm encodes only the location mention and its context.We design a transformer network for predicting the country, state, and feature class of a location mention, and a deterministic algorithm that leverages the country, state, and feature class predictions as constraints in a search for compatible entries in the ontology.Our architecture, GeoPLACE, achieves new state-of-the-art performance on multiple datasets.Code and models are available at https://github.com/clulab/geonorm.",
        "author": "Zeyu Zhang; Egoitz Laparra; Steven Bethard",
        "authorids": "/z/zeyu-zhang/; /e/egoitz-laparra/; /s/steven-bethard/",
        "bibtex": "@inproceedings{zhang-etal-2024-improving-toponym,\n    title = \"Improving Toponym Resolution by Predicting Attributes to Constrain Geographical Ontology Entries\",\n    author = \"Zhang, Zeyu  and\n      Laparra, Egoitz  and\n      Bethard, Steven\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.3/\",\n    doi = \"10.18653/v1/2024.naacl-short.3\",\n    pages = \"35--44\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.3.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.3/",
        "pdf_size": 508743,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17299063325632472722&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "School of Information, University of Arizona; School of Information, University of Arizona; School of Information, University of Arizona",
        "aff_domain": "arizona.edu;arizona.edu;arizona.edu",
        "email": "arizona.edu;arizona.edu;arizona.edu",
        "github": "https://github.com/clulab/geonorm",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Arizona",
        "aff_unique_dep": "School of Information",
        "aff_unique_url": "https://www.arizona.edu",
        "aff_unique_abbr": "UArizona",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.167",
        "title": "In-Context Example Ordering Guided by Label Distributions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "By allowing models to predict without task-specific training, in-context learning (ICL) with pretrained LLMs has enormous potential in NLP. However, a number of problems persist in ICL. In particular, its performance is sensitive to the choice and order of in-context examples. Given the same set of in-context examples with different orderings, model performance may vary from near random to near state-of-the-art. In this work, we formulate in-context example ordering as an optimization problem. We examine three problem settings that differ in the assumptions they make about what is known about the task. Inspired by the idea of learning from label proportions, we propose two principles for in-context example ordering guided by model\u2019s probability predictions. We apply our proposed principles to thirteen text classification datasets and nine different autoregressive LLMs with 700M to 13B parameters. We demonstrate that our approach outperforms the baselines by improving the classification accuracy, reducing model miscalibration, and also by selecting better in-context examples.",
        "author": "Zhichao Xu; Daniel Cohen; Bei Wang; Vivek Srikumar",
        "authorids": "/z/zhichao-xu/; /d/daniel-cohen/; /b/bei-wang/; /v/vivek-srikumar/",
        "bibtex": "@inproceedings{xu-etal-2024-context,\n    title = \"In-Context Example Ordering Guided by Label Distributions\",\n    author = \"Xu, Zhichao  and\n      Cohen, Daniel  and\n      Wang, Bei  and\n      Srikumar, Vivek\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.167/\",\n    doi = \"10.18653/v1/2024.findings-naacl.167\",\n    pages = \"2623--2640\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.167.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.167/",
        "pdf_size": 447359,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8657458166683942030&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Kahlert School of Computing, University of Utah + Scientific Computing and Imaging Institute, University of Utah; Dataminr, Inc.; Kahlert School of Computing, University of Utah + Scientific Computing and Imaging Institute, University of Utah; Kahlert School of Computing, University of Utah",
        "aff_domain": "utah.edu; ; ; ",
        "email": "utah.edu; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;1;0+0;0",
        "aff_unique_norm": "University of Utah;Dataminr",
        "aff_unique_dep": "Kahlert School of Computing;",
        "aff_unique_url": "https://www.utah.edu;https://www.dataminr.com",
        "aff_unique_abbr": "U of U;Dataminr",
        "aff_campus_unique_index": "0+0;0+0;0",
        "aff_campus_unique": "Salt Lake City;",
        "aff_country_unique_index": "0+0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.267",
        "title": "In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In-context learning (ICL) is now a common method for teaching large language models (LLMs) new tasks: given labeled examples in the input context, the LLM learns to perform the task without weight updates. Do models guided via ICL infer the underlying structure of the task defined by the context, or do they rely on superficial heuristics that only generalize to identically distributed examples? We address this question using transformations tasks and an NLI task that assess sensitivity to syntax\u2014a requirement for robust language understanding. We further investigate whether out-of-distribution generalization can be improved via chain-of-thought prompting, where the model is provided with a sequence of intermediate computation steps that illustrate how the task ought to be performed. In experiments with models from the GPT, PaLM, and Llama 2 families, we find large variance across LMs. The variance is explained more by the composition of the pre-training corpus and supervision methods than by model size; in particular, models pre-trained on code generalize better, and benefit more from chain-of-thought prompting.",
        "author": "Aaron Mueller; Albert Webson; Jackson Petty; Tal Linzen",
        "authorids": "/a/aaron-mueller/; /a/albert-webson/; /j/jackson-petty/; /t/tal-linzen/",
        "bibtex": "@inproceedings{mueller-etal-2024-context,\n    title = \"In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax\",\n    author = \"Mueller, Aaron  and\n      Webson, Albert  and\n      Petty, Jackson  and\n      Linzen, Tal\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.267/\",\n    doi = \"10.18653/v1/2024.naacl-long.267\",\n    pages = \"4761--4779\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.267.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.267/",
        "pdf_size": 664724,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5272705029508690159&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Northeastern University+Technion \u2013 Israel Institute of Technology; Google DeepMind; New York University+Google Research; Google Research",
        "aff_domain": "northeastern.edu;google.com;nyu.edu;google.com",
        "email": "northeastern.edu;google.com;nyu.edu;google.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2;3+2;2",
        "aff_unique_norm": "Northeastern University;Technion \u2013 Israel Institute of Technology;Google;New York University",
        "aff_unique_dep": ";;Google DeepMind;",
        "aff_unique_url": "https://www.northeastern.edu;https://www.technion.ac.il/en/;https://deepmind.com;https://www.nyu.edu",
        "aff_unique_abbr": "NEU;Technion;DeepMind;NYU",
        "aff_campus_unique_index": ";1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0+1;2;0+0;0",
        "aff_country_unique": "United States;Israel;United Kingdom"
    },
    {
        "id": "2024.naacl-long.58",
        "title": "In-context Learning and Gradient Descent Revisited",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In-context learning (ICL) has shown impressive results in few-shot learning tasks, yet its underlying mechanism is still not fully understood. A recent line of work suggests that ICL performs gradient descent (GD)-based optimization implicitly. While appealing, much of the research focuses on simplified settings, where the parameters of a shallow model are optimized. In this work, we revisit evidence for ICL-GD correspondence on realistic NLP tasks and models. We find gaps in evaluation, both in terms of problematic metrics and insufficient baselines. We show that surprisingly, even untrained models achieve comparable ICL-GD similarity scores despite not exhibiting ICL.Next, we explore a major discrepancy in the flow of information throughout the model between ICL and GD, which we term Layer Causality. We propose a simple GD-based optimization procedure that respects layer causality, and show it improves similarity scores significantly.",
        "author": "Gilad Deutch; Nadav Magar; Tomer Natan; Guy Dar",
        "authorids": "/g/gilad-deutch/; /n/nadav-magar/; /t/tomer-natan/; /g/guy-dar/",
        "bibtex": "@inproceedings{deutch-etal-2024-context,\n    title = \"In-context Learning and Gradient Descent Revisited\",\n    author = \"Deutch, Gilad  and\n      Magar, Nadav  and\n      Natan, Tomer  and\n      Dar, Guy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.58/\",\n    doi = \"10.18653/v1/2024.naacl-long.58\",\n    pages = \"1017--1028\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.58.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.58/",
        "pdf_size": 401910,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9603317283622474842&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The Blavatnik School of Computer Science; Tel Aviv University; ; ",
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "github": "https://github.com/GiilDe/ft-vs-icl",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Blavatnik School of Computer Science;Tel Aviv University",
        "aff_unique_dep": "Computer Science;",
        "aff_unique_url": "https://www.bsg.ox.ac.uk;https://www.tau.ac.il",
        "aff_unique_abbr": ";TAU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Israel"
    },
    {
        "id": "2024.findings-naacl.23",
        "title": "Incorporating Exponential Smoothing into MLP: a Simple but Effective Sequence Model",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.",
        "author": "JiqunChu JiqunChu; Zuoquan Lin",
        "authorids": "/j/jiqunchu-jiqunchu/; /z/zuoquan-lin/",
        "bibtex": "@inproceedings{jiqunchu-lin-2024-incorporating,\n    title = \"Incorporating Exponential Smoothing into {MLP}: a Simple but Effective Sequence Model\",\n    author = \"JiqunChu, JiqunChu  and\n      Lin, Zuoquan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.23/\",\n    doi = \"10.18653/v1/2024.findings-naacl.23\",\n    pages = \"326--337\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.23.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.23/",
        "pdf_size": 497889,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1907724187441100525&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Peking University; Peking University",
        "aff_domain": "pku.edu.cn;pku.edu.cn",
        "email": "pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/PKUAI-LINGroup/ETSMLP",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "Peking U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.487",
        "title": "IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The pervasive influence of social biases in language data has sparked the need for benchmark datasets that capture and evaluate these biases in Large Language Models (LLMs). Existing efforts predominantly focus on English language and the Western context, leaving a void for a reliable dataset that encapsulates India\u2019s unique socio-cultural nuances. To bridge this gap, we introduce IndiBias, a comprehensive benchmarking dataset designed specifically for evaluating social biases in the Indian context. We filter and translate the existing CrowS-Pairs dataset to create a benchmark dataset suited to the Indian context in Hindi language. Additionally, we leverage LLMs including ChatGPT and InstructGPT to augment our dataset with diverse societal biases and stereotypes prevalent in India. The included bias dimensions encompass gender, religion, caste, age, region, physical appearance, and occupation. We also build a resource to address intersectional biases along three intersectional dimensions. Our dataset contains 800 sentence pairs and 300 tuples for bias measurement across different demographics. The dataset is available in English and Hindi, providing a size comparable to existing benchmark datasets. Furthermore, using IndiBias we compare ten different language models on multiple bias measurement metrics. We observed that the language models exhibit more bias across a majority of the intersectional groups. All the scripts utilized and datasets created in this study are publicly available.",
        "author": "Nihar Sahoo; Pranamya Kulkarni; Arif Ahmad; Tanu Goyal; Narjis Asad; Aparna Garimella; Pushpak Bhattacharyya",
        "authorids": "/n/nihar-sahoo/; /p/pranamya-kulkarni/; /a/arif-ahmad/; /t/tanu-goyal/; /n/narjis-asad/; /a/aparna-garimella/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{sahoo-etal-2024-indibias,\n    title = \"{I}ndi{B}ias: A Benchmark Dataset to Measure Social Biases in Language Models for {I}ndian Context\",\n    author = \"Sahoo, Nihar  and\n      Kulkarni, Pranamya  and\n      Ahmad, Arif  and\n      Goyal, Tanu  and\n      Asad, Narjis  and\n      Garimella, Aparna  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.487/\",\n    doi = \"10.18653/v1/2024.naacl-long.487\",\n    pages = \"8786--8806\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.487.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.487/",
        "pdf_size": 3744052,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10491701156623573159&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "IIT Bombay, India; IIT Bombay, India; IIT Bombay, India; IIT Bombay, India; Google, India; Adobe Research, India; IIT Bombay, India",
        "aff_domain": "cse.iitb.ac.in;gmail.com;cse.iitb.ac.in;gmail.com;google.com;adobe.com;cse.iitb.ac.in",
        "email": "cse.iitb.ac.in;gmail.com;cse.iitb.ac.in;gmail.com;google.com;adobe.com;cse.iitb.ac.in",
        "github": "https://github.com/sahoonihar/IndiBias",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1;2;0",
        "aff_unique_norm": "Indian Institute of Technology Bombay;Google;Adobe",
        "aff_unique_dep": ";Google;Adobe Research",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.google.com;https://research.adobe.com",
        "aff_unique_abbr": "IITB;Google;Adobe",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Bombay;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2024.naacl-long.425",
        "title": "IndiSentiment140: Sentiment Analysis Dataset for Indian Languages with Emphasis on Low-Resource Languages using Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Sentiment analysis, a fundamental aspect of Natural Language Processing (NLP), involves the classification of emotions, opinions, and attitudes in text data. In the context of India, with its vast linguistic diversity and low-resource languages, the challenge is to support sentiment analysis in numerous Indian languages. This study explores the use of machine translation to bridge this gap. The investigation examines the feasibility of machine translation for creating sentiment analysis datasets in 22 Indian languages. Google Translate, with its extensive language support, is employed for this purpose in translating the Sentiment140 dataset. The study aims to provide insights into the practicality of using machine translation in the context of India\u2019s linguistic diversity for sentiment analysis datasets. Our findings indicate that a dataset generated using Google Translate has the potential to serve as a foundational framework for tackling the low-resource challenges commonly encountered in sentiment analysis for Indian languages.",
        "author": "Saurabh Kumar; Ranbir Sanasam; Sukumar Nandi",
        "authorids": "/s/saurabh-kumar/; /r/ranbir-sanasam/; /s/sukumar-nandi/",
        "bibtex": "@inproceedings{kumar-etal-2024-indisentiment140,\n    title = \"{I}ndi{S}entiment140: Sentiment Analysis Dataset for {I}ndian Languages with Emphasis on Low-Resource Languages using Machine Translation\",\n    author = \"Kumar, Saurabh  and\n      Sanasam, Ranbir  and\n      Nandi, Sukumar\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.425/\",\n    doi = \"10.18653/v1/2024.naacl-long.425\",\n    pages = \"7689--7698\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.425.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.425/",
        "pdf_size": 271511,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1295152863390876518&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Engineering, Indian Institute of Technology Guwahati; Department of Computer Science and Engineering, Indian Institute of Technology Guwahati; Department of Computer Science and Engineering, Indian Institute of Technology Guwahati",
        "aff_domain": "iitg.ac.in;iitg.ac.in;iitg.ac.in",
        "email": "iitg.ac.in;iitg.ac.in;iitg.ac.in",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Guwahati",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.iitg.ac.in",
        "aff_unique_abbr": "IIT Guwahati",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Guwahati",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2024.naacl-long.37",
        "title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Instruction tuning effectively optimizes Large Language Models (LLMs) for downstream tasks. Due to the changing environment in real-life applications, LLMs necessitate continual task-specific adaptation without catastrophic forgetting. Considering the heavy computational cost, replay-based Continual Learning (CL) methods are the simplest and most widely used for LLMs to address the forgetting issue. However, traditional replay-based methods do not fully utilize instructions to customize the replay strategy. In this work, we propose a novel paradigm called Instruction-based Continual Learning (InsCL). InsCL dynamically replays previous data based on task similarity, calculated by Wasserstein Distance with instructions. Moreover, we further introduce an Instruction Information Metric (InsInfo) to quantify the complexity and diversity of instructions. According to InsInfo, InsCL guides the replay process more inclined to high-quality data. We conduct extensive experiments over 16 tasks with different training orders, observing consistent performance improvements of InsCL. When all tasks have been trained, InsCL achieves performance gains of 3.0 Relative Gain compared with Random Replay, and 27.96 Relative Gain compared with No Replay.",
        "author": "Yifan Wang; Yafei Liu; Chufan Shi; Haoling Li; Chen Chen; Haonan Lu; Yujiu Yang",
        "authorids": "/y/yifan-wang/; /y/yafei-liu/; /c/chufan-shi/; /h/haoling-li/; /c/chen-chen/; /h/haonan-lu/; /y/yujiu-yang/",
        "bibtex": "@inproceedings{wang-etal-2024-inscl,\n    title = \"{I}ns{CL}: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions\",\n    author = \"Wang, Yifan  and\n      Liu, Yafei  and\n      Shi, Chufan  and\n      Li, Haoling  and\n      Chen, Chen  and\n      Lu, Haonan  and\n      Yang, Yujiu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.37/\",\n    doi = \"10.18653/v1/2024.naacl-long.37\",\n    pages = \"663--677\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.37.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.37/",
        "pdf_size": 2464118,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11414747290849081783&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Tsinghua University; OPPO AI Center; Tsinghua University; Tsinghua University; OPPO AI Center; OPPO AI Center; Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;oppo.com;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;oppo.com;oppo.com;sz.tsinghua.edu.cn",
        "email": "mails.tsinghua.edu.cn;oppo.com;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;oppo.com;oppo.com;sz.tsinghua.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;0;0;1;1;0",
        "aff_unique_norm": "Tsinghua University;OPPO",
        "aff_unique_dep": ";OPPO AI Center",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.oppo.com",
        "aff_unique_abbr": "THU;OPPO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-short.63",
        "title": "InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We introduce InstructABSA, an instruction learning paradigm for Aspect-Based Sentiment Analysis (ABSA) subtasks.Our method introduces positive, negative, and neutral examples to each training sample, and instruction tune the model (Tk-Instruct) for ABSA subtasks, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on Term Extraction (ATE), Sentiment Classification(ATSC) and Sentiment Pair Extraction (ASPE) subtasks.In particular, InstructABSA outperforms the previous state-of-the-art (SOTA) on the Rest14 ATE subtask by 5.69% points, the Rest15 ATSC subtask by 9.59% points, and the Lapt14 AOPE subtask by 3.37% points, surpassing 7x larger models.We get competitive results on AOOE, AOPE, AOSTE, and ACOSQE subtasks indicating strong generalization ability to all subtasks. Exploring sample efficiency reveals that just 50% train data is required to get competitive results with other instruction tuning approaches. Lastly, we assess the quality of instructions and observe that InstructABSA\u2019s performance experiences a decline of ~10% when adding misleading examples",
        "author": "Kevin Scaria; Himanshu Gupta; Siddharth Goyal; Saurabh Sawant; Swaroop Mishra; Chitta Baral",
        "authorids": "/k/kevin-scaria/; /h/himanshu-gupta/; /s/siddharth-goyal/; /s/saurabh-sawant/; /s/swaroop-mishra/; /c/chitta-baral/",
        "bibtex": "@inproceedings{scaria-etal-2024-instructabsa,\n    title = \"{I}nstruct{ABSA}: Instruction Learning for Aspect Based Sentiment Analysis\",\n    author = \"Scaria, Kevin  and\n      Gupta, Himanshu  and\n      Goyal, Siddharth  and\n      Sawant, Saurabh  and\n      Mishra, Swaroop  and\n      Baral, Chitta\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.63/\",\n    doi = \"10.18653/v1/2024.naacl-short.63\",\n    pages = \"720--736\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.63.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.63/",
        "pdf_size": 1440552,
        "gs_citation": 66,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9578318834460063578&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Arizona State University\u2020; Arizona State University\u2020; Arizona State University\u2020; Arizona State University\u2662; Arizona State University\u2662; Arizona State University",
        "aff_domain": "asu.edu;asu.edu; ; ; ; ",
        "email": "asu.edu;asu.edu; ; ; ; ",
        "github": "https://github.com/kevinscaria/InstructABSA",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Arizona State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.asu.edu",
        "aff_unique_abbr": "ASU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.270",
        "title": "InstructEval: Systematic Evaluation of Instruction Selection Methods",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In-context learning (ICL) performs tasks by prompting a large language model (LLM) using an instruction and a small set of annotated examples called demonstrations. Recent work has shown that precise details of the inputs used in the ICL prompt significantly impact performance, which has incentivized instruction selection algorithms. The effect of instruction-choice however is severely underexplored, with existing analyses restricted to shallow subsets of models and tasks, limiting the generalizability of their insights. We develop InstructEval, an ICL evaluation suite to conduct a thorough assessment of these techniques. The suite includes 13 open-sourced LLMs of varying scales from four model families, and covers nine tasks across three categories. Using the suite, we evaluate the relative performance of seven popular instruction selection methods over five metrics relevant to ICL. Our experiments reveal that using curated manually-written instructions or simple instructions without any task-specific descriptions often elicits superior ICL performance overall than that of automatic instruction-induction methods, pointing to a lack of generalizability among the latter. We release our evaluation suite (at https://github.com/princeton-nlp/InstructEval) for benchmarking instruction selection approaches and enabling more generalizable methods in this space.",
        "author": "Anirudh Ajith; Chris Pan; Mengzhou Xia; Ameet Deshpande; Karthik Narasimhan",
        "authorids": "/a/anirudh-ajith/; /c/chris-pan/; /m/mengzhou-xia/; /a/ameet-deshpande/; /k/karthik-narasimhan/",
        "bibtex": "@inproceedings{ajith-etal-2024-instructeval,\n    title = \"{I}nstruct{E}val: Systematic Evaluation of Instruction Selection Methods\",\n    author = \"Ajith, Anirudh  and\n      Pan, Chris  and\n      Xia, Mengzhou  and\n      Deshpande, Ameet  and\n      Narasimhan, Karthik\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.270/\",\n    doi = \"10.18653/v1/2024.findings-naacl.270\",\n    pages = \"4336--4350\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.270.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.270/",
        "pdf_size": 445749,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14660938610711093951&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Princeton University; Department of Computer Science, Princeton University; Department of Computer Science, Princeton University; Department of Computer Science, Princeton University; Department of Computer Science, Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "github": "https://github.com/princeton-nlp/InstructEval",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.379",
        "title": "Instructing Large Language Models to Identify and Ignore Irrelevant Conditions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Math word problem (MWP) solving requires generating a reasoning path based on a given problem description that often contains irrelevant conditions.Existing chain-of-thought (CoT) prompting methods elicited multi-step reasoning abilities of large language models (LLMs) to solve MWPs.However, they were seriously confused by the irrelevant conditions, resulting in low accuracy.In this paper, we propose a novel approach named I3C that instructs LLMs to identify and ignore irrelevant conditions.It identifies a set of irrelevant condition candidates that have a weak semantic relevance with the question.Then it prompts LLMs to verify the irrelevant conditions.Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths.Moreover, we propose to select (problem, reasoning paths) pairs as demonstrations to enhance I3C with few-shot reasoning. We develop I3C-Select that selects the most confusing problems based on the semantic relevance measurement.We conduct extensive experiments on eight MWP datasets.I3C can be combined with any CoT prompting methods to improve the performance of solving MWPs.Notably, with GPT-3.5-Turbo and I3C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art few-shot prompting method Complex-CoT by +11.7 and +11.1.Our implementation is made publicly available at https://wzy6642.github.io/I3C.github.io/.",
        "author": "Zhenyu Wu; Chao Shen; Meng Jiang",
        "authorids": "/z/zhenyu-wu/; /c/chao-shen/; /m/meng-jiang/",
        "bibtex": "@inproceedings{wu-etal-2024-instructing,\n    title = \"Instructing Large Language Models to Identify and Ignore Irrelevant Conditions\",\n    author = \"Wu, Zhenyu  and\n      Shen, Chao  and\n      Jiang, Meng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.379/\",\n    doi = \"10.18653/v1/2024.naacl-long.379\",\n    pages = \"6799--6819\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.379.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.379/",
        "pdf_size": 1589817,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17915465429154384519&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.findings-naacl.82",
        "title": "Instruction Tuning with Human Curriculum",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this work, we (1) introduce Curriculum Instruction Tuning, (2) explore the potential advantages of employing diverse curriculum strategies, and (3) delineate a synthetic instruction-response generation framework that complements our theoretical approach. Distinct from the existing instruction tuning dataset, our generation pipeline is systematically structured to emulate the sequential and orderly characteristic of human learning. Additionally, we describe a methodology for generating instruction-response datasets that extensively span the various stages of human education, from middle school through the graduate level, utilizing educational subject catalogs.Before training, we meticulously organize the instruction data to ensure that questions escalate in difficulty regarding (A) the subject matter and (B) the intricacy of the instructions. The findings of our study reveal that substantial improvements in performance can be achieved through the mere application of curriculum ordering to instruction data\u2014achieving gains of +4.76 on TruthfulQA, +2.98 on MMLU, +2.8 on OpenbookQA, and +1.28 on ARC-hard\u2014compared to random shuffling. This enhancement is achieved without incurring additional computational expenses. Through comprehensive experimentation, we observe that the advantages of our proposed method are consistently evident across nine benchmarks.",
        "author": "Bruce W Lee; Hyunsoo Cho; Kang Min Yoo",
        "authorids": "/b/bruce-w-lee/; /h/hyunsoo-cho/; /k/kang-min-yoo/",
        "bibtex": "@inproceedings{lee-etal-2024-instruction,\n    title = \"Instruction Tuning with Human Curriculum\",\n    author = \"Lee, Bruce W  and\n      Cho, Hyunsoo  and\n      Yoo, Kang Min\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.82/\",\n    doi = \"10.18653/v1/2024.findings-naacl.82\",\n    pages = \"1281--1309\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.82.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.82/",
        "pdf_size": 3022756,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5995285511634966666&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Pennsylvania+NA VER Cloud+NA VER AI Lab; Ewha Womans University+NA VER Cloud+NA VER AI Lab; Seoul National University+NA VER Cloud+NA VER AI Lab",
        "aff_domain": "seas.upenn.edu;ewha.ac.kr;navercorp.com",
        "email": "seas.upenn.edu;ewha.ac.kr;navercorp.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1+2;3+1+2;4+1+2",
        "aff_unique_norm": "University of Pennsylvania;NAVER Cloud;NAVER Corporation;Ewha Womans University;Seoul National University",
        "aff_unique_dep": ";;AI Lab;;",
        "aff_unique_url": "https://www.upenn.edu;https://www.naver.com;https://www.naver.com;http://www.ewha.ac.kr;https://www.snu.ac.kr",
        "aff_unique_abbr": "UPenn;NAVER;NAVER;Ewha;SNU",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1+1;1+1+1;1+1+1",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "2024.findings-naacl.233",
        "title": "Instruction-following Evaluation through Verbalizer Manipulation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While instruction-tuned models have shown remarkable success in various natural language processing tasks, accurately evaluating their ability to follow instructions remains challenging. Existing benchmarks primarily focus on common instructions that align well with what the model learned during training. However, proficiency in responding to these instructions does not necessarily imply strong ability in instruction following. In this paper, we propose a novel instruction-following evaluation protocol called verbalizer manipulation. It instructs the model to verbalize the task label with words aligning with model priors to different extents, adopting verbalizers from highly aligned (e.g., outputting \u201cpositive\u201d for positive sentiment), to minimally aligned (e.g., outputting \u201cnegative\u201d for positive sentiment). Verbalizer manipulation can be seamlessly integrated with any classification benchmark to examine the model\u2019s reliance on priors and its ability to override them to accurately follow the instructions. We conduct a comprehensive evaluation of four major model families across nine datasets, employing twelve sets of verbalizers for each of them. We observe that the instruction-following abilities of models, across different families and scales, are significantly distinguished by their performance on less natural verbalizers. Even the strongest GPT-4 model struggles to perform better than random guessing on the most challenging verbalizer, emphasizing the need for continued advancements to improve their instruction-following abilities.",
        "author": "Shiyang Li; Jun Yan; Hai Wang; Zheng Tang; Xiang Ren; Vijay Srinivasan; Hongxia Jin",
        "authorids": "/s/shiyang-li/; /j/jun-yan/; /h/hai-wang/; /z/zheng-tang/; /x/xiang-ren/; /v/vijay-srinivasan/; /h/hongxia-jin/",
        "bibtex": "@inproceedings{li-etal-2024-instruction,\n    title = \"Instruction-following Evaluation through Verbalizer Manipulation\",\n    author = \"Li, Shiyang  and\n      Yan, Jun  and\n      Wang, Hai  and\n      Tang, Zheng  and\n      Ren, Xiang  and\n      Srinivasan, Vijay  and\n      Jin, Hongxia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.233/\",\n    doi = \"10.18653/v1/2024.findings-naacl.233\",\n    pages = \"3678--3692\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.233.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.233/",
        "pdf_size": 620188,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3294772868387818937&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Samsung Research America; Samsung Research America+University of Southern California; Samsung Research America; Samsung Research America; University of Southern California; Samsung Research America; Samsung Research America",
        "aff_domain": "samsung.com;usc.edu;samsung.com;samsung.com;usc.edu;samsung.com;samsung.com",
        "email": "samsung.com;usc.edu;samsung.com;samsung.com;usc.edu;samsung.com;samsung.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0+1;0;0;1;0;0",
        "aff_unique_norm": "Samsung;University of Southern California",
        "aff_unique_dep": "Samsung Research America;",
        "aff_unique_url": "https://www.samsung.com/us/careers/research/;https://www.usc.edu",
        "aff_unique_abbr": "SRA;USC",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.180",
        "title": "Instructional Fingerprinting of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (eg restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License.",
        "author": "Jiashu Xu; Fei Wang; Mingyu Ma; Pang Wei Koh; Chaowei Xiao; Muhao Chen",
        "authorids": "/j/jiashu-xu/; /f/fei-wang/; /m/mingyu-ma/; /p/pang-wei-koh/; /c/chaowei-xiao/; /m/muhao-chen/",
        "bibtex": "@inproceedings{xu-etal-2024-instructional,\n    title = \"Instructional Fingerprinting of Large Language Models\",\n    author = \"Xu, Jiashu  and\n      Wang, Fei  and\n      Ma, Mingyu  and\n      Koh, Pang Wei  and\n      Xiao, Chaowei  and\n      Chen, Muhao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.180/\",\n    doi = \"10.18653/v1/2024.naacl-long.180\",\n    pages = \"3277--3306\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.180.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.180/",
        "pdf_size": 3766042,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7300116045338218773&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Harvard; USC; UCLA; UW Seattle; UW-Madison; UC Davis",
        "aff_domain": "g.harvard.edu; ; ; ; ; ",
        "email": "g.harvard.edu; ; ; ; ; ",
        "github": "https://cnut1648.github.io/Model-Fingerprint",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;3;4;5",
        "aff_unique_norm": "Harvard University;University of Southern California;University of California, Los Angeles;University of Washington;University of Wisconsin-Madison;University of California, Davis",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.harvard.edu;https://www.usc.edu;https://www.ucla.edu;https://www.washington.edu;https://www.wisc.edu;https://www.ucdavis.edu",
        "aff_unique_abbr": "Harvard;USC;UCLA;UW;UW-Madison;UC Davis",
        "aff_campus_unique_index": "1;1;2;3;4",
        "aff_campus_unique": ";Los Angeles;Seattle;Madison;Davis",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.171",
        "title": "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We investigate security concerns of the emergent instruction tuning paradigm, that models are trained on crowdsourced datasets with task instructions to achieve superior performance. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions (~1000 tokens) and control model behavior through data poisoning, without even the need to modify data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets. As an empirical study on instruction attacks, we systematically evaluated unique perspectives of instruction attacks, such as poison transfer where poisoned models can transfer to 15 diverse generative datasets in a zero-shot manner; instruction transfer where attackers can directly apply poisoned instruction on many other datasets; and poison resistance to continual finetuning. Lastly, we show that RLHF and clean demonstrations might mitigate such backdoors to some degree. These findings highlight the need for more robust defenses against poisoning attacks in instruction-tuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.",
        "author": "Jiashu Xu; Mingyu Ma; Fei Wang; Chaowei Xiao; Muhao Chen",
        "authorids": "/j/jiashu-xu/; /m/mingyu-ma/; /f/fei-wang/; /c/chaowei-xiao/; /m/muhao-chen/",
        "bibtex": "@inproceedings{xu-etal-2024-instructions,\n    title = \"Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models\",\n    author = \"Xu, Jiashu  and\n      Ma, Mingyu  and\n      Wang, Fei  and\n      Xiao, Chaowei  and\n      Chen, Muhao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.171/\",\n    doi = \"10.18653/v1/2024.naacl-long.171\",\n    pages = \"3111--3126\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.171.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.171/",
        "pdf_size": 4314376,
        "gs_citation": 129,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6222808093228859631&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Harvard; UCLA; USC; University of Wisconsin, Madison; UC, Davis",
        "aff_domain": "harvard.edu;cs.ucla.edu;usc.edu;wisc.edu;ucdavis.edu",
        "email": "harvard.edu;cs.ucla.edu;usc.edu;wisc.edu;ucdavis.edu",
        "github": "",
        "project": "https://cnut1648.github.io/instruction-attack/",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;4",
        "aff_unique_norm": "Harvard University;University of California, Los Angeles;University of Southern California;University of Wisconsin;University of California, Davis",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.harvard.edu;https://www.ucla.edu;https://www.usc.edu;https://www.wisc.edu;https://www.ucdavis.edu",
        "aff_unique_abbr": "Harvard;UCLA;USC;UW;UC Davis",
        "aff_campus_unique_index": "1;1;2;3",
        "aff_campus_unique": ";Los Angeles;Madison;Davis",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.374",
        "title": "Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. The effectiveness of addressing hate speech involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. The first two phases of CoARL involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses reinforcement learning to fine-tune outputs for effectiveness and nontoxicity. CoARL outperforms existing benchmarks in intent-conditioned counterspeech generation, showing an average improvement of \u223c3 points in intent-conformity and \u223c4 points in argument-quality metrics. Extensive human evaluation supports CoARL\u2019s efficacy in generating superior and more context-appropriate responses compared to existing systems, including prominent LLMs like ChatGPT.",
        "author": "Amey Hengle; Aswini Padhi; Sahajpreet Singh; Anil Bandhakavi; Md Shad Akhtar; Tanmoy Chakraborty",
        "authorids": "/a/amey-hengle/; /a/aswini-padhi/; /s/sahajpreet-singh/; /a/anil-bandhakavi/; /m/md-shad-akhtar/; /t/tanmoy-chakraborty/",
        "bibtex": "@inproceedings{hengle-etal-2024-intent,\n    title = \"Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with {RLAIF}\",\n    author = \"Hengle, Amey  and\n      Padhi, Aswini  and\n      Singh, Sahajpreet  and\n      Bandhakavi, Anil  and\n      Akhtar, Md Shad  and\n      Chakraborty, Tanmoy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.374/\",\n    doi = \"10.18653/v1/2024.naacl-long.374\",\n    pages = \"6716--6733\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.374.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.374/",
        "pdf_size": 2724588,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11873991162034590124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.420",
        "title": "Interplay of Machine Translation, Diacritics, and Diacritization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We investigate two research questions: (1) how do machine translation (MT) and diacritization influence the performance of each other in a multi-task learning setting (2) the effect of keeping (vs. removing) diacritics on MT performance. We examine these two questions in both high-resource (HR) and low-resource (LR) settings across 55 different languages (36 African languages and 19 European languages). For (1), results show that diacritization significantly benefits MT in the LR scenario, doubling or even tripling performance for some languages, but harms MT in the HR scenario. We find that MT harms diacritization in LR but benefits significantly in HR for some languages. For (2), MT performance is similar regardless of diacritics being kept or removed. In addition, we propose two classes of metrics to measure the complexity of a diacritical system, finding these metrics to correlate positively with the performance of our diacritization models. Overall, our work provides insights for developing MT and diacritization systems under different data size conditions and may have implications that generalize beyond the 55 languages we investigate.",
        "author": "Wei-Rui Chen; Ife Adebara; Muhammad Abdul-Mageed",
        "authorids": "/w/wei-rui-chen/; /i/ife-adebara/; /m/muhammad-abdul-mageed/",
        "bibtex": "@inproceedings{chen-etal-2024-interplay,\n    title = \"Interplay of Machine Translation, Diacritics, and Diacritization\",\n    author = \"Chen, Wei-Rui  and\n      Adebara, Ife  and\n      Abdul-Mageed, Muhammad\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.420/\",\n    doi = \"10.18653/v1/2024.naacl-long.420\",\n    pages = \"7559--7601\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.420.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.420/",
        "pdf_size": 5275402,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15084072065014237018&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Deep Learning & Natural Language Processing Group, The University of British Columbia; Deep Learning & Natural Language Processing Group, The University of British Columbia; Deep Learning & Natural Language Processing Group, The University of British Columbia + Department of Natural Language Processing & Department of Machine Learning, MBZUAI + Invertible AI",
        "aff_domain": "ubc.ca;ubc.ca;ubc.ca",
        "email": "ubc.ca;ubc.ca;ubc.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0+1+2",
        "aff_unique_norm": "University of British Columbia;Mohamed bin Zayed University of Artificial Intelligence;Invertible AI",
        "aff_unique_dep": "Department of Computer Science;Department of Natural Language Processing;",
        "aff_unique_url": "https://www.ubc.ca;https://www.mbzuai.ac.ae;https://www.invertible.ai",
        "aff_unique_abbr": "UBC;MBZUAI;Invertible AI",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Vancouver;",
        "aff_country_unique_index": "0;0;0+1+2",
        "aff_country_unique": "Canada;United Arab Emirates;United States"
    },
    {
        "id": "2024.findings-naacl.136",
        "title": "Interpreting Answers to Yes-No Questions in Dialogues from Multiple Domains",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "People often answer yes-no questions without explicitly saying yes, no, or similar polar key-words. Figuring out the meaning of indirectanswers is challenging, even for large language models. In this paper, we investigate this problem working with dialogues from multiple domains. We present new benchmarks in three diverse domains: movie scripts, tennis interviews, and airline customer service. We present an approach grounded on distant supervision and blended training to quickly adapt to a new dialogue domain. Experimental results show that our approach is never detrimental and yields F1 improvements as high as 11-34%.",
        "author": "Zijie Wang; Farzana Rashid; Eduardo Blanco",
        "authorids": "/z/zijie-wang/; /f/farzana-rashid/; /e/eduardo-blanco/",
        "bibtex": "@inproceedings{wang-etal-2024-interpreting,\n    title = \"Interpreting Answers to Yes-No Questions in Dialogues from Multiple Domains\",\n    author = \"Wang, Zijie  and\n      Rashid, Farzana  and\n      Blanco, Eduardo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.136/\",\n    doi = \"10.18653/v1/2024.findings-naacl.136\",\n    pages = \"2111--2128\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.136.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.136/",
        "pdf_size": 257936,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3520570715991965024&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Arizona; University of North Carolina Asheville; University of Arizona",
        "aff_domain": "arizona.edu;unca.edu;arizona.edu",
        "email": "arizona.edu;unca.edu;arizona.edu",
        "github": "https://github.com/wang-zijie/yn-question-multi-domains2111",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Arizona;University of North Carolina Asheville",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.arizona.edu;https://www.unca.edu",
        "aff_unique_abbr": "UA;UNCA",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Asheville",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.255",
        "title": "Interpreting User Requests in the Context of Natural Language Standing Instructions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Users of natural language interfaces, frequently powered by Large Language Models (LLMs), must often repeat their full set of preferences each time they make a similar request. We describe an approach to LLM-based dialogue modeling in which persistent user constraints and preferences \u2013 collectively termed standing instructions \u2013 are provided as additional context for such interfaces. For example, when a user states \u201cI\u2019m hungry\u201d, a previously expressed preference for Persian food can be automatically added to the LLM prompt, influencing the search for relevant restaurants.We develop NLSI, a language-to-program dataset consisting of over 2.4K English dialogues spanning 17 domains, in which each dialogue is paired with a user profile (a set of user-specific standing instructions) and corresponding structured representations (a sequence of API calls). A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue. NLSI contains diverse phenomena, from simple preferences to interdependent instructions such as triggering a hotel search whenever the user is booking tickets to an event. We conduct experiments on NLSI using prompting with large language models and various retrieval approaches, achieving a maximum of 46% exact match on API prediction. Our results demonstrate the challenges in identifying the relevant standing instructions and their interpretation into API calls",
        "author": "Nikita Moghe; Patrick Xia; Jacob Andreas; Jason Eisner; Benjamin Van Durme; Harsh Jhamtani",
        "authorids": "/n/nikita-moghe/; /p/patrick-xia/; /j/jacob-andreas/; /j/jason-eisner/; /b/benjamin-van-durme/; /h/harsh-jhamtani/",
        "bibtex": "@inproceedings{moghe-etal-2024-interpreting,\n    title = \"Interpreting User Requests in the Context of Natural Language Standing Instructions\",\n    author = \"Moghe, Nikita  and\n      Xia, Patrick  and\n      Andreas, Jacob  and\n      Eisner, Jason  and\n      Van Durme, Benjamin  and\n      Jhamtani, Harsh\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.255/\",\n    doi = \"10.18653/v1/2024.findings-naacl.255\",\n    pages = \"4043--4060\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.255.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.255/",
        "pdf_size": 412226,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16661289491532392478&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Informatics, University of Edinburgh; Microsoft Semantic Machines; Microsoft Semantic Machines; Microsoft Semantic Machines; Microsoft Semantic Machines; Microsoft Semantic Machines",
        "aff_domain": "ed.ac.uk; ; ; ; ;microsoft.com",
        "email": "ed.ac.uk; ; ; ; ;microsoft.com",
        "github": "https://github.com/nikitacs16/nlsi",
        "project": "https://huggingface.co/datasets/nikitam/nlsi",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "University of Edinburgh;Microsoft",
        "aff_unique_dep": "School of Informatics;Semantic Machines",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.microsoft.com",
        "aff_unique_abbr": "Edinburgh;Microsoft",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Edinburgh;",
        "aff_country_unique_index": "0;1;1;1;1;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2024.findings-naacl.232",
        "title": "Investigating Acceleration of LLaMA Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with \u2018LITE\u2019",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) have achieved remarkable performance across a wide variety of tasks; however, their large size makes their inference slow and computationally expensive. Focusing on this problem, we study instruction tuning LLMs with additional explicit Losses from the Intermediate layers (LITE) and show that it enables these layers to acquire \u2018good\u2019 generation ability without affecting the generation ability of the final layer. We then perform \u2018dynamic confidence-based early exiting\u2019 at token level from the intermediate layers which improves the computational efficiency of text generation without sacrificing the quality of the generation. We conduct comprehensive experiments by instruction tuning LLaMA-2 models on the Alpaca dataset and evaluate on four different instruction test sets. We show that dynamic early exiting achieves consistent and considerable inference cost improvements (37.86% for 7B and 46.35% for 13B model) while maintaining the generation quality. We further conduct a thorough analysis of the results and dissect the efficiency improvements which reveals several important findings.",
        "author": "Neeraj Varshney; Agneet Chatterjee; Mihir Parmar; Chitta Baral",
        "authorids": "/n/neeraj-varshney/; /a/agneet-chatterjee/; /m/mihir-parmar/; /c/chitta-baral/",
        "bibtex": "@inproceedings{varshney-etal-2024-investigating,\n    title = \"Investigating Acceleration of {LL}a{MA} Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with {\\textquoteleft}{LITE}'\",\n    author = \"Varshney, Neeraj  and\n      Chatterjee, Agneet  and\n      Parmar, Mihir  and\n      Baral, Chitta\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.232/\",\n    doi = \"10.18653/v1/2024.findings-naacl.232\",\n    pages = \"3656--3677\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.232.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.232/",
        "pdf_size": 974555,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1448866890714656418&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-long.482",
        "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks. This issue is especially critical for closed-source models and certain open-source models where training data transparency is lacking. In this paper we study data contamination by proposing two methods tailored for both open-source and proprietary LLMs. We first introduce a retrieval-based system to explore potential overlaps between evaluation benchmarks and pretraining corpora. We further present a novel investigation protocol named Testset Slot Guessing (TS-Guessing), applicable to both open and proprietary models. This approach entails masking a wrong answer in a multiple-choice question and prompting the model to fill in the gap. Additionally, it involves obscuring an unlikely word in an evaluation example and asking the model to produce it. We find that certain commercial LLMs could surprisingly guess the missing option in various test sets. Specifically, in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match rate of 52% and 57%, respectively, in guessing the missing options in benchmark test data. We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field.",
        "author": "Chunyuan Deng; Yilun Zhao; Xiangru Tang; Mark Gerstein; Arman Cohan",
        "authorids": "/c/chunyuan-deng/; /y/yilun-zhao/; /x/xiangru-tang/; /m/mark-gerstein/; /a/arman-cohan/",
        "bibtex": "@inproceedings{deng-etal-2024-investigating,\n    title = \"Investigating Data Contamination in Modern Benchmarks for Large Language Models\",\n    author = \"Deng, Chunyuan  and\n      Zhao, Yilun  and\n      Tang, Xiangru  and\n      Gerstein, Mark  and\n      Cohan, Arman\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.482/\",\n    doi = \"10.18653/v1/2024.naacl-long.482\",\n    pages = \"8706--8719\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.482.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.482/",
        "pdf_size": 631467,
        "gs_citation": 101,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7743470201105788007&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Georgia Institute of Technology; Yale University; Yale University; Yale University; Yale University + Allen Institute for AI",
        "aff_domain": "gatech.edu; ; ;yale.edu; ",
        "email": "gatech.edu; ; ;yale.edu; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;1+2",
        "aff_unique_norm": "Georgia Institute of Technology;Yale University;Allen Institute for AI",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.gatech.edu;https://www.yale.edu;https://allenai.org",
        "aff_unique_abbr": "Georgia Tech;Yale;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.266",
        "title": "Investigating the Emergent Audio Classification Ability of ASR Foundation Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text and vision foundation models can perform many tasks in a zero-shot setting, a desirable property that enables these systems to be applied in general and low-resource settings. There has been far less work, however, on the zero-shot abilities of ASR foundation models, with these systems typically fine-tuned to specific tasks or constrained to applications that match their training criterion and data annotation. In this work we investigate the ability of Whisper and MMS, ASR foundation models trained primarily for speech recognition, to perform zero-shot audio classification. We use simple template-based text prompts at the decoder and use the resulting decoding probabilities to generate zero-shot predictions. Without training the model on extra data or adding any new parameters, we demonstrate that Whisper shows promising zero-shot classification performance on a range of 8 audio-classification datasets, outperforming the accuracy of existing state-of-the-art zero-shot baselines by an average of 9%. One important step to unlock the emergent ability is debiasing, where a simple unsupervised reweighting method of the class probabilities yields consistent significant performance gains. We further show that performance increases with model size, implying that as ASR foundation models scale up, they may exhibit improved zero-shot performance.",
        "author": "Rao Ma; Adian Liusie; Mark Gales; Kate Knill",
        "authorids": "/r/rao-ma/; /a/adian-liusie/; /m/mark-gales/; /k/kate-knill/",
        "bibtex": "@inproceedings{ma-etal-2024-investigating,\n    title = \"Investigating the Emergent Audio Classification Ability of {ASR} Foundation Models\",\n    author = \"Ma, Rao  and\n      Liusie, Adian  and\n      Gales, Mark  and\n      Knill, Kate\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.266/\",\n    doi = \"10.18653/v1/2024.naacl-long.266\",\n    pages = \"4746--4760\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.266.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.266/",
        "pdf_size": 1656071,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13259313268203460823&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "ALTA Institute, Department of Engineering, University of Cambridge; ALTA Institute, Department of Engineering, University of Cambridge; ALTA Institute, Department of Engineering, University of Cambridge; ALTA Institute, Department of Engineering, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;eng.cam.ac.uk;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;eng.cam.ac.uk;cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.findings-naacl.195",
        "title": "IruMozhi: Automatically classifying diglossia in Tamil",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Tamil, a Dravidian language of South Asia, is a highly diglossic language with two very different registers in everyday use: Literary Tamil (preferred in writing and formal communication) and Spoken Tamil (confined to speech and informal media). Spoken Tamil is under-studied in modern NLP systems compared to Literary Tamil written in the Tamil script, as evidenced by a lack of datasets explicitly targetting the Spoken variety. In this paper, we release IruMozhi, a human-translated dataset of parallel text in Literary and Spoken Tamil. Using IruMozhi, we train classifiers on the task of identifying which Tamil variety a text belongs to. We use these models to gauge the availability of pretraining data in Spoken Tamil, to audit the composition of existing labelled datasets for Tamil, and to encourage future work on the variety.",
        "author": "Kabilan Prasanna; Aryaman Arora",
        "authorids": "/k/kabilan-prasanna/; /a/aryaman-arora/",
        "bibtex": "@inproceedings{prasanna-arora-2024-irumozhi,\n    title = \"{I}ru{M}ozhi: Automatically classifying diglossia in {T}amil\",\n    author = \"Prasanna, Kabilan  and\n      Arora, Aryaman\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.195/\",\n    doi = \"10.18653/v1/2024.findings-naacl.195\",\n    pages = \"3096--3103\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.195.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.195/",
        "pdf_size": 8141979,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:kXORfhKhF-MJ:scholar.google.com/&scioq=IruMozhi:+Automatically+classifying+diglossia+in+Tamil&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Academies of Loudoun; Stanford University",
        "aff_domain": "gmail.com;stanford.edu",
        "email": "gmail.com;stanford.edu",
        "github": "https://github.com/kebathan/diglossia",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Academies of Loudoun;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.stanford.edu",
        "aff_unique_abbr": ";Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.44",
        "title": "Is Prompt Transfer Always Effective? An Empirical Study of Prompt Transfer for Question Answering",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Prompt tuning, which freezes all parameters of a pre-trained model and only trains a soft prompt, has emerged as a parameter-efficient approach. For the reason that the prompt initialization becomes sensitive when the model size is small, the prompt transfer that uses the trained prompt as an initialization for the target task has recently been introduced. Since previous works have compared tasks in large categories (e.g., summarization, sentiment analysis), the factors that influence prompt transfer have not been sufficiently explored. In this paper, we characterize the question answering task based on features such as answer format and empirically investigate the transferability of soft prompts for the first time. We analyze the impact of initialization during prompt transfer and find that the train dataset size of source and target tasks have the influence significantly. Furthermore, we propose a novel approach for measuring catastrophic forgetting and investigate how it occurs in terms of the amount of evidence. Our findings can help deeply understand transfer learning in prompt tuning.",
        "author": "Minji Jung; Soyeon Park; Jeewoo Sul; Yong Suk Choi",
        "authorids": "/m/minji-jung/; /s/soyeon-park/; /j/jeewoo-sul/; /y/yong-suk-choi/",
        "bibtex": "@inproceedings{jung-etal-2024-prompt,\n    title = \"Is Prompt Transfer Always Effective? An Empirical Study of Prompt Transfer for Question Answering\",\n    author = \"Jung, Minji  and\n      Park, Soyeon  and\n      Sul, Jeewoo  and\n      Choi, Yong Suk\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.44/\",\n    doi = \"10.18653/v1/2024.naacl-short.44\",\n    pages = \"528--539\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.44.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.44/",
        "pdf_size": 7017993,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6558443680398160043&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Intelligence and Convergence Hanyang University, Seoul, Korea + LG Electronics; Department of Computer Science Hanyang University, Seoul, Korea; Department of Computer Science Hanyang University, Seoul, Korea + LG Electronics; Department of Intelligence and Convergence Hanyang University, Seoul, Korea + Department of Computer Science Hanyang University, Seoul, Korea",
        "aff_domain": "hanyang.ac.kr;hanyang.ac.kr;hanyang.ac.kr;hanyang.ac.kr",
        "email": "hanyang.ac.kr;hanyang.ac.kr;hanyang.ac.kr;hanyang.ac.kr",
        "github": "https://github.com/ailab-prompt-transfer/qa_prompt_transfer",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;0+1;0+0",
        "aff_unique_norm": "Hanyang University;LG",
        "aff_unique_dep": "Department of Intelligence and Convergence;LG Electronics",
        "aff_unique_url": "http://www.hanyang.ac.kr;https://www.lg.com",
        "aff_unique_abbr": "HYU;LG",
        "aff_campus_unique_index": "0;0;0;0+0",
        "aff_campus_unique": "Seoul;",
        "aff_country_unique_index": "0+0;0;0+0;0+0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-long.474",
        "title": "Is Reference Necessary in the Evaluation of NLG Systems? When and Where?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The majority of automatic metrics for evaluating NLG systems are reference-based. However, the challenge of collecting human annotation results in a lack of reliable references in numerous application scenarios. Despite recent advancements in reference-free metrics, it has not been well understood when and where they can be used as an alternative to reference-based metrics. In this study, by employing diverse analytical approaches, we comprehensively assess the performance of both metrics across a wide range of NLG tasks, encompassing eight datasets and eight evaluation models. Based on solid experiments, the results show that reference-free metrics exhibit a higher correlation with human judgment and greater sensitivity to deficiencies in language quality. However, their effectiveness varies across tasks and is influenced by the quality of candidate texts. Therefore, it\u2019s important to assess the performance of reference-free metrics before applying them to a new task, especially when inputs are in uncommon form or when the answer space is highly variable. Our study can provide insight into the appropriate application of automatic metrics and the impact of metric choice on evaluation performance.",
        "author": "Shuqian Sheng; Yi Xu; Luoyi Fu; Jiaxin Ding; Lei Zhou; Xinbing Wang; Chenghu Zhou",
        "authorids": "/s/shuqian-sheng/; /y/yi-xu/; /l/luoyi-fu/; /j/jiaxin-ding/; /l/lei-zhou/; /x/xinbing-wang/; /c/chenghu-zhou/",
        "bibtex": "@inproceedings{sheng-etal-2024-reference,\n    title = \"Is Reference Necessary in the Evaluation of {NLG} Systems? When and Where?\",\n    author = \"Sheng, Shuqian  and\n      Xu, Yi  and\n      Fu, Luoyi  and\n      Ding, Jiaxin  and\n      Zhou, Lei  and\n      Wang, Xinbing  and\n      Zhou, Chenghu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.474/\",\n    doi = \"10.18653/v1/2024.naacl-long.474\",\n    pages = \"8580--8596\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.474.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.474/",
        "pdf_size": 510666,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10211058241692936553&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; IGSNRR, Chinese Academy of Sciences",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn; ; ; ; ",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn; ; ; ; ",
        "github": "https://github.com/susisheng/NLGEvaluation",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Institute of Geographic Sciences and Natural Resources Research",
        "aff_unique_dep": ";Chinese Academy of Sciences",
        "aff_unique_url": "https://www.sjtu.edu.cn;http://www.igsnrr.cas.cn",
        "aff_unique_abbr": "SJTU;IGSNRR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.250",
        "title": "Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based Reinforcement Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Traditional Automatic Video Dubbing (AVD) pipeline consists of three key modules, namely, Automatic Speech Recognition (ASR), Neural Machine Translation (NMT), and Text-to-Speech (TTS). Within AVD pipelines, isometric-NMT algorithms are employed to regulate the length of the synthesized output text. This is done to guarantee synchronization with respect to the alignment of video and audio subsequent to the dubbing process. Previous approaches have focused on aligning the number of characters and words in the source and target language texts of Machine Translation models. However, our approach aims to align the number of phonemes instead, as they are closely associated with speech duration. In this paper, we present the development of an isometric NMT system using Reinforcement Learning (RL), with a focus on optimizing the alignment of phoneme counts in the source and target language sentence pairs. To evaluate our models, we propose the Phoneme Count Compliance (PCC) score, which is a measure of length compliance. Our approach demonstrates a substantial improvement of approximately 36% in the PCC score compared to the state-of-the-art models when applied to English-Hindi language pairs. Moreover, we propose a student-teacher architecture within the framework of our RL approach to maintain a trade-off between the phoneme count and translation quality.",
        "author": "Shivam Mhaskar; Nirmesh Shah; Mohammadi Zaki; Ashishkumar Gudmalwar; Pankaj Wasnik; Rajiv Shah",
        "authorids": "/s/shivam-mhaskar/; /n/nirmesh-shah/; /m/mohammadi-zaki/; /a/ashishkumar-gudmalwar/; /p/pankaj-wasnik/; /r/rajiv-shah/",
        "bibtex": "@inproceedings{mhaskar-etal-2024-isometric,\n    title = \"Isometric Neural Machine Translation using Phoneme Count Ratio Reward-based Reinforcement Learning\",\n    author = \"Mhaskar, Shivam  and\n      Shah, Nirmesh  and\n      Zaki, Mohammadi  and\n      Gudmalwar, Ashishkumar  and\n      Wasnik, Pankaj  and\n      Shah, Rajiv\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.250/\",\n    doi = \"10.18653/v1/2024.findings-naacl.250\",\n    pages = \"3966--3976\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.250.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.250/",
        "pdf_size": 5495347,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13733080573443820362&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.78",
        "title": "IterAlign: Iterative Constitutional Alignment of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM. Empirical results on several safety benchmark datasets and multiple base LLMs show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the LLM alignment by up to 13.5% in harmlessness.",
        "author": "Xiusi Chen; Hongzhi Wen; Sreyashi Nag; Chen Luo; Qingyu Yin; Ruirui Li; Zheng Li; Wei Wang",
        "authorids": "/x/xiusi-chen/; /h/hongzhi-wen/; /s/sreyashi-nag/; /c/chen-luo/; /q/qingyu-yin/; /r/ruirui-li/; /z/zheng-li/; /w/wei-wang/",
        "bibtex": "@inproceedings{chen-etal-2024-iteralign,\n    title = \"{I}ter{A}lign: Iterative Constitutional Alignment of Large Language Models\",\n    author = \"Chen, Xiusi  and\n      Wen, Hongzhi  and\n      Nag, Sreyashi  and\n      Luo, Chen  and\n      Yin, Qingyu  and\n      Li, Ruirui  and\n      Li, Zheng  and\n      Wang, Wei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.78/\",\n    doi = \"10.18653/v1/2024.naacl-long.78\",\n    pages = \"1423--1433\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.78.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.78/",
        "pdf_size": 419198,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12841185080698346111&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 7,
        "aff": "University of California, Los Angeles; Michigan State University; Amazon; Amazon; Amazon; Amazon; Amazon; University of California, Los Angeles",
        "aff_domain": "cs.ucla.edu;msu.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;cs.ucla.edu",
        "email": "cs.ucla.edu;msu.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;2;2;2;2;0",
        "aff_unique_norm": "University of California, Los Angeles;Michigan State University;Amazon",
        "aff_unique_dep": ";;Amazon.com, Inc.",
        "aff_unique_url": "https://www.ucla.edu;https://www.msu.edu;https://www.amazon.com",
        "aff_unique_abbr": "UCLA;MSU;Amazon",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.449",
        "title": "IterCQR: Iterative Conversational Query Reformulation with Retrieval Guidance",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Conversational search aims to retrieve passages containing essential information to answer queries in a multi-turn conversation. In conversational search, reformulating context-dependent conversational queries into stand-alone forms is imperative to effectively utilize off-the-shelf retrievers. Previous methodologies for conversational query reformulation frequently depend on human-annotated rewrites.However, these manually crafted queries often result in sub-optimal retrieval performance and require high collection costs.To address these challenges, we propose **Iter**ative **C**onversational **Q**uery **R**eformulation (**IterCQR**), a methodology that conducts query reformulation without relying on human rewrites. IterCQR iteratively trains the conversational query reformulation (CQR) model by directly leveraging information retrieval (IR) signals as a reward.Our IterCQR training guides the CQR model such that generated queries contain necessary information from the previous dialogue context.Our proposed method shows state-of-the-art performance on two widely-used datasets, demonstrating its effectiveness on both sparse and dense retrievers. Moreover, IterCQR exhibits superior performance in challenging settings such as generalization on unseen datasets and low-resource scenarios.",
        "author": "Yunah Jang; Kang-il Lee; Hyunkyung Bae; Hwanhee Lee; Kyomin Jung",
        "authorids": "/y/yunah-jang/; /k/kang-il-lee/; /h/hyunkyung-bae/; /h/hwanhee-lee/; /k/kyomin-jung/",
        "bibtex": "@inproceedings{jang-etal-2024-itercqr,\n    title = \"{I}ter{CQR}: Iterative Conversational Query Reformulation with Retrieval Guidance\",\n    author = \"Jang, Yunah  and\n      Lee, Kang-il  and\n      Bae, Hyunkyung  and\n      Lee, Hwanhee  and\n      Jung, Kyomin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.449/\",\n    doi = \"10.18653/v1/2024.naacl-long.449\",\n    pages = \"8121--8138\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.449.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.449/",
        "pdf_size": 571225,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3579652940623173676&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Dept. of ECE, Seoul National University; Dept. of ECE, Seoul National University; LG AI Research; Chung-Ang University; Dept. of ECE, Seoul National University+IPAI, Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;lgresearch.ai;cau.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;lgresearch.ai;cau.ac.kr;snu.ac.kr",
        "github": "https://github.com/YunahJang/IterCQR",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0+0",
        "aff_unique_norm": "Seoul National University;LG;Chung-Ang University",
        "aff_unique_dep": "Dept. of Electrical and Computer Engineering;LG AI Research;",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.lgaires.com;http://www.cau.ac.kr",
        "aff_unique_abbr": "SNU;LG AI;CAU",
        "aff_campus_unique_index": "0;0;0+0",
        "aff_campus_unique": "Seoul;",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.findings-naacl.107",
        "title": "It\u2019s All Relative! \u2013 A Synthetic Query Generation Approach for Improving Zero-Shot Relevance Prediction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) have shown promising ability to generate synthetic query-document pairs by prompting with as few as 8 demonstrations. This has enabled building better IR models, especially for tasks with no training data. Typically, such synthetic query generation (QGen) approaches condition on an input context (e.g. a text document) and generate a query relevant to that context, or condition the QGen additionally on the relevance label (e.g. relevant vs irrelevant) to generate queries across relevance buckets. However, we find that such QGen approaches are sub-optimal as they require the model to reason about the desired label and the input from a handful of examples. In this work, we propose to reduce this burden of LLMs by generating queries simultaneously for different labels. We hypothesize that instead of asking the model to generate, say, an irrelevant query given an input context, asking the model to generate an irrelevant query relative to a relevant query is a much simpler task. Extensive experimentation across nine IR datasets shows that synthetic queries generated in such a fashion translates to better downstream performance.",
        "author": "Aditi Chaudhary; Karthik Raman; Michael Bendersky",
        "authorids": "/a/aditi-chaudhary/; /k/karthik-raman/; /m/michael-bendersky/",
        "bibtex": "@inproceedings{chaudhary-etal-2024-relative,\n    title = \"It{'}s All Relative! {--} A Synthetic Query Generation Approach for Improving Zero-Shot Relevance Prediction\",\n    author = \"Chaudhary, Aditi  and\n      Raman, Karthik  and\n      Bendersky, Michael\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.107/\",\n    doi = \"10.18653/v1/2024.findings-naacl.107\",\n    pages = \"1645--1664\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.107.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.107/",
        "pdf_size": 251899,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7208082591575424158&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.naacl-long.87",
        "title": "JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The permanence of online content combined with the enhanced authorship identification techniques calls for stronger computational methods to protect the identity and privacy of online authorship when needed, e.g., blind reviews for scientific papers, anonymous online reviews, or anonymous interactions in the mental health forums. In this paper, we propose an unsupervised inference-time approach to authorship obfuscation to address the unique challenges of authorship obfuscation: lack of supervision data for diverse authorship and domains, and the need for a sufficient level of revision beyond simple paraphrasing to obfuscate the authorship, all the while preserving the original content and fluency.We introduce JAMDEC, a user-controlled, inference-time algorithm for authorship obfuscation that can be in principle applied to any text and authorship. Our approach builds on small language models such as GPT2-XL in order to help avoid disclosing the original content to proprietary LLM\u2019s APIs, while also reducing the performance gap between small and large language models via algorithmic enhancement. The key idea behind our approach is to boost the creative power of smaller language models through constrained decoding, while also allowing for user-specified controls and flexibility. Experimental results demonstrate that our approach based on GPT2-XL outperforms previous state-of-the-art methods based on comparably small models, while performing competitively against GPT3.5 175B, a propriety model that is two orders of magnitudes larger.",
        "author": "Jillian Fisher; Ximing Lu; Jaehun Jung; Liwei Jiang; Zaid Harchaoui; Yejin Choi",
        "authorids": "/j/jillian-fisher/; /x/ximing-lu/; /j/jaehun-jung/; /l/liwei-jiang/; /z/zaid-harchaoui/; /y/yejin-choi/",
        "bibtex": "@inproceedings{fisher-etal-2024-jamdec,\n    title = \"{JAMDEC}: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models\",\n    author = \"Fisher, Jillian  and\n      Lu, Ximing  and\n      Jung, Jaehun  and\n      Jiang, Liwei  and\n      Harchaoui, Zaid  and\n      Choi, Yejin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.87/\",\n    doi = \"10.18653/v1/2024.naacl-long.87\",\n    pages = \"1552--1581\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.87.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.87/",
        "pdf_size": 6723184,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1208821893679033289&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Washington; University of Washington + Allen Institute for Artificial Intelligence; University of Washington; University of Washington + Allen Institute for Artificial Intelligence; University of Washington; University of Washington + Allen Institute for Artificial Intelligence",
        "aff_domain": "uw.edu; ; ; ; ; ",
        "email": "uw.edu; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0+1;0;0+1;0;0+1",
        "aff_unique_norm": "University of Washington;Allen Institute for Artificial Intelligence",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UW;AI2",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0+0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.42",
        "title": "KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive Angular margin Contrastive Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Previous work on multimodal sentence embedding has proposed multimodal contrastive learning and achieved promising results. However, by taking the rest of the batch as negative samples without reviewing when forming contrastive pairs, those studies encountered many suspicious and noisy negative examples, significantly affecting the methods\u2019 overall performance. In this work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning of Sentence Embeddings), a novel approach that enhances the discrimination and generalizability of multimodal representation and inherits the knowledge from the teacher model to learn the difference between positive and negative instances and via that, can detect noisy and wrong negative samples effectively before they are calculated in the contrastive objective. Furthermore, to overcome the limitation of modeling the variation within negative pairs, we introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin Supervised Contrastive Learning for Multimodal sentence embeddings), that enhances the discriminative representation by strengthening the margin within the angular space while capturing varying semantics within the negative. Experimental results on widely used Semantic Textual Similarity (STS) benchmarks demonstrate the effectiveness of our approach.",
        "author": "Cong-Duy Nguyen; Thong Nguyen; Xiaobao Wu; Anh Tuan Luu",
        "authorids": "/c/cong-duy-nguyen/; /t/thong-nguyen/; /x/xiaobao-wu/; /l/luu-anh-tuan/",
        "bibtex": "@inproceedings{nguyen-etal-2024-kdmcse,\n    title = \"{KDMCSE}: Knowledge Distillation Multimodal Sentence Embeddings with Adaptive Angular margin Contrastive Learning\",\n    author = \"Nguyen, Cong-Duy  and\n      Nguyen, Thong  and\n      Wu, Xiaobao  and\n      Luu, Anh Tuan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.42/\",\n    doi = \"10.18653/v1/2024.naacl-long.42\",\n    pages = \"733--749\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.42.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.42/",
        "pdf_size": 1715541,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13895480179786266713&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Nanyang Technological University, Singapore; National University of Singapore, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/duyngtr16061999/KDMCSE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Nanyang Technological University;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.nus.edu.sg",
        "aff_unique_abbr": "NTU;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2024.naacl-long.134",
        "title": "KTRL+F: Knowledge-Augmented In-Document Search",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce a new problem KTRL+F, a knowledge-augmented in-document search that necessitates real-time identification of all semantic targets within a document with the awareness of external sources through a single natural query. KTRL+F addresses following unique challenges for in-document search: 1) utilizing knowledge outside the document for extended use of additional information about targets, and 2) balancing between real-time applicability with the performance.We analyze various baselines in KTRL+F and find limitations of existing models, such as hallucinations, high latency, or difficulties in leveraging external knowledge. Therefore, we propose a Knowledge-Augmented Phrase Retrieval model that shows a promising balance between speed and performance by simply augmenting external knowledge in phrase embedding. We also conduct a user study to verify whether solving KTRL+F can enhance search experience for users. It demonstrates that even with our simple model, users can reduce the time for searching with less queries and reduced extra visits to other sources for collecting evidence. We encourage the research community to work on KTRL+F to enhance more efficient in-document information access.",
        "author": "Hanseok Oh; Haebin Shin; Miyoung Ko; Hyunji Lee; Minjoon Seo",
        "authorids": "/h/hanseok-oh/; /h/haebin-shin/; /m/miyoung-ko/; /h/hyunji-lee/; /m/minjoon-seo/",
        "bibtex": "@inproceedings{oh-etal-2024-ktrl,\n    title = \"{KTRL}+{F}: Knowledge-Augmented In-Document Search\",\n    author = \"Oh, Hanseok  and\n      Shin, Haebin  and\n      Ko, Miyoung  and\n      Lee, Hyunji  and\n      Seo, Minjoon\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.134/\",\n    doi = \"10.18653/v1/2024.naacl-long.134\",\n    pages = \"2416--2436\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.134.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.134/",
        "pdf_size": 1879784,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17734871024842038388&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "KAIST AI; KAIST AI + Samsung Research; KAIST AI; KAIST AI; KAIST AI",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "github": "https://github.com/kaistAI/KtrlF",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;Samsung",
        "aff_unique_dep": "KAIST AI;Samsung Research",
        "aff_unique_url": "https://www.kaist.edu;https://research.samsung.com",
        "aff_unique_abbr": "KAIST;Samsung",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-long.480",
        "title": "Keep it Private: Unsupervised Privatization of Online Text",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Authorship obfuscation techniques hold the promise of helping people protect their privacy in online communications by automatically rewriting text to hide the identity of the original author. However, obfuscation has been evaluated in narrow settings in the NLP literature and has primarily been addressed with superficial edit operations that can lead to unnatural outputs. In this work, we introduce an automatic text privatization framework that fine-tunes a large language model via reinforcement learning to produce rewrites that balance soundness, sense, and privacy. We evaluate it extensively on a large-scale test set of English Reddit posts by 68k authors composed of short-medium length texts. We study how the performance changes among evaluative conditions including authorial profile length and authorship detection strategy. Our method maintains high text quality according to both automated metrics and human evaluation, and successfully evades several automated authorship attacks.",
        "author": "Calvin Bao; Marine Carpuat",
        "authorids": "/c/calvin-bao/; /m/marine-carpuat/",
        "bibtex": "@inproceedings{bao-carpuat-2024-keep,\n    title = \"{K}eep it {P}rivate: Unsupervised Privatization of Online Text\",\n    author = \"Bao, Calvin  and\n      Carpuat, Marine\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.480/\",\n    doi = \"10.18653/v1/2024.naacl-long.480\",\n    pages = \"8678--8693\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.480.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.480/",
        "pdf_size": 818461,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3991348616588316160&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Maryland, College Park; University of Maryland, College Park",
        "aff_domain": "umd.edu;umd.edu",
        "email": "umd.edu;umd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "College Park",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.401",
        "title": "Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Zero-shot cross-lingual transfer, which implies finetuning of the multilingual pretrained language model on input-output pairs in one language and using it to make task predictions for inputs in other languages, was widely studied for natural language understanding but is understudied for generation. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size, and NLLB-200 can be competitive in some cases. Our final zero-shot models reach the performance of the approach based on data translation which is usually considered as an upper baseline for zero-shot cross-lingual transfer in generation.",
        "author": "Nadezhda Chirkova; Vassilina Nikoulina",
        "authorids": "/n/nadezhda-chirkova/; /v/vassilina-nikoulina/",
        "bibtex": "@inproceedings{chirkova-nikoulina-2024-key,\n    title = \"Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks\",\n    author = \"Chirkova, Nadezhda  and\n      Nikoulina, Vassilina\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.401/\",\n    doi = \"10.18653/v1/2024.naacl-long.401\",\n    pages = \"7222--7238\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.401.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.401/",
        "pdf_size": 787789,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15423953648403963341&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Naver Labs Europe, Grenoble, France; Naver Labs Europe, Grenoble, France",
        "aff_domain": "naverlabs.com;naverlabs.com",
        "email": "naverlabs.com;naverlabs.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "NAVER LABS Europe",
        "aff_unique_dep": "",
        "aff_unique_url": "https://labs.naver.com",
        "aff_unique_abbr": "NLE",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Grenoble",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2024.naacl-long.202",
        "title": "Know When To Stop: A Study of Semantic Drift in Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this work, we explicitly show that modern LLMs tend to generate correct facts first, then \u201cdrift away\u201d and generate incorrect facts later: this was occasionally observed but never properly measured. We develop a semantic drift score that measures the degree of separation between correct and incorrect facts in generated texts and confirm our hypothesis when generating Wikipedia-style biographies. This correct-then-incorrect generation pattern suggests that factual accuracy can be improved by knowing when to stop generation. Therefore, we explore the trade-off between information quantity and factual accuracy for several early stopping methods and manage to improve factuality by a large margin. We further show that reranking with semantic similarity can further improve these results, both compared to the baseline and when combined with early stopping. Finally, we try calling external API to bring the model back to the right generation path, but do not get positive results. Overall, our methods generalize and can be applied to any long-form text generation to produce more reliable information, by balancing trade-offs between factual accuracy, information quantity and computational cost.",
        "author": "Ava Spataru; Eric Hambro; Elena Voita; Nicola Cancedda",
        "authorids": "/a/ava-spataru/; /e/eric-hambro/; /e/elena-voita/; /n/nicola-cancedda/",
        "bibtex": "@inproceedings{spataru-2024-know,\n    title = \"Know When To Stop: A Study of Semantic Drift in Text Generation\",\n    author = \"Spataru, Ava  and\n      Hambro, Eric  and\n      Voita, Elena  and\n      Cancedda, Nicola\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.202/\",\n    doi = \"10.18653/v1/2024.naacl-long.202\",\n    pages = \"3656--3671\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.202.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.202/",
        "pdf_size": 1060608,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9013648808122116693&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "FAIR, Meta; Anthropic+FAIR, Meta; FAIR, Meta; FAIR, Meta",
        "aff_domain": "meta.com;gmail.com;meta.com;meta.com",
        "email": "meta.com;gmail.com;meta.com;meta.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+0;0;0",
        "aff_unique_norm": "Meta;Anthropic",
        "aff_unique_dep": "Facebook AI Research (FAIR);",
        "aff_unique_url": "https://meta.com;https://www.anthropic.com",
        "aff_unique_abbr": "Meta;Anthropic",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.396",
        "title": "KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Parameter-efficient finetuning (PEFT) is a key technique for adapting large language models (LLMs) to downstream tasks. In this paper, we study leveraging knowledge graph embeddings to improve the effectiveness of PEFT. We propose a knowledgeable adaptation method called KnowLA. It inserts an adaptation layer into an LLM to integrate the embeddings of entities appearing in the input text. The adaptation layer is trained in combination with LoRA on instruction data. Experiments on six benchmarks with two popular LLMs and three knowledge graphs demonstrate the effectiveness and robustness of KnowLA. We show that KnowLA can help activate the relevant parameterized knowledge in an LLM to answer a question without changing its parameters or input prompts.",
        "author": "Xindi Luo; Zequn Sun; Jing Zhao; Zhe Zhao; Wei Hu",
        "authorids": "/x/xindi-luo/; /z/zequn-sun/; /j/jing-zhao/; /z/zhe-zhao/; /w/wei-hu/",
        "bibtex": "@inproceedings{luo-etal-2024-knowla,\n    title = \"{K}now{LA}: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation\",\n    author = \"Luo, Xindi  and\n      Sun, Zequn  and\n      Zhao, Jing  and\n      Zhao, Zhe  and\n      Hu, Wei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.396/\",\n    doi = \"10.18653/v1/2024.naacl-long.396\",\n    pages = \"7153--7166\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.396.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.396/",
        "pdf_size": 524305,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14516192764288755627&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; Tencent AI Lab, China; Tencent AI Lab, China; State Key Laboratory for Novel Software Technology, Nanjing University, China + National Institute of Healthcare Data Science, Nanjing University, China",
        "aff_domain": "gmail.com;nju.edu.cn;tencent.com;tencent.com;nju.edu.cn",
        "email": "gmail.com;nju.edu.cn;tencent.com;tencent.com;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;1;0+0",
        "aff_unique_norm": "Nanjing University;Tencent",
        "aff_unique_dep": "State Key Laboratory for Novel Software Technology;Tencent AI Lab",
        "aff_unique_url": "http://www.nju.edu.cn;https://ai.tencent.com",
        "aff_unique_abbr": "Nanjing U;Tencent AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.390",
        "title": "Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) have shown great potential in Natural Language Processing (NLP) tasks.However, recent literature reveals that LLMs hallucinate intermittently, which impedes their reliability for further utilization. In this paper, we propose a novel self-detection method to detect which questions an LLM does not know.Our proposal is empirical and applicable for continually upgrading LLMs compared with state-of-the-art methods. Specifically, we examine the divergence of the LLM\u2019s behaviors on different verbalizations for a question and examine the atypicality of the verbalized input. We combine the two components to identify whether the model generates a non-factual response to the question. The above components can be accomplished by utilizing the LLM itself without referring to any other external resources. We conduct comprehensive experiments and demonstrate the effectiveness of our method for recently released LLMs involving Llama 2, Vicuna, ChatGPT, and GPT-4 across factoid question-answering, arithmetic reasoning, and commonsense reasoning tasks.",
        "author": "Yukun Zhao; Lingyong Yan; Weiwei Sun; Guoliang Xing; Chong Meng; Shuaiqiang Wang; Zhicong Cheng; Zhaochun Ren; Dawei Yin",
        "authorids": "/y/yukun-zhao/; /l/lingyong-yan/; /w/weiwei-sun-sd/; /g/guoliang-xing/; /c/chong-meng/; /s/shuaiqiang-wang/; /z/zhicong-cheng/; /z/zhaochun-ren/; /d/dawei-yin/",
        "bibtex": "@inproceedings{zhao-etal-2024-knowing,\n    title = \"Knowing What {LLM}s {DO} {NOT} Know: A Simple Yet Effective Self-Detection Method\",\n    author = \"Zhao, Yukun  and\n      Yan, Lingyong  and\n      Sun, Weiwei  and\n      Xing, Guoliang  and\n      Meng, Chong  and\n      Wang, Shuaiqiang  and\n      Cheng, Zhicong  and\n      Ren, Zhaochun  and\n      Yin, Dawei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.390/\",\n    doi = \"10.18653/v1/2024.naacl-long.390\",\n    pages = \"7051--7063\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.390.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.390/",
        "pdf_size": 357304,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7000893795020687330&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Shandong University, Qingdao, China+Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Shandong University, Qingdao, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Leiden University, Leiden, The Netherlands+Baidu Inc., Beijing, China; Baidu Inc., Beijing, China",
        "aff_domain": "baidu.com;baidu.com;gmail.com;baidu.com;baidu.com;baidu.com;baidu.com;liacs.leidenuniv.nl;acm.org",
        "email": "baidu.com;baidu.com;gmail.com;baidu.com;baidu.com;baidu.com;baidu.com;liacs.leidenuniv.nl;acm.org",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;1;0;1;1;1;1;2+1;1",
        "aff_unique_norm": "Shandong University;Baidu;Leiden University",
        "aff_unique_dep": ";Baidu Inc.;",
        "aff_unique_url": "http://www.sdu.edu.cn;https://www.baidu.com;https://www.universiteitleiden.nl",
        "aff_unique_abbr": "SDU;Baidu;LU",
        "aff_campus_unique_index": "0+1;1;0;1;1;1;1;2+1;1",
        "aff_campus_unique": "Qingdao;Beijing;Leiden",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;1+0;0",
        "aff_country_unique": "China;Netherlands"
    },
    {
        "id": "2024.findings-naacl.207",
        "title": "Knowledgeable In-Context Tuning: Exploring and Exploiting Factual Knowledge for In-Context Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) enable in-context learning (ICL) by conditioning on a few labeled training examples as a text-based prompt, eliminating the need for parameter updates and achieving competitive performance. In this paper, we demonstrate that factual knowledge is imperative for the performance of ICL in three core facets: the inherent knowledge learned in LLMs, the factual knowledge derived from the selected in-context examples, and the knowledge biases in LLMs for output generation. To unleash the power of LLMs in few-shot learning scenarios, we introduce a novel Knowledgeable In-Context Tuning (KICT) framework to further improve the performance of ICL:1) injecting knowledge into LLMs during continual self-supervised pre-training, 2) judiciously selecting the examples for ICL with high knowledge relevance, and 3) calibrating the prediction results based on prior knowledge.We evaluate the proposed approaches on autoregressive models (e.g., GPT-style LLMs) over multiple text classification and question-answering tasks. Experimental results demonstrate that KICT substantially outperforms strong baselines and improves by more than 13% and 7% on text classification and question-answering tasks, respectively.",
        "author": "Jianing Wang; Chengyu Wang; Chuanqi Tan; Jun Huang; Ming Gao",
        "authorids": "/j/jianing-wang/; /c/chengyu-wang/; /c/chuanqi-tan/; /j/jun-huang/; /m/ming-gao/",
        "bibtex": "@inproceedings{wang-etal-2024-knowledgeable,\n    title = \"Knowledgeable In-Context Tuning: Exploring and Exploiting Factual Knowledge for In-Context Learning\",\n    author = \"Wang, Jianing  and\n      Wang, Chengyu  and\n      Tan, Chuanqi  and\n      Huang, Jun  and\n      Gao, Ming\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.207/\",\n    doi = \"10.18653/v1/2024.findings-naacl.207\",\n    pages = \"3261--3280\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.207.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.207/",
        "pdf_size": 1911664,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14401314116036342994&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Data Science and Engineering, East China Normal University, Shanghai, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; School of Data Science and Engineering, East China Normal University, Shanghai, China + KLATASDS-MOE, School of Statistics, East China Normal University, Shanghai, China",
        "aff_domain": "gmail.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;dase.ecnu.edu.cn",
        "email": "gmail.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;dase.ecnu.edu.cn",
        "github": "https://github.com/HugAILab/HugNLP",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0+0",
        "aff_unique_norm": "East China Normal University;Alibaba Group",
        "aff_unique_dep": "School of Data Science and Engineering;",
        "aff_unique_url": "http://www.ecnu.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "ECNU;Alibaba",
        "aff_campus_unique_index": "0;1;1;1;0+0",
        "aff_campus_unique": "Shanghai;Hangzhou",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.170",
        "title": "Krey\u00f2l-MT: Building MT for Latin American, Caribbean and Colonial African Creole Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A majority of language technologies are tailored for a small number of high-resource languages, while relatively many low-resource languages are neglected. One such group, Creole languages, have long been marginalized in academic study, though their speakers could benefit from machine translation (MT). These languages are predominantly used in much of Latin America, Africa and the Caribbean. We present the largest cumulative dataset to date for Creole language MT, including 14.5M unique Creole sentences with parallel translations\u201411.6M of which we release publicly, and the largest bitexts gathered to date for 41 languages\u2014the first ever for 21. In addition, we provide MT models supporting all 41 Creole languages in 172 translation directions. Given our diverse dataset, we produce a model for Creole language MT exposed to more genre diversity then ever before, which outperforms a genre-specific Creole MT model on its own benchmark for 23 of 34 translation directions.",
        "author": "Nathaniel Robinson; Raj Dabre; Ammon Shurtz; Rasul Dent; Onenamiyi Onesi; Claire Monroc; Lo\u00efc Grobol; Hasan Muhammad; Ashi Garg; Naome Etori; Vijay Murari Tiyyala; Olanrewaju Samuel; Matthew Stutzman; Bismarck Odoom; Sanjeev Khudanpur; Stephen Richardson; Kenton Murray",
        "authorids": "/n/nathaniel-robinson/; /r/raj-dabre/; /a/ammon-shurtz/; /r/rasul-dent/; /o/onenamiyi-onesi/; /c/claire-monroc/; /l/loic-grobol/; /h/hasan-muhammad/; /a/ashi-garg/; /n/naome-etori/; /v/vijay-murari-tiyyala/; /o/olanrewaju-samuel/; /m/matthew-stutzman/; /b/bismarck-odoom/; /s/sanjeev-khudanpur/; /s/stephen-richardson/; /k/kenton-murray/",
        "bibtex": "@inproceedings{robinson-etal-2024-kreyol,\n    title = \"Krey{\\`o}l-{MT}: Building {MT} for {L}atin {A}merican, {C}aribbean and Colonial {A}frican Creole Languages\",\n    author = {Robinson, Nathaniel  and\n      Dabre, Raj  and\n      Shurtz, Ammon  and\n      Dent, Rasul  and\n      Onesi, Onenamiyi  and\n      Monroc, Claire  and\n      Grobol, Lo{\\\"i}c  and\n      Muhammad, Hasan  and\n      Garg, Ashi  and\n      Etori, Naome  and\n      Tiyyala, Vijay Murari  and\n      Samuel, Olanrewaju  and\n      Stutzman, Matthew  and\n      Odoom, Bismarck  and\n      Khudanpur, Sanjeev  and\n      Richardson, Stephen  and\n      Murray, Kenton},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.170/\",\n    doi = \"10.18653/v1/2024.naacl-long.170\",\n    pages = \"3083--3110\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.170.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.170/",
        "pdf_size": 814485,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16440865015390264670&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": "Johns Hopkins University, USA; Brigham Young University, USA; National Institute of Information and Communications Technology, Japan; Inria Paris; Nile University of Nigeria; Universit\u00e9 Paris Nanterre; University of Minnesota - Twin Cities, USA; University of Toronto; Johns Hopkins University, USA; Johns Hopkins University, USA; Johns Hopkins University, USA; University of Toronto; Brigham Young University, USA; Johns Hopkins University, USA; Johns Hopkins University, USA; Brigham Young University, USA; Johns Hopkins University, USA",
        "aff_domain": "jhu.edu; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;",
        "email": "jhu.edu; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ;",
        "github": "",
        "project": "",
        "author_num": 17,
        "aff_unique_index": "0;1;2;3;4;5;6;7;0;0;0;7;1;0;0;1;0",
        "aff_unique_norm": "Johns Hopkins University;Brigham Young University;National Institute of Information and Communications Technology;INRIA;Nile University;Universit\u00e9 Paris Nanterre;University of Minnesota;University of Toronto",
        "aff_unique_dep": ";;;;;;;",
        "aff_unique_url": "https://www.jhu.edu;https://www.byu.edu;https://www.nict.go.jp/;https://www.inria.fr;https://www.nileuniversity.edu.ng;https://www.univ-parisnanterre.fr;https://www.minnesota.edu;https://www.utoronto.ca",
        "aff_unique_abbr": "JHU;BYU;NICT;Inria;NUN;UPN;UMN;U of T",
        "aff_campus_unique_index": "1;2;3",
        "aff_campus_unique": ";Paris;Nanterre;Twin Cities",
        "aff_country_unique_index": "0;0;1;2;3;2;0;4;0;0;0;4;0;0;0;0;0",
        "aff_country_unique": "United States;Japan;France;Nigeria;Canada"
    },
    {
        "id": "2024.naacl-short.36",
        "title": "LEAF: Language Learners\u2019 English Essays and Feedback Corpus",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "This paper addresses the issue of automated feedback generation for English language learners by presenting a corpus of English essays and their corresponding feedback, called LEAF, collected from the \u201cessayforum\u201d website. The corpus comprises approximately 6K essay-feedback pairs, offering a diverse and valuable resource for developing personalized feedback generation systems that address the critical deficiencies within essays, spanning from rectifying grammatical errors to offering insights on argumentative aspects and organizational coherence. Using this corpus, we present and compare multiple feedback generation baselines. Our findings shed light on the challenges of providing personalized feedback and highlight the potential of the LEAF corpus in advancing automated essay evaluation.",
        "author": "Shabnam Behzad; Omid Kashefi; Swapna Somasundaran",
        "authorids": "/s/shabnam-behzad/; /o/omid-kashefi/; /s/swapna-somasundaran/",
        "bibtex": "@inproceedings{behzad-etal-2024-leaf,\n    title = \"{LEAF}: Language Learners' {E}nglish Essays and Feedback Corpus\",\n    author = \"Behzad, Shabnam  and\n      Kashefi, Omid  and\n      Somasundaran, Swapna\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.36/\",\n    doi = \"10.18653/v1/2024.naacl-short.36\",\n    pages = \"433--442\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.36.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.36/",
        "pdf_size": 143965,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7497280398229655478&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Georgetown University; Educational Testing Service (ETS); Educational Testing Service (ETS)",
        "aff_domain": "cs.georgetown.edu;ets.org;ets.org",
        "email": "cs.georgetown.edu;ets.org;ets.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Georgetown University;Educational Testing Service",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.georgetown.edu;https://www.ets.org",
        "aff_unique_abbr": "GU;ETS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.46",
        "title": "LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Linguistic entrainment, or alignment, represents a phenomenon where linguistic patterns employed by conversational participants converge to one another. While entrainment has been shown to produce a more natural user experience, most dialogue systems do not have any provisions for it. In this work, we introduce methods for achieving dialogue entrainment in a GPT-2-based end-to-end task-oriented dialogue system through the utilization of shared vocabulary. We experiment with training instance weighting, entrainment-specific loss, and additional conditioning to generate responses that align with the user. We demonstrate that all three approaches produce significantly better entrainment than the base, non-entrainment-optimized model, as confirmed by both automated and manual evaluation metrics.",
        "author": "Nalin Kumar; Ondrej Dusek",
        "authorids": "/n/nalin-kumar/; /o/ondrej-dusek/",
        "bibtex": "@inproceedings{kumar-dusek-2024-leeets,\n    title = \"{LEEET}s-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems\",\n    author = \"Kumar, Nalin  and\n      Dusek, Ondrej\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.46/\",\n    doi = \"10.18653/v1/2024.findings-naacl.46\",\n    pages = \"727--735\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.46.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.46/",
        "pdf_size": 308790,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:LpxDzDmrqakJ:scholar.google.com/&scioq=LEEETs-Dial:+Linguistic+Entrainment+in+End-to-End+Task-oriented+Dialogue+systems&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "Charles University, Faculty of Mathematics and Physics; Charles University, Faculty of Mathematics and Physics",
        "aff_domain": "student.cuni.cz;ufal.mff.cuni.cz",
        "email": "student.cuni.cz;ufal.mff.cuni.cz",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Charles University",
        "aff_unique_dep": "Faculty of Mathematics and Physics",
        "aff_unique_url": "https://www.cuni.cz",
        "aff_unique_abbr": "Charles U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Czech Republic"
    },
    {
        "id": "2024.findings-naacl.16",
        "title": "LETI: Learning to Generate from Textual Interactions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Fine-tuning pre-trained language models (LMs) is essential for enhancing their capabilities.Existing techniques commonly fine-tune on input-output pairs (e.g., instruction tuning) or with numerical rewards that gauge the output quality (e.g., RLHF). We explore LMs\u2019 potential to **le**arn from **t**extual **i**nteractions (**LETI**) that not only check their correctness with *binary labels* but also pinpoint and explain errors in their outputs through *textual feedback*.Our focus is the code generation task, where the model produces code based on natural language instructions. This setting invites a natural and scalable way to acquire textual feedback: the error messages and stack traces from code execution using a Python interpreter. LETI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback. Prepended to this fine-tuning text, a binary reward token is used to differentiate correct and buggy solutions.LETI requires *no* ground-truth outputs for training and even outperforms a fine-tuned baseline that does. LETI not only improves the performance of LMs on a code generation dataset MBPP, but also generalizes to other datasets. Trained on MBPP, it achieves comparable or better performance than the base LMs on unseen problems in HumanEval. Furthermore, compared to binary feedback, we observe that textual feedback leads to improved generation quality and sample efficiency, achieving the same performance with fewer than half of the gradient steps.LETI is equally applicable in natural language tasks when they can be formulated as code generation, which we empirically verified on event argument extraction.",
        "author": "Xingyao Wang; Hao Peng; Reyhaneh Jabbarvand; Heng Ji",
        "authorids": "/x/xingyao-wang/; /h/hao-peng/; /r/reyhaneh-jabbarvand/; /h/heng-ji/",
        "bibtex": "@inproceedings{wang-etal-2024-leti,\n    title = \"{LETI}: Learning to Generate from Textual Interactions\",\n    author = \"Wang, Xingyao  and\n      Peng, Hao  and\n      Jabbarvand, Reyhaneh  and\n      Ji, Heng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.16/\",\n    doi = \"10.18653/v1/2024.findings-naacl.16\",\n    pages = \"223--239\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.16.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.16/",
        "pdf_size": 4062483,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2686597194145113573&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;illinois.edu",
        "github": "https://github.com/xingyaoww/LeTI",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.32",
        "title": "LLM-Driven Knowledge Injection Advances Zero-Shot and Cross-Target Stance Detection",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Stance detection aims at inferring an author\u2019s attitude towards a specific target in a text. Prior methods mainly consider target-related background information for a better understanding of targets while neglecting the accompanying input texts. In this study, we propose to prompt Large Language Models (LLMs) to explicitly extract the relationship between paired text and target as contextual knowledge. We then inject such LLM-driven knowledge into a generation model BART to exploit the rich contexts and semantics. Moreover, to further enhance the decoding capability of BART, a novel prototypical contrastive scheme is designed to align input contents with stance labels. Our experimental results demonstrate the state-of-the-art performance across several publicly available datasets, showcasing effectiveness in both zero-shot and cross-target stance detection scenarios. We publicly release our code to facilitate future research.",
        "author": "Zhao Zhang; Yiming Li; Jin Zhang; Hui Xu",
        "authorids": "/z/zhao-zhang/; /y/yiming-li/; /j/jin-zhang/; /h/hui-xu/",
        "bibtex": "@inproceedings{zhang-etal-2024-llm-driven,\n    title = \"{LLM}-Driven Knowledge Injection Advances Zero-Shot and Cross-Target Stance Detection\",\n    author = \"Zhang, Zhao  and\n      Li, Yiming  and\n      Zhang, Jin  and\n      Xu, Hui\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.32/\",\n    doi = \"10.18653/v1/2024.naacl-short.32\",\n    pages = \"371--378\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.32.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.32/",
        "pdf_size": 484989,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16948697704002113967&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences + University of Chinese Academy of Sciences; Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences + University of Chinese Academy of Sciences; CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences; CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences",
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "github": "https://github.com/zhangzhao219/LKI-BART",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0;0",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.39",
        "title": "LLM-Rec: Personalized Recommendation via Prompting Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Text-based recommendation holds a wide range of practical applications due to its versatility, as textual descriptions can represent nearly any type of item. However, directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study, we introduce a novel approach, coined LLM-Rec, which incorporates four distinct prompting strategies of text enrichment for improving personalized text-based recommendations. Our empirical experiments reveal that using LLM-augmented text significantly enhances recommendation quality. Even basic MLP (Multi-Layer Perceptron) models achieve comparable or even better results than complex content-based methods. Notably, the success of LLM-Rec lies in its prompting strategies, which effectively tap into the language model\u2019s comprehension of both general and specific item characteristics. This highlights the importance of employing diverse prompts and input augmentation techniques to boost the recommendation effectiveness of LLMs.",
        "author": "Hanjia Lyu; Song Jiang; Hanqing Zeng; Yinglong Xia; Qifan Wang; Si Zhang; Ren Chen; Chris Leung; Jiajie Tang; Jiebo Luo",
        "authorids": "/h/hanjia-lyu/; /s/song-jiang/; /h/hanqing-zeng/; /y/yinglong-xia/; /q/qifan-wang/; /s/si-zhang/; /r/ren-chen/; /c/chris-leung/; /j/jiajie-tang/; /j/jiebo-luo/",
        "bibtex": "@inproceedings{lyu-etal-2024-llm,\n    title = \"{LLM}-Rec: Personalized Recommendation via Prompting Large Language Models\",\n    author = \"Lyu, Hanjia  and\n      Jiang, Song  and\n      Zeng, Hanqing  and\n      Xia, Yinglong  and\n      Wang, Qifan  and\n      Zhang, Si  and\n      Chen, Ren  and\n      Leung, Chris  and\n      Tang, Jiajie  and\n      Luo, Jiebo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.39/\",\n    doi = \"10.18653/v1/2024.findings-naacl.39\",\n    pages = \"583--612\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.39.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.39/",
        "pdf_size": 1739921,
        "gs_citation": 146,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4356677214877592971&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Rochester; UCLA; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; University of Rochester",
        "aff_domain": "ur.rochester.edu; ; ; ; ; ; ; ; ;cs.rochester.edu",
        "email": "ur.rochester.edu; ; ; ; ; ; ; ; ;cs.rochester.edu",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;2;2;2;2;2;2;2;0",
        "aff_unique_norm": "University of Rochester;University of California, Los Angeles;Meta",
        "aff_unique_dep": ";;Meta AI",
        "aff_unique_url": "https://www.rochester.edu;https://www.ucla.edu;https://meta.com",
        "aff_unique_abbr": "U of R;UCLA;Meta",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.29",
        "title": "LLM-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be Detected?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "With the rapid development and widespread application of Large Language Models (LLMs), the use of Machine-Generated Text (MGT) has become increasingly common, bringing with it potential risks, especially in terms of quality and integrity in fields like news, education, and science. Current research mainly focuses on purely MGT detection, without adequately addressing mixed scenarios including AI-revised Human-Written Text (HWT) or human-revised MGT. To tackle this challenge, we define mixtext, a form of mixed text involving both AI and human-generated content. Then we introduce MixSet, the first dataset dedicated to studying these mixtext scenarios. Leveraging MixSet, we executed comprehensive experiments to assess the efficacy of prevalent MGT detectors in handling mixtext situations, evaluating their performance in terms of effectiveness, robustness, and generalization. Our findings reveal that existing detectors struggle to identify mixtext, particularly in dealing with subtle modifications and style adaptability. This research underscores the urgent need for more fine-grain detectors tailored for mixtext, offering valuable insights for future research. Code and Models are available at https://github.com/Dongping-Chen/MixSet.",
        "author": "Qihui Zhang; Chujie Gao; Dongping Chen; Yue Huang; Yixin Huang; Zhenyang Sun; Shilin Zhang; Weiye Li; Zhengyan Fu; Yao Wan; Lichao Sun",
        "authorids": "/q/qihui-zhang/; /c/chujie-gao/; /d/dongping-chen/; /y/yue-huang/; /y/yixin-huang/; /z/zhenyang-sun/; /s/shilin-zhang/; /w/weiye-li/; /z/zhengyan-fu/; /y/yao-wan/; /l/lichao-sun/",
        "bibtex": "@inproceedings{zhang-etal-2024-llm,\n    title = \"{LLM}-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be Detected?\",\n    author = \"Zhang, Qihui  and\n      Gao, Chujie  and\n      Chen, Dongping  and\n      Huang, Yue  and\n      Huang, Yixin  and\n      Sun, Zhenyang  and\n      Zhang, Shilin  and\n      Li, Weiye  and\n      Fu, Zhengyan  and\n      Wan, Yao  and\n      Sun, Lichao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.29/\",\n    doi = \"10.18653/v1/2024.findings-naacl.29\",\n    pages = \"409--436\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.29.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.29/",
        "pdf_size": 2483276,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17485586343218101716&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Lehigh University; Lehigh University; Huazhong University of Science and Technology; University of Notre Dame; Institut Polytechnique de Paris; Lehigh University+LAIR Lab; Lehigh University+LAIR Lab; Lehigh University+LAIR Lab; Lehigh University+LAIR Lab; Huazhong University of Science and Technology; Lehigh University",
        "aff_domain": "gmail.com;gmail.com;gmail.com; ; ; ; ; ; ; ;gmail.com",
        "email": "gmail.com;gmail.com;gmail.com; ; ; ; ; ; ; ;gmail.com",
        "github": "https://github.com/Dongping-Chen/MixSet",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;0;1;2;3;0+4;0+4;0+4;0+4;1;0",
        "aff_unique_norm": "Lehigh University;Huazhong University of Science and Technology;University of Notre Dame;Institut Polytechnique de Paris;University of California, Santa Barbara",
        "aff_unique_dep": ";;;;Department of Computer Science",
        "aff_unique_url": "https://www.lehigh.edu;http://www.hust.edu.cn;https://www.nd.edu;https://www.ipparis.fr;https://www.cs.ucsb.edu",
        "aff_unique_abbr": "Lehigh;HUST;Notre Dame;IP Paris;UCSB LAIR Lab",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Santa Barbara",
        "aff_country_unique_index": "0;0;1;0;2;0+0;0+0;0+0;0+0;1;0",
        "aff_country_unique": "United States;China;France"
    },
    {
        "id": "2024.naacl-industry.36",
        "title": "LLM-based Frameworks for API Argument Filling in Task-Oriented Conversational Systems",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Task-orientated conversational agents interact with users and assist them via leveraging external APIs. A typical task-oriented conversational system can be broken down into three phases: external API selection, argument filling, and response generation. The focus of our work is the task of argument filling, which is in charge of accurately providing arguments required by the selected API. Upon comprehending the dialogue history and the pre-defined API schema, the argument filling task is expected to provide the external API with the necessary information to generate a desirable agent action. In this paper, we study the application of Large Language Models (LLMs) for the problem of API argument filling task. Our initial investigation reveals that LLMs require an additional grounding process to successfully perform argument filling, inspiring us to design training and prompting frameworks to ground their responses. Our experimental results demonstrate that when paired with proposed techniques, the argument filling performance of LLMs noticeably improves, paving a new way toward building an automated argument filling framework.",
        "author": "Jisoo Mok; Mohammad Kachuee; Shuyang Dai; Shayan Ray; Tara Taghavi; Sungroh Yoon",
        "authorids": "/j/jisoo-mok/; /m/mohammad-kachuee/; /s/shuyang-dai/; /s/shayan-ray/; /t/tara-taghavi/; /s/sungroh-yoon/",
        "bibtex": "@inproceedings{mok-etal-2024-llm,\n    title = \"{LLM}-based Frameworks for {API} Argument Filling in Task-Oriented Conversational Systems\",\n    author = \"Mok, Jisoo  and\n      Kachuee, Mohammad  and\n      Dai, Shuyang  and\n      Ray, Shayan  and\n      Taghavi, Tara  and\n      Yoon, Sungroh\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.36/\",\n    doi = \"10.18653/v1/2024.naacl-industry.36\",\n    pages = \"419--426\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.36.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.36/",
        "pdf_size": 1260282,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9450639528527740618&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of ECE, Seoul National University+Interdisciplinary Program in Artificial Intelligence, Seoul National University; Amazon; Amazon; Amazon; Amazon; Department of ECE, Seoul National University+Interdisciplinary Program in Artificial Intelligence, Seoul National University",
        "aff_domain": "snu.ac.kr; ; ; ; ; ",
        "email": "snu.ac.kr; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0;1;1;1;1;0+0",
        "aff_unique_norm": "Seoul National University;Amazon",
        "aff_unique_dep": "Department of Electrical and Computer Engineering;Amazon.com, Inc.",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.amazon.com",
        "aff_unique_abbr": "SNU;Amazon",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "Seoul;",
        "aff_country_unique_index": "0+0;1;1;1;1;0+0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2024.naacl-long.132",
        "title": "LLM-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable proficiency in comprehending and generating natural language. On the other hand, medical assistants hold the potential to offer substantial benefits for individuals. However, the exploration of LLM-based personalized medical assistant remains relatively scarce. Typically, patients converse differently based on their background and preferences which necessitates the task of enhancing user-oriented medical assistant. While one can fully train an LLM for this objective, the resource consumption is unaffordable. Prior research has explored memory-based methods to enhance the response with aware of previous mistakes for new queries during a dialogue session. We contend that a mere memory module is inadequate and fully training an LLM can be excessively costly. In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning (PEFT) schema, to personalize medical assistants. To encourage further research into this area, we are releasing a new conversation dataset generated based on an open-source medical corpus and our implementation.",
        "author": "Kai Zhang; Yangyang Kang; Fubang Zhao; Xiaozhong Liu",
        "authorids": "/k/kai-zhang/; /y/yangyang-kang/; /f/fubang-zhao/; /x/xiaozhong-liu/",
        "bibtex": "@inproceedings{zhang-etal-2024-llm-based,\n    title = \"{LLM}-based Medical Assistant Personalization with Short- and Long-Term Memory Coordination\",\n    author = \"Zhang, Kai  and\n      Kang, Yangyang  and\n      Zhao, Fubang  and\n      Liu, Xiaozhong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.132/\",\n    doi = \"10.18653/v1/2024.naacl-long.132\",\n    pages = \"2386--2398\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.132.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.132/",
        "pdf_size": 4262167,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16527468027664548782&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Worcester Polytechnic Institute, Worcester, USA; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Worcester Polytechnic Institute, Worcester, USA",
        "aff_domain": "wpi.edu;alibaba-inc.com;alibaba-inc.com;wpi.edu",
        "email": "wpi.edu;alibaba-inc.com;alibaba-inc.com;wpi.edu",
        "github": "https://github.com/MatthewKKai/MaLP",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Worcester Polytechnic Institute;Alibaba Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.wpi.edu;https://www.alibaba.com",
        "aff_unique_abbr": "WPI;Alibaba",
        "aff_campus_unique_index": "0;1;1;0",
        "aff_campus_unique": "Worcester;Hangzhou",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.findings-naacl.92",
        "title": "LLMRefine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent large language models (LLM) areleveraging human feedback to improve theirgeneration quality. However, human feedbackis costly to obtain, especially during inference.In this work, we propose LLMRefine, aninference time optimization method to refineLLM\u2019s output. The core idea is to usea learned fine-grained feedback model topinpoint defects and guide LLM to refinethem iteratively. Using original LLM as aproposal of edits, LLMRefine searches fordefect-less text via simulated annealing, tradingoff the exploration and exploitation. Weconduct experiments on three text generationtasks, including machine translation, long-form question answering (QA), and topicalsummarization. LLMRefine consistentlyoutperforms all baseline approaches, achievingimprovements up to 1.7 MetricX points ontranslation tasks, 8.1 ROUGE-L on ASQA, 2.2ROUGE-L on topical summarization.",
        "author": "Wenda Xu; Daniel Deutsch; Mara Finkelstein; Juraj Juraska; Biao Zhang; Zhongtao Liu; William Yang Wang; Lei Li; Markus Freitag",
        "authorids": "/w/wenda-xu/; /d/daniel-deutsch/; /m/mara-finkelstein/; /j/juraj-juraska/; /b/biao-zhang/; /z/zhongtao-liu/; /w/william-yang-wang/; /l/lei-li/; /m/markus-freitag/",
        "bibtex": "@inproceedings{xu-etal-2024-llmrefine,\n    title = \"{LLMR}efine: Pinpointing and Refining Large Language Models via Fine-Grained Actionable Feedback\",\n    author = \"Xu, Wenda  and\n      Deutsch, Daniel  and\n      Finkelstein, Mara  and\n      Juraska, Juraj  and\n      Zhang, Biao  and\n      Liu, Zhongtao  and\n      Wang, William Yang  and\n      Li, Lei  and\n      Freitag, Markus\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.92/\",\n    doi = \"10.18653/v1/2024.findings-naacl.92\",\n    pages = \"1429--1445\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.92.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.92/",
        "pdf_size": 1025030,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8826959889146474388&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Santa Barbara; Google; Google; Google; Google; Google; University of California, Santa Barbara; Carnegie Mellon University; Google",
        "aff_domain": "cs.ucsb.edu;google.com; ; ; ; ;cs.ucsb.edu; ;google.com",
        "email": "cs.ucsb.edu;google.com; ; ; ; ;cs.ucsb.edu; ;google.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;1;1;1;1;0;2;1",
        "aff_unique_norm": "University of California, Santa Barbara;Google;Carnegie Mellon University",
        "aff_unique_dep": ";Google;",
        "aff_unique_url": "https://www.ucsb.edu;https://www.google.com;https://www.cmu.edu",
        "aff_unique_abbr": "UCSB;Google;CMU",
        "aff_campus_unique_index": "0;1;1;1;1;1;0;1",
        "aff_campus_unique": "Santa Barbara;Mountain View;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.24",
        "title": "LLMs Are Few-Shot In-Context Low-Resource Language Learners",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages.Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages.Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages.Our study concludes the significance of few-shot in-context information on enhancing the low-resource understanding quality of LLMs through semantically relevant information by closing the language gap in the target language and aligning the semantics between the targeted low-resource and the high-resource language that the model is proficient in. Our work highlights the importance of advancing ICL research, particularly for low-resource languages.",
        "author": "Samuel Cahyawijaya; Holy Lovenia; Pascale Fung",
        "authorids": "/s/samuel-cahyawijaya/; /h/holy-lovenia/; /p/pascale-fung/",
        "bibtex": "@inproceedings{cahyawijaya-etal-2024-llms,\n    title = \"{LLM}s Are Few-Shot In-Context Low-Resource Language Learners\",\n    author = \"Cahyawijaya, Samuel  and\n      Lovenia, Holy  and\n      Fung, Pascale\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.24/\",\n    doi = \"10.18653/v1/2024.naacl-long.24\",\n    pages = \"405--433\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.24.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.24/",
        "pdf_size": 2339060,
        "gs_citation": 44,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15309901505521358774&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "HKUST; AI Singapore; HKUST",
        "aff_domain": "connect.ust.hk;aisingapore.org;ece.ust.hk",
        "email": "connect.ust.hk;aisingapore.org;ece.ust.hk",
        "github": "https://github.com/SamuelCahyawijaya/in-context-alignment",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;AI Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ust.hk;https://www.aisingapore.gov.sg",
        "aff_unique_abbr": "HKUST;AI Singapore",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2024.findings-naacl.292",
        "title": "LLaMA-Rider: Spurring Large Language Models to Explore the Open World",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently, various studies have leveraged Large Language Models (LLMs) to help decision-making and planning in environments and try to align the LLMs\u2019 knowledge with the world conditions. Nonetheless, the capacity of LLMs to continuously acquire environmental knowledge and adapt in an open world remains uncertain. In this paper, we propose an approach to spur LLMs to explore the open world, gather experiences, and learn to improve their task-solving capabilities. In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment. This facilitates exploration and enhances the model\u2019s performance. Besides, we integrate sub-task relabeling to assist LLMs in maintaining consistency in sub-task planning and help the model learn the combinatorial nature between tasks, enabling it to complete a wider range of tasks through training based on the acquired exploration experiences. By evaluation in Minecraft, an open-ended sandbox world, we demonstrate that our approach LLaMA-Rider enhances the efficiency of the LLM in exploring the environment, and effectively improves the LLM\u2019s ability to accomplish more tasks through fine-tuning with merely 1.3k instances of collected data, showing minimal training costs compared to the baseline using reinforcement learning. The code is available at https://github.com/PKU-RL/LLaMA-Rider.",
        "author": "Yicheng Feng; Yuxuan Wang; Jiazheng Liu; Sipeng Zheng; Zongqing Lu",
        "authorids": "/y/yicheng-feng/; /y/yuxuan-wang/; /j/jiazheng-liu/; /s/sipeng-zheng/; /z/zongqing-lu/",
        "bibtex": "@inproceedings{feng-etal-2024-llama,\n    title = \"{LL}a{MA}-Rider: Spurring Large Language Models to Explore the Open World\",\n    author = \"Feng, Yicheng  and\n      Wang, Yuxuan  and\n      Liu, Jiazheng  and\n      Zheng, Sipeng  and\n      Lu, Zongqing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.292/\",\n    doi = \"10.18653/v1/2024.findings-naacl.292\",\n    pages = \"4705--4724\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.292.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.292/",
        "pdf_size": 2029812,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15268025450892846610&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Peking University; School of Computer Science, Peking University; School of Computer Science, Peking University; BAAI; School of Computer Science, Peking University + BAAI",
        "aff_domain": "pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;baai.ac.cn;pku.edu.cn",
        "email": "pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;baai.ac.cn;pku.edu.cn",
        "github": "https://github.com/PKU-RL/LLaMA-Rider",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0+1",
        "aff_unique_norm": "Peking University;Beijing Academy of Artificial Intelligence",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.baaic.cn",
        "aff_unique_abbr": "PKU;BAAI",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.305",
        "title": "LLatrieval: LLM-Verified Retrieval for Verifiable Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Verifiable generation aims to let the large language model (LLM) generate text with supporting documents, which enables the user to flexibly verify the answer and makes the LLM\u2019s output more reliable. Retrieval plays a crucial role in verifiable generation. Specifically, the retrieved documents not only supplement knowledge to help the LLM generate correct answers, but also serve as supporting evidence for the user to verify the LLM\u2019s output. However, the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs. If the retriever does not correctly find the supporting documents, the LLM can not generate the correct and verifiable answer, which overshadows the LLM\u2019s remarkable abilities. To address these limitations, we propose **LLatrieval** (**L**arge **La**nguage Model Verified Re**trieval**),where the LLM updates the retrieval result until it verifies that the retrieved documents can sufficiently support answering the question. Thus, the LLM can iteratively provide feedback to retrieval and facilitate the retrieval result to fully support verifiable generation. Experiments on ALCE show that LLatrieval significantly outperforms extensive baselines and achieves state-of-the-art results.",
        "author": "Xiaonan Li; Changtai Zhu; Linyang Li; Zhangyue Yin; Tianxiang Sun; Xipeng Qiu",
        "authorids": "/x/xiaonan-li/; /c/changtai-zhu/; /l/linyang-li/; /z/zhangyue-yin/; /t/tianxiang-sun/; /x/xipeng-qiu/",
        "bibtex": "@inproceedings{li-etal-2024-llatrieval,\n    title = \"{LL}atrieval: {LLM}-Verified Retrieval for Verifiable Generation\",\n    author = \"Li, Xiaonan  and\n      Zhu, Changtai  and\n      Li, Linyang  and\n      Yin, Zhangyue  and\n      Sun, Tianxiang  and\n      Qiu, Xipeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.305/\",\n    doi = \"10.18653/v1/2024.naacl-long.305\",\n    pages = \"5453--5471\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.305.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.305/",
        "pdf_size": 627151,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12321937792830140679&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University + Shanghai Key Laboratory of Intelligent Information Processing, Fudan University",
        "aff_domain": "fudan.edu.cn;m.fudan.edu.cn; ; ; ;fudan.edu.cn",
        "email": "fudan.edu.cn;m.fudan.edu.cn; ; ; ;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0+0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.222",
        "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Today\u2019s large language models (LLMs) typically train on short text segments (e.g., <4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encod- ing scientific articles, code repositories, or long dialogues. Through both theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis reveals that commonly used techniques like using a sliding-window attention pattern or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-Infinite, a simple and effective method for enhancing LLMs\u2019 capabilities of handling long contexts. LM-Infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-Infinite brings substantial efficiency improvements: it achieves 2.7\u00d7 decoding speed up and 7.5\u00d7 memory saving over the original model. Our code will be publicly available upon publication.",
        "author": "Chi Han; Qifan Wang; Hao Peng; Wenhan Xiong; Yu Chen; Heng Ji; Sinong Wang",
        "authorids": "/c/chi-han/; /q/qifan-wang/; /h/hao-peng/; /w/wenhan-xiong/; /y/yu-chen/; /h/heng-ji/; /s/sinong-wang/",
        "bibtex": "@inproceedings{han-etal-2024-lm,\n    title = \"{LM}-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models\",\n    author = \"Han, Chi  and\n      Wang, Qifan  and\n      Peng, Hao  and\n      Xiong, Wenhan  and\n      Chen, Yu  and\n      Ji, Heng  and\n      Wang, Sinong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.222/\",\n    doi = \"10.18653/v1/2024.naacl-long.222\",\n    pages = \"3991--4008\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.222.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.222/",
        "pdf_size": 28738614,
        "gs_citation": 79,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12956769114939120755&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois Urbana-Champaign+Meta; Meta; University of Illinois Urbana-Champaign; GenAI Meta; Anytime AI+Meta; University of Illinois Urbana-Champaign; GenAI Meta",
        "aff_domain": "illinois.edu;meta.com;illinois.edu;meta.com;anytime-ai.com;illinois.edu;meta.com",
        "email": "illinois.edu;meta.com;illinois.edu;meta.com;anytime-ai.com;illinois.edu;meta.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;0;1;2+1;0;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Meta;Anytime AI",
        "aff_unique_dep": ";Meta Platforms, Inc.;",
        "aff_unique_url": "https://illinois.edu;https://meta.com;",
        "aff_unique_abbr": "UIUC;Meta;",
        "aff_campus_unique_index": "0;0;;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0+0;0;0;0+0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2024.naacl-demo.12",
        "title": "LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Foundation models have demonstrated a great ability to achieve general human-level intelligence far beyond traditional approaches. As the technique keeps attracting attention from the AI community, more and more foundation models have become publicly available.However, most of those models exhibit a major deficiency in specialized-domain and specialized-task applications, where the step of domain- and task-aware finetuning is still required to obtain scientific language models. As the number of available foundation models and specialized tasks keeps growing, the job of training scientific language models becomes highly nontrivial. In this paper, we take the first step to address this issue. We introduce an extensible and lightweight toolkit, LMFlow, which aims to simplify the domain- and task-aware finetuning of general foundation models.LMFlow offers a complete finetuning workflow for a foundation model to support specialized training with limited computing resources.Furthermore, it supports continuous pretraining, instruction tuning, parameter-efficient finetuning, alignment tuning, inference acceleration, long context generalization, model customization, and even multimodal finetuning, along with carefully designed and extensible APIs. This toolkit has been thoroughly tested and is available at https://github.com/OptimalScale/LMFlow.",
        "author": "Shizhe Diao; Rui Pan; Hanze Dong; KaShun Shum; Jipeng Zhang; Wei Xiong; Tong Zhang",
        "authorids": "/s/shizhe-diao/; /r/rui-pan/; /h/hanze-dong/; /k/kashun-shum/; /j/jipeng-zhang/; /w/wei-xiong/; /t/tong-zhang/",
        "bibtex": "@inproceedings{diao-etal-2024-lmflow,\n    title = \"{LMF}low: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models\",\n    author = \"Diao, Shizhe  and\n      Pan, Rui  and\n      Dong, Hanze  and\n      Shum, KaShun  and\n      Zhang, Jipeng  and\n      Xiong, Wei  and\n      Zhang, Tong\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.12/\",\n    doi = \"10.18653/v1/2024.naacl-demo.12\",\n    pages = \"116--127\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.12.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.12/",
        "pdf_size": 389064,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7799749128577001198&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "The Hong Kong University of Science and Technology\u2661; The Hong Kong University of Science and Technology\u2661; The Hong Kong University of Science and Technology\u2661; The Hong Kong University of Science and Technology\u2661; The Hong Kong University of Science and Technology\u2661; University of Illinois Urbana-Champaign\u2660; University of Illinois Urbana-Champaign\u2660",
        "aff_domain": "ust.hk;ust.hk;ust.hk; ; ;illinois.edu;illinois.edu",
        "email": "ust.hk;ust.hk;ust.hk; ; ;illinois.edu;illinois.edu",
        "github": "https://github.com/OptimalScale/LMFlow",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;1",
        "aff_unique_norm": "Hong Kong University of Science and Technology;University of Illinois Urbana-Champaign",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ust.hk;https://illinois.edu",
        "aff_unique_abbr": "HKUST;UIUC",
        "aff_campus_unique_index": "0;0;0;0;0;1;1",
        "aff_campus_unique": "Hong Kong SAR;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;0;0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.326",
        "title": "LSTDial: Enhancing Dialogue Generation via Long- and Short-Term Measurement Feedback",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generating high-quality responses is a key challenge for any open domain dialogue systems. However, even though there exist a variety of quality dimensions especially designed for dialogue evaluation (e.g., coherence and diversity scores), current dialogue systems rarely utilize them to guide the response generation during training. To alleviate this issue, we propose LSTDial (Long- and Short-Term Dialogue), a novel two-stage framework which generates and utilizes conversation evaluation as explicit feedback during training. Specifically, we fine-tune pre-trained dialogue systems through using turn-level quality feedback in the first stage and further train ever-improving dialogue agents through using dialogue-level quality feedback in the second stage. By using our approach on dialogue systems, capable of enabling dialogue generation with both short-term capabilities (generating more fluent, relevant and varied responses at the turn-level) and long-term capabilities (generating more coherent, engaging and informative responses at the dialogue-level). We implement LSTDial on four strong baseline models and experiment with two open-domain dialogue datasets. Experimental results show that LSTDial achieves significant improvement, enabling to generate better dialogue responses in terms of both human and automatic evaluation.",
        "author": "Guanghui Ye; Huan Zhao; Zixing Zhang; Xupeng Zha; Zhihua Jiang",
        "authorids": "/g/guanghui-ye/; /h/huan-zhao/; /z/zixing-zhang/; /x/xupeng-zha/; /z/zhihua-jiang/",
        "bibtex": "@inproceedings{ye-etal-2024-lstdial,\n    title = \"{LSTD}ial: Enhancing Dialogue Generation via Long- and Short-Term Measurement Feedback\",\n    author = \"Ye, Guanghui  and\n      Zhao, Huan  and\n      Zhang, Zixing  and\n      Zha, Xupeng  and\n      Jiang, Zhihua\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.326/\",\n    doi = \"10.18653/v1/2024.naacl-long.326\",\n    pages = \"5857--5871\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.326.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.326/",
        "pdf_size": 686265,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6574990553807297997&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer Science and Electronic Engineering, Hunan University, China; College of Computer Science and Electronic Engineering, Hunan University, China; College of Computer Science and Electronic Engineering, Hunan University, China; College of Computer Science and Electronic Engineering, Hunan University, China; Department of Computer Science, Jinan University, China",
        "aff_domain": "hnu.edu.cn;hnu.edu.cn; ; ; ",
        "email": "hnu.edu.cn;hnu.edu.cn; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Hunan University;Jinan University",
        "aff_unique_dep": "College of Computer Science and Electronic Engineering;Department of Computer Science",
        "aff_unique_url": "http://www.hnu.edu.cn;http://www.jnu.edu.cn",
        "aff_unique_abbr": ";JNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.373",
        "title": "LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Diffusion models have exhibited remarkable capabilities in text-to-image generation. However, their performance in image-to-text generation, specifically image captioning, has lagged behind Auto-Regressive (AR) models, casting doubt on their applicability for such tasks. In this work, we revisit diffusion models, highlighting their capacity for holistic context modeling and parallel decoding. With these benefits, diffusion models can alleviate the inherent limitations of AR methods, including their slow inference speed, error propagation, and unidirectional constraints. Furthermore, we identify the prior underperformance of diffusion models stemming from the absence of an effective latent space for image-text alignment, and the discrepancy between continuous diffusion processes and discrete textual data. In response, we introduce a novel architecture, LaDiC, which utilizes a split BERT to create a dedicated latent space for captions and integrates a regularization module to manage varying text lengths. Our framework also includes a diffuser for semantic image-to-text conversion and a Back&Refine technique to enhance token interactivity during inference. LaDiC achieves state-of-the-art performance for diffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2 CIDEr, demonstrating exceptional performance without pre-training or ancillary modules. This indicates strong competitiveness with AR models, revealing the previously untapped potential of diffusion models in image-to-text generation.",
        "author": "Yuchi Wang; Shuhuai Ren; Rundong Gao; Linli Yao; Qingyan Guo; Kaikai An; Jianhong Bai; Xu Sun",
        "authorids": "/y/yuchi-wang/; /s/shuhuai-ren/; /r/rundong-gao/; /l/linli-yao/; /q/qingyan-guo/; /k/kaikai-an/; /j/jianhong-bai/; /x/xu-sun/",
        "bibtex": "@inproceedings{wang-etal-2024-ladic,\n    title = \"{L}a{D}i{C}: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?\",\n    author = \"Wang, Yuchi  and\n      Ren, Shuhuai  and\n      Gao, Rundong  and\n      Yao, Linli  and\n      Guo, Qingyan  and\n      An, Kaikai  and\n      Bai, Jianhong  and\n      Sun, Xu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.373/\",\n    doi = \"10.18653/v1/2024.naacl-long.373\",\n    pages = \"6699--6715\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.373.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.373/",
        "pdf_size": 2957806,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14758200027819400747&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "National Key Laboratory for Multimedia Information Processing, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University; Tsinghua University; National Key Laboratory for Multimedia Information Processing, Peking University; Zhejiang University; National Key Laboratory for Multimedia Information Processing, Peking University",
        "aff_domain": "stu.pku.edu.cn;stu.pku.edu.cn; ; ; ; ; ;pku.edu.cn",
        "email": "stu.pku.edu.cn;stu.pku.edu.cn; ; ; ; ; ;pku.edu.cn",
        "github": "https://github.com/wangyuchi369/LaDiC",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;1;0;2;0",
        "aff_unique_norm": "Peking University;Tsinghua University;Zhejiang University",
        "aff_unique_dep": "National Key Laboratory for Multimedia Information Processing;;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.tsinghua.edu.cn;https://www.zju.edu.cn",
        "aff_unique_abbr": "PKU;THU;ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.60",
        "title": "LangNav: Language as a Perceptual Representation for Navigation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We explore the use of language as a perceptual representation for vision-and-language navigation (VLN), with a focus on low-data settings. Our approach uses off-the-shelf vision systems for image captioning and object detection to convert an agent\u2019s egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore several use cases of our language-based navigation (LangNav) approach on the R2R VLN benchmark: generating synthetic trajectories from a prompted language model (GPT-4) with which to finetune a smaller language model; domain transfer where we transfer a policy learned on one simulated environment (ALFRED) to another (more realistic) environment (R2R); and combining both vision- and language-based representations for VLN. Our approach is found to improve upon baselines that rely on visual features in settings where only a few expert trajectories (10-100) are available, demonstrating the potential of language as a perceptual representation for navigation.",
        "author": "Bowen Pan; Rameswar Panda; SouYoung Jin; Rogerio Feris; Aude Oliva; Phillip Isola; Yoon Kim",
        "authorids": "/b/bowen-pan/; /r/rameswar-panda/; /s/souyoung-jin/; /r/rogerio-feris/; /a/aude-oliva/; /p/phillip-isola/; /y/yoon-kim/",
        "bibtex": "@inproceedings{pan-etal-2024-langnav,\n    title = \"{L}ang{N}av: Language as a Perceptual Representation for Navigation\",\n    author = \"Pan, Bowen  and\n      Panda, Rameswar  and\n      Jin, SouYoung  and\n      Feris, Rogerio  and\n      Oliva, Aude  and\n      Isola, Phillip  and\n      Kim, Yoon\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.60/\",\n    doi = \"10.18653/v1/2024.findings-naacl.60\",\n    pages = \"950--974\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.60.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.60/",
        "pdf_size": 1913693,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15739144564300280860&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "MIT CSAIL; MIT-IBM Watson AI Lab; Dartmouth College; MIT-IBM Watson AI Lab; MIT CSAIL; MIT CSAIL; MIT CSAIL",
        "aff_domain": "mit.edu;ibm.com;dartmouth.edu;us.ibm.com;mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;ibm.com;dartmouth.edu;us.ibm.com;mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Dartmouth College",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory;",
        "aff_unique_url": "https://www.csail.mit.edu;https://www.dartmouth.edu",
        "aff_unique_abbr": "MIT CSAIL;Dartmouth",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.38",
        "title": "Language Agnostic Code Embeddings",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, code language models have achieved notable advancements in addressing a diverse array of essential code comprehension and generation tasks. Yet, the field lacks a comprehensive deep dive and understanding of the code embeddings of multilingual code models. In this paper, we present a comprehensive study on multilingual code embeddings, focusing on the cross-lingual capabilities of these embeddings across different programming languages. Through probing experiments, we demonstrate that code embeddings comprise two distinct components: one deeply tied to the nuances and syntax of a specific language, and the other remaining agnostic to these details, primarily focusing on semantics. Further, we show that when we isolate and eliminate this language-specific component, we witness significant improvements in downstream code retrieval tasks, leading to an absolute increase of up to +17 in the Mean Reciprocal Rank (MRR).",
        "author": "Saiteja Utpala; Alex Gu; Pin-Yu Chen",
        "authorids": "/s/saiteja-utpala/; /a/alex-gu/; /p/pin-yu-chen/",
        "bibtex": "@inproceedings{utpala-etal-2024-language,\n    title = \"Language Agnostic Code Embeddings\",\n    author = \"Utpala, Saiteja  and\n      Gu, Alex  and\n      Chen, Pin-Yu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.38/\",\n    doi = \"10.18653/v1/2024.naacl-long.38\",\n    pages = \"678--691\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.38.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.38/",
        "pdf_size": 2820877,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13097750075967336142&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Cohere For AI; MIT; IBM Research",
        "aff_domain": "gmail.com;mit.edu;ibm.com",
        "email": "gmail.com;mit.edu;ibm.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Cohere;Massachusetts Institute of Technology;IBM",
        "aff_unique_dep": "Cohere AI;;IBM Research",
        "aff_unique_url": "https://cohere.ai;https://web.mit.edu;https://www.ibm.com/research",
        "aff_unique_abbr": "Cohere;MIT;IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.352",
        "title": "Language Model Based Unsupervised Dependency Parsing with Conditional Mutual Information and Grammatical Constraints",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Previous methods based on Large Language Models (LLM) perform unsupervised dependency parsing by maximizing bi-lexical dependence scores. However, these previous methods adopt dependence scores that are difficult to interpret. These methods cannot incorporate grammatical constraints that previous grammar-based parsing research has shown beneficial to improving parsing performance. In this work, we apply Conditional Mutual Information (CMI), an interpretable metric, to measure the bi-lexical dependence and incorporate grammatical constraints into LLM-based unsupervised parsing. We incorporate Part-Of-Speech information as a grammatical constraint at the CMI estimation stage and integrate two additional grammatical constraints at the subsequent tree decoding stage. We find that the CMI score positively correlates with syntactic dependencies and has a stronger correlation with the syntactic dependencies than baseline scores. Our experiment confirms the benefits and applicability of the proposed grammatical constraints across five languages and eight datasets. The CMI parsing model outperforms state-of-the-art LLM-based models and similarly constrained grammar-based models. Our analysis reveals that the CMI model is strong in retrieving dependency relations with rich lexical interactions but is weak in retrieving relations with sparse lexical interactions, indicating a potential limitation in CMI-based unsupervised parsing methods.",
        "author": "Junjie Chen; Xiangheng He; Yusuke Miyao",
        "authorids": "/j/junjie-chen/; /x/xiangheng-he/; /y/yusuke-miyao/",
        "bibtex": "@inproceedings{chen-etal-2024-language,\n    title = \"Language Model Based Unsupervised Dependency Parsing with Conditional Mutual Information and Grammatical Constraints\",\n    author = \"Chen, Junjie  and\n      He, Xiangheng  and\n      Miyao, Yusuke\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.352/\",\n    doi = \"10.18653/v1/2024.naacl-long.352\",\n    pages = \"6355--6366\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.352.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.352/",
        "pdf_size": 910249,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:abW_J_JVuO4J:scholar.google.com/&scioq=Language+Model+Based+Unsupervised+Dependency+Parsing+with+Conditional+Mutual+Information+and+Grammatical+Constraints&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "The University of Tokyo; Imperial College London; The University of Tokyo",
        "aff_domain": "g.ecc.u-tokyo.ac.jp;imperial.ac.uk;is.s.u-tokyo.ac.jp",
        "email": "g.ecc.u-tokyo.ac.jp;imperial.ac.uk;is.s.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Tokyo;Imperial College London",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.imperial.ac.uk",
        "aff_unique_abbr": "UTokyo;ICL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Japan;United Kingdom"
    },
    {
        "id": "2024.naacl-short.51",
        "title": "Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Situations and events evoke emotions in humans, but to what extent do they inform the prediction of emotion detection models? This work investigates how well human-annotated emotion triggers correlate with features that models deemed salient in their prediction of emotions. First, we introduce a novel dataset EmoTrigger, consisting of 900 social media posts sourced from three different datasets; these were annotated by experts for emotion triggers with high agreement. Using EmoTrigger, we evaluate the ability of large language models (LLMs) to identify emotion triggers, and conduct a comparative analysis of the features considered important for these tasks between LLMs and fine-tuned models. Our analysis reveals that emotion triggers are largely not considered salient features for emotion prediction models, instead there is intricate interplay between various features and the task of emotion detection.",
        "author": "Smriti Singh; Cornelia Caragea; Junyi Jessy Li",
        "authorids": "/s/smriti-singh/; /c/cornelia-caragea/; /j/junyi-jessy-li/",
        "bibtex": "@inproceedings{singh-etal-2024-language,\n    title = \"Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion\",\n    author = \"Singh, Smriti  and\n      Caragea, Cornelia  and\n      Li, Junyi Jessy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.51/\",\n    doi = \"10.18653/v1/2024.naacl-short.51\",\n    pages = \"603--614\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.51.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.51/",
        "pdf_size": 532096,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1610863030416817493&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "The University of Texas at Austin; University of Illinois Chicago; The University of Texas at Austin",
        "aff_domain": "utexas.edu;uic.edu;utexas.edu",
        "email": "utexas.edu;uic.edu;utexas.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Texas at Austin;University of Illinois at Chicago",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utexas.edu;https://www.uic.edu",
        "aff_unique_abbr": "UT Austin;UIC",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Austin;Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.62",
        "title": "Language Models Hallucinate, but May Excel at Fact Verification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless, LLMs frequently \u201challucinate,\u201d resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments. Surprisingly, FLAN-T5-11B , the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these LLMs on high-quality evidence, as well as their deficiencies in robustness and generalization ability. Our study presents insights for developing trustworthy generation models.",
        "author": "Jian Guan; Jesse Dodge; David Wadden; Minlie Huang; Hao Peng",
        "authorids": "/j/jian-guan/; /j/jesse-dodge/; /d/david-wadden/; /m/minlie-huang/; /h/hao-peng/",
        "bibtex": "@inproceedings{guan-etal-2024-language,\n    title = \"Language Models Hallucinate, but May Excel at Fact Verification\",\n    author = \"Guan, Jian  and\n      Dodge, Jesse  and\n      Wadden, David  and\n      Huang, Minlie  and\n      Peng, Hao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.62/\",\n    doi = \"10.18653/v1/2024.naacl-long.62\",\n    pages = \"1090--1111\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.62.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.62/",
        "pdf_size": 553068,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6334474605864335663&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China; Allen Institute for AI; Allen Institute for AI; The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China; University of Illinois Urbana-Champaign",
        "aff_domain": "mails.tsinghua.edu.cn;allenai.org;allenai.org;tsinghua.edu.cn;illinois.edu",
        "email": "mails.tsinghua.edu.cn;allenai.org;allenai.org;tsinghua.edu.cn;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;2",
        "aff_unique_norm": "Tsinghua University;Allen Institute for AI;University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Institute for Artificial Intelligence;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://allenai.org;https://illinois.edu",
        "aff_unique_abbr": "Tsinghua;AI2;UIUC",
        "aff_campus_unique_index": "0;0;2",
        "aff_campus_unique": "Beijing;;Urbana-Champaign",
        "aff_country_unique_index": "0;1;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.281",
        "title": "Language Models Implement Simple Word2Vec-style Vector Arithmetic",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple vector arithmetic style mechanism to solve some relational tasks using regularities encoded in the hidden space of the model (e.g., Poland:Warsaw::China:Beijing). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, uppercasing, and past-tensing) a key part of the mechanism reduces to a simple additive update typically applied by the feedforward (FFN) networks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a growing body of work on the interpretability of LMs, and offer reason to be optimistic that, despite the massive and non-linear nature of the models, the strategies they ultimately use to solve tasks can sometimes reduce to familiar and even intuitive algorithms.",
        "author": "Jack Merullo; Carsten Eickhoff; Ellie Pavlick",
        "authorids": "/j/jack-merullo/; /c/carsten-eickhoff/; /e/ellie-pavlick/",
        "bibtex": "@inproceedings{merullo-etal-2024-language,\n    title = \"Language Models Implement Simple {W}ord2{V}ec-style Vector Arithmetic\",\n    author = \"Merullo, Jack  and\n      Eickhoff, Carsten  and\n      Pavlick, Ellie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.281/\",\n    doi = \"10.18653/v1/2024.naacl-long.281\",\n    pages = \"5030--5047\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.281.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.281/",
        "pdf_size": 1359029,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1877407992689383917&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, Brown University; School of Medicine, University of T\u00fcbingen; Department of Computer Science, Brown University",
        "aff_domain": "brown.edu;uni-tuebingen.de;brown.edu",
        "email": "brown.edu;uni-tuebingen.de;brown.edu",
        "github": "https://github.com/jmerullo/lm_vector_arithmetic",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Brown University;University of T\u00fcbingen",
        "aff_unique_dep": "Department of Computer Science;School of Medicine",
        "aff_unique_url": "https://www.brown.edu;https://www.uni-tuebingen.de",
        "aff_unique_abbr": "Brown;Uni T\u00fcbingen",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "2024.naacl-industry.18",
        "title": "Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In difficult decision-making scenarios, it is common to have conflicting opinions among expert human decision-makers as there may not be a single right answer. Such decisions may be guided by different attributes that can be used to characterize an individual\u2019s decision. We introduce a novel dataset for medical triage decision-making, labeled with a set of decision-maker attributes (DMAs). This dataset consists of 62 scenarios, covering six different DMAs, including ethical principles such as fairness and moral desert. We present a novel software framework for human-aligned decision-making by utilizing these DMAs, paving the way for trustworthy AI with better guardrails. Specifically, we demonstrate how large language models (LLMs) can serve as ethical decision-makers, and how their decisions can be aligned to different DMAs using zero-shot prompting. Our experiments focus on different open-source models with varying sizes and training techniques, such as Falcon, Mistral, and Llama 2. Finally, we also introduce a new form of weighted self-consistency that improves the overall quantified performance. Our results provide new research directions in the use of LLMs as alignable decision-makers. The dataset and open-source software are publicly available at: https://github.com/ITM-Kitware/llm-alignable-dm.",
        "author": "Brian Hu; Bill Ray; Alice Leung; Amy Summerville; David Joy; Christopher Funk; Arslan Basharat",
        "authorids": "/b/brian-hu/; /b/bill-ray/; /a/alice-leung/; /a/amy-summerville/; /d/david-joy/; /c/christopher-funk/; /a/arslan-basharat/",
        "bibtex": "@inproceedings{hu-etal-2024-language,\n    title = \"Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain\",\n    author = \"Hu, Brian  and\n      Ray, Bill  and\n      Leung, Alice  and\n      Summerville, Amy  and\n      Joy, David  and\n      Funk, Christopher  and\n      Basharat, Arslan\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.18/\",\n    doi = \"10.18653/v1/2024.naacl-industry.18\",\n    pages = \"213--227\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.18.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.18/",
        "pdf_size": 1596075,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7971654082036421266&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Kitware, Inc.; Kitware, Inc.; Raytheon/BBN Technologies Corp.; Kairos Research, LLC; Kitware, Inc.; Kitware, Inc.; Kitware, Inc.",
        "aff_domain": "kitware.com;kitware.com;rtx.com;kairosresearch.com;kitware.com;kitware.com;kitware.com",
        "email": "kitware.com;kitware.com;rtx.com;kairosresearch.com;kitware.com;kitware.com;kitware.com",
        "github": "https://github.com/ITM-Kitware/llm-alignable-dm",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;2;0;0;0",
        "aff_unique_norm": "Kitware, Inc.;Raytheon Technologies;Kairos Research",
        "aff_unique_dep": ";;Research",
        "aff_unique_url": "https://www.kitware.com;https://www.raytheon.com;",
        "aff_unique_abbr": "Kitware;RTX;Kairos",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.254",
        "title": "Language Models can be Deductive Solvers",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decision-making. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge. The state-of-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers. Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of external logical solvers and no answer to the logical questions. In this paper, we introduce LoGiPT, a novel language model that directly internalizes and emulates the reasoning processes of logical solvers and avoids parsing errors by learning strict adherence to solver syntax and grammar. LoGiPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers. Experimental results on two public deductive reasoning benchmarks show that LoGiPT outperforms state-of-the-art solver-augmented LMs and few-shot prompting methods on competitive LLMs like GPT-4. This project is available in https://github.com/Cyril-JZ/LoGiPT.",
        "author": "Jiazhan Feng; Ruochen Xu; Junheng Hao; Hiteshi Sharma; Yelong Shen; Dongyan Zhao; Weizhu Chen",
        "authorids": "/j/jiazhan-feng/; /r/ruochen-xu/; /j/junheng-hao/; /h/hiteshi-sharma/; /y/yelong-shen/; /d/dongyan-zhao/; /w/weizhu-chen/",
        "bibtex": "@inproceedings{feng-etal-2024-language,\n    title = \"Language Models can be Deductive Solvers\",\n    author = \"Feng, Jiazhan  and\n      Xu, Ruochen  and\n      Hao, Junheng  and\n      Sharma, Hiteshi  and\n      Shen, Yelong  and\n      Zhao, Dongyan  and\n      Chen, Weizhu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.254/\",\n    doi = \"10.18653/v1/2024.findings-naacl.254\",\n    pages = \"4026--4042\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.254.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.254/",
        "pdf_size": 413004,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11848217028895218955&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Wangxuan Institute of Computer Technology, Peking University, Beijing + Microsoft AI, Redmond; Microsoft AI, Redmond; Microsoft AI, Redmond; Microsoft AI, Redmond; Microsoft AI, Redmond; Wangxuan Institute of Computer Technology, Peking University, Beijing + National Key Laboratory of General Artificial Intelligence, Beijing + Microsoft AI, Redmond; Microsoft AI, Redmond",
        "aff_domain": "pku.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;pku.edu.cn;microsoft.com",
        "email": "pku.edu.cn;microsoft.com;microsoft.com;microsoft.com;microsoft.com;pku.edu.cn;microsoft.com",
        "github": "https://github.com/Cyril-JZ/LoGiPT",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;1;1;0+2+1;1",
        "aff_unique_norm": "Peking University;Microsoft;National Key Laboratory of General Artificial Intelligence",
        "aff_unique_dep": "Wangxuan Institute of Computer Technology;Microsoft AI;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.microsoft.com;",
        "aff_unique_abbr": "PKU;Microsoft;",
        "aff_campus_unique_index": "0+1;1;1;1;1;0+1;1",
        "aff_campus_unique": "Beijing;Redmond;",
        "aff_country_unique_index": "0+1;1;1;1;1;0+0+1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-short.68",
        "title": "Language-Independent Representations Improve Zero-Shot Summarization",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Finetuning pretrained models on downstream generation tasks often leads to catastrophic forgetting in zero-shot conditions. In this work, we focus on summarization and tackle the problem through the lens of language-independent representations. After training on monolingual summarization, we perform zero-shot transfer to new languages or language pairs. We first show naively finetuned models are highly language-specific in both output behavior and internal representations, resulting in poor zero-shot performance. Next, we propose query-key (QK) finetuning to decouple task-specific knowledge from the pretrained language generation abilities. Then, after showing downsides of the standard adversarial language classifier, we propose a balanced variant that more directly enforces language-agnostic representations. Moreover, our qualitative analyses show removing source language identity correlates to zero-shot summarization performance. Our code is openly available.",
        "author": "Vladimir Solovyev; Danni Liu; Jan Niehues",
        "authorids": "/v/vladimir-solovyev/; /d/danni-liu/; /j/jan-niehues/",
        "bibtex": "@inproceedings{solovyev-etal-2024-language,\n    title = \"Language-Independent Representations Improve Zero-Shot Summarization\",\n    author = \"Solovyev, Vladimir  and\n      Liu, Danni  and\n      Niehues, Jan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.68/\",\n    doi = \"10.18653/v1/2024.naacl-short.68\",\n    pages = \"772--782\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.68.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.68/",
        "pdf_size": 298959,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:zLJyvJwVCWQJ:scholar.google.com/&scioq=Language-Independent+Representations+Improve+Zero-Shot+Summarization&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Karlsruhe Institute of Technology, Germany; Karlsruhe Institute of Technology, Germany; Karlsruhe Institute of Technology, Germany",
        "aff_domain": "gmail.com;kit.edu;kit.edu",
        "email": "gmail.com;kit.edu;kit.edu",
        "github": "https://github.com/vladsolovyev/fairseq_summarization/tree/main/summarization_scripts",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Karlsruhe Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kit.edu",
        "aff_unique_abbr": "KIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.215",
        "title": "LanguageFlow: Advancing Diffusion Language Generation with Probabilistic Flows",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent works have demonstrated success in controlling sentence attributes (e.g., sentiment) and structure (e.g., syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow (LF).Our method is based on the reformulation of the standard probabilistic flow models.Language rectified flow learns (neural) ordinary differentialequation models to transport between the source distribution and the target distribution, henceproviding a unified and effective solution to generative modeling and domain transfer.From the source distribution, our language rectified flow yields fast simulation and effectively decreases the inference time. Experiments on three challenging fine-grained control tasks and multiple high-quality text editing show that our method consistently outperforms its baselines. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks.",
        "author": "Shujian Zhang; Lemeng Wu; Chengyue Gong; Xingchao Liu",
        "authorids": "/s/shujian-zhang/; /l/lemeng-wu/; /c/chengyue-gong/; /x/xingchao-liu/",
        "bibtex": "@inproceedings{zhang-etal-2024-languageflow,\n    title = \"{L}anguage{F}low: Advancing Diffusion Language Generation with Probabilistic Flows\",\n    author = \"Zhang, Shujian  and\n      Wu, Lemeng  and\n      Gong, Chengyue  and\n      Liu, Xingchao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.215/\",\n    doi = \"10.18653/v1/2024.naacl-long.215\",\n    pages = \"3893--3905\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.215.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.215/",
        "pdf_size": 352993,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16006919146749690708&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin",
        "aff_domain": "utexas.edu;utexas.edu;utexas.edu;utexas.edu",
        "email": "utexas.edu;utexas.edu;utexas.edu;utexas.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Texas at Austin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.utexas.edu",
        "aff_unique_abbr": "UT Austin",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Austin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.477",
        "title": "Large Human Language Models: A Need and the Challenges",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "As research in human-centered NLP advances, there is a growing recognition of the importance of incorporating human and social factors into NLP models. At the same time, our NLP systems have become heavily reliant on LLMs, most of which do not model authors. To build NLP systems that can truly understand human language, we must better integrate human contexts into LLMs. This brings to the fore a range of design considerations and challenges in terms of what human aspects to capture, how to represent them, and what modeling strategies to pursue. To address these, we advocate for three positions toward creating large human language models (LHLMs) using concepts from psychological and behavioral sciences: First, LM training should include the human context. Second, LHLMs should recognize that people are more than their group(s). Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human context. We refer to relevant advances and present open challenges that need to be addressed and their possible solutions in realizing these goals.",
        "author": "Nikita Soni; H. Andrew Schwartz; Jo\u00e3o Sedoc; Niranjan Balasubramanian",
        "authorids": "/n/nikita-soni/; /h/h-andrew-schwartz/; /j/joao-sedoc/; /n/niranjan-balasubramanian/",
        "bibtex": "@inproceedings{soni-etal-2024-large,\n    title = \"Large Human Language Models: A Need and the Challenges\",\n    author = \"Soni, Nikita  and\n      Schwartz, H. Andrew  and\n      Sedoc, Jo{\\~a}o  and\n      Balasubramanian, Niranjan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.477/\",\n    doi = \"10.18653/v1/2024.naacl-long.477\",\n    pages = \"8631--8646\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.477.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.477/",
        "pdf_size": 736177,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15045781596779413932&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Stony Brook University; Stony Brook University; New York University; Stony Brook University",
        "aff_domain": "cs.stonybrook.edu;cs.stonybrook.edu;stern.nyu.edu;cs.stonybrook.edu",
        "email": "cs.stonybrook.edu;cs.stonybrook.edu;stern.nyu.edu;cs.stonybrook.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Stony Brook University;New York University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stonybrook.edu;https://www.nyu.edu",
        "aff_unique_abbr": "SBU;NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.37",
        "title": "Large Language Models Encode the Practice of Medicine",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Healthcare tasks such as predicting clinical outcomes across medical and surgical populations, disease prediction, predicting patient health journeys, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of billions of administrative claims, which essentially encapsulates the practice of medicine, offering a unique perspective on patient care and treatment patterns. Our model, MediClaimGPT, a 125M parameter Transformer demonstrates strong zero-shot predictive capabilities, accurately forecasting patient health events across four evaluation datasets, with its capabilities further demonstrated in various downstream tasks. A significant application of MediClaimGPT is in generating high-quality, clinically plausible synthetic claims data, enhancing healthcare data utility while preserving patient privacy. This research underscores the potential of language models in handling complex datasets and their strategic application in healthcare and related fields.",
        "author": "Teja Kanchinadam; Gauher Shaheen",
        "authorids": "/t/teja-kanchinadam/; /g/gauher-shaheen/",
        "bibtex": "@inproceedings{kanchinadam-shaheen-2024-large,\n    title = \"Large Language Models Encode the Practice of Medicine\",\n    author = \"Kanchinadam, Teja  and\n      Shaheen, Gauher\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.37/\",\n    doi = \"10.18653/v1/2024.naacl-industry.37\",\n    pages = \"427--436\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.37.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.37/",
        "pdf_size": 767525,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:9iYA3NkmVJgJ:scholar.google.com/&scioq=Large+Language+Models+Encode+the+Practice+of+Medicine&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Enterprise Data Science & AI, Elevance Health Inc., Indianapolis, IN, USA; Enterprise Data Science & AI, Elevance Health Inc., Indianapolis, IN, USA",
        "aff_domain": "carelon.com;carelon.com",
        "email": "carelon.com;carelon.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Elevance Health Inc.",
        "aff_unique_dep": "Enterprise Data Science & AI",
        "aff_unique_url": "https://www.elevancehealth.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Indianapolis",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.81",
        "title": "Large Language Models Help Humans Verify Truthfulness \u2013 Except When They Are Convincingly Wrong",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they get, LLMs should not only provide information but also help users fact-check it. We conduct human experiments with 80 crowdworkers to compare language models with search engines (information retrieval systems) at facilitating fact-checking. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than those using search engines while achieving similar accuracy. However, they over-rely on the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information\u2014explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users\u2019 over-reliance on LLMs, but cannot significantly outperform search engines. Further, showing both search engine results and LLM explanations offers no complementary benefits compared to search engines alone. Taken together, our study highlights that natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences.",
        "author": "Chenglei Si; Navita Goyal; Tongshuang Wu; Chen Zhao; Shi Feng; Hal Daum\u00e9 Iii; Jordan Boyd-Graber",
        "authorids": "/c/chenglei-si/; /n/navita-goyal/; /t/tongshuang-wu/; /c/chen-zhao/; /s/shi-feng/; /h/hal-daume-iii/; /j/jordan-boyd-graber/",
        "bibtex": "@inproceedings{si-etal-2024-large,\n    title = \"Large Language Models Help Humans Verify Truthfulness {--} Except When They Are Convincingly Wrong\",\n    author = \"Si, Chenglei  and\n      Goyal, Navita  and\n      Wu, Tongshuang  and\n      Zhao, Chen  and\n      Feng, Shi  and\n      Daum{\\'e} Iii, Hal  and\n      Boyd-Graber, Jordan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.81/\",\n    doi = \"10.18653/v1/2024.naacl-long.81\",\n    pages = \"1459--1474\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.81.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.81/",
        "pdf_size": 1615527,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12642053685529783295&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Stanford University; University of Maryland; Carnegie Mellon University; NYU Shanghai; New York University; Microsoft Research; University of Maryland",
        "aff_domain": "stanford.edu; ; ; ; ; ; ",
        "email": "stanford.edu; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;3;4;5;1",
        "aff_unique_norm": "Stanford University;University of Maryland;Carnegie Mellon University;New York University Shanghai;New York University;Microsoft",
        "aff_unique_dep": ";;;;;Microsoft Research",
        "aff_unique_url": "https://www.stanford.edu;https://www/umd.edu;https://www.cmu.edu;https://shanghai.nyu.edu;https://www.nyu.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Stanford;UMD;CMU;NYU Shanghai;NYU;MSR",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Stanford;;Shanghai",
        "aff_country_unique_index": "0;0;0;1;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.findings-naacl.130",
        "title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions\u2014commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 85% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model\u2019s bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs\u2019 predictions, leading to up to 8 percentage points improvement across different models and benchmarks.",
        "author": "Pouya Pezeshkpour; Estevam Hruschka",
        "authorids": "/p/pouya-pezeshkpour/; /e/estevam-hruschka/",
        "bibtex": "@inproceedings{pezeshkpour-hruschka-2024-large,\n    title = \"Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions\",\n    author = \"Pezeshkpour, Pouya  and\n      Hruschka, Estevam\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.130/\",\n    doi = \"10.18653/v1/2024.findings-naacl.130\",\n    pages = \"2006--2017\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.130.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.130/",
        "pdf_size": 439433,
        "gs_citation": 196,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1537954561847429082&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Megagon Labs; Megagon Labs",
        "aff_domain": "megagon.ai;megagon.ai",
        "email": "megagon.ai;megagon.ai",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Megagon Labs",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.megagonlabs.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.97",
        "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets.We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP).Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10.Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.",
        "author": "Zhen Qin; Rolf Jagerman; Kai Hui; Honglei Zhuang; Junru Wu; Le Yan; Jiaming Shen; Tianqi Liu; Jialu Liu; Donald Metzler; Xuanhui Wang; Michael Bendersky",
        "authorids": "/z/zhen-qin/; /r/rolf-jagerman/; /k/kai-hui/; /h/honglei-zhuang/; /j/junru-wu/; /l/le-yan/; /j/jiaming-shen/; /t/tianqi-liu/; /j/jialu-liu/; /d/donald-metzler/; /x/xuanhui-wang/; /m/michael-bendersky/",
        "bibtex": "@inproceedings{qin-etal-2024-large,\n    title = \"Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting\",\n    author = \"Qin, Zhen  and\n      Jagerman, Rolf  and\n      Hui, Kai  and\n      Zhuang, Honglei  and\n      Wu, Junru  and\n      Yan, Le  and\n      Shen, Jiaming  and\n      Liu, Tianqi  and\n      Liu, Jialu  and\n      Metzler, Donald  and\n      Wang, Xuanhui  and\n      Bendersky, Michael\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.97/\",\n    doi = \"10.18653/v1/2024.findings-naacl.97\",\n    pages = \"1504--1518\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.97.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.97/",
        "pdf_size": 260689,
        "gs_citation": 255,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16114746473883483909&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 12,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.436",
        "title": "Large Language Models can Contrastively Refine their Generation for Better Sentence Representation Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, large language models (LLMs) have emerged as a groundbreaking technology and their unparalleled text generation capabilities have sparked interest in their application to the fundamental sentence representation learning task. Existing methods have explored utilizing LLMs as data annotators to generate synthesized data for training contrastive learning based sentence embedding models such as SimCSE. However, since contrastive learning models are sensitive to the quality of sentence pairs, the effectiveness of these methods is largely influenced by the content generated from LLMs, highlighting the need for more refined generation in the context of sentence representation learning. Building upon this premise, we propose MultiCSR, a multi-level contrastive sentence representation learning framework that decomposes the process of prompting LLMs to generate a corpus for training base sentence embedding models into three stages (i.e., sentence generation, sentence pair construction, in-batch training) and refines the generated content at these three distinct stages, ensuring only high-quality sentence pairs are utilized to train a base contrastive learning model. Our extensive experiments reveal that MultiCSR enables a less advanced LLM to surpass the performance of ChatGPT, while applying it to ChatGPT achieves better state-of-the-art results. Comprehensive analyses further underscore the potential of our framework in various application scenarios and achieving better sentence representation learning with LLMs.",
        "author": "Huiming Wang; Zhaodonghui Li; Liying Cheng; De Wen Soh; Lidong Bing",
        "authorids": "/h/huiming-wang/; /z/zhaodonghui-li/; /l/liying-cheng/; /d/de-wen-soh/; /l/lidong-bing/",
        "bibtex": "@inproceedings{wang-etal-2024-large-language,\n    title = \"Large Language Models can Contrastively Refine their Generation for Better Sentence Representation Learning\",\n    author = \"Wang, Huiming  and\n      Li, Zhaodonghui  and\n      Cheng, Liying  and\n      Soh, De Wen  and\n      Bing, Lidong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.436/\",\n    doi = \"10.18653/v1/2024.naacl-long.436\",\n    pages = \"7874--7891\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.436.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.436/",
        "pdf_size": 1630281,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16354928829817306681&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Singapore University of Technology and Design+DAMO Academy, Alibaba Group, Singapore; DAMO Academy, Alibaba Group, Singapore+Nanyang Technological University, Singapore; DAMO Academy, Alibaba Group, Singapore+Hupan Lab, 310023, Hangzhou, China; Singapore University of Technology and Design; DAMO Academy, Alibaba Group, Singapore+Hupan Lab, 310023, Hangzhou, China",
        "aff_domain": "mymail.sutd.edu.sg;alibaba-inc.com;alibaba-inc.com;sutd.edu.sg;alibaba-inc.com",
        "email": "mymail.sutd.edu.sg;alibaba-inc.com;alibaba-inc.com;sutd.edu.sg;alibaba-inc.com",
        "github": "https://github.com/Circle-Ming/MultiCSR",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1+2;1+3;0;1+3",
        "aff_unique_norm": "Singapore University of Technology and Design;Alibaba Group;Nanyang Technological University;Hupan Lab",
        "aff_unique_dep": ";DAMO Academy;;",
        "aff_unique_url": "https://www.sutd.edu.sg;https://www.alibaba.com;https://www.ntu.edu.sg;",
        "aff_unique_abbr": "SUTD;Alibaba;NTU;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+1;0;0+1",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "2024.findings-naacl.171",
        "title": "LatticeGen: Hiding Generated Text in a Lattice for Privacy-Aware Large Language Model Generation on Cloud",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In the current user-server interaction paradigm of prompted generation with large language models (LLMs) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text private to themselves. For privacy-aware text generation on cloud, we propose LatticeGen, a cooperative protocol in which the server still handles most of the computation while the client controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the client and hidden in a noised lattice. Only the client knows which tokens are the true ones. Considering potential attacks from a hypothetically malicious server and how the client can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as measured by BERTScore).",
        "author": "Mengke Zhang; Tianxing He; Tianle Wang; Lu Mi; Niloofar Mireshghallah; Binyi Chen; Hao Wang; Yulia Tsvetkov",
        "authorids": "/m/mengke-zhang/; /t/tianxing-he/; /t/tianle-wang/; /l/lu-mi/; /n/niloofar-mireshghallah/; /b/binyi-chen/; /h/hao-wang/; /y/yulia-tsvetkov/",
        "bibtex": "@inproceedings{zhang-etal-2024-latticegen,\n    title = \"{L}attice{G}en: Hiding Generated Text in a Lattice for Privacy-Aware Large Language Model Generation on Cloud\",\n    author = \"Zhang, Mengke  and\n      He, Tianxing  and\n      Wang, Tianle  and\n      Mi, Lu  and\n      Mireshghallah, Niloofar  and\n      Chen, Binyi  and\n      Wang, Hao  and\n      Tsvetkov, Yulia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.171/\",\n    doi = \"10.18653/v1/2024.findings-naacl.171\",\n    pages = \"2674--2690\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.171.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.171/",
        "pdf_size": 1279264,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3280554887837452605&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of California, San Diego; University of Washington; University of California, San Diego; University of Washington+Allen Institute for Brain Science; University of Washington; Espresso Systems; Rutgers University; University of Washington",
        "aff_domain": "ucsd.edu;cs.washington.edu; ; ; ; ; ; ",
        "email": "ucsd.edu;cs.washington.edu; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;0;1+2;1;3;4;1",
        "aff_unique_norm": "University of California, San Diego;University of Washington;Allen Institute for Brain Science;Espresso Systems;Rutgers University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.washington.edu;https://www.alleninstitute.org;;https://www.rutgers.edu",
        "aff_unique_abbr": "UCSD;UW;Allen Institute;;Rutgers",
        "aff_campus_unique_index": "0;0;",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0+0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2024.findings-naacl.169",
        "title": "Laying Anchors: Semantically Priming Numerals in Language Modeling",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings.",
        "author": "Mandar Sharma; Rutuja Taware; Pravesh Koirala; Nikhil Muralidhar; Naren Ramakrishnan",
        "authorids": "/m/mandar-sharma/; /r/rutuja-taware/; /p/pravesh-koirala/; /n/nikhil-muralidhar/; /n/naren-ramakrishnan/",
        "bibtex": "@inproceedings{sharma-etal-2024-laying,\n    title = \"Laying Anchors: Semantically Priming Numerals in Language Modeling\",\n    author = \"Sharma, Mandar  and\n      Taware, Rutuja  and\n      Koirala, Pravesh  and\n      Muralidhar, Nikhil  and\n      Ramakrishnan, Naren\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.169/\",\n    doi = \"10.18653/v1/2024.findings-naacl.169\",\n    pages = \"2653--2660\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.169.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.169/",
        "pdf_size": 2639712,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16132813747818440928&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Virginia Tech; Virginia Tech; Vanderbilt University; Stevens Institute of Technology; Virginia Tech",
        "aff_domain": "vt.edu;vt.edu;vanderbilt.edu;stevens.edu;cs.vt.edu",
        "email": "vt.edu;vt.edu;vanderbilt.edu;stevens.edu;cs.vt.edu",
        "github": "https://github.com/Mandar-Sharma/Laying-Anchors",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Virginia Tech;Vanderbilt University;Stevens Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.vt.edu;https://www.vanderbilt.edu;https://www.stevens.edu",
        "aff_unique_abbr": "VT;Vanderbilt;SIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.207",
        "title": "LayoutPointer: A Spatial-Context Adaptive Pointer Network for Visual Information Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Visual Information Extraction (VIE), as a crucial task of Document Intelligence, involves two primary sub-tasks: Semantic Entity Recognition (SER) and Relation Extraction (RE). However, VIE faces two significant challenges. Firstly, most existing models inadequately utilize spatial information of entities, often failing to predict connections or incorrectly linking spatially distant entities. Secondly, the improper input order of tokens challenges in extracting complete entity pairs from documents with multi-line entities when text is extracted via PDF parser or OCR. To address these challenges, we propose LayoutPointer, a Spatial-Context Adaptive Pointer Network. LayoutPointer explicitly enhances spatial-context relationships by incorporating 2D relative position information and adaptive spatial constraints within self-attention. Furthermore, we recast the RE task as a specialized cycle detection problem, employing a unique tail-to-head pointer to restore the semantic order among multi-line entities. To better evaluate the effectiveness of our proposed method, we reconstruct a multi-line dataset named MLFUD, which more accurately reflects real-world scenarios. Fine-tuning experimental results on FUNSD, XFUND, and MLFUD datasets demonstrate that LayoutPointer significantly outperforms existing state-of-the-art methods in F1 scores for RE tasks (e.g., 5.71% improvement on XFUND using LayoutPointerBASE-X over LayoutLMv3).",
        "author": "Huang Siyuan; Yongping Xiong; Wu Guibin",
        "authorids": "/h/huang-siyuan/; /y/yongping-xiong/; /w/wu-guibin/",
        "bibtex": "@inproceedings{siyuan-etal-2024-layoutpointer,\n    title = \"{L}ayout{P}ointer: A Spatial-Context Adaptive Pointer Network for Visual Information Extraction\",\n    author = \"Siyuan, Huang  and\n      Xiong, Yongping  and\n      Guibin, Wu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.207/\",\n    doi = \"10.18653/v1/2024.naacl-long.207\",\n    pages = \"3737--3748\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.207.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.207/",
        "pdf_size": 2110978,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11031916603596976211&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Chizhou University",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;czu.edu.cn",
        "email": "bupt.edu.cn;bupt.edu.cn;czu.edu.cn",
        "github": "https://github.com/ThinkSYR/LayoutPointer",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Chizhou University",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.bupt.edu.cn/;http://www.chzhou.edu.cn",
        "aff_unique_abbr": "BUPT;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.416",
        "title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty ofsuch reasoning. We use Lean, a theorem proving framework, to address these challenges. By formalizing logical reasoning problems intotheorems within Lean, we can solve them by proving or disproving the corresponding theorems. This method reduces the risk of logical inconsistencies with the help of Lean\u2019s symbolic solver. It also enhances our ability to treat complex reasoning tasks using Lean\u2019s extensive library of theorem proofs. Our method achieves state-of-the-art performance on the FOLIO dataset and achieves performance near this level on ProofWriter. Notably, these results were accomplished by fine-tuning on fewer than 100 in-domain samples for each dataset",
        "author": "Dongwei Jiang; Marcio Fonseca; Shay Cohen",
        "authorids": "/d/dongwei-jiang/; /m/marcio-fonseca/; /s/shay-b-cohen/",
        "bibtex": "@inproceedings{jiang-etal-2024-leanreasoner,\n    title = \"{L}ean{R}easoner: Boosting Complex Logical Reasoning with Lean\",\n    author = \"Jiang, Dongwei  and\n      Fonseca, Marcio  and\n      Cohen, Shay\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.416/\",\n    doi = \"10.18653/v1/2024.naacl-long.416\",\n    pages = \"7497--7510\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.416.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.416/",
        "pdf_size": 397967,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15286185148617608200&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Johns Hopkins University; University of Edinburgh; University of Edinburgh",
        "aff_domain": "jhu.edu;ed.ac.uk;inf.ed.ac.uk",
        "email": "jhu.edu;ed.ac.uk;inf.ed.ac.uk",
        "github": "https://github.com/Some-random/theorem-proving-reasoning",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Johns Hopkins University;University of Edinburgh",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.jhu.edu;https://www.ed.ac.uk",
        "aff_unique_abbr": "JHU;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2024.findings-naacl.84",
        "title": "Learning Cross-Architecture Instruction Embeddings for Binary Code Analysis in Low-Resource Architectures",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Binary code analysis is indispensable for a variety of software security tasks. Applying deep learning to binary code analysis has drawn great attention because of its notable performance. Today, source code is frequently compiled for various Instruction Set Architectures (ISAs). It is thus critical to expand binary analysis capabilities to multiple ISAs. Given a binary analysis task, the scale of available data on different ISAs varies. As a result, the rich datasets (e.g., malware) for certain ISAs, such as x86, lead to a disproportionate focus on these ISAs and a negligence of other ISAs, such as PowerPC, which suffer from the \u201cdata scarcity\u201d problem. To address the problem, we propose to learn cross-architecture instruction embeddings (CAIE), where semantically-similar instructions, regardless of their ISAs, have close embeddings in a shared space. Consequently, we can transfer a model trained on a data-rich ISA to another ISA with less available data. We consider four ISAs (x86, ARM, MIPS, and PowerPC) and conduct both intrinsic and extrinsic evaluations (including malware detection and function similarity comparison). The results demonstrate the effectiveness of our approach to generate high-quality CAIE with good transferability.",
        "author": "Junzhe Wang; Qiang Zeng; Lannan Luo",
        "authorids": "/j/junzhe-wang/; /q/qiang-zeng/; /l/lannan-luo/",
        "bibtex": "@inproceedings{wang-etal-2024-learning-cross,\n    title = \"Learning Cross-Architecture Instruction Embeddings for Binary Code Analysis in Low-Resource Architectures\",\n    author = \"Wang, Junzhe  and\n      Zeng, Qiang  and\n      Luo, Lannan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.84/\",\n    doi = \"10.18653/v1/2024.findings-naacl.84\",\n    pages = \"1320--1332\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.84.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.84/",
        "pdf_size": 442911,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7995690234011042224&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "George Mason University; George Mason University; George Mason University",
        "aff_domain": "gmu.edu;gmu.edu;gmu.edu",
        "email": "gmu.edu;gmu.edu;gmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "George Mason University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gmu.edu",
        "aff_unique_abbr": "GMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.202",
        "title": "Learning Mutually Informed Representations for Characters and Subwords",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Most pretrained language models rely on subword tokenization, which processes text as a sequence of subword tokens. However, different granularities of text, such as characters, subwords, and words, can contain different kinds of information. Previous studies have shown that incorporating multiple input granularities improves model generalization, yet very few of them outputs useful representations for each granularity. In this paper, we introduce the entanglement model, aiming to combine character and subword language models. Inspired by vision-language models, our model treats characters and subwords as separate modalities, and it generates mutually informed representations for both granularities as output. We evaluate our model on text classification, named entity recognition, POS-tagging, and character-level sequence labeling (intraword code-switching). Notably, the entanglement model outperforms its backbone language models, particularly in the presence of noisy texts and low-resource languages. Furthermore, the entanglement model even outperforms larger pre-trained models on all English sequence labeling tasks and classification tasks. We make our code publically available.",
        "author": "Yilin Wang; Xinyi Hu; Matthew Gormley",
        "authorids": "/y/yilin-wang/; /x/xinyi-hu/; /m/matthew-r-gormley/",
        "bibtex": "@inproceedings{wang-etal-2024-learning-mutually,\n    title = \"Learning Mutually Informed Representations for Characters and Subwords\",\n    author = \"Wang, Yilin  and\n      Hu, Xinyi  and\n      Gormley, Matthew\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.202/\",\n    doi = \"10.18653/v1/2024.findings-naacl.202\",\n    pages = \"3201--3213\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.202.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.202/",
        "pdf_size": 667912,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:xsnF4OkafvAJ:scholar.google.com/&scioq=Learning+Mutually+Informed+Representations+for+Characters+and+Subwords&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "Harvard University; Carnegie Mellon University; Carnegie Mellon University",
        "aff_domain": "g.harvard.edu;alumni.cmu.edu;cs.cmu.edu",
        "email": "g.harvard.edu;alumni.cmu.edu;cs.cmu.edu",
        "github": "https://github.com/TonyW42/noisy-IE",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Harvard University;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.harvard.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Harvard;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.429",
        "title": "Learning to Compress Prompt in Natural Language Formats",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framework compressing original prompts into NL formatted Capsule Prompt while maintaining prompt utility and transferability. Specifically, to tackle the first challenge, the Nano-Capsulator is optimized by a reward function that interacts with the proposed semantics preserving loss. To address the second question, the Nano-Capsulator is optimized by a reward function featuring length constraints. Experimental results demonstrate that the Capsule Prompt can reduce 81.4% of the original length, decrease inference latency up to 4.5x, and save 80.1% of budget overheads while providing transferability across diverse LLMs and different datasets.",
        "author": "Yu-Neng Chuang; Tianwei Xing; Chia-Yuan Chang; Zirui Liu; Xun Chen; Xia Hu",
        "authorids": "/y/yu-neng-chuang/; /t/tianwei-xing/; /c/chia-yuan-chang/; /z/zirui-liu/; /x/xun-chen/; /x/xia-hu/",
        "bibtex": "@inproceedings{chuang-etal-2024-learning,\n    title = \"Learning to Compress Prompt in Natural Language Formats\",\n    author = \"Chuang, Yu-Neng  and\n      Xing, Tianwei  and\n      Chang, Chia-Yuan  and\n      Liu, Zirui  and\n      Chen, Xun  and\n      Hu, Xia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.429/\",\n    doi = \"10.18653/v1/2024.naacl-long.429\",\n    pages = \"7756--7767\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.429.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.429/",
        "pdf_size": 999035,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1677761568040861989&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Rice University; Samsung Research America; Texas A&M University; Rice University; Samsung Research America; Rice University",
        "aff_domain": "rice.edu;samsung.com;tamu.edu;rice.edu;samsung.com;rice.edu",
        "email": "rice.edu;samsung.com;tamu.edu;rice.edu;samsung.com;rice.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;0;1;0",
        "aff_unique_norm": "Rice University;Samsung;Texas A&M University",
        "aff_unique_dep": ";Samsung Research America;",
        "aff_unique_url": "https://www.rice.edu;https://www.samsung.com/us/careers/research/;https://www.tamu.edu",
        "aff_unique_abbr": "Rice;SRA;TAMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.472",
        "title": "LegalDiscourse: Interpreting When Laws Apply and To Whom",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While legal AI has made strides in recent years, it still struggles with basic legal concepts: _when_ does a law apply? _Who_ does it applies to? _What_ does it do? We take a _discourse_ approach to addressing these problems and introduce a novel taxonomy for span-and-relation parsing of legal texts. We create a dataset, _LegalDiscourse_ of 602 state-level law paragraphs consisting of 3,715 discourse spans and 1,671 relations. Our trained annotators have an agreement-rate \ud835\udf05>.8, yet few-shot GPT3.5 performs poorly at span identification and relation classification. Although fine-tuning improves performance, GPT3.5 still lags far below human level. We demonstrate the usefulness of our schema by creating a web application with journalists. We collect over 100,000 laws for 52 U.S. states and territories using 20 scrapers we built, and apply our trained models to 6,000 laws using U.S. Census population numbers. We describe two journalistic outputs stemming from this application: (1) an investigation into the increase in liquor licenses following population growth and (2) a decrease in applicable laws under different under-count projections.",
        "author": "Alexander Spangher; Zihan Xue; Te-Lin Wu; Mark Hansen; Jonathan May",
        "authorids": "/a/alexander-spangher/; /z/zihan-xue/; /t/te-lin-wu/; /m/mark-hansen/; /j/jonathan-may/",
        "bibtex": "@inproceedings{spangher-etal-2024-legaldiscourse,\n    title = \"{L}egal{D}iscourse: Interpreting When Laws Apply and To Whom\",\n    author = \"Spangher, Alexander  and\n      Xue, Zihan  and\n      Wu, Te-Lin  and\n      Hansen, Mark  and\n      May, Jonathan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.472/\",\n    doi = \"10.18653/v1/2024.naacl-long.472\",\n    pages = \"8536--8559\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.472.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.472/",
        "pdf_size": 1651224,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17399470648652952174&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Information Sciences Institute, University of Southern California; University of California, Los Angeles; University of California, Los Angeles; Columbia University; University of Southern California",
        "aff_domain": "usc.edu; ; ; ; ",
        "email": "usc.edu; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;2;0",
        "aff_unique_norm": "University of Southern California;University of California, Los Angeles;Columbia University",
        "aff_unique_dep": "Information Sciences Institute;;",
        "aff_unique_url": "https://www.usc.edu;https://www.ucla.edu;https://www.columbia.edu",
        "aff_unique_abbr": "USC;UCLA;Columbia",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.432",
        "title": "Leitner-Guided Memory Replay for Cross-lingual Continual Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cross-lingual continual learning aims to continuously fine-tune a downstream model on emerging data from new languages. One major challenge in cross-lingual continual learning is catastrophic forgetting: a stability-plasticity dilemma, where performance on previously seen languages decreases as the model learns to transfer to new languages. Experience replay, which revisits data from a fixed-size memory of old languages while training on new ones, is among the most successful approaches for solving this dilemma. Faced with the challenge of dynamically storing the memory with high-quality examples while complying with its fixed size limitations, we consider Leitner queuing, a human-inspired spaced-repetition technique, to determine what should be replayed at each phase of learning. Via a controlled set of quantitative and qualitative analyses across different memory strategies, we show that, just like humans, carefully picking informative examples to be prioritized in cross-lingual memory replay helps tame the stability-plasticity dilemma. Compared to vanilla and strong memory replay baselines, our Leitner-guided approach significantly and consistently decreases forgetting while maintaining accuracy across natural language understanding tasks, language orders, and languages.",
        "author": "Meryem M\u2019hamdi; Jonathan May",
        "authorids": "/m/meryem-mhamdi/; /j/jonathan-may/",
        "bibtex": "@inproceedings{mhamdi-may-2024-leitner,\n    title = \"Leitner-Guided Memory Replay for Cross-lingual Continual Learning\",\n    author = \"M{'}hamdi, Meryem  and\n      May, Jonathan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.432/\",\n    doi = \"10.18653/v1/2024.naacl-long.432\",\n    pages = \"7808--7821\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.432.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.432/",
        "pdf_size": 1249958,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9165838848470862952&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Microsoft; Information Sciences Institute, University of Southern California",
        "aff_domain": "microsoft.com;isi.edu",
        "email": "microsoft.com;isi.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Microsoft;University of Southern California",
        "aff_unique_dep": "Microsoft Corporation;Information Sciences Institute",
        "aff_unique_url": "https://www.microsoft.com;https://www.usc.edu",
        "aff_unique_abbr": "Microsoft;USC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.27",
        "title": "Less is More for Improving Automatic Evaluation of Factual Consistency",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Assessing the factual consistency of automatically generated texts in relation to source context is crucial for developing reliable natural language generation applications. Recent literature proposes AlignScore which uses a unified alignment model to evaluate factual consistency and substantially outperforms previous methods across many benchmark tasks. In this paper, we take a closer look of datasets used in AlignScore and uncover an unexpected finding: utilizing a smaller number of data points can actually improve performance. We process the original AlignScore training dataset to remove noise, augment with robustness-enhanced samples, and utilize a subset comprising 10% of the data to train an improved factual consistency evaluation model, we call LIM-RA (Less Is More for Robust AlignScore). LIM-RA demonstrates superior performance, consistently outperforming AlignScore and other strong baselines like ChatGPT across four benchmarks (two utilizing traditional natural language generation datasets and two focused on large language model outputs). Our experiments show that LIM-RA achieves the highest score on 24 of the 33 test datasets, while staying competitive on the rest, establishing the new state-of-the-art benchmarks.",
        "author": "Tong Wang; Ninad Kulkarni; Yanjun Qi",
        "authorids": "/t/tong-wang/; /n/ninad-kulkarni/; /y/yanjun-qi/",
        "bibtex": "@inproceedings{wang-etal-2024-less,\n    title = \"Less is More for Improving Automatic Evaluation of Factual Consistency\",\n    author = \"Wang, Tong  and\n      Kulkarni, Ninad  and\n      Qi, Yanjun\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.27/\",\n    doi = \"10.18653/v1/2024.naacl-industry.27\",\n    pages = \"324--334\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.27.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.27/",
        "pdf_size": 377405,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7863261926268680615&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.naacl-long.279",
        "title": "Leveraging Code to Improve In-Context Learning for Semantic Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In-context learning (ICL) is an appealing approach for semantic parsing due to its few-shot nature and improved generalization. However, learning to parse to rare domain-specific languages (DSLs) from just a few demonstrations is challenging, limiting the performance of even the most capable LLMs.In this work, we show how pre-existing coding abilities of LLMs can be leveraged for semantic parsing by (1) using general-purpose programming languages such as Python instead of DSLs and (2) augmenting prompts with a structured domain description that includes, e.g., the available classes and functions. We show that both these changes significantly improve accuracy across three popular datasets; combined, they lead to dramatic improvements (e.g., 7.9% to 66.5% on SMCalFlow compositional split) and can substantially improve compositional generalization, nearly closing the performance gap between easier i.i.d. and harder compositional splits. Finally, comparisons across multiple PLs and DSL variations suggest that the similarity of a target language to general-purpose code is more important than prevalence in pretraining corpora. Our findings provide an improved methodology for building semantic parsers in the modern context of ICL with LLMs.",
        "author": "Ben Bogin; Shivanshu Gupta; Peter Clark; Ashish Sabharwal",
        "authorids": "/b/ben-bogin/; /s/shivanshu-gupta/; /p/peter-clark/; /a/ashish-sabharwal/",
        "bibtex": "@inproceedings{bogin-etal-2024-leveraging,\n    title = \"Leveraging Code to Improve In-Context Learning for Semantic Parsing\",\n    author = \"Bogin, Ben  and\n      Gupta, Shivanshu  and\n      Clark, Peter  and\n      Sabharwal, Ashish\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.279/\",\n    doi = \"10.18653/v1/2024.naacl-long.279\",\n    pages = \"4971--5012\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.279.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.279/",
        "pdf_size": 525022,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12666709456124427672&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Allen Institute for AI; University of California Irvine; Allen Institute for AI; Allen Institute for AI",
        "aff_domain": "allenai.org;uci.edu;allenai.org;allenai.org",
        "email": "allenai.org;uci.edu;allenai.org;allenai.org",
        "github": "https://github.com/allenai/code-semparse",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Allen Institute for AI;University of California, Irvine",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://allenai.org;https://www.uci.edu",
        "aff_unique_abbr": "AI2;UCI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Irvine",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.28",
        "title": "Leveraging Contextual Information for Effective Entity Salience Detection",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In text documents such as news articles, the content and key events usually revolve around a subset of all the entities mentioned in a document. These entities, often deemed as salient entities, provide useful cues of the aboutness of a document to a reader. Identifying the salience of entities was found helpful in several downstream applications such as search, ranking, and entity-centric summarization, among others. Prior work on salient entity detection mainly focused on machine learning models that require heavy feature engineering. We show that fine-tuning medium-sized language models with a cross-encoder style architecture yields substantial performance gains over feature engineering approaches. To this end, we conduct a comprehensive benchmarking of four publicly available datasets using models representative of the medium-sized pre-trained language model family. Additionally, we show that zero-shot prompting of instruction-tuned language models yields inferior results, indicating the task\u2019s uniqueness and complexity.",
        "author": "Rajarshi Bhowmik; Marco Ponza; Atharva Tendle; Anant Gupta; Rebecca Jiang; Xingyu Lu; Qian Zhao; Daniel Preotiuc-Pietro",
        "authorids": "/r/rajarshi-bhowmik/; /m/marco-ponza/; /a/atharva-tendle/; /a/anant-gupta/; /r/rebecca-jiang/; /x/xingyu-lu/; /q/qian-zhao/; /d/daniel-preotiuc-pietro/",
        "bibtex": "@inproceedings{bhowmik-etal-2024-leveraging,\n    title = \"Leveraging Contextual Information for Effective Entity Salience Detection\",\n    author = \"Bhowmik, Rajarshi  and\n      Ponza, Marco  and\n      Tendle, Atharva  and\n      Gupta, Anant  and\n      Jiang, Rebecca  and\n      Lu, Xingyu  and\n      Zhao, Qian  and\n      Preotiuc-Pietro, Daniel\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.28/\",\n    doi = \"10.18653/v1/2024.findings-naacl.28\",\n    pages = \"395--408\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.28.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.28/",
        "pdf_size": 1218424,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11612923782136616614&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Bloomberg; Bloomberg; Bloomberg; Bloomberg; Bloomberg\u2020; Bloomberg; Bloomberg; Bloomberg",
        "aff_domain": "bloomberg.net; ; ; ; ; ; ;bloomberg.net",
        "email": "bloomberg.net; ; ; ; ; ; ;bloomberg.net",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Bloomberg",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bloomberg.com",
        "aff_unique_abbr": "Bloomberg",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.22",
        "title": "Leveraging Customer Feedback for Multi-modal Insight Extraction",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Businesses can benefit from customer feedback in different modalities, such as text and images, to enhance their products and services. However, it is difficult to extract actionable and relevant pairs of text segments and images from customer feedback in a single pass. In this paper, we propose a novel multi-modal method that fuses image and text information in a latent space and decodes it to extract the relevant feedback segments using an image-text grounded text decoder. We also introduce a weakly-supervised data generation technique that produces training data for this task. We evaluate our model on unseen data and demonstrate that it can effectively mine actionable insights from multi-modal customer feedback, outperforming the existing baselines by 14 points in F1 score.",
        "author": "Sandeep Mukku; Abinesh Kanagarajan; Pushpendu Ghosh; Chetan Aggarwal",
        "authorids": "/s/sandeep-mukku/; /a/abinesh-kanagarajan/; /p/pushpendu-ghosh/; /c/chetan-aggarwal/",
        "bibtex": "@inproceedings{mukku-etal-2024-leveraging,\n    title = \"Leveraging Customer Feedback for Multi-modal Insight Extraction\",\n    author = \"Mukku, Sandeep  and\n      Kanagarajan, Abinesh  and\n      Ghosh, Pushpendu  and\n      Aggarwal, Chetan\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.22/\",\n    doi = \"10.18653/v1/2024.naacl-industry.22\",\n    pages = \"266--278\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.22.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.22/",
        "pdf_size": 4798598,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3W5pMvybtfwJ:scholar.google.com/&scioq=Leveraging+Customer+Feedback+for+Multi-modal+Insight+Extraction&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "Amazon; Amazon; Amazon; Amazon",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon.com, Inc.",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.97",
        "title": "Leveraging Generative Large Language Models with Visual Instruction and Demonstration Retrieval for Multimodal Sarcasm Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multimodal sarcasm detection aims to identify sarcasm in the given image-text pairs and has wide applications in the multimodal domains. Previous works primarily design complex network structures to fuse the image-text modality features for classification. However, such complicated structures may risk overfitting on in-domain data, reducing the performance in out-of-distribution (OOD) scenarios. Additionally, existing methods typically do not fully utilize cross-modal features, limiting their performance on in-domain datasets. Therefore, to build a more reliable multimodal sarcasm detection model, we propose a generative multimodal sarcasm model consisting of a designed instruction template and a demonstration retrieval module based on the large language model. Moreover, to assess the generalization of current methods, we introduce an OOD test set, RedEval. Experimental results demonstrate that our method is effective and achieves state-of-the-art (SOTA) performance on the in-domain MMSD2.0 and OOD RedEval datasets.",
        "author": "Binghao Tang; Boda Lin; Haolong Yan; Si Li",
        "authorids": "/b/binghao-tang/; /b/boda-lin/; /h/haolong-yan/; /s/si-li/",
        "bibtex": "@inproceedings{tang-etal-2024-leveraging,\n    title = \"Leveraging Generative Large Language Models with Visual Instruction and Demonstration Retrieval for Multimodal Sarcasm Detection\",\n    author = \"Tang, Binghao  and\n      Lin, Boda  and\n      Yan, Haolong  and\n      Li, Si\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.97/\",\n    doi = \"10.18653/v1/2024.naacl-long.97\",\n    pages = \"1732--1742\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.97.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.97/",
        "pdf_size": 2068697,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18047098556286381961&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-industry.38",
        "title": "Leveraging Interesting Facts to Enhance User Engagement with Conversational Interfaces",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Conversational Task Assistants (CTAs) guide users in performing a multitude of activities, such as making recipes. However, ensuring that interactions remain engaging, interesting, and enjoyable for CTA users is not trivial, especially for time-consuming or challenging tasks. Grounded in psychological theories of human interest, we propose to engage users with contextual and interesting statements or facts during interactions with a multi-modal CTA, to reduce fatigue and task abandonment before a task is complete. To operationalize this idea, we train a high-performing classifier (82% F1-score) to automatically identify relevant and interesting facts for users. We use it to create an annotated dataset of task-specific interesting facts for the domain of cooking. Finally, we design and validate a dialogue policy to incorporate the identified relevant and interesting facts into a conversation, to improve user engagement and task completion. Live testing on a leading multi-modal voice assistant shows that 66% of the presented facts were received positively, leading to a 40% gain in the user satisfaction rating, and a 37% increase in conversation length. These findings emphasize that strategically incorporating interesting facts into the CTA experience can promote real-world user participation for guided task interactions.",
        "author": "Nikhita Vedula; Giuseppe Castellucci; Eugene Agichtein; Oleg Rokhlenko; Shervin Malmasi",
        "authorids": "/n/nikhita-vedula/; /g/giuseppe-castellucci/; /e/eugene-agichtein/; /o/oleg-rokhlenko/; /s/shervin-malmasi/",
        "bibtex": "@inproceedings{vedula-etal-2024-leveraging,\n    title = \"Leveraging Interesting Facts to Enhance User Engagement with Conversational Interfaces\",\n    author = \"Vedula, Nikhita  and\n      Castellucci, Giuseppe  and\n      Agichtein, Eugene  and\n      Rokhlenko, Oleg  and\n      Malmasi, Shervin\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.38/\",\n    doi = \"10.18653/v1/2024.naacl-industry.38\",\n    pages = \"437--446\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.38.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.38/",
        "pdf_size": 288276,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:DYWF6XSCvTIJ:scholar.google.com/&scioq=Leveraging+Interesting+Facts+to+Enhance+User+Engagement+with+Conversational+Interfaces&hl=en&as_sdt=0,33",
        "gs_version_total": 6,
        "aff": "Amazon.com, Inc. Seattle, WA, USA; Amazon.com, Inc. Seattle, WA, USA; Amazon.com, Inc. Seattle, WA, USA; Amazon.com, Inc. Seattle, WA, USA; Amazon.com, Inc. Seattle, WA, USA",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "https://www.github.com/vnik18/cta-interesting-facts",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon.com, Inc.",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.30",
        "title": "Leveraging LLMs for Dialogue Quality Measurement",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In task-oriented conversational AI evaluation, unsupervised methods poorly correlate with human judgments, and supervised approaches lack generalization. Recent advances in large language models (LLMs) show robust zero- and few-shot capabilities across NLP tasks. Our paper explores using LLMs for automated dialogue quality evaluation, experimenting with various configurations on public and proprietary datasets. Manipulating factors such as model size, in-context examples, and selection techniques, we examine \u201cchain-of-thought\u201d (CoT) reasoning and label extraction procedures. Our results show that (1) larger models yield more accurate dialogue labels; (2) algorithmic selection of in-context examples outperforms random selection,; (3) CoT reasoning where an LLM is asked to provide justifications before outputting final labels improves performance; and (4) fine-tuned LLMs outperform out-of-the-box ones. In addition, we find that suitably tuned LLMs exhibit high accuracy in dialogue evaluation compared to human judgments.",
        "author": "Jinghan Jia; Abi Komma; Timothy Leffel; Xujun Peng; Ajay Nagesh; Tamer Soliman; Aram Galstyan; Anoop Kumar",
        "authorids": "/j/jinghan-jia/; /a/abi-komma/; /t/timothy-leffel/; /x/xujun-peng/; /a/ajay-nagesh/; /t/tamer-soliman/; /a/aram-galstyan/; /a/anoop-kumar/",
        "bibtex": "@inproceedings{jia-etal-2024-leveraging,\n    title = \"Leveraging {LLM}s for Dialogue Quality Measurement\",\n    author = \"Jia, Jinghan  and\n      Komma, Abi  and\n      Leffel, Timothy  and\n      Peng, Xujun  and\n      Nagesh, Ajay  and\n      Soliman, Tamer  and\n      Galstyan, Aram  and\n      Kumar, Anoop\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.30/\",\n    doi = \"10.18653/v1/2024.naacl-industry.30\",\n    pages = \"359--367\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.30.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.30/",
        "pdf_size": 362470,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9215802593018845884&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science & Engineering, Michigan State University; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations",
        "aff_domain": "msu.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;gmail.com",
        "email": "msu.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;1;1;1",
        "aff_unique_norm": "Michigan State University;Amazon",
        "aff_unique_dep": "Computer Science & Engineering;AGI Foundations",
        "aff_unique_url": "https://www.msu.edu;https://www.amazon.com",
        "aff_unique_abbr": "MSU;Amazon",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "East Lansing;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.426",
        "title": "Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "There has been limited success for dense retrieval models in multilingual retrieval, due to uneven and scarce training data available across multiple languages. Synthetic training data generation is promising (e.g., InPars or Promptagator), but has been investigated only for English. Therefore, to study model capabilities across both cross-lingual and monolingual retrieval tasks, we develop **SWIM-IR**, a synthetic retrieval training dataset containing 33 (high to very-low resource) languages for fine-tuning multilingual dense retrievers without requiring any human supervision. To construct SWIM-IR, we propose SAP (summarize-then-ask prompting), where the large language model (LLM) generates a textual summary prior to the query generation step. SAP assists the LLM in generating informative queries in the target language. Using SWIM-IR, we explore synthetic fine-tuning of multilingual dense retrieval models and evaluate them robustly on three retrieval benchmarks: XOR-Retrieve (cross-lingual), MIRACL (monolingual) and XTREME-UP (cross-lingual). Our models, called SWIM-X, are competitive with human-supervised dense retrieval models, e.g., mContriever-X, finding that SWIM-IR can cheaply substitute for expensive human-labeled retrieval training data. SWIM-IR dataset and SWIM-X models are available at: https://github.com/google-research-datasets/SWIM-IR.",
        "author": "Nandan Thakur; Jianmo Ni; Gustavo Hernandez Abrego; John Wieting; Jimmy Lin; Daniel Cer",
        "authorids": "/n/nandan-thakur/; /j/jianmo-ni/; /g/gustavo-hernandez-abrego/; /j/john-wieting/; /j/jimmy-lin/; /d/daniel-cer/",
        "bibtex": "@inproceedings{thakur-etal-2024-leveraging,\n    title = \"Leveraging {LLM}s for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval\",\n    author = \"Thakur, Nandan  and\n      Ni, Jianmo  and\n      Hernandez Abrego, Gustavo  and\n      Wieting, John  and\n      Lin, Jimmy  and\n      Cer, Daniel\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.426/\",\n    doi = \"10.18653/v1/2024.naacl-long.426\",\n    pages = \"7699--7724\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.426.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.426/",
        "pdf_size": 2387396,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2279384590372560130&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Google Research+University of Waterloo; Google DeepMind; Google Research; Google DeepMind; University of Waterloo; Google Research+Google DeepMind",
        "aff_domain": "uwaterloo.ca;google.com; ; ; ;google.com",
        "email": "uwaterloo.ca;google.com; ; ; ;google.com",
        "github": "https://github.com/google-research-datasets/SWIM-IR",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0;0;0;1;0+0",
        "aff_unique_norm": "Google;University of Waterloo",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://uwaterloo.ca",
        "aff_unique_abbr": "Google Research;UW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0+1;2;0;2;1;0+2",
        "aff_country_unique": "United States;Canada;United Kingdom"
    },
    {
        "id": "2024.naacl-industry.14",
        "title": "Leveraging Natural Language Processing and Large Language Models for Assisting Due Diligence in the Legal Domain",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Due diligence is a crucial legal process that mitigates potential risks of mergers and acquisitions (M&A). However, despite its prominent importance, there has been a lack of research regarding leveraging NLP techniques for due diligence. In this study, our aim is to explore the most efficient deep-learning model architecture for due diligence in terms of performance and latency, and evaluate the potential of large language models (LLMs) as an efficient due diligence assistant. To our knowledge, this is the first study that employs pre-trained language models (PLMs) and LLMs for the due diligence problem. Our experimental results suggest that methodologies that have demonstrated promising performance in the general domain encounter challenges when applied in due diligence due to the inherent lengthy nature of legal documents. We also ascertain that LLMs can be a useful tool for helping lawyers who perform due diligence.",
        "author": "Myeongjun Jang; G\u00e1bor Stikkel",
        "authorids": "/m/myeongjun-jang/; /g/gabor-stikkel/",
        "bibtex": "@inproceedings{jang-stikkel-2024-leveraging,\n    title = \"Leveraging Natural Language Processing and Large Language Models for Assisting Due Diligence in the Legal Domain\",\n    author = \"Jang, Myeongjun  and\n      Stikkel, G{\\'a}bor\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.14/\",\n    doi = \"10.18653/v1/2024.naacl-industry.14\",\n    pages = \"155--164\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.14.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.14/",
        "pdf_size": 662479,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:-0GGHqhO7OkJ:scholar.google.com/&scioq=Leveraging+Natural+Language+Processing+and+Large+Language+Models+for+Assisting+Due+Diligence+in+the+Legal+Domain&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Department of Computer Science, University of Oxford, UK; Data Science Lab, Clifford Chance, UK",
        "aff_domain": "cs.ox.ac.uk;cliffordchance.com",
        "email": "cs.ox.ac.uk;cliffordchance.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Oxford;Clifford Chance",
        "aff_unique_dep": "Department of Computer Science;Data Science Lab",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.cliffordchance.com",
        "aff_unique_abbr": "Oxford;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-short.33",
        "title": "Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model\u2019s representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.",
        "author": "Shadi Iskander; Kira Radinsky; Yonatan Belinkov",
        "authorids": "/s/shadi-iskander/; /k/kira-radinsky/; /y/yonatan-belinkov/",
        "bibtex": "@inproceedings{iskander-etal-2024-leveraging,\n    title = \"Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information\",\n    author = \"Iskander, Shadi  and\n      Radinsky, Kira  and\n      Belinkov, Yonatan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.33/\",\n    doi = \"10.18653/v1/2024.naacl-short.33\",\n    pages = \"379--390\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.33.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.33/",
        "pdf_size": 1694236,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9897196674039422567&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Technion \u2013 Israel Institute of Technology; Technion \u2013 Israel Institute of Technology; Technion \u2013 Israel Institute of Technology",
        "aff_domain": "campus.technion.ac.il;cs.technion.ac.il;technion.ac.il",
        "email": "campus.technion.ac.il;cs.technion.ac.il;technion.ac.il",
        "github": "https://github.com/technion-cs-nlp/DAFair",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion \u2013 Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2024.findings-naacl.291",
        "title": "Leveraging Summarization for Unsupervised Dialogue Topic Segmentation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Traditional approaches to dialogue segmentation perform reasonably well on synthetic or written dialogues but suffer when dealing with spoken, noisy dialogs. In addition, such methods require careful tuning of hyperparameters. We propose to leverage a novel approach that is based on dialogue summaries. Experiments on different datasets showed that the new approach outperforms popular state-of-the-art algorithms in unsupervised topic segmentation and requires less setup.",
        "author": "Aleksei Artemiev; Daniil Parinov; Alexey Grishanov; Ivan Borisov; Alexey Vasilev; Daniil Muravetskii; Aleksey Rezvykh; Aleksei Goncharov; Andrey Savchenko",
        "authorids": "/a/aleksei-artemiev/; /d/daniil-parinov/; /a/alexey-grishanov/; /i/ivan-borisov/; /a/alexey-vasilev/; /d/daniil-muravetskii/; /a/aleksey-rezvykh/; /a/aleksei-goncharov/; /a/andrey-savchenko/",
        "bibtex": "@inproceedings{artemiev-etal-2024-leveraging,\n    title = \"Leveraging Summarization for Unsupervised Dialogue Topic Segmentation\",\n    author = \"Artemiev, Aleksei  and\n      Parinov, Daniil  and\n      Grishanov, Alexey  and\n      Borisov, Ivan  and\n      Vasilev, Alexey  and\n      Muravetskii, Daniil  and\n      Rezvykh, Aleksey  and\n      Goncharov, Aleksei  and\n      Savchenko, Andrey\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.291/\",\n    doi = \"10.18653/v1/2024.findings-naacl.291\",\n    pages = \"4697--4704\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.291.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.291/",
        "pdf_size": 605024,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1701234806191904461&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "MIL team, Moscow, Russia; MIL team, Moscow, Russia; Sber AI Lab, Moscow, Russia + Moscow Institute of Physics and Technology, Moscow, Russia; Sber, Moscow, Russia; Sber AI Lab, Moscow, Russia; MIL team, Moscow, Russia + Moscow Institute of Physics and Technology, Moscow, Russia; Sber, Moscow, Russia; MIL team, Moscow, Russia + Moscow Institute of Physics and Technology, Moscow, Russia; Sber AI Lab, Moscow, Russia",
        "aff_domain": "phystech.edu; ; ; ; ; ; ; ;",
        "email": "phystech.edu; ; ; ; ; ; ; ;",
        "github": "https://github.com/milteam/unsupervised-summary-based-segmentation4697",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;1+2;1;1;0+2;1;0+2;1",
        "aff_unique_norm": "MIL Team;Sberbank;Moscow Institute of Physics and Technology",
        "aff_unique_dep": ";Sber AI Lab;",
        "aff_unique_url": ";https://sberbank.ru;https://www.mipt.ru/en",
        "aff_unique_abbr": ";Sber;MIPT",
        "aff_campus_unique_index": "1+1;1;1;1;1;1;1",
        "aff_campus_unique": ";Moscow",
        "aff_country_unique_index": "0;0;0+0;0;0;0+0;0;0+0;0",
        "aff_country_unique": "Russian Federation"
    },
    {
        "id": "2024.naacl-long.387",
        "title": "Leveraging the Structure of Pre-trained Embeddings to Minimize Annotation Effort",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most current state-of-the-art approaches for text classification are based on fine-tuning the representations computed by large language models (LLMs). This strategy has led to significant improvements in classification performance and contributed to a reduction of the amount of labeled data required for training a model. However, for some challenging classification tasks, providing enough annotations to ensure a reliable classification continues to be the main bottleneck. This is especially true in settings of highly imbalanced class distributions. This paper proposes to tackle this bottleneck by exploiting the structural properties of pre-trained embeddings. We develop a label propagation method that uses pre-trained embeddings to spread information from the labeled samples to nearby samples in the induced space, ensuring the optimal use of annotations. Our approach is simple and relatively low-cost since it only requires computing some distances in the embedded space. We conduct experiments on different text classification datasets showing that the proposed method is efficient and significantly outperforms both self-training and random walk label propagation strategies.",
        "author": "Cesar Gonzalez-Gutierrez; Ariadna Quattoni",
        "authorids": "/c/cesar-gonzalez-gutierrez/; /a/ariadna-quattoni/",
        "bibtex": "@inproceedings{gonzalez-gutierrez-quattoni-2024-leveraging,\n    title = \"Leveraging the Structure of Pre-trained Embeddings to Minimize Annotation Effort\",\n    author = \"Gonzalez-Gutierrez, Cesar  and\n      Quattoni, Ariadna\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.387/\",\n    doi = \"10.18653/v1/2024.naacl-long.387\",\n    pages = \"7003--7017\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.387.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.387/",
        "pdf_size": 467127,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:9hsF1x1GGs4J:scholar.google.com/&scioq=Leveraging+the+Structure+of+Pre-trained+Embeddings+to+Minimize+Annotation+Effort&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Universitat Polit\u00e8cnica de Catalunya, Barcelona, Spain; Universitat Polit\u00e8cnica de Catalunya, Barcelona, Spain",
        "aff_domain": "upc.edu;cs.upc.edu",
        "email": "upc.edu;cs.upc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Universitat Polit\u00e8cnica de Catalunya",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upc.edu",
        "aff_unique_abbr": "UPC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Barcelona",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "2024.naacl-short.60",
        "title": "LifeTox: Unveiling Implicit Toxicity in Life Advice",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "As large language models become increasingly integrated into daily life, detecting implicit toxicity across diverse contexts is crucial. To this end, we introduce LifeTox, a dataset designed for identifying implicit toxicity within a broad range of advice-seeking scenarios. Unlike existing safety datasets, LifeTox comprises diverse contexts derived from personal experiences through open-ended questions. Our experiments demonstrate that RoBERTa fine-tuned on LifeTox matches or surpasses the zero-shot performance of large language models in toxicity classification tasks. These results underscore the efficacy of LifeTox in addressing the complex challenges inherent in implicit toxicity. We open-sourced the dataset and the LifeTox moderator family; 350M, 7B, and 13B.",
        "author": "Minbeom Kim; Jahyun Koo; Hwanhee Lee; Joonsuk Park; Hwaran Lee; Kyomin Jung",
        "authorids": "/m/minbeom-kim/; /j/jahyun-koo/; /h/hwanhee-lee/; /j/joonsuk-park/; /h/hwaran-lee/; /k/kyomin-jung/",
        "bibtex": "@inproceedings{kim-etal-2024-lifetox,\n    title = \"{L}ife{T}ox: Unveiling Implicit Toxicity in Life Advice\",\n    author = \"Kim, Minbeom  and\n      Koo, Jahyun  and\n      Lee, Hwanhee  and\n      Park, Joonsuk  and\n      Lee, Hwaran  and\n      Jung, Kyomin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.60/\",\n    doi = \"10.18653/v1/2024.naacl-short.60\",\n    pages = \"688--698\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.60.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.60/",
        "pdf_size": 1393562,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18408234446783589757&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "https://huggingface.co/datasets/mbkim/LifeTox",
        "author_num": 6
    },
    {
        "id": "2024.naacl-short.50",
        "title": "Lifelong Event Detection with Embedding Space Separation and Compaction",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "To mitigate forgetting, existing lifelong event detection methods typically maintain a memory module and replay the stored memory data during the learning of a new task. However, the simple combination of memory data and new-task samples can still result in substantial forgetting of previously acquired knowledge, which may occur due to the potential overlap between the feature distribution of new data and the previously learned embedding space. Moreover, the model suffers from overfitting on the few memory samples rather than effectively remembering learned patterns. To address the challenges of forgetting and overfitting, we propose a novel method based on embedding space separation and compaction. Our method alleviates forgetting of previously learned tasks by forcing the feature distribution of new data away from the previous embedding space. It also mitigates overfitting by a memory calibration mechanism that encourages memory data to be close to its prototype to enhance intra-class compactness. In addition, the learnable parameters of the new task are initialized by drawing upon acquired knowledge from the previously learned task to facilitate forward knowledge transfer. With extensive experiments, we demonstrate that our method can significantly outperform previous state-of-the-art approaches.",
        "author": "Chengwei Qin; Ruirui Chen; Ruochen Zhao; Wenhan Xia; Shafiq Joty",
        "authorids": "/c/chengwei-qin/; /r/ruirui-chen/; /r/ruochen-zhao/; /w/wenhan-xia/; /s/shafiq-joty/",
        "bibtex": "@inproceedings{qin-etal-2024-lifelong,\n    title = \"Lifelong Event Detection with Embedding Space Separation and Compaction\",\n    author = \"Qin, Chengwei  and\n      Chen, Ruirui  and\n      Zhao, Ruochen  and\n      Xia, Wenhan  and\n      Joty, Shafiq\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.50/\",\n    doi = \"10.18653/v1/2024.naacl-short.50\",\n    pages = \"594--602\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.50.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.50/",
        "pdf_size": 465368,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13106915960947129524&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Nanyang Technological University; Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR)+Nanyang Technological University; Nanyang Technological University; Princeton University; Nanyang Technological University+Salesforce Research",
        "aff_domain": "e.ntu.edu.sg;ihpc.a-star.edu.sg;e.ntu.edu.sg;princeton.edu;ntu.edu.sg",
        "email": "e.ntu.edu.sg;ihpc.a-star.edu.sg;e.ntu.edu.sg;princeton.edu;ntu.edu.sg",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+0;0;2;0+3",
        "aff_unique_norm": "Nanyang Technological University;Agency for Science, Technology and Research;Princeton University;Salesforce",
        "aff_unique_dep": ";Institute of High Performance Computing;;Salesforce Research",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.a-star.edu.sg;https://www.princeton.edu;https://research.salesforce.com",
        "aff_unique_abbr": "NTU;A*STAR;Princeton;Salesforce",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;1;0+1",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "2024.naacl-long.360",
        "title": "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop LinkPrompt, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of LinkPrompt, as well as the transferability of UATs generated by LinkPrompt to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo. The resource is available at https://github.com/SavannahXu79/LinkPrompt.",
        "author": "Yue Xu; Wenjie Wang",
        "authorids": "/y/yue-xu/; /w/wenjie-wang/",
        "bibtex": "@inproceedings{xu-wang-2024-linkprompt,\n    title = \"{L}ink{P}rompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models\",\n    author = \"Xu, Yue  and\n      Wang, Wenjie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.360/\",\n    doi = \"10.18653/v1/2024.naacl-long.360\",\n    pages = \"6473--6486\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.360.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.360/",
        "pdf_size": 734410,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=308613313781573570&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Information Science and Technology, ShanghaiTech University; School of Information Science and Technology, ShanghaiTech University",
        "aff_domain": "shanghaitech.edu.cn;shanghaitech.edu.cn",
        "email": "shanghaitech.edu.cn;shanghaitech.edu.cn",
        "github": "https://github.com/SavannahXu79/LinkPrompt",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ShanghaiTech University",
        "aff_unique_dep": "School of Information Science and Technology",
        "aff_unique_url": "https://www.shanghaitech.edu.cn",
        "aff_unique_abbr": "ShanghaiTech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Shanghai",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-short.40",
        "title": "Llama meets EU: Investigating the European political spectrum through the lens of LLMs",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Instruction-finetuned Large Language Models inherit clear political leanings that have been shown to influence downstream task performance. We expand this line of research beyond the two-party system in the US and audit Llama Chat in the context of EU politics in various settings to analyze the model\u2019s political knowledge and its ability to reason in context. We adapt, i.e., further fine-tune, Llama Chat on speeches of individual euro-parties from debates in the European Parliament to reevaluate its political leaning based on the EUandI questionnaire. Llama Chat shows considerable knowledge of national parties\u2019 positions and is capable of reasoning in context. The adapted, party-specific, models are substantially re-aligned towards respective positions which we see as a starting point for using chat-based LLMs as data-driven conversational engines to assist research in political science.",
        "author": "Ilias Chalkidis; Stephanie Brandl",
        "authorids": "/i/ilias-chalkidis/; /s/stephanie-brandl/",
        "bibtex": "@inproceedings{chalkidis-brandl-2024-llama,\n    title = \"Llama meets {EU}: Investigating the {E}uropean political spectrum through the lens of {LLM}s\",\n    author = \"Chalkidis, Ilias  and\n      Brandl, Stephanie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.40/\",\n    doi = \"10.18653/v1/2024.naacl-short.40\",\n    pages = \"481--498\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.40.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.40/",
        "pdf_size": 4165454,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1647678504384042208&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Copenhagen, Denmark; Department of Computer Science, University of Copenhagen, Denmark",
        "aff_domain": "di.ku.dk;di.ku.dk",
        "email": "di.ku.dk;di.ku.dk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Copenhagen",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ku.dk",
        "aff_unique_abbr": "UCPH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "2024.naacl-long.174",
        "title": "LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named LoRETTA_adp and LoRETTA_rep. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight reparameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to 100\u00d7 fewer parameters on the LLaMA-2-7B models. Furthermore, empirical results demonstrate that the proposed methods exhibit remarkable anti-overfitting capability, effectively improve training efficiency, and enjoy better multi-task learning performance. Plug-and-play loretta library built upon the Huggingface framework and PEFT library are provided.",
        "author": "Yifan Yang; Jiajun Zhou; Ngai Wong; Zheng Zhang",
        "authorids": "/y/yifan-yang/; /j/jiajun-zhou/; /n/ngai-wong/; /z/zheng-zhang/",
        "bibtex": "@inproceedings{yang-etal-2024-loretta,\n    title = \"{L}o{RETTA}: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models\",\n    author = \"Yang, Yifan  and\n      Zhou, Jiajun  and\n      Wong, Ngai  and\n      Zhang, Zheng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.174/\",\n    doi = \"10.18653/v1/2024.naacl-long.174\",\n    pages = \"3161--3176\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.174.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.174/",
        "pdf_size": 618684,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11149377831509471124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Santa Barbara; The University of Hong Kong+University of California, Santa Barbara; The University of Hong Kong; University of California, Santa Barbara",
        "aff_domain": "cs.ucsb.edu;eee.hku.hk;eee.hku.hk;ece.ucsb.edu",
        "email": "cs.ucsb.edu;eee.hku.hk;eee.hku.hk;ece.ucsb.edu",
        "github": "https://github.com/yifanycc/loretta",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+0;1;0",
        "aff_unique_norm": "University of California, Santa Barbara;University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsb.edu;https://www.hku.hk",
        "aff_unique_abbr": "UCSB;HKU",
        "aff_campus_unique_index": "0;1+0;1;0",
        "aff_campus_unique": "Santa Barbara;Hong Kong SAR",
        "aff_country_unique_index": "0;1+0;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.208",
        "title": "Long-form evaluation of model editing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Evaluations of model editing, a technique for changing the factual knowledge held by Large Language Models (LLMs), currently only use the \u2018next few token\u2019 completions after a prompt. As a result, the impact of these methods on longer natural language generation is largely unknown. We introduce long-form evaluation of model editing (LEME) a novel evaluation protocol that measures the efficacy and impact of model editing in long-form generative settings. Our protocol consists of a machine-rated survey and a classifier which correlates well with human ratings. Importantly, we find that our protocol has very little relationship with previous short-form metrics (despite being designed to extend efficacy, generalization, locality, and portability into a long-form setting), indicating that our method introduces a novel set of dimensions for understanding model editing methods. Using this protocol, we benchmark a number of model editing techniques and present several findings including that, while some methods (ROME and MEMIT) perform well in making consistent edits within a limited scope, they suffer much more from factual drift than other methods. Finally, we present a qualitative analysis that illustrates common failure modes in long-form generative settings including internal consistency, lexical cohesion, and locality issues.",
        "author": "Domenic Rosati; Robie Gonzales; Jinkun Chen; Xuemin Yu; Yahya Kayani; Frank Rudzicz; Hassan Sajjad",
        "authorids": "/d/domenic-rosati/; /r/robie-gonzales/; /j/jinkun-chen/; /x/xuemin-yu/; /y/yahya-kayani/; /f/frank-rudzicz/; /h/hassan-sajjad/",
        "bibtex": "@inproceedings{rosati-etal-2024-long,\n    title = \"Long-form evaluation of model editing\",\n    author = \"Rosati, Domenic  and\n      Gonzales, Robie  and\n      Chen, Jinkun  and\n      Yu, Xuemin  and\n      Kayani, Yahya  and\n      Rudzicz, Frank  and\n      Sajjad, Hassan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.208/\",\n    doi = \"10.18653/v1/2024.naacl-long.208\",\n    pages = \"3749--3780\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.208.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.208/",
        "pdf_size": 2898368,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5195257841370596353&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2024.naacl-industry.2",
        "title": "Lossless Acceleration of Large Language Model via Adaptive N-gram Parallel Decoding",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "While Large Language Models (LLMs) have shown remarkable abilities, they are hindered by significant resource consumption and considerable latency due to autoregressive processing. In this study, we introduce Adaptive N-gram Parallel Decoding (ANPD), an innovative and lossless approach that accelerates inference by allowing the simultaneous generation of multiple tokens. ANPD incorporates a two-stage approach: it begins with a rapid drafting phase that employs an N-gram module, which adapts based on the current interactive context, followed by a verification phase, during which the original LLM assesses and confirms the proposed tokens. Consequently, ANPD preserves the integrity of the LLM\u2019s original output while enhancing processing speed. We further leverage a multi-level architecture for the N-gram module to enhance the precision of the initial draft, consequently reducing inference latency. ANPD eliminates the need for retraining or extra GPU memory, making it an efficient and plug-and-play enhancement. In our experiments, models such as LLaMA and its fine-tuned variants have shown speed improvements up to 3.67x, validating the effectiveness of our proposed ANPD.",
        "author": "Jie Ou; Yueming Chen; Prof. Tian",
        "authorids": "/j/jie-ou/; /y/yueming-chen/; /p/prof-tian/",
        "bibtex": "@inproceedings{ou-etal-2024-lossless,\n    title = \"Lossless Acceleration of Large Language Model via Adaptive N-gram Parallel Decoding\",\n    author = \"Ou, Jie  and\n      Chen, Yueming  and\n      Tian, Prof.\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.2/\",\n    doi = \"10.18653/v1/2024.naacl-industry.2\",\n    pages = \"10--22\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.2.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.2/",
        "pdf_size": 5091918,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17840211847601271278&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China",
        "aff_domain": "gmail.com;gmail.com;uestc.edu.cn",
        "email": "gmail.com;gmail.com;uestc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Electronic Science and Technology of China",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.uestc.edu.cn",
        "aff_unique_abbr": "UESTC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chengdu",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-short.45",
        "title": "Lost in Space: Probing Fine-grained Spatial Understanding in Vision and Language Resamplers",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "An effective method for combining frozen large language models (LLM) and visual encoders involves a resampler module that creates a \u2018visual prompt\u2019 which is provided to the LLM, along with the textual prompt. While this approach has enabled impressive performance across many coarse-grained tasks like image captioning and visual question answering, more fine-grained tasks that require spatial understanding have not been thoroughly examined. In this paper, we use diagnostic classifiers to measure the extent to which the visual prompt produced by the resampler encodes spatial information. Our results show that this information is largely absent from the resampler output when kept frozen during training of the classifiers. However, when the resampler and classifier are trained jointly, we observe a significant performance boost. This shows that the compression achieved by the resamplers can in principle encode the requisite spatial information, but that more object-aware objectives are needed at the pretraining stage to facilitate this capability.",
        "author": "Georgios Pantazopoulos; Alessandro Suglia; Oliver Lemon; Arash Eshghi",
        "authorids": "/g/georgios-pantazopoulos/; /a/alessandro-suglia/; /o/oliver-lemon/; /a/arash-eshghi/",
        "bibtex": "@inproceedings{pantazopoulos-etal-2024-lost,\n    title = \"Lost in Space: Probing Fine-grained Spatial Understanding in Vision and Language Resamplers\",\n    author = \"Pantazopoulos, Georgios  and\n      Suglia, Alessandro  and\n      Lemon, Oliver  and\n      Eshghi, Arash\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.45/\",\n    doi = \"10.18653/v1/2024.naacl-short.45\",\n    pages = \"540--549\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.45.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.45/",
        "pdf_size": 722768,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9585804638003482081&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Heriot-Watt University+Alana AI; Heriot-Watt University+Alana AI; Heriot-Watt University+Alana AI; Heriot-Watt University+Alana AI",
        "aff_domain": "hw.ac.uk;hw.ac.uk;hw.ac.uk;hw.ac.uk",
        "email": "hw.ac.uk;hw.ac.uk;hw.ac.uk;hw.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Heriot-Watt University;Alana AI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.hw.ac.uk;https://www.alana.ai",
        "aff_unique_abbr": "HWU;Alana AI",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1;0+1;0+1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2024.naacl-long.269",
        "title": "Lost in Transcription: Identifying and Quantifying the Accuracy Biases of Automatic Speech Recognition Systems Against Disfluent Speech",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Automatic speech recognition (ASR) systems, increasingly prevalent in education, healthcare, employment, and mobile technology, face significant challenges in inclusivity, particularly for the 80 million-strong global community of people who stutter. These systems often fail to accurately interpret speech patterns deviating from typical fluency, leading to critical usability issues and misinterpretations. This study evaluates six leading ASRs, analyzing their performance on both a real-world dataset of speech samples from individuals who stutter and a synthetic dataset derived from the widely-used LibriSpeech benchmark. The synthetic dataset, uniquely designed to incorporate various stuttering events, enables an in-depth analysis of each ASR\u2019s handling of disfluent speech. Our comprehensive assessment includes metrics such as word error rate (WER), character error rate (CER), and semantic accuracy of the transcripts. The results reveal a consistent and statistically significant accuracy bias across all ASRs against disfluent speech, manifesting in significant syntactical and semantic inaccuracies in transcriptions. These findings highlight a critical gap in current ASR technologies, underscoring the need for effective bias mitigation strategies. Addressing this bias is imperative not only to improve the technology\u2019s usability for people who stutter but also to ensure their equitable and inclusive participation in the rapidly evolving digital landscape.",
        "author": "Dena Mujtaba; Nihar Mahapatra; Megan Arney; J Yaruss; Hope Gerlach-Houck; Caryn Herring; Jia Bin",
        "authorids": "/d/dena-mujtaba/; /n/nihar-mahapatra/; /m/megan-arney/; /j/j-yaruss/; /h/hope-gerlach-houck/; /c/caryn-herring/; /j/jia-bin/",
        "bibtex": "@inproceedings{mujtaba-etal-2024-lost,\n    title = \"Lost in Transcription: Identifying and Quantifying the Accuracy Biases of Automatic Speech Recognition Systems Against Disfluent Speech\",\n    author = \"Mujtaba, Dena  and\n      Mahapatra, Nihar  and\n      Arney, Megan  and\n      Yaruss, J  and\n      Gerlach-Houck, Hope  and\n      Herring, Caryn  and\n      Bin, Jia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.269/\",\n    doi = \"10.18653/v1/2024.naacl-long.269\",\n    pages = \"4795--4809\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.269.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.269/",
        "pdf_size": 1919419,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5446291814719315409&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Michigan State University; Michigan State University; Michigan State University; Michigan State University; Western Michigan University + FRIENDS: The National Association of Young People Who Stutter; FRIENDS: The National Association of Young People Who Stutter; Michigan State University",
        "aff_domain": "msu.edu;msu.edu;msu.edu;msu.edu;wmich.edu;friendswhostutter.org;msu.edu",
        "email": "msu.edu;msu.edu;msu.edu;msu.edu;wmich.edu;friendswhostutter.org;msu.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;1+2;2;0",
        "aff_unique_norm": "Michigan State University;Western Michigan University;FRIENDS: The National Association of Young People Who Stutter",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.msu.edu;https://www.wmich.edu;",
        "aff_unique_abbr": "MSU;WMU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.48",
        "title": "Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Benchmarks of the multilingual capabilities of text-to-image (T2I) models compare generated images prompted in a test language to an expected image distribution over a concept set. One such benchmark, \u201cConceptual Coverage Across Languages\u201d (CoCo-CroLa), assesses the tangible noun inventory of T2I models by prompting them to generate pictures from a concept list translated to seven languages and comparing the output image populations. Unfortunately, we find that this benchmark contains translation errors of varying severity in Spanish, Japanese, and Chinese. We provide corrections for these errors and analyze how impactful they are on the utility and validity of CoCo-CroLa as a benchmark. We reassess multiple baseline T2I models with the revisions, compare the outputs elicited under the new translations to those conditioned on the old, and show that a correction\u2019s impactfulness on the image-domain benchmark results can be predicted in the text domain with similarity scores. Our findings will guide the future development of T2I multilinguality metrics by providing analytical tools for practical translation decisions.",
        "author": "Michael Saxon; Yiran Luo; Sharon Levy; Chitta Baral; Yezhou Yang; William Yang Wang",
        "authorids": "/m/michael-saxon/; /y/yiran-luo/; /s/sharon-levy/; /c/chitta-baral/; /y/yezhou-yang/; /w/william-yang-wang/",
        "bibtex": "@inproceedings{saxon-etal-2024-lost,\n    title = \"Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts\",\n    author = \"Saxon, Michael  and\n      Luo, Yiran  and\n      Levy, Sharon  and\n      Baral, Chitta  and\n      Yang, Yezhou  and\n      Wang, William Yang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.48/\",\n    doi = \"10.18653/v1/2024.naacl-short.48\",\n    pages = \"572--582\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.48.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.48/",
        "pdf_size": 6758051,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17135521205150936600&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.194",
        "title": "Low-Cost Generation and Evaluation of Dictionary Example Sentences",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Dictionary example sentences play an important role in illustrating word definitions and usage, but manually creating quality sentences is challenging. Prior works have demonstrated that language models can be trained to generate example sentences. However, they relied on costly customized models and word sense datasets for generation and evaluation of their work. Rapid advancements in foundational models present the opportunity to create low-cost, zero-shot methods for the generation and evaluation of dictionary example sentences. We introduce a new automatic evaluation metric called OxfordEval that measures the win-rate of generated sentences against existing Oxford Dictionary sentences. OxfordEval shows high alignment with human judgments, enabling large-scale automated quality evaluation. We experiment with various LLMs and configurations to generate dictionary sentences across word classes. We complement this with a novel approach of using masked language models to identify and select sentences that best exemplify word meaning. The eventual model, FM-MLM, achieves over 85.1% win rate against Oxford baseline sentences according to OxfordEval, compared to 39.8% win rate for prior model-generated sentences.",
        "author": "Bill Cai; Ng Clarence; Daniel Liang; Shelvia Hotama",
        "authorids": "/b/bill-cai/; /n/ng-clarence/; /d/daniel-liang/; /s/shelvia-hotama/",
        "bibtex": "@inproceedings{cai-etal-2024-low,\n    title = \"Low-Cost Generation and Evaluation of Dictionary Example Sentences\",\n    author = \"Cai, Bill  and\n      Clarence, Ng  and\n      Liang, Daniel  and\n      Hotama, Shelvia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.194/\",\n    doi = \"10.18653/v1/2024.naacl-long.194\",\n    pages = \"3538--3549\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.194.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.194/",
        "pdf_size": 271987,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12163870445014989167&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Amazon Web Services \u2020; Ministry of Education, Singapore \u2020; Ministry of Education, Singapore \u2021; Amazon Web Services \u2021",
        "aff_domain": "amazon.com;moe.gov.sg;moe.gov.sg;amazon.com",
        "email": "amazon.com;moe.gov.sg;moe.gov.sg;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Amazon;Ministry of Education",
        "aff_unique_dep": "Amazon Web Services;",
        "aff_unique_url": "https://aws.amazon.com;https://www.moe.gov.sg",
        "aff_unique_abbr": "AWS;MOE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "id": "2024.findings-naacl.77",
        "title": "Low-Rank Adaptation for Multilingual Summarization: An Empirical Study",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning outperforms full fine-tuning and the dynamic composition of language-specific LoRA modules.",
        "author": "Chenxi Whitehouse; Fantine Huot; Jasmijn Bastings; Mostafa Dehghani; Chu-Cheng Lin; Mirella Lapata",
        "authorids": "/c/chenxi-whitehouse/; /f/fantine-huot/; /j/jasmijn-bastings/; /m/mostafa-dehghani/; /c/chu-cheng-lin/; /m/mirella-lapata/",
        "bibtex": "@inproceedings{whitehouse-etal-2024-low,\n    title = \"Low-Rank Adaptation for Multilingual Summarization: An Empirical Study\",\n    author = \"Whitehouse, Chenxi  and\n      Huot, Fantine  and\n      Bastings, Jasmijn  and\n      Dehghani, Mostafa  and\n      Lin, Chu-Cheng  and\n      Lapata, Mirella\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.77/\",\n    doi = \"10.18653/v1/2024.findings-naacl.77\",\n    pages = \"1202--1228\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.77.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.77/",
        "pdf_size": 2923029,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18285697734230598448&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Cambridge + Google DeepMind; Google DeepMind; Google DeepMind; Google DeepMind; Google; Google DeepMind",
        "aff_domain": "cl.cam.ac.uk;google.com;google.com;google.com;google.com;google.com",
        "email": "cl.cam.ac.uk;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;1",
        "aff_unique_norm": "University of Cambridge;Google",
        "aff_unique_dep": ";Google DeepMind",
        "aff_unique_url": "https://www.cam.ac.uk;https://deepmind.com",
        "aff_unique_abbr": "Cambridge;DeepMind",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Cambridge;;Mountain View",
        "aff_country_unique_index": "0+0;0;0;0;1;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2024.naacl-demo.2",
        "title": "Low-code LLM: Graphical User Interface over Large Language Models",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Utilizing Large Language Models (LLMs) for complex tasks is challenging, often involving a time-consuming and uncontrollable prompt engineering process. This paper introduces a novel human-LLM interaction framework, Low-code LLM. It incorporates six types of simple low-code visual programming interactions to achieve more controllable and stable responses. Through visual interaction with a graphical user interface, users can incorporate their ideas into the process without writing trivial prompts. The proposed Low-code LLM framework consists of a Planning LLM that designs a structured planning workflow for complex tasks, which can be correspondingly edited and confirmed by users through low-code visual programming operations, and an Executing LLM that generates responses following the user-confirmed workflow. We highlight three advantages of the low-code LLM: user-friendly interaction, controllable generation, and wide applicability. We demonstrate its benefits using four typical applications. By introducing this framework, we aim to bridge the gap between humans and LLMs, enabling more effective and efficient utilization of LLMs for complex tasks. The code, prompts, and experimental details are available at https://github.com/moymix/TaskMatrix/tree/main/LowCodeLLM. A system demonstration video can be found at https://www.youtube.com/watch?v=jb2C1vaeO3E.",
        "author": "Yuzhe Cai; Shaoguang Mao; Wenshan Wu; Zehua Wang; Yaobo Liang; Tao Ge; Chenfei Wu; WangYou WangYou; Ting Song; Yan Xia; Nan Duan; Furu Wei",
        "authorids": "/y/yuzhe-cai/; /s/shaoguang-mao/; /w/wenshan-wu/; /z/zehua-wang/; /y/yaobo-liang/; /t/tao-ge/; /c/chenfei-wu/; /w/wangyou-wangyou/; /t/ting-song/; /y/yan-xia/; /n/nan-duan/; /f/furu-wei/",
        "bibtex": "@inproceedings{cai-etal-2024-low-code,\n    title = \"Low-code {LLM}: Graphical User Interface over Large Language Models\",\n    author = \"Cai, Yuzhe  and\n      Mao, Shaoguang  and\n      Wu, Wenshan  and\n      Wang, Zehua  and\n      Liang, Yaobo  and\n      Ge, Tao  and\n      Wu, Chenfei  and\n      WangYou, WangYou  and\n      Song, Ting  and\n      Xia, Yan  and\n      Duan, Nan  and\n      Wei, Furu\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.2/\",\n    doi = \"10.18653/v1/2024.naacl-demo.2\",\n    pages = \"12--25\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.2.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.2/",
        "pdf_size": 1764814,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4572140096377500634&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "LowcodeLLM",
        "author_num": 12,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "MSR Asia",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Asia",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.13",
        "title": "Low-resource neural machine translation with morphological modeling",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Morphological modeling in neural machine translation (NMT) is a promising approach to achieving open-vocabulary machine translation for morphologically-rich languages. However, existing methods such as sub-word tokenization and character-based models are limited to the surface forms of the words. In this work, we propose a framework-solution for modeling complex morphology in low-resource settings. A two-tier transformer architecture is chosen to encode morphological information at the inputs. At the target-side output, a multi-task multi-label training scheme coupled with a beam search-based decoder are found to improve machine translation performance. An attention augmentation scheme to the transformer model is proposed in a generic form to allow integration of pre-trained language models and also facilitate modeling of word order relationships between the source and target languages. Several data augmentation techniques are evaluated and shown to increase translation performance in low-resource settings. We evaluate our proposed solution on Kinyarwanda \u2194 English translation using public-domain parallel text. Our final models achieve competitive performance in relation to large multi-lingual models. We hope that our results will motivate more use of explicit morphological information and the proposed model and data augmentations in low-resource NMT.",
        "author": "Antoine Nzeyimana",
        "authorids": "/a/antoine-nzeyimana/",
        "bibtex": "@inproceedings{nzeyimana-2024-low,\n    title = \"Low-resource neural machine translation with morphological modeling\",\n    author = \"Nzeyimana, Antoine\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.13/\",\n    doi = \"10.18653/v1/2024.findings-naacl.13\",\n    pages = \"182--195\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.13.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.13/",
        "pdf_size": 454493,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13390353990750022282&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Massachusetts Amherst",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.380",
        "title": "Lower Bounds on the Expressivity of Recurrent Neural Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The recent successes and spread of large neural language models (LMs) call for a thorough understanding of their abilities. Describing their abilities through LMs\u2019 representational capacity is a lively area of research. Investigations of the representational capacity of neural LMs have predominantly focused on their ability to recognize formal languages. For example, recurrent neural networks (RNNs) as classifiers are tightly linked to regular languages, i.e., languages defined by finite-state automata (FSAs). Such results, however, fall short of describing the capabilities of RNN language models (LMs), which are definitionally distributions over strings. We take a fresh look at the represen- tational capacity of RNN LMs by connecting them to probabilistic FSAs and demonstrate that RNN LMs with linearly bounded precision can express arbitrary regular LMs.",
        "author": "Anej Svete; Franz Nowak; Anisha Sahabdeen; Ryan Cotterell",
        "authorids": "/a/anej-svete/; /f/franz-nowak/; /a/anisha-sahabdeen/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{svete-etal-2024-lower,\n    title = \"Lower Bounds on the Expressivity of Recurrent Neural Language Models\",\n    author = \"Svete, Anej  and\n      Nowak, Franz  and\n      Sahabdeen, Anisha  and\n      Cotterell, Ryan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.380/\",\n    doi = \"10.18653/v1/2024.naacl-long.380\",\n    pages = \"6820--6844\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.380.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.380/",
        "pdf_size": 574767,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3872575508473280674&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich",
        "aff_domain": "ethz.ch;ethz.ch;ethz.ch;ethz.ch",
        "email": "ethz.ch;ethz.ch;ethz.ch;ethz.ch",
        "github": "https://github.com/rycolab/nondeterministic-rnns",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2024.naacl-short.41",
        "title": "M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Document translation poses a challenge for Neural Machine Translation (NMT) systems. Most document-level NMT systems rely on meticulously curated sentence-level parallel data, assuming flawless extraction of text from documents along with their precise reading order. These systems also tend to disregard additional visual cues such as the document layout, deeming it irrelevant. However, real-world documents often possess intricate text layouts that defy these assumptions. Extracting information from Optical Character Recognition (OCR) or heuristic rules can result in errors, and the layout (e.g., paragraphs, headers) may convey relationships between distant sections of text. This complexity is particularly evident in widely used PDF documents, which represent information visually. This paper addresses this gap by introducing M3T a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications.",
        "author": "Benjamin Hsu; Xiaoyu Liu; Huayang Li; Yoshinari Fujinuma; Maria Nadejde; Xing Niu; Ron Litman; Yair Kittenplon; Raghavendra Pappagari",
        "authorids": "/b/benjamin-hsu/; /x/xiaoyu-liu/; /h/huayang-li/; /y/yoshinari-fujinuma/; /m/maria-nadejde/; /x/xing-niu/; /r/ron-litman/; /y/yair-kittenplon/; /r/raghavendra-pappagari/",
        "bibtex": "@inproceedings{hsu-etal-2024-m3t,\n    title = \"{M}3{T}: A New Benchmark Dataset for Multi-Modal Document-Level Machine Translation\",\n    author = \"Hsu, Benjamin  and\n      Liu, Xiaoyu  and\n      Li, Huayang  and\n      Fujinuma, Yoshinari  and\n      Nadejde, Maria  and\n      Niu, Xing  and\n      Litman, Ron  and\n      Kittenplon, Yair  and\n      Pappagari, Raghavendra\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.41/\",\n    doi = \"10.18653/v1/2024.naacl-short.41\",\n    pages = \"499--507\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.41.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.41/",
        "pdf_size": 8170974,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13715545301055785782&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "AWS AI Labs; University of Maryland, College Park + AWS AI Labs; Nara Institute of Science and Technology + AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs",
        "aff_domain": "amazon.com;umd.edu;is.naist.jp;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;umd.edu;is.naist.jp;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1+0;2+0;0;0;0;0;0;0",
        "aff_unique_norm": "Amazon;University of Maryland;Nara Institute of Science and Technology",
        "aff_unique_dep": "AWS AI Labs;;",
        "aff_unique_url": "https://aws.amazon.com;https://www/umd.edu;https://www.nist.go.jp",
        "aff_unique_abbr": "AWS;UMD;NIST",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";College Park",
        "aff_country_unique_index": "0;0+0;1+0;0;0;0;0;0;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "2024.naacl-long.270",
        "title": "MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce MAFALDA, a benchmark for fallacy classification that merges and unites previous fallacy datasets. It comes with a taxonomy that aligns, refines, and unifies existing classifications of fallacies. We further provide a manual annotation of a part of the dataset together with manual explanations for each annotation. We propose a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity. We then evaluate several language models under a zero-shot learning setting and human performances on MAFALDA to assess their capability to detect and classify fallacies.",
        "author": "Chadi Helwe; Tom Calamai; Pierre-Henri Paris; Chlo\u00e9 Clavel; Fabian Suchanek",
        "authorids": "/c/chadi-helwe/; /t/tom-calamai/; /p/pierre-henri-paris/; /c/chloe-clavel/; /f/fabian-suchanek/",
        "bibtex": "@inproceedings{helwe-etal-2024-mafalda,\n    title = \"{MAFALDA}: A Benchmark and Comprehensive Study of Fallacy Detection and Classification\",\n    author = \"Helwe, Chadi  and\n      Calamai, Tom  and\n      Paris, Pierre-Henri  and\n      Clavel, Chlo{\\'e}  and\n      Suchanek, Fabian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.270/\",\n    doi = \"10.18653/v1/2024.naacl-long.270\",\n    pages = \"4810--4845\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.270.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.270/",
        "pdf_size": 1079259,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17110337496949885564&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris, France+INRIA Saclay, Amundi, France; T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris, France+INRIA Saclay, Amundi, France; T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris, France; INRIA Paris, France; T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris, France",
        "aff_domain": "telecom-paris.fr;inria.fr;telecom-paris.fr;inria.fr;telecom-paris.fr",
        "email": "telecom-paris.fr;inria.fr;telecom-paris.fr;inria.fr;telecom-paris.fr",
        "github": "https://github.com/ChadiHelwe/MAFALDA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+1;0;1;0",
        "aff_unique_norm": "T\u00e9l\u00e9com Paris;INRIA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.telecom-paris.fr;https://www.inria.fr",
        "aff_unique_abbr": "T\u00e9l\u00e9com Paris;INRIA",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Saclay;Paris",
        "aff_country_unique_index": "0+0;0+0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2024.naacl-long.288",
        "title": "MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce Multimodal Augmented Generative Images Dialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images . Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialogue datasets, using automated and human evaluation. Our results show that MAGID is comparable to or better than baselines, with significant improvements in human evaluation, especially against retrieval baselines where the image database is small.",
        "author": "Hossein Aboutalebi; Hwanjun Song; Yusheng Xie; Arshit Gupta; Lijia Sun; Hang Su; Igor Shalyminov; Nikolaos Pappas; Siffi Singh; Saab Mansour",
        "authorids": "/h/hossein-aboutalebi/; /h/hwanjun-song/; /y/yusheng-xie/; /a/arshit-gupta/; /l/lijia-sun/; /h/hang-su/; /i/igor-shalyminov/; /n/nikolaos-pappas/; /s/siffi-singh/; /s/saab-mansour/",
        "bibtex": "@inproceedings{aboutalebi-etal-2024-magid,\n    title = \"{MAGID}: An Automated Pipeline for Generating Synthetic Multi-modal Datasets\",\n    author = \"Aboutalebi, Hossein  and\n      Song, Hwanjun  and\n      Xie, Yusheng  and\n      Gupta, Arshit  and\n      Sun, Lijia  and\n      Su, Hang  and\n      Shalyminov, Igor  and\n      Pappas, Nikolaos  and\n      Singh, Siffi  and\n      Mansour, Saab\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.288/\",\n    doi = \"10.18653/v1/2024.naacl-long.288\",\n    pages = \"5150--5167\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.288.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.288/",
        "pdf_size": 13166840,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5273032358154649449&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Cheriton School of Computer Science, University of Waterloo + AWS AI Labs; Korea Advanced Institute of Science and Technology + AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs",
        "aff_domain": "uwaterloo.ca; ; ; ; ; ; ; ; ; ",
        "email": "uwaterloo.ca; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "https://e77p.short.gy/MAGID",
        "author_num": 10,
        "aff_unique_index": "0+1;2+1;1;1;1;1;1;1;1;1",
        "aff_unique_norm": "University of Waterloo;Amazon;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "Cheriton School of Computer Science;AWS AI Labs;",
        "aff_unique_url": "https://uwaterloo.ca;https://aws.amazon.com;https://www.kaist.ac.kr",
        "aff_unique_abbr": "UWaterloo;AWS;KAIST",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "Waterloo;",
        "aff_country_unique_index": "0+1;2+1;1;1;1;1;1;1;1;1",
        "aff_country_unique": "Canada;United States;South Korea"
    },
    {
        "id": "2024.naacl-long.107",
        "title": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses.While effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them.In this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM.Specifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts. In each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning.On adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing. Notably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following.",
        "author": "Suyu Ge; Chunting Zhou; Rui Hou; Madian Khabsa; Yi-Chia Wang; Qifan Wang; Jiawei Han; Yuning Mao",
        "authorids": "/s/suyu-ge/; /c/chunting-zhou/; /r/rui-hou/; /m/madian-khabsa/; /y/yi-chia-wang/; /q/qifan-wang/; /j/jiawei-han/; /y/yuning-mao/",
        "bibtex": "@inproceedings{ge-etal-2024-mart,\n    title = \"{MART}: Improving {LLM} Safety with Multi-round Automatic Red-Teaming\",\n    author = \"Ge, Suyu  and\n      Zhou, Chunting  and\n      Hou, Rui  and\n      Khabsa, Madian  and\n      Wang, Yi-Chia  and\n      Wang, Qifan  and\n      Han, Jiawei  and\n      Mao, Yuning\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.107/\",\n    doi = \"10.18653/v1/2024.naacl-long.107\",\n    pages = \"1927--1937\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.107.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.107/",
        "pdf_size": 449212,
        "gs_citation": 99,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=441956053349872759&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "GenAI; Meta; University of Illinois Urbana-Champaign; GenAI; Meta; University of Illinois Urbana-Champaign; GenAI; Meta",
        "aff_domain": "illinois.edu; ; ; ; ; ; ;meta.com",
        "email": "illinois.edu; ; ; ; ; ; ;meta.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;0;1;2;0;1",
        "aff_unique_norm": "GenAI;Meta;University of Illinois Urbana-Champaign",
        "aff_unique_dep": ";Meta Platforms, Inc.;",
        "aff_unique_url": ";https://meta.com;https://illinois.edu",
        "aff_unique_abbr": ";Meta;UIUC",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "1;1;1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "2024.naacl-long.54",
        "title": "MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Tool-augmented Large Language Models (TALMs) are known to enhance the skillset of large language models (LLMs), thereby, leading to their improved reasoning abilities across many tasks. While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complementary benefits offered by tools for knowledge retrieval and mathematical equation solving are open research questions. In this work, we present MathSensei, a tool-augmented large language model for mathematical reasoning. We study the complementary benefits of the tools - knowledge retriever (Bing Web Search), program generator + executor (Python), and symbolic equation solver (Wolfram-Alpha API) through evaluations on mathematical reasoning datasets. We perform exhaustive ablations on MATH, a popular dataset for evaluating mathematical reasoning on diverse mathematical disciplines. We also conduct experiments involving well-known tool planners to study the impact of tool sequencing on the model performance. MathSensei achieves 13.5% better accuracy over gpt-3.5-turbo with Chain-of-Thought on the MATH dataset. We further observe that TALMs are not as effective for simpler math word problems (in GSM-8K), and the benefit increases as the complexity and required knowledge increases (progressively over AQuA, MMLU-Math, and higher level complex questions in MATH). The code and data are available at https://github.com/Debrup-61/MathSensei.",
        "author": "Debrup Das; Debopriyo Banerjee; Somak Aditya; Ashish Kulkarni",
        "authorids": "/d/debrup-das/; /d/debopriyo-banerjee/; /s/somak-aditya/; /a/ashish-kulkarni/",
        "bibtex": "@inproceedings{das-etal-2024-mathsensei,\n    title = \"{MATHSENSEI}: A Tool-Augmented Large Language Model for Mathematical Reasoning\",\n    author = \"Das, Debrup  and\n      Banerjee, Debopriyo  and\n      Aditya, Somak  and\n      Kulkarni, Ashish\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.54/\",\n    doi = \"10.18653/v1/2024.naacl-long.54\",\n    pages = \"942--966\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.54.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.54/",
        "pdf_size": 2519454,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1558844238516580368&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "IIT Kharagpur; Rakuten Group, Inc. Rakuten Institute of Technology India; IIT Kharagpur; Rakuten Group, Inc. Rakuten Institute of Technology India",
        "aff_domain": "iitkgp.ac.in;rakuten.com;cse.iitkgp.ac.in;rakuten.com",
        "email": "iitkgp.ac.in;rakuten.com;cse.iitkgp.ac.in;rakuten.com",
        "github": "https://github.com/Debrup-61/MathSensei",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Indian Institute of Technology Kharagpur;Rakuten Group, Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitkgp.ac.in;https://www.rakuten.com",
        "aff_unique_abbr": "IIT KGP;Rakuten",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Kharagpur;",
        "aff_country_unique_index": "0;1;0;1",
        "aff_country_unique": "India;Japan"
    },
    {
        "id": "2024.findings-naacl.96",
        "title": "MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Due to the success of large-scale visual-language pretraining (VLP) models and the widespread use of image-text retrieval in industry areas, it is now critically necessary to reduce the model size and streamline their mobile-device deployment. Single- and dual-stream model structures are commonly used in image-text retrieval with the goal of closing the semantic gap between textual and visual modalities. While single-stream models use deep feature fusion to achieve more accurate cross-model alignment, dual-stream models are better at offline indexing and fast inference. We propose a Multi-teacher Cross-modality Alignment Distillation (MCAD) technique to integrate the advantages of single- and dual-stream models. By incorporating the fused single-stream features into the image and text features of the dual-stream model, we formulate new modified teacher similarity distributions and features. Then, we conduct both distribution and feature distillation to boost the capability of the student dual-stream model, achieving high retrieval performance without increasing inference complexity. Extensive experiments demonstrate the remarkable performance and high efficiency of MCAD on image-text retrieval tasks. Furthermore, we implement a lightweight CLIP model on Snapdragon/Dimensity chips with only ~100M running memory and ~8.0ms search latency, achieving the mobile-device application of VLP models.",
        "author": "Youbo Lei; Feifei He; Chen Chen; Yingbin Mo; Sijia Li; Defeng Xie; Haonan Lu",
        "authorids": "/y/youbo-lei/; /f/feifei-he/; /c/chen-chen/; /y/yingbin-mo/; /s/sijia-li/; /d/defeng-xie/; /h/haonan-lu/",
        "bibtex": "@inproceedings{lei-etal-2024-mcad,\n    title = \"{MCAD}: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval\",\n    author = \"Lei, Youbo  and\n      He, Feifei  and\n      Chen, Chen  and\n      Mo, Yingbin  and\n      Li, Sijia  and\n      Xie, Defeng  and\n      Lu, Haonan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.96/\",\n    doi = \"10.18653/v1/2024.findings-naacl.96\",\n    pages = \"1491--1503\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.96.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.96/",
        "pdf_size": 1998043,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:VNeoZ34tgggJ:scholar.google.com/&scioq=MCAD:+Multi-teacher+Cross-modal+Alignment+Distillation+for+efficient+image-text+retrieval&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Xi\u2019an Jiaotong University; OPPO AI Center; OPPO AI Center; University of Sydney; OPPO AI Center; OPPO AI Center; OPPO AI Center",
        "aff_domain": "stu.xjtu.edu.cn;oppo.com;oppo.com;sydney.edu.au;oppo.com;oppo.com;oppo.com",
        "email": "stu.xjtu.edu.cn;oppo.com;oppo.com;sydney.edu.au;oppo.com;oppo.com;oppo.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;2;1;1;1",
        "aff_unique_norm": "Xi'an Jiao Tong University;OPPO;University of Sydney",
        "aff_unique_dep": ";OPPO AI Center;",
        "aff_unique_url": "https://www.xjtu.edu.cn;https://www.oppo.com;https://www.sydney.edu.au",
        "aff_unique_abbr": "XJTU;OPPO;USYD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;0;0",
        "aff_country_unique": "China;Australia"
    },
    {
        "id": "2024.findings-naacl.245",
        "title": "MCECR: A Novel Dataset for Multilingual Cross-Document Event Coreference Resolution",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Event coreference resolution (ECR) is a critical task in information extraction of natural language processing, aiming to identify and link event mentions across multiple documents. Despite recent progress, existing datasets for ECR primarily focus on within-document event coreference and English text, lacking cross-document ECR datasets for multiple languages beyond English. To address this issue, this work presents the first multiligual dataset for cross-document ECR, called MCECR (Multilingual Cross-Document Event Coreference Resolution), that manually annotates a diverse collection of documents for event mentions and coreference in five languages, i.e., English, Spanish, Hindi, Turkish, and Ukrainian. Using sampled articles from Wikinews over various topics as the seeds, our dataset fetches related news articles from the Google search engine to increase the number of non-singleton event clusters. In total, we annotate 5,802 news articles, providing a substantial and varied dataset for multilingual ECR in both within-document and cross-document scenarios. Extensive analysis of the proposed dataset reveals the challenging nature of multilingual event coreference resolution tasks, promoting MCECR as a strong benchmark dataset for future research in this area.",
        "author": "Amir Pouran Ben Veyseh; Viet Dac Lai; Chien Nguyen; Franck Dernoncourt; Thien Nguyen",
        "authorids": "/a/amir-pouran-ben-veyseh/; /v/viet-dac-lai/; /c/chien-nguyen/; /f/franck-dernoncourt/; /t/thien-nguyen/",
        "bibtex": "@inproceedings{pouran-ben-veyseh-etal-2024-mcecr,\n    title = \"{MCECR}: A Novel Dataset for Multilingual Cross-Document Event Coreference Resolution\",\n    author = \"Pouran Ben Veyseh, Amir  and\n      Lai, Viet Dac  and\n      Nguyen, Chien  and\n      Dernoncourt, Franck  and\n      Nguyen, Thien\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.245/\",\n    doi = \"10.18653/v1/2024.findings-naacl.245\",\n    pages = \"3869--3880\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.245.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.245/",
        "pdf_size": 206498,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:WRJKgTN5XrMJ:scholar.google.com/&scioq=MCECR:+A+Novel+Dataset+for+Multilingual+Cross-Document+Event+Coreference+Resolution&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Dept. of Computer Science, University of Oregon, OR, USA; Dept. of Computer Science, University of Oregon, OR, USA; Dept. of Computer Science, University of Oregon, OR, USA; Adobe Research, USA; Dept. of Computer Science, University of Oregon, OR, USA",
        "aff_domain": "cs.uoregon.edu;cs.uoregon.edu;uoregon.edu;adobe.com;uoregon.edu",
        "email": "cs.uoregon.edu;cs.uoregon.edu;uoregon.edu;adobe.com;uoregon.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Oregon;Adobe",
        "aff_unique_dep": "Dept. of Computer Science;Adobe Research",
        "aff_unique_url": "https://www.uoregon.edu;https://research.adobe.com",
        "aff_unique_abbr": "UO;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.235",
        "title": "MDR: Model-Specific Demonstration Retrieval at Inference Time for In-Context Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, retrieval-based in-context learning (ICL) methods for selecting demonstrations have been widely investigated. Existing methods train a dense retriever to retrieve the most appropriate demonstrations for a given test query, which improves ICL performance. However, we find that distinct LLMs exhibit different biases for \u201cwhat is a good demonstration\u201d since they possess differences in training data, model architectures and training methods. As a result, a demonstration suitable for one LLM may not be appropriate for others.Previous approaches ignore the model bias and fail to retrieve the most appropriate demonstrations for different inference LLMs, resulting in a degradation of ICL performance.To address this problem, we propose a simple yet effective metric to evaluate the appropriateness of demonstrations for a specific inference LLM. Furthermore, we introduce a Model-specific Demonstration Retrieval (MDR) method for ICL at inference time, which considers the biases of different LLMs. We test MDR on seen and unseen tasks with multi-scale inference LLMs, such as GPT-Neo-2.7B, LLaMA-7B and Vicuna-13B. Experiments on 23 datasets across 11 data domains highlight the remarkable effectiveness of MDR, showcasing improvements of up to 41.2% in comparison to methods that neglect model biases.",
        "author": "Huazheng Wang; Jinming Wu; Haifeng Sun; Zixuan Xia; Daixuan Cheng; Jingyu Wang; Qi Qi; Jianxin Liao",
        "authorids": "/h/huazheng-wang/; /j/jinming-wu/; /h/haifeng-sun/; /z/zixuan-xia/; /d/daixuan-cheng/; /j/jingyu-wang/; /q/qi-qi/; /j/jianxin-liao/",
        "bibtex": "@inproceedings{wang-etal-2024-mdr,\n    title = \"{MDR}: Model-Specific Demonstration Retrieval at Inference Time for In-Context Learning\",\n    author = \"Wang, Huazheng  and\n      Wu, Jinming  and\n      Sun, Haifeng  and\n      Xia, Zixuan  and\n      Cheng, Daixuan  and\n      Wang, Jingyu  and\n      Qi, Qi  and\n      Liao, Jianxin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.235/\",\n    doi = \"10.18653/v1/2024.naacl-long.235\",\n    pages = \"4189--4204\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.235.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.235/",
        "pdf_size": 1232806,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7427162662859357254&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;gmail.com;bupt.edu.cn;bupt.edu.cn;gmail.com",
        "email": "bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;bupt.edu.cn;gmail.com;bupt.edu.cn;bupt.edu.cn;gmail.com",
        "github": "https://github.com/kiming-ng/MDR",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications",
        "aff_unique_dep": "State Key Laboratory of Networking and Switching Technology",
        "aff_unique_url": "http://www.bupt.edu.cn",
        "aff_unique_abbr": "BUPT",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.143",
        "title": "MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "There has been a surge in LLM evaluation research to understand LLM capabilities and limitations. However, much of this research has been confined to English, leaving LLM building and evaluation for non-English languages relatively unexplored. Several new LLMs have been introduced recently, necessitating their evaluation on non-English languages. This study aims to perform a thorough evaluation of the non-English capabilities of SoTA LLMs (GPT-3.5-Turbo, GPT-4, PaLM2, Gemini-Pro, Mistral, Llama2, and Gemma) by comparing them on the same set of multilingual datasets. Our benchmark comprises 22 datasets covering 83 languages, including low-resource African languages. We also include two multimodal datasets in the benchmark and compare the performance of LLaVA models, GPT-4-Vision and Gemini-Pro-Vision. Our experiments show that larger models such as GPT-4, Gemini-Pro and PaLM2 outperform smaller models on various tasks, notably on low-resource languages, with GPT-4 outperforming PaLM2 and Gemini-Pro on more datasets. We also perform a study on data contamination and find that several models are likely to be contaminated with multilingual evaluation benchmarks, necessitating approaches to detect and handle contamination while assessing the multilingual performance of LLMs.",
        "author": "Sanchit Ahuja; Divyanshu Aggarwal; Varun Gumma; Ishaan Watts; Ashutosh Sathe; Millicent Ochieng; Rishav Hada; Prachi Jain; Mohamed Ahmed; Kalika Bali; Sunayana Sitaram",
        "authorids": "/s/sanchit-ahuja/; /d/divyanshu-aggarwal/; /v/varun-gumma/; /i/ishaan-watts/; /a/ashutosh-sathe/; /m/millicent-ochieng/; /r/rishav-hada/; /p/prachi-jain/; /m/mohamed-ahmed/; /k/kalika-bali/; /s/sunayana-sitaram/",
        "bibtex": "@inproceedings{ahuja-etal-2024-megaverse,\n    title = \"{MEGAVERSE}: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks\",\n    author = \"Ahuja, Sanchit  and\n      Aggarwal, Divyanshu  and\n      Gumma, Varun  and\n      Watts, Ishaan  and\n      Sathe, Ashutosh  and\n      Ochieng, Millicent  and\n      Hada, Rishav  and\n      Jain, Prachi  and\n      Ahmed, Mohamed  and\n      Bali, Kalika  and\n      Sitaram, Sunayana\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.143/\",\n    doi = \"10.18653/v1/2024.naacl-long.143\",\n    pages = \"2598--2637\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.143.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.143/",
        "pdf_size": 1180178,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15057956359283385676&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation",
        "aff_domain": "microsoft.com; ; ; ; ; ; ; ; ; ;microsoft.com",
        "email": "microsoft.com; ; ; ; ; ; ; ; ; ;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Corporation",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.64",
        "title": "MEMORY-VQ: Compression for Tractable Internet-Scale Memory",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Retrieval augmentation is a powerful but expensive method to make language models more knowledgeable about the world. Memory-based methods like LUMEN (de Jong et al., 2023a) pre-compute token representations for retrieved passages to drastically speed up inference. However, memory also leads to much greater storage requirements from storing pre-computed representations. We propose MEMORY-VQ, a new method to reduce storage requirements of memory-augmented models without sacrificing performance. Our method uses a vector quantization variational autoencoder (VQ-VAE) to compress token representations. We apply MEMORY-VQ to the LUMEN model to obtain LUMEN-VQ, a memory model that achieves a 16x compression rate with comparable performance on the KILT benchmark. LUMEN-VQ enables practical retrieval augmentation even for extremely large retrieval corpora.",
        "author": "Yury Zemlyanskiy; Michiel de Jong; Luke Vilnis; Santiago Ontanon; William Cohen; Sumit Sanghai; Joshua Ainslie",
        "authorids": "/y/yury-zemlyanskiy/; /m/michiel-de-jong/; /l/luke-vilnis/; /s/santiago-ontanon/; /w/william-cohen/; /s/sumit-sanghai/; /j/joshua-ainslie/",
        "bibtex": "@inproceedings{zemlyanskiy-etal-2024-memory,\n    title = \"{MEMORY}-{VQ}: Compression for Tractable {I}nternet-Scale Memory\",\n    author = \"Zemlyanskiy, Yury  and\n      de Jong, Michiel  and\n      Vilnis, Luke  and\n      Ontanon, Santiago  and\n      Cohen, William  and\n      Sanghai, Sumit  and\n      Ainslie, Joshua\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.64/\",\n    doi = \"10.18653/v1/2024.naacl-short.64\",\n    pages = \"737--744\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.64.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.64/",
        "pdf_size": 177924,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:KcOuZxufsK4J:scholar.google.com/&scioq=MEMORY-VQ:+Compression+for+Tractable+Internet-Scale+Memory&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "augmentcode.com;augmentcode.com; ; ; ; ; ",
        "email": "augmentcode.com;augmentcode.com; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.148",
        "title": "METAL: Towards Multilingual Meta-Evaluation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that LLMs excel on many standard NLP benchmarks. However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics. However, past work has shown that LLM-based evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization. This dataset is created specifically to evaluate LLM-based evaluators, which we refer to as meta-evaluation (METAL). We compare the performance of LLM-based evaluators created using GPT-3.5-Turbo, GPT-4, and PaLM2. Our results indicate that LLM-based evaluators based on GPT-4 perform the best across languages, while GPT-3.5-Turbo performs poorly. Additionally, we perform an analysis of the reasoning provided by LLM-based evaluators and find that it often does not match the reasoning provided by human judges.",
        "author": "Rishav Hada; Varun Gumma; Mohamed Ahmed; Kalika Bali; Sunayana Sitaram",
        "authorids": "/r/rishav-hada/; /v/varun-gumma/; /m/mohamed-ahmed/; /k/kalika-bali/; /s/sunayana-sitaram/",
        "bibtex": "@inproceedings{hada-etal-2024-metal,\n    title = \"{METAL}: Towards Multilingual Meta-Evaluation\",\n    author = \"Hada, Rishav  and\n      Gumma, Varun  and\n      Ahmed, Mohamed  and\n      Bali, Kalika  and\n      Sitaram, Sunayana\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.148/\",\n    doi = \"10.18653/v1/2024.findings-naacl.148\",\n    pages = \"2280--2298\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.148.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.148/",
        "pdf_size": 492325,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=353186442516609952&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation; Microsoft Corporation",
        "aff_domain": "gmail.com;gmail.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "gmail.com;gmail.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Corporation",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.110",
        "title": "MICo: Preventative Detoxification of Large Language Models through Inhibition Control",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) are powerful tools which have been both dominant and commonplace in the field of Artificial Intelligence. Yet, LLMs have a tendency to devolve into toxic degeneration, wherein otherwise safe and unproblematic models begin generating toxic content. For the sake of social responsibility and inspired by the biological mechanisms of inhibition control, we introduce the paradigm of Education for Societal Norms (ESN). By collecting and labeling examples as acceptable and unacceptable (in this case toxic and non-toxic), and including a corresponding acceptable rewrite with every unacceptable example, we introduce a new mechanism for LLM detoxification. We annotate a dataset of 2,850 entries and use it to fine-tune a model, which we call a Model with Inhibition Control (MICo). Evaluating this model on toxicity detection capability, rewrite detoxification, meaning preservation, and overall toxicity reduction, we discover significant improvements over the baseline model. In our experiments we show that overall toxicity of this model is more than 60% reduced, with over 75% reduction in severe toxicity.",
        "author": "Roy Siegelmann; Ninareh Mehrabi; Palash Goyal; Prasoon Goyal; Lisa Bauer; Jwala Dhamala; Aram Galstyan; Rahul Gupta; Reza Ghanadan",
        "authorids": "/r/roy-siegelmann/; /n/ninareh-mehrabi/; /p/palash-goyal/; /p/prasoon-goyal/; /l/lisa-bauer/; /j/jwala-dhamala/; /a/aram-galstyan/; /r/rahul-gupta/; /r/reza-ghanadan/",
        "bibtex": "@inproceedings{siegelmann-etal-2024-mico,\n    title = \"{MIC}o: Preventative Detoxification of Large Language Models through Inhibition Control\",\n    author = \"Siegelmann, Roy  and\n      Mehrabi, Ninareh  and\n      Goyal, Palash  and\n      Goyal, Prasoon  and\n      Bauer, Lisa  and\n      Dhamala, Jwala  and\n      Galstyan, Aram  and\n      Gupta, Rahul  and\n      Ghanadan, Reza\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.110/\",\n    doi = \"10.18653/v1/2024.findings-naacl.110\",\n    pages = \"1696--1703\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.110.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.110/",
        "pdf_size": 1182857,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16068965455956817811&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Johns Hopkins University; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations",
        "aff_domain": "jhu.edu;amazon.com; ; ; ; ; ; ; ",
        "email": "jhu.edu;amazon.com; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;1;1;1;1;1;1;1",
        "aff_unique_norm": "Johns Hopkins University;Amazon",
        "aff_unique_dep": ";AGI Foundations",
        "aff_unique_url": "https://www.jhu.edu;https://www.amazon.com",
        "aff_unique_abbr": "JHU;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.138",
        "title": "MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Query expansion, pivotal in search engines, enhances the representation of user information needs with additional terms. While existing methods expand queries using retrieved or generated contextual documents, each approach has notable limitations. Retrieval-based methods often fail to accurately capture search intent, particularly with brief or ambiguous queries. Generation-based methods, utilizing large language models (LLMs), generally lack corpus-specific knowledge and entail high fine-tuning costs. To address these gaps, we propose a novel zero-shot query expansion framework utilizing LLMs for mutual verification. Specifically, we first design a query-query-document generation method, leveraging LLMs\u2019 zero-shot reasoning ability to produce diverse sub-queries and corresponding documents. Then, a mutual verification process synergizes generated and retrieved documents for optimal expansion. Our proposed method is fully zero-shot, and extensive experiments on three public benchmark datasets are conducted to demonstrate its effectiveness over existing methods. Our code is available online at https://github.com/Applied-Machine-Learning-Lab/MILL to ease reproduction.",
        "author": "Pengyue Jia; Yiding Liu; Xiangyu Zhao; Xiaopeng Li; Changying Hao; Shuaiqiang Wang; Dawei Yin",
        "authorids": "/p/pengyue-jia/; /y/yiding-liu/; /x/xiangyu-zhao/; /x/xiaopeng-li/; /c/changying-hao/; /s/shuaiqiang-wang/; /d/dawei-yin/",
        "bibtex": "@inproceedings{jia-etal-2024-mill,\n    title = \"{MILL}: Mutual Verification with Large Language Models for Zero-Shot Query Expansion\",\n    author = \"Jia, Pengyue  and\n      Liu, Yiding  and\n      Zhao, Xiangyu  and\n      Li, Xiaopeng  and\n      Hao, Changying  and\n      Wang, Shuaiqiang  and\n      Yin, Dawei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.138/\",\n    doi = \"10.18653/v1/2024.naacl-long.138\",\n    pages = \"2498--2518\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.138.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.138/",
        "pdf_size": 481967,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8538950000096405447&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "City University of Hong Kong; Baidu Inc.+City University of Hong Kong; Baidu Inc.; Baidu Inc.; Baidu Inc.; Baidu Inc.; Baidu Inc.",
        "aff_domain": "my.cityu.edu.hk;gmail.com;cityu.edu.hk;my.cityu.edu.hk;gmail.com;gmail.com;acm.org",
        "email": "my.cityu.edu.hk;gmail.com;cityu.edu.hk;my.cityu.edu.hk;gmail.com;gmail.com;acm.org",
        "github": "https://github.com/Applied-Machine-Learning-Lab/MILL",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1+0;1;1;1;1;1",
        "aff_unique_norm": "City University of Hong Kong;Baidu",
        "aff_unique_dep": ";Baidu Inc.",
        "aff_unique_url": "https://www.cityu.edu.hk;https://www.baidu.com",
        "aff_unique_abbr": "CityU;Baidu",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.70",
        "title": "MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "With the rapid development of large language models (LLMs) and their integration into large multimodal models (LMMs), there has beenimpressive progress in zero-shot completion of user-oriented vision-language tasks. However, a gap remains in the domain of chartimage understanding due to the distinct abstract components in charts. To address this, we introduce a large-scale MultiModal ChartInstruction (MMC-Instruction) dataset comprising 600k instances supporting diverse tasks and chart types. Leveraging this data, we de-velop MultiModal Chart Assistant (MMCA), an LMM that achieves state-of-the-art performance on existing chart QA benchmarks. Recognizing the need for a comprehensive evaluation of LMM chart understanding, we also propose a MultiModal Chart Benchmark (MMC-Benchmark), a comprehensive human-annotated benchmark with nine distinct tasks evaluating reasoning capabilities over charts.Extensive experiments on MMC-Benchmark reveal the limitations of existing LMMs on correctly interpreting charts, even for the mostrecent GPT-4V model. Our work provides an instruction-tuning methodology and benchmark to advance multimodal understanding ofcharts. Code and data are available at https://github.com/FuxiaoLiu/MMC.",
        "author": "Fuxiao Liu; Xiaoyang Wang; Wenlin Yao; Jianshu Chen; Kaiqiang Song; Sangwoo Cho; Yaser Yacoob; Dong Yu",
        "authorids": "/f/fuxiao-liu/; /x/xiaoyang-wang/; /w/wenlin-yao/; /j/jianshu-chen/; /k/kaiqiang-song/; /s/sangwoo-cho/; /y/yaser-yacoob/; /d/dong-yu/",
        "bibtex": "@inproceedings{liu-etal-2024-mmc,\n    title = \"{MMC}: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning\",\n    author = \"Liu, Fuxiao  and\n      Wang, Xiaoyang  and\n      Yao, Wenlin  and\n      Chen, Jianshu  and\n      Song, Kaiqiang  and\n      Cho, Sangwoo  and\n      Yacoob, Yaser  and\n      Yu, Dong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.70/\",\n    doi = \"10.18653/v1/2024.naacl-long.70\",\n    pages = \"1287--1310\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.70.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.70/",
        "pdf_size": 4197033,
        "gs_citation": 123,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2144513985628099223&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Maryland, College Park+Tencent AI Lab, Bellevue, USA; Tencent AI Lab, Bellevue, USA; Tencent AI Lab, Bellevue, USA; Tencent AI Lab, Bellevue, USA; Tencent AI Lab, Bellevue, USA; Tencent AI Lab, Bellevue, USA; University of Maryland, College Park+Tencent AI Lab, Bellevue, USA; Tencent AI Lab, Bellevue, USA",
        "aff_domain": "umd.edu;global.tencent.com;global.tencent.com;global.tencent.com;global.tencent.com;global.tencent.com;umd.edu;global.tencent.com",
        "email": "umd.edu;global.tencent.com;global.tencent.com;global.tencent.com;global.tencent.com;global.tencent.com;umd.edu;global.tencent.com",
        "github": "https://github.com/FuxiaoLiu/MMC",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;1;1;1;1;1;0+1;1",
        "aff_unique_norm": "University of Maryland;Tencent",
        "aff_unique_dep": ";AI Lab",
        "aff_unique_url": "https://www/umd.edu;https://ai.tencent.com",
        "aff_unique_abbr": "UMD;Tencent AI Lab",
        "aff_campus_unique_index": "0+1;1;1;1;1;1;0+1;1",
        "aff_campus_unique": "College Park;Bellevue",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.252",
        "title": "MOKA: Moral Knowledge Augmentation for Moral Event Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "News media often strive to minimize explicit moral language in news articles, yet most articles are dense with moral values as expressed through the reported events themselves. However, values that are reflected in the intricate dynamics among *participating entities* and *moral events* are far more challenging for most NLP systems to detect, including LLMs. To study this phenomenon, we annotate a new dataset, **MORAL EVENTS**, consisting of 5,494 structured event annotations on 474 news articles by diverse US media across the political spectrum. We further propose **MOKA**, a moral event extraction framework with **MO**ral **K**nowledge **A**ugmentation, which leverages knowledge derived from moral words and moral scenarios to produce structural representations of morality-bearing events. Experiments show that **MOKA** outperforms competitive baselines across three moral event understanding tasks. Further analysis shows even ostensibly nonpartisan media engage in the selective reporting of moral events.",
        "author": "Xinliang Frederick Zhang; Winston Wu; Nicholas Beauchamp; Lu Wang",
        "authorids": "/x/xinliang-frederick-zhang/; /w/winston-wu/; /n/nicholas-beauchamp/; /l/lu-wang/",
        "bibtex": "@inproceedings{zhang-etal-2024-moka,\n    title = \"{MOKA}: Moral Knowledge Augmentation for Moral Event Extraction\",\n    author = \"Zhang, Xinliang Frederick  and\n      Wu, Winston  and\n      Beauchamp, Nicholas  and\n      Wang, Lu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.252/\",\n    doi = \"10.18653/v1/2024.naacl-long.252\",\n    pages = \"4481--4502\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.252.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.252/",
        "pdf_size": 591574,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2622643345105018217&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science and Engineering, University of Michigan, Ann Arbor, MI; Department of Computer Science, University of Hawaii at Hilo, Hilo, HI; Department of Political Science, Northeastern University, Boston, MA; Computer Science and Engineering, University of Michigan, Ann Arbor, MI",
        "aff_domain": "umich.edu;umich.edu;hawaii.edu;northeastern.edu",
        "email": "umich.edu;umich.edu;hawaii.edu;northeastern.edu",
        "github": "https://github.com/launchnlp/MOKA",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "University of Michigan;University of Hawaii at Hilo;Northeastern University",
        "aff_unique_dep": "Computer Science and Engineering;Department of Computer Science;Department of Political Science",
        "aff_unique_url": "https://www.umich.edu;https://www.hilo.hawaii.edu;https://www.northeastern.edu",
        "aff_unique_abbr": "UM;UH Hilo;NEU",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Ann Arbor;Hilo;Boston",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.442",
        "title": "MOSAICo: a Multilingual Open-text Semantically Annotated Interlinked Corpus",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Several Natural Language Understanding (NLU) tasks focus on linking text to explicit knowledge, including Word Sense Disambiguation, Semantic Role Labeling, Semantic Parsing, and Relation Extraction. In addition to the importance of connecting raw text with explicit knowledge bases, the integration of such carefully curated knowledge into deep learning models has been shown to be beneficial across a diverse range of applications, including Language Modeling and Machine Translation. Nevertheless, the scarcity of semantically-annotated corpora across various tasks and languages limits the potential advantages significantly. To address this issue, we put forward MOSAICo, the first endeavor aimed at equipping the research community with the key ingredients to model explicit semantic knowledge at a large scale, providing hundreds of millions of silver yet high-quality annotations for four NLU tasks across five languages. We describe the creation process of MOSAICo, demonstrate its quality and variety, and analyze the interplay between different types of semantic information. MOSAICo, available at https://github.com/SapienzaNLP/mosaico, aims to drop the requirement of closed, licensed datasets and represents a step towards a level playing field across languages and tasks in NLU.",
        "author": "Simone Conia; Edoardo Barba; Abelardo Carlos Martinez Lorenzo; Pere-Llu\u00eds Huguet Cabot; Riccardo Orlando; Luigi Procopio; Roberto Navigli",
        "authorids": "/s/simone-conia/; /e/edoardo-barba/; /a/abelardo-carlos-martinez-lorenzo/; /p/pere-lluis-huguet-cabot/; /r/riccardo-orlando/; /l/luigi-procopio/; /r/roberto-navigli/",
        "bibtex": "@inproceedings{conia-etal-2024-mosaico,\n    title = \"{MOSAIC}o: a Multilingual Open-text Semantically Annotated Interlinked Corpus\",\n    author = \"Conia, Simone  and\n      Barba, Edoardo  and\n      Martinez Lorenzo, Abelardo Carlos  and\n      Huguet Cabot, Pere-Llu{\\'i}s  and\n      Orlando, Riccardo  and\n      Procopio, Luigi  and\n      Navigli, Roberto\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.442/\",\n    doi = \"10.18653/v1/2024.naacl-long.442\",\n    pages = \"7990--8004\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.442.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.442/",
        "pdf_size": 449799,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1480230869913016484&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Sapienza NLP Group, Sapienza University of Rome; Sapienza NLP Group, Sapienza University of Rome; Sapienza NLP Group, Sapienza University of Rome; Sapienza NLP Group, Sapienza University of Rome; Sapienza NLP Group, Sapienza University of Rome; Litus AI, Italy; Sapienza NLP Group, Sapienza University of Rome",
        "aff_domain": "uniroma1.it;uniroma1.it;uniroma1.it;uniroma1.it;uniroma1.it;litus.ai;uniroma1.it",
        "email": "uniroma1.it;uniroma1.it;uniroma1.it;uniroma1.it;uniroma1.it;litus.ai;uniroma1.it",
        "github": "https://github.com/SapienzaNLP/mosaico",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;1;0",
        "aff_unique_norm": "Sapienza University of Rome;Litus AI",
        "aff_unique_dep": "Sapienza NLP Group;",
        "aff_unique_url": "https://www.uniroma1.it;",
        "aff_unique_abbr": "Sapienza;",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Rome;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2024.naacl-long.90",
        "title": "MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The task of scientific Natural Language Inference (NLI) involves predicting the semantic relation between two sentences extracted from research articles. This task was recently proposed along with a new dataset called SciNLI derived from papers published in the computational linguistics domain. In this paper, we aim to introduce diversity in the scientific NLI task and present MSciNLI, a dataset containing 132,320 sentence pairs extracted from five new scientific domains. The availability of multiple domains makes it possible to study domain shift for scientific NLI. We establish strong baselines on MSciNLI by fine-tuning Pre-trained Language Models (PLMs) and prompting Large Language Models (LLMs). The highest Macro F1 scores of PLM and LLM baselines are 77.21% and 51.77%, respectively, illustrating that MSciNLI is challenging for both types of models. Furthermore, we show that domain shift degrades the performance of scientific NLI models which demonstrates the diverse characteristics of different domains in our dataset. Finally, we use both scientific NLI datasets in an intermediate task transfer learning setting and show that they can improve the performance of downstream tasks in the scientific domain. We make our dataset and code available on Github.",
        "author": "Mobashir Sadat; Cornelia Caragea",
        "authorids": "/m/mobashir-sadat/; /c/cornelia-caragea/",
        "bibtex": "@inproceedings{sadat-caragea-2024-mscinli,\n    title = \"{MS}ci{NLI}: A Diverse Benchmark for Scientific Natural Language Inference\",\n    author = \"Sadat, Mobashir  and\n      Caragea, Cornelia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.90/\",\n    doi = \"10.18653/v1/2024.naacl-long.90\",\n    pages = \"1610--1629\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.90.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.90/",
        "pdf_size": 4633319,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8551460513276990464&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science, University of Illinois Chicago; Computer Science, University of Illinois Chicago",
        "aff_domain": "uic.edu;uic.edu",
        "email": "uic.edu;uic.edu",
        "github": "https://github.com/msadat3/MSciNLI",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Illinois Chicago",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.358",
        "title": "MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation, yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods ignore the capability of student and teacher models, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student. Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the MT model on about 10% examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve MT performances on unseen contexts and words.",
        "author": "Jiahuan Li; Shanbo Cheng; Shujian Huang; Jiajun Chen",
        "authorids": "/j/jiahuan-li/; /s/shanbo-cheng/; /s/shujian-huang/; /j/jiajun-chen/",
        "bibtex": "@inproceedings{li-etal-2024-mt,\n    title = \"{MT}-{PATCHER}: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation\",\n    author = \"Li, Jiahuan  and\n      Cheng, Shanbo  and\n      Huang, Shujian  and\n      Chen, Jiajun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.358/\",\n    doi = \"10.18653/v1/2024.naacl-long.358\",\n    pages = \"6445--6459\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.358.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.358/",
        "pdf_size": 867812,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1083392594814866026&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, China\u2663; ByteDance Research\u2660; National Key Laboratory for Novel Software Technology, Nanjing University, China\u2663\u2020; National Key Laboratory for Novel Software Technology, Nanjing University, China\u2663\u2020",
        "aff_domain": "smail.nju.edu.cn;bytedance.com;nju.edu.cn;nju.edu.cn",
        "email": "smail.nju.edu.cn;bytedance.com;nju.edu.cn;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Nanjing University;ByteDance",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;Research",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.bytedance.com",
        "aff_unique_abbr": "Nanjing U;ByteDance",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.446",
        "title": "MaCSC: Towards Multimodal-augmented Pre-trained Language Models via Conceptual Prototypes and Self-balancing Calibration",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained language models (PLMs) that rely solely on textual data may exhibit limitations in multimodal semantics comprehension. Existing solutions attempt to alleviate this issue by incorporating explicit image retrieval or generation techniques.However, these methods: (1) focus exclusively on the static image modality; (2) inevitably encounter modality gaps and noise; (3) indiscriminately treat all modalities.In this paper, we propose a novel multimodal-augmented framework termed MaCSC, which can infuse multimodal semantics into PLMs and facilitate a self-balancing calibration of information allocation.Specifically, MaCSC obtains modal-specific conceptual prototypes from contrastive pre-training models (e.g., CLIP),and aggregates the intra- and inter-modal semantics of the conceptual prototype to enhance PLMs.In addition, we utilize a novel self-balancing contrastive loss to achieve multi-scale self-balancing calibration of multimodal information during fine-tuning PLMs.Experimental results show that MaCSC consistently improves the performance of PLMs across various architectures and scales, and outperforms competitive baselines on multiple NLP tasks.",
        "author": "Xianwei Zhuang; Zhichang Wang; Xuxin Cheng; Yuxin Xie; Liming Liang; Yuexian Zou",
        "authorids": "/x/xianwei-zhuang/; /z/zhichang-wang/; /x/xuxin-cheng/; /y/yuxin-xie/; /l/liming-liang/; /y/yuexian-zou/",
        "bibtex": "@inproceedings{zhuang-etal-2024-macsc,\n    title = \"{M}a{CSC}: Towards Multimodal-augmented Pre-trained Language Models via Conceptual Prototypes and Self-balancing Calibration\",\n    author = \"Zhuang, Xianwei  and\n      Wang, Zhichang  and\n      Cheng, Xuxin  and\n      Xie, Yuxin  and\n      Liang, Liming  and\n      Zou, Yuexian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.446/\",\n    doi = \"10.18653/v1/2024.naacl-long.446\",\n    pages = \"8077--8090\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.446.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.446/",
        "pdf_size": 2198348,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7611393548729987129&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "School of ECE, Peking University, China; School of ECE, Peking University, China; School of ECE, Peking University, China; School of ECE, Peking University, China; School of ECE, Peking University, China; School of ECE, Peking University, China",
        "aff_domain": "stu.pku.edu.cn;stu.pku.edu.cn;stu.pku.edu.cn;stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;stu.pku.edu.cn;stu.pku.edu.cn;stu.pku.edu.cn;stu.pku.edu.cn;pku.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "School of ECE",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.297",
        "title": "MacGyver: Are Large Language Models Creative Problem Solvers?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking.This work (1) introduces a fresh arena for intelligent agents focusing on intricate aspects of physical reasoning, planning, and unconventional thinking, which supplements the existing spectrum of machine intelligence; and (2) provides insight into the constrained problem-solving capabilities of both humans and AI.",
        "author": "Yufei Tian; Abhilasha Ravichander; Lianhui Qin; Ronan Le Bras; Raja Marjieh; Nanyun Peng; Yejin Choi; Thomas Griffiths; Faeze Brahman",
        "authorids": "/y/yufei-tian/; /a/abhilasha-ravichander/; /l/lianhui-qin/; /r/ronan-le-bras/; /r/raja-marjieh/; /n/nanyun-peng/; /y/yejin-choi/; /t/thomas-l-griffiths/; /f/faeze-brahman/",
        "bibtex": "@inproceedings{tian-etal-2024-macgyver,\n    title = \"{M}ac{G}yver: Are Large Language Models Creative Problem Solvers?\",\n    author = \"Tian, Yufei  and\n      Ravichander, Abhilasha  and\n      Qin, Lianhui  and\n      Le Bras, Ronan  and\n      Marjieh, Raja  and\n      Peng, Nanyun  and\n      Choi, Yejin  and\n      Griffiths, Thomas  and\n      Brahman, Faeze\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.297/\",\n    doi = \"10.18653/v1/2024.naacl-long.297\",\n    pages = \"5303--5324\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.297.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.297/",
        "pdf_size": 5326377,
        "gs_citation": 31,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18289649900830159492&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Los Angeles; Allen Institute for Artificial Intelligence; University of California, San Diego; Allen Institute for Artificial Intelligence; Princeton University; University of California, Los Angeles; University of Washington; Princeton University; University of Washington",
        "aff_domain": "cs.ucla.edu;allenai.org; ; ; ; ; ; ; ",
        "email": "cs.ucla.edu;allenai.org; ; ; ; ; ; ; ",
        "github": "https://github.com/allenai/MacGyver",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;2;1;3;0;4;3;4",
        "aff_unique_norm": "University of California, Los Angeles;Allen Institute for Artificial Intelligence;University of California, San Diego;Princeton University;University of Washington",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.ucla.edu;https://allenai.org;https://www.ucsd.edu;https://www.princeton.edu;https://www.washington.edu",
        "aff_unique_abbr": "UCLA;AI2;UCSD;Princeton;UW",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Los Angeles;;San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.195",
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed by further analysis, show that TRICE can make the large language model selectively use tools by improving the accuracy of tool usage while enhancing insufficient tool learning and mitigating excessive reliance on tools.",
        "author": "Shuofei Qiao; Honghao Gui; Chengfei Lv; Qianghuai Jia; Huajun Chen; Ningyu Zhang",
        "authorids": "/s/shuofei-qiao/; /h/honghao-gui/; /c/chengfei-lv/; /q/qianghuai-jia/; /h/huajun-chen/; /n/ningyu-zhang/",
        "bibtex": "@inproceedings{qiao-etal-2024-making,\n    title = \"Making Language Models Better Tool Learners with Execution Feedback\",\n    author = \"Qiao, Shuofei  and\n      Gui, Honghao  and\n      Lv, Chengfei  and\n      Jia, Qianghuai  and\n      Chen, Huajun  and\n      Zhang, Ningyu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.195/\",\n    doi = \"10.18653/v1/2024.naacl-long.195\",\n    pages = \"3550--3568\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.195.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.195/",
        "pdf_size": 2099550,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16363121049023985796&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "1College of Computer Science and Technology, Zhejiang University + 2ZJU-Ant Group Joint Research Center for Knowledge Graphs, Zhejiang University + 3ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University; 1College of Computer Science and Technology, Zhejiang University + 2ZJU-Ant Group Joint Research Center for Knowledge Graphs, Zhejiang University; 4Alibaba Group; 4Alibaba Group; 1College of Computer Science and Technology, Zhejiang University + 2ZJU-Ant Group Joint Research Center for Knowledge Graphs, Zhejiang University + 3ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University; 1College of Computer Science and Technology, Zhejiang University + 2ZJU-Ant Group Joint Research Center for Knowledge Graphs, Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn; ; ;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn; ; ;zju.edu.cn;zju.edu.cn",
        "github": "https://github.com/zjunlp/TRICE",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0+0;0+0;1;1;0+0+0;0+0",
        "aff_unique_norm": "Zhejiang University;Alibaba Group",
        "aff_unique_dep": "College of Computer Science and Technology;",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "ZJU;Alibaba",
        "aff_campus_unique_index": "1;;1;",
        "aff_campus_unique": ";Hangzhou",
        "aff_country_unique_index": "0+0+0;0+0;0;0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.211",
        "title": "MapGuide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Decoding continuous language from brain activity is a formidable yet promising field of research. It is particularly significant for aiding people with speech disabilities to communicate through brain signals. This field addresses the complex task of mapping brain signals to text. The previous best attempt reverse-engineered this process in an indirect way: it began by learning to encode brain activity from text and then guided text generation by aligning with predicted brain responses. In contrast, we propose a simple yet effective method that guides text reconstruction by directly comparing them with the predicted text embeddings mapped from brain activities. Comprehensive experiments reveal that our method significantly outperforms the current state-of-the-art model, showing average improvements of 77% and 54% on BLEU and METEOR scores. We further validate the proposed modules through detailed ablation studies and case analyses and highlight a critical correlation: the more precisely we map brain activities to text embeddings, the better the text reconstruction results. Such insight can simplify the task of reconstructing language from brain activities for future work, emphasizing the importance of improving brain-to-text-embedding mapping techniques.",
        "author": "Xinpei Zhao; Jingyuan Sun; Shaonan Wang; Jing Ye; Xiaohan Zhang; Chengqing Zong",
        "authorids": "/x/xinpei-zhao/; /j/jingyuan-sun/; /s/shaonan-wang/; /j/jing-ye/; /x/xiaohan-zhang/; /c/chengqing-zong/",
        "bibtex": "@inproceedings{zhao-etal-2024-mapguide,\n    title = \"{M}ap{G}uide: A Simple yet Effective Method to Reconstruct Continuous Language from Brain Activities\",\n    author = \"Zhao, Xinpei  and\n      Sun, Jingyuan  and\n      Wang, Shaonan  and\n      Ye, Jing  and\n      Zhang, Xiaohan  and\n      Zong, Chengqing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.211/\",\n    doi = \"10.18653/v1/2024.naacl-long.211\",\n    pages = \"3822--3832\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.211.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.211/",
        "pdf_size": 864231,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12510174321995568520&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.306",
        "title": "Mapping Long-term Causalities in Psychiatric Symptomatology and Life Events from Social Media",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Social media is a valuable data source for exploring mental health issues. However, previous studies have predominantly focused on the semantic content of these posts, overlooking the importance of their temporal attributes, as well as the evolving nature of mental disorders and symptoms.In this paper, we study the causality between psychiatric symptoms and life events, as well as among different symptoms from social media posts, which leads to better understanding of the underlying mechanisms of mental disorders. By applying these extracted causality features to tasks such as diagnosis point detection and early risk detection of depression, we notice considerable performance enhancement. This indicates that causality information extracted from social media data can boost the efficacy of mental disorder diagnosis and treatment planning.",
        "author": "Siyuan Chen; Meilin Wang; Minghao Lv; Zhiling Zhang; Juqianqian Juqianqian; Dejiyangla Dejiyangla; Yujia Peng; Kenny Zhu; Mengyue Wu",
        "authorids": "/s/siyuan-chen/; /m/meilin-wang/; /m/minghao-lv/; /z/zhiling-zhang/; /j/juqianqian-juqianqian/; /d/dejiyangla-dejiyangla/; /y/yujia-peng/; /k/kenny-zhu/; /m/mengyue-wu/",
        "bibtex": "@inproceedings{chen-etal-2024-mapping,\n    title = \"Mapping Long-term Causalities in Psychiatric Symptomatology and Life Events from Social Media\",\n    author = \"Chen, Siyuan  and\n      Wang, Meilin  and\n      Lv, Minghao  and\n      Zhang, Zhiling  and\n      Juqianqian, Juqianqian  and\n      Dejiyangla, Dejiyangla  and\n      Peng, Yujia  and\n      Zhu, Kenny  and\n      Wu, Mengyue\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.306/\",\n    doi = \"10.18653/v1/2024.naacl-long.306\",\n    pages = \"5472--5487\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.306.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.306/",
        "pdf_size": 1149304,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:zMY08yULur4J:scholar.google.com/&scioq=Mapping+Long-term+Causalities+in+Psychiatric+Symptomatology+and+Life+Events+from+Social+Media&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "X-LANCE, MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China; X-LANCE, MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China; X-LANCE, MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China; Independent researcher; Beijing Key Laboratory of Behavior and Mental Health, School of Psychological and Cognitive Sciences, Peking University, China; Beijing Key Laboratory of Behavior and Mental Health, School of Psychological and Cognitive Sciences, Peking University, China; Beijing Key Laboratory of Behavior and Mental Health, School of Psychological and Cognitive Sciences, Peking University, China+National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence, China; University of Texas at Arlington, USA; X-LANCE, MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China",
        "aff_domain": "; ; ; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;1;2;2;2+3;4;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;Independent Researcher;Peking University;Beijing Institute for General Artificial Intelligence;University of Texas at Arlington",
        "aff_unique_dep": "AI Institute;;School of Psychological and Cognitive Sciences;National Key Laboratory of General Artificial Intelligence;",
        "aff_unique_url": "https://www.sjtu.edu.cn;;http://www.pku.edu.cn;http://www.bigaiai.cn;https://www.uta.edu",
        "aff_unique_abbr": "SJTU;;Peking U;BIGAI;UTA",
        "aff_campus_unique_index": "1;1;1;2",
        "aff_campus_unique": ";Beijing;Arlington",
        "aff_country_unique_index": "0;0;0;0;0;0+0;2;0",
        "aff_country_unique": "China;;United States"
    },
    {
        "id": "2024.naacl-long.344",
        "title": "Massive End-to-end Speech Recognition Models with Time Reduction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We investigate massive end-to-end automatic speech recognition (ASR) models with efficiency improvements achieved by time reduction. The encoders of our models use the neural architecture of Google\u2019s universal speech model (USM), with additional funnel pooling layers to significantly reduce the frame rate and speed up training and inference. We also explore a few practical methods to mitigate potential accuracy loss due to time reduction, while enjoying most efficiency gain. Our methods are demonstrated to work with both Connectionist Temporal Classification (CTC) and RNN-Transducer (RNN-T), with up to 2B model parameters, and over two domains. For a large-scale voice search recognition task, we perform extensive studies on vocabulary size, time reduction strategy, and its generalization performance on long-form test sets, and show that a 900M RNN-T is very tolerant to severe time reduction, with as low encoder output frame rate as 640ms. We also provide ablation studies on the Librispeech benchmark for important training hyperparameters and architecture designs, in training 600M RNN-T models at the frame rate of 160ms.",
        "author": "Weiran Wang; Rohit Prabhavalkar; Haozhe Shan; Zhong Meng; Dongseong Hwang; Qiujia Li; Khe Chai Sim; Bo Li; James Qin; Xingyu Cai; Adam Stooke; Chengjian Zheng; Yanzhang He; Tara Sainath; Pedro Moreno Mengibar",
        "authorids": "/w/weiran-wang/; /r/rohit-prabhavalkar/; /h/haozhe-shan/; /z/zhong-meng/; /d/dongseong-hwang/; /q/qiujia-li/; /k/khe-chai-sim/; /b/bo-li/; /j/james-qin/; /x/xingyu-cai/; /a/adam-stooke/; /c/chengjian-zheng/; /y/yanzhang-he/; /t/tara-sainath/; /p/pedro-moreno-mengibar/",
        "bibtex": "@inproceedings{wang-etal-2024-massive,\n    title = \"Massive End-to-end Speech Recognition Models with Time Reduction\",\n    author = \"Wang, Weiran  and\n      Prabhavalkar, Rohit  and\n      Shan, Haozhe  and\n      Meng, Zhong  and\n      Hwang, Dongseong  and\n      Li, Qiujia  and\n      Sim, Khe Chai  and\n      Li, Bo  and\n      Qin, James  and\n      Cai, Xingyu  and\n      Stooke, Adam  and\n      Zheng, Chengjian  and\n      He, Yanzhang  and\n      Sainath, Tara  and\n      Moreno Mengibar, Pedro\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.344/\",\n    doi = \"10.18653/v1/2024.naacl-long.344\",\n    pages = \"6206--6217\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.344.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.344/",
        "pdf_size": 234839,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4373890161162015014&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Google; Google; Google; Google; Google; Google; Google; Google; Google; Google; Google; Google; Google; Google; Google",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 15,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.81",
        "title": "Matching Varying-Length Texts via Topic-Informed and Decoupled Sentence Embeddings",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Measuring semantic similarity between texts is a crucial task in natural language processing. While existing semantic text matching focuses on pairs of similar-length sequences, matching texts with non-comparable lengths has broader applications in specific domains, such as comparing professional document summaries and content. Current approaches struggle with text pairs of non-comparable lengths due to truncation issues. To address this, we split texts into natural sentences and decouple sentence representations using supervised contrastive learning (SCL). Meanwhile, we adopt the embedded topic model (ETM) for specific domain data. Our experiments demonstrate the effectiveness of our model, based on decoupled and topic-informed sentence embeddings, in matching texts of significantly different lengths across three well-studied datasets.",
        "author": "Xixi Zhou; Chunbin Gu; Xin Jie; Jiajun Bu; Haishuai Wang",
        "authorids": "/x/xixi-zhou/; /c/chunbin-gu/; /x/xin-jie/; /j/jiajun-bu/; /h/haishuai-wang/",
        "bibtex": "@inproceedings{zhou-etal-2024-matching,\n    title = \"Matching Varying-Length Texts via Topic-Informed and Decoupled Sentence Embeddings\",\n    author = \"Zhou, Xixi  and\n      Gu, Chunbin  and\n      Jie, Xin  and\n      Bu, Jiajun  and\n      Wang, Haishuai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.81/\",\n    doi = \"10.18653/v1/2024.findings-naacl.81\",\n    pages = \"1274--1280\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.81.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.81/",
        "pdf_size": 364146,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:P9wtN0jhYwgJ:scholar.google.com/&scioq=Matching+Varying-Length+Texts+via+Topic-Informed+and+Decoupled+Sentence+Embeddings&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University; Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University; Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University; Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University; Zhejiang Provincial Key Laboratory of Service Robot, College of Computer Science, Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "College of Computer Science",
        "aff_unique_url": "http://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.418",
        "title": "Measuring Cross-lingual Transfer in Bytes",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multilingual pretraining has been a successful solution to the challenges posed by the lack of resources for languages. These models can transfer knowledge to target languages with minimal or no examples. Recent research suggests that monolingual models also have a similar capability, but the mechanisms behind this transfer remain unclear. Some studies have explored factors like language contamination and syntactic similarity. An emerging line of research suggests that the representations learned by language models contain two components: a language-specific and a language-agnostic component. The latter is responsible for transferring a more universal knowledge. However, there is a lack of comprehensive exploration of these properties across diverse target languages. To investigate this hypothesis, we conducted an experiment inspired by the work on the Scaling Laws for Transfer. We measured the amount of data transferred from a source language to a target language and found that models initialized from diverse languages perform similarly to a target language in a cross-lingual setting. This was surprising because the amount of data transferred to 10 diverse target languages, such as Spanish, Korean, and Finnish, was quite similar. We also found evidence that this transfer is not related to language contamination or language proximity, which strengthens the hypothesis that the model also relies on language-agnostic knowledge. Our experiments have opened up new possibilities for measuring how much data represents the language-agnostic representations learned during pretraining.",
        "author": "Leandro De Souza; Thales Almeida; Roberto Lotufo; Rodrigo Frassetto Nogueira",
        "authorids": "/l/leandro-de-souza/; /t/thales-almeida/; /r/roberto-lotufo/; /r/rodrigo-frassetto-nogueira/",
        "bibtex": "@inproceedings{de-souza-etal-2024-measuring,\n    title = \"Measuring Cross-lingual Transfer in Bytes\",\n    author = \"De Souza, Leandro  and\n      Almeida, Thales  and\n      Lotufo, Roberto  and\n      Frassetto Nogueira, Rodrigo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.418/\",\n    doi = \"10.18653/v1/2024.naacl-long.418\",\n    pages = \"7526--7537\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.418.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.418/",
        "pdf_size": 304107,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15697935272032065603&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "FEEC, UNICAMP, Brazil; IC, UNICAMP, Brazil+Maritaca AI, Brazil; FEEC, UNICAMP, Brazil+NeuralMind, Brazil; FEEC, UNICAMP, Brazil+Maritaca AI, Brazil",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "https://github.com/lersouza/language-transfer",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1+2;0+3;0+2",
        "aff_unique_norm": "Universidade de Campinas;UNICAMP;Maritaca AI;NeuralMind",
        "aff_unique_dep": "Faculdade de Engenharia El\u00e9trica e de Computa\u00e7\u00e3o;Institute of Computing;;",
        "aff_unique_url": "https://www.unicamp.br;https://www.unicamp.br;;",
        "aff_unique_abbr": "UNICAMP;UNICAMP;;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Campinas;",
        "aff_country_unique_index": "0;0+0;0+0;0+0",
        "aff_country_unique": "Brazil"
    },
    {
        "id": "2024.naacl-long.158",
        "title": "Measuring Entrainment in Spontaneous Code-switched Speech",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "It is well-known that speakers who entrain to one another have more successful conversations than those who do not. Previous research has shown that interlocutors entrain on linguistic features in both written and spoken monolingual domains. More recent work on code-switched communication has also shown preliminary evidence of entrainment on certain aspects of code-switching (CSW). However, such studies of entrainment in code-switched domains have been extremely few and restricted to human-machine textual interactions. Our work studies code-switched spontaneous speech between humans, finding that (1) patterns of written and spoken entrainment in monolingual settings largely generalize to code-switched settings, and (2) some patterns of entrainment on code-switching in dialogue agent-generated text generalize to spontaneous code-switched speech. Our findings give rise to important implications for the potentially \u201cuniversal\u201d nature of entrainment as a communication phenomenon, and potential applications in inclusive and interactive speech technology.",
        "author": "Debasmita Bhattacharya; Siying Ding; Alayna Nguyen; Julia Hirschberg",
        "authorids": "/d/debasmita-bhattacharya/; /s/siying-ding/; /a/alayna-nguyen/; /j/julia-hirschberg/",
        "bibtex": "@inproceedings{bhattacharya-etal-2024-measuring,\n    title = \"Measuring Entrainment in Spontaneous Code-switched Speech\",\n    author = \"Bhattacharya, Debasmita  and\n      Ding, Siying  and\n      Nguyen, Alayna  and\n      Hirschberg, Julia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.158/\",\n    doi = \"10.18653/v1/2024.naacl-long.158\",\n    pages = \"2865--2876\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.158.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.158/",
        "pdf_size": 451539,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:YnmIxpWdJSYJ:scholar.google.com/&scioq=Measuring+Entrainment+in+Spontaneous+Code-switched+Speech&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Columbia University; Department of Computer Science, Columbia University; Department of Computer Science, Columbia University; Department of Computer Science, Columbia University",
        "aff_domain": "cs.columbia.edu;barnard.edu;columbia.edu;cs.columbia.edu",
        "email": "cs.columbia.edu;barnard.edu;columbia.edu;cs.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.43",
        "title": "Measuring Social Norms of Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models\u2019 ability to understand social norms. This method further improves large language models to be on par with humans. Given the increasing adoption of large language models in real-world applications, our finding is particularly important and presents a unique direction for future improvements.",
        "author": "Ye Yuan; Kexin Tang; Jianhao Shen; Ming Zhang; Chenguang Wang",
        "authorids": "/y/ye-yuan/; /k/kexin-tang/; /j/jianhao-shen/; /m/ming-zhang/; /c/chenguang-wang/",
        "bibtex": "@inproceedings{yuan-etal-2024-measuring,\n    title = \"Measuring Social Norms of Large Language Models\",\n    author = \"Yuan, Ye  and\n      Tang, Kexin  and\n      Shen, Jianhao  and\n      Zhang, Ming  and\n      Wang, Chenguang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.43/\",\n    doi = \"10.18653/v1/2024.findings-naacl.43\",\n    pages = \"650--699\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.43.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.43/",
        "pdf_size": 1385913,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16279289269302981252&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Peking University + National Key Laboratory for Multimedia Information Processing, Peking University + Peking University-Anker Embodied AI Lab; School of Computer Science, Peking University + National Key Laboratory for Multimedia Information Processing, Peking University; School of Computer Science, Peking University + National Key Laboratory for Multimedia Information Processing, Peking University; School of Computer Science, Peking University + National Key Laboratory for Multimedia Information Processing, Peking University + Peking University-Anker Embodied AI Lab; Washington University in St. Louis",
        "aff_domain": "pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;pku.edu.cn;wustl.edu",
        "email": "pku.edu.cn;stu.pku.edu.cn;pku.edu.cn;pku.edu.cn;wustl.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0+0;0+0;0+0;0+0+0;1",
        "aff_unique_norm": "Peking University;Washington University in St. Louis",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "http://www.pku.edu.cn;https://wustl.edu",
        "aff_unique_abbr": "PKU;WashU",
        "aff_campus_unique_index": "0;0;0;0;2",
        "aff_campus_unique": "Beijing;;St. Louis",
        "aff_country_unique_index": "0+0+0;0+0;0+0;0+0+0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.11",
        "title": "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing an LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.",
        "author": "Yangyi Chen; Karan Sikka; Michael Cogswell; Heng Ji; Ajay Divakaran",
        "authorids": "/y/yangyi-chen/; /k/karan-sikka/; /m/michael-cogswell/; /h/heng-ji/; /a/ajay-divakaran/",
        "bibtex": "@inproceedings{chen-etal-2024-measuring,\n    title = \"Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models\",\n    author = \"Chen, Yangyi  and\n      Sikka, Karan  and\n      Cogswell, Michael  and\n      Ji, Heng  and\n      Divakaran, Ajay\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.11/\",\n    doi = \"10.18653/v1/2024.naacl-long.11\",\n    pages = \"192--210\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.11.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.11/",
        "pdf_size": 26226398,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=867873108849690203&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "SRI International+University of Illinois Urbana-Champaign; SRI International; SRI International; University of Illinois Urbana-Champaign; SRI International",
        "aff_domain": "illinois.edu; ; ; ; ",
        "email": "illinois.edu; ; ; ; ",
        "github": "https://github.com/Yangyi-Chen/CoTConsistency",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;0;1;0",
        "aff_unique_norm": "SRI International;University of Illinois Urbana-Champaign",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sri.com;https://illinois.edu",
        "aff_unique_abbr": "SRI;UIUC",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.125",
        "title": "MedCycle: Unpaired Medical Report Generation via Cycle-Consistency",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Generating medical reports for X-ray images presents a significant challenge, particularly in unpaired scenarios where access to paired image-report data for training is unavailable. Previous works have typically learned a joint embedding space for images and reports, necessitating a specific labeling schema for both. We introduce an innovative approach that eliminates the need for consistent labeling schemas, thereby enhancing data accessibility and enabling the use of incompatible datasets. This approach is based on cycle-consistent mapping functions that transform image embeddings into report embeddings, coupled with report auto encoding for medical report generation. Our model and objectives consider intricate local details and the overarching semantic context within images and reports. This approach facilitates the learning of effective mapping functions, resulting in the generation of coherent reports. It outperforms state-of-the-art results in unpaired chest X-ray report generation, demonstrating improvements in both language and clinical metrics.",
        "author": "Elad Hirsch; Gefen Dawidowicz; Ayellet Tal",
        "authorids": "/e/elad-hirsch/; /g/gefen-dawidowicz/; /a/ayellet-tal/",
        "bibtex": "@inproceedings{hirsch-etal-2024-medcycle,\n    title = \"{M}ed{C}ycle: Unpaired Medical Report Generation via Cycle-Consistency\",\n    author = \"Hirsch, Elad  and\n      Dawidowicz, Gefen  and\n      Tal, Ayellet\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.125/\",\n    doi = \"10.18653/v1/2024.findings-naacl.125\",\n    pages = \"1929--1944\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.125.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.125/",
        "pdf_size": 1948537,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6351507853065691179&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Technion \u2013 Israel Institute of Technology; Technion \u2013 Israel Institute of Technology; Technion \u2013 Israel Institute of Technology",
        "aff_domain": "campus.technion.ac.il;campus.technion.ac.il;ee.technion.ac.il",
        "email": "campus.technion.ac.il;campus.technion.ac.il;ee.technion.ac.il",
        "github": "https://github.com/eladhi/MedCycle",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion \u2013 Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2024.naacl-long.227",
        "title": "Media Bias Detection Across Families of Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Bias in reporting can influence the public\u2019s opinion on relevant societal issues. Examples include informational bias (selective presentation of content) and lexical bias (specific framing of content through linguistic choices). The recognition of media bias is arguably an area where NLP can contribute to the \u201csocial good\u201d. Traditional NLP models have shown good performance in classifying media bias, but require careful model design and extensive tuning. In this paper, we ask how well prompting of large language models can recognize media bias. Through an extensive empirical study including a wide selection of pre-trained models, we find that prompt-based techniques can deliver comparable performance to traditional models with greatly reduced effort and that, similar to traditional models, the availability of context substantially improves results. We further show that larger models can leverage different kinds of context simultaneously, obtaining further performance improvements.",
        "author": "Iffat Maab; Edison Marrese-Taylor; Sebastian Pad\u00f3; Yutaka Matsuo",
        "authorids": "/i/iffat-maab/; /e/edison-marrese-taylor/; /s/sebastian-pado/; /y/yutaka-matsuo/",
        "bibtex": "@inproceedings{maab-etal-2024-media,\n    title = \"Media Bias Detection Across Families of Language Models\",\n    author = \"Maab, Iffat  and\n      Marrese-Taylor, Edison  and\n      Pad{\\'o}, Sebastian  and\n      Matsuo, Yutaka\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.227/\",\n    doi = \"10.18653/v1/2024.naacl-long.227\",\n    pages = \"4083--4098\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.227.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.227/",
        "pdf_size": 1122896,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2399381995275357348&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "The University of Tokyo; The University of Tokyo+National Institute of Advanced Industrial Science and Technology; University of Stuttgart; The University of Tokyo",
        "aff_domain": "weblab.t.u-tokyo.ac.jp;weblab.t.u-tokyo.ac.jp;ims.uni-stuttgart.de;weblab.t.u-tokyo.ac.jp",
        "email": "weblab.t.u-tokyo.ac.jp;weblab.t.u-tokyo.ac.jp;ims.uni-stuttgart.de;weblab.t.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0+1;2;0",
        "aff_unique_norm": "University of Tokyo;National Institute of Advanced Industrial Science and Technology;University of Stuttgart",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.u-tokyo.ac.jp;https://www.aist.go.jp;https://www.uni-stuttgart.de",
        "aff_unique_abbr": "UTokyo;AIST;USTuttgart",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;1;0",
        "aff_country_unique": "Japan;Germany"
    },
    {
        "id": "2024.naacl-long.249",
        "title": "Memory Augmented Language Models through Mixture of Word Experts",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to aggressively decouple learning capacity and FLOPs through Mixture-of-Experts (MoE) style models with large knowledge-rich vocabulary based routing functions. Our proposed approach, dubbed Mixture of Word Experts (MoWE), can be seen as a memory augmented model, where a large set of word-specific experts play the role of a sparse memory. We demonstrate that MoWE performs significantly better than the T5 family of models with similar number of FLOPs in a variety of NLP tasks. Moreover, MoWE outperforms traditional MoE models on knowledge intensive tasks and has similar performance to complex memory augmented approaches that often require to invoke custom mechanisms to search the sparse memory.",
        "author": "Cicero Nogueira dos Santos; James Lee-Thorp; Isaac Noble; Chung-Ching Chang; David Uthus",
        "authorids": "/c/cicero-dos-santos/; /j/james-lee-thorp/; /i/isaac-noble/; /c/chung-ching-chang/; /d/david-c-uthus/",
        "bibtex": "@inproceedings{nogueira-dos-santos-etal-2024-memory,\n    title = \"Memory Augmented Language Models through Mixture of Word Experts\",\n    author = \"Nogueira dos Santos, Cicero  and\n      Lee-Thorp, James  and\n      Noble, Isaac  and\n      Chang, Chung-Ching  and\n      Uthus, David\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.249/\",\n    doi = \"10.18653/v1/2024.naacl-long.249\",\n    pages = \"4425--4438\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.249.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.249/",
        "pdf_size": 310094,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7671936762506499314&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Google Research; Google Research; Google Research; Google Research; Google Research",
        "aff_domain": "google.com;google.com;google.com;google.com;google.com",
        "email": "google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google Research",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.106",
        "title": "Metacognitive Prompting Improves Understanding in Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. Recent advancements in prompting have enhanced reasoning in logic-intensive tasks for LLMs, yet the nuanced understanding abilities of these models, crucial for processing and interpreting complex information, remain underexplored. In this study, we introduce Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. We conduct extensive experiments on four prevalent LLMs: Llama2, PaLM2, GPT-3.5, and GPT-4, across ten natural language understanding (NLU) datasets from GLUE, SuperGLUE, BLUE, and LexGLUE benchmarks. Additionally, we compare our method with chain-of-thought prompting and its advanced versions. The results show that GPT-4 consistently excels across all tasks, while other models have shown significant progress in some tasks when used in conjunction with MP. Furthermore, MP consistently outperforms existing prompting methods in both general and domain-specific NLU tasks. This study underscores the potential to amplify the understanding abilities of LLMs and highlights the benefits of mirroring human introspective reasoning in NLU tasks.",
        "author": "Yuqing Wang; Yun Zhao",
        "authorids": "/y/yuqing-wang/; /y/yun-zhao/",
        "bibtex": "@inproceedings{wang-zhao-2024-metacognitive,\n    title = \"Metacognitive Prompting Improves Understanding in Large Language Models\",\n    author = \"Wang, Yuqing  and\n      Zhao, Yun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.106/\",\n    doi = \"10.18653/v1/2024.naacl-long.106\",\n    pages = \"1914--1926\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.106.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.106/",
        "pdf_size": 440553,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14023465368403403750&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Stanford University; Meta Platforms, Inc.",
        "aff_domain": "stanford.edu;meta.com",
        "email": "stanford.edu;meta.com",
        "github": "https://github.com/EternityYW/Metacognitive-Prompting",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Stanford University;Meta",
        "aff_unique_dep": ";Meta Platforms, Inc.",
        "aff_unique_url": "https://www.stanford.edu;https://www.meta.com",
        "aff_unique_abbr": "Stanford;Meta",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.149",
        "title": "Metaphor Detection with Context Enhancement and Curriculum Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Metaphor detection is a challenging task for natural language processing (NLP) systems. Previous works failed to sufficiently utilize the internal and external semantic relationships between target words and their context. Furthermore, they have faced challenges in tackling the problem of data sparseness due to the very limited available training data. To address these two challenges, we propose a novel model called MiceCL. By leveraging the difference between the literal meaning of the target word and the meaning of the sentence as the sentence external difference, MiceCL can better handle the semantic relationships. Additionally, we propose a curriculum learning framework for automatically assessing difficulty of the sentence with a pre-trained model. By starting from easy examples and gradually progressing to more difficult ones, we can ensure that the model will not deal with complex data when its ability is weak so that to avoid wasting limited data. Experimental results demonstrate that MiceCL achieves competitive performance across multiple datasets, with a significantly improved convergence speed compared to other models.",
        "author": "Kaidi Jia; Rongsheng Li",
        "authorids": "/k/kaidi-jia/; /r/rongsheng-li/",
        "bibtex": "@inproceedings{jia-li-2024-metaphor,\n    title = \"Metaphor Detection with Context Enhancement and Curriculum Learning\",\n    author = \"Jia, Kaidi  and\n      Li, Rongsheng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.149/\",\n    doi = \"10.18653/v1/2024.naacl-long.149\",\n    pages = \"2726--2737\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.149.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.149/",
        "pdf_size": 257866,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1734723800782949189&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "College of Computer Science and Technology, Harbin Engineering University, Harbin 150001, China; College of Computer Science and Technology, Harbin Engineering University, Harbin 150001, China",
        "aff_domain": "hrbeu.edu.cn; ",
        "email": "hrbeu.edu.cn; ",
        "github": "https://github.com/Evilxya/MiceCL.git",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Harbin Engineering University",
        "aff_unique_dep": "College of Computer Science and Technology",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Harbin",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.123",
        "title": "Methods, Applications, and Directions of Learning-to-Rank in NLP Research",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Learning-to-rank (LTR) algorithms aim to order a set of items according to some criteria. They are at the core of applications such as web search and social media recommendations, and are an area of rapidly increasing interest, with the rise of large language models (LLMs) and the widespread impact of these technologies on society. In this paper, we survey the diverse use cases of LTR methods in natural language processing (NLP) research, looking at previously under-studied aspects such as multilingualism in LTR applications and statistical significance testing for LTR problems. We also consider how large language models are changing the LTR landscape. This survey is aimed at NLP researchers and practitioners interested in understanding the formalisms and best practices regarding the application of LTR approaches in their research.",
        "author": "Justin Lee; Gabriel Bernier-Colborne; Tegan Maharaj; Sowmya Vajjala",
        "authorids": "/j/justin-lee/; /g/gabriel-bernier-colborne/; /t/tegan-maharaj/; /s/sowmya-vajjala/",
        "bibtex": "@inproceedings{lee-etal-2024-methods,\n    title = \"Methods, Applications, and Directions of Learning-to-Rank in {NLP} Research\",\n    author = \"Lee, Justin  and\n      Bernier-Colborne, Gabriel  and\n      Maharaj, Tegan  and\n      Vajjala, Sowmya\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.123/\",\n    doi = \"10.18653/v1/2024.findings-naacl.123\",\n    pages = \"1900--1917\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.123.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.123/",
        "pdf_size": 250385,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2682653954930831387&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "University of Toronto, Canada; National Research Council, Canada; University of Toronto, Canada; National Research Council, Canada",
        "aff_domain": "mail.utoronto.ca;nrc-cnrc.gc.ca;utoronto.ca;nrc-cnrc.gc.ca",
        "email": "mail.utoronto.ca;nrc-cnrc.gc.ca;utoronto.ca;nrc-cnrc.gc.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "University of Toronto;National Research Council",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utoronto.ca;https://www.nrc-cnrc.gc.ca",
        "aff_unique_abbr": "U of T;NRC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.findings-naacl.18",
        "title": "MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Generative language models are usually pre-trained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose a **MiLe Loss** function for **mi**tigating the bias of **le**arning difficulties with tokens. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn tokens. On the Pile dataset, we train generative language models at different scales of 468M, 1.2B, and 6.7B parameters. Experiments reveal that models incorporating the proposed MiLe Loss can gain consistent performance improvement on downstream benchmarks.",
        "author": "Zhenpeng Su; Zijia Lin; Baixue Baixue; Hui Chen; Songlin Hu; Wei Zhou; Guiguang Ding; Xing W",
        "authorids": "/z/zhenpeng-su/; /z/zijia-lin/; /b/baixue-baixue/; /h/hui-chen/; /s/songlin-hu/; /w/wei-zhou/; /g/guiguang-ding/; /x/xing-w/",
        "bibtex": "@inproceedings{su-etal-2024-mile,\n    title = \"{M}i{L}e Loss: a New Loss for Mitigating the Bias of Learning Difficulties in Generative Language Models\",\n    author = \"Su, Zhenpeng  and\n      Lin, Zijia  and\n      Baixue, Baixue  and\n      Chen, Hui  and\n      Hu, Songlin  and\n      Zhou, Wei  and\n      Ding, Guiguang  and\n      W, Xing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.18/\",\n    doi = \"10.18653/v1/2024.findings-naacl.18\",\n    pages = \"250--262\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.18.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.18/",
        "pdf_size": 392807,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8192642603385838523&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2024.findings-naacl.200",
        "title": "MindAgent: Emergent Gaming Interaction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Foundation Models (LFMs) can perform complex scheduling in a multi-agent system and can coordinate agents to complete sophisticated tasks that require extensive collaboration.However, despite the introduction of numerous gaming frameworks, the community lacks adequate benchmarks that support the implementation of a general multi-agent infrastructure encompassing collaboration between LFMs and human-NPCs. We propose a novel infrastructure\u2014Mindagent\u2014for evaluating planning and coordination capabilities in the context of gaming interaction. In particular, our infrastructure leverages an existing gaming framework to (i) act as the coordinator for a multi-agent system, (ii) collaborate with human players via instructions, and (iii) enable in-context learning based on few-shot prompting with feedback.Furthermore, we introduce \u201cCuisineworld\u201d, a new gaming scenario and its related benchmark that supervises multiple agents playing the game simultaneously and measures multi-agent collaboration efficiency. We have conducted comprehensive evaluations with a new auto-metric Collaboration Score: CoS for assessing the collaboration efficiency. Finally, Mindagent can be deployed in real-world gaming scenarios in a customized VR version of Cuisineworld and adapted in the \u201cMinecraft\u201d domain. Our work involving LFMs within our new infrastructure for general-purpose scheduling and coordination can elucidate how such skills may be obtained by learning from large language corpora.",
        "author": "Ran Gong; Qiuyuan Huang; Xiaojian Ma; Yusuke Noda; Zane Durante; Zilong Zheng; Demetri Terzopoulos; Li Fei-Fei; Jianfeng Gao; Hoi Vo",
        "authorids": "/r/ran-gong/; /q/qiuyuan-huang/; /x/xiaojian-ma/; /y/yusuke-noda/; /z/zane-durante/; /z/zilong-zheng/; /d/demetri-terzopoulos/; /l/li-fei-fei/; /j/jianfeng-gao/; /h/hoi-vo/",
        "bibtex": "@inproceedings{gong-etal-2024-mindagent,\n    title = \"{M}ind{A}gent: Emergent Gaming Interaction\",\n    author = \"Gong, Ran  and\n      Huang, Qiuyuan  and\n      Ma, Xiaojian  and\n      Noda, Yusuke  and\n      Durante, Zane  and\n      Zheng, Zilong  and\n      Terzopoulos, Demetri  and\n      Fei-Fei, Li  and\n      Gao, Jianfeng  and\n      Vo, Hoi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.200/\",\n    doi = \"10.18653/v1/2024.findings-naacl.200\",\n    pages = \"3154--3183\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.200.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.200/",
        "pdf_size": 24818667,
        "gs_citation": 100,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16342531411881217960&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, Los Angeles (UCLA); Microsoft Research, Redmond; Microsoft Gaming; Stanford University; Beijing Institute for General AI (BIGAI); University of California, Los Angeles (UCLA); University of California, Los Angeles (UCLA); Stanford University; Microsoft Research, Redmond; Microsoft Gaming",
        "aff_domain": "ucla.edu;microsoft.com;ucla.edu;microsoft.com;stanford.edu;bigai.com;ucla.edu;stanford.edu;microsoft.com;microsoft.com",
        "email": "ucla.edu;microsoft.com;ucla.edu;microsoft.com;stanford.edu;bigai.com;ucla.edu;stanford.edu;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;1;2;3;0;0;2;1;1",
        "aff_unique_norm": "University of California, Los Angeles;Microsoft;Stanford University;Beijing Institute for General AI",
        "aff_unique_dep": ";Microsoft Research;;",
        "aff_unique_url": "https://www.ucla.edu;https://www.microsoft.com/en-us/research;https://www.stanford.edu;http://www.bigmodel.cn/",
        "aff_unique_abbr": "UCLA;MSR;Stanford;BIGAI",
        "aff_campus_unique_index": "0;1;3;0;0;3;1",
        "aff_campus_unique": "Los Angeles;Redmond;;Stanford",
        "aff_country_unique_index": "0;0;0;0;1;0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.376",
        "title": "Mind\u2019s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) have achieved remarkable advancements in natural language processing. However, the massive scale and computational demands of these models present formidable challenges when considering their practical deployment in resource-constrained environments. While techniques such as chain-of-thought (CoT) distillation have displayed promise in distilling LLMs into small language models (SLMs), there is a risk that distilled SLMs may still inherit flawed reasoning and hallucinations from LLMs. To address these issues, we propose a twofold methodology: First, we introduce a novel method for distilling the self-evaluation capability from LLMs into SLMs, aiming to mitigate the adverse effects of flawed reasoning and hallucinations inherited from LLMs. Second, we advocate for distilling more comprehensive thinking by incorporating multiple distinct CoTs and self-evaluation outputs, to ensure a more thorough and robust knowledge transfer into SLMs. Experiments on three NLP benchmarks demonstrate that our method significantly improves the performance of distilled SLMs, offering a new perspective for developing more effective and efficient SLMs in resource-constrained environments.",
        "author": "Weize Liu; Guocong Li; Kai Zhang; Bang Du; Qiyuan Chen; Xuming Hu; Hongxia Xu; Jintai Chen; Jian Wu",
        "authorids": "/w/weize-liu/; /g/guocong-li/; /k/kai-zhang/; /b/bang-du/; /q/qiyuan-chen/; /x/xuming-hu/; /h/hongxia-xu/; /j/jintai-chen/; /j/jian-wu/",
        "bibtex": "@inproceedings{liu-etal-2024-minds,\n    title = \"Mind{'}s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models\",\n    author = \"Liu, Weize  and\n      Li, Guocong  and\n      Zhang, Kai  and\n      Du, Bang  and\n      Chen, Qiyuan  and\n      Hu, Xuming  and\n      Xu, Hongxia  and\n      Chen, Jintai  and\n      Wu, Jian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.376/\",\n    doi = \"10.18653/v1/2024.naacl-long.376\",\n    pages = \"6748--6763\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.376.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.376/",
        "pdf_size": 525167,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7212172004931163477&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;;;;",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 9
    },
    {
        "id": "2024.naacl-long.419",
        "title": "MisgenderMender: A Community-Informed Approach to Interventions for Misgendering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering.Misgendering, the act of incorrectly addressing someone\u2019s gender, inflicts serious harm and is pervasive in everyday technologies, yet there is a notable lack of research to combat it. We are the first to address this lack of research into interventions for misgendering by conducting a survey of gender-diverse individuals in the US to understand perspectives about automated interventions for text-based misgendering. Based on survey insights on the prevalence of misgendering, desired solutions, and associated concerns, we introduce a misgendering interventions task and evaluation dataset, MisgenderMender. We define the task with two sub-tasks: (i) detecting misgendering, followed by (ii) correcting misgendering where misgendering is present, in domains where editing is appropriate. MisgenderMender comprises 3790 instances of social media content and LLM-generations about non-cisgender public figures, annotated for the presence of misgendering, with additional annotations for correcting misgendering in LLM-generated text. Using this dataset, we set initial benchmarks by evaluating existing NLP systems and highlighting challenges for future models to address. We release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendermender/",
        "author": "Tamanna Hossain; Sunipa Dev; Sameer Singh",
        "authorids": "/t/tamanna-hossain/; /s/sunipa-dev/; /s/sameer-singh/",
        "bibtex": "@inproceedings{hossain-etal-2024-misgendermender,\n    title = \"{M}isgender{M}ender: A Community-Informed Approach to Interventions for Misgendering\",\n    author = \"Hossain, Tamanna  and\n      Dev, Sunipa  and\n      Singh, Sameer\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.419/\",\n    doi = \"10.18653/v1/2024.naacl-long.419\",\n    pages = \"7538--7558\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.419.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.419/",
        "pdf_size": 446200,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6329312986811908570&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Irvine; Google Research; University of California, Irvine",
        "aff_domain": "uci.edu;google.com;uci.edu",
        "email": "uci.edu;google.com;uci.edu",
        "github": "https://tamannahossainkay.github.io/misgendermender/",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Irvine;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.uci.edu;https://research.google",
        "aff_unique_abbr": "UCI;Google Research",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Irvine;Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.257",
        "title": "Mitigating Bias for Question Answering Models by Tracking Bias Influence",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applied to multiple QA formulations across multiple bias categories. It can significantly reduce the bias level in all 9 bias categories in the BBQ dataset while maintaining comparable QA accuracy.",
        "author": "Mingyu Ma; Jiun-Yu Kao; Arpit Gupta; Yu-Hsiang Lin; Wenbo Zhao; Tagyoung Chung; Wei Wang; Kai-Wei Chang; Nanyun Peng",
        "authorids": "/m/mingyu-ma/; /j/jiun-yu-kao/; /a/arpit-gupta/; /y/yu-hsiang-lin/; /w/wenbo-zhao/; /t/tagyoung-chung/; /w/wei-wang/; /k/kai-wei-chang/; /n/nanyun-peng/",
        "bibtex": "@inproceedings{ma-etal-2024-mitigating,\n    title = \"Mitigating Bias for Question Answering Models by Tracking Bias Influence\",\n    author = \"Ma, Mingyu  and\n      Kao, Jiun-Yu  and\n      Gupta, Arpit  and\n      Lin, Yu-Hsiang  and\n      Zhao, Wenbo  and\n      Chung, Tagyoung  and\n      Wang, Wei  and\n      Chang, Kai-Wei  and\n      Peng, Nanyun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.257/\",\n    doi = \"10.18653/v1/2024.naacl-long.257\",\n    pages = \"4592--4610\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.257.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.257/",
        "pdf_size": 9922926,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1415337304176786906&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "University of California, Los Angeles; Amazon AGI; Amazon AGI; Amazon AGI; Amazon AGI; Amazon AGI; University of California, Los Angeles; University of California, Los Angeles + Amazon AGI; University of California, Los Angeles + Amazon AGI",
        "aff_domain": "cs.ucla.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "email": "cs.ucla.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;cs.ucla.edu;cs.ucla.edu;cs.ucla.edu",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;1;1;1;1;0;0+1;0+1",
        "aff_unique_norm": "University of California, Los Angeles;Amazon",
        "aff_unique_dep": ";Amazon AGI",
        "aff_unique_url": "https://www.ucla.edu;https://www.amazon.com",
        "aff_unique_abbr": "UCLA;Amazon",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0+0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.117",
        "title": "Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "A primary challenge in abstractive summarization is hallucination\u2014the phenomenon where a model generates plausible text that is absent in the source text. We hypothesize that the domain (or topic) of the source text triggers the model to generate text that is highly probable in the domain, neglecting the details of the source text. To alleviate this model bias, we introduce a decoding strategy based on domain-conditional pointwise mutual information. This strategy adjusts the generation probability of each token by comparing it with the token\u2019s marginal probability within the domain of the source text. According to evaluation on the XSUM dataset, our method demonstrates improvement in terms of faithfulness and source relevance.",
        "author": "Kyubyung Chae; Jaepill Choi; Yohan Jo; Taesup Kim",
        "authorids": "/k/kyubyung-chae/; /j/jaepill-choi/; /y/yohan-jo/; /t/taesup-kim/",
        "bibtex": "@inproceedings{chae-etal-2024-mitigating,\n    title = \"Mitigating Hallucination in Abstractive Summarization with Domain-Conditional Mutual Information\",\n    author = \"Chae, Kyubyung  and\n      Choi, Jaepill  and\n      Jo, Yohan  and\n      Kim, Taesup\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.117/\",\n    doi = \"10.18653/v1/2024.findings-naacl.117\",\n    pages = \"1809--1820\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.117.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.117/",
        "pdf_size": 448416,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10049537697251193046&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Graduate School of Data Science, Seoul National University; Graduate School of Data Science, Seoul National University; Graduate School of Data Science, Seoul National University; Graduate School of Data Science, Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "github": "https://github.com/qqplot/dcpmi",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Graduate School of Data Science",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-long.160",
        "title": "Mitigating Language-Level Performance Disparity in mPLMs via Teacher Language Selection and Cross-lingual Self-Distillation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large-scale multilingual Pretrained Language Models (mPLMs) yield impressive performance on cross-language tasks, yet significant performance disparities exist across different languages within the same mPLM. Previous studies endeavored to narrow these disparities by supervise fine-tuning the mPLMs with multilingual data.However, obtaining labeled multilingual data is time-consuming, and fine-tuning mPLM with limited labeled multilingual data merely encapsulates the knowledge specific to the labeled data.Therefore, we introduce **ALSACE** to leverage the learned knowledge from the well-performing languages to guide under-performing ones within the same mPLM, eliminating the need for additional labeled multilingual data. Experiments show that ALSACE effectively mitigates language-level performance disparity across various mPLMs while showing the competitive performance on different multilingual NLU tasks, ranging from full resource to limited resource settings. The code for our approach is available at https://github.com/pkunlp-icler/ALSACE.",
        "author": "Haozhe Zhao; Zefan Cai; Shuzheng Si; Liang Chen; Yufeng He; Kaikai An; Baobao Chang",
        "authorids": "/h/haozhe-zhao/; /z/zefan-cai/; /s/shuzheng-si/; /l/liang-chen/; /y/yufeng-he/; /k/kaikai-an/; /b/baobao-chang/",
        "bibtex": "@inproceedings{zhao-etal-2024-mitigating,\n    title = \"Mitigating Language-Level Performance Disparity in m{PLM}s via Teacher Language Selection and Cross-lingual Self-Distillation\",\n    author = \"Zhao, Haozhe  and\n      Cai, Zefan  and\n      Si, Shuzheng  and\n      Chen, Liang  and\n      He, Yufeng  and\n      An, Kaikai  and\n      Chang, Baobao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.160/\",\n    doi = \"10.18653/v1/2024.naacl-long.160\",\n    pages = \"2893--2907\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.160.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.160/",
        "pdf_size": 695173,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:k74odKfNFdcJ:scholar.google.com/&scioq=Mitigating+Language-Level+Performance+Disparity+in+mPLMs+via+Teacher+Language+Selection+and+Cross-lingual+Self-Distillation&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "National Key Laboratory for Multimedia Information Processing, Peking University + School of Software and Microelectronics, Peking University, China; National Key Laboratory for Multimedia Information Processing, Peking University + School of Software and Microelectronics, Peking University, China; National Key Laboratory for Multimedia Information Processing, Peking University + School of Software and Microelectronics, Peking University, China; National Key Laboratory for Multimedia Information Processing, Peking University; National Key Laboratory for Multimedia Information Processing, Peking University + School of Software and Microelectronics, Peking University, China; National Key Laboratory for Multimedia Information Processing, Peking University + School of Software and Microelectronics, Peking University, China; National Key Laboratory for Multimedia Information Processing, Peking University + Collaborative Innovation Center for Language Ability, Xuzhou, 221009, China",
        "aff_domain": "gmail.com;gmail.com;stu.pku.edu.cn;pku.edu.cn; ; ; ",
        "email": "gmail.com;gmail.com;stu.pku.edu.cn;pku.edu.cn; ; ; ",
        "github": "https://github.com/pkunlp-icler/ALSACE",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+0;0+0;0+0;0;0+0;0+0;0+1",
        "aff_unique_norm": "Peking University;Collaborative Innovation Center for Language Ability",
        "aff_unique_dep": "National Key Laboratory for Multimedia Information Processing;",
        "aff_unique_url": "http://www.pku.edu.cn;",
        "aff_unique_abbr": "PKU;",
        "aff_campus_unique_index": ";;;;;1",
        "aff_campus_unique": ";Xuzhou",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.485",
        "title": "Mix-Initiative Response Generation with Dynamic Prefix Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Mixed initiative serves as one of the key factors in controlling conversation directions. For a speaker, responding passively or leading proactively would result in rather different responses. However, most dialogue systems focus on training a holistic response generation model without any distinction among different initiatives. It leads to the cross-contamination problem, where the model confuses different initiatives and generates inappropriate responses. Moreover, obtaining plenty of human annotations for initiative labels can be expensive. To address this issue, we propose a general mix-Initiative Dynamic Prefix Tuning framework (IDPT) to decouple different initiatives from the generation model, which learns initiative-aware prefixes in both supervised and unsupervised settings. Specifically, IDPT decouples initiative factors into different prefix parameters and uses the attention mechanism to adjust the selection of initiatives in guiding generation dynamically. The prefix parameters can be tuned towards accurate initiative prediction as well as mix-initiative response generation. Extensive experiments on two public dialogue datasets show that the proposed IDPT outperforms previous baselines on both automatic metrics and human evaluations. It also manages to generate appropriate responses with manipulated initiatives.",
        "author": "Yuxiang Nie; Heyan Huang; Xian-Ling Mao; Lizi Liao",
        "authorids": "/y/yuxiang-nie/; /h/he-yan-huang/; /x/xian-ling-mao/; /l/lizi-liao/",
        "bibtex": "@inproceedings{nie-etal-2024-mix,\n    title = \"Mix-Initiative Response Generation with Dynamic Prefix Tuning\",\n    author = \"Nie, Yuxiang  and\n      Huang, Heyan  and\n      Mao, Xian-Ling  and\n      Liao, Lizi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.485/\",\n    doi = \"10.18653/v1/2024.naacl-long.485\",\n    pages = \"8748--8761\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.485.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.485/",
        "pdf_size": 533458,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8012260445794224572&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology; School of Computer Science and Technology, Beijing Institute of Technology; Singapore Management University",
        "aff_domain": "bit.edu.cn;bit.edu.cn;bit.edu.cn;smu.edu.sg",
        "email": "bit.edu.cn;bit.edu.cn;bit.edu.cn;smu.edu.sg",
        "github": "https://github.com/JerrryNie/Mix-Initiative-Dialogue",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Beijing Institute of Technology;Singapore Management University",
        "aff_unique_dep": "School of Computer Science and Technology;",
        "aff_unique_url": "http://www.bit.edu.cn/;https://www.smu.edu.sg",
        "aff_unique_abbr": "BIT;SMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2024.naacl-long.172",
        "title": "Modeling Empathetic Alignment in Conversation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Empathy requires perspective-taking: empathetic responses require a person to reason about what another has experienced and communicate that understanding in language. However, most NLP approaches to empathy do not explicitly model this alignment process. Here, we introduce a new approach to recognizing alignment in empathetic speech, grounded in Appraisal Theory. We introduce a new dataset of over 9.2K span-level annotations of different types of appraisals of a person\u2019s experience and over 3K empathetic alignments between a speaker\u2019s and observer\u2019s speech. Through computational experiments, we show that these appraisals and alignments can be accurately recognized. In experiments in over 9.2M Reddit conversations, we find that appraisals capture meaningful groupings of behavior but that most responses have minimal alignment. However, we find that mental health professionals engage with substantially more empathetic alignment.",
        "author": "Jiamin Yang; David Jurgens",
        "authorids": "/j/jiamin-yang/; /d/david-jurgens/",
        "bibtex": "@inproceedings{yang-jurgens-2024-modeling,\n    title = \"Modeling Empathetic Alignment in Conversation\",\n    author = \"Yang, Jiamin  and\n      Jurgens, David\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.172/\",\n    doi = \"10.18653/v1/2024.naacl-long.172\",\n    pages = \"3127--3148\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.172.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.172/",
        "pdf_size": 2983375,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12500544236802465519&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Chicago; University of Michigan",
        "aff_domain": "uchicago.edu;umich.edu",
        "email": "uchicago.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Chicago;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uchicago.edu;https://www.umich.edu",
        "aff_unique_abbr": "UChicago;UM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.6",
        "title": "Modeling and Detecting Company Risks from News",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Identifying risks associated with a company is important to investors and the wellbeing of the overall financial markets. In this study, we build a computational framework to automatically extract company risk factors from news articles. Our newly proposed schema comprises seven distinct aspects, such as supply chain, regulations, and competition. We annotate 666 news articles and benchmark various machine learning models. While large language mod- els have achieved remarkable progress in various types of NLP tasks, our experiment shows that zero-shot and few-shot prompting state-of- the-art LLMs (e.g., Llama-2) can only achieve moderate to low performances in identifying risk factors. In contrast, fine-tuning pre-trained language models yields better results on most risk factors. Using this model, we analyze over 277K Bloomberg News articles and demonstrate that identifying risk factors from news could provide extensive insights into the operations of companies and industries.",
        "author": "Jiaxin Pei; Soumya Vadlamannati; Liang-Kang Huang; Daniel Preotiuc-Pietro; Xinyu Hua",
        "authorids": "/j/jiaxin-pei/; /s/soumya-vadlamannati/; /l/liang-kang-huang/; /d/daniel-preotiuc-pietro/; /x/xinyu-hua/",
        "bibtex": "@inproceedings{pei-etal-2024-modeling,\n    title = \"Modeling and Detecting Company Risks from News\",\n    author = \"Pei, Jiaxin  and\n      Vadlamannati, Soumya  and\n      Huang, Liang-Kang  and\n      Preotiuc-Pietro, Daniel  and\n      Hua, Xinyu\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.6/\",\n    doi = \"10.18653/v1/2024.naacl-industry.6\",\n    pages = \"63--72\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.6.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.6/",
        "pdf_size": 500867,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5520887593094813515&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "University of Michigan, Ann Arbor, MI, USA+Bloomberg, New York, NY, USA; Bloomberg, New York, NY, USA; Bloomberg, New York, NY, USA; Bloomberg, New York, NY, USA; Bloomberg, New York, NY, USA",
        "aff_domain": "umich.edu;bloomberg.net;bloomberg.net;bloomberg.net;bloomberg.net",
        "email": "umich.edu;bloomberg.net;bloomberg.net;bloomberg.net;bloomberg.net",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;1",
        "aff_unique_norm": "University of Michigan;Bloomberg",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umich.edu;https://www.bloomberg.com",
        "aff_unique_abbr": "UM;Bloomberg",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Ann Arbor;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.65",
        "title": "Modeling the Sacred: Considerations when Using Religious Texts in Natural Language Processing",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This position paper concerns the use of religious texts in Natural Language Processing (NLP), which is of special interest to the Ethics of NLP. Religious texts are expressions of culturally important values, and machine learned models have a propensity to reproduce cultural values encoded in their training data. Furthermore, translations of religious texts are frequently used by NLP researchers when language data is scarce. This repurposes the translations from their original uses and motivations, which often involve attracting new followers. This paper argues that NLP\u2019s use of such texts raises considerations that go beyond model biases, including data provenance, cultural contexts, and their use in proselytism. We argue for more consideration of researcher positionality, and of the perspectives of marginalized linguistic and religious communities.",
        "author": "Ben Hutchinson",
        "authorids": "/b/ben-hutchinson/",
        "bibtex": "@inproceedings{hutchinson-2024-modeling,\n    title = \"Modeling the Sacred: Considerations when Using Religious Texts in Natural Language Processing\",\n    author = \"Hutchinson, Ben\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.65/\",\n    doi = \"10.18653/v1/2024.findings-naacl.65\",\n    pages = \"1029--1043\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.65.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.65/",
        "pdf_size": 193672,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12624963224969078142&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Google Research, Australia",
        "aff_domain": "google.com",
        "email": "google.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "aff_unique_index": "0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google Research",
        "aff_unique_url": "https://research.google",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Australia",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2024.naacl-long.328",
        "title": "Modularized Multilingual NMT with Fine-grained Interlingua",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently, one popular alternative in Multilingual NMT (MNMT) is modularized MNMT that has both language-specific encoders and decoders. However, due to the absence of layer-sharing, the modularized MNMT failed to produce satisfactory language-independent (Interlingua) features, leading to performance degradation in zero-shot translation. To address this issue, a solution was proposed to share the top of language-specific encoder layers, enabling the successful generation of interlingua features. Nonetheless, it should be noted that this sharing structure does not guarantee the explicit propagation of language-specific features to their respective language-specific decoders. Consequently, to overcome this challenge, we present our modularized MNMT approach, where a modularized encoder is divided into three distinct encoder modules based on different sharing criteria: (1) source language-specific (Encs); (2) universal (Encall); (3) target language-specific (Enct). By employing these sharing strategies, Encall propagates the interlingua features, after which Enct propagates the target language-specific features to the language-specific decoders. Additionally, we suggest the Denoising Bi-path Autoencoder (DBAE) to fortify the Denoising Autoencoder (DAE) by leveraging Enct. For experimental purposes, our training corpus comprises both En-to-Any and Any-to-En directions. We adjust the size of our corpus to simulate both balanced and unbalanced settings. Our method demonstrates an improved average BLEU score by \"+2.90\u201d in En-to-Any directions and by \"+3.06\u201d in zero-shot compared to other MNMT baselines.",
        "author": "Sungjun Lim; Yoonjung Choi; Sangha Kim",
        "authorids": "/s/sungjun-lim/; /y/yoonjung-choi/; /s/sangha-kim/",
        "bibtex": "@inproceedings{lim-etal-2024-modularized,\n    title = \"Modularized Multilingual {NMT} with Fine-grained Interlingua\",\n    author = \"Lim, Sungjun  and\n      Choi, Yoonjung  and\n      Kim, Sangha\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.328/\",\n    doi = \"10.18653/v1/2024.naacl-long.328\",\n    pages = \"5884--5899\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.328.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.328/",
        "pdf_size": 4642529,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:oAwSLYpaOn8J:scholar.google.com/&scioq=Modularized+Multilingual+NMT+with+Fine-grained+Interlingua&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Samsung Research, Seoul, Republic of Korea; Samsung Research, Seoul, Republic of Korea; Samsung Research, Seoul, Republic of Korea",
        "aff_domain": "samsung.com;samsung.com;samsung.com",
        "email": "samsung.com;samsung.com;samsung.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Samsung",
        "aff_unique_dep": "Samsung Research",
        "aff_unique_url": "https://www.samsung.com/global/research/",
        "aff_unique_abbr": "Samsung",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.findings-naacl.115",
        "title": "More Samples or More Prompts? Exploring Effective Few-Shot In-Context Learning for LLMs with In-Context Sampling",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While most existing works on LLM prompting techniques focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can not we design and leverage multiple prompts together to further improve the LLM\u2019s performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompting technique to produce confident predictions by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with three open-source LLMs (FlanT5-XL, Mistral-7B, and Mixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI) and one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhance LLMs\u2019 performance. An in-depth evaluation with three data similarity-based ICS strategies suggests that these strategies can further elevate LLM\u2019s performance, which sheds light on a new yet promising future research direction.",
        "author": "Bingsheng Yao; Guiming Chen; Ruishi Zou; Yuxuan Lu; Jiachen Li; Shao Zhang; Yisi Sang; Sijia Liu; James Hendler; Dakuo Wang",
        "authorids": "/b/bingsheng-yao/; /g/guiming-chen/; /r/ruishi-zou/; /y/yuxuan-lu/; /j/jiachen-li/; /s/shao-zhang/; /y/yisi-sang/; /s/sijia-liu/; /j/james-hendler/; /d/dakuo-wang/",
        "bibtex": "@inproceedings{yao-etal-2024-samples,\n    title = \"More Samples or More Prompts? Exploring Effective Few-Shot In-Context Learning for {LLM}s with In-Context Sampling\",\n    author = \"Yao, Bingsheng  and\n      Chen, Guiming  and\n      Zou, Ruishi  and\n      Lu, Yuxuan  and\n      Li, Jiachen  and\n      Zhang, Shao  and\n      Sang, Yisi  and\n      Liu, Sijia  and\n      Hendler, James  and\n      Wang, Dakuo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.115/\",\n    doi = \"10.18653/v1/2024.findings-naacl.115\",\n    pages = \"1772--1790\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.115.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.115/",
        "pdf_size": 2055220,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18267628062960512089&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Rensselaer Polytechnic Institute + Northeastern University; The Chinese University of Hong Kong, Shenzhen; Tongji University; Northeastern University; Northeastern University; Shanghai Jiao Tong University; Northeastern University; Michigan State University; Rensselaer Polytechnic Institute; Northeastern University",
        "aff_domain": "northeastern.edu; ; ; ; ; ; ; ; ;northeastern.edu",
        "email": "northeastern.edu; ; ; ; ; ; ; ; ;northeastern.edu",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;2;3;1;1;4;1;5;0;1",
        "aff_unique_norm": "Rensselaer Polytechnic Institute;Northeastern University;Chinese University of Hong Kong;Tongji University;Shanghai Jiao Tong University;Michigan State University",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.rpi.edu;https://www.northeastern.edu;https://www.cuhk.edu.cn;https://www.tongji.edu.cn;https://www.sjtu.edu.cn;https://www.msu.edu",
        "aff_unique_abbr": "RPI;NEU;CUHK;Tongji;SJTU;MSU",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0+0;1;1;0;0;1;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-short.26",
        "title": "More room for language: Investigating the effect of retrieval on language models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Retrieval-augmented language models pose a promising alternative to standard language modeling. During pretraining, these models search in a corpus of documents for contextually relevant information that could aid the language modeling objective. We introduce an \u2018ideal retrieval\u2019 methodology to study these models in a fully controllable setting. We conduct an extensive evaluation to examine how retrieval augmentation affects the behavior of the underlying language model. Among other things, we observe that these models: (i) save substantially less world knowledge in their weights, (ii) are better at understanding local context and inter-word dependencies, but (iii) are worse at comprehending global context.",
        "author": "David Samuel; Lucas Charpentier; Sondre Wold",
        "authorids": "/d/david-samuel/; /l/lucas-georges-gabriel-charpentier/; /s/sondre-wold/",
        "bibtex": "@inproceedings{samuel-etal-2024-room,\n    title = \"More room for language: Investigating the effect of retrieval on language models\",\n    author = \"Samuel, David  and\n      Charpentier, Lucas  and\n      Wold, Sondre\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.26/\",\n    doi = \"10.18653/v1/2024.naacl-short.26\",\n    pages = \"282--305\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.26.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.26/",
        "pdf_size": 629726,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12576481111330497938&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Language Technology Group, University of Oslo; Language Technology Group, University of Oslo; ",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Oslo",
        "aff_unique_dep": "Language Technology Group",
        "aff_unique_url": "https://www.uio.no",
        "aff_unique_abbr": "UiO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Norway"
    },
    {
        "id": "2024.naacl-short.67",
        "title": "MuLan: A Study of Fact Mutability in Language Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Facts are subject to contingencies and can be true or false in different circumstances. One such contingency is time, wherein some facts mutate over a given period, e.g., the president of a country or the winner of a championship. Trustworthy language models ideally identify mutable facts as such and process them accordingly. We create MuLan, a benchmark for evaluating the ability of English language models to anticipate time-contingency, covering both 1:1 and 1:N relations. We hypothesize that mutable facts are encoded differently than immutable ones, hence being easier to update. In a detailed evaluation of six popular large language models, we consistently find differences in the LLMs\u2019 confidence, representations, and update behavior, depending on the mutability of a fact. Our findings should inform future work on the injection of and induction of time-contingent knowledge to/from LLMs.",
        "author": "Constanza Fierro; Nicolas Garneau; Emanuele Bugliarello; Yova Kementchedjhieva; Anders S\u00f8gaard",
        "authorids": "/c/constanza-fierro/; /n/nicolas-garneau/; /e/emanuele-bugliarello/; /y/yova-kementchedjhieva/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{fierro-etal-2024-mulan,\n    title = \"{M}u{L}an: A Study of Fact Mutability in Language Models\",\n    author = \"Fierro, Constanza  and\n      Garneau, Nicolas  and\n      Bugliarello, Emanuele  and\n      Kementchedjhieva, Yova  and\n      S{\\o}gaard, Anders\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.67/\",\n    doi = \"10.18653/v1/2024.naacl-short.67\",\n    pages = \"762--771\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.67.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.67/",
        "pdf_size": 301899,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5588096443172646072&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Copenhagen\u2020; Department of Computer Science, University of Copenhagen\u2020; Google Research\u266e; Mohamed bin Zayed University of Artificial Intelligence\u2021; Department of Computer Science, University of Copenhagen\u2020",
        "aff_domain": "di.ku.dk;di.ku.dk; ;mbzuai.ac.ae;di.ku.dk",
        "email": "di.ku.dk;di.ku.dk; ;mbzuai.ac.ae;di.ku.dk",
        "github": "https://github.com/coastalcph/fact_mutability",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University of Copenhagen;Google;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": "Department of Computer Science;Google Research;",
        "aff_unique_url": "https://www.ku.dk;https://research.google;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "UCPH;Google Research;MBZUAI",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;1;2;0",
        "aff_country_unique": "Denmark;United States;United Arab Emirates"
    },
    {
        "id": "2024.findings-naacl.185",
        "title": "MuMath: Multi-perspective Data Augmentation for Mathematical Reasoning in Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recently, the tool-use Large Language Models (LLMs) that integrate with external Python interpreters have significantly enhanced mathematical reasoning capabilities for open-source LLMs. However, these models fall short in demonstrating the calculation process, which compromises user-friendliness and understanding of problem-solving steps. Conversely, while tool-free methods offer a clear display of the problem-solving process, their accuracy leaves room for improvement.These tool-free methods typically employ a somewhat narrow range of augmentation techniques such as rephrasing and difficulty enhancement to boost performance. In response to this issue, we have amalgamated and further refined these strengths while broadening the scope of augmentation methods to construct a **mu**lti-perspective augmentation dataset for **math**ematics\u2014termed **MuMath** (\ud835\udf07-Math) Dataset.Subsequently, we finetune LLaMA-2 on the MuMath dataset to derive the MuMath model. Our experiments indicate that our MuMath-70B model achieves new state-of-the-art performance among tool-free methods\u2014achieving 88.3% on GSM8K and 34.5% on MATH .We release the MuMath dataset along with its corresponding models and code for public use.",
        "author": "Weihao You; Shuo Yin; Xudong Zhao; Zhilong Ji; Guoqiang Zhong; Jinfeng Bai",
        "authorids": "/w/weihao-you/; /s/shuo-yin/; /x/xudong-zhao/; /z/zhilong-ji/; /g/guoqiang-zhong/; /j/jinfeng-bai/",
        "bibtex": "@inproceedings{you-etal-2024-mumath,\n    title = \"{M}u{M}ath: Multi-perspective Data Augmentation for Mathematical Reasoning in Large Language Models\",\n    author = \"You, Weihao  and\n      Yin, Shuo  and\n      Zhao, Xudong  and\n      Ji, Zhilong  and\n      Zhong, Guoqiang  and\n      Bai, Jinfeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.185/\",\n    doi = \"10.18653/v1/2024.findings-naacl.185\",\n    pages = \"2932--2958\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.185.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.185/",
        "pdf_size": 2752489,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13305114625852980366&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Tomorrow Advancing Life; Tomorrow Advancing Life + College of Computer Science and Technology, Ocean University of China + School of Economics and Management, East China Jiaotong University; Tomorrow Advancing Life + College of Computer Science and Technology, Ocean University of China + School of Economics and Management, East China Jiaotong University; Tomorrow Advancing Life; College of Computer Science and Technology, Ocean University of China; Tomorrow Advancing Life",
        "aff_domain": "tal.com;foxmail.com;163.com;tal.com;ouc.edu.cn;gmail.com",
        "email": "tal.com;foxmail.com;163.com;tal.com;ouc.edu.cn;gmail.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0+1+2;0+1+2;0;1;0",
        "aff_unique_norm": "Tomorrow Advancing Life;Ocean University of China;East China Jiao Tong University",
        "aff_unique_dep": ";College of Computer Science and Technology;School of Economics and Management",
        "aff_unique_url": ";http://www.ouc.edu.cn;http://www.ecjtu.edu.cn",
        "aff_unique_abbr": ";;ECJTU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1+1;1+1;1",
        "aff_country_unique": ";China"
    },
    {
        "id": "2024.findings-naacl.142",
        "title": "Multi-Granularity Guided Fusion-in-Decoder",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In Open-domain Question Answering (ODQA), it is essential to discern relevant contexts as evidence and avoid spurious ones among retrieved results. The model architecture that uses concatenated multiple contexts in the decoding phase, *i.e.*, Fusion-in-Decoder, demonstrates promising performance but generates incorrect outputs from seemingly plausible contexts. To address this problem, we propose the ***M**ulti-**G**ranularity guided **F**usion-**i**n-**D**ecoder (**MGFiD**)*, discerning evidence across multiple levels of granularity. Based on multi-task learning, MGFiD harmonizes passage re-ranking with sentence classification. It aggregates evident sentences into an *anchor vector* that instructs the decoder. Additionally, it improves decoding efficiency by reusing the results of passage re-ranking for *passage pruning*. Through our experiments, MGFiD outperforms existing models on the Natural Questions (NQ) and TriviaQA (TQA) datasets, highlighting the benefits of its multi-granularity solution.",
        "author": "Eunseong Choi; Hyeri Lee; Jongwuk Lee",
        "authorids": "/e/eunseong-choi/; /h/hyeri-lee/; /j/jongwuk-lee/",
        "bibtex": "@inproceedings{choi-etal-2024-multi,\n    title = \"Multi-Granularity Guided Fusion-in-Decoder\",\n    author = \"Choi, Eunseong  and\n      Lee, Hyeri  and\n      Lee, Jongwuk\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.142/\",\n    doi = \"10.18653/v1/2024.findings-naacl.142\",\n    pages = \"2201--2212\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.142.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.142/",
        "pdf_size": 4184861,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13614849638829685596&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Sungkyunkwan University, Republic of Korea; Sungkyunkwan University, Republic of Korea; Sungkyunkwan University, Republic of Korea",
        "aff_domain": "skku.edu;skku.edu;skku.edu",
        "email": "skku.edu;skku.edu;skku.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sungkyunkwan University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.skku.edu",
        "aff_unique_abbr": "SKKU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-long.80",
        "title": "Multi-Operational Mathematical Derivations in Latent Space",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper investigates the possibility of approximating multiple mathematical operations in latent space for expression derivation. To this end, we introduce different multi-operational representation paradigms, modelling mathematical operations as explicit geometric transformations. By leveraging a symbolic engine, we construct a large-scale dataset comprising 1.7M derivation steps stemming from 61K premises and 6 operators, analysing the properties of each paradigm when instantiated with state-of-the-art neural encoders.Specifically, we investigate how different encoding mechanisms can approximate expression manipulation in latent space, exploring the trade-off between learning different operators and specialising within single operations, as well as the ability to support multi-step derivations and out-of-distribution generalisation. Our empirical analysis reveals that the multi-operational paradigm is crucial for disentangling different operators, while discriminating the conclusions for a single operation is achievable in the original expression encoder. Moreover, we show that architectural choices can heavily affect the training dynamics, structural organisation, and generalisation of the latent space, resulting in significant variations across paradigms and classes of encoders.",
        "author": "Marco Valentino; Jordan Meadows; Lan Zhang; Andre Freitas",
        "authorids": "/m/marco-valentino/; /j/jordan-meadows/; /l/lan-zhang/; /a/andre-freitas/",
        "bibtex": "@inproceedings{valentino-etal-2024-multi-operational,\n    title = \"Multi-Operational Mathematical Derivations in Latent Space\",\n    author = \"Valentino, Marco  and\n      Meadows, Jordan  and\n      Zhang, Lan  and\n      Freitas, Andre\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.80/\",\n    doi = \"10.18653/v1/2024.naacl-long.80\",\n    pages = \"1446--1458\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.80.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.80/",
        "pdf_size": 644337,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15358623329254486487&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Idiap Research Institute, Switzerland + Department of Computer Science, University of Manchester, United Kingdom + Cancer Biomarker Centre, CRUK Manchester Institute, United Kingdom; Department of Computer Science, University of Manchester, United Kingdom; Department of Computer Science, University of Manchester, United Kingdom; Idiap Research Institute, Switzerland + Department of Computer Science, University of Manchester, United Kingdom + Cancer Biomarker Centre, CRUK Manchester Institute, United Kingdom",
        "aff_domain": "idiap.ch;idiap.ch;manchester.ac.uk;manchester.ac.uk",
        "email": "idiap.ch;idiap.ch;manchester.ac.uk;manchester.ac.uk",
        "github": "https://github.com/neuro-symbolic-ai/latent_mathematical_reasoning",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;1;1;0+1+2",
        "aff_unique_norm": "Idiap Research Institute;University of Manchester;CRUK Manchester Institute",
        "aff_unique_dep": ";Department of Computer Science;Cancer Biomarker Centre",
        "aff_unique_url": "https://www.idiap.ch;https://www.manchester.ac.uk;https://www.cruk.manchester.ac.uk",
        "aff_unique_abbr": "Idiap;UoM;CRUK MI",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Manchester",
        "aff_country_unique_index": "0+1+1;1;1;0+1+1",
        "aff_country_unique": "Switzerland;United Kingdom"
    },
    {
        "id": "2024.findings-naacl.189",
        "title": "Multi-Review Fusion-in-Context",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Grounded text generation, encompassing tasks such as long-form question-answering and summarization, necessitates both content selection and content consolidation. Current end-to-end methods are difficult to control and interpret due to their opaqueness.Accordingly, recent works have proposed a modular approach, with separate components for each step. Specifically, we focus on the second subtask, of generating coherent text given pre-selected content in a multi-document setting. Concretely, we formalize Fusion-in-Context (FiC) as a standalone task, whose input consists of source texts with highlighted spans of targeted content. A model then needs to generate a coherent passage that includes all and only the target information.Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which strongly correlate to human judgment. Several baseline models exhibit promising outcomes and provide insightful analyses.This study lays the groundwork for further exploration of modular text generation in the multi-document setting, offering potential improvements in the quality and reliability of generated content. Our benchmark, FuseReviews, including the dataset, evaluation framework, and designated leaderboard, can be found at https://fusereviews.github.io/.",
        "author": "Aviv Slobodkin; Ori Shapira; Ran Levy; Ido Dagan",
        "authorids": "/a/aviv-slobodkin/; /o/ori-shapira/; /r/ran-levy/; /i/ido-dagan/",
        "bibtex": "@inproceedings{slobodkin-etal-2024-multi,\n    title = \"Multi-Review Fusion-in-Context\",\n    author = \"Slobodkin, Aviv  and\n      Shapira, Ori  and\n      Levy, Ran  and\n      Dagan, Ido\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.189/\",\n    doi = \"10.18653/v1/2024.findings-naacl.189\",\n    pages = \"3003--3021\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.189.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.189/",
        "pdf_size": 1038964,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10456829881537151892&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Bar-Ilan University+Amazon; Amazon; Amazon; Bar-Ilan University",
        "aff_domain": "gmail.com;amazon.com;amazon.com;cs.biu.ac.il",
        "email": "gmail.com;amazon.com;amazon.com;cs.biu.ac.il",
        "github": "",
        "project": "https://fusereviews.github.io/",
        "author_num": 4,
        "aff_unique_index": "0+1;1;1;0",
        "aff_unique_norm": "Bar-Ilan University;Amazon",
        "aff_unique_dep": ";Amazon.com, Inc.",
        "aff_unique_url": "https://www.biu.ac.il;https://www.amazon.com",
        "aff_unique_abbr": "BIU;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;0",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "2024.naacl-long.98",
        "title": "Multi-Scale Prompt Memory-Augmented Model for Black-Box Scenarios",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Black-box few-shot text classification handles text classification in limited data without accessing the parameters and gradients of language models (LMs). Existing black-box optimization methods have demonstrated strong few-shot learning capabilities. However, they still require numerous LMs\u2019 calls to search optimal prompts, thus resulting in overfitting performance and increasing computational cost. To address this issue, we present MuSKPrompt (Multi-scale Knowledge Prompt for Memory Model), an efficient multi-scale knowledge prompt-based memory model in black-box few-shot text classification task. MuSKPrompt extracts instance-level and class-level knowledge at different scales and stores them in memory banks during training. Then, it references multi-scale memory banks to perform quick inference on new samples via a novel scoring module. MuSKPrompt achieves competitive performance in limited data through multi-scale instance-level and class-level knowledge. Moreover, it realizes gradient-free optimization with zero training parameters in the black-box scenario. Experiments on different benchmarks and parameter analysis demonstrate the effectiveness and efficiency of MuSKPrompt in black-box few-shot text classification tasks.",
        "author": "Xiaojun Kuang; C. L. Philip Chen; Shuzhen Li; Tong Zhang",
        "authorids": "/x/xiaojun-kuang/; /c/c-l-philip-chen/; /s/shuzhen-li/; /t/tong-zhang/",
        "bibtex": "@inproceedings{kuang-etal-2024-multi,\n    title = \"Multi-Scale Prompt Memory-Augmented Model for Black-Box Scenarios\",\n    author = \"Kuang, Xiaojun  and\n      Chen, C. L. Philip  and\n      Li, Shuzhen  and\n      Zhang, Tong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.98/\",\n    doi = \"10.18653/v1/2024.naacl-long.98\",\n    pages = \"1743--1757\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.98.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.98/",
        "pdf_size": 1843238,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:NW5JLkG9yfUJ:scholar.google.com/&scioq=Multi-Scale+Prompt+Memory-Augmented+Model+for+Black-Box+Scenarios&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "School of Computer Science & Engineering, South China University of Technology; School of Computer Science & Engineering, South China University of Technology; School of Computer Science & Engineering, South China University of Technology; School of Computer Science & Engineering, South China University of Technology",
        "aff_domain": "gmail.com;scut.edu.cn;scut.edu.cn; ",
        "email": "gmail.com;scut.edu.cn;scut.edu.cn; ",
        "github": "github.com/cuteyuqing/MuSKPromptMethod",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "South China University of Technology",
        "aff_unique_dep": "School of Computer Science & Engineering",
        "aff_unique_url": "https://www.scut.edu.cn",
        "aff_unique_abbr": "SCUT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.273",
        "title": "Multi-stage Retrieve and Re-rank Model for Automatic Medical Coding Recommendation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The International Classification of Diseases (ICD) serves as a definitive medical classification system encompassing a wide range of diseases and conditions. The primary objective of ICD indexing is to allocate a subset of ICD codes to a medical record, which facilitates standardized documentation and management of various health conditions. Most existing approaches have suffered from selecting the proper label subsets from an extremely large ICD collection with a heavy long-tailed label distribution. In this paper, we leverage a multi-stage \u201cretrieve and re-rank\u201d framework as a novel solution to ICD indexing, via a hybrid discrete retrieval method, and re-rank retrieved candidates with contrastive learning that allows the model to make more accurate predictions from a simplified label space. The retrieval model is a hybrid of auxiliary knowledge of the electronic health records (EHR) and a discrete retrieval method (BM25), which efficiently collects high-quality candidates. In the last stage, we propose a label co-occurrence guided contrastive re-ranking model, which re-ranks the candidate labels by pulling together the clinical notes with positive ICD codes. Experimental results show the proposed method achieves state-of-the-art performance on a number of measures on the MIMIC-III benchmark.",
        "author": "Xindi Wang; Robert Mercer; Frank Rudzicz",
        "authorids": "/x/xindi-wang/; /r/robert-mercer/; /f/frank-rudzicz/",
        "bibtex": "@inproceedings{wang-etal-2024-multi,\n    title = \"Multi-stage Retrieve and Re-rank Model for Automatic Medical Coding Recommendation\",\n    author = \"Wang, Xindi  and\n      Mercer, Robert  and\n      Rudzicz, Frank\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.273/\",\n    doi = \"10.18653/v1/2024.naacl-long.273\",\n    pages = \"4881--4891\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.273.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.273/",
        "pdf_size": 804910,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2064698880895955159&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, University of Western Ontario, Canada + Vector Institute for Artificial Intelligence, Canada; Department of Computer Science, University of Western Ontario, Canada; Vector Institute for Artificial Intelligence, Canada + Faculty of Computer Science, Dalhousie University, Canada + Department of Computer Science, University of Toronto, Canada",
        "aff_domain": "uwo.ca;csd.uwo.ca;dal.ca",
        "email": "uwo.ca;csd.uwo.ca;dal.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;1+2+3",
        "aff_unique_norm": "University of Western Ontario;Vector Institute for Artificial Intelligence;Dalhousie University;University of Toronto",
        "aff_unique_dep": "Department of Computer Science;;Faculty of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.uwo.ca;https://vectorinstitute.ai/;https://www.dal.ca;https://www.utoronto.ca",
        "aff_unique_abbr": "UWO;Vector Institute;Dal;U of T",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0+0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-short.12",
        "title": "MultiParaDetox: Extending Text Detoxification with Parallel Data to New Languages",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Text detoxification is a textual style transfer (TST) task where a text is paraphrased from a toxic surface form, e.g. featuring rude words, to the neutral register. Recently, text detoxification methods found their applications in various task such as detoxification of Large Language Models (LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic speech combating in social networks (Deng et al., 2023; Mun et al., 2023; Agarwal et al., 2023). All these applications are extremely important to ensure safe communication in modern digital worlds. However, the previous approaches for parallel text detoxification corpora collection\u2014ParaDetox (Logacheva et al., 2022) and APPADIA (Atwell et al., 2022)\u2014were explored only in monolingual setup. In this work, we aim to extend ParaDetox pipeline to multiple languages presenting MultiParaDetox to automate parallel detoxification corpus collection for potentially any language. Then, we experiment with different text detoxification models\u2014from unsupervised baselines to LLMs and fine-tuned models on the presented parallel corpora\u2014showing the great benefit of parallel corpus presence to obtain state-of-the-art text detoxification models for any language.",
        "author": "Daryna Dementieva; Nikolay Babakov; Alexander Panchenko",
        "authorids": "/d/daryna-dementieva/; /n/nikolay-babakov/; /a/alexander-panchenko/",
        "bibtex": "@inproceedings{dementieva-etal-2024-multiparadetox,\n    title = \"{M}ulti{P}ara{D}etox: Extending Text Detoxification with Parallel Data to New Languages\",\n    author = \"Dementieva, Daryna  and\n      Babakov, Nikolay  and\n      Panchenko, Alexander\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.12/\",\n    doi = \"10.18653/v1/2024.naacl-short.12\",\n    pages = \"124--140\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.12.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.12/",
        "pdf_size": 1443112,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8510284289284812401&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Technical University of Munich; Centro Singular de Investigaci\u00f3n en Tecnolox\u00edas Intelixentes (CiTIUS), Universidade de Santiago de Compostela; Skolkovo Institute of Science and Technology+Artificial Intelligence Research Institute",
        "aff_domain": "tum.de;usc.es;skol.tech",
        "email": "tum.de;usc.es;skol.tech",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2+3",
        "aff_unique_norm": "Technical University of Munich;Universidade de Santiago de Compostela;Skolkovo Institute of Science and Technology;Artificial Intelligence Research Institute",
        "aff_unique_dep": ";Centro Singular de Investigaci\u00f3n en Tecnolox\u00edas Intelixentes (CiTIUS);;",
        "aff_unique_url": "https://www.tum.de;https://www.usc.es;https://www.skoltech.ru;",
        "aff_unique_abbr": "TUM;USC;Skoltech;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2+3",
        "aff_country_unique": "Germany;Spain;Russian Federation;United States"
    },
    {
        "id": "2024.findings-naacl.176",
        "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages? 2) Which factors affect LLMs\u2019 performance in translation? We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our empirical results show that translation capabilities of LLMs are continually involving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system like Google Translate, especially on low-resource languages. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages. Second, instruction semantics can surprisingly be ignored when given in-context exemplars. Third, cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs. Code will be released at: https://github.com/NJUNLP/MMT-LLM.",
        "author": "Wenhao Zhu; Hongyi Liu; Qingxiu Dong; Jingjing Xu; Shujian Huang; Lingpeng Kong; Jiajun Chen; Lei Li",
        "authorids": "/w/wenhao-zhu/; /h/hongyi-liu/; /q/qingxiu-dong/; /j/jingjing-xu/; /s/shujian-huang/; /l/lingpeng-kong/; /j/jiajun-chen/; /l/lei-li/",
        "bibtex": "@inproceedings{zhu-etal-2024-multilingual,\n    title = \"Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\",\n    author = \"Zhu, Wenhao  and\n      Liu, Hongyi  and\n      Dong, Qingxiu  and\n      Xu, Jingjing  and\n      Huang, Shujian  and\n      Kong, Lingpeng  and\n      Chen, Jiajun  and\n      Li, Lei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.176/\",\n    doi = \"10.18653/v1/2024.findings-naacl.176\",\n    pages = \"2765--2781\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.176.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.176/",
        "pdf_size": 8377135,
        "gs_citation": 121,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16902872133358447781&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University+Shanghai AI Lab; Shanghai Jiao Tong University; Peking University; Shanghai AI Lab; National Key Laboratory for Novel Software Technology, Nanjing University; The University of Hong Kong; National Key Laboratory for Novel Software Technology, Nanjing University; Language Technologies Institute, Carnegie Mellon University",
        "aff_domain": "smail.nju.edu.cn;sjtu.edu.cn;stu.pku.edu.cn;pku.edu.cn;nju.edu.cn;cs.hku.hk;nju.edu.cn;cs.cmu.edu",
        "email": "smail.nju.edu.cn;sjtu.edu.cn;stu.pku.edu.cn;pku.edu.cn;nju.edu.cn;cs.hku.hk;nju.edu.cn;cs.cmu.edu",
        "github": "https://github.com/NJUNLP/MMT-LLM",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;2;3;1;0;4;0;5",
        "aff_unique_norm": "Nanjing University;Shanghai AI Lab;Shanghai Jiao Tong University;Peking University;University of Hong Kong;Carnegie Mellon University",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;;;;;Language Technologies Institute",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.shanghaiailab.com;https://www.sjtu.edu.cn;http://www.pku.edu.cn;https://www.hku.hk;https://www.cmu.edu",
        "aff_unique_abbr": "Nanjing University;SAIL;SJTU;Peking U;HKU;CMU",
        "aff_campus_unique_index": ";1;2",
        "aff_campus_unique": ";Hong Kong SAR;Pittsburgh",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.471",
        "title": "Multilingual Models for ASR in Chibchan Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present experiments on Automatic Speech Recognition (ASR) for Bribri and Cab\u00e9car, two languages from the Chibchan family. We fine-tune four ASR algorithms (Wav2Vec2, Whisper, MMS & WavLM) to create monolingual models, with the Wav2Vec2 model demonstrating the best performance. We then proceed to use Wav2Vec2 for (1) experiments on training joint and transfer learning models for both languages, and (2) an analysis of the errors, with a focus on the transcription of tone. Results show effective transfer learning for both Bribri and Cab\u00e9car, but especially for Bribri. A post-processing spell checking step further reduced character and word error rates. As for the errors, tone is where the Bribri models make the most errors, whereas the simpler tonal system of Cab\u00e9car is better transcribed by the model. Our work contributes to developing better ASR technology, an important tool that could facilitate transcription, one of the major bottlenecks in language documentation efforts. Our work also assesses how existing pre-trained models and algorithms perform for genuine extremely low resource-languages.",
        "author": "Rolando Coto-Solano; Tai Wan Kim; Alexander Jones; Sharid Lo\u00e1iciga",
        "authorids": "/r/rolando-coto-solano/; /t/tai-wan-kim/; /a/alexander-jones/; /s/sharid-loaiciga/",
        "bibtex": "@inproceedings{coto-solano-etal-2024-multilingual,\n    title = \"Multilingual Models for {ASR} in Chibchan Languages\",\n    author = \"Coto-Solano, Rolando  and\n      Kim, Tai Wan  and\n      Jones, Alexander  and\n      Lo{\\'a}iciga, Sharid\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.471/\",\n    doi = \"10.18653/v1/2024.naacl-long.471\",\n    pages = \"8521--8535\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.471.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.471/",
        "pdf_size": 519342,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6tK_p2mYNsQJ:scholar.google.com/&scioq=Multilingual+Models+for+ASR+in+Chibchan+Languages&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Dept. of Linguistics, Dartmouth College; Dept. of Linguistics, Dartmouth College; Dept. of Linguistics, Dartmouth College; Dept. of Philosophy, Linguistics, and Theory of Science, University of Gothenburg",
        "aff_domain": "dartmouth.edu;dartmouth.edu;dartmouth.edu;gu.se",
        "email": "dartmouth.edu;dartmouth.edu;dartmouth.edu;gu.se",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "Dartmouth College;University of Gothenburg",
        "aff_unique_dep": "Dept. of Linguistics;Dept. of Philosophy, Linguistics, and Theory of Science",
        "aff_unique_url": "https://www.dartmouth.edu;https://www.gu.se",
        "aff_unique_abbr": "Dartmouth;GU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Gothenburg",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "United States;Sweden"
    },
    {
        "id": "2024.naacl-long.433",
        "title": "Multilingual Nonce Dependency Treebanks: Understanding how Language Models Represent and Process Syntactic Structure",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce SPUD (Semantically Perturbed Universal Dependencies), a framework for creating nonce treebanks for the multilingual Universal Dependencies (UD) corpora. SPUD data satisfies syntactic argument structure, provides syntactic annotations, and ensures grammaticality via language-specific rules. We create nonce data in Arabic, English, French, German, and Russian, and demonstrate two use cases of SPUD treebanks. First, we investigate the effect of nonce data on word co-occurrence statistics, as measured by perplexity scores of autoregressive (ALM) and masked language models (MLM). We find that ALM scores are significantly more affected by nonce data than MLM scores. Second, we show how nonce data affects the performance of syntactic dependency probes. We replicate the findings of M\u00fcller-Eberstein et al. (2022) on nonce test data and show that the performance declines on both MLMs and ALMs wrt. original test data. However, a majority of the performance is kept, suggesting that the probe indeed learns syntax independently from semantics.",
        "author": "David Arps; Laura Kallmeyer; Younes Samih; Hassan Sajjad",
        "authorids": "/d/david-arps/; /l/laura-kallmeyer/; /y/younes-samih/; /h/hassan-sajjad/",
        "bibtex": "@inproceedings{arps-etal-2024-multilingual,\n    title = \"Multilingual Nonce Dependency Treebanks: Understanding how Language Models Represent and Process Syntactic Structure\",\n    author = \"Arps, David  and\n      Kallmeyer, Laura  and\n      Samih, Younes  and\n      Sajjad, Hassan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.433/\",\n    doi = \"10.18653/v1/2024.naacl-long.433\",\n    pages = \"7822--7844\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.433.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.433/",
        "pdf_size": 1050779,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10084403212327506208&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Heinrich-Heine-Universit\u00e4t D\u00fcsseldorf; Heinrich-Heine-Universit\u00e4t D\u00fcsseldorf; IBM Research; Dalhousie University",
        "aff_domain": "hhu.de;hhu.de;ibm.com;dal.ca",
        "email": "hhu.de;hhu.de;ibm.com;dal.ca",
        "github": "https://github.com/davidarps/spud",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Heinrich-Heine-Universit\u00e4t;IBM;Dalhousie University",
        "aff_unique_dep": ";IBM Research;",
        "aff_unique_url": "https://www.hhu.de;https://www.ibm.com/research;https://www.dal.ca",
        "aff_unique_abbr": "HHU;IBM;Dal",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;2",
        "aff_country_unique": "Germany;United States;Canada"
    },
    {
        "id": "2024.naacl-long.339",
        "title": "Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual Knowledge Alignment, But Only Shallowly",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite their strong ability to retrieve knowledge in English, current large language models show imbalance abilities in different languages. Two approaches are proposed to address this, i.e., multilingual pretraining and multilingual instruction tuning. However, whether and how do such methods contribute to the cross-lingual knowledge alignment inside the models is unknown. In this paper, we propose CLiKA, a systematic framework to assess the cross-lingual knowledge alignment of LLMs in the Performance, Consistency and Conductivity levels, and explored the effect of multilingual pretraining and instruction tuning on the degree of alignment. Results show that: while both multilingual pretraining and instruction tuning are beneficial for cross-lingual knowledge alignment, the training strategy needs to be carefully designed. Namely, continued pretraining improves the alignment of the target language at the cost of other languages, while mixed pretraining affect other languages less. Also, the overall cross-lingual knowledge alignment, especially in the conductivity level, is unsatisfactory for all tested LLMs, and neither multilingual pretraining nor instruction tuning can substantially improve the cross-lingual knowledge conductivity.",
        "author": "Changjiang Gao; Hongda Hu; Peng Hu; Jiajun Chen; Jixing Li; Shujian Huang",
        "authorids": "/c/changjiang-gao/; /h/hongda-hu/; /p/peng-hu/; /j/jiajun-chen/; /j/jixing-li/; /s/shujian-huang/",
        "bibtex": "@inproceedings{gao-etal-2024-multilingual,\n    title = \"Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual Knowledge Alignment, But Only Shallowly\",\n    author = \"Gao, Changjiang  and\n      Hu, Hongda  and\n      Hu, Peng  and\n      Chen, Jiajun  and\n      Li, Jixing  and\n      Huang, Shujian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.339/\",\n    doi = \"10.18653/v1/2024.naacl-long.339\",\n    pages = \"6101--6117\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.339.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.339/",
        "pdf_size": 3390815,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3580816395806351339&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University; National Key Laboratory for Novel Software Technology, Nanjing University; National Key Laboratory for Novel Software Technology, Nanjing University; National Key Laboratory for Novel Software Technology, Nanjing University; Department of Linguistics and Translation, City University of Hong Kong; National Key Laboratory for Novel Software Technology, Nanjing University",
        "aff_domain": "smail.nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn;cityu.edu.hk;nju.edu.cn",
        "email": "smail.nju.edu.cn;smail.nju.edu.cn;smail.nju.edu.cn;nju.edu.cn;cityu.edu.hk;nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "Nanjing University;City University of Hong Kong",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;Department of Linguistics and Translation",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.cityu.edu.hk",
        "aff_unique_abbr": "Nanjing University;CityU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.307",
        "title": "Multimodal Chart Retrieval: A Comparison of Text, Table and Image Based Approaches",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We investigate multimodal chart retrieval, addressing the challenge of retrieving image-based charts using textual queries. We compare four approaches: (a) OCR with text retrieval, (b) chart derendering (DePlot) followed by table retrieval, (c) a direct image understanding model (PaLI-3), and (d) a combined PaLI-3 + DePlot approach. As the table retrieval component we introduce Tab-GTR, a text retrieval model augmented with table structure embeddings, achieving state-of-the-art results on the NQ-Tables benchmark with 48.88% R@1. On in-distribution data, the DePlot-based method (b) outperforms PaLI-3 (c), while being significantly more efficient (300M vs 3B trainable parameters). However, DePlot struggles with complex charts, indicating a need for improvements in chart derendering - specifically in terms of chart data diversity and the richness of text/table representations. We found no clear winner between methods (b) and (c) in general, with the best performance achieved by the combined approach (d), and further show that it benefits the most from multi-task training.",
        "author": "Averi Nowak; Francesco Piccinno; Yasemin Altun",
        "authorids": "/a/averi-nowak/; /f/francesco-piccinno/; /y/yasemin-altun/",
        "bibtex": "@inproceedings{nowak-etal-2024-multimodal,\n    title = \"Multimodal Chart Retrieval: A Comparison of Text, Table and Image Based Approaches\",\n    author = \"Nowak, Averi  and\n      Piccinno, Francesco  and\n      Altun, Yasemin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.307/\",\n    doi = \"10.18653/v1/2024.naacl-long.307\",\n    pages = \"5488--5505\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.307.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.307/",
        "pdf_size": 897703,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8394173040529324922&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Google DeepMind; Google DeepMind; Google DeepMind",
        "aff_domain": "google.com;google.com;google.com",
        "email": "google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google DeepMind",
        "aff_unique_url": "https://deepmind.com",
        "aff_unique_abbr": "DeepMind",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-industry.25",
        "title": "Multimodal Contextual Dialogue Breakdown Detection for Conversational AI Models",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Detecting dialogue breakdown in real time is critical for conversational AI systems, because it enables taking corrective action to successfully complete a task. In spoken dialog systems, this breakdown can be caused by a variety of unexpected situations including high levels of background noise, causing STT mistranscriptions, or unexpected user flows.In particular, industry settings like healthcare, require high precision and high flexibility to navigate differently based on the conversation history and dialogue states. This makes it both more challenging and more critical to accurately detect dialog breakdown. To accurately detect breakdown, we found it requires processing audio inputs along with downstream NLP model inferences on transcribed text in real time. In this paper, we introduce a Multimodal Contextual Dialogue Breakdown (MultConDB) model. This model significantly outperforms other known best models by achieving an F1 of 69.27.",
        "author": "Md Messal Monem Miah; Ulie Schnaithmann; Arushi Raghuvanshi; Youngseo Son",
        "authorids": "/m/md-messal-monem-miah/; /u/ulie-schnaithmann/; /a/arushi-raghuvanshi/; /y/youngseo-son/",
        "bibtex": "@inproceedings{miah-etal-2024-multimodal,\n    title = \"Multimodal Contextual Dialogue Breakdown Detection for Conversational {AI} Models\",\n    author = \"Miah, Md Messal Monem  and\n      Schnaithmann, Ulie  and\n      Raghuvanshi, Arushi  and\n      Son, Youngseo\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.25/\",\n    doi = \"10.18653/v1/2024.naacl-industry.25\",\n    pages = \"303--314\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.25.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.25/",
        "pdf_size": 10829476,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:kMnZaINQkt4J:scholar.google.com/&scioq=Multimodal+Contextual+Dialogue+Breakdown+Detection+for+Conversational+AI+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Infinitus Systems, Inc.; Infinitus Systems, Inc.; Infinitus Systems, Inc.; Infinitus Systems, Inc.",
        "aff_domain": "infinitus.ai;infinitus.ai;infinitus.ai;infinitus.ai",
        "email": "infinitus.ai;infinitus.ai;infinitus.ai;infinitus.ai",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Infinitus Systems, Inc.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.197",
        "title": "Multimodal Multi-loss Fusion Network for Sentiment Analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper investigates the optimal selection and fusion of feature encoders across multiple modalities and combines these in one neural network to improve sentiment detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying surprisingly important findings relating to subnet performance. We have also found that integrating context significantly enhances model performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS). These results suggest a roadmap toward an optimized feature selection and fusion approach for enhancing sentiment detection in neural networks.",
        "author": "Zehui Wu; Ziwei Gong; Jaywon Koo; Julia Hirschberg",
        "authorids": "/z/zehui-wu/; /z/ziwei-gong/; /j/jaywon-koo/; /j/julia-hirschberg/",
        "bibtex": "@inproceedings{wu-etal-2024-multimodal,\n    title = \"Multimodal Multi-loss Fusion Network for Sentiment Analysis\",\n    author = \"Wu, Zehui  and\n      Gong, Ziwei  and\n      Koo, Jaywon  and\n      Hirschberg, Julia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.197/\",\n    doi = \"10.18653/v1/2024.naacl-long.197\",\n    pages = \"3588--3602\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.197.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.197/",
        "pdf_size": 490657,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5002017904695885287&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Columbia University; Department of Computer Science, Columbia University; Department of Computer Science, Columbia University; Department of Computer Science, Columbia University",
        "aff_domain": "columbia.edu;columbia.edu;columbia.edu;cs.columbia.edu",
        "email": "columbia.edu;columbia.edu;columbia.edu;cs.columbia.edu",
        "github": "https://github.com/zehuiwu/MMML",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.7",
        "title": "Multiple-Question Multiple-Answer Text-VQA",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do text-VQA in encoder-decoder transformer models. To the best of our knowledge, almost all previous approaches for text-VQA process a single question and its associated content to predict a single answer. However, in industry applications, users may come up with multiple questions about a single image. In order to answer multiple questions from the same image, each question and content are fed into the model multiple times. In contrast, our proposed MQMA approach takes multiple questions and content as input at the encoder and predicts multiple answers at the decoder in an auto-regressive manner at the same time. We make several novel architectural modifications to standard encoder-decoder transformers to support MQMA. We also propose a novel MQMA denoising pre-training task which is designed to teach the model to align and delineate multiple questions and content with associated answers. MQMA pre-trained model achieves state-of-the-art results on multiple text-VQA datasets, each with strong baselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%) absolute improvements over the previous state-of-the-art approaches.",
        "author": "Peng Tang; Srikar Appalaraju; R. Manmatha; Yusheng Xie; Vijay Mahadevan",
        "authorids": "/p/peng-tang/; /s/srikar-appalaraju/; /r/r-manmatha/; /y/yusheng-xie/; /v/vijay-mahadevan/",
        "bibtex": "@inproceedings{tang-etal-2024-multiple,\n    title = \"Multiple-Question Multiple-Answer Text-{VQA}\",\n    author = \"Tang, Peng  and\n      Appalaraju, Srikar  and\n      Manmatha, R.  and\n      Xie, Yusheng  and\n      Mahadevan, Vijay\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.7/\",\n    doi = \"10.18653/v1/2024.naacl-industry.7\",\n    pages = \"73--88\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.7.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.7/",
        "pdf_size": 7054682,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11248344884177733672&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "AWS AI Labs",
        "aff_unique_url": "https://aws.amazon.com",
        "aff_unique_abbr": "AWS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.231",
        "title": "MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) have shown immense potential in multimodal applications, yet the convergence of textual and musical domains remains not well-explored. To address this gap, we present MusiLingo, a novel system for music caption generation and music-related query responses. MusiLingo employs a single projection layer to align music representations from the pre-trained frozen music audio model MERT (CITATION) with a frozen LLM, bridging the gap between music audio and textual contexts. We train it on an extensive music caption dataset and fine-tune it with instructional data. Due to the scarcity of high-quality music Q&A datasets, we created the MusicInstruct (MI) dataset from captions in the MusicCaps datasets, tailored for open-ended music inquiries. Empirical evaluations demonstrate its competitive performance in generating music captions and composing music-related Q&A pairs. Our introduced dataset enables notable advancements beyond previous ones.",
        "author": "Zihao Deng; Yinghao Ma; Yudong Liu; Rongchen Guo; Ge Zhang; Wenhu Chen; Wenhao Huang; Emmanouil Benetos",
        "authorids": "/z/zihao-deng/; /y/yinghao-ma/; /y/yudong-liu/; /r/rongchen-guo/; /g/ge-zhang/; /w/wenhu-chen/; /w/wenhao-huang/; /e/emmanouil-benetos/",
        "bibtex": "@inproceedings{deng-etal-2024-musilingo,\n    title = \"{M}usi{L}ingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response\",\n    author = \"Deng, Zihao  and\n      Ma, Yinghao  and\n      Liu, Yudong  and\n      Guo, Rongchen  and\n      Zhang, Ge  and\n      Chen, Wenhu  and\n      Huang, Wenhao  and\n      Benetos, Emmanouil\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.231/\",\n    doi = \"10.18653/v1/2024.findings-naacl.231\",\n    pages = \"3643--3655\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.231.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.231/",
        "pdf_size": 784202,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6532843674568950982&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2024.naacl-long.459",
        "title": "Mustango: Toward Controllable Text-to-Music Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The quality of the text-to-music models has reached new heights due to recent advancements in diffusion models. The controllability of various musical aspects, however, has barely been explored. In this paper, we propose Mustango: a music-domain-knowledge-inspired text-to-music system based on diffusion. Mustango aims to control the generated music, not only with general text captions, but with more rich captions that can include specific instructions related to chords, beats, tempo, and key. At the core of Mustango is MuNet, a Music-Domain-Knowledge-Informed UNet guidance module that steers the generated music to include the music-specific conditions, which we predict from the text prompt, as well as the general text embedding, during the reverse diffusion process. To overcome the limited availability of open datasets of music with text captions, we propose a novel data augmentation method that includes altering the harmonic, rhythmic, and dynamic aspects of music audio and using state-of-the-art Music Information Retrieval methods to extract the music features which will then be appended to the existing descriptions in text format. We release the resulting MusicBench dataset which contains over 52K instances and includes music-theory-based descriptions in the caption text. Through extensive experiments, we show that the quality of the music generated by Mustango is state-of-the-art, and the controllability through music-specific text prompts greatly outperforms other models such as MusicGen and AudioLDM2.",
        "author": "Jan Melechovsky; Zixun Guo; Deepanway Ghosal; Navonil Majumder; Dorien Herremans; Soujanya Poria",
        "authorids": "/j/jan-melechovsky/; /z/zixun-guo/; /d/deepanway-ghosal/; /n/navonil-majumder/; /d/dorien-herremans/; /s/soujanya-poria/",
        "bibtex": "@inproceedings{melechovsky-etal-2024-mustango,\n    title = \"Mustango: Toward Controllable Text-to-Music Generation\",\n    author = \"Melechovsky, Jan  and\n      Guo, Zixun  and\n      Ghosal, Deepanway  and\n      Majumder, Navonil  and\n      Herremans, Dorien  and\n      Poria, Soujanya\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.459/\",\n    doi = \"10.18653/v1/2024.naacl-long.459\",\n    pages = \"8293--8316\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.459.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.459/",
        "pdf_size": 11853159,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10625300525616440212&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.193",
        "title": "My Heart Skipped a Beat! Recognizing Expressions of Embodied Emotion in Natural Language",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Humans frequently experience emotions. When emotions arise, they affect not only our mental state but can also change our physical state. For example, we often open our eyes wide when we are surprised, or clap our hands when we feel excited. Physical manifestations of emotions are referred to as embodied emotion in the psychology literature. From an NLP perspective, recognizing descriptions of physical movements or physiological responses associated with emotions is a type of implicit emotion recognition. Our work introduces a new task of recognizing expressions of embodied emotion in natural language. We create a dataset of sentences that contains 7,300 body part mentions with human annotations for embodied emotion. We develop a classification model for this task and present two methods to acquire weakly labeled instances of embodied emotion by extracting emotional manner expressions and by prompting a language model. Our experiments show that the weakly labeled data can train an effective classification model without gold data, and can also improve performance when combined with gold data. Our dataset is publicly available at https://github.com/yyzhuang1991/Embodied-Emotions.",
        "author": "Yuan Zhuang; Tianyu Jiang; Ellen Riloff",
        "authorids": "/y/yuan-zhuang/; /t/tianyu-jiang/; /e/ellen-riloff/",
        "bibtex": "@inproceedings{zhuang-etal-2024-heart,\n    title = \"My Heart Skipped a Beat! Recognizing Expressions of Embodied Emotion in Natural Language\",\n    author = \"Zhuang, Yuan  and\n      Jiang, Tianyu  and\n      Riloff, Ellen\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.193/\",\n    doi = \"10.18653/v1/2024.naacl-long.193\",\n    pages = \"3525--3537\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.193.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.193/",
        "pdf_size": 784226,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3818071312326194944&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Utah; University of Cincinnati; University of Arizona",
        "aff_domain": "utah.edu;uc.edu;cs.arizona.edu",
        "email": "utah.edu;uc.edu;cs.arizona.edu",
        "github": "https://github.com/yyzhuang1991/Embodied-Emotions",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Utah;University of Cincinnati;University of Arizona",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.utah.edu;https://www.uc.edu;https://www.arizona.edu",
        "aff_unique_abbr": "Utah;UC;UA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.385",
        "title": "NLP Progress in Indigenous Latin American Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The paper focuses on the marginalization of indigenous language communities in the face of rapid technological advancements. We highlight the cultural richness of these languages and the risk they face of being overlooked in the realm of Natural Language Processing (NLP). We aim to bridge the gap between these communities and researchers, emphasizing the need for inclusive technological advancements that respect indigenous community perspectives. We show the NLP progress of indigenous Latin American languages and the survey that covers the status of indigenous languages in Latin America, their representation in NLP, and the challenges and innovations required for their preservation and development. The paper contributes to the current literature in understanding the need and progress of NLP for indigenous communities of Latin America, specifically low-resource and indigenous communities in general.",
        "author": "Atnafu Tonja; Fazlourrahman Balouchzahi; Sabur Butt; Olga Kolesnikova; Hector Ceballos; Alexander Gelbukh; Thamar Solorio",
        "authorids": "/a/atnafu-tonja/; /f/fazlourrahman-balouchzahi/; /s/sabur-butt/; /o/olga-kolesnikova/; /h/hector-ceballos/; /a/alexander-gelbukh/; /t/thamar-solorio/",
        "bibtex": "@inproceedings{tonja-etal-2024-nlp,\n    title = \"{NLP} Progress in Indigenous {L}atin {A}merican Languages\",\n    author = \"Tonja, Atnafu  and\n      Balouchzahi, Fazlourrahman  and\n      Butt, Sabur  and\n      Kolesnikova, Olga  and\n      Ceballos, Hector  and\n      Gelbukh, Alexander  and\n      Solorio, Thamar\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.385/\",\n    doi = \"10.18653/v1/2024.naacl-long.385\",\n    pages = \"6972--6987\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.385.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.385/",
        "pdf_size": 895397,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16900434467405722315&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Instituto Polit\u00e9cnico Nacional, Mexico; Instituto Polit\u00e9cnico Nacional, Mexico; Tecnol\u00f3gico de Monterrey, Mexico; Instituto Polit\u00e9cnico Nacional, Mexico; Tecnol\u00f3gico de Monterrey, Mexico; Instituto Polit\u00e9cnico Nacional, Mexico; MBZUAI, Masdar City, UAE + University of Houston, Houston, USA",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;0;1;0;2+3",
        "aff_unique_norm": "Instituto Polit\u00e9cnico Nacional;Tecnol\u00f3gico de Monterrey;Mohamed bin Zayed University of Artificial Intelligence;University of Houston",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ipn.mx;https://www.tec.mx;https://www.mbzuali.ac.ae;https://www.uh.edu",
        "aff_unique_abbr": "IPN;Tec de Monterrey;MBZUAI;UH",
        "aff_campus_unique_index": "1+2",
        "aff_campus_unique": ";Masdar City;Houston",
        "aff_country_unique_index": "0;0;0;0;0;0;1+2",
        "aff_country_unique": "Mexico;United Arab Emirates;United States"
    },
    {
        "id": "2024.naacl-long.331",
        "title": "NLP Systems That Can\u2019t Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The use of words to convey speaker\u2019s intent is traditionally distinguished from the \u2018mention\u2019 of words for quoting what someone said, or pointing out properties of a word. Here we show that computationally modeling this use-mention distinction is crucial for dealing with counterspeech online. Counterspeech that refutes problematic content often mentions harmful language but is not harmful itself (e.g., calling a vaccine dangerous is not the same as expressing disapproval of someone for calling vaccines dangerous). We show that even recent language models fail at distinguishing use from mention, and that this failure propagates to two key downstream tasks: misinformation and hate speech detection, resulting in censorship of counterspeech. We introduce prompting mitigations that teach the use-mention distinction, and show they reduce these errors. Our work highlights the importance of the use-mention distinction for NLP and CSS and offers ways to address it.",
        "author": "Kristina Gligoric; Myra Cheng; Lucia Zheng; Esin Durmus; Dan Jurafsky",
        "authorids": "/k/kristina-gligoric/; /m/myra-cheng/; /l/lucia-zheng/; /e/esin-durmus/; /d/dan-jurafsky/",
        "bibtex": "@inproceedings{gligoric-etal-2024-nlp,\n    title = \"{NLP} Systems That Can{'}t Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps\",\n    author = \"Gligoric, Kristina  and\n      Cheng, Myra  and\n      Zheng, Lucia  and\n      Durmus, Esin  and\n      Jurafsky, Dan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.331/\",\n    doi = \"10.18653/v1/2024.naacl-long.331\",\n    pages = \"5942--5959\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.331.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.331/",
        "pdf_size": 613464,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11515611112947272338&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Stanford University; Stanford University; Stanford University; Stanford University; Stanford University",
        "aff_domain": "cs.stanford.edu; ; ; ; ",
        "email": "cs.stanford.edu; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.221",
        "title": "NLP for Counterspeech against Hate: A Survey and How-To Guide",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In recent years, counterspeech has emerged as one of the most promising strategies to fight online hate. These non-escalatory responses tackle online abuse while preserving the freedom of speech of the users, and can have a tangible impact in reducing online and offline violence. Recently, there has been growing interest from the Natural Language Processing (NLP) community in addressing the challenges of analysing, collecting, classifying, and automatically generating counterspeech, to reduce the huge burden of manually producing it. In particular, researchers have taken different directions in addressing these challenges, thus providing a variety of related tasks and resources. In this paper, we provide a guide for doing research on counterspeech, by describing - with detailed examples - the steps to undertake, and providing best practices that can be learnt from the NLP studies on this topic. Finally, we discuss open challenges and future directions of counterspeech research in NLP.",
        "author": "Helena Bonaldi; Yi-Ling Chung; Gavin Abercrombie; Marco Guerini",
        "authorids": "/h/helena-bonaldi/; /y/yi-ling-chung/; /g/gavin-abercrombie/; /m/marco-guerini/",
        "bibtex": "@inproceedings{bonaldi-etal-2024-nlp,\n    title = \"{NLP} for Counterspeech against Hate: A Survey and How-To Guide\",\n    author = \"Bonaldi, Helena  and\n      Chung, Yi-Ling  and\n      Abercrombie, Gavin  and\n      Guerini, Marco\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.221/\",\n    doi = \"10.18653/v1/2024.findings-naacl.221\",\n    pages = \"3480--3499\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.221.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.221/",
        "pdf_size": 249405,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14990914033346671740&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Fondazione Bruno Kessler, Italy+University of Trento, Italy; The Alan Turing Institute; The Interaction Lab, Heriot-Watt University; Fondazione Bruno Kessler, Italy",
        "aff_domain": "fbk.eu;turing.ac.uk;hw.ac.uk;fbk.eu",
        "email": "fbk.eu;turing.ac.uk;hw.ac.uk;fbk.eu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2;3;0",
        "aff_unique_norm": "Fondazione Bruno Kessler;University of Trento;Alan Turing Institute;Heriot-Watt University",
        "aff_unique_dep": ";;;The Interaction Lab",
        "aff_unique_url": "https://www.fbk.eu;https://www.unitn.it;https://www.turing.ac.uk;https://www.hw.ac.uk",
        "aff_unique_abbr": "FBK;UniTN;ATI;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;1;0",
        "aff_country_unique": "Italy;United Kingdom"
    },
    {
        "id": "2024.naacl-long.431",
        "title": "Naive Bayes-based Context Extension for Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) have shown promising in-context learning abilities. However, conventional In-Context Learning (ICL) approaches are often impeded by length limitations of transformer architecture, which pose challenges when attempting to effectively integrate supervision from a substantial number of demonstration examples. In this paper, we introduce a novel framework, called Naive Bayes-based Context Extension (NBCE), to enable existing LLMs to perform ICL with an increased number of demonstrations by significantly expanding their context size. Importantly, this expansion does not require fine-tuning or dependence on particular model architectures, all the while preserving linear efficiency. NBCE initially splits the context into equal-sized windows fitting the target LLM\u2019s maximum length. Then, it introduces a voting mechanism to select the most relevant window, regarded as the posterior context. Finally, it employs Bayes\u2019 theorem to generate the test task. Our experimental results demonstrate that NBCE substantially enhances performance, particularly as the number of demonstration examples increases, consistently outperforming alternative methods. The NBCE code will be made publicly accessible. The code NBCE is available at: https://github.com/amurtadha/NBCE-master",
        "author": "Jianlin Su; Murtadha Ahmed; Bo Wen; Luo Ao; Mingren Zhu; Yunfeng Liu",
        "authorids": "/j/jianlin-su/; /m/murtadha-ahmed/; /b/bo-wen/; /l/luo-ao/; /m/mingren-zhu/; /y/yunfeng-liu/",
        "bibtex": "@inproceedings{su-etal-2024-naive,\n    title = \"Naive {B}ayes-based Context Extension for Large Language Models\",\n    author = \"Su, Jianlin  and\n      Ahmed, Murtadha  and\n      Wen, Bo  and\n      Ao, Luo  and\n      Zhu, Mingren  and\n      Liu, Yunfeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.431/\",\n    doi = \"10.18653/v1/2024.naacl-long.431\",\n    pages = \"7791--7807\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.431.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.431/",
        "pdf_size": 3098295,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9659745753688702704&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Zhuiyi Technology Co. Ltd., Shenzhen, Guangdong, China; Zhuiyi Technology Co. Ltd., Shenzhen, Guangdong, China; Zhuiyi Technology Co. Ltd., Shenzhen, Guangdong, China; Zhuiyi Technology Co. Ltd., Shenzhen, Guangdong, China; Zhuiyi Technology Co. Ltd., Shenzhen, Guangdong, China; Zhuiyi Technology Co. Ltd., Shenzhen, Guangdong, China",
        "aff_domain": "wezhuiyi.com;wezhuiyi.com;wezhuiyi.com;wezhuiyi.com;wezhuiyi.com;wezhuiyi.com",
        "email": "wezhuiyi.com;wezhuiyi.com;wezhuiyi.com;wezhuiyi.com;wezhuiyi.com;wezhuiyi.com",
        "github": "https://github.com/amurtadha/NBCE-master",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Zhuiyi Technology Co. Ltd.",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.1",
        "title": "Named Entity Recognition Under Domain Shift via Metric Learning for Life Sciences",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Named entity recognition is a key component of Information Extraction (IE), particularly in scientific domains such as biomedicine and chemistry, where large language models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability of transfer learning for enhancing a named entity recognition model trained in the biomedical domain (the source domain) to be used in the chemical domain (the target domain). A common practice for training such a model in a few-shot learning setting is to pretrain the model on the labeled source data, and then, to finetune it on a hand-full of labeled target examples. In our experiments, we observed that such a model is prone to mislabeling the source entities, which can often appear in the text, as the target entities. To alleviate this problem, we propose a model to transfer the knowledge from the source domain to the target domain, but, at the same time, to project the source entities and target entities into separate regions of the feature space. This diminishes the risk of mislabeling the source entities as the target entities. Our model consists of two stages: 1) entity grouping in the source domain, which incorporates knowledge from annotated events to establish relations between entities, and 2) entity discrimination in the target domain, which relies on pseudo labeling and contrastive learning to enhance discrimination between the entities in the two domains. We conduct our extensive experiments across three source and three target datasets, demonstrating that our method outperforms the baselines by up to 5% absolute value. Code, data, and resources are publicly available for research purposes: https://github.com/Lhtie/Bio-Domain-Transfer .",
        "author": "Hongyi Liu; Qingyun Wang; Payam Karisani; Heng Ji",
        "authorids": "/h/hongyi-liu/; /q/qingyun-wang/; /p/payam-karisani/; /h/heng-ji/",
        "bibtex": "@inproceedings{liu-etal-2024-named,\n    title = \"Named Entity Recognition Under Domain Shift via Metric Learning for Life Sciences\",\n    author = \"Liu, Hongyi  and\n      Wang, Qingyun  and\n      Karisani, Payam  and\n      Ji, Heng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.1/\",\n    doi = \"10.18653/v1/2024.naacl-long.1\",\n    pages = \"1--21\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.1.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.1/",
        "pdf_size": 1127913,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7142656894490522346&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Shanghai Jiao Tong University; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "aff_domain": "sjtu.edu.cn;illinois.edu;illinois.edu;illinois.edu",
        "email": "sjtu.edu.cn;illinois.edu;illinois.edu;illinois.edu",
        "github": "https://github.com/Lhtie/Bio-Domain-Transfer",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;University of Illinois Urbana-Champaign",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://illinois.edu",
        "aff_unique_abbr": "SJTU;UIUC",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.findings-naacl.33",
        "title": "Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models trained primarily in a monolingual setting have demonstrated their ability to generalize to machine translation using zero- and few-shot examples with in-context learning. However, even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting. In this paper, we investigate the factors contributing to this gap and find that this gap can largely be closed (for about 70%) by matching the writing styles of the target corpus. Additionally, we explore potential approaches to enhance zero-shot baselines without the need for parallel demonstration examples, providing valuable insights into how these methods contribute to improving translation metrics.",
        "author": "Weiting Tan; Haoran Xu; Lingfeng Shen; Shuyue Stella Li; Kenton Murray; Philipp Koehn; Benjamin Van Durme; Yunmo Chen",
        "authorids": "/w/weiting-tan/; /h/haoran-xu/; /l/lingfeng-shen/; /s/shuyue-stella-li/; /k/kenton-murray/; /p/philipp-koehn/; /b/benjamin-van-durme/; /y/yunmo-chen/",
        "bibtex": "@inproceedings{tan-etal-2024-narrowing,\n    title = \"Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles\",\n    author = \"Tan, Weiting  and\n      Xu, Haoran  and\n      Shen, Lingfeng  and\n      Li, Shuyue Stella  and\n      Murray, Kenton  and\n      Koehn, Philipp  and\n      Van Durme, Benjamin  and\n      Chen, Yunmo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.33/\",\n    doi = \"10.18653/v1/2024.findings-naacl.33\",\n    pages = \"490--502\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.33.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.33/",
        "pdf_size": 360678,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14914822453512093575&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University",
        "aff_domain": "jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "email": "jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.173",
        "title": "Native Language Identification in Texts: A Survey",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present the first comprehensive survey of Native Language Identification (NLI) applied to texts. NLI is the task of automatically identifying an author\u2019s native language (L1) based on their second language (L2) production. NLI is an important task with practical applications in second language teaching and NLP. The task has been widely studied for both text and speech, particularly for L2 English due to the availability of suitable corpora. Speech-based NLI relies heavily on accent modeled by pronunciation patterns and prosodic cues while text-based NLI relies primarily on modeling spelling errors and grammatical patterns that reveal properties of an individuals\u2019 L1 influencing L2 production. We survey over one hundred papers on the topic including the papers associated with the NLI and INLI shared tasks. We describe several text representations and computational techniques used in text-based NLI. Finally, we present a comprehensive account of publicly available datasets used for the task thus far.",
        "author": "Dhiman Goswami; Sharanya Thilagan; Kai North; Shervin Malmasi; Marcos Zampieri",
        "authorids": "/d/dhiman-goswami/; /s/sharanya-thilagan/; /k/kai-north/; /s/shervin-malmasi/; /m/marcos-zampieri/",
        "bibtex": "@inproceedings{goswami-etal-2024-native,\n    title = \"Native Language Identification in Texts: A Survey\",\n    author = \"Goswami, Dhiman  and\n      Thilagan, Sharanya  and\n      North, Kai  and\n      Malmasi, Shervin  and\n      Zampieri, Marcos\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.173/\",\n    doi = \"10.18653/v1/2024.naacl-long.173\",\n    pages = \"3149--3160\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.173.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.173/",
        "pdf_size": 177015,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3243030752162799390&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "George Mason University; George Mason University; George Mason University; Amazon.com, Inc.; George Mason University",
        "aff_domain": "gmu.edu; ; ; ; ",
        "email": "gmu.edu; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "George Mason University;Amazon",
        "aff_unique_dep": ";Amazon.com, Inc.",
        "aff_unique_url": "https://www.gmu.edu;https://www.amazon.com",
        "aff_unique_abbr": "GMU;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.259",
        "title": "Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "How can we perform computations over natural language representations to solve tasks that require symbolic and numeric reasoning? We propose natural language embedded programs (NLEP) as a unifying framework for addressing math/symbolic reasoning, natural language understanding, and instruction following tasks. Our approach prompts a language model to generate full Python programs that define functions over data structures which contain natural language representations of structured knowledge. A Python interpreter then executes the generated code and prints the output. Despite using a task-general prompt, we find that this approach can improve upon strong baselines across a range of different tasks including math and symbolic reasoning, text classification, question answering, and instruction following. We found that the generated programs are interpretable since they outline the exact reasoning process followed by the program interpreter.",
        "author": "Tianhua Zhang; Jiaxin Ge; Hongyin Luo; Yung-Sung Chuang; Mingye Gao; Yuan Gong; Yoon Kim; Xixin Wu; Helen Meng; James Glass",
        "authorids": "/t/tianhua-zhang/; /j/jiaxin-ge/; /h/hongyin-luo/; /y/yung-sung-chuang/; /m/mingye-gao/; /y/yuan-gong/; /y/yoon-kim/; /x/xixin-wu/; /h/helen-meng/; /j/james-glass/",
        "bibtex": "@inproceedings{zhang-etal-2024-natural,\n    title = \"Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning\",\n    author = \"Zhang, Tianhua  and\n      Ge, Jiaxin  and\n      Luo, Hongyin  and\n      Chuang, Yung-Sung  and\n      Gao, Mingye  and\n      Gong, Yuan  and\n      Kim, Yoon  and\n      Wu, Xixin  and\n      Meng, Helen  and\n      Glass, James\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.259/\",\n    doi = \"10.18653/v1/2024.findings-naacl.259\",\n    pages = \"4131--4155\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.259.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.259/",
        "pdf_size": 750984,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13668199448688252438&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "The Chinese University of Hong Kong; Peking University; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology; The Chinese University of Hong Kong; Massachusetts Institute of Technology; The Chinese University of Hong Kong; Massachusetts Institute of Technology",
        "aff_domain": "se.cuhk.edu.hk;stu.pku.edu.cn;mit.edu; ; ; ;se.cuhk.edu.hk; ;se.cuhk.edu.hk;mit.edu",
        "email": "se.cuhk.edu.hk;stu.pku.edu.cn;mit.edu; ; ; ;se.cuhk.edu.hk; ;se.cuhk.edu.hk;mit.edu",
        "github": "https://github.com/luohongyin/LangCode",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;2;2;2;2;0;2;0;2",
        "aff_unique_norm": "Chinese University of Hong Kong;Peking University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cuhk.edu.hk;http://www.pku.edu.cn;https://web.mit.edu",
        "aff_unique_abbr": "CUHK;Peking U;MIT",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;1;1;1;1;0;1;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.findings-naacl.83",
        "title": "Natural Language-based State Representation in Deep Reinforcement Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper investigates the potential of using natural language descriptions as an alternative to direct image-based observations for learning policies in reinforcement learning. Due to the inherent challenges in managing image-based observations, which include abundant information and irrelevant features, we propose a method that compresses images into a natural language form for state representation. This approach allows better interpretability and leverages the processing capabilities of large-language models. We conducted several experiments involving tasks that required image-based observation. The results demonstrated that policies trained using natural language descriptions of images yield better generalization than those trained directly from images, emphasizing the potential of this approach in practical settings.",
        "author": "Md Masudur Rahman; Yexiang Xue",
        "authorids": "/m/md-masudur-rahman/; /y/yexiang-xue/",
        "bibtex": "@inproceedings{rahman-xue-2024-natural,\n    title = \"Natural Language-based State Representation in Deep Reinforcement Learning\",\n    author = \"Rahman, Md Masudur  and\n      Xue, Yexiang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.83/\",\n    doi = \"10.18653/v1/2024.findings-naacl.83\",\n    pages = \"1310--1319\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.83.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.83/",
        "pdf_size": 5321131,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_iH0u95uAe4J:scholar.google.com/&scioq=Natural+Language-based+State+Representation+in+Deep+Reinforcement+Learning&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Purdue University; Department of Computer Science, Purdue University",
        "aff_domain": "purdue.edu;purdue.edu",
        "email": "purdue.edu;purdue.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Purdue University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.purdue.edu",
        "aff_unique_abbr": "Purdue",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.57",
        "title": "Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Federated embodied agent learning protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Results on two VLN datasets (R2R and RxR) show that NAW can easily navigate the deployed VLN agent regardless of the language instruction, without affecting its performance on normal test sets. Then, we propose a new Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated VLN, which provides the server with a \u201dprompt\u201d of the vision-and-language alignment variance between the benign and malicious clients so that they can be distinguished during training. We validate the effectiveness of the PBA method on protecting the global model from the NAW attack, which outperforms other state-of-the-art defense methods by a large margin in the defense metrics on R2R and RxR.",
        "author": "Yunchao Zhang; Zonglin Di; Kaiwen Zhou; Cihang Xie; Xin Wang",
        "authorids": "/y/yunchao-zhang/; /z/zonglin-di/; /k/kaiwen-zhou/; /c/cihang-xie/; /x/xin-wang/",
        "bibtex": "@inproceedings{zhang-etal-2024-navigation,\n    title = \"Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning\",\n    author = \"Zhang, Yunchao  and\n      Di, Zonglin  and\n      Zhou, Kaiwen  and\n      Xie, Cihang  and\n      Wang, Xin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.57/\",\n    doi = \"10.18653/v1/2024.naacl-long.57\",\n    pages = \"1002--1016\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.57.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.57/",
        "pdf_size": 27342389,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2932376380467440930&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, Santa Cruz; University of California, Santa Cruz; University of California, Santa Cruz; University of California, Santa Cruz; University of California, Santa Cruz",
        "aff_domain": "ucsc.edu;ucsc.edu;ucsc.edu;ucsc.edu;ucsc.edu",
        "email": "ucsc.edu;ucsc.edu;ucsc.edu;ucsc.edu;ucsc.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of California, Santa Cruz",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsc.edu",
        "aff_unique_abbr": "UCSC",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Santa Cruz",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.281",
        "title": "NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we harvest the dramatic improvements in knowledge capabilities of language models into a large-scale comparative knowledge base. While the ease of acquisition of such comparative knowledge is much higher from extreme-scale models like GPT-4, compared to their considerably smaller and weaker counterparts such as GPT-2, not even the most powerful models are exempt from making errors. We thus ask: to what extent are models at different scales able to generate valid and diverse comparative knowledge?We introduce NeuroComparatives, a novel framework for comparative knowledge distillation overgenerated from language models such as GPT-variants and LLaMA, followed by stringent filtering of the generated knowledge. Our framework acquires comparative knowledge between everyday objects, producing a corpus of up to 8.8M comparisons over 1.74M entity pairs - 10X larger and 30% more diverse than existing resources. Moreover, human evaluations show that NeuroComparatives outperform existing resources in terms of validity (up to 32% absolute improvement). Our acquired NeuroComparatives leads to performance improvements on five downstream tasks.We find that neuro-symbolic manipulation of smaller models offers complementary benefits to the currently dominant practice of prompting extreme-scale language models for knowledge distillation.",
        "author": "Phillip Howard; Junlin Wang; Vasudev Lal; Gadi Singer; Yejin Choi; Swabha Swayamdipta",
        "authorids": "/p/phillip-howard/; /j/junlin-wang/; /v/vasudev-lal/; /g/gadi-singer/; /y/yejin-choi/; /s/swabha-swayamdipta/",
        "bibtex": "@inproceedings{howard-etal-2024-neurocomparatives,\n    title = \"{N}euro{C}omparatives: Neuro-Symbolic Distillation of Comparative Knowledge\",\n    author = \"Howard, Phillip  and\n      Wang, Junlin  and\n      Lal, Vasudev  and\n      Singer, Gadi  and\n      Choi, Yejin  and\n      Swayamdipta, Swabha\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.281/\",\n    doi = \"10.18653/v1/2024.findings-naacl.281\",\n    pages = \"4502--4520\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.281.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.281/",
        "pdf_size": 5900741,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16600888879053310470&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Intel Labs\u2662; Duke University\u2020; Intel Labs\u2662; Intel Labs\u2662; Paul G. Allen School of Computer Science & Engineering, University of Washington\u2661; Allen Institute for AI\u2663+University of Southern California\u2660",
        "aff_domain": "intel.com; ; ; ; ; ",
        "email": "intel.com; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;2;3+4",
        "aff_unique_norm": "Intel;Duke University;University of Washington;Allen Institute for AI;University of Southern California",
        "aff_unique_dep": "Intel Labs;;Paul G. Allen School of Computer Science & Engineering;;",
        "aff_unique_url": "https://www.intel.com;https://www.duke.edu;https://www.cs.washington.edu;https://allenai.org;https://www.usc.edu",
        "aff_unique_abbr": "Intel;Duke;UW;AI2;USC",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.50",
        "title": "Neurocache: Efficient Vector Retrieval for Long-range Language Modeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper introduces Neurocache, an approach to extend the effective context size of large language models (LLMs) using an external vector cache to store its past states. Like recent vector retrieval approaches, Neurocache uses an efficient k-nearest-neighbor (kNN) algorithm to retrieve relevant past states and incorporate them into the attention process. Neurocache improves upon previous methods by (1) storing compressed states, which reduces cache size; (2) performing a single retrieval operation per token which increases inference speed; and (3) extending the retrieval window to neighboring states, which improves both language modeling and downstream task accuracy. Our experiments show the effectiveness of Neurocache both for models trained from scratch and for pre-trained models such as Llama2-7B and Mistral-7B when enhanced with the cache mechanism. We also compare Neurocache with text retrieval methods and show improvements in single-document question-answering and few-shot learning tasks. We made the source code available under: https://github.com/alisafaya/neurocache",
        "author": "Ali Safaya; Deniz Yuret",
        "authorids": "/a/ali-safaya/; /d/deniz-yuret/",
        "bibtex": "@inproceedings{safaya-yuret-2024-neurocache,\n    title = \"Neurocache: Efficient Vector Retrieval for Long-range Language Modeling\",\n    author = \"Safaya, Ali  and\n      Yuret, Deniz\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.50/\",\n    doi = \"10.18653/v1/2024.naacl-long.50\",\n    pages = \"870--883\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.50.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.50/",
        "pdf_size": 940597,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1132960032706714706&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "KUIS AI Center + Computer Engineering Department, Ko\u00e7 University; KUIS AI Center + Computer Engineering Department, Ko\u00e7 University",
        "aff_domain": "ku.edu.tr;ku.edu.tr",
        "email": "ku.edu.tr;ku.edu.tr",
        "github": "https://github.com/alisafaya/neurocache",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Kwansei Gakuin University;Ko\u00e7 University",
        "aff_unique_dep": "AI Center;Computer Engineering Department",
        "aff_unique_url": "https://www.kwansei.ac.jp;https://www.ku.edu.tr",
        "aff_unique_abbr": "KUIS;Ko\u00e7",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1",
        "aff_country_unique": "Japan;T\u00fcrkiye"
    },
    {
        "id": "2024.naacl-demo.17",
        "title": "Newspaper Signaling for Crisis Prediction",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "To establish sophisticated monitoring of newspaper articles for detecting crisis-related signals, natural language processing has to cope with unstructured data, media, and cultural bias as well as multiple languages. So far, research on detecting signals in newspaper articles is focusing on structured data, restricted language settings, and isolated application domains. When considering complex crisis-related signals, a high number of diverse newspaper articles in terms of language and culture reduces potential biases. We demonstrate MENDEL \u2013 a model for multi-lingual and open-domain newspaper signaling for detecting crisis-related indicators in newspaper articles. The model works with unstructured news data and combines multiple transformer-based models for pre-processing (STANZA) and content filtering (RoBERTa, GPT-3.5). Embedded in a Question-Answering (QA) setting, MENDEL supports multiple languages (>66) and can detect early newspaper signals for open crisis domains in real-time.",
        "author": "Prajvi Saxena; Sabine Janzen; Wolfgang Maass",
        "authorids": "/p/prajvi-saxena/; /s/sabine-janzen/; /w/wolfgang-maass/",
        "bibtex": "@inproceedings{saxena-etal-2024-newspaper,\n    title = \"Newspaper Signaling for Crisis Prediction\",\n    author = \"Saxena, Prajvi  and\n      Janzen, Sabine  and\n      Maass, Wolfgang\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.17/\",\n    doi = \"10.18653/v1/2024.naacl-demo.17\",\n    pages = \"166--173\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.17.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.17/",
        "pdf_size": 547954,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1507439843773468594&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "German Research Center for Artificial Intelligence, Saarbr\u00fccken, Germany; German Research Center for Artificial Intelligence, Saarbr\u00fccken, Germany; German Research Center for Artificial Intelligence + Saarland University, Saarbr\u00fccken, Germany",
        "aff_domain": "dfki.de;dfki.de;dfki.de",
        "email": "dfki.de;dfki.de;dfki.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0+1",
        "aff_unique_norm": "German Research Center for Artificial Intelligence;Saarland University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.dfki.de;https://www.uni-saarland.de",
        "aff_unique_abbr": "DFKI;UdS",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Saarbr\u00fccken;",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.272",
        "title": "No Context Needed: Contextual Quandary In Idiomatic Reasoning With Pre-Trained Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Reasoning in the presence of idiomatic expressions (IEs) remains a challenging frontier in natural language understanding (NLU). Unlike standard text, the non-compositional nature of an IE makes it difficult for model comprehension, as their figurative or non-literal mean- ing usually cannot be inferred from the constituent words alone. It stands to reason that in these challenging circumstances, pre-trained language models (PTLMs) should make use of the surrounding context to infer additional in- formation about the IE. In this paper, we investigate the utilization of said context for idiomatic reasoning tasks, which is under-explored relative to arithmetic or commonsense reason- ing (Liu et al., 2022; Yu et al., 2023). Preliminary findings point to a surprising observation: general purpose PTLMs are actually negatively affected by the context, as performance almost always increases with its removal. In these scenarios, models may see gains of up to 3.89%. As a result, we argue that only IE-aware models remain suitable for idiomatic reasoning tasks, given the unexpected and unexplainable manner in which general purpose PTLMs reason over IEs. Additionally, we conduct studies to examine how models utilize the context in various situations, as well as an in-depth analysis on dataset formation and quality. Finally, we provide some explanations and insights into the reasoning process itself based on our results.",
        "author": "Kellen Cheng; Suma Bhat",
        "authorids": "/k/kellen-cheng/; /s/suma-bhat/",
        "bibtex": "@inproceedings{cheng-bhat-2024-context,\n    title = \"No Context Needed: Contextual Quandary In Idiomatic Reasoning With Pre-Trained Language Models\",\n    author = \"Cheng, Kellen  and\n      Bhat, Suma\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.272/\",\n    doi = \"10.18653/v1/2024.naacl-long.272\",\n    pages = \"4863--4880\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.272.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.272/",
        "pdf_size": 819240,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:L0KqNts9Zi4J:scholar.google.com/&scioq=No+Context+Needed:+Contextual+Quandary+In+Idiomatic+Reasoning+With+Pre-Trained+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Princeton University; Princeton University + University of Illinois, Urbana-Champaign",
        "aff_domain": "princeton.edu;princeton.edu",
        "email": "princeton.edu;princeton.edu",
        "github": "https://github.com/kellentan/No_Contexts",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "Princeton University;University of Illinois",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.princeton.edu;https://illinois.edu",
        "aff_unique_abbr": "Princeton;UIUC",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.93",
        "title": "Noisy Multi-Label Text Classification via Instance-Label Pair Correction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In noisy label learning, instance selection based on small-loss criteria has been proven to be highly effective. However, in the case of noisy multi-label text classification (NMLTC), the presence of noise is not limited to the instance-level but extends to the (instance-label) pair-level.This gives rise to two main challenges.(1) The loss information at the pair-level fails to capture the variations between instances. (2) There are two types of noise at the pair-level: false positives and false negatives. Identifying false negatives from a large pool of negative pairs presents an exceedingly difficult task. To tackle these issues, we propose a novel approach called instance-label pair correction (iLaCo), which aims to address the problem of noisy pair selection and correction in NMLTC tasks.Specifically, we first introduce a holistic selection metric that identifies noisy pairs by simultaneously considering global loss information and instance-specific ranking information.Secondly, we employ a filter guided by label correlation to focus exclusively on negative pairs with label relevance. This filter significantly reduces the difficulty of identifying false negatives.Experimental analysis indicates that our framework effectively corrects noisy pairs in NMLTC datasets, leading to a significant improvement in model performance.",
        "author": "Pengyu Xu; Mingyang Song; Linkaida Liu; Bing Liu; Hongjian Sun; Liping Jing; Jian Yu",
        "authorids": "/p/pengyu-xu/; /m/mingyang-song/; /l/linkaida-liu/; /b/bing-liu/; /h/hongjian-sun/; /l/liping-jing/; /j/jian-yu/",
        "bibtex": "@inproceedings{xu-etal-2024-noisy,\n    title = \"Noisy Multi-Label Text Classification via Instance-Label Pair Correction\",\n    author = \"Xu, Pengyu  and\n      Song, Mingyang  and\n      Liu, Linkaida  and\n      Liu, Bing  and\n      Sun, Hongjian  and\n      Jing, Liping  and\n      Yu, Jian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.93/\",\n    doi = \"10.18653/v1/2024.findings-naacl.93\",\n    pages = \"1446--1458\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.93.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.93/",
        "pdf_size": 819716,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16399013793068529582&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Beijing Key Lab of Traffic Data Analysis and Mining; Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China",
        "aff_domain": "bjtu.edu.cn; ; ; ; ; ; ",
        "email": "bjtu.edu.cn; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;1",
        "aff_unique_norm": "Beijing Key Lab of Traffic Data Analysis and Mining;Beijing Jiao Tong University",
        "aff_unique_dep": "Traffic Data Analysis and Mining;",
        "aff_unique_url": ";http://www.bjtu.edu.cn",
        "aff_unique_abbr": ";BJTU",
        "aff_campus_unique_index": "1;1;1;1;1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.266",
        "title": "Non-contrastive sentence representations via self-supervision",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Sample contrastive methods, typically referred to simply as contrastive are the foundation of most unsupervised methods to learn text and sentence embeddings. On the other hand, a different class of self-supervised non-contrastive loss functions and methods have been considered in the computer vision community and referred to as dimension contrastive. In this paper, we thoroughly compare this class of methods with the standard baseline for contrastive sentence embeddings, SimCSE. We find that self-supervised embeddings trained using dimension contrastive objectives can outperform SimCSE on downstream tasks without needing auxiliary loss functions.",
        "author": "Duccio Pappadopulo; Marco Farina",
        "authorids": "/d/duccio-pappadopulo/; /m/marco-farina/",
        "bibtex": "@inproceedings{pappadopulo-farina-2024-non,\n    title = \"Non-contrastive sentence representations via self-supervision\",\n    author = \"Pappadopulo, Duccio  and\n      Farina, Marco\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.266/\",\n    doi = \"10.18653/v1/2024.findings-naacl.266\",\n    pages = \"4274--4284\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.266.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.266/",
        "pdf_size": 355002,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9579141044772872533&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Bloomberg; Bloomberg",
        "aff_domain": "bloomberg.net;bloomberg.net",
        "email": "bloomberg.net;bloomberg.net",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Bloomberg",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.bloomberg.com",
        "aff_unique_abbr": "Bloomberg",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.215",
        "title": "Normalizing without Modernizing: Keeping Historical Wordforms of Middle French while Reducing Spelling Variants",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Conservation of historical documents benefits from computational methods by alleviating the manual labor related to digitization and modernization of textual content. Languages usually evolve over time and keeping historical wordforms is crucial for diachronic studies and digital humanities. However, spelling conventions did not necessarily exist when texts were originally written and orthographic variations are commonly observed depending on scribes and time periods. In this study, we propose to automatically normalize orthographic wordforms found in historical archives written in Middle French during the 16th century without fully modernizing textual content. We leverage pre-trained models in a low resource setting based on a manually curated parallel corpus and produce additional resources with artificial data generation approaches. Results show that causal language models and knowledge distillation improve over a strong baseline, thus validating the proposed methods.",
        "author": "Raphael Rubino; Johanna Gerlach; Jonathan Mutal; Pierrette Bouillon",
        "authorids": "/r/raphael-rubino/; /j/johanna-gerlach/; /j/jonathan-mutal/; /p/pierrette-bouillon/",
        "bibtex": "@inproceedings{rubino-etal-2024-normalizing,\n    title = \"Normalizing without Modernizing: Keeping Historical Wordforms of {M}iddle {F}rench while Reducing Spelling Variants\",\n    author = \"Rubino, Raphael  and\n      Gerlach, Johanna  and\n      Mutal, Jonathan  and\n      Bouillon, Pierrette\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.215/\",\n    doi = \"10.18653/v1/2024.findings-naacl.215\",\n    pages = \"3394--3402\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.215.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.215/",
        "pdf_size": 226179,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15981637273618464900&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "TIM/FTI, University of Geneva; TIM/FTI, University of Geneva; TIM/FTI, University of Geneva; TIM/FTI, University of Geneva",
        "aff_domain": "unige.ch;unige.ch;unige.ch;unige.ch",
        "email": "unige.ch;unige.ch;unige.ch;unige.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Geneva",
        "aff_unique_dep": "TIM/FTI",
        "aff_unique_url": "https://www.unige.ch",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2024.naacl-long.367",
        "title": "Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Most research about natural language generation (NLG) relies on evaluation benchmarks with limited references for a sample, which may result in poor correlations with human judgements. The underlying reason is that one semantic meaning can actually be expressed in different forms, and the evaluation with a single or few references may not accurately reflect the quality of the model\u2019s hypotheses. To address this issue, this paper presents a simple and effective method, named **Div-Ref**, to enhance existing evaluation benchmarks by enriching the number of references. We leverage large language models (LLMs) to diversify the expression of a single reference into multiple high-quality ones to cover the semantic space of the reference sentence as much as possible. We conduct comprehensive experiments to empirically demonstrate that diversifying the expression of reference can significantly enhance the correlation between automatic evaluation and human evaluation. This idea is compatible with recent LLM-based evaluation which can similarly derive advantages from incorporating multiple references. *We strongly encourage future generation benchmarks to include more references, even if they are generated by LLMs, which is once for all.* We release all the code and data at https://github.com/RUCAIBox/Div-Ref to facilitate research.",
        "author": "Tianyi Tang; Hongyuan Lu; Yuchen Jiang; Haoyang Huang; Dongdong Zhang; Xin Zhao; Tom Kocmi; Furu Wei",
        "authorids": "/t/tianyi-tang/; /h/hongyuan-lu/; /y/yuchen-jiang/; /h/haoyang-huang/; /d/dongdong-zhang/; /w/wayne-xin-zhao/; /t/tom-kocmi/; /f/furu-wei/",
        "bibtex": "@inproceedings{tang-etal-2024-metrics,\n    title = \"Not All Metrics Are Guilty: Improving {NLG} Evaluation by Diversifying References\",\n    author = \"Tang, Tianyi  and\n      Lu, Hongyuan  and\n      Jiang, Yuchen  and\n      Huang, Haoyang  and\n      Zhang, Dongdong  and\n      Zhao, Xin  and\n      Kocmi, Tom  and\n      Wei, Furu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.367/\",\n    doi = \"10.18653/v1/2024.naacl-long.367\",\n    pages = \"6596--6610\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.367.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.367/",
        "pdf_size": 481801,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:9SSkRhMo5ZUJ:scholar.google.com/&scioq=Not+All+Metrics+Are+Guilty:+Improving+NLG+Evaluation+by+Diversifying+References&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "Gaoling School of Artificial Intelligence, Renmin University of China; The Chinese University of Hong Kong; AIWaves Inc.; Microsoft Research Asia, China; Microsoft Research Asia, China; Gaoling School of Artificial Intelligence, Renmin University of China + Beijing Key Laboratory of Big Data Management and Analysis Methods; Microsoft; Microsoft Research Asia, China",
        "aff_domain": "outlook.com;outlook.com;aiwaves.cn;microsoft.com;microsoft.com;gmail.com;microsoft.com;microsoft.com",
        "email": "outlook.com;outlook.com;aiwaves.cn;microsoft.com;microsoft.com;gmail.com;microsoft.com;microsoft.com",
        "github": "https://github.com/RUCAIBox/Div-Ref",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;3;3;0+4;3;3",
        "aff_unique_norm": "Renmin University of China;Chinese University of Hong Kong;AIWaves Inc.;Microsoft;Beijing Key Laboratory of Big Data Management and Analysis Methods",
        "aff_unique_dep": "Gaoling School of Artificial Intelligence;;;Microsoft Research Asia;Big Data Management and Analysis",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.cuhk.edu.hk;;https://www.microsoft.com/en-us/research/group/asia;",
        "aff_unique_abbr": "RUC;CUHK;;MSRA;",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Beijing;Hong Kong SAR;",
        "aff_country_unique_index": "0;0;1;0;0;0+0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.244",
        "title": "ODD: A Benchmark Dataset for the Natural Language Processing Based Opioid Related Aberrant Behavior Detection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Opioid related aberrant behaviors (ORABs) present novel risk factors for opioid overdose. This paper introduces a novel biomedical natural language processing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset designed to identify ORABs from patients\u2019 EHR notes and classify them into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiazepines, 7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants of Health. We explored two state-of-the-art natural language processing models (fine-tuning and prompt-tuning approaches) to identify ORAB. Experimental results show that the prompt-tuning models outperformed the fine-tuning models in most categories and the gains were especially higher among uncommon categories (Suggested Aberrant Behavior, Confirmed Aberrant Behaviors, Diagnosed Opioid Dependence, and Medication Change). Although the best model achieved the highest 88.17% on macro average area under precision recall curve, uncommon classes still have a large room for performance improvement. ODD is publicly available.",
        "author": "Sunjae Kwon; Xun Wang; Weisong Liu; Emily Druhl; Minhee Sung; Joel Reisman; Wenjun Li; Robert Kerns; William Becker; Hong Yu",
        "authorids": "/s/sunjae-kwon/; /x/xun-wang/; /w/weisong-liu/; /e/emily-druhl/; /m/minhee-sung/; /j/joel-reisman/; /w/wenjun-li/; /r/robert-kerns/; /w/william-becker/; /h/hong-yu/",
        "bibtex": "@inproceedings{kwon-etal-2024-odd,\n    title = \"{ODD}: A Benchmark Dataset for the Natural Language Processing Based Opioid Related Aberrant Behavior Detection\",\n    author = \"Kwon, Sunjae  and\n      Wang, Xun  and\n      Liu, Weisong  and\n      Druhl, Emily  and\n      Sung, Minhee  and\n      Reisman, Joel  and\n      Li, Wenjun  and\n      Kerns, Robert  and\n      Becker, William  and\n      Yu, Hong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.244/\",\n    doi = \"10.18653/v1/2024.naacl-long.244\",\n    pages = \"4338--4359\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.244.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.244/",
        "pdf_size": 387758,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14080977237663306238&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "UMass Amherst; Microsoft; UMass Lowell; U.S. Department of Veterans Affairs; Yale University; UMass Chan Medical School; UMass Lowell; Yale University; Yale University; UMass Amherst+UMass Lowell+U.S. Department of Veterans Affairs+UMass Chan Medical School",
        "aff_domain": "umass.edu;gmail.com;uml.edu;va.gov;yale.edu;va.gov;uml.edu;yale.edu;yale.edu;uml.edu",
        "email": "umass.edu;gmail.com;uml.edu;va.gov;yale.edu;va.gov;uml.edu;yale.edu;yale.edu;uml.edu",
        "github": "https://github.com/soon91jae/ORAB_MIMIC",
        "project": "https://www.physionet.org/content/nlp-opioid-behavior-detection/1.0.0/",
        "author_num": 10,
        "aff_unique_index": "0;1;2;3;4;5;2;4;4;0+2+3+5",
        "aff_unique_norm": "University of Massachusetts Amherst;Microsoft;University of Massachusetts Lowell;U.S. Department of Veterans Affairs;Yale University;University of Massachusetts Chan Medical School",
        "aff_unique_dep": ";Microsoft Corporation;;;;Medical School",
        "aff_unique_url": "https://www.umass.edu;https://www.microsoft.com;https://www.uml.edu;https://www.va.gov;https://www.yale.edu;https://umassmed.edu",
        "aff_unique_abbr": "UMass Amherst;Microsoft;UMass Lowell;VA;Yale;UMass Chan",
        "aff_campus_unique_index": "0;2;3;2;0+2+3",
        "aff_campus_unique": "Amherst;;Lowell;Chan",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0+0+0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.226",
        "title": "OSCaR: Object State Captioning and State Change Representation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating Multimodal Large Language Models (MLLMs). Our experiments demonstrate that while MLLMs show some skill, they lack a full understanding of object state changes. The benchmark includes a fine-tuned model that, despite initial capabilities, requires significant improvements in accuracy and generalization ability for effective understanding of these changes. Our code and dataset are available at https://github.com/nguyennm1024/OSCaR.",
        "author": "Nguyen Nguyen; Jing Bi; Ali Vosoughi; Yapeng Tian; Pooyan Fazli; Chenliang Xu",
        "authorids": "/n/nguyen-nguyen/; /j/jing-bi/; /a/ali-vosoughi/; /y/yapeng-tian/; /p/pooyan-fazli/; /c/chenliang-xu/",
        "bibtex": "@inproceedings{nguyen-etal-2024-oscar,\n    title = \"{OSC}a{R}: Object State Captioning and State Change Representation\",\n    author = \"Nguyen, Nguyen  and\n      Bi, Jing  and\n      Vosoughi, Ali  and\n      Tian, Yapeng  and\n      Fazli, Pooyan  and\n      Xu, Chenliang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.226/\",\n    doi = \"10.18653/v1/2024.findings-naacl.226\",\n    pages = \"3565--3576\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.226.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.226/",
        "pdf_size": 677489,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11533533633191139937&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Rochester; University of Rochester; University of Rochester; University of Texas at Dallas; Arizona State University; University of Rochester",
        "aff_domain": "rochester.edu;rochester.edu;rochester.edu;utdallas.edu;asu.edu;rochester.edu",
        "email": "rochester.edu;rochester.edu;rochester.edu;utdallas.edu;asu.edu;rochester.edu",
        "github": "https://github.com/nguyennm1024/OSCaR",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;2;0",
        "aff_unique_norm": "University of Rochester;University of Texas at Dallas;Arizona State University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.rochester.edu;https://www.utdallas.edu;https://www.asu.edu",
        "aff_unique_abbr": "U of R;UT Dallas;ASU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Dallas",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.218",
        "title": "Okay, Let\u2019s Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In NLP, Event Coreference Resolution (ECR) is the task of connecting event clusters that refer to the same underlying real-life event, usually via neural systems. In this work, we investigate using abductive free-text rationales (FTRs) generated by modern autoregressive LLMs as distant supervision of smaller student models for cross-document coreference (CDCR) of events. We implement novel rationale-oriented event clustering and knowledge distillation methods for event coreference scoring that leverage enriched information from the FTRs for improved CDCR without additional annotation or expensive document clustering. Our model using coreference-specific knowledge distillation achieves SOTA B3 F1 on the ECB+ and GVC corpora and we establish a new baseline on the AIDA Phase 1 corpus. Our code can be found at https://github.com/csu-signal/llama_cdcr.",
        "author": "Abhijnan Nath; Shadi Manafi Avari; Avyakta Chelle; Nikhil Krishnaswamy",
        "authorids": "/a/abhijnan-nath/; /s/shadi-manafi-avari/; /a/avyakta-chelle/; /n/nikhil-krishnaswamy/",
        "bibtex": "@inproceedings{nath-etal-2024-okay,\n    title = \"Okay, Let{'}s Do This! Modeling Event Coreference with Generated Rationales and Knowledge Distillation\",\n    author = \"Nath, Abhijnan  and\n      Manafi Avari, Shadi  and\n      Chelle, Avyakta  and\n      Krishnaswamy, Nikhil\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.218/\",\n    doi = \"10.18653/v1/2024.naacl-long.218\",\n    pages = \"3931--3946\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.218.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.218/",
        "pdf_size": 970382,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6304361671083740699&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Situated Grounding and Natural Language (SIGNAL) Lab, Department of Computer Science, Colorado State University, Fort Collins, CO, USA; Situated Grounding and Natural Language (SIGNAL) Lab, Department of Computer Science, Colorado State University, Fort Collins, CO, USA; Situated Grounding and Natural Language (SIGNAL) Lab, Department of Computer Science, Colorado State University, Fort Collins, CO, USA; Situated Grounding and Natural Language (SIGNAL) Lab, Department of Computer Science, Colorado State University, Fort Collins, CO, USA",
        "aff_domain": "colostate.edu; ; ;colostate.edu",
        "email": "colostate.edu; ; ;colostate.edu",
        "github": "https://github.com/csu-signal/llama_cdcr",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Colorado State University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.colostate.edu",
        "aff_unique_abbr": "CSU",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Fort Collins",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.284",
        "title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This study introduces a new long-form database question answering dataset designed to evaluate how Large Language Models (LLMs) interact with a SQL interpreter. The task necessitates LLMs to strategically generate multiple SQL queries to retrieve sufficient data from a database, to reason with the acquired context, and to synthesize them into a comprehensive analytical narrative. Our findings highlight that this task poses great challenges even for the state-of-the-art **GPT-4** model. We propose and evaluate two interaction strategies, and provide a fine-grained analysis of the individual stages within the interaction. A key discovery is the identification of two primary bottlenecks hindering effective interaction: the capacity for planning and the ability to generate multiple SQL queries. To address the challenge of accurately assessing answer quality, we introduce a multi-agent evaluation framework that simulates the academic peer-review process, enhancing the precision and reliability of our evaluations. This framework allows for a more nuanced understanding of the strengths and limitations of current LLMs in complex retrieval and reasoning tasks.",
        "author": "Linyong Nan; Ellen Zhang; Weijin Zou; Yilun Zhao; Wenfei Zhou; Arman Cohan",
        "authorids": "/l/linyong-nan/; /e/ellen-zhang/; /w/weijin-zou/; /y/yilun-zhao/; /w/wenfei-zhou/; /a/arman-cohan/",
        "bibtex": "@inproceedings{nan-etal-2024-evaluating,\n    title = \"On Evaluating the Integration of Reasoning and Action in {LLM} Agents with Database Question Answering\",\n    author = \"Nan, Linyong  and\n      Zhang, Ellen  and\n      Zou, Weijin  and\n      Zhao, Yilun  and\n      Zhou, Wenfei  and\n      Cohan, Arman\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.284/\",\n    doi = \"10.18653/v1/2024.findings-naacl.284\",\n    pages = \"4556--4579\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.284.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.284/",
        "pdf_size": 594593,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15360172534360200132&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Yale University; Yale University; LinkedIn; Yale University; NVIDIA Corporation; Yale University",
        "aff_domain": "yale.edu;yale.edu; ; ; ;yale.edu",
        "email": "yale.edu;yale.edu; ; ; ;yale.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;2;0",
        "aff_unique_norm": "Yale University;LinkedIn Corporation;NVIDIA",
        "aff_unique_dep": ";;NVIDIA Corporation",
        "aff_unique_url": "https://www.yale.edu;https://www.linkedin.com;https://www.nvidia.com",
        "aff_unique_abbr": "Yale;LinkedIn;NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.60",
        "title": "On Large Language Models\u2019 Hallucination with Regard to Known Facts",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen.Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space.We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token\u2019s information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model.Leveraging the dynamic curve as a feature, we build a classifier capable of accurately detecting hallucinatory predictions with an 88% success rate. Our study shed light on understanding the reasons for LLMs\u2019 hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.",
        "author": "Che Jiang; Biqing Qi; Xiangyu Hong; Dayuan Fu; Yang Cheng; Fandong Meng; Mo Yu; Bowen Zhou; Jie Zhou",
        "authorids": "/c/che-jiang/; /b/biqing-qi/; /x/xiangyu-hong/; /d/dayuan-fu/; /y/yang-cheng/; /f/fandong-meng/; /m/mo-yu/; /b/bowen-zhou/; /j/jie-zhou/",
        "bibtex": "@inproceedings{jiang-etal-2024-large,\n    title = \"On Large Language Models' Hallucination with Regard to Known Facts\",\n    author = \"Jiang, Che  and\n      Qi, Biqing  and\n      Hong, Xiangyu  and\n      Fu, Dayuan  and\n      Cheng, Yang  and\n      Meng, Fandong  and\n      Yu, Mo  and\n      Zhou, Bowen  and\n      Zhou, Jie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.60/\",\n    doi = \"10.18653/v1/2024.naacl-long.60\",\n    pages = \"1041--1053\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.60.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.60/",
        "pdf_size": 2868056,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1364928176298382401&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Electronic Engineering, Tsinghua University; Department of Electronic Engineering, Tsinghua University; Department of Electronic Engineering, Tsinghua University; Department of Electronic Engineering, Tsinghua University; Department of Electronic Engineering, Tsinghua University; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Pattern Recognition Center, WeChat AI, Tencent Inc, China; Department of Electronic Engineering, Tsinghua University; Pattern Recognition Center, WeChat AI, Tencent Inc, China",
        "aff_domain": "mails.tsinghua.edu.cn; ; ; ; ;global.tencent.com;global.tencent.com;tsinghua.edu.cn; ",
        "email": "mails.tsinghua.edu.cn; ; ; ; ;global.tencent.com;global.tencent.com;tsinghua.edu.cn; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;1;1;0;1",
        "aff_unique_norm": "Tsinghua University;Tencent",
        "aff_unique_dep": "Department of Electronic Engineering;Pattern Recognition Center, WeChat AI",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.tencent.com",
        "aff_unique_abbr": "THU;Tencent",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.478",
        "title": "On Learning to Summarize with Large Language Models as References",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent studies have found that summaries generated by large language models (LLMs) are favored by human annotators over the original reference summaries in commonly used summarization datasets. Therefore, we study an LLM-as-reference learning setting for smaller text summarization models to investigate whether their performance can be substantially improved. To this end, we use LLMs as both oracle summary generators for standard supervised fine-tuning and oracle summary evaluators for efficient contrastive learning that leverages the LLMs\u2019 supervision signals. We conduct comprehensive experiments with source news articles and find that (1) summarization models trained under the LLM-as-reference setting achieve significant performance improvement in both LLM and human evaluations; (2) contrastive learning outperforms standard supervised fine-tuning under both low and high resource settings. Our experimental results also enable a meta-analysis of LLMs\u2019 summary evaluation capacities under a challenging setting, showing that LLMs are not well-aligned with human evaluators. Particularly, our expert human evaluation reveals remaining nuanced performance gaps between LLMs and our fine-tuned models, which LLMs fail to capture. Thus, we call for further studies into both the potential and challenges of using LLMs in summarization model development.",
        "author": "Yixin Liu; Kejian Shi; Katherine He; Longtian Ye; Alexander Fabbri; Pengfei Liu; Dragomir Radev; Arman Cohan",
        "authorids": "/y/yixin-liu/; /k/kejian-shi/; /k/katherine-he/; /l/longtian-ye/; /a/alexander-richard-fabbri/; /p/pengfei-liu/; /d/dragomir-radev/; /a/arman-cohan/",
        "bibtex": "@inproceedings{liu-etal-2024-learning,\n    title = \"On Learning to Summarize with Large Language Models as References\",\n    author = \"Liu, Yixin  and\n      Shi, Kejian  and\n      He, Katherine  and\n      Ye, Longtian  and\n      Fabbri, Alexander  and\n      Liu, Pengfei  and\n      Radev, Dragomir  and\n      Cohan, Arman\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.478/\",\n    doi = \"10.18653/v1/2024.naacl-long.478\",\n    pages = \"8647--8664\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.478.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.478/",
        "pdf_size": 372662,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3489817904748450891&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Yale University; Yale University; Yale University; Yale University; Salesforce AI; Shanghai Jiao Tong University; Yale University; Yale University+Allen Institute for AI",
        "aff_domain": "yale.edu; ; ; ; ; ;yale.edu;yale.edu",
        "email": "yale.edu; ; ; ; ; ;yale.edu;yale.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;0;1;2;0;0+3",
        "aff_unique_norm": "Yale University;Salesforce;Shanghai Jiao Tong University;Allen Institute for AI",
        "aff_unique_dep": ";Salesforce AI;;",
        "aff_unique_url": "https://www.yale.edu;https://www.salesforce.com;https://www.sjtu.edu.cn;https://allenai.org",
        "aff_unique_abbr": "Yale;Salesforce;SJTU;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;1;0;0+0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.8",
        "title": "On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Structured data, prevalent in tables, databases, and knowledge graphs, poses a significant challenge in its representation. With the advent of large language models (LLMs), there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly model structure, often as a graph. Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear.This work investigates the linear handling of structured data in encoder-decoder language models, specifically T5. Our findings reveal the model\u2019s ability to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing. We also uncover insights into the model\u2019s internal mechanisms, including the ego-centric nature of structure node encodings and the potential for model compression due to modality fusion redundancy. Overall, this work sheds light on the inner workings of linearization-based methods and could potentially provide guidance for future research.",
        "author": "Yutong Shao; Ndapa Nakashole",
        "authorids": "/y/yutong-shao/; /n/ndapandula-nakashole/",
        "bibtex": "@inproceedings{shao-nakashole-2024-linearizing,\n    title = \"On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-{SQL}\",\n    author = \"Shao, Yutong  and\n      Nakashole, Ndapa\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.8/\",\n    doi = \"10.18653/v1/2024.naacl-long.8\",\n    pages = \"131--156\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.8.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.8/",
        "pdf_size": 4040655,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11174277665644851796&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Computer Science and Engineering, University of California, San Diego; Computer Science and Engineering, University of California, San Diego",
        "aff_domain": "eng.ucsd.edu;eng.ucsd.edu",
        "email": "eng.ucsd.edu;eng.ucsd.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.73",
        "title": "On Narrative Question Answering Skills",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Narrative Question Answering is an important task for evaluating and improving reading comprehension abilities in both humans and machines. However, there is a lack of consensus on the skill taxonomy that would enable systematic and comprehensive assessment and learning of the various aspects of Narrative Question Answering. Existing task-level skill views oversimplify the multidimensional nature of tasks, while question-level taxonomies face issues in evaluation and methodology. To address these challenges, we introduce a more inclusive skill taxonomy that synthesizes and redefines narrative understanding skills from previous taxonomies and includes a generation skill dimension from the answering perspective.",
        "author": "Emil Kalbaliyev; Kairit Sirts",
        "authorids": "/e/emil-kalbaliyev/; /k/kairit-sirts/",
        "bibtex": "@inproceedings{kalbaliyev-sirts-2024-narrative,\n    title = \"On Narrative Question Answering Skills\",\n    author = \"Kalbaliyev, Emil  and\n      Sirts, Kairit\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.73/\",\n    doi = \"10.18653/v1/2024.naacl-short.73\",\n    pages = \"814--820\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.73.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.73/",
        "pdf_size": 201498,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:GdikLu_r9uAJ:scholar.google.com/&scioq=On+Narrative+Question+Answering+Skills&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Institute of Computer Science, University of Tartu, Tartu, Estonia; Institute of Computer Science, University of Tartu, Tartu, Estonia",
        "aff_domain": "ut.ee;ut.ee",
        "email": "ut.ee;ut.ee",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Tartu",
        "aff_unique_dep": "Institute of Computer Science",
        "aff_unique_url": "https://www.ut.ee",
        "aff_unique_abbr": "UT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tartu",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Estonia"
    },
    {
        "id": "2024.naacl-short.21",
        "title": "On Retrieval Augmentation and the Limitations of Language Model Training",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Augmenting a language model (LM) with k-nearest neighbors (kNN) retrieval on its training data alone can decrease its perplexity, though the underlying reasons for this remain elusive. In this work, we rule out one previously posited possibility \u2014 the \u201csoftmax bottleneck.\u201d We then create a new dataset to evaluate LM generalization ability in the setting where training data contains additional information that is not causally relevant. This task is challenging even for GPT-3.5 Turbo. We show that, for both GPT-2 and Mistral 7B, kNN retrieval augmentation consistently improves per formance in this setting. Finally, to make kNN retrieval more accessible, we propose using amulti-layer perceptron model that maps datastore keys to values as a drop-in replacement for traditional retrieval. This reduces storage costsby over 25x.",
        "author": "Ting-Rui Chiang; Xinyan Yu; Joshua Robinson; Ollie Liu; Isabelle Lee; Dani Yogatama",
        "authorids": "/t/ting-rui-chiang/; /x/xinyan-yu/; /j/joshua-robinson/; /o/ollie-liu/; /i/isabelle-lee/; /d/dani-yogatama/",
        "bibtex": "@inproceedings{chiang-etal-2024-retrieval,\n    title = \"On Retrieval Augmentation and the Limitations of Language Model Training\",\n    author = \"Chiang, Ting-Rui  and\n      Yu, Xinyan  and\n      Robinson, Joshua  and\n      Liu, Ollie  and\n      Lee, Isabelle  and\n      Yogatama, Dani\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.21/\",\n    doi = \"10.18653/v1/2024.naacl-short.21\",\n    pages = \"229--238\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.21.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.21/",
        "pdf_size": 338487,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:NXOO-JuKuMYJ:scholar.google.com/&scioq=On+Retrieval+Augmentation+and+the+Limitations+of+Language+Model+Training&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "University of Southern California; University of Southern California; University of Southern California; University of Southern California; University of Southern California; University of Southern California",
        "aff_domain": "usc.edu;usc.edu;usc.edu;usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu;usc.edu;usc.edu;usc.edu",
        "github": "https://github.com/usc-tamagotchi/on-knnlm",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.386",
        "title": "On the Effectiveness of Adversarial Robustness for Abuse Mitigation with Counterspeech",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work on automated approaches to counterspeech have mostly focused on synthetic data but seldom look into how the public deals with abuse. While these systems identifying and generating counterspeech have the potential for abuse mitigation, it remains unclear how robust a model is against adversarial attacks across multiple domains and how models trained on synthetic data can handle unseen user-generated abusive content in the real world. To tackle these issues, this paper first explores the dynamics of abuse and replies using our novel dataset of 6,955 labelled tweets targeted at footballers for studying public figure abuse. We then curate DynaCounter, a new English dataset of 1,911 pairs of abuse and replies addressing nine minority identity groups, collected in an adversarial human-in-the-loop process over four rounds. Our analysis shows that adversarial attacks do not necessarily result in better generalisation. We further present a study of multi-domain counterspeech generation, comparing Flan-T5 and T5 models. We observe that handling certain abuse targets is particularly challenging.",
        "author": "Yi-Ling Chung; Jonathan Bright",
        "authorids": "/y/yi-ling-chung/; /j/jonathan-bright/",
        "bibtex": "@inproceedings{chung-bright-2024-effectiveness,\n    title = \"On the Effectiveness of Adversarial Robustness for Abuse Mitigation with Counterspeech\",\n    author = \"Chung, Yi-Ling  and\n      Bright, Jonathan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.386/\",\n    doi = \"10.18653/v1/2024.naacl-long.386\",\n    pages = \"6988--7002\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.386.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.386/",
        "pdf_size": 1042154,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:BXc7MNqfnD0J:scholar.google.com/&scioq=On+the+Effectiveness+of+Adversarial+Robustness+for+Abuse+Mitigation+with+Counterspeech&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "The Alan Turing Institute; The Alan Turing Institute",
        "aff_domain": "turing.ac.uk;turing.ac.uk",
        "email": "turing.ac.uk;turing.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Alan Turing Institute",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.turing.ac.uk",
        "aff_unique_abbr": "ATI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-long.384",
        "title": "On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Current decoder-based pre-trained language models (PLMs) successfully demonstrate multilingual capabilities. However, it is unclear how these models handle multilingualism.We analyze the neuron-level internal behavior of multilingual decoder-based PLMs, Specifically examining the existence of neurons that fire \u201cuniquely for each language\u201d within decoder-only multilingual PLMs.We analyze six languages: English, German, French, Spanish, Chinese, and Japanese, and show that language-specific neurons are unique, with a slight overlap (< 5%) between languages. These neurons are mainly distributed in the models\u2019 first and last few layers. This trend remains consistent across languages and models.Additionally, we tamper with less than 1% of the total neurons in each model during inference and demonstrate that tampering with a few language-specific neurons drastically changes the probability of target language occurrence in text generation.",
        "author": "Takeshi Kojima; Itsuki Okimura; Yusuke Iwasawa; Hitomi Yanaka; Yutaka Matsuo",
        "authorids": "/t/takeshi-kojima/; /i/itsuki-okimura/; /y/yusuke-iwasawa/; /h/hitomi-yanaka/; /y/yutaka-matsuo/",
        "bibtex": "@inproceedings{kojima-etal-2024-multilingual,\n    title = \"On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons\",\n    author = \"Kojima, Takeshi  and\n      Okimura, Itsuki  and\n      Iwasawa, Yusuke  and\n      Yanaka, Hitomi  and\n      Matsuo, Yutaka\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.384/\",\n    doi = \"10.18653/v1/2024.naacl-long.384\",\n    pages = \"6919--6971\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.384.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.384/",
        "pdf_size": 13575630,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16763244468442230438&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The University of Tokyo; The University of Tokyo; The University of Tokyo; The University of Tokyo; The University of Tokyo",
        "aff_domain": "weblab.t.u-tokyo.ac.jp; ; ; ; ",
        "email": "weblab.t.u-tokyo.ac.jp; ; ; ; ",
        "github": "https://github.com/kojima-takeshi188/lang_neuron",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Tokyo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2024.naacl-short.25",
        "title": "On the Role of Summary Content Units in Text Summarization Evaluation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "At the heart of the Pyramid evaluation method for text summarization lie human written summary content units (SCUs). These SCUs areconcise sentences that decompose a summary into small facts. Such SCUs can be used to judge the quality of a candidate summary, possibly partially automated via natural language inference (NLI) systems. Interestingly, with the aim to fully automate the Pyramid evaluation, Zhang and Bansal (2021) show that SCUs can be approximated by automatically generated semantic role triplets (STUs). However, several questions currently lack answers, in particular: i) Are there other ways of approximating SCUs that can offer advantages?ii) Under which conditions are SCUs (or their approximations) offering the most value? In this work, we examine two novel strategiesto approximate SCUs: generating SCU approximations from AMR meaning representations (SMUs) and from large language models (SGUs), respectively. We find that while STUs and SMUs are competitive, the best approximation quality is achieved by SGUs. We also show through a simple sentence-decomposition baseline (SSUs) that SCUs (and their approximations) offer the most value when rankingshort summaries, but may not help as much when ranking systems or longer summaries.",
        "author": "Marcel Nawrath; Agnieszka Nowak; Tristan Ratz; Danilo Walenta; Juri Opitz; Leonardo Ribeiro; Jo\u00e3o Sedoc; Daniel Deutsch; Simon Mille; Yixin Liu; Sebastian Gehrmann; Lining Zhang; Saad Mahamood; Miruna Clinciu; Khyathi Chandu; Yufang Hou",
        "authorids": "/m/marcel-nawrath/; /a/agnieszka-nowak/; /t/tristan-ratz/; /d/danilo-walenta/; /j/juri-opitz/; /l/leonardo-ribeiro/; /j/joao-sedoc/; /d/daniel-deutsch/; /s/simon-mille/; /y/yixin-liu/; /s/sebastian-gehrmann/; /l/lining-zhang/; /s/saad-mahamood/; /m/miruna-clinciu/; /k/khyathi-chandu/; /y/yufang-hou/",
        "bibtex": "@inproceedings{nawrath-etal-2024-role,\n    title = \"On the Role of Summary Content Units in Text Summarization Evaluation\",\n    author = \"Nawrath, Marcel  and\n      Nowak, Agnieszka  and\n      Ratz, Tristan  and\n      Walenta, Danilo  and\n      Opitz, Juri  and\n      Ribeiro, Leonardo  and\n      Sedoc, Jo{\\~a}o  and\n      Deutsch, Daniel  and\n      Mille, Simon  and\n      Liu, Yixin  and\n      Gehrmann, Sebastian  and\n      Zhang, Lining  and\n      Mahamood, Saad  and\n      Clinciu, Miruna  and\n      Chandu, Khyathi  and\n      Hou, Yufang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.25/\",\n    doi = \"10.18653/v1/2024.naacl-short.25\",\n    pages = \"272--281\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.25.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.25/",
        "pdf_size": 338775,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=516430777530039779&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 16
    },
    {
        "id": "2024.naacl-short.38",
        "title": "On the True Distribution Approximation of Minimum Bayes-Risk Decoding",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in text generation.MBR decoding considers texts sampled from a model as pseudo-references and selects the text with the highest similarity to the others.Therefore, sampling is one of the key elements of MBR decoding, and previous studies reported that the performance varies by sampling methods.From a theoretical standpoint, this performance variation is likely tied to how closely the samples approximate the true distribution of references.However, this approximation has not been the subject of in-depth study.In this study, we propose using anomaly detection to measure the degree of approximation.We first closely examine the performance variation and then show that previous hypotheses about samples do not correlate well with the variation, but our introduced anomaly scores do.The results are the first to empirically support the link between the performance and the core assumption of MBR decoding.",
        "author": "Atsumoto Ohashi; Ukyo Honda; Tetsuro Morimura; Yuu Jinnai",
        "authorids": "/a/atsumoto-ohashi/; /u/ukyo-honda/; /t/tetsuro-morimura/; /y/yuu-jinnai/",
        "bibtex": "@inproceedings{ohashi-etal-2024-true,\n    title = \"On the True Distribution Approximation of Minimum {B}ayes-Risk Decoding\",\n    author = \"Ohashi, Atsumoto  and\n      Honda, Ukyo  and\n      Morimura, Tetsuro  and\n      Jinnai, Yuu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.38/\",\n    doi = \"10.18653/v1/2024.naacl-short.38\",\n    pages = \"459--468\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.38.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.38/",
        "pdf_size": 409387,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13710999263426948022&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Nagoya University; CyberAgent; CyberAgent; CyberAgent",
        "aff_domain": "s.mail.nagoya-u.ac.jp;cyberagent.co.jp;cyberagent.co.jp;cyberagent.co.jp",
        "email": "s.mail.nagoya-u.ac.jp;cyberagent.co.jp;cyberagent.co.jp;cyberagent.co.jp",
        "github": "https://github.com/CyberAgentAILab/mbr-anomaly",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "Nagoya University;CyberAgent",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nagoya-u.ac.jp;https://www.cyberagent.co.jp",
        "aff_unique_abbr": "Nagoya U;CA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2024.findings-naacl.290",
        "title": "On the Way to Gentle AI Counselor: Politeness Cause Elicitation and Intensity Tagging in Code-mixed Hinglish Conversations for Social Good",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Politeness is a multifaceted concept influenced by individual perceptions of what is considered polite or impolite. With this objective, we introduce a novel task - Politeness Cause Elicitation and Intensity Tagging (PCEIT). This task focuses on conversations and aims to identify the underlying reasons behind the use of politeness and gauge the degree of politeness conveyed. To address this objective, we create HING-POEM, a new conversational dataset in Hinglish (a blend of Hindi and English) for mental health and legal counseling of crime victims. The rationale for the domain selection lies in the paramount importance of politeness in mental health and legal counseling of crime victims to ensure a compassionate and cordial atmosphere for them. We enrich the HING-POEM dataset by annotating it with politeness labels, politeness causal spans, and intensity values at the level of individual utterances. In the context of the introduced PCEIT task, we present PAANTH (Politeness CAuse ElicitAion and INtensity Tagging in Hinglish), a comprehensive framework based on Contextual Enhanced Attentive Convolution Transformer. We conduct extensive quantitative and qualitative evaluations to establish the effectiveness of our proposed approach using the newly constructed dataset. Our approach is compared against state-of-the-art baselines, and these analyses help demonstrate the superiority of our method.",
        "author": "Priyanshu Priya; Gopendra Singh; Mauajama Firdaus; Jyotsna Agrawal; Asif Ekbal",
        "authorids": "/p/priyanshu-priya/; /g/gopendra-singh/; /m/mauajama-firdaus/; /j/jyotsna-agrawal/; /a/asif-ekbal/",
        "bibtex": "@inproceedings{priya-etal-2024-way,\n    title = \"On the Way to Gentle {AI} Counselor: Politeness Cause Elicitation and Intensity Tagging in Code-mixed {H}inglish Conversations for Social Good\",\n    author = \"Priya, Priyanshu  and\n      Singh, Gopendra  and\n      Firdaus, Mauajama  and\n      Agrawal, Jyotsna  and\n      Ekbal, Asif\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.290/\",\n    doi = \"10.18653/v1/2024.findings-naacl.290\",\n    pages = \"4678--4696\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.290.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.290/",
        "pdf_size": 702624,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12547018806764662406&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science and Engineering, Indian Institute of Technology Patna, India; Department of Computer Science and Engineering, Indian Institute of Technology Patna, India; University of Alberta, Canada; National Institute of Mental Health and Neurosciences, Banglore, India; Department of Computer Science and Engineering, Indian Institute of Technology Patna, India",
        "aff_domain": "iitp.ac.in;iitp.ac.in;gmail.com;nimhans.ac.in;iitp.ac.in",
        "email": "iitp.ac.in;iitp.ac.in;gmail.com;nimhans.ac.in;iitp.ac.in",
        "github": "",
        "project": "PAANTH",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Indian Institute of Technology Patna;University of Alberta;National Institute of Mental Health and Neurosciences",
        "aff_unique_dep": "Department of Computer Science and Engineering;;",
        "aff_unique_url": "https://www.iitp.ac.in;https://www.ualberta.ca;",
        "aff_unique_abbr": "IIT Patna;UAlberta;",
        "aff_campus_unique_index": "0;0;2;0",
        "aff_campus_unique": "Patna;;Banglore",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "India;Canada"
    },
    {
        "id": "2024.findings-naacl.35",
        "title": "On-the-Fly Fusion of Large Language Models and Machine Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We propose on-the-fly ensembling of a neural machine translation (NMT) model with a large language model (LLM), prompted on the same task and input. Through experiments on 4 language directions with varying data amounts, we find that a slightly weaker-at-translation LLM can improve translations of a NMT model, and such an ensemble can produce better translations than ensembling two stronger NMT models.We demonstrate that our ensemble method can be combined with various techniques from LLM prompting, such as in context learning and translation context.",
        "author": "Hieu Hoang; Huda Khayrallah; Marcin Junczys-Dowmunt",
        "authorids": "/h/hieu-hoang/; /h/huda-khayrallah/; /m/marcin-junczys-dowmunt/",
        "bibtex": "@inproceedings{hoang-etal-2024-fly,\n    title = \"On-the-Fly Fusion of Large Language Models and Machine Translation\",\n    author = \"Hoang, Hieu  and\n      Khayrallah, Huda  and\n      Junczys-Dowmunt, Marcin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.35/\",\n    doi = \"10.18653/v1/2024.findings-naacl.35\",\n    pages = \"520--532\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.35.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.35/",
        "pdf_size": 905868,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13067538381585520088&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Microsoft, 1 Microsoft Way, Redmond, WA 98052, USA; Microsoft, 1 Microsoft Way, Redmond, WA 98052, USA; Microsoft, 1 Microsoft Way, Redmond, WA 98052, USA",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Corporation",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Redmond",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.212",
        "title": "On-the-fly Definition Augmentation of LLMs for Biomedical NER",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite their general capabilities, LLMs still struggle on biomedicalNER tasks, which are difficult due to the presence of specialized terminology and lack of training data. In this work we set out to improve LLM performance on biomedical NER in limited data settings via a new knowledge augmentation approach which incorporates definitions of relevant concepts on-the-fly. During this process, to provide a test bed for knowledge augmentation, we perform a comprehensive exploration of prompting strategies. Our experiments show that definition augmentation is useful for both open source and closed LLMs.For example, it leads to a relative improvement of 15% (on average) in GPT-4 performance (F1) across all (six) of our test datasets. We conduct extensive ablations and analyses to demonstrate that our performance improvements stem from adding relevant definitional knowledge. We find that careful prompting strategies also improve LLM performance, allowing them to outperform fine-tuned language models in few-shot settings. To facilitate future research in this direction, we release our code at https://github.com/allenai/beacon.",
        "author": "Monica Munnangi; Sergey Feldman; Byron Wallace; Silvio Amir; Tom Hope; Aakanksha Naik",
        "authorids": "/m/monica-munnangi/; /s/sergey-feldman/; /b/byron-c-wallace/; /s/silvio-amir/; /t/tom-hope/; /a/aakanksha-naik/",
        "bibtex": "@inproceedings{munnangi-etal-2024-fly,\n    title = \"On-the-fly Definition Augmentation of {LLM}s for Biomedical {NER}\",\n    author = \"Munnangi, Monica  and\n      Feldman, Sergey  and\n      Wallace, Byron  and\n      Amir, Silvio  and\n      Hope, Tom  and\n      Naik, Aakanksha\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.212/\",\n    doi = \"10.18653/v1/2024.naacl-long.212\",\n    pages = \"3833--3854\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.212.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.212/",
        "pdf_size": 753635,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13198940754481283161&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Northeastern University\u2662; Allen Institute for AI\u2663; Northeastern University\u2662; Northeastern University\u2662; Allen Institute for AI\u2663; Allen Institute for AI\u2663",
        "aff_domain": "northeastern.edu;allenai.org;northeastern.edu;northeastern.edu;allenai.org;allenai.org",
        "email": "northeastern.edu;allenai.org;northeastern.edu;northeastern.edu;allenai.org;allenai.org",
        "github": "https://github.com/allenai/beacon",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;1;1",
        "aff_unique_norm": "Northeastern University;Allen Institute for AI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.northeastern.edu;https://allenai.org",
        "aff_unique_abbr": "NEU;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.314",
        "title": "Open-Vocabulary Federated Learning with Multimodal Prototyping",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing federated learning (FL) studies usuallyassume the training label space and test labelspace are identical. However, in real-world applications, this assumption is too ideal to betrue. A new user could come up with queriesthat involve data from unseen classes, and suchopen-vocabulary queries would directly defectsuch FL systems. Therefore, in this work, weexplicitly focus on the under-explored openvocabulary challenge in FL. That is, for a newuser, the global server shall understand her/hisquery that involves arbitrary unknown classes.To address this problem, we leverage the pretrained vision-language models (VLMs). Inparticular, we present a novel adaptation framework tailored for VLMs in the context of FL,named as Federated Multimodal Prototyping(Fed-MP). Fed-MP adaptively aggregates thelocal model weights based on light-weightclient residuals, and makes predictions basedon a novel multimodal prototyping mechanism.Fed-MP exploits the knowledge learned fromthe seen classes, and robustifies the adaptedVLM to unseen categories. Our empirical evaluation on various datasets validates the effectiveness of Fed-MP.",
        "author": "Huimin Zeng; Zhenrui Yue; Dong Wang",
        "authorids": "/h/huimin-zeng/; /z/zhenrui-yue/; /d/dong-wang/",
        "bibtex": "@inproceedings{zeng-etal-2024-open,\n    title = \"Open-Vocabulary Federated Learning with Multimodal Prototyping\",\n    author = \"Zeng, Huimin  and\n      Yue, Zhenrui  and\n      Wang, Dong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.314/\",\n    doi = \"10.18653/v1/2024.naacl-long.314\",\n    pages = \"5644--5656\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.314.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.314/",
        "pdf_size": 665779,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1623130365221481418&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.24",
        "title": "OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Object navigation (ObjectNav) requires an agent to navigate through unseen environments to find queried objects. Many previous methods attempted to solve this task by relying on supervised or reinforcement learning, where they are trained on limited household datasets with close-set objects. However, two key challenges are unsolved: understanding free-form natural language instructions that demand open-set objects, and generalizing to new environments in a zero-shot manner. Aiming to solve the two challenges, in this paper, we propose **OpenFMNav**, an **Open**-set **F**oundation **M**odel based framework for zero-shot object **Nav**igation. We first unleash the reasoning abilities of large language models (LLMs) to extract proposed objects from natural language instructions that meet the user\u2019s demand. We then leverage the generalizability of large vision language models (VLMs) to actively discover and detect candidate objects from the scene, building a *Versatile Semantic Score Map (VSSM)*. Then, by conducting common sense reasoning on *VSSM*, our method can perform effective language-guided exploration and exploitation of the scene and finally reach the goal. By leveraging the reasoning and generalizing abilities of foundation models, our method can understand free-form human instructions and perform effective open-set zero-shot navigation in diverse environments. Extensive experiments on the HM3D ObjectNav benchmark show that our method surpasses all the strong baselines on all metrics, proving our method\u2019s effectiveness. Furthermore, we perform real robot demonstrations to validate our method\u2019s open-set-ness and generalizability to real-world environments.",
        "author": "Yuxuan Kuang; Hai Lin; Meng Jiang",
        "authorids": "/y/yuxuan-kuang/; /h/hai-lin/; /m/meng-jiang/",
        "bibtex": "@inproceedings{kuang-etal-2024-openfmnav,\n    title = \"{O}pen{FMN}av: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models\",\n    author = \"Kuang, Yuxuan  and\n      Lin, Hai  and\n      Jiang, Meng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.24/\",\n    doi = \"10.18653/v1/2024.findings-naacl.24\",\n    pages = \"338--351\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.24.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.24/",
        "pdf_size": 13730359,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10819020869005848181&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Peking University; University of Notre Dame; University of Notre Dame",
        "aff_domain": "stu.pku.edu.cn;nd.edu;nd.edu",
        "email": "stu.pku.edu.cn;nd.edu;nd.edu",
        "github": "",
        "project": "https://yxkryptonite.github.io/OpenFMNav/",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Peking University;University of Notre Dame",
        "aff_unique_dep": ";",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.nd.edu",
        "aff_unique_abbr": "Peking U;Notre Dame",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-demo.8",
        "title": "OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable ability to generate fitting responses to natural language instructions. However, an open research question concerns the inherent biases of trained models and their responses. For instance, if the data used to tune an LLM is dominantly written by persons with a specific political bias, we might expect generated answers to share this bias. Current research work seeks to de-bias such models, or suppress potentially biased answers.With this demonstration, we take a different view on biases in instruction-tuning: Rather than aiming to suppress them, we aim to make them explicit and transparent. To this end, we present OpinionGPT, a web demo in which users can ask questions and select all biases they wish to investigate. The demo will answer this question using a model fine-tuned on text representing each of the selected biases, allowing side-by-side comparison. To train the underlying model, we identified 11 different biases (political, geographic, gender, age) and derived an instruction-tuning corpus in which each answer was written by members of one of these demographics. This paper presents OpinionGPT, illustrates how we trained the bias-aware model and showcases the web application (available at https://opiniongpt.informatik.hu-berlin.de).",
        "author": "Patrick Haller; Ansar Aynetdinov; Alan Akbik",
        "authorids": "/p/patrick-haller-zurich/; /a/ansar-aynetdinov/; /a/alan-akbik/",
        "bibtex": "@inproceedings{haller-etal-2024-opiniongpt,\n    title = \"{O}pinion{GPT}: Modelling Explicit Biases in Instruction-Tuned {LLM}s\",\n    author = \"Haller, Patrick  and\n      Aynetdinov, Ansar  and\n      Akbik, Alan\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.8/\",\n    doi = \"10.18653/v1/2024.naacl-demo.8\",\n    pages = \"78--86\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.8.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.8/",
        "pdf_size": 227459,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9417859807261746331&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Humboldt-Universitat zu Berlin; Humboldt-Universitat zu Berlin; Humboldt-Universitat zu Berlin",
        "aff_domain": "hu-berlin.de;hu-berlin.de;hu-berlin.de",
        "email": "hu-berlin.de;hu-berlin.de;hu-berlin.de",
        "github": "",
        "project": "https://opiniongpt.informatik.hu-berlin.de",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Humboldt University of Berlin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.hu-berlin.de",
        "aff_unique_abbr": "HU Berlin",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berlin",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-industry.23",
        "title": "Optimizing LLM Based Retrieval Augmented Generation Pipelines in the Financial Domain",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Retrieval Augmented Generation (RAG) is a prominent approach in real-word applications for grounding large language model (LLM) generations in up to date and domain-specific knowledge. However, there is a lack of systematic investigations of the impact of each component (retrieval quality, prompts, generation models) on the generation quality of a RAG pipeline in real world scenarios. In this study, we benchmark 6 LLMs in 15 retrieval scenarios exploring 9 prompts over 2 real world financial domain datasets. We thoroughly discuss the impact of each component in RAG pipeline on answer generation quality and formulate specific recommendations for the design of RAG systems.",
        "author": "Yiyun Zhao; Prateek Singh; Hanoz Bhathena; Bernardo Ramos; Aviral Joshi; Swaroop Gadiyaram; Saket Sharma",
        "authorids": "/y/yiyun-zhao/; /p/prateek-singh/; /h/hanoz-bhathena/; /b/bernardo-ramos/; /a/aviral-joshi/; /s/swaroop-gadiyaram/; /s/saket-sharma/",
        "bibtex": "@inproceedings{zhao-etal-2024-optimizing,\n    title = \"Optimizing {LLM} Based Retrieval Augmented Generation Pipelines in the Financial Domain\",\n    author = \"Zhao, Yiyun  and\n      Singh, Prateek  and\n      Bhathena, Hanoz  and\n      Ramos, Bernardo  and\n      Joshi, Aviral  and\n      Gadiyaram, Swaroop  and\n      Sharma, Saket\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.23/\",\n    doi = \"10.18653/v1/2024.naacl-industry.23\",\n    pages = \"279--294\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.23.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.23/",
        "pdf_size": 2878271,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2946122530125876629&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Machine Learning Center of Excellence, JPMorgan Chase & Co.; Machine Learning Center of Excellence, JPMorgan Chase & Co.; Machine Learning Center of Excellence, JPMorgan Chase & Co.; Machine Learning Center of Excellence, JPMorgan Chase & Co.; Machine Learning Center of Excellence, JPMorgan Chase & Co.; Machine Learning Center of Excellence, JPMorgan Chase & Co.; Machine Learning Center of Excellence, JPMorgan Chase & Co.",
        "aff_domain": "jpmchase.com; ; ; ; ; ; ",
        "email": "jpmchase.com; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "JPMorgan Chase & Co.",
        "aff_unique_dep": "Machine Learning Center of Excellence",
        "aff_unique_url": "https://www.jpmorganchase.com",
        "aff_unique_abbr": "JPMorgan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.79",
        "title": "OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) have revolutionized the landscape of Natural Language Processing, but are computationally expensive. To reduce the cost without sacrificing performance, previous studies have explored various approaches to harness the potential of Smaller Language Models (SLMs) as cost-effective alternatives to their larger counterparts. Driven by findings that SLMs and LLMs exhibit complementary strengths in a structured knowledge extraction task, this work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance. In dialogue state tracking tasks, the proposed routing framework enhances performance substantially compared to relying solely on LLMs, while reducing the computational costs by over 50%.",
        "author": "Chia-Hsuan Lee; Hao Cheng; Mari Ostendorf",
        "authorids": "/c/chia-hsuan-lee/; /h/hao-cheng/; /m/mari-ostendorf/",
        "bibtex": "@inproceedings{lee-etal-2024-orchestrallm,\n    title = \"{O}rchestra{LLM}: Efficient Orchestration of Language Models for Dialogue State Tracking\",\n    author = \"Lee, Chia-Hsuan  and\n      Cheng, Hao  and\n      Ostendorf, Mari\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.79/\",\n    doi = \"10.18653/v1/2024.naacl-long.79\",\n    pages = \"1434--1445\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.79.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.79/",
        "pdf_size": 797575,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16651316315740497409&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Washington; Microsoft Research; University of Washington",
        "aff_domain": "uw.edu;microsoft.com;uw.edu",
        "email": "uw.edu;microsoft.com;uw.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Washington;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.washington.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UW;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.74",
        "title": "Order-Based Pre-training Strategies for Procedural Text Understanding",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "In this paper, we propose sequence-based pre-training methods to enhance procedural understanding in natural language processing. Procedural text, containing sequential instructions to accomplish a task, is difficult to understand due to the changing attributes of entities in the context. We focus on recipes as they are commonly represented as ordered instructions, and use this order as a supervision signal. Our work is one of the first to compare several \u2018order-as-supervision\u2019 transformer pre-training methods, including Permutation Classification, Embedding Regression, and Skip-Clip, and show that these methods give improved results compared to baselines and SoTA LLMs on two downstream Entity-Tracking datasets: NPN-Cooking dataset in recipe domain and ProPara dataset in open domain. Our proposed methods address the non-trivial Entity Tracking Task that requires prediction of entity states across procedure steps, which requires understanding the order of steps. These methods show an improvement over the best baseline by 1.6% and 7-9% on NPN-Cooking and ProPara Datasets respectively across metrics.",
        "author": "Abhilash Nandy; Yash Kulkarni; Pawan Goyal; Niloy Ganguly",
        "authorids": "/a/abhilash-nandy/; /y/yash-kulkarni/; /p/pawan-goyal/; /n/niloy-ganguly/",
        "bibtex": "@inproceedings{nandy-etal-2024-order,\n    title = \"Order-Based Pre-training Strategies for Procedural Text Understanding\",\n    author = \"Nandy, Abhilash  and\n      Kulkarni, Yash  and\n      Goyal, Pawan  and\n      Ganguly, Niloy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.74/\",\n    doi = \"10.18653/v1/2024.naacl-short.74\",\n    pages = \"821--828\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.74.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.74/",
        "pdf_size": 344412,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16729394414941635290&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Indian Institute of Technology Kharagpur; Indian Institute of Technology Kharagpur; Indian Institute of Technology Kharagpur; Indian Institute of Technology Kharagpur",
        "aff_domain": "kgpian.iitkgp.ac.in; ; ; ",
        "email": "kgpian.iitkgp.ac.in; ; ; ",
        "github": "https://github.com/abhi1nandy2/Order_As_Supervision",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kharagpur",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.iitkgp.ac.in",
        "aff_unique_abbr": "IIT Kharagpur",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Kharagpur",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2024.naacl-long.119",
        "title": "P3Sum: Preserving Author\u2019s Perspective in News Summarization with Diffusion Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this work, we take a first step towards designing summarization systems that are faithful to the author\u2019s intent, not only the semantic content of the article. Focusing on a case study of preserving political perspectives in news summarization, we find that existing approaches alter the political opinions and stances of news articles in more than 50% of summaries, misrepresenting the intent and perspectives of the news authors. We thus propose P3Sum, a diffusion model-based summarization approach controlled by political perspective classifiers. In P3Sum, the political leaning of a generated summary is iteratively evaluated at each decoding step, and any drift from the article\u2019s original stance incurs a loss back-propagated to the embedding layers, steering the political stance of the summary at inference time. Extensive experiments on three news summarization datasets demonstrate that P3Sum outperforms state-of-the-art summarization systems and large language models by up to 13.7% in terms of the success rate of stance preservation, with competitive performance on standard metrics of summarization quality. Our findings present a first analysis of preservation of pragmatic features in summarization, highlight the lacunae in existing summarization models\u2014that even state-of-the-art models often struggle to preserve author\u2019s intents\u2014and develop new summarization systems that are more faithful to author\u2019s perspectives.",
        "author": "Yuhan Liu; Shangbin Feng; Xiaochuang Han; Vidhisha Balachandran; Chan Young Park; Sachin Kumar; Yulia Tsvetkov",
        "authorids": "/y/yuhan-liu/; /s/shangbin-feng/; /x/xiaochuang-han/; /v/vidhisha-balachandran/; /c/chan-young-park/; /s/sachin-kumar/; /y/yulia-tsvetkov/",
        "bibtex": "@inproceedings{liu-etal-2024-p3sum,\n    title = \"{P}$^3${S}um: Preserving Author{'}s Perspective in News Summarization with Diffusion Language Models\",\n    author = \"Liu, Yuhan  and\n      Feng, Shangbin  and\n      Han, Xiaochuang  and\n      Balachandran, Vidhisha  and\n      Park, Chan Young  and\n      Kumar, Sachin  and\n      Tsvetkov, Yulia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.119/\",\n    doi = \"10.18653/v1/2024.naacl-long.119\",\n    pages = \"2154--2173\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.119.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.119/",
        "pdf_size": 469633,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1181311525660239119&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Xi\u2019an Jiaotong University; University of Washington; University of Washington; Carnegie Mellon University; Carnegie Mellon University; Allen Institute for AI; University of Washington",
        "aff_domain": "stu.xjtu.edu.cn;cs.washington.edu; ;cs.cmu.edu;cs.cmu.edu;allenai.org;cs.washington.edu",
        "email": "stu.xjtu.edu.cn;cs.washington.edu; ;cs.cmu.edu;cs.cmu.edu;allenai.org;cs.washington.edu",
        "github": "https://github.com/lyh6560new/P3Sum",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;2;2;3;1",
        "aff_unique_norm": "Xi'an Jiao Tong University;University of Washington;Carnegie Mellon University;Allen Institute for AI",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.xjtu.edu.cn;https://www.washington.edu;https://www.cmu.edu;https://allenai.org",
        "aff_unique_abbr": "XJTU;UW;CMU;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.findings-naacl.225",
        "title": "PAELLA: Parameter-Efficient Lightweight Language-Agnostic Captioning Model",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We introduce PAELLA, a Parameter-Efficient Lightweight Language-Agnostic image captioning model designed to be both parameter and data-efficient using retrieval augmentation. The model is trained by learning a small mapping network with 34M parameters between a pre-trained visual model and a multilingual language model that is conditioned on two types of input: (i) the image itself, and (ii) a set of retrieved captions in the target language. The retrieved examples play a key role in guiding the model to generate captions across languages. Through retrieval, the model can be lightweight in terms of the number of trainable parameters, which only exist in its mapping network, and also in the amount of multilingual training data that is required. Experiments on the XM3600 dataset, featuring 36 languages, show that PAELLA can outperform or compete against some models with 3\u201377\u00d7 more learned parameters and 35\u2013863\u00d7 more data, particularly in low-resource languages. We also find that PAELLA can be trained on only monolingual data and still show strong zero-shot abilities in other languages.",
        "author": "Rita Ramos; Emanuele Bugliarello; Bruno Martins; Desmond Elliott",
        "authorids": "/r/rita-ramos/; /e/emanuele-bugliarello/; /b/bruno-martins/; /d/desmond-elliott/",
        "bibtex": "@inproceedings{ramos-etal-2024-paella,\n    title = \"{PAELLA}: Parameter-Efficient Lightweight Language-Agnostic Captioning Model\",\n    author = \"Ramos, Rita  and\n      Bugliarello, Emanuele  and\n      Martins, Bruno  and\n      Elliott, Desmond\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.225/\",\n    doi = \"10.18653/v1/2024.findings-naacl.225\",\n    pages = \"3549--3564\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.225.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.225/",
        "pdf_size": 1013137,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13580001023764162332&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "INESC-ID, Instituto Superior T\u00e9cnico, University of Lisbon; Google Research; INESC-ID, Instituto Superior T\u00e9cnico, University of Lisbon; Department of Computer Science, University of Copenhagen",
        "aff_domain": "tecnico.ulisboa.pt; ; ; ",
        "email": "tecnico.ulisboa.pt; ; ; ",
        "github": "https://github.com/RitaRamo/paella",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of Lisbon;Google;University of Copenhagen",
        "aff_unique_dep": "Instituto Superior T\u00e9cnico;Google Research;Department of Computer Science",
        "aff_unique_url": "https://www IST.utl.pt;https://research.google;https://www.ku.dk",
        "aff_unique_abbr": "IST;Google Research;UCPH",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;1;0;2",
        "aff_country_unique": "Portugal;United States;Denmark"
    },
    {
        "id": "2024.findings-naacl.131",
        "title": "PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "CLIP-based classifiers rely on the prompt containing a class name that is known to the text encoder. Therefore, they perform poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB \u2013 an explainable and editable classifier to (1) express the class name into a set of text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a huge margin (\u223c10\u00d7 in top-1 accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art (SOTA) on the supervised-learning setting (88.80% and 92.20% accuracy on CUB-200 and Stanford Dogs-120, respectively) but also the first to enable users to edit the text descriptors to form a new classifier without any re-training. Compared to concept bottleneck models, PEEB is also the SOTA in both zero-shot and supervised-learning settings.",
        "author": "Thang Pham; Peijie Chen; Tin Nguyen; Seunghyun Yoon; Trung Bui; Anh Nguyen",
        "authorids": "/t/thang-pham/; /p/peijie-chen/; /t/tin-nguyen/; /s/seunghyun-yoon/; /t/trung-bui/; /a/anh-nguyen/",
        "bibtex": "@inproceedings{pham-etal-2024-peeb,\n    title = \"{PEEB}: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck\",\n    author = \"Pham, Thang  and\n      Chen, Peijie  and\n      Nguyen, Tin  and\n      Yoon, Seunghyun  and\n      Bui, Trung  and\n      Nguyen, Anh\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.131/\",\n    doi = \"10.18653/v1/2024.findings-naacl.131\",\n    pages = \"2018--2053\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.131.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.131/",
        "pdf_size": 2692680,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13983632641476862078&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Auburn University\u2020; Auburn University\u2020; Auburn University\u2020; Adobe Research\u00a7; Adobe Research\u00a7; Auburn University\u2020",
        "aff_domain": "auburn.edu;auburn.edu;auburn.edu;adobe.com;adobe.com;gmail.com",
        "email": "auburn.edu;auburn.edu;auburn.edu;adobe.com;adobe.com;gmail.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;1;0",
        "aff_unique_norm": "Auburn University;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.auburn.edu;https://research.adobe.com",
        "aff_unique_abbr": "Auburn;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.423",
        "title": "PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We investigate pre-training techniques for abstractive multi-document summarization (MDS), which is much less studied than summarizing single documents. Though recent work has demonstrated the effectiveness of highlighting information salience for pre-training strategy design, they struggle to generate abstractive and reflective summaries, which are critical properties for MDS. To this end, we present **PELMS**, a pre-trained model that uses pre-training objectives based on semantic coherence heuristics and faithfulness constraints together with unlabeled multi-document inputs, to promote the generation of concise, fluent, and faithful summaries. To support the training of PELMS, we compile **MultiPT**, a multi-document pre-training corpus containing over 93 million documents to form more than 3million unlabeled topic-centric document clusters, covering diverse genres such as product reviews, news, and general knowledge. We perform extensive evaluation of PELMS in low-shot settings on a wide range of MDS datasets. Our approach consistently outperforms competitive comparisons with respect to overall informativeness, abstractiveness, coherence, and faithfulness, and with minimal fine-tuning can match performance of language models at a much larger scale (e.g., GPT-4).",
        "author": "Joseph Peper; Wenzhao Qiu; Lu Wang",
        "authorids": "/j/joseph-j-peper/; /w/wenzhao-qiu/; /l/lu-wang/",
        "bibtex": "@inproceedings{peper-etal-2024-pelms,\n    title = \"{PELMS}: Pre-training for Effective Low-Shot Multi-Document Summarization\",\n    author = \"Peper, Joseph  and\n      Qiu, Wenzhao  and\n      Wang, Lu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.423/\",\n    doi = \"10.18653/v1/2024.naacl-long.423\",\n    pages = \"7652--7674\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.423.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.423/",
        "pdf_size": 2792818,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11472122374286190009&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Computer Science and Engineering, University of Michigan; Computer Science and Engineering, University of Michigan; Computer Science and Engineering, University of Michigan",
        "aff_domain": "umich.edu;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Michigan",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.umich.edu",
        "aff_unique_abbr": "UM",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Ann Arbor",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.336",
        "title": "PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained language models (PLMs) show impressive performance in various downstream NLP tasks. However, pre-training large language models demands substantial memory and training compute. Furthermore, due to the substantial resources required, many PLM weights are confidential. Consequently, users are compelled to share their data with model owners for fine-tuning specific tasks. To overcome the limitations, we introduce Plug-in External Memory Adaptation (PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM fine-tuning without requiring access to all the weights. PEMA integrates with context representations from test data during inference to perform downstream tasks. It uses external memory to store PLM-generated context representations mapped with target tokens. Our method utilizes weight matrices of LoRA-like bottlenecked adapter in the PLM\u2019s final layer to enhance efficiency. Our approach also includes Gradual Unrolling, a novel interpolation strategy to improve generation quality. We validate PEMA\u2019s effectiveness through experiments on syntactic and real datasets for machine translation and style transfer. Our findings show that PEMA outperforms other PEFT approaches in memory and latency efficiency for training, and also excels in maintaining sentence meaning and generating appropriate language and styles.",
        "author": "HyunJin Kim; Young Jin Kim; JinYeong Bak",
        "authorids": "/h/hyunjin-kim/; /y/young-jin-kim/; /j/jinyeong-bak/",
        "bibtex": "@inproceedings{kim-etal-2024-pema,\n    title = \"{PEMA}: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models\",\n    author = \"Kim, HyunJin  and\n      Kim, Young Jin  and\n      Bak, JinYeong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.336/\",\n    doi = \"10.18653/v1/2024.naacl-long.336\",\n    pages = \"6045--6064\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.336.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.336/",
        "pdf_size": 1174181,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8412402943728962686&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Sungkyunkwan University; Microsoft + Sungkyunkwan University; Sungkyunkwan University",
        "aff_domain": "skku.edu;microsoft.com;skku.edu",
        "email": "skku.edu;microsoft.com;skku.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Sungkyunkwan University;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "https://www.skku.edu;https://www.microsoft.com",
        "aff_unique_abbr": "SKKU;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "2024.naacl-long.34",
        "title": "PILOT: Legal Case Outcome Prediction with Case Law",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Machine learning shows promise in predicting the outcome of legal cases, but most research has concentrated on civil law cases rather than case law systems. We identified two unique challenges in making legal case outcome predictions with case law. First, it is crucial to identify relevant precedent cases that serve as fundamental evidence for judges during decision-making. Second, it is necessary to consider the evolution of legal principles over time, as early cases may adhere to different legal contexts. In this paper, we proposed a new framework named PILOT (PredictIng Legal case OuTcome) for case outcome prediction. It comprises two modules for relevant case retrieval and temporal pattern handling, respectively. To benchmark the performance of existing legal case outcome prediction models, we curated a dataset from a large-scale case law database. We demonstrate the importance of accurately identifying precedent cases and mitigating the temporal shift when making predictions for case law, as our method shows a significant improvement over the prior methods that focus on civil law case outcome predictions.",
        "author": "Lang Cao; Zifeng Wang; Cao Xiao; Jimeng Sun",
        "authorids": "/l/lang-cao/; /z/zifeng-wang/; /c/cao-xiao/; /j/jimeng-sun/",
        "bibtex": "@inproceedings{cao-etal-2024-pilot,\n    title = \"{PILOT}: Legal Case Outcome Prediction with Case Law\",\n    author = \"Cao, Lang  and\n      Wang, Zifeng  and\n      Xiao, Cao  and\n      Sun, Jimeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.34/\",\n    doi = \"10.18653/v1/2024.naacl-long.34\",\n    pages = \"609--621\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.34.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.34/",
        "pdf_size": 513979,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11366152241349026046&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; GE Healthcare; University of Illinois Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;ge.com;illinois.edu",
        "email": "illinois.edu;illinois.edu;ge.com;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;GE Healthcare",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.gehealthcare.com",
        "aff_unique_abbr": "UIUC;GEHC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.131",
        "title": "POLYIE: A Dataset of Information Extraction from Polymer Material Scientific Literature",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Scientific information extraction (SciIE), which aims to automatically extract information from scientific literature, is becoming more important than ever. However, there are no existing SciIE datasets for polymer materials, which is an important class of materials used ubiquitously in our daily lives. To bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer materials. POLYIE is curated from 146 full-length polymer scholarly articles, which are annotated with different named entities (i.e., materials, properties, values, conditions) as well as their N-ary relations by domain experts. POLYIE presents several unique challenges due to diverse lexical formats of entities, ambiguity between entities, and variable-length relations. We evaluate state-of-the-art named entity extraction and relation extraction models on POLYIE, analyze their strengths and weaknesses, and highlight some difficult cases for these models. To the best of our knowledge, POLYIE is the first SciIE benchmark for polymer materials, and we hope it will lead to more research efforts from the community on this challenging task. Our code and data are available on: https://github.com/jerry3027/PolyIE.",
        "author": "Jerry Cheung; Yuchen Zhuang; Yinghao Li; Pranav Shetty; Wantian Zhao; Sanjeev Grampurohit; Rampi Ramprasad; Chao Zhang",
        "authorids": "/j/jerry-cheung/; /y/yuchen-zhuang/; /y/yinghao-li/; /p/pranav-shetty/; /w/wantian-zhao/; /s/sanjeev-grampurohit/; /r/rampi-ramprasad/; /c/chao-zhang-tu/",
        "bibtex": "@inproceedings{cheung-etal-2024-polyie,\n    title = \"{POLYIE}: A Dataset of Information Extraction from Polymer Material Scientific Literature\",\n    author = \"Cheung, Jerry  and\n      Zhuang, Yuchen  and\n      Li, Yinghao  and\n      Shetty, Pranav  and\n      Zhao, Wantian  and\n      Grampurohit, Sanjeev  and\n      Ramprasad, Rampi  and\n      Zhang, Chao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.131/\",\n    doi = \"10.18653/v1/2024.naacl-long.131\",\n    pages = \"2370--2385\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.131.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.131/",
        "pdf_size": 923650,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11313450019774995873&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "College of Computing; College of Computing; College of Computing; School of Materials Science and Engineering; College of Computing; College of Computing; School of Materials Science and Engineering; College of Computing",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;mse.gatech.edu;gatech.edu",
        "email": "gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu;mse.gatech.edu;gatech.edu",
        "github": "https://github.com/jerry3027/PolyIE",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;1;0;0;1;0",
        "aff_unique_norm": "College of Computing;School of Materials Science and Engineering",
        "aff_unique_dep": ";Materials Science and Engineering",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2024.findings-naacl.222",
        "title": "PRODIGy: a PROfile-based DIalogue Generation dataset",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Providing dialogue agents with a profile representation can improve their consistency and coherence, leading to better conversations. However, current profile-based dialogue datasets for training such agents contain either explicit profile representations that are simple and dialogue-specific, or implicit representations that are difficult to collect. In this work, we introduce the PRODIGy (PROfile-based DIalogue Generation) dataset, which brings diverse representations together, providing a more comprehensive profile dimension set for each speaker. This resource comprises more than 20k dialogues, sourced from movie scripts, aligned with speaker representations such as communication style, biography, personality and gender. Initial experiments with diverse baselines show that providing generative language models with these aspects of a profile, both separately and jointly, enhances models\u2019 performance. This improvement holds true in both in-domain and cross-domain settings, for both fine-tuned and instruction-based LLMs.",
        "author": "Daniela Occhipinti; Serra Sinem Tekiro\u011flu; Marco Guerini",
        "authorids": "/d/daniela-occhipinti/; /s/serra-sinem-tekiroglu/; /m/marco-guerini/",
        "bibtex": "@inproceedings{occhipinti-etal-2024-prodigy,\n    title = \"{PRODIG}y: a {PRO}file-based {DI}alogue Generation dataset\",\n    author = \"Occhipinti, Daniela  and\n      Tekiro{\\u{g}}lu, Serra Sinem  and\n      Guerini, Marco\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.222/\",\n    doi = \"10.18653/v1/2024.findings-naacl.222\",\n    pages = \"3500--3514\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.222.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.222/",
        "pdf_size": 532101,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1668371816608497010&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Fondazione Bruno Kessler+University of Trento; Fondazione Bruno Kessler; Fondazione Bruno Kessler",
        "aff_domain": "fbk.eu;fbk.eu;fbk.eu",
        "email": "fbk.eu;fbk.eu;fbk.eu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;0",
        "aff_unique_norm": "Fondazione Bruno Kessler;University of Trento",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.fbk.eu;https://www.unitn.it",
        "aff_unique_abbr": "FBK;UniTN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2024.naacl-long.142",
        "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.",
        "author": "Xuekai Zhu; Biqing Qi; Kaiyan Zhang; Xinwei Long; Zhouhan Lin; Bowen Zhou",
        "authorids": "/x/xuekai-zhu/; /b/biqing-qi/; /k/kaiyan-zhang/; /x/xinwei-long/; /z/zhouhan-lin/; /b/bowen-zhou/",
        "bibtex": "@inproceedings{zhu-etal-2024-pad,\n    title = \"{P}a{D}: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning\",\n    author = \"Zhu, Xuekai  and\n      Qi, Biqing  and\n      Zhang, Kaiyan  and\n      Long, Xinwei  and\n      Lin, Zhouhan  and\n      Zhou, Bowen\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.142/\",\n    doi = \"10.18653/v1/2024.naacl-long.142\",\n    pages = \"2571--2597\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.142.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.142/",
        "pdf_size": 9657057,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13225970688228800219&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Electronic Engineering, Tsinghua University, Beijing, China+Frontis.AI, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China+Frontis.AI, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; LUMIA Lab, Shanghai Jiao Tong University; Department of Electronic Engineering, Tsinghua University, Beijing, China+Frontis.AI, Beijing, China",
        "aff_domain": "gmail.com; ; ; ; ;tsinghua.edu.cn",
        "email": "gmail.com; ; ; ; ;tsinghua.edu.cn",
        "github": "https://github.com/Xuekai-Zhu/pad",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;0;0;2;0+1",
        "aff_unique_norm": "Tsinghua University;Frontis.AI;Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Electronic Engineering;;LUMIA Lab",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "THU;;SJTU",
        "aff_campus_unique_index": "0+0;0+0;0;0;1;0+0",
        "aff_campus_unique": "Beijing;Shanghai",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.329",
        "title": "ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Analogy-making is central to human cognition, allowing us to adapt to novel situations \u2013 an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy.In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs\u2019 and humans\u2019 analogy recognition in binary and multiple-choice settings, and found that humans outperform the best models (\u223c13% gap) after a light supervision. We demonstrate that our silver-set is useful for training models. Lastly, we show challenging distractors confuse LLMs, but not humans. We hope our pipeline will encourage research in this emerging field.",
        "author": "Oren Sultan; Yonatan Bitton; Ron Yosef; Dafna Shahaf",
        "authorids": "/o/oren-sultan/; /y/yonatan-bitton/; /r/ron-yosef/; /d/dafna-shahaf/",
        "bibtex": "@inproceedings{sultan-etal-2024-parallelparc,\n    title = \"{P}arallel{PARC}: A Scalable Pipeline for Generating Natural-Language Analogies\",\n    author = \"Sultan, Oren  and\n      Bitton, Yonatan  and\n      Yosef, Ron  and\n      Shahaf, Dafna\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.329/\",\n    doi = \"10.18653/v1/2024.naacl-long.329\",\n    pages = \"5900--5924\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.329.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.329/",
        "pdf_size": 3863955,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2838935213429400848&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "The Hebrew University of Jerusalem; The Hebrew University of Jerusalem; The Hebrew University of Jerusalem; The Hebrew University of Jerusalem",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2024.naacl-long.410",
        "title": "Parameter-Efficient Instruction Tuning of Large Language Models For Extreme Financial Numeral Labelling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We study the problem of automatically annotating relevant numerals (GAAP metrics) occurring in the financial documents with their corresponding XBRL tags. Different from prior works, we investigate the feasibility of solving this extreme classification problem using a generative paradigm through instruction tuning of Large Language Models (LLMs). To this end, we leverage metric metadata informationto frame our target outputs while proposing a parameter efficient solution for the task using LoRA. We perform experiments on two recently released financial numeric labeling datasets. Our proposed model, **FLAN-FinXC**, achieves new state-of-the-art performances on both the datasets, outperforming several strong baselines. We explain the better scores of our proposed model by demonstrating its capability for zero-shot as well as the least frequently occurring tags. Also, even when we fail to predict the XBRL tags correctly, our generated output has substantial overlap with the ground-truth in majority of the cases.",
        "author": "Subhendu Khatuya; Rajdeep Mukherjee; Akash Ghosh; Manjunath Hegde; Koustuv Dasgupta; Niloy Ganguly; Saptarshi Ghosh; Pawan Goyal",
        "authorids": "/s/subhendu-khatuya/; /r/rajdeep-mukherjee/; /a/akash-ghosh/; /m/manjunath-hegde/; /k/koustuv-dasgupta/; /n/niloy-ganguly/; /s/saptarshi-ghosh/; /p/pawan-goyal/",
        "bibtex": "@inproceedings{khatuya-etal-2024-parameter,\n    title = \"Parameter-Efficient Instruction Tuning of Large Language Models For Extreme Financial Numeral Labelling\",\n    author = \"Khatuya, Subhendu  and\n      Mukherjee, Rajdeep  and\n      Ghosh, Akash  and\n      Hegde, Manjunath  and\n      Dasgupta, Koustuv  and\n      Ganguly, Niloy  and\n      Ghosh, Saptarshi  and\n      Goyal, Pawan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.410/\",\n    doi = \"10.18653/v1/2024.naacl-long.410\",\n    pages = \"7391--7403\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.410.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.410/",
        "pdf_size": 499773,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1087485181151173796&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Indian Institute of Technology Kharagpur, India; Indian Institute of Technology Kharagpur, India; Indian Institute of Technology Kharagpur, India; Language Modeling, Goldman Sachs; Language Modeling, Goldman Sachs; Indian Institute of Technology Kharagpur, India; Indian Institute of Technology Kharagpur, India; Indian Institute of Technology Kharagpur, India",
        "aff_domain": "gmail.com;iitkgp.ac.in;gmail.com;gs.com;gs.com;cse.iitkgp.ac.in;cse.iitkgp.ac.in;cse.iitkgp.ac.in",
        "email": "gmail.com;iitkgp.ac.in;gmail.com;gs.com;gs.com;cse.iitkgp.ac.in;cse.iitkgp.ac.in;cse.iitkgp.ac.in",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;1;1;0;0;0",
        "aff_unique_norm": "Indian Institute of Technology Kharagpur;Goldman Sachs",
        "aff_unique_dep": ";Language Modeling",
        "aff_unique_url": "https://www.iitkgp.ac.in;https://www.goldmansachs.com",
        "aff_unique_abbr": "IIT Kharagpur;GS",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Kharagpur;",
        "aff_country_unique_index": "0;0;0;1;1;0;0;0",
        "aff_country_unique": "India;United States"
    },
    {
        "id": "2024.naacl-long.153",
        "title": "Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper studies the relationship between the surface form of a mathematical problem and its solvability by large language models. We find that subtle alterations in the surface form can significantly impact the answer distribution and the solve rate, exposing the language model\u2019s lack of robustness and sensitivity to the surface form in reasoning through complex problems. To improve mathematical reasoning performance, we propose Self-Consistency-over-Paraphrases (SCoP), which diversifies reasoning paths from specific surface forms of the problem. We evaluate our approach on four mathematics reasoning benchmarks over three large language models and show that SCoP improves mathematical reasoning performance over vanilla self-consistency, particularly for problems initially deemed unsolvable. Finally, we provide additional experiments and discussion regarding problem difficulty and surface forms, including cross-model difficulty agreement and paraphrasing transferability, and Variance of Variations (VOV) for language model evaluation.",
        "author": "Yue Zhou; Yada Zhu; Diego Antognini; Yoon Kim; Yang Zhang",
        "authorids": "/y/yue-zhou/; /y/yada-zhu/; /d/diego-antognini/; /y/yoon-kim/; /y/yang-zhang/",
        "bibtex": "@inproceedings{zhou-etal-2024-paraphrase,\n    title = \"Paraphrase and Solve: Exploring and Exploiting the Impact of Surface Form on Mathematical Reasoning in Large Language Models\",\n    author = \"Zhou, Yue  and\n      Zhu, Yada  and\n      Antognini, Diego  and\n      Kim, Yoon  and\n      Zhang, Yang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.153/\",\n    doi = \"10.18653/v1/2024.naacl-long.153\",\n    pages = \"2793--2804\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.153.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.153/",
        "pdf_size": 673107,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9074956655195883072&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Illinois Chicago; MIT-IBM Watson AI Lab, IBM Research; MIT-IBM Watson AI Lab, IBM Research; MIT CSAIL; MIT-IBM Watson AI Lab, IBM Research",
        "aff_domain": "uic.edu;us.ibm.com;ibm.com;mit.edu;ibm.com",
        "email": "uic.edu;us.ibm.com;ibm.com;mit.edu;ibm.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "University of Illinois at Chicago;IBM;Massachusetts Institute of Technology",
        "aff_unique_dep": ";AI Lab;Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.uic.edu;https://www.ibmwatsonai.org/;https://www.csail.mit.edu",
        "aff_unique_abbr": "UIC;MIT-IBM AI Lab;MIT CSAIL",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Chicago;;Cambridge",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.147",
        "title": "PatentEval: Understanding Errors in Patent Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this work, we introduce a comprehensive error typology specifically designed for evaluating two distinct tasks in machine-generated patent texts: claims-to-abstract generation, and the generation of the next claim given previous ones. We have also developed a benchmark, PatentEval, for systematically assessing language models in this context. Our study includes a comparative analysis, annotated by humans, of various models. These range from those specifically adapted during training for tasks within the patent domain to the latest general-purpose large language models (LLMs). Furthermore, we explored and evaluated some metrics to approximate human judgments in patent text evaluation, analyzing the extent to which these metrics align with expert assessments. These approaches provide valuable insights into the capabilities and limitations of current language models in the specialized field of patent text generation.",
        "author": "You Zuo; Kim Gerdes; \u00c9ric Clergerie; Beno\u00eet Sagot",
        "authorids": "/y/you-zuo/; /k/kim-gerdes/; /e/eric-clergerie/; /b/benoit-sagot/",
        "bibtex": "@inproceedings{zuo-etal-2024-patenteval,\n    title = \"{P}atent{E}val: Understanding Errors in Patent Generation\",\n    author = \"Zuo, You  and\n      Gerdes, Kim  and\n      Clergerie, {\\'E}ric  and\n      Sagot, Beno{\\^i}t\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.147/\",\n    doi = \"10.18653/v1/2024.naacl-long.147\",\n    pages = \"2687--2710\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.147.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.147/",
        "pdf_size": 797082,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5937452843299314007&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Inria, Paris, France + Qatent, Paris, France; Qatent, Paris, France + LISN, CNRS and University Paris-Saclay, Orsay, France; Inria, Paris, France; Inria, Paris, France",
        "aff_domain": "inria.fr;lisn.fr;inria.fr;inria.fr",
        "email": "inria.fr;lisn.fr;inria.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1+2;0;0",
        "aff_unique_norm": "INRIA;Qatent;CNRS",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.inria.fr;;https://www.cnrs.fr",
        "aff_unique_abbr": "Inria;;CNRS",
        "aff_campus_unique_index": "0+0;0;0;0",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2024.naacl-long.220",
        "title": "Pedagogically Aligned Objectives Create Reliable Automatic Cloze Tests",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The cloze training objective of Masked Language Models makes them a natural choice for generating plausible distractors for human cloze questions. However, distractors must also be both distinct and incorrect, neither of which is directly addressed by existing neural methods. Evaluation of recent models has also relied largely on automated metrics, which cannot demonstrate the reliability or validity of human comprehension tests. In this work, we first formulate the pedagogically motivated objectives of plausibility, incorrectness, and distinctiveness in terms of conditional distributions from language models. Second, we present an unsupervised, interpretable method that uses these objectives to jointly optimize sets of distractors. Third, we test the reliability and validity of the resulting cloze tests compared to other methods with human participants. We find our method has stronger correlation with teacher-created comprehension tests than the state-of-the-art neural method and is more internally consistent. Our implementation is freely available and can quickly create a multiple choice cloze test from any given passage.",
        "author": "Brian Ondov; Kush Attal; Dina Demner-Fushman",
        "authorids": "/b/brian-ondov/; /k/kush-attal/; /d/dina-demner-fushman/",
        "bibtex": "@inproceedings{ondov-etal-2024-pedagogically,\n    title = \"Pedagogically Aligned Objectives Create Reliable Automatic Cloze Tests\",\n    author = \"Ondov, Brian  and\n      Attal, Kush  and\n      Demner-Fushman, Dina\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.220/\",\n    doi = \"10.18653/v1/2024.naacl-long.220\",\n    pages = \"3961--3972\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.220.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.220/",
        "pdf_size": 530898,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15823897667481337280&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "National Library of Medicine; National Library of Medicine; NYU Grossman School of Medicine",
        "aff_domain": "nih.gov;mail.nih.gov;nyulangone.org",
        "email": "nih.gov;mail.nih.gov;nyulangone.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "National Library of Medicine;New York University Grossman School of Medicine",
        "aff_unique_dep": ";School of Medicine",
        "aff_unique_url": "https://www.nlm.nih.gov;https://med.nyu.edu",
        "aff_unique_abbr": "NLM;NYU Grossman SOM",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.229",
        "title": "PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas\u2019 self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas\u2019 writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80%. Interestingly, the accuracy drops significantly when the annotators were informed of AI authorship.",
        "author": "Hang Jiang; Xiajie Zhang; Xubo Cao; Cynthia Breazeal; Deb Roy; Jad Kabbara",
        "authorids": "/h/hang-jiang/; /x/xiajie-zhang/; /x/xubo-cao/; /c/cynthia-breazeal/; /d/deb-roy/; /j/jad-kabbara/",
        "bibtex": "@inproceedings{jiang-etal-2024-personallm,\n    title = \"{P}ersona{LLM}: Investigating the Ability of Large Language Models to Express Personality Traits\",\n    author = \"Jiang, Hang  and\n      Zhang, Xiajie  and\n      Cao, Xubo  and\n      Breazeal, Cynthia  and\n      Roy, Deb  and\n      Kabbara, Jad\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.229/\",\n    doi = \"10.18653/v1/2024.findings-naacl.229\",\n    pages = \"3605--3627\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.229.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.229/",
        "pdf_size": 1773080,
        "gs_citation": 107,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17007364389848400219&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Massachusetts Institute of Technology; Massachusetts Institute of Technology; Stanford University; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;stanford.edu;mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;stanford.edu;mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.stanford.edu",
        "aff_unique_abbr": "MIT;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.286",
        "title": "Personalized Federated Learning for Text Classification with Gradient-Free Prompt Tuning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In this paper, we study personalized federated learning for text classification with Pretrained Language Models (PLMs). We identify two challenges in efficiently leveraging PLMs for personalized federated learning: 1) Communication. PLMs are usually large in size, e.g., with hundreds of millions of parameters, inducing huge communication cost in a federated setting. 2) Local Training. Training with PLMs generally requires back-propagation, during which memory consumption can be several times that of the forward-propagation. This may not be affordable when the PLMs are trained locally on the clients that are resource constrained, e.g., mobile devices with limited access to memory resources. Additionally, the proprietary PLMs can be provided as concealed APIs, for which the back-propagation operations may not be available. In solving these, we propose a training framework that includes an approach of discrete local search for gradient-free local training, along with a compression mechanism inspired from the linear word analogy that allows communicating with discretely indexed tokens, thus significantly reducing the communication cost. Experiments show that our gradient-free framework achieves superior performance compared with baselines.",
        "author": "Rui Wang; Tong Yu; Ruiyi Zhang; Sungchul Kim; Ryan Rossi; Handong Zhao; Junda Wu; Subrata Mitra; Lina Yao; Ricardo Henao",
        "authorids": "/r/rui-wang/; /t/tong-yu/; /r/ruiyi-zhang/; /s/sungchul-kim/; /r/ryan-rossi/; /h/handong-zhao/; /j/junda-wu/; /s/subrata-mitra/; /l/lina-yao/; /r/ricardo-henao/",
        "bibtex": "@inproceedings{wang-etal-2024-personalized,\n    title = \"Personalized Federated Learning for Text Classification with Gradient-Free Prompt Tuning\",\n    author = \"Wang, Rui  and\n      Yu, Tong  and\n      Zhang, Ruiyi  and\n      Kim, Sungchul  and\n      Rossi, Ryan  and\n      Zhao, Handong  and\n      Wu, Junda  and\n      Mitra, Subrata  and\n      Yao, Lina  and\n      Henao, Ricardo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.286/\",\n    doi = \"10.18653/v1/2024.findings-naacl.286\",\n    pages = \"4597--4612\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.286.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.286/",
        "pdf_size": 434298,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10443444553770403335&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Duke University; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Adobe Research; University of California San Diego; Adobe Research; University of New South Wales + CSIRO\u2019s Data61; Duke University + KAUST",
        "aff_domain": "outlook.com;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;ucsd.edu;adobe.com;data61.csiro.au;duke.edu",
        "email": "outlook.com;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;ucsd.edu;adobe.com;data61.csiro.au;duke.edu",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;1;1;1;1;2;1;3+4;0+5",
        "aff_unique_norm": "Duke University;Adobe;University of California, San Diego;University of New South Wales;CSIRO;King Abdullah University of Science and Technology",
        "aff_unique_dep": ";Adobe Research;;;Data61;",
        "aff_unique_url": "https://www.duke.edu;https://research.adobe.com;https://ucsd.edu;https://www.unsw.edu.au;https://www.csiro.au;https://www.kaust.edu.sa",
        "aff_unique_abbr": "Duke;Adobe;UCSD;UNSW;CSIRO;KAUST",
        "aff_campus_unique_index": "1;;",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;1+1;0+2",
        "aff_country_unique": "United States;Australia;Saudi Arabia"
    },
    {
        "id": "2024.naacl-long.255",
        "title": "Personalized Jargon Identification for Enhanced Interdisciplinary Communication",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Scientific jargon can confuse researchers when they read materials from other domains. Identifying and translating jargon for individual researchers could speed up research, but current methods of jargon identification mainly use corpus-level familiarity indicators rather than modeling researcher-specific needs, which can vary greatly based on each researcher\u2019s background. We collect a dataset of over 10K term familiarity annotations from 11 computer science researchers for terms drawn from 100 paper abstracts. Analysis of this data reveals that jargon familiarity and information needs vary widely across annotators, even within the same sub-domain (e.g., NLP). We investigate features representing domain, subdomain, and individual knowledge to predict individual jargon familiarity. We compare supervised and prompt-based approaches, finding that prompt-based methods using information about the individual researcher (e.g., personal publications, self-defined subfield of research) yield the highest accuracy, though the task remains difficult and supervised approaches have lower false positive rates. This research offers insights into features and methods for the novel task of integrating personal data into scientific jargon identification.",
        "author": "Yue Guo; Joseph Chee Chang; Maria Antoniak; Erin Bransom; Trevor Cohen; Lucy Wang; Tal August",
        "authorids": "/y/yue-guo/; /j/joseph-chee-chang/; /m/maria-antoniak/; /e/erin-bransom/; /t/trevor-cohen/; /l/lucy-lu-wang/; /t/tal-august/",
        "bibtex": "@inproceedings{guo-etal-2024-personalized,\n    title = \"Personalized Jargon Identification for Enhanced Interdisciplinary Communication\",\n    author = \"Guo, Yue  and\n      Chang, Joseph Chee  and\n      Antoniak, Maria  and\n      Bransom, Erin  and\n      Cohen, Trevor  and\n      Wang, Lucy  and\n      August, Tal\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.255/\",\n    doi = \"10.18653/v1/2024.naacl-long.255\",\n    pages = \"4535--4550\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.255.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.255/",
        "pdf_size": 1945260,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17715669511467023506&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "University of Washington; Allen Institute for AI; Allen Institute for AI; Allen Institute for AI; University of Washington; University of Washington + Allen Institute for AI; Allen Institute for AI",
        "aff_domain": "uw.edu;allenai.org;allenai.org;allenai.org;uw.edu;uw.edu;allenai.org",
        "email": "uw.edu;allenai.org;allenai.org;allenai.org;uw.edu;uw.edu;allenai.org",
        "github": "https://github.com/talaugust/PersonalizedJargon",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;0;0+1;1",
        "aff_unique_norm": "University of Washington;Allen Institute for AI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.washington.edu;https://allenai.org",
        "aff_unique_abbr": "UW;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.8",
        "title": "Personalized Review Recommendation based on Implicit dimension mining",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Users usually browse product reviews before buying products from e-commerce websites. Lots of e-commerce websites can recommend reviews. However, existing research on review recommendation mainly focuses on the general usefulness of reviews and ignores personalized and implicit requirements. To address the issue, we propose a Large language model driven Personalized Review Recommendation model based on Implicit dimension mining (PRR-LI). The model mines implicit dimensions from reviews and requirements, and encodes them in the form of \u201ctext + dimension\u201d. The experiments show that our model significantly outperforms other state-of-the-art textual models on the Amazon-MRHP dataset, with some of the metrics outperforming the state-of-the-art multimodal models. And we prove that encoding \u201ctext + dimension\u201d is better than encoding \u201ctext\u201d and \u201cdimension\u201d separately in review recommendation.",
        "author": "Bei Xu; Yifan Xu",
        "authorids": "/b/bei-xu/; /y/yifan-xu/",
        "bibtex": "@inproceedings{xu-xu-2024-personalized,\n    title = \"Personalized Review Recommendation based on Implicit dimension mining\",\n    author = \"Xu, Bei  and\n      Xu, Yifan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.8/\",\n    doi = \"10.18653/v1/2024.naacl-short.8\",\n    pages = \"86--91\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.8.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.8/",
        "pdf_size": 1094584,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9598721631763747985&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing 210023, China + Jiangsu Key Laboratory of Big Data Security & Intelligent Processing, Nanjing 210023, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing 210023, China + Jiangsu Key Laboratory of Big Data Security & Intelligent Processing, Nanjing 210023, China",
        "aff_domain": "njupt.edu.cn;foxmail.com",
        "email": "njupt.edu.cn;foxmail.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "Nanjing University of Posts and Telecommunications;Jiangsu Key Laboratory of Big Data Security & Intelligent Processing",
        "aff_unique_dep": "School of Computer Science;Big Data Security & Intelligent Processing",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "0+0;0+0",
        "aff_campus_unique": "Nanjing",
        "aff_country_unique_index": "0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.364",
        "title": "PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we conduct a study to utilize LLMs as a solution for decision making that requires complex data analysis. We define **Decision QA** as the task of answering the best decision, dbest, for a decision-making question Q, business rules R and a database D. Since there is no benchmark that can examine Decision QA, we propose Decision QA benchmark, **DQA**. It has two scenarios, Locating and Building, constructed from two video games (Europa Universalis IV and Victoria 3) that have almost the same goal as Decision QA. To address Decision QA effectively, we also propose a new RAG technique called the *iterative plan-then-retrieval augmented generation* (**PlanRAG**). Our PlanRAG-based LM generates the plan for decision making as the first step, and the retriever generates the queries for data analysis as the second step. The proposed method outperforms the state-of-the-art iterative RAG method by 15.8% in the Locating scenario and by 7.4% in the Building scenario, respectively. We release our code and benchmark at https://github.com/myeon9h/PlanRAG.",
        "author": "Myeonghwa Lee; Seonho An; Min-Soo Kim",
        "authorids": "/m/myeonghwa-lee/; /s/seonho-an/; /m/min-soo-kim/",
        "bibtex": "@inproceedings{lee-etal-2024-planrag,\n    title = \"{P}lan{RAG}: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers\",\n    author = \"Lee, Myeonghwa  and\n      An, Seonho  and\n      Kim, Min-Soo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.364/\",\n    doi = \"10.18653/v1/2024.naacl-long.364\",\n    pages = \"6537--6555\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.364.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.364/",
        "pdf_size": 1503739,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7525992937151511307&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computing, KAIST; School of Computing, KAIST; School of Computing, KAIST",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "github": "https://github.com/myeon9h/PlanRAG",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "KAIST",
        "aff_unique_dep": "School of Computing",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.findings-naacl.61",
        "title": "Planning and Editing What You Retrieve for Enhanced Tool Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel PLUTO (Planning, Learning, and Understanding for TOols) approach, encompassing \u201cPlan-and-Retrieve (P&R)\u201d and \u201cEdit-and-Ground (E&G)\u201d paradigms. The P&R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E&G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall and NDCG in tool retrieval tasks, significantly surpassing current state-of-the-art models.",
        "author": "Tenghao Huang; Dongwon Jung; Vaibhav Kumar; Mohammad Kachuee; Xiang Li; Puyang Xu; Muhao Chen",
        "authorids": "/t/tenghao-huang/; /d/dongwon-jung/; /v/vaibhav-kumar/; /m/mohammad-kachuee/; /x/xiang-li/; /p/puyang-xu/; /m/muhao-chen/",
        "bibtex": "@inproceedings{huang-etal-2024-planning,\n    title = \"Planning and Editing What You Retrieve for Enhanced Tool Learning\",\n    author = \"Huang, Tenghao  and\n      Jung, Dongwon  and\n      Kumar, Vaibhav  and\n      Kachuee, Mohammad  and\n      Li, Xiang  and\n      Xu, Puyang  and\n      Chen, Muhao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.61/\",\n    doi = \"10.18653/v1/2024.findings-naacl.61\",\n    pages = \"975--988\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.61.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.61/",
        "pdf_size": 1196882,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13160653510987742989&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2024.findings-naacl.139",
        "title": "Plug-in Language Model: Controlling Text Generation with a Simple Regression Model",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large-scale pre-trained language models have displayed unrivaled capacity in generating text that closely resembles human-written text. Nevertheless, generating texts adhering to specific conditions without fine-tuning or adding new parameters can be challenging. Contemporary approaches commonly rely on either prompts or auxiliary models to avoid modifying the language models. These auxiliary models are designed to assess whether a generated token contributes to meeting the desired requirements. These approaches adjust the distribution of the next token during the inference phase by leveraging the prediction score of the desired attribute to calculate gradients. However, these auxiliary models typically require the language model\u2019s latent states. This prerequisite challenges integrating various existing black box attribute models or tools. We present the Plug-in Language Model (PiLM) as a solution to address the limitations. PiLM leverages reinforcement learning to utilize black box tools directly, adjusting the latent state to control text generation. However, performing backpropagation during the inference phase is time-consuming for PiLM. By replacing backpropagation with a simple regression model, PiLM can achieve an inference time comparable to that of the original LLM. Experiment results show that our approaches in this paper outperform existing state-of-the-art methods that rely on gradient-based, weighted decoding, or prompt-based methodologies.",
        "author": "Nai-Chi Yang; Wei-Yun Ma; Pu-Jen Cheng",
        "authorids": "/n/nai-chi-yang/; /w/wei-yun-ma/; /p/pu-jen-cheng/",
        "bibtex": "@inproceedings{yang-etal-2024-plug,\n    title = \"Plug-in Language Model: Controlling Text Generation with a Simple Regression Model\",\n    author = \"Yang, Nai-Chi  and\n      Ma, Wei-Yun  and\n      Cheng, Pu-Jen\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.139/\",\n    doi = \"10.18653/v1/2024.findings-naacl.139\",\n    pages = \"2165--2181\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.139.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.139/",
        "pdf_size": 346359,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:mBTKzAHz8R4J:scholar.google.com/&scioq=Plug-in+Language+Model:+Controlling+Text+Generation+with+a+Simple+Regression+Model&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Academia Sinica+National Taiwan University; Academia Sinica; National Taiwan University",
        "aff_domain": "iis.sinica.edu.tw;iis.sinica.edu.tw;csie.ntu.edu.tw",
        "email": "iis.sinica.edu.tw;iis.sinica.edu.tw;csie.ntu.edu.tw",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0;1",
        "aff_unique_norm": "Academia Sinica;National Taiwan University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sinica.edu.tw;https://www.ntu.edu.tw",
        "aff_unique_abbr": "Academia Sinica;NTU",
        "aff_campus_unique_index": "0+0;0;0",
        "aff_campus_unique": "Taiwan",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.291",
        "title": "Polarity Calibration for Opinion Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Opinion summarization is automatically generating summaries from a variety of subjective information, such as product reviews or political opinions. The challenge of opinions summarization lies in presenting divergent or even conflicting opinions. We conduct an analysis of previous summarization models, which reveals their inclination to amplify the polarity bias, emphasizing the majority opinions while ignoring the minority opinions. To address this issue and make the summarizer express both sides of opinions, we introduce the concept of polarity calibration, which aims to align the polarity of output summary with that of input text. Specifically, we develop a reinforcement training approach for polarity calibration. This approach feeds the polarity distance between output summary and input text as reward into the summarizer, and also balance polarity calibration with content preservation and language naturality. We evaluate our Polarity Calibration model (PoCa) on two types of opinions summarization tasks: summarizing product reviews and political opinions articles. Automatic and human evaluation demonstrate that our approach can mitigate the polarity mismatch between output summary and input text, as well as maintain the content semantic and language quality.",
        "author": "Yuanyuan Lei; Kaiqiang Song; Sangwoo Cho; Xiaoyang Wang; Ruihong Huang; Dong Yu",
        "authorids": "/y/yuanyuan-lei/; /k/kaiqiang-song/; /s/sangwoo-cho/; /x/xiaoyang-wang/; /r/ruihong-huang/; /d/dong-yu/",
        "bibtex": "@inproceedings{lei-etal-2024-polarity,\n    title = \"Polarity Calibration for Opinion Summarization\",\n    author = \"Lei, Yuanyuan  and\n      Song, Kaiqiang  and\n      Cho, Sangwoo  and\n      Wang, Xiaoyang  and\n      Huang, Ruihong  and\n      Yu, Dong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.291/\",\n    doi = \"10.18653/v1/2024.naacl-long.291\",\n    pages = \"5211--5224\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.291.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.291/",
        "pdf_size": 1468923,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6003178113883720985&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 4,
        "aff": "Texas A&M University; Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA; Tencent AI Lab, Bellevue, WA; Texas A&M University; Tencent AI Lab, Bellevue, WA",
        "aff_domain": "tamu.edu;global.tencent.com;global.tencent.com;global.tencent.com;tamu.edu;global.tencent.com",
        "email": "tamu.edu;global.tencent.com;global.tencent.com;global.tencent.com;tamu.edu;global.tencent.com",
        "github": "https://github.com/yuanyuanlei-nlp/polarity_calibration_naacl_2024",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;0;1",
        "aff_unique_norm": "Texas A&M University;Tencent",
        "aff_unique_dep": ";AI Lab",
        "aff_unique_url": "https://www.tamu.edu;https://ai.tencent.com",
        "aff_unique_abbr": "TAMU;Tencent AI Lab",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Bellevue",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.483",
        "title": "Pre-trained Language Models for Entity Blocking: A Reproducibility Study",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Entity Resolution (ER) is an essential task in data integration and its goal is to find records that represent the same entity in a dataset. Deep learning models, especially large pre-trained language models, have achieved state-of-the-art results on this task. A typical ER pipeline consists of Entity Blocking and Entity Matching: Entity Blocking finds candidate record pairs that potentially match and Entity Matching determines if the pairs match. The goal of the entity blocking step is to include as many matching pairs as possible while including as few non-matching pairs as possible. On the other hand, the blocking task can also be considered as an Information Retrieval (IR) task. However, state-of-the-art neural IR models that are based on large language models have not been evaluated on the ER task. What\u2019s more, the generalization ability of state-of-the-art methods for entity blocking is not well-studied but an import aspect in real-world applications. In this work, we evaluate state-of-the-art models for Entity Blocking along with neural IR models on a wide range of real-world datasets, and also study their in-distribution and out-of-distribution generalization abilities.",
        "author": "Runhui Wang; Yongfeng Zhang",
        "authorids": "/r/runhui-wang/; /y/yongfeng-zhang/",
        "bibtex": "@inproceedings{wang-zhang-2024-pre,\n    title = \"Pre-trained Language Models for Entity Blocking: A Reproducibility Study\",\n    author = \"Wang, Runhui  and\n      Zhang, Yongfeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.483/\",\n    doi = \"10.18653/v1/2024.naacl-long.483\",\n    pages = \"8720--8730\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.483.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.483/",
        "pdf_size": 356308,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17424076196038863281&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2024.naacl-long.403",
        "title": "Pregnant Questions: The Importance of Pragmatic Awareness in Maternal Health Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Questions posed by information-seeking users often contain implicit false or potentially harmful assumptions. In a high-risk domain such as maternal and infant health, a question-answering system must recognize these pragmatic constraints and go beyond simply answering user questions, examining them in context to respond helpfully. To achieve this, we study assumptions and implications, or pragmatic inferences, made when mothers ask questions about pregnancy and infant care by collecting a dataset of 2,727 inferences from 500 questions across three diverse sources. We study how health experts naturally address these inferences when writing answers, and illustrate that informing existing QA pipelines with pragmatic inferences produces responses that are more complete, mitigating the propagation of harmful beliefs.",
        "author": "Neha Srikanth; Rupak Sarkar; Heran Mane; Elizabeth Aparicio; Quynh Nguyen; Rachel Rudinger; Jordan Boyd-Graber",
        "authorids": "/n/neha-srikanth/; /r/rupak-sarkar/; /h/heran-mane/; /e/elizabeth-aparicio/; /q/quynh-nguyen/; /r/rachel-rudinger/; /j/jordan-boyd-graber/",
        "bibtex": "@inproceedings{srikanth-etal-2024-pregnant,\n    title = \"Pregnant Questions: The Importance of Pragmatic Awareness in Maternal Health Question Answering\",\n    author = \"Srikanth, Neha  and\n      Sarkar, Rupak  and\n      Mane, Heran  and\n      Aparicio, Elizabeth  and\n      Nguyen, Quynh  and\n      Rudinger, Rachel  and\n      Boyd-Graber, Jordan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.403/\",\n    doi = \"10.18653/v1/2024.naacl-long.403\",\n    pages = \"7253--7268\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.403.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.403/",
        "pdf_size": 933132,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4220413738105442943&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of Maryland; Department of Computer Science, University of Maryland; Department of Epidemiology and Biostatistics, University of Maryland; Department of Behavioral and Community Health, University of Maryland; Department of Epidemiology and Biostatistics, University of Maryland; Department of Computer Science, University of Maryland; Department of Computer Science, University of Maryland",
        "aff_domain": "umd.edu;umd.edu; ; ; ; ; ",
        "email": "umd.edu;umd.edu; ; ; ; ; ",
        "github": "https://github.com/nehasrikn/pragmatic-inferences-qa7253",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "University of Maryland",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www/umd.edu",
        "aff_unique_abbr": "UMD",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.127",
        "title": "Principles from Clinical Research for NLP Model Generalization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The NLP community typically relies on performance of a model on a held-out test set to assess generalization. Performance drops observed in datasets outside of official test sets are generally attributed to \u201cout-of-distribution\u201d effects. Here, we explore the foundations of generalizability and study the factors that affect it, articulating lessons from clinical studies. In clinical research, generalizability is an act of reasoning that depends on (a) *internal validity* of experiments to ensure controlled measurement of cause and effect, and (b) *external validity* or transportability of the results to the wider population. We demonstrate how learning spurious correlations, such as the distance between entities in relation extraction tasks, can affect a model\u2019s internal validity and in turn adversely impact generalization. We, therefore, present the need to ensure internal validity when building machine learning models in NLP. Our recommendations also apply to generative large language models, as they are known to be sensitive to even minor semantic preserving alterations. We also propose adapting the idea of *matching* in randomized controlled trials and observational studies to NLP evaluation to measure causation.",
        "author": "Aparna Elangovan; Jiayuan He; Yuan Li; Karin Verspoor",
        "authorids": "/a/aparna-elangovan/; /j/jiayuan-he/; /y/yuan-li/; /k/karin-verspoor/",
        "bibtex": "@inproceedings{elangovan-etal-2024-principles,\n    title = \"Principles from Clinical Research for {NLP} Model Generalization\",\n    author = \"Elangovan, Aparna  and\n      He, Jiayuan  and\n      Li, Yuan  and\n      Verspoor, Karin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.127/\",\n    doi = \"10.18653/v1/2024.naacl-long.127\",\n    pages = \"2293--2309\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.127.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.127/",
        "pdf_size": 1075446,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:TkAo9bLj6HgJ:scholar.google.com/&scioq=Principles+from+Clinical+Research+for+NLP+Model+Generalization&hl=en&as_sdt=0,33",
        "gs_version_total": 4,
        "aff": "The University of Melbourne, Australia + RMIT University, Australia; RMIT University, Australia + The University of Melbourne, Australia; RMIT University, Australia + The University of Melbourne, Australia; RMIT University, Australia + The University of Melbourne, Australia",
        "aff_domain": "student.unimelb.edu.au;rmit.edu.au;rmit.edu.au;rmit.edu.au",
        "email": "student.unimelb.edu.au;rmit.edu.au;rmit.edu.au;rmit.edu.au",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1+0;1+0;1+0",
        "aff_unique_norm": "University of Melbourne;RMIT University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.rmit.edu.au",
        "aff_unique_abbr": "UniMelb;RMIT",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2024.findings-naacl.212",
        "title": "Probing the Category of Verbal Aspect in Transformer Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We investigate how pretrained language models (PLM) encode the grammatical category of verbal aspect in Russian. Encoding of aspect in transformer LMs has not been studied previously in any language. A particular challenge is posed by \u201dalternative contexts\u201d: where either the perfective or the imperfective aspect is suitable grammatically and semantically. We perform probing using BERT and RoBERTa on alternative and non-alternative contexts. First, we assess the models\u2019 performance on aspect prediction, via behavioral probing. Next, we examine the models\u2019 performance when their contextual representations are substituted with counterfactual representations, via causal probing. These counterfactuals alter the value of the \u201cboundedness\u201d feature\u2014a semantic feature, which characterizes the action in the context. Experiments show that BERT and RoBERTa do encode aspect\u2014mostly in their final layers. The counterfactual interventions affect perfective and imperfective in opposite ways, which is consistent with grammar: perfective is positively affected by adding the meaning of boundedness, and vice versa. The practical implications of our probing results are that fine-tuning only the last layers of BERT on predicting aspect is faster and more effective than fine-tuning the whole model. The model has high predictive uncertainty about aspect in alternative contexts, which tend to lack explicit hints about the boundedness of the described action.",
        "author": "Anisia Katinskaia; Roman Yangarber",
        "authorids": "/a/anisia-katinskaia/; /r/roman-yangarber/",
        "bibtex": "@inproceedings{katinskaia-yangarber-2024-probing,\n    title = \"Probing the Category of Verbal Aspect in Transformer Language Models\",\n    author = \"Katinskaia, Anisia  and\n      Yangarber, Roman\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.212/\",\n    doi = \"10.18653/v1/2024.findings-naacl.212\",\n    pages = \"3347--3366\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.212.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.212/",
        "pdf_size": 4124008,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11145053269643471583&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science\u2662Department of Digital Humanities; Department of Computer Science\u2662Department of Digital Humanities",
        "aff_domain": "helsinki.fi;helsinki.fi",
        "email": "helsinki.fi;helsinki.fi",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Department of Computer Science",
        "aff_unique_dep": "Computer Science",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2024.findings-naacl.150",
        "title": "Product Description and QA Assisted Self-Supervised Opinion Summarization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "In e-commerce, opinion summarization is the process of summarizing the consensus opinions found in product reviews. However, the potential of additional sources such as product description and question-answers (QA) has been considered less often. Moreover, the absence of any supervised training data makes this task challenging. To address this, we propose a novel synthetic dataset creation (SDC) strategy that leverages information from reviews as well as additional sources for selecting one of the reviews as a pseudo-summary to enable supervised training. Our Multi-Encoder Decoder framework for Opinion Summarization (MEDOS) employs a separate encoder for each source, enabling effective selection of information while generating the summary. For evaluation, due to the unavailability of test sets with additional sources, we extend the Amazon, Oposum+, and Flipkart test sets and leverage ChatGPT to annotate summaries. Experiments across nine test sets demonstrate that the combination of our SDC approach and MEDOS model achieves on average a 14.5% improvement in ROUGE-1 F1 over the SOTA. Moreover, comparative analysis underlines the significance of incorporating additional sources for generating more informative summaries. Human evaluations further indicate that MEDOS scores relatively higher in coherence and fluency with 0.41 and 0.5 (\u22121 to 1) respectively, compared to existing models. To the best of our knowledge, we are the first to generate opinion summaries leveraging additional sources in a self-supervised setting.",
        "author": "Tejpalsingh Siledar; Rupasai Rangaraju; Sankara Muddu; Suman Banerjee; Amey Patil; Sudhanshu Singh; Muthusamy Chelliah; Nikesh Garera; Swaprava Nath; Pushpak Bhattacharyya",
        "authorids": "/t/tejpalsingh-siledar/; /r/rupasai-rangaraju/; /s/sankara-muddu/; /s/suman-banerjee/; /a/amey-patil/; /s/sudhanshu-singh/; /m/muthusamy-chelliah/; /n/nikesh-garera/; /s/swaprava-nath/; /p/pushpak-bhattacharyya/",
        "bibtex": "@inproceedings{siledar-etal-2024-product,\n    title = \"Product Description and {QA} Assisted Self-Supervised Opinion Summarization\",\n    author = \"Siledar, Tejpalsingh  and\n      Rangaraju, Rupasai  and\n      Muddu, Sankara  and\n      Banerjee, Suman  and\n      Patil, Amey  and\n      Singh, Sudhanshu  and\n      Chelliah, Muthusamy  and\n      Garera, Nikesh  and\n      Nath, Swaprava  and\n      Bhattacharyya, Pushpak\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.150/\",\n    doi = \"10.18653/v1/2024.findings-naacl.150\",\n    pages = \"2315--2332\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.150.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.150/",
        "pdf_size": 3514046,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5118884022505770397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Computer Science and Engineering, IIT Bombay, India; Computer Science and Engineering, IIT Bombay, India; Computer Science and Engineering, IIT Bombay, India; Computer Science and Engineering, IIT Bombay, India; Computer Science and Engineering, IIT Bombay, India; Flipkart, India; Flipkart, India; Flipkart, India; Flipkart, India; Flipkart, India",
        "aff_domain": "cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in; ; ; ; ; ",
        "email": "cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in;cse.iitb.ac.in; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;1;1;1;1;1",
        "aff_unique_norm": "IIT Bombay;Flipkart",
        "aff_unique_dep": "Computer Science and Engineering;",
        "aff_unique_url": "https://www.iitb.ac.in;https://www.flipkart.com",
        "aff_unique_abbr": "IITB;Flipkart",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Bombay;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "India"
    },
    {
        "id": "2024.naacl-long.125",
        "title": "Program-Aided Reasoners (Better) Know What They Know",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to \u201cknow what they know\u201d, which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types - LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.",
        "author": "Anubha Kabra; Sanketh Rangreji; Yash Mathur; Aman Madaan; Emmy Liu; Graham Neubig",
        "authorids": "/a/anubha-kabra/; /s/sanketh-rangreji/; /y/yash-mathur/; /a/aman-madaan/; /e/emmy-liu/; /g/graham-neubig/",
        "bibtex": "@inproceedings{kabra-etal-2024-program,\n    title = \"Program-Aided Reasoners (Better) Know What They Know\",\n    author = \"Kabra, Anubha  and\n      Rangreji, Sanketh  and\n      Mathur, Yash  and\n      Madaan, Aman  and\n      Liu, Emmy  and\n      Neubig, Graham\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.125/\",\n    doi = \"10.18653/v1/2024.naacl-long.125\",\n    pages = \"2262--2278\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.125.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.125/",
        "pdf_size": 2600334,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15069563711308854811&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "https://anonymous.4open.science/r/code-calibrates-A61D/",
        "author_num": 6
    },
    {
        "id": "2024.findings-naacl.119",
        "title": "Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably, without the help of the CoT method and the prompt \u201cLet\u2019s think step by step\u201d, Prompt Space shows superior performance over the few-shot method. Overall, our approach provides a robust and effective mathematical framework for selecting simple and effective prompts. This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs. Our code is publicly available at https://github.com/YouBLEI/Prompt-Space",
        "author": "Fobo Shi; Peijun Qing; Dong Yang; Nan Wang; Youbo Lei; Haonan Lu; Xiaodong Lin; Duantengchuan Li",
        "authorids": "/f/fobo-shi/; /p/peijun-qing/; /d/dong-yang/; /n/nan-wang/; /y/youbo-lei/; /h/haonan-lu/; /x/xiaodong-lin/; /d/duantengchuan-li/",
        "bibtex": "@inproceedings{shi-etal-2024-prompt,\n    title = \"Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models\",\n    author = \"Shi, Fobo  and\n      Qing, Peijun  and\n      Yang, Dong  and\n      Wang, Nan  and\n      Lei, Youbo  and\n      Lu, Haonan  and\n      Lin, Xiaodong  and\n      Li, Duantengchuan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.119/\",\n    doi = \"10.18653/v1/2024.findings-naacl.119\",\n    pages = \"1836--1862\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.119.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.119/",
        "pdf_size": 2993404,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6593441198642818619&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Wuhan University; Dartmouth College; OPPO Research Institute; Xi\u2019an Jiaotong University; OPPO Research Institute; OPPO Research Institute; Rugster Unversity; Wuhan University",
        "aff_domain": "; ;my.cityu.edu.hk; ; ;oppo.com; ; ",
        "email": "; ;my.cityu.edu.hk; ; ;oppo.com; ; ",
        "github": "https://github.com/YouBLEI/Prompt-Space",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;3;2;2;4;0",
        "aff_unique_norm": "Wuhan University;Dartmouth College;OPPO Research Institute;Xi'an Jiao Tong University;Rutgers University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "http://www.whu.edu.cn/;https://www.dartmouth.edu;https://www.oppo.com/en;https://www.xjtu.edu.cn;https://www.rutgers.edu",
        "aff_unique_abbr": "WHU;Dartmouth;OPPO RI;XJTU;Rutgers",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-industry.10",
        "title": "Prompt Tuned Embedding Classification for Industry Sector Allocation",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "We introduce Prompt Tuned Embedding Classification (PTEC) for classifying companies within an investment firm\u2019s proprietary industry taxonomy, supporting their thematic investment strategy. PTEC assigns companies to the sectors they primarily operate in, conceptualizing this process as a multi-label text classification task. Prompt Tuning, usually deployed as a text-to-text (T2T) classification approach, ensures low computational cost while maintaining high task performance. However, T2T classification has limitations on multi-label tasks due to the generation of non-existing labels, permutation invariance of the label sequence, and a lack of confidence scores. PTEC addresses these limitations by utilizing a classification head in place of the Large Language Models (LLMs) language head. PTEC surpasses both baselines and human performance while lowering computational demands. This indicates the continuing need to adapt state-of-the-art methods to domain-specific tasks, even in the era of LLMs with strong generalization abilities.",
        "author": "Valentin Buchner; Lele Cao; Jan-Christoph Kalo; Vilhelm Von Ehrenheim",
        "authorids": "/v/valentin-buchner/; /l/lele-cao/; /j/jan-christoph-kalo/; /v/vilhelm-von-ehrenheim/",
        "bibtex": "@inproceedings{buchner-etal-2024-prompt,\n    title = \"Prompt Tuned Embedding Classification for Industry Sector Allocation\",\n    author = \"Buchner, Valentin  and\n      Cao, Lele  and\n      Kalo, Jan-Christoph  and\n      Von Ehrenheim, Vilhelm\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.10/\",\n    doi = \"10.18653/v1/2024.naacl-industry.10\",\n    pages = \"108--118\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.10.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.10/",
        "pdf_size": 790957,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7515345366080217465&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Motherbrain, EQT Group, Stockholm, Sweden+Vrije Universiteit Amsterdam; Motherbrain, EQT Group, Stockholm, Sweden; University of Amsterdam; Motherbrain, EQT Group, Stockholm, Sweden",
        "aff_domain": "eqtpartners.com;eqtpartners.com;uva.nl;eqtpartners.com",
        "email": "eqtpartners.com;eqtpartners.com;uva.nl;eqtpartners.com",
        "github": "https://github.com/EQTPartners/PTEC",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0;2;0",
        "aff_unique_norm": "Motherbrain;Vrije Universiteit Amsterdam;University of Amsterdam",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.vu.nl;https://www.uva.nl",
        "aff_unique_abbr": ";VU Amsterdam;UvA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0;1;0",
        "aff_country_unique": "Sweden;Netherlands"
    },
    {
        "id": "2024.naacl-long.268",
        "title": "Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.github.io .",
        "author": "Yongqi Wang; Ruofan Hu; Rongjie Huang; Zhiqing Hong; Ruiqi Li; Wenrui Liu; Fuming You; Tao Jin; Zhou Zhao",
        "authorids": "/y/yongqi-wang/; /r/ruofan-hu/; /r/rongjie-huang/; /z/zhiqing-hong/; /r/ruiqi-li/; /w/wenrui-liu/; /f/fuming-you/; /t/tao-jin/; /z/zhou-zhao/",
        "bibtex": "@inproceedings{wang-etal-2024-prompt,\n    title = \"Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt\",\n    author = \"Wang, Yongqi  and\n      Hu, Ruofan  and\n      Huang, Rongjie  and\n      Hong, Zhiqing  and\n      Li, Ruiqi  and\n      Liu, Wenrui  and\n      You, Fuming  and\n      Jin, Tao  and\n      Zhao, Zhou\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.268/\",\n    doi = \"10.18653/v1/2024.naacl-long.268\",\n    pages = \"4780--4794\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.268.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.268/",
        "pdf_size": 587059,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10305936552962695765&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn; ; ; ; ; ; ; ",
        "email": "zju.edu.cn;zju.edu.cn; ; ; ; ; ; ; ",
        "github": "",
        "project": "http://prompt-singer.github.io",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.zju.edu.cn",
        "aff_unique_abbr": "ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.177",
        "title": "PromptFix: Few-shot Backdoor Removal via Adversarial Prompt Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Pre-trained language models (PLMs) have attracted enormous attention over the past few years with their unparalleled performances. Meanwhile, the soaring cost to train PLMs as well as their amazing generalizability have jointly contributed to few-shot fine-tuning and prompting as the most popular training paradigms for natural language processing (NLP) models. Nevertheless, existing studies have shown that these NLP models can be backdoored such that model behavior is manipulated when trigger tokens are presented.In this paper, we propose PromptFix, a novel backdoor mitigation strategy for NLP models via adversarial prompt-tuning in few-shot settings.Unlike existing NLP backdoor removal methods, which rely on accurate trigger inversion and subsequent model fine-tuning, PromptFix keeps the model parameters intact and only utilizes two extra sets of soft tokens which approximate the trigger and counteract it respectively. The use of soft tokens and adversarial optimization eliminates the need to enumerate possible backdoor configurations and enables an adaptive balance between trigger finding and preservation of performance.Experiments with various backdoor attacks validate the effectiveness of the proposed method and the performances when domain shift is present further shows PromptFix\u2019s applicability to models pretrained on unknown data source which is the common case in prompt tuning scenarios.",
        "author": "Tianrong Zhang; Zhaohan Xi; Ting Wang; Prasenjit Mitra; Jinghui Chen",
        "authorids": "/t/tianrong-zhang/; /z/zhaohan-xi/; /t/ting-wang/; /p/prasenjit-mitra/; /j/jinghui-chen/",
        "bibtex": "@inproceedings{zhang-etal-2024-promptfix,\n    title = \"{P}rompt{F}ix: Few-shot Backdoor Removal via Adversarial Prompt Tuning\",\n    author = \"Zhang, Tianrong  and\n      Xi, Zhaohan  and\n      Wang, Ting  and\n      Mitra, Prasenjit  and\n      Chen, Jinghui\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.177/\",\n    doi = \"10.18653/v1/2024.naacl-long.177\",\n    pages = \"3212--3225\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.177.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.177/",
        "pdf_size": 483880,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6652982913500088575&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "School of Information Science & Technology, Pennsylvania State University; School of Information Science & Technology, Pennsylvania State University; Department of Computer Science, Stony Brook University; School of Information Science & Technology, Pennsylvania State University; School of Information Science & Technology, Pennsylvania State University",
        "aff_domain": "psu.edu;psu.edu;cs.stonybrook.edu;psu.edu;psu.edu",
        "email": "psu.edu;psu.edu;cs.stonybrook.edu;psu.edu;psu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Pennsylvania State University;Stony Brook University",
        "aff_unique_dep": "School of Information Science & Technology;Department of Computer Science",
        "aff_unique_url": "https://www.psu.edu;https://www.stonybrook.edu",
        "aff_unique_abbr": "PSU;SBU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stony Brook",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.236",
        "title": "Prompting Few-shot Multi-hop Question Generation via Comprehending Type-aware Semantics",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Given several documents, multi-hop question generation (MQG) is a task aims to generate complicated questions that require reasoning over multiple pieces of these documents to find the answer. To perform this task, existing studies focus on designing advanced architectures to locate essential keywords or sentences in multiple documents and then generate questions accordingly, where they normally do not note that question types could provide crucial hints for extracting key information from the documents for MQG. In general, supervised approaches are used that rely on large annotated data, which is not available in many low-resource scenarios and thus makes MQG hard in these domains. Consider the recent success of large language models (LLMs) on natural language processing tasks using limited labeled data under few-shot settings, in this paper, we propose an approach named type-aware semantics extraction-based chain-of-thought method (TASE-CoT) for few-shot MQG. Specifically, our approach firstly extracts question types and essential semantic phrases from the given documents and the answer. Then, we design a three-step CoT template to leverage the extracted question type and semantic phrases to predict multi-hop questions. Extensive experiments and the results demonstrate the effectiveness of our approach and the proposed modules.",
        "author": "Zefeng Lin; Weidong Chen; Yan Song; Yongdong Zhang",
        "authorids": "/z/zefeng-lin/; /w/weidong-chen/; /y/yan-song/; /y/yongdong-zhang/",
        "bibtex": "@inproceedings{lin-etal-2024-prompting,\n    title = \"Prompting Few-shot Multi-hop Question Generation via Comprehending Type-aware Semantics\",\n    author = \"Lin, Zefeng  and\n      Chen, Weidong  and\n      Song, Yan  and\n      Zhang, Yongdong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.236/\",\n    doi = \"10.18653/v1/2024.findings-naacl.236\",\n    pages = \"3730--3740\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.236.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.236/",
        "pdf_size": 693439,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9183022270845162980&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China",
        "aff_domain": "mail.ustc.edu.cn;ustc.edu.cn;gmail.com;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;ustc.edu.cn;gmail.com;ustc.edu.cn",
        "github": "https://github.com/synlp/TASE-CoT",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.178",
        "title": "Prompting Vision-Language Models For Aspect-Controlled Generation of Referring Expressions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Referring Expression Generation (REG) is the task of generating a description that unambiguously identifies a given target in the scene. Different from Image Captioning (IC), REG requires learning fine-grained characteristics of not only the scene objects but also their surrounding context. Referring expressions are usually not singular; an object can often be uniquely referenced in numerous ways, for instance, by color, by location, or by relationship with other objects. Most prior works, however, have not explored this \u2018aspect-based multiplicity\u2019 of referring expressions. Hence, in this work, we focus on the Aspect-Controlled REG task, which requires generating a referring expression conditioned on the input aspect(s), where an aspect captures a style of reference. By changing the input aspect such as color, location, action etc., one can generate multiple distinct expressions per target region. To solve this new task, we first modify BLIP for aligning image-regions and text-expressions. We achieve this through a novel approach for feeding the input by drawing a bounding box around the target image-region and prompting the model to generate the referring expression. Our base REG model already beats all prior works in CIDEr score. To tackle Aspect-Controlled REG, we append \u2018aspect tokens\u2019 to the prompt and show that distinct expressions can be generated by just changing the prompt. Finally, to prove the high-quality and diversity of the data generated by our proposed aspect-controlled REG model, we also perform data-augmentation-based evaluation on the downstream Referring Expression Comprehension (REC) task. With just half of the real data augmented with the generated synthetic data, we achieve performance comparable to training with 100% of real data, using a SOTA REC model.",
        "author": "Danfeng Guo; Sanchit Agarwal; Arpit Gupta; Jiun-Yu Kao; Emre Barut; Tagyoung Chung; Jing Huang; Mohit Bansal",
        "authorids": "/d/danfeng-guo/; /s/sanchit-agarwal/; /a/arpit-gupta/; /j/jiun-yu-kao/; /e/emre-barut/; /t/tagyoung-chung/; /j/jing-huang/; /m/mohit-bansal/",
        "bibtex": "@inproceedings{guo-etal-2024-prompting,\n    title = \"Prompting Vision-Language Models For Aspect-Controlled Generation of Referring Expressions\",\n    author = \"Guo, Danfeng  and\n      Agarwal, Sanchit  and\n      Gupta, Arpit  and\n      Kao, Jiun-Yu  and\n      Barut, Emre  and\n      Chung, Tagyoung  and\n      Huang, Jing  and\n      Bansal, Mohit\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.178/\",\n    doi = \"10.18653/v1/2024.findings-naacl.178\",\n    pages = \"2793--2807\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.178.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.178/",
        "pdf_size": 7163119,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:BwTH7rWtO4oJ:scholar.google.com/&scioq=Prompting+Vision-Language+Models+For+Aspect-Controlled+Generation+of+Referring+Expressions&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2024.naacl-long.7",
        "title": "Promptly Predicting Structures: The Return of Inference",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prompt-based methods have been used extensively across NLP to build zero- and few-shot label predictors. Many NLP tasks are naturally structured: that is, their outputs consist of multiple labels which constrain each other. Annotating data for such tasks can be cumbersome. Can the promise of the prompt-based paradigm be extended to such structured outputs? In this paper, we present a framework for constructing zero- and few-shot linguistic structure predictors. Our key insight is that we can use structural constraints\u2014and combinatorial inference derived from them\u2014to filter out inconsistent structures predicted by large language models. We instantiated this framework on two structured prediction tasks, and five datasets. Across all cases, our results show that enforcing consistency not only constructs structurally valid outputs, but also improves performance over the unconstrained variants.",
        "author": "Maitrey Mehta; Valentina Pyatkin; Vivek Srikumar",
        "authorids": "/m/maitrey-mehta/; /v/valentina-pyatkin/; /v/vivek-srikumar/",
        "bibtex": "@inproceedings{mehta-etal-2024-promptly,\n    title = \"Promptly Predicting Structures: The Return of Inference\",\n    author = \"Mehta, Maitrey  and\n      Pyatkin, Valentina  and\n      Srikumar, Vivek\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.7/\",\n    doi = \"10.18653/v1/2024.naacl-long.7\",\n    pages = \"112--130\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.7.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.7/",
        "pdf_size": 647627,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12405688856822722169&as_sdt=5,44&sciodt=0,44&hl=en",
        "gs_version_total": 4,
        "aff": "Kahlert School of Computing, University of Utah; Allen Institute for AI+University of Washington; Kahlert School of Computing, University of Utah",
        "aff_domain": "cs.utah.edu;allenai.org;cs.utah.edu",
        "email": "cs.utah.edu;allenai.org;cs.utah.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1+2;0",
        "aff_unique_norm": "University of Utah;Allen Institute for AI;University of Washington",
        "aff_unique_dep": "Kahlert School of Computing;;",
        "aff_unique_url": "https://www.utah.edu;https://allenai.org;https://www.washington.edu",
        "aff_unique_abbr": "U of U;AI2;UW",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Salt Lake City;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.91",
        "title": "Pruning as a Domain-specific LLM Extractor",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) have exhibited remarkable proficiency across a wide array of NLP tasks. However, the escalation in model size also engenders substantial deployment costs. While few efforts have explored model pruning techniques to reduce the size of LLMs, they mainly center on general or task-specific weights. This leads to suboptimal performance due to lacking specificity on the target domain or generality on different tasks when applied to domain-specific challenges. This work introduces an innovative unstructured dual-pruning methodology, D-Pruner, for domain-specific compression on LLM. It extracts a compressed, domain-specific, and task- agnostic LLM by identifying LLM weights that are pivotal for general capabilities, like linguistic capability and multi-task solving, and domain-specific knowledge. More specifically, we first assess general weight importance by quantifying the error incurred upon their removal with the help of an open-domain calibration dataset. Then, we utilize this general weight importance to refine the training loss, so that it preserves generality when fitting into a specific domain. Moreover, by efficiently approximating weight importance with the refined training loss on a domain-specific calibration dataset, we obtain a pruned model emphasizing generality and specificity. Our comprehensive experiments across various tasks in healthcare and legal domains show the effectiveness of D-Pruner in domain-specific compression. Our code is available at https://github.com/psunlpgroup/D-Pruner.",
        "author": "Nan Zhang; Yanchi Liu; Xujiang Zhao; Wei Cheng; Runxue Bao; Rui Zhang; Prasenjit Mitra; Haifeng Chen",
        "authorids": "/n/nan-zhang/; /y/yanchi-liu/; /x/xujiang-zhao/; /w/wei-cheng/; /r/runxue-bao/; /r/rui-zhang/; /p/prasenjit-mitra/; /h/haifeng-chen/",
        "bibtex": "@inproceedings{zhang-etal-2024-pruning,\n    title = \"Pruning as a Domain-specific {LLM} Extractor\",\n    author = \"Zhang, Nan  and\n      Liu, Yanchi  and\n      Zhao, Xujiang  and\n      Cheng, Wei  and\n      Bao, Runxue  and\n      Zhang, Rui  and\n      Mitra, Prasenjit  and\n      Chen, Haifeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.91/\",\n    doi = \"10.18653/v1/2024.findings-naacl.91\",\n    pages = \"1417--1428\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.91.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.91/",
        "pdf_size": 867351,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7465493225007555605&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The Pennsylvania State University\u2663; NEC Labs America\u2662; The Pennsylvania State University\u2663; NEC Labs America\u2662; NEC Labs America\u2662; The Pennsylvania State University\u2663; The Pennsylvania State University\u2663; NEC Labs America\u2662",
        "aff_domain": "psu.edu;nec-labs.com;nec-labs.com;nec-labs.com;nec-labs.com;psu.edu;psu.edu;nec-labs.com",
        "email": "psu.edu;nec-labs.com;nec-labs.com;nec-labs.com;nec-labs.com;psu.edu;psu.edu;nec-labs.com",
        "github": "https://github.com/psunlpgroup/D-Pruner",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;0;1;1;0;0;1",
        "aff_unique_norm": "Pennsylvania State University;NEC Labs America",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.psu.edu;https://www.nec-labs.com",
        "aff_unique_abbr": "PSU;NEC LA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.129",
        "title": "Psychometric Predictive Power of Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Instruction tuning aligns the response of large language models (LLMs) with human preferences.Despite such efforts in human\u2013LLM alignment, we find that instruction tuning does not always make LLMs human-like from a cognitive modeling perspective. More specifically, next-word probabilities estimated by instruction-tuned LLMs are often worse at simulating human reading behavior than those estimated by base LLMs.In addition, we explore prompting methodologies for simulating human reading behavior with LLMs. Our results show that prompts reflecting a particular linguistic hypothesis improve psychometric predictive power, but are still inferior to small base models.These findings highlight that recent advancements in LLMs, i.e., instruction tuning and prompting, do not offer better estimates than direct probability measurements from base LLMs in cognitive modeling. In other words, pure next-word probability remains a strong predictor for human reading behavior, even in the age of LLMs.",
        "author": "Tatsuki Kuribayashi; Yohei Oseki; Timothy Baldwin",
        "authorids": "/t/tatsuki-kuribayashi/; /y/yohei-oseki/; /t/timothy-baldwin/",
        "bibtex": "@inproceedings{kuribayashi-etal-2024-psychometric,\n    title = \"Psychometric Predictive Power of Large Language Models\",\n    author = \"Kuribayashi, Tatsuki  and\n      Oseki, Yohei  and\n      Baldwin, Timothy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.129/\",\n    doi = \"10.18653/v1/2024.findings-naacl.129\",\n    pages = \"1983--2005\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.129.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.129/",
        "pdf_size": 1038560,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13735731055784765815&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "MBZUAI+The University of Melbourne; The University of Tokyo; MBZUAI+The University of Melbourne",
        "aff_domain": "mbzuai.ac.ae;g.ecc.u-tokyo.ac.jp;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae;g.ecc.u-tokyo.ac.jp;mbzuai.ac.ae",
        "github": "https://github.com/kuribayashi4/llm-cognitive-modeling",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;0+1",
        "aff_unique_norm": "Mohamed bin Zayed University of Artificial Intelligence;University of Melbourne;University of Tokyo",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.mbzuai.ac.ae;https://www.unimelb.edu.au;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "MBZUAI;UniMelb;UTokyo",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;2;0+1",
        "aff_country_unique": "United Arab Emirates;Australia;Japan"
    },
    {
        "id": "2024.findings-naacl.166",
        "title": "Q-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper introduces Q-tuning, a novel approach for continual prompt tuning that enables the lifelong learning of a pre-trained language model. When learning a new task, Q-tuning trains a task-specific prompt by adding it to a prompt queue consisting of the prompts from older tasks. To better transfer the knowledge of old tasks, we design an adaptive knowledge aggregation technique that reweighs previous prompts in the queue with a learnable low-rank matrix. Once the prompt queue reaches its maximum capacity, we leverage a PCA-based eviction rule to reduce the queue\u2019s size, allowing the newly trained prompt to be added while preserving the primary knowledge of old tasks. In order to mitigate the accumulation of information loss caused by the eviction, we additionally propose a globally shared prefix prompt and a memory retention regularization based on information theory. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods substantially on continual prompt tuning benchmarks. Moreover, our approach enables lifelong learning on linearly growing task sequences while requiring constant complexity for training and inference.",
        "author": "Yanhui Guo; Shaoyuan Xu; Jinmiao Fu; Jia Liu; Chaosheng Dong; Bryan Wang",
        "authorids": "/y/yanhui-guo/; /s/shaoyuan-xu/; /j/jinmiao-fu/; /j/jia-liu/; /c/chaosheng-dong/; /b/bryan-wang/",
        "bibtex": "@inproceedings{guo-etal-2024-q,\n    title = \"{Q}-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language Learning\",\n    author = \"Guo, Yanhui  and\n      Xu, Shaoyuan  and\n      Fu, Jinmiao  and\n      Liu, Jia  and\n      Dong, Chaosheng  and\n      Wang, Bryan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.166/\",\n    doi = \"10.18653/v1/2024.findings-naacl.166\",\n    pages = \"2595--2622\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.166.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.166/",
        "pdf_size": 3279862,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6962023944886068288&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.115",
        "title": "QualEval: Qualitative Evaluation for Model Improvement",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Quantitative evaluation metrics have been pivotal in gauging the advancements of AI systems like large language models (LLMs).However, due to the intricate nature of real-world tasks, a single scalar to quantify and compare performance trivializes the fine-grained nuances of model behavior. Additionally, metrics do not yield actionable diagnostics for model improvement, thus requiring extensive manual efforts of scientists, involving sifting through vast datasets and attempting hit-or-miss adjustments to training data or setups. In this work, we address the shortcomings of quantitative metrics by proposing QualEval, which uses automated qualitative evaluation as a vehicle for model improvement. QualEval uses a powerful LLM reasoner and our novel flexible linear programming solver to generate human-readable insights that when applied, accelerate model improvement. The insights are supported by a dashboard report with fine-grained visualizations and human-interpretable analyses. We corroborate the faithfulness of QualEval by demonstrating that leveraging its insights, for example, improves the absolute performance of the Llama 2 model by up to 15% points relative on a challenging dialogue task (DialogSum) when compared to baselines. QualEval successfully increases the pace and quality of model development by eliminating the need of arduous manual analysis, thus serving as a data-scientist-in-a-box.",
        "author": "Vishvak Murahari; Ameet Deshpande; Peter Clark; Tanmay Rajpurohit; Ashish Sabharwal; Karthik Narasimhan; Ashwin Kalyan",
        "authorids": "/v/vishvak-murahari/; /a/ameet-deshpande/; /p/peter-clark/; /t/tanmay-rajpurohit/; /a/ashish-sabharwal/; /k/karthik-narasimhan/; /a/ashwin-kalyan/",
        "bibtex": "@inproceedings{murahari-etal-2024-qualeval,\n    title = \"{Q}ual{E}val: Qualitative Evaluation for Model Improvement\",\n    author = \"Murahari, Vishvak  and\n      Deshpande, Ameet  and\n      Clark, Peter  and\n      Rajpurohit, Tanmay  and\n      Sabharwal, Ashish  and\n      Narasimhan, Karthik  and\n      Kalyan, Ashwin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.115/\",\n    doi = \"10.18653/v1/2024.naacl-long.115\",\n    pages = \"2093--2111\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.115.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.115/",
        "pdf_size": 32944622,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16924907597993779907&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Princeton University; Princeton University + Allen Institute for AI; Allen Institute for AI; Georgia Tech; Allen Institute for AI; Princeton University + Princeton Language Initiative; Allen Institute for AI",
        "aff_domain": "cs.princeton.edu;cs.princeton.edu; ; ; ; ; ",
        "email": "cs.princeton.edu;cs.princeton.edu; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0+1;1;2;1;0+0;1",
        "aff_unique_norm": "Princeton University;Allen Institute for AI;Georgia Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.princeton.edu;https://allenai.org;https://www.gatech.edu",
        "aff_unique_abbr": "Princeton;AI2;Georgia Tech",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.116",
        "title": "Quantum-inspired Language Model with Lindblad Master Equation and Interference Measurement for Sentiment Analysis",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Quantum-inspired models have demonstrated superior performance in many downstream language tasks, such as question answering and sentiment analysis. However, recent models primarily focus on embedding and measurement operations, overlooking the significance of the quantum evolution process. In this work, we present a novel quantum-inspired neural network, LI-QiLM, which integrates the Lindblad Master Equation (LME) to model the evolution process and the interferometry to the measurement process, providing more physical meaning to strengthen the interpretability. We conduct comprehensive experiments on six sentiment analysis datasets. Compared to the traditional neural networks, transformer-based pre-trained models and quantum-inspired models, such as CICWE-QNN and ComplexQNN, the proposed method demonstrates superior performance in accuracy and F1-score on six commonly used datasets for sentiment analysis. Additional ablation tests verify the effectiveness of LME and interferometry.",
        "author": "Kehuan Yan; Peichao Lai; Yilei Wang",
        "authorids": "/k/kehuan-yan/; /p/peichao-lai/; /y/yilei-wang/",
        "bibtex": "@inproceedings{yan-etal-2024-quantum,\n    title = \"Quantum-inspired Language Model with Lindblad Master Equation and Interference Measurement for Sentiment Analysis\",\n    author = \"Yan, Kehuan  and\n      Lai, Peichao  and\n      Wang, Yilei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.116/\",\n    doi = \"10.18653/v1/2024.naacl-long.116\",\n    pages = \"2112--2121\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.116.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.116/",
        "pdf_size": 673755,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:EDN-DIfoMXIJ:scholar.google.com/&scioq=Quantum-inspired+Language+Model+with+Lindblad+Master+Equation+and+Interference+Measurement+for+Sentiment+Analysis&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China",
        "aff_domain": "fzu.edu.cn; ;fzu.edu.cn",
        "email": "fzu.edu.cn; ;fzu.edu.cn",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Fuzhou University",
        "aff_unique_dep": "College of Computer and Data Science",
        "aff_unique_url": "https://www.fzu.edu.cn",
        "aff_unique_abbr": "FZU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Fuzhou",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.31",
        "title": "Query-Efficient Textual Adversarial Example Generation for Black-Box Attacks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Deep neural networks for Natural Language Processing (NLP) have been demonstrated to be vulnerable to textual adversarial examples. Existing black-box attacks typically require thousands of queries on the target model, making them expensive in real-world applications. In this paper, we propose a new approach that guides the word substitutions using prior knowledge from the training set to improve the attack efficiency. Specifically, we introduce Adversarial Boosting Preference (ABP), a metric that quantifies the importance of words and guides adversarial word substitutions. We then propose two query-efficient attack strategies based on ABP: query-free attack (ABPfree) and guided search attack (ABPguide). Extensive evaluations for text classification demonstrate that ABPfree generates more natural adversarial examples than existing universal attacks, ABPguide significantly reduces the number of queries by a factor of 10 500 while achieving comparable or even better performance than black-box attack baselines. Furthermore, we introduce the first ensemble attack ABPens in NLP, which gains further performance improvements and achieves better transferability and generalization by the ensemble of the ABP across different models and domains. Code is available at https://github.com/BaiDingHub/ABP.",
        "author": "Zhen Yu; Zhenhua Chen; Kun He",
        "authorids": "/z/zhen-yu/; /z/zhenhua-chen/; /k/kun-he/",
        "bibtex": "@inproceedings{yu-etal-2024-query,\n    title = \"Query-Efficient Textual Adversarial Example Generation for Black-Box Attacks\",\n    author = \"Yu, Zhen  and\n      Chen, Zhenhua  and\n      He, Kun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.31/\",\n    doi = \"10.18653/v1/2024.naacl-long.31\",\n    pages = \"556--569\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.31.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.31/",
        "pdf_size": 922993,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10226966897622304836&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China",
        "aff_domain": "hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "email": "hust.edu.cn;hust.edu.cn;hust.edu.cn",
        "github": "https://github.com/BaiDingHub/ABP",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Huazhong University of Science and Technology",
        "aff_unique_dep": "School of Computer Science and Technology",
        "aff_unique_url": "http://www.hust.edu.cn",
        "aff_unique_abbr": "HUST",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Wuhan",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-demo.11",
        "title": "QueryExplorer: An Interactive Query Generation Assistant for Search and Exploration",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Formulating effective search queries remains a challenging task, particularly when users lack expertise in a specific domain or are not proficient in the language of the content. Providing example documents of interest might be easier for a user. However, such query-by-example scenarios are prone to concept drift, and the retrieval effectiveness is highly sensitive to the query generation method, without a clear way to incorporate user feedback. To enable exploration and to support Human-In-The-Loop experiments we propose QueryExplorer\u2013 an interactive query generation, reformulation, and retrieval interface with support for Hug-gingFace generation models and PyTerrier\u2019sretrieval pipelines and datasets, and extensivelogging of human feedback. To allow users to create and modify effective queries, our demo supports complementary approaches of using LLMs interactively, assisting the user with edits and feedback at multiple stages of the query formulation process. With support for recording fine-grained interactions and user annotations, QueryExplorer can serve as a valuable experimental and research platform for annotation, qualitative evaluation, and conducting Human-in-the-Loop (HITL) experiments for complex search tasks where users struggle to formulate queries.",
        "author": "Kaustubh Dhole; Shivam Bajaj; Ramraj Chandradevan; Eugene Agichtein",
        "authorids": "/k/kaustubh-dhole/; /s/shivam-bajaj/; /r/ramraj-chandradevan/; /e/eugene-agichtein/",
        "bibtex": "@inproceedings{dhole-etal-2024-queryexplorer,\n    title = \"{Q}uery{E}xplorer: An Interactive Query Generation Assistant for Search and Exploration\",\n    author = \"Dhole, Kaustubh  and\n      Bajaj, Shivam  and\n      Chandradevan, Ramraj  and\n      Agichtein, Eugene\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.11/\",\n    doi = \"10.18653/v1/2024.naacl-demo.11\",\n    pages = \"107--115\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.11.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.11/",
        "pdf_size": 804103,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3075391285491609195&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Computer Science, Emory University, Atlanta; Cox Communications, Atlanta; Dept. of Computer Science, Emory University, Atlanta; Dept. of Computer Science, Emory University, Atlanta",
        "aff_domain": "emory.edu;cox.com;emory.edu;emory.edu",
        "email": "emory.edu;cox.com;emory.edu;emory.edu",
        "github": "https://github.com/emory-irlab/query-explorer",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Emory University;Cox Communications",
        "aff_unique_dep": "Dept. of Computer Science;",
        "aff_unique_url": "https://www.emory.edu;https://www.cox.com",
        "aff_unique_abbr": "Emory;Cox",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.54",
        "title": "R-BASS : Relevance-aided Block-wise Adaptation for Speech Summarization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "End-to-end speech summarization on long recordings is challenging because of the high computational cost. Block-wise Adaptation for Speech Summarization (BASS) summarizes arbitrarily long sequences by sequentially processing abutting chunks of audio. Despite the benefits of BASS, it has higher compute time due to sequential processing of all blocks, regardless of whether they are relevant to the final summary. In this paper, we propose R-BASS, a new relevance-aware block-wise adaptation method. First, we introduce two approaches to automatically estimate block relevance based on lexical and semantic similarity between the block-level transcript and the summary. Experiments on the How2 dataset show that using ground truth relevance during inference improves efficiency by 63.9 % by dropping irrelevant blocks. Finally, we incorporate relevance scores into training using a novel relevance loss and relevance predictor, and the proposed R-BASS model makes it possible to drop 86.3 % of the blocks while retaining comparable performance, resulting in a 2.2x speedup over BASS.",
        "author": "Roshan Sharma; Ruchira Sharma; Hira Dhamyal; Rita Singh; Bhiksha Raj",
        "authorids": "/r/roshan-sharma/; /r/ruchira-sharma/; /h/hira-dhamyal/; /r/rita-singh/; /b/bhiksha-raj/",
        "bibtex": "@inproceedings{sharma-etal-2024-r,\n    title = \"{R}-{BASS} : Relevance-aided Block-wise Adaptation for Speech Summarization\",\n    author = \"Sharma, Roshan  and\n      Sharma, Ruchira  and\n      Dhamyal, Hira  and\n      Singh, Rita  and\n      Raj, Bhiksha\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.54/\",\n    doi = \"10.18653/v1/2024.findings-naacl.54\",\n    pages = \"848--857\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.54.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.54/",
        "pdf_size": 246722,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:fLS2Wr2fjmgJ:scholar.google.com/&scioq=R-BASS+:+Relevance-aided+Block-wise+Adaptation+for+Speech+Summarization&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Carnegie Mellon University; University of Massachusetts, Amherst; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University+Mohammed bin Zayed University of AI, Abu Dhabi",
        "aff_domain": "cmu.edu; ; ; ; ",
        "email": "cmu.edu; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;0;0+2",
        "aff_unique_norm": "Carnegie Mellon University;University of Massachusetts Amherst;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.umass.edu;https://mbzuai.ac.ae",
        "aff_unique_abbr": "CMU;UMass Amherst;MBZUAI",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Amherst;Abu Dhabi",
        "aff_country_unique_index": "0;0;0;0;0+1",
        "aff_country_unique": "United States;United Arab Emirates"
    },
    {
        "id": "2024.naacl-long.36",
        "title": "R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper introduces Robust Spin (R-Spin), a data-efficient domain-specific self-supervision method for speaker and noise-invariant speech representations by learning discrete acoustic units with speaker-invariant clustering (Spin). R-Spin resolves Spin\u2019s issues and enhances content representations by learning to predict acoustic pieces. R-Spin offers a 12X reduction in computational resources compared to previous state-of-the-art methods while outperforming them in severely distorted speech scenarios. This paper provides detailed analyses to show how discrete units contribute to speech encoder training and improving robustness in diverse acoustic environments.",
        "author": "Heng-Jui Chang; James Glass",
        "authorids": "/h/heng-jui-chang/; /j/james-glass/",
        "bibtex": "@inproceedings{chang-glass-2024-r,\n    title = \"{R}-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces\",\n    author = \"Chang, Heng-Jui  and\n      Glass, James\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.36/\",\n    doi = \"10.18653/v1/2024.naacl-long.36\",\n    pages = \"642--662\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.36.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.36/",
        "pdf_size": 2225165,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17277304566249980271&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "MIT CSAIL; MIT CSAIL",
        "aff_domain": "mit.edu; ",
        "email": "mit.edu; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT CSAIL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.394",
        "title": "R-Tuning: Instructing Large Language Models to Say \u2018I Don\u2019t Know\u2019",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the disparity in knowledge encompassed by pre-trained parameters compared to that of instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate R-Tuning effectively improves a model\u2019s ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty results in better calibration and an improved ability to estimate the uncertainty than uncertainty-based testing. Our code is available at https://github.com/shizhediao/R-Tuning",
        "author": "Hanning Zhang; Shizhe Diao; Yong Lin; Yi Fung; Qing Lian; Xingyao Wang; Yangyi Chen; Heng Ji; Tong Zhang",
        "authorids": "/h/hanning-zhang/; /s/shizhe-diao/; /y/yong-lin/; /y/yi-fung/; /q/qing-lian/; /x/xingyao-wang/; /y/yangyi-chen/; /h/heng-ji/; /t/tong-zhang/",
        "bibtex": "@inproceedings{zhang-etal-2024-r,\n    title = \"{R}-Tuning: Instructing Large Language Models to Say {\\textquoteleft}{I} Don{'}t Know'\",\n    author = \"Zhang, Hanning  and\n      Diao, Shizhe  and\n      Lin, Yong  and\n      Fung, Yi  and\n      Lian, Qing  and\n      Wang, Xingyao  and\n      Chen, Yangyi  and\n      Ji, Heng  and\n      Zhang, Tong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.394/\",\n    doi = \"10.18653/v1/2024.naacl-long.394\",\n    pages = \"7113--7139\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.394.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.394/",
        "pdf_size": 8513019,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17801259215541815640&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "The Hong Kong University of Science and Technology\u2660; The Hong Kong University of Science and Technology\u2660; The Hong Kong University of Science and Technology\u2660; University of Illinois Urbana-Champaign\u2661; The Hong Kong University of Science and Technology\u2660; University of Illinois Urbana-Champaign\u2661; University of Illinois Urbana-Champaign\u2661; University of Illinois Urbana-Champaign\u2661; University of Illinois Urbana-Champaign\u2661",
        "aff_domain": "ust.hk;ust.hk;ust.hk;illinois.edu;ust.hk;illinois.edu;illinois.edu;illinois.edu;ust.hk",
        "email": "ust.hk;ust.hk;ust.hk;illinois.edu;ust.hk;illinois.edu;illinois.edu;illinois.edu;ust.hk",
        "github": "https://github.com/shizhediao/R-Tuning",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;1;0;1;1;1;1",
        "aff_unique_norm": "Hong Kong University of Science and Technology;University of Illinois Urbana-Champaign",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ust.hk;https://illinois.edu",
        "aff_unique_abbr": "HKUST;UIUC",
        "aff_campus_unique_index": "0;0;0;1;0;1;1;1;1",
        "aff_campus_unique": "Hong Kong SAR;Urbana-Champaign",
        "aff_country_unique_index": "0;0;0;1;0;1;1;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.484",
        "title": "RE2: Region-Aware Relation Extraction from Visually Rich Documents",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Current research in form understanding predominantly relies on large pre-trained language models, necessitating extensive data for pre-training. However, the importance of layout structure (i.e., the spatial relationship between the entity blocks in the visually rich document) to relation extraction has been overlooked. In this paper, we propose REgion-Aware Relation Extraction (RE2) that leverages region-level spatial structure among the entity blocks to improve their relation prediction. We design an edge-aware graph attention network to learn the interaction between entities while considering their spatial relationship defined by their region-level representations. We also introduce a constraint objective to regularize the model towards consistency with the inherent constraints of the relation extraction task. To support the research on relation extraction from visually rich documents and demonstrate the generalizability of RE2, we build a new benchmark dataset, DiverseForm, that covers a wide range of domains. Extensive experiments on DiverseForm and several public benchmark datasets demonstrate significant superiority and transferability of RE2 across various domains and languages, with up to 18.88% absolute F-score gain over all high-performing baselines",
        "author": "Pritika Ramu; Sijia Wang; Lalla Mouatadid; Joy Rimchala; Lifu Huang",
        "authorids": "/p/pritika-ramu/; /s/sijia-wang/; /l/lalla-mouatadid/; /j/joy-rimchala/; /l/lifu-huang/",
        "bibtex": "@inproceedings{ramu-etal-2024-re2,\n    title = \"$RE^2$: Region-Aware Relation Extraction from Visually Rich Documents\",\n    author = \"Ramu, Pritika  and\n      Wang, Sijia  and\n      Mouatadid, Lalla  and\n      Rimchala, Joy  and\n      Huang, Lifu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.484/\",\n    doi = \"10.18653/v1/2024.naacl-long.484\",\n    pages = \"8731--8747\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.484.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.484/",
        "pdf_size": 10315761,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5564078065200147394&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Adobe Research; Virginia Tech; Intuit AI Research; Intuit AI Research; Virginia Tech",
        "aff_domain": "adobe.com;vt.edu;intuit.com;intuit.com;vt.edu",
        "email": "adobe.com;vt.edu;intuit.com;intuit.com;vt.edu",
        "github": "https://github.com/VT-NLP/Form-Document-IE",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;2;1",
        "aff_unique_norm": "Adobe;Virginia Tech;Intuit",
        "aff_unique_dep": "Adobe Research;;Intuit AI Research",
        "aff_unique_url": "https://research.adobe.com;https://www.vt.edu;https://intuit.com/",
        "aff_unique_abbr": "Adobe;VT;Intuit",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.36",
        "title": "READ: Improving Relation Extraction from an ADversarial Perspective",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent works in relation extraction (RE) have achieved promising benchmark accuracy; however, our adversarial attack experiments show that these works excessively rely on entities, making their generalization capability questionable. To address this issue, we propose an adversarial training method specifically designed for RE. Our approach introduces both sequence- and token-level perturbations to the sample and uses a separate perturbation vocabulary to improve the search for entity and context perturbations.Furthermore, we introduce a probabilistic strategy for leaving clean tokens in the context during adversarial training. This strategy enables a larger attack budget for entities and coaxes the model to leverage relational patterns embedded in the context. Extensive experiments show that compared to various adversarial training methods, our method significantly improves both the accuracy and robustness of the model. Additionally, experiments on different data availability settings highlight the effectiveness of our method in low-resource scenarios.We also perform in-depth analyses of our proposed method and provide further hints.We will release our code at https://github.com/David-Li0406/READ.",
        "author": "Dawei Li; William Hogan; Jingbo Shang",
        "authorids": "/d/dawei-li/; /w/william-hogan/; /j/jingbo-shang/",
        "bibtex": "@inproceedings{li-etal-2024-read,\n    title = \"{READ}: Improving Relation Extraction from an {AD}versarial Perspective\",\n    author = \"Li, Dawei  and\n      Hogan, William  and\n      Shang, Jingbo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.36/\",\n    doi = \"10.18653/v1/2024.findings-naacl.36\",\n    pages = \"533--548\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.36.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.36/",
        "pdf_size": 442231,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17824456938779785725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, San Diego; University of California, San Diego; University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu",
        "email": "ucsd.edu;ucsd.edu;ucsd.edu",
        "github": "https://github.com/David-Li0406/READ",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.64",
        "title": "REMATCH: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Knowledge graphs play a pivotal role in various applications, such as question-answering and fact-checking. Abstract Meaning Representation (AMR) represents text as knowledge graphs. Evaluating the quality of these graphs involves matching them structurally to each other and semantically to the source text. Existing AMR metrics are inefficient and struggle to capture semantic similarity. We also lack a systematic evaluation benchmark for assessing structural similarity between AMR graphs. To overcome these limitations, we introduce a novel AMR similarity metric, rematch, alongside a new evaluation for structural similarity called RARE. Among state-of-the-art metrics, rematch ranks second in structural similarity; and first in semantic similarity by 1\u20135 percentage points on the STS-B and SICK-R benchmarks. Rematch is also five times faster than the next most efficient metric.",
        "author": "Zoher Kachwala; Jisun An; Haewoon Kwak; Filippo Menczer",
        "authorids": "/z/zoher-kachwala/; /j/jisun-an/; /h/haewoon-kwak/; /f/filippo-menczer/",
        "bibtex": "@inproceedings{kachwala-etal-2024-rematch,\n    title = \"{REMATCH}: Robust and Efficient Matching of Local Knowledge Graphs to Improve Structural and Semantic Similarity\",\n    author = \"Kachwala, Zoher  and\n      An, Jisun  and\n      Kwak, Haewoon  and\n      Menczer, Filippo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.64/\",\n    doi = \"10.18653/v1/2024.findings-naacl.64\",\n    pages = \"1018--1028\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.64.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.64/",
        "pdf_size": 821270,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=228987024587666746&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Observatory on Social Media; Observatory on Social Media; Observatory on Social Media; Observatory on Social Media",
        "aff_domain": "iu.edu;acm.org;acm.org; ",
        "email": "iu.edu;acm.org;acm.org; ",
        "github": "https://github.com/osome-iu/Rematch-RARE",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Observatory on Social Media",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2024.findings-naacl.196",
        "title": "RENOVI: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Norm violations occur when individuals fail to conform to culturally accepted behaviors, which may lead to potential conflicts. Remediating norm violations requires social awareness and cultural sensitivity of the nuances at play. To equip interactive AI systems with a remediation ability, we offer ReNoVi \u2014 a large-scale corpus of 9,258 multi-turn dialogues annotated with social norms, as well as define a sequence of tasks to help understand and remediate norm violations step by step. ReNoVi consists of two parts: 512 human-authored dialogues (real data), and 8,746 synthetic conversations generated by ChatGPT through prompt learning. While collecting sufficient human-authored data is costly, synthetic conversations provide suitable amounts of data to help mitigate the scarcity of training data, as well as the chance to assess the alignment between LLMs and humans in the awareness of social norms. We thus harness the power of ChatGPT to generate synthetic training data for our task. To ensure the quality of both human-authored and synthetic data, we follow a quality control protocol during data collection. Our experimental results demonstrate the importance of remediating norm violations in socio-cultural conversations, as well as the improvement in performance obtained from synthetic data.",
        "author": "Haolan Zhan; Zhuang Li; Xiaoxi Kang; Tao Feng; Yuncheng Hua; Lizhen Qu; Yi Ying; Mei Rianto Chandra; Kelly Rosalin; Jureynolds Jureynolds; Suraj Sharma; Shilin Qu; Linhao Luo; Ingrid Zukerman; Lay-Ki Soon; Zhaleh Semnani Azad; Reza Haf",
        "authorids": "/h/haolan-zhan/; /z/zhuang-li/; /x/xiaoxi-kang/; /t/tao-feng/; /y/yuncheng-hua/; /l/lizhen-qu/; /y/yi-ying/; /m/mei-rianto-chandra/; /k/kelly-rosalin/; /j/jureynolds-jureynolds/; /s/suraj-sharma/; /s/shilin-qu/; /l/linhao-luo/; /i/ingrid-zukerman/; /l/lay-ki-soon/; /z/zhaleh-semnani-azad/; /r/reza-haf/",
        "bibtex": "@inproceedings{zhan-etal-2024-renovi,\n    title = \"{RENOVI}: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations\",\n    author = \"Zhan, Haolan  and\n      Li, Zhuang  and\n      Kang, Xiaoxi  and\n      Feng, Tao  and\n      Hua, Yuncheng  and\n      Qu, Lizhen  and\n      Ying, Yi  and\n      Chandra, Mei Rianto  and\n      Rosalin, Kelly  and\n      Jureynolds, Jureynolds  and\n      Sharma, Suraj  and\n      Qu, Shilin  and\n      Luo, Linhao  and\n      Zukerman, Ingrid  and\n      Soon, Lay-Ki  and\n      Semnani Azad, Zhaleh  and\n      Haf, Reza\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.196/\",\n    doi = \"10.18653/v1/2024.findings-naacl.196\",\n    pages = \"3104--3117\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.196.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.196/",
        "pdf_size": 2303632,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8023087356226304195&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Faculty of IT, Monash University, Australia; Faculty of IT, Monash University, Australia; School of IT, Monash University Malaysia; Faculty of IT, Monash University, Australia; Faculty of IT, Monash University, Australia; Faculty of IT, Monash University, Australia; Binus University, Indonesia; Binus University, Indonesia; Binus University, Indonesia; Binus University, Indonesia; California State University, Northridge, CA; Faculty of IT, Monash University, Australia; Faculty of IT, Monash University, Australia; School of IT, Monash University Malaysia; California State University, Northridge, CA; Faculty of IT, Monash University, Australia; Faculty of IT, Monash University, Australia",
        "aff_domain": "monash.edu;monash.edu;monash.edu;monash.edu;monash.edu;monash.edu;binus.edu;binus.edu;binus.edu;binus.edu;csun.edu;monash.edu;monash.edu;monash.edu;csun.edu;monash.edu;monash.edu",
        "email": "monash.edu;monash.edu;monash.edu;monash.edu;monash.edu;monash.edu;binus.edu;binus.edu;binus.edu;binus.edu;csun.edu;monash.edu;monash.edu;monash.edu;csun.edu;monash.edu;monash.edu",
        "github": "https://github.com/zhanhl316/ReNoVi",
        "project": "",
        "author_num": 17,
        "aff_unique_index": "0;0;1;0;0;0;2;2;2;2;3;0;0;1;3;0;0",
        "aff_unique_norm": "Monash University;Monash University Malaysia;Binus University;California State University, Northridge",
        "aff_unique_dep": "Faculty of IT;School of IT;;",
        "aff_unique_url": "https://www.monash.edu;https://www.monash.edu.my;https://binus.ac.id;https://www.csun.edu",
        "aff_unique_abbr": "Monash;MUM;Binus;CSUN",
        "aff_campus_unique_index": "1;2;1;2",
        "aff_campus_unique": ";Malaysia;Northridge",
        "aff_country_unique_index": "0;0;1;0;0;0;2;2;2;2;3;0;0;1;3;0;0",
        "aff_country_unique": "Australia;Malaysia;Indonesia;United States"
    },
    {
        "id": "2024.naacl-long.463",
        "title": "REPLUG: Retrieval-Augmented Black-Box Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.",
        "author": "Weijia Shi; Sewon Min; Michihiro Yasunaga; Minjoon Seo; Richard James; Mike Lewis; Luke Zettlemoyer; Wen-tau Yih",
        "authorids": "/w/weijia-shi/; /s/sewon-min/; /m/michihiro-yasunaga/; /m/minjoon-seo/; /r/richard-james/; /m/mike-lewis/; /l/luke-zettlemoyer/; /w/wen-tau-yih/",
        "bibtex": "@inproceedings{shi-etal-2024-replug,\n    title = \"{REPLUG}: Retrieval-Augmented Black-Box Language Models\",\n    author = \"Shi, Weijia  and\n      Min, Sewon  and\n      Yasunaga, Michihiro  and\n      Seo, Minjoon  and\n      James, Richard  and\n      Lewis, Mike  and\n      Zettlemoyer, Luke  and\n      Yih, Wen-tau\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.463/\",\n    doi = \"10.18653/v1/2024.naacl-long.463\",\n    pages = \"8371--8384\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.463.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.463/",
        "pdf_size": 1108571,
        "gs_citation": 172,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8250661032220819823&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Washington; University of Washington; Stanford University; KAIST; FAIR, Meta; FAIR, Meta; University of Washington + FAIR, Meta; FAIR, Meta",
        "aff_domain": "uw.edu; ; ; ; ; ; ; ",
        "email": "uw.edu; ; ; ; ; ; ; ",
        "github": "github.com/swj0419/REPLUG",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;1;2;3;3;0+3;3",
        "aff_unique_norm": "University of Washington;Stanford University;Korea Advanced Institute of Science and Technology;Meta",
        "aff_unique_dep": ";;;Facebook AI Research (FAIR)",
        "aff_unique_url": "https://www.washington.edu;https://www.stanford.edu;https://www.kaist.ac.kr;https://meta.com",
        "aff_unique_abbr": "UW;Stanford;KAIST;Meta",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;1;0;0;0+0;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "2024.findings-naacl.37",
        "title": "REQUAL-LM: Reliability and Equity through Aggregation in Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The extensive scope of large language models (LLMs) across various domains underscores the critical importance of responsibility in their application, beyond natural language processing. In particular, the randomized nature of LLMs, coupled with inherent biases and historical stereotypes in data, raises critical concerns regarding reliability and equity. Addressing these challenges are necessary before using LLMs for applications with societal impact. Towards addressing this gap, we introduce REQUAL-LM, a novel method for finding reliable and equitable LLM outputs through aggregation. Specifically, we develop a Montecarlo method based on repeated sampling to find a reliable output close to the mean of the underlying distribution of possible outputs. We formally define the terms such as reliability and bias, and design an equity-aware aggregation to minimize harmful bias while finding a highly reliable output. REQUAL-LM does not require specialized hardware, does not impose a significant computing load, and uses LLMs as a blackbox. This design choice enables seamless scalability alongside the rapid advancement of LLM technologies. Our system does not require retraining the LLMs, which makes it deployment ready and easy to adapt. Our comprehensive experiments using various tasks and datasets demonstrate that REQUAL-LM effectively mitigates bias and selects a more equitable response, specifically the outputs that properly represents minority groups.",
        "author": "Sana Ebrahimi; Nima Shahbazi; Abolfazl Asudeh",
        "authorids": "/s/sana-ebrahimi/; /n/nima-shahbazi/; /a/abolfazl-asudeh/",
        "bibtex": "@inproceedings{ebrahimi-etal-2024-requal,\n    title = \"{REQUAL}-{LM}: Reliability and Equity through Aggregation in Large Language Models\",\n    author = \"Ebrahimi, Sana  and\n      Shahbazi, Nima  and\n      Asudeh, Abolfazl\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.37/\",\n    doi = \"10.18653/v1/2024.findings-naacl.37\",\n    pages = \"549--560\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.37.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.37/",
        "pdf_size": 652429,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14238564076217127260&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "University of Illinois Chicago; University of Illinois Chicago; University of Illinois Chicago",
        "aff_domain": "uic.edu;uic.edu;uic.edu",
        "email": "uic.edu;uic.edu;uic.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois at Chicago",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uic.edu",
        "aff_unique_abbr": "UIC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chicago",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.323",
        "title": "RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Chain-of-thought (CoT) has impressively unlocked the reasoning potential of large language models (LLMs). Yet, it falls short when tackling problems that require multiple reasoning steps. This limitation arises from the complex nature of multi-step reasoning processes: later stages often depend not only on the immediately preceding step, but also on the results from several steps earlier. Such complexities indicate the reasoning process is naturally a graph. The almost linear structure of CoT, however, struggles to capture this complex reasoning graph. To address this challenge, we propose Residual Connection Prompting (ResPrompt), a new prompting strategy that advances multi-step reasoning in LLMs. The core of our idea is to reconstruct the reasoning graph within prompts. We achieve this by integrating necessary connections\u2013links present in reasoning graph but missing in the linear CoT flow\u2013into the prompts. Termed \u201cresidual connections\u201d, these links can transform linear CoT into the complex reasoning graphs that multi-step problems entail. On benchmarks across math, sequential, and commonsense domains, ResPrompt demonstrates clear improvements in multi-step reasoning compared with CoT. Through extensive ablation studies and analyses, we pinpoint how to effectively build residual connections and also identify situations where it might be unnecessary.",
        "author": "Song Jiang; Zahra Shakeri; Aaron Chan; Maziar Sanjabi; Hamed Firooz; Yinglong Xia; Bugra Akyildiz; Yizhou Sun; Jinchao Li; Qifan Wang; Asli Celikyilmaz",
        "authorids": "/s/song-jiang/; /z/zahra-shakeri/; /a/aaron-chan/; /m/maziar-sanjabi/; /h/hamed-firooz/; /y/yinglong-xia/; /b/bugra-akyildiz/; /y/yizhou-sun/; /j/jinchao-li/; /q/qifan-wang/; /a/asli-celikyilmaz/",
        "bibtex": "@inproceedings{jiang-etal-2024-resprompt,\n    title = \"{RESPROMPT}: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models\",\n    author = \"Jiang, Song  and\n      Shakeri, Zahra  and\n      Chan, Aaron  and\n      Sanjabi, Maziar  and\n      Firooz, Hamed  and\n      Xia, Yinglong  and\n      Akyildiz, Bugra  and\n      Sun, Yizhou  and\n      Li, Jinchao  and\n      Wang, Qifan  and\n      Celikyilmaz, Asli\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.323/\",\n    doi = \"10.18653/v1/2024.naacl-long.323\",\n    pages = \"5784--5809\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.323.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.323/",
        "pdf_size": 864438,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17275942629146912731&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, Los Angeles; Meta AI; Meta AI; Meta AI; LinkedIn, Inc.; Meta AI; Meta AI; University of California, Los Angeles; Meta AI; Meta AI; FAIR, Meta",
        "aff_domain": "cs.ucla.edu;meta.com;meta.com;meta.com;linkedin.com;meta.com;meta.com;cs.ucla.edu;meta.com;meta.com;meta.com",
        "email": "cs.ucla.edu;meta.com;meta.com;meta.com;linkedin.com;meta.com;meta.com;cs.ucla.edu;meta.com;meta.com;meta.com",
        "github": "",
        "project": "",
        "author_num": 11,
        "aff_unique_index": "0;1;1;1;2;1;1;0;1;1;1",
        "aff_unique_norm": "University of California, Los Angeles;Meta;LinkedIn",
        "aff_unique_dep": ";Meta AI;",
        "aff_unique_url": "https://www.ucla.edu;https://meta.com;https://www.linkedin.com",
        "aff_unique_abbr": "UCLA;Meta;LinkedIn",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.88",
        "title": "REST: Retrieval-Based Speculative Decoding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm designed to speed up language model generation. The key insight driving the development of REST is the observation that the process of text generation often includes certain common phases and patterns. Unlike previous methods that rely on a draft language model for speculative decoding, REST harnesses the power of retrieval to generate draft tokens. This method draws from the reservoir of existing knowledge, retrieving and employing relevant tokens based on the current context. Its plug-and-play nature allows for seamless integration and acceleration of any language model, all without necessitating additional training. When benchmarked on 7B and 13B language models in a single-batch setting, REST achieves a significant speedup of 1.62 \u00d7 to 2.36 \u00d7 on code or text generation. The source code of REST is available at https://github.com/FasterDecoding/REST.",
        "author": "Zhenyu He; Zexuan Zhong; Tianle Cai; Jason Lee; Di He",
        "authorids": "/z/zhenyu-he/; /z/zexuan-zhong/; /t/tianle-cai/; /j/jason-lee/; /d/di-he/",
        "bibtex": "@inproceedings{he-etal-2024-rest,\n    title = \"{REST}: Retrieval-Based Speculative Decoding\",\n    author = \"He, Zhenyu  and\n      Zhong, Zexuan  and\n      Cai, Tianle  and\n      Lee, Jason  and\n      He, Di\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.88/\",\n    doi = \"10.18653/v1/2024.naacl-long.88\",\n    pages = \"1582--1595\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.88.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.88/",
        "pdf_size": 504301,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5007000964961910761&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "National Key Lab of General AI, School of Artificial Intelligence, Peking University+Princeton University; Princeton University; Princeton University; Princeton University; National Key Lab of General AI, School of Artificial Intelligence, Peking University",
        "aff_domain": "stu.pku.edu.cn;cs.princeton.edu;princeton.edu;princeton.edu;pku.edu.cn",
        "email": "stu.pku.edu.cn;cs.princeton.edu;princeton.edu;princeton.edu;pku.edu.cn",
        "github": "https://github.com/FasterDecoding/REST",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;0",
        "aff_unique_norm": "Peking University;Princeton University",
        "aff_unique_dep": "School of Artificial Intelligence;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.princeton.edu",
        "aff_unique_abbr": "PKU;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;1;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-industry.11",
        "title": "REXEL: An End-to-end Model for Document-Level Relation Extraction and Entity Linking",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Extracting structured information from unstructured text is critical for many downstream NLP applications and is traditionally achieved by closed information extraction (cIE). However, existing approaches for cIE suffer from two limitations: (i) they are often pipelines which makes them prone to error propagation, and/or (ii) they are restricted to sentence level which prevents them from capturing long-range dependencies and results in expensive inference time. We address these limitations by proposing REXEL, a highly efficient and accurate model for the joint task of document level cIE (DocIE). REXEL performs mention detection, entity typing, entity disambiguation, coreference resolution and document-level relation classification in a single forward pass to yield facts fully linked to a reference knowledge graph. It is on average 11 times faster than competitive existing approaches in a similar setting and performs competitively both when optimised for any of the individual sub-task and a variety of combinations of different joint tasks, surpassing the baselines by an average of more than 6 F1 points. The combination of speed and accuracy makes REXEL an accurate cost-efficient system for extracting structured information at web-scale. We also release an extension of the DocRED dataset to enable benchmarking of future work on DocIE, which will be available at https://github.com/amazon-science/e2e-docie.",
        "author": "Nacime Bouziani; Shubhi Tyagi; Joseph Fisher; Jens Lehmann; Andrea Pierleoni",
        "authorids": "/n/nacime-bouziani/; /s/shubhi-tyagi/; /j/joseph-fisher/; /j/jens-lehmann/; /a/andrea-pierleoni/",
        "bibtex": "@inproceedings{bouziani-etal-2024-rexel,\n    title = \"{REXEL}: An End-to-end Model for Document-Level Relation Extraction and Entity Linking\",\n    author = \"Bouziani, Nacime  and\n      Tyagi, Shubhi  and\n      Fisher, Joseph  and\n      Lehmann, Jens  and\n      Pierleoni, Andrea\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.11/\",\n    doi = \"10.18653/v1/2024.naacl-industry.11\",\n    pages = \"119--130\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.11.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.11/",
        "pdf_size": 654956,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12989526915530431402&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "I-X Centre for AI In Science, Imperial College London, London, UK+Amazon Alexa AI, Cambridge, UK; Amazon Alexa AI, Cambridge, UK; Amazon Alexa AI, Cambridge, UK; Amazon Alexa AI, Cambridge, UK; Amazon Alexa AI, Cambridge, UK",
        "aff_domain": "imperial.ac.uk;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "imperial.ac.uk;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "https://github.com/amazon-science/e2e-docie",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;1",
        "aff_unique_norm": "Imperial College London;Amazon",
        "aff_unique_dep": "I-X Centre for AI In Science;Alexa AI",
        "aff_unique_url": "https://www.imperial.ac.uk;https://www.amazon.com",
        "aff_unique_abbr": "ICL;Amazon",
        "aff_campus_unique_index": "0+1;1;1;1;1",
        "aff_campus_unique": "London;Cambridge",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.findings-naacl.108",
        "title": "RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Reinforcement learning from human feedback (RLHF) has been extensively employed to align large language models with user intent. However, proximal policy optimization (PPO) based RLHF is occasionally unstable requiring significant hyperparameter finetuning, and computationally expensive to maximize the estimated reward during alignment. Recently, direct preference optimization (DPO) is proposed to address those challenges. However, DPO often relies on contrastive responses generated from human annotator and alternative LLM, instead of the policy model, limiting the effectiveness of the RLHF. In this paper, we addresses both challenges by systematically combining rejection sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the development of a supervised fine-tuned policy model (SFT). A varied set of k responses per prompt are sampled directly from the SFT model. RS-DPO identifies pairs of contrastive samples based on their reward distribution. Finally, we apply DPO with the contrastive samples to align the model to human preference. Our experiments indicate that our proposed method effectively fine-tunes LLMs with limited resource environments, leading to improved alignment with user intent. Furthermore, it outperforms existing methods, including RS, PPO, and DPO.",
        "author": "Saeed Khaki; JinJin Li; Lan Ma; Liu Yang; Prathap Ramachandra",
        "authorids": "/s/saeed-khaki/; /j/jinjin-li/; /l/lan-ma/; /l/liu-yang/; /p/prathap-ramachandra/",
        "bibtex": "@inproceedings{khaki-etal-2024-rs,\n    title = \"{RS}-{DPO}: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models\",\n    author = \"Khaki, Saeed  and\n      Li, JinJin  and\n      Ma, Lan  and\n      Yang, Liu  and\n      Ramachandra, Prathap\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.108/\",\n    doi = \"10.18653/v1/2024.findings-naacl.108\",\n    pages = \"1665--1680\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.108.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.108/",
        "pdf_size": 1116754,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5237943816870400172&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Amazon; Amazon; Amazon; Amazon; Amazon",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon.com, Inc.",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.121",
        "title": "RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document Abstractive Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "For long document summarization, discourse structure is important to discern the key content of the text and the differences in importance level between sentences. Unfortunately, the integration of rhetorical structure theory (RST) into parameter-efficient fine-tuning strategies for long document summarization remains unexplored. Therefore, this paper introduces RST-LoRA and proposes four RST-aware variants to explicitly incorporate RST into the LoRA model. Our empirical evaluation demonstrates that incorporating the type and uncertainty of rhetorical relations can complementarily enhance the performance of LoRA in summarization tasks. Furthermore, the best-performing variant we introduced outperforms the vanilla LoRA and full-parameter fine-tuning models, as confirmed by multiple automatic and human evaluations, and even surpasses previous state-of-the-art methods.",
        "author": "Dongqi Liu; Vera Demberg",
        "authorids": "/d/dongqi-liu/; /v/vera-demberg/",
        "bibtex": "@inproceedings{pu-demberg-2024-rst,\n    title = \"{RST}-{L}o{RA}: A Discourse-Aware Low-Rank Adaptation for Long Document Abstractive Summarization\",\n    author = \"Liu, Dongqi  and\n      Demberg, Vera\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.121/\",\n    doi = \"10.18653/v1/2024.naacl-long.121\",\n    pages = \"2200--2220\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.121.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.121/",
        "pdf_size": 3736155,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13848301607061473436&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science; Department of Language Science and Technology",
        "aff_domain": "lst.uni-saarland.de;lst.uni-saarland.de",
        "email": "lst.uni-saarland.de;lst.uni-saarland.de",
        "github": "",
        "project": "https://dongqi.me/projects/RST-LoRA",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Unknown Institution;University of California, San Diego",
        "aff_unique_dep": "Department of Computer Science;Department of Language Science and Technology",
        "aff_unique_url": ";https://ling.ucsd.edu",
        "aff_unique_abbr": ";UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";La Jolla",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "2024.naacl-demo.5",
        "title": "RTSUM: Relation Triple-based Interpretable Summarization with Multi-level Salience Visualization",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "In this paper, we present RTSum, an unsupervised summarization framework that utilizes relation triples as the basic unit for summarization. Given an input document, RTSum first selects salient relation triples via multi-level salience scoring and then generates a concise summary from the selected relation triples by using a text-to-text language model. On the basis of RTSum, we also develop a web demo for an interpretable summarizing tool, providing fine-grained interpretations with the output summary. With support for customization options, our tool visualizes the salience for textual units at three distinct levels: sentences, relation triples, and phrases. The code, demo, and video are publicly available.",
        "author": "Seonglae Cho; Myungha Jang; Jinyoung Yeo; Dongha Lee",
        "authorids": "/s/seonglae-cho/; /m/myungha-jang/; /j/jinyoung-yeo/; /d/dongha-lee/",
        "bibtex": "@inproceedings{cho-etal-2024-rtsum,\n    title = \"{RTSUM}: Relation Triple-based Interpretable Summarization with Multi-level Salience Visualization\",\n    author = \"Cho, Seonglae  and\n      Jang, Myungha  and\n      Yeo, Jinyoung  and\n      Lee, Dongha\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.5/\",\n    doi = \"10.18653/v1/2024.naacl-demo.5\",\n    pages = \"53--60\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.5.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.5/",
        "pdf_size": 5367780,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:8Vu9VYaQprwJ:scholar.google.com/&scioq=RTSUM:+Relation+Triple-based+Interpretable+Summarization+with+Multi-level+Salience+Visualization&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff": "Yonsei University, Republic of Korea; Yonsei University, Republic of Korea; Yonsei University, Republic of Korea; Yonsei University, Republic of Korea",
        "aff_domain": "yonsei.ac.kr;gmail.com;yonsei.ac.kr;yonsei.ac.kr",
        "email": "yonsei.ac.kr;gmail.com;yonsei.ac.kr;yonsei.ac.kr",
        "github": "https://github.com/seonglae/RTSum",
        "project": "https://youtu.be/sFRO0xfqvVM",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Yonsei University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yonsei.ac.kr",
        "aff_unique_abbr": "Yonsei",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-long.458",
        "title": "Rationale-based Opinion Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Opinion summarization aims to generate concise summaries that present popular opinions of a large group of reviews. However, these summaries can be too generic and lack supporting details. To address these issues, we propose a new paradigm for summarizing reviews, rationale-based opinion summarization. Rationale-based opinion summaries output the representative opinions as well as one or more corresponding rationales. To extract good rationales, we define four desirable properties: relatedness, specificity, popularity, and diversity and present a Gibbs-sampling-based method to extract rationales. Overall, we propose RATION, an unsupervised extractive system that has two components: an Opinion Extractor (to extract representative opinions) and Rationales Extractor (to extract corresponding rationales). We conduct automatic and human evaluations to show that rationales extracted by RATION have the proposed properties and its summaries are more useful than conventional summaries. The implementation of our work is available at https://github.com/leehaoyuan/RATION.",
        "author": "Haoyuan Li; Snigdha Chaturvedi",
        "authorids": "/h/haoyuan-li/; /s/snigdha-chaturvedi/",
        "bibtex": "@inproceedings{li-chaturvedi-2024-rationale,\n    title = \"Rationale-based Opinion Summarization\",\n    author = \"Li, Haoyuan  and\n      Chaturvedi, Snigdha\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.458/\",\n    doi = \"10.18653/v1/2024.naacl-long.458\",\n    pages = \"8274--8292\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.458.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.458/",
        "pdf_size": 1293284,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10863939262086304020&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "UNC Chapel Hill; UNC Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu",
        "github": "https://github.com/leehaoyuan/RATION",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.70",
        "title": "Re-evaluating the Need for Visual Signals in Unsupervised Grammar Induction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Are multimodal inputs necessary for grammar induction? Recent work has shown that multimodal training inputs can improve grammar induction. However, these improvements are based on comparisons to weak text-only baselines that were trained on relatively little textual data. To determine whether multimodal inputs are needed in regimes with large amounts of textual training data, we design a stronger text-only baseline, which we refer to as LC-PCFG. LC-PCFG is a C-PFCG that incorporates embeddings from text-only large language models (LLMs). We use a fixed grammar family to directly compare LC-PCFG to various multimodal grammar induction methods. We compare performance on four benchmark datasets. LC-PCFG provides an up to 17% relative improvement in Corpus-F1 compared to state-of-the-art multimodal grammar induction methods. LC-PCFG is also more computationally efficient, providing an up to 85% reduction in parameter count and 8.8\u00d7 reduction in training time compared to multimodal approaches. These results suggest that multimodal inputs may not be necessary for grammar induction, and emphasize the importance of strong vision-free baselines for evaluating the benefit of multimodal approaches.",
        "author": "Boyi Li; Rodolfo Corona; Karttikeya Mangalam; Catherine Chen; Daniel Flaherty; Serge Belongie; Kilian Weinberger; Jitendra Malik; Trevor Darrell; Dan Klein",
        "authorids": "/b/boyi-li/; /r/rodolfo-corona/; /k/karttikeya-mangalam/; /c/catherine-chen-bu/; /d/daniel-flaherty/; /s/serge-belongie/; /k/kilian-weinberger/; /j/jitendra-malik/; /t/trevor-darrell/; /d/dan-klein/",
        "bibtex": "@inproceedings{li-etal-2024-evaluating,\n    title = \"Re-evaluating the Need for Visual Signals in Unsupervised Grammar Induction\",\n    author = \"Li, Boyi  and\n      Corona, Rodolfo  and\n      Mangalam, Karttikeya  and\n      Chen, Catherine  and\n      Flaherty, Daniel  and\n      Belongie, Serge  and\n      Weinberger, Kilian  and\n      Malik, Jitendra  and\n      Darrell, Trevor  and\n      Klein, Dan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.70/\",\n    doi = \"10.18653/v1/2024.findings-naacl.70\",\n    pages = \"1113--1123\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.70.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.70/",
        "pdf_size": 653099,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13025645741211629725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; UC Berkeley; University of Copenhagen; Cornell University; UC Berkeley; UC Berkeley; UC Berkeley",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;1;2;0;0;0",
        "aff_unique_norm": "University of California, Berkeley;University of Copenhagen;Cornell University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.ku.dk;https://www.cornell.edu",
        "aff_unique_abbr": "UC Berkeley;UCPH;Cornell",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0;1;0;0;0;0",
        "aff_country_unique": "United States;Denmark"
    },
    {
        "id": "2024.findings-naacl.85",
        "title": "ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the potential of data contamination (e.g., leading to memorization), good static benchmark performance does not ensure that model can reliably use the provided evidence for responding, which is essential to avoid hallucination when the required knowledge is new or private. Inspired by adversarial machine learning, we investigate the feasibility of automatically perturbing existing static one for dynamic evaluation. Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs\u2019 reliability in using new evidence for answering.We implement ReEval using ChatGPT and evaluate the resulting variants of two popular open-domain QA datasets on a collection ofLLMs under various prompting settings. Our generated data is human-readable and useful to trigger hallucination in LLM. Accurate models on static data are observed to produce unsupported answers from the perturbed evidence, with pronounced accuracy drops across LLMs including GPT-4. We find that our adversarial examples are transferable across all considered LLMs. The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.",
        "author": "Xiaodong Yu; Hao Cheng; Xiaodong Liu; Dan Roth; Jianfeng Gao",
        "authorids": "/x/xiaodong-yu/; /h/hao-cheng/; /x/xiaodong-liu/; /d/dan-roth/; /j/jianfeng-gao/",
        "bibtex": "@inproceedings{yu-etal-2024-reeval,\n    title = \"{R}e{E}val: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks\",\n    author = \"Yu, Xiaodong  and\n      Cheng, Hao  and\n      Liu, Xiaodong  and\n      Roth, Dan  and\n      Gao, Jianfeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.85/\",\n    doi = \"10.18653/v1/2024.findings-naacl.85\",\n    pages = \"1333--1351\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.85.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.85/",
        "pdf_size": 4232415,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6027002451640268491&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Pennsylvania\u2660; Microsoft Research\u2663; Microsoft Research\u2663; University of Pennsylvania\u2660; Microsoft Research\u2663",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "",
        "project": "https://autodebug-llm.github.io/",
        "author_num": 5,
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "University of Pennsylvania;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.upenn.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UPenn;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.140",
        "title": "ReFACT: Updating Text-to-Image Models by Editing the Text Encoder",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Our world is marked by unprecedented technological, global, and socio-political transformations, posing a significant challenge to textto-image generative models. These models encode factual associations within their parameters that can quickly become outdated, diminishing their utility for end-users. To that end, we introduce ReFACT, a novel approach for editing factual associations in text-to-image models without relaying on explicit input from end-users or costly re-training. ReFACT updates the weights of a specific layer in the text encoder, modifying only a tiny portion of the model\u2019s parameters and leaving the rest of the model unaffected.We empirically evaluate ReFACT on an existing benchmark, alongside a newly curated dataset.Compared to other methods, ReFACT achieves superior performance in both generalization to related concepts and preservation of unrelated concepts.Furthermore, ReFACT maintains image generation quality, making it a practical tool for updating and correcting factual information in text-to-image models.",
        "author": "Dana Arad; Hadas Orgad; Yonatan Belinkov",
        "authorids": "/d/dana-arad/; /h/hadas-orgad/; /y/yonatan-belinkov/",
        "bibtex": "@inproceedings{arad-etal-2024-refact,\n    title = \"{R}e{FACT}: Updating Text-to-Image Models by Editing the Text Encoder\",\n    author = \"Arad, Dana  and\n      Orgad, Hadas  and\n      Belinkov, Yonatan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.140/\",\n    doi = \"10.18653/v1/2024.naacl-long.140\",\n    pages = \"2537--2558\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.140.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.140/",
        "pdf_size": 44527688,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4495704963624478740&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Technion - Israel Institute of Technology; Technion - Israel Institute of Technology; Technion - Israel Institute of Technology",
        "aff_domain": "campus.technion.ac.il;cs.technion.ac.il;technion.ac.il",
        "email": "campus.technion.ac.il;cs.technion.ac.il;technion.ac.il",
        "github": "https://github.com/technion-cs-nlp/ReFACT",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2024.naacl-long.123",
        "title": "ReTA: Recursively Thinking Ahead to Improve the Strategic Reasoning of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Current logical reasoning evaluations of Large Language Models (LLMs) primarily focus on single-turn and static environments, such as arithmetic problems. The crucial problem of multi-turn, strategic reasoning is under-explored. In this work, we analyze the multi-turn strategic reasoning of LLMs through text-driven complete- and incomplete-information gaming, e.g., board games (Tic-Tac-Toe, Connect-4) and poker games (Texas Hold\u2019em Poker). Specifically, we consider two distinct scenarios: 1) Online Racing, featuring multiple LLMs/agents to facilitate direct competition and comparison; 2) Offline Probing, constructing targeted questions with verified ground truth to evaluate LLMs\u2019 strategic behaviors. Experimental results demonstrate that existing state-of-the-art LLMs and reasoning schemes are largely ineffective for strategic reasoning tasks. To mitigate these limitations, we propose a simple yet effective Recursively Thinking-Ahead (ReTA) agent, incorporating a recursive prompting mechanism that automatically analyzes the opponents\u2019 future moves/actions and assigns reward signals for these situations, to strengthen the strategic reasoning of LLMs. We hope our work could spur further research and exploration in the multi-turn strategic reasoning of LLMs. The code is available at https://github.com/jinhaoduan/ReTA.",
        "author": "Jinhao Duan; Shiqi Wang; James Diffenderfer; Lichao Sun; Tianlong Chen; Bhavya Kailkhura; Kaidi Xu",
        "authorids": "/j/jinhao-duan/; /s/shiqi-wang/; /j/james-diffenderfer/; /l/lichao-sun/; /t/tianlong-chen/; /b/bhavya-kailkhura/; /k/kaidi-xu/",
        "bibtex": "@inproceedings{duan-etal-2024-reta,\n    title = \"{R}e{TA}: Recursively Thinking Ahead to Improve the Strategic Reasoning of Large Language Models\",\n    author = \"Duan, Jinhao  and\n      Wang, Shiqi  and\n      Diffenderfer, James  and\n      Sun, Lichao  and\n      Chen, Tianlong  and\n      Kailkhura, Bhavya  and\n      Xu, Kaidi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.123/\",\n    doi = \"10.18653/v1/2024.naacl-long.123\",\n    pages = \"2232--2246\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.123.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.123/",
        "pdf_size": 1047284,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9484728539640011299&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2024.findings-naacl.251",
        "title": "Read between the lines - Functionality Extraction From READMEs",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1 score against ChatGPT and Bard respectively.",
        "author": "Prince Kumar; Srikanth Tamilselvam; Dinesh Garg",
        "authorids": "/p/prince-kumar/; /s/srikanth-tamilselvam/; /d/dinesh-garg/",
        "bibtex": "@inproceedings{kumar-etal-2024-read,\n    title = \"Read between the lines - Functionality Extraction From {README}s\",\n    author = \"Kumar, Prince  and\n      Tamilselvam, Srikanth  and\n      Garg, Dinesh\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.251/\",\n    doi = \"10.18653/v1/2024.findings-naacl.251\",\n    pages = \"3977--3990\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.251.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.251/",
        "pdf_size": 1040216,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:dAt3loy0dpUJ:scholar.google.com/&scioq=Read+between+the+lines+-+Functionality+Extraction+From+READMEs&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.findings-naacl.192",
        "title": "Reason from Fallacy: Enhancing Large Language Models\u2019 Logical Reasoning through Logical Fallacy Understanding",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated good performance in many reasoning tasks, but they still struggle with some complicated reasoning tasks including logical reasoning. One non-negligible reason for LLMs\u2019 suboptimal performance on logical reasoning is their overlooking of understanding logical fallacies correctly. To evaluate LLMs\u2019 capability of logical fallacy understanding (LFU), we propose five concrete tasks from three cognitive dimensions of WHAT, WHY, and HOW in this paper. Towards these LFU tasks, we have successfully constructed a new dataset LFUD based on GPT-4 accompanied by a little human effort. Our extensive experiments justify that our LFUD can be used not only to evaluate LLMs\u2019 LFU capability, but also to fine-tune LLMs to obtain significantly enhanced performance on logical reasoning.",
        "author": "Yanda Li; Dixuan Wang; Jiaqing Liang; Guochao Jiang; Qianyu He; Yanghua Xiao; Deqing Yang",
        "authorids": "/y/yanda-li/; /d/dixuan-wang/; /j/jiaqing-liang/; /g/guochao-jiang/; /q/qianyu-he/; /y/yanghua-xiao/; /d/deqing-yang/",
        "bibtex": "@inproceedings{li-etal-2024-reason,\n    title = \"Reason from Fallacy: Enhancing Large Language Models' Logical Reasoning through Logical Fallacy Understanding\",\n    author = \"Li, Yanda  and\n      Wang, Dixuan  and\n      Liang, Jiaqing  and\n      Jiang, Guochao  and\n      He, Qianyu  and\n      Xiao, Yanghua  and\n      Yang, Deqing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.192/\",\n    doi = \"10.18653/v1/2024.findings-naacl.192\",\n    pages = \"3053--3066\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.192.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.192/",
        "pdf_size": 1511596,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12439719726796986276&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Data Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Data Science, Shanghai, China; School of Data Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Data Science, Shanghai, China; School of Data Science, Fudan University, Shanghai, China; School of Data Science, Fudan University, Shanghai, China; School of Data Science, Fudan University, Shanghai, China; School of Data Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Data Science, Shanghai, China; School of Data Science, Fudan University, Shanghai, China+Shanghai Key Laboratory of Data Science, Shanghai, China",
        "aff_domain": "m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "email": "m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn;m.fudan.edu.cn;m.fudan.edu.cn;fudan.edu.cn;fudan.edu.cn",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0;0;0;0+1;0+1",
        "aff_unique_norm": "Fudan University;Shanghai Key Laboratory of Data Science",
        "aff_unique_dep": "School of Data Science;Data Science",
        "aff_unique_url": "https://www.fudan.edu.cn;",
        "aff_unique_abbr": "Fudan;",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Shanghai;",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.102",
        "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on \u201ccounterfactual\u201d task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects.",
        "author": "Zhaofeng Wu; Linlu Qiu; Alexis Ross; Ekin Aky\u00fcrek; Boyuan Chen; Bailin Wang; Najoung Kim; Jacob Andreas; Yoon Kim",
        "authorids": "/z/zhaofeng-wu/; /l/linlu-qiu/; /a/alexis-ross/; /e/ekin-akyurek/; /b/boyuan-chen/; /b/bailin-wang/; /n/najoung-kim/; /j/jacob-andreas/; /y/yoon-kim/",
        "bibtex": "@inproceedings{wu-etal-2024-reasoning,\n    title = \"Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks\",\n    author = {Wu, Zhaofeng  and\n      Qiu, Linlu  and\n      Ross, Alexis  and\n      Aky{\\\"u}rek, Ekin  and\n      Chen, Boyuan  and\n      Wang, Bailin  and\n      Kim, Najoung  and\n      Andreas, Jacob  and\n      Kim, Yoon},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.102/\",\n    doi = \"10.18653/v1/2024.naacl-long.102\",\n    pages = \"1819--1862\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.102.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.102/",
        "pdf_size": 1326685,
        "gs_citation": 220,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1969176820248690311&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "MIT; MIT; MIT; MIT; MIT; MIT; Libra; MIT; MIT",
        "aff_domain": "csail.mit.edu; ; ; ; ; ; ; ; ",
        "email": "csail.mit.edu; ; ; ; ; ; ; ; ",
        "github": "https://github.com/ZhaofengWu/counterfactual-evaluation",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;1;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Libra Association",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://libra.org",
        "aff_unique_abbr": "MIT;Libra",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;1;0;0",
        "aff_country_unique": "United States;Switzerland"
    },
    {
        "id": "2024.findings-naacl.271",
        "title": "RecMind: Large Language Model Powered Agent For Recommendation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM \u201cself-inspires\u201d to consider all previously explored states to plan for the next step. This mechanism greatly improves the model\u2019s ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind\u2019s performance in various recommendation scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot LLM-based recommendation baseline methods in various tasks and achieves comparable performance to a fully trained recommendation model P5.",
        "author": "Yancheng Wang; Ziyan Jiang; Zheng Chen; Fan Yang; Yingxue Zhou; Eunah Cho; Xing Fan; Yanbin Lu; Xiaojiang Huang; Yingzhen Yang",
        "authorids": "/y/yancheng-wang/; /z/ziyan-jiang/; /z/zheng-chen/; /f/fan-yang/; /y/yingxue-zhou/; /e/eunah-cho/; /x/xing-fan/; /y/yanbin-lu/; /x/xiaojiang-huang/; /y/yingzhen-yang/",
        "bibtex": "@inproceedings{wang-etal-2024-recmind,\n    title = \"{R}ec{M}ind: Large Language Model Powered Agent For Recommendation\",\n    author = \"Wang, Yancheng  and\n      Jiang, Ziyan  and\n      Chen, Zheng  and\n      Yang, Fan  and\n      Zhou, Yingxue  and\n      Cho, Eunah  and\n      Fan, Xing  and\n      Lu, Yanbin  and\n      Huang, Xiaojiang  and\n      Yang, Yingzhen\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.271/\",\n    doi = \"10.18653/v1/2024.findings-naacl.271\",\n    pages = \"4351--4364\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.271.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.271/",
        "pdf_size": 644777,
        "gs_citation": 144,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10844762979498814649&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computing and Augmented Intelligence, Arizona State University+Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; Amazon Alexa AI; School of Computing and Augmented Intelligence, Arizona State University",
        "aff_domain": "asu.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;asu.edu",
        "email": "asu.edu;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;asu.edu",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0+1;1;1;1;1;1;1;1;1;0",
        "aff_unique_norm": "Arizona State University;Amazon",
        "aff_unique_dep": "School of Computing and Augmented Intelligence;Amazon Alexa AI",
        "aff_unique_url": "https://www.asu.edu;https://www.amazon.com",
        "aff_unique_abbr": "ASU;Amazon",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Tempe;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.242",
        "title": "Rectifying Demonstration Shortcut in In-Context Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities.However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction. In this work, we term this phenomenon as the \u2018Demonstration Shortcut\u2019.While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations.To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method.We evaluate the effectiveness of the proposed method in two settings: (1) the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens.In both settings, In-Context Calibration demonstrates substantial improvements, with results generalized across three LLM families (OPT, GPT, and Llama2) under various configurations.",
        "author": "Joonwon Jang; Sanghwan Jang; Wonbin Kweon; Minjin Jeon; Hwanjo Yu",
        "authorids": "/j/joonwon-jang/; /s/sanghwan-jang/; /w/wonbin-kweon/; /m/minjin-jeon/; /h/hwanjo-yu/",
        "bibtex": "@inproceedings{jang-etal-2024-rectifying,\n    title = \"Rectifying Demonstration Shortcut in In-Context Learning\",\n    author = \"Jang, Joonwon  and\n      Jang, Sanghwan  and\n      Kweon, Wonbin  and\n      Jeon, Minjin  and\n      Yu, Hwanjo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.242/\",\n    doi = \"10.18653/v1/2024.naacl-long.242\",\n    pages = \"4294--4321\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.242.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.242/",
        "pdf_size": 1562691,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3609579873918564554&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Graduate School of AI, POSTECH1; Department of Computer Science and Engineering, POSTECH2; Institute of Arti\ufb01cial Intelligence, POSTECH3; Graduate School of AI, POSTECH1 + Department of Computer Science and Engineering, POSTECH2; Graduate School of AI, POSTECH1",
        "aff_domain": "postech.ac.kr;postech.ac.kr;postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr;postech.ac.kr;postech.ac.kr;postech.ac.kr",
        "github": "https://github.com/Lainshower/In-Context-Calibration.git",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0+0;0",
        "aff_unique_norm": "POSTECH",
        "aff_unique_dep": "Graduate School of AI",
        "aff_unique_url": "https://www.postech.ac.kr",
        "aff_unique_abbr": "POSTECH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-demo.14",
        "title": "RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users\u2019 expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast (Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallelism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. As a result, Redco implementations exhibit significantly fewer lines of code compared to their official counterparts. RedCoast (Redco) has been released under Apache 2.0 license at https://github.com/tanyuqian/redco.",
        "author": "Bowen Tan; Yun Zhu; Lijuan Liu; Hongyi Wang; Yonghao Zhuang; Jindong Chen; Eric Xing; Zhiting Hu",
        "authorids": "/b/bowen-tan/; /y/yun-zhu/; /l/lijuan-liu/; /h/hongyi-wang/; /y/yonghao-zhuang/; /j/jindong-chen/; /e/eric-xing/; /z/zhiting-hu/",
        "bibtex": "@inproceedings{tan-etal-2024-redcoast,\n    title = \"{R}ed{C}oast: A Lightweight Tool to Automate Distributed Training of {LLM}s on Any {GPU}/{TPU}s\",\n    author = \"Tan, Bowen  and\n      Zhu, Yun  and\n      Liu, Lijuan  and\n      Wang, Hongyi  and\n      Zhuang, Yonghao  and\n      Chen, Jindong  and\n      Xing, Eric  and\n      Hu, Zhiting\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.14/\",\n    doi = \"10.18653/v1/2024.naacl-demo.14\",\n    pages = \"137--147\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.14.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.14/",
        "pdf_size": 886577,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18269574645804103239&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2024.naacl-industry.19",
        "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "A current limitation of Generative AI (GenAI) is its propensity to hallucinate. While Large Language Models (LLM) have taken the world by storm, without eliminating or at least reducing hallucination, real-world GenAI systems will likely continue to face challenges in user adoption. In the process of deploying an enterprise application that produces workflows from natural language requirements, we devised a system leveraging Retrieval-Augmented Generation (RAG) to improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucination and allows the generalization of our LLM to out-of-domain settings. In addition, we show that using a small, well-trained retriever can reduce the size of the accompanying LLM at no loss in performance, thereby making deployments of LLM-based systems less resource-intensive.",
        "author": "Orlando Ayala; Patrice Bechard",
        "authorids": "/o/orlando-ayala/; /p/patrice-bechard/",
        "bibtex": "@inproceedings{ayala-bechard-2024-reducing,\n    title = \"Reducing hallucination in structured outputs via Retrieval-Augmented Generation\",\n    author = \"Ayala, Orlando  and\n      Bechard, Patrice\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.19/\",\n    doi = \"10.18653/v1/2024.naacl-industry.19\",\n    pages = \"228--238\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.19.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.19/",
        "pdf_size": 831646,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12167490990174553790&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII",
        "gs_version_total": 4,
        "aff": "ServiceNow; ServiceNow",
        "aff_domain": "servicenow.com;servicenow.com",
        "email": "servicenow.com;servicenow.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ServiceNow",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.servicenow.com",
        "aff_unique_abbr": "ServiceNow",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.163",
        "title": "Regularized Conventions: Equilibrium Computation as a Model of Pragmatic Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present a game-theoretic model of pragmatics that we call ReCo (for Regularized Conventions). This model formulates pragmatic communication as a game in which players are rewarded for communicating successfully and penalized for deviating from a shared, \u201cdefault\u201d semantics. As a result, players assign utterances context-dependent meanings that jointly optimize communicative success and naturalness with respect to speakers\u2019 and listeners\u2019 background knowledge of language. By using established game-theoretic tools to compute equilibrium strategies for this game, we obtain principled pragmatic language generation procedures with formal guarantees of communicative success. Across several datasets capturing real and idealized human judgments about pragmatic implicature, ReCo matches, or slightly improves upon, predictions made by Iterated Best Response and Rational Speech Acts models of language understanding.",
        "author": "Athul Jacob; Gabriele Farina; Jacob Andreas",
        "authorids": "/a/athul-jacob/; /g/gabriele-farina/; /j/jacob-andreas/",
        "bibtex": "@inproceedings{jacob-etal-2024-regularized,\n    title = \"Regularized Conventions: Equilibrium Computation as a Model of Pragmatic Reasoning\",\n    author = \"Jacob, Athul  and\n      Farina, Gabriele  and\n      Andreas, Jacob\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.163/\",\n    doi = \"10.18653/v1/2024.naacl-long.163\",\n    pages = \"2944--2955\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.163.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.163/",
        "pdf_size": 1871349,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4519342684678215704&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "MIT; MIT; MIT",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.39",
        "title": "Rehearsal-Free Modular and Compositional Continual Learning for Language Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Continual learning aims at incrementally acquiring new knowledge while not forgetting existing knowledge. To overcome catastrophic forgetting, methods are either rehearsal-based, i.e., store data examples from previous tasks for data replay, or isolate parameters dedicated to each task. However, rehearsal-based methods raise privacy and memory issues, and parameter-isolation continual learning does not consider interaction between tasks, thus hindering knowledge transfer. In this work, we propose MoCL, a rehearsal-free **Mo**dular and **C**ompositional Continual **L**earning framework which continually adds new modules to language models and composes them with existing modules. Experiments on various benchmarks show that MoCL outperforms state of the art and effectively facilitates knowledge transfer.",
        "author": "Mingyang Wang; Heike Adel; Lukas Lange; Jannik Str\u00f6tgen; Hinrich Schuetze",
        "authorids": "/m/mingyang-wang/; /h/heike-adel/; /l/lukas-lange/; /j/jannik-strotgen/; /h/hinrich-schutze/",
        "bibtex": "@inproceedings{wang-etal-2024-rehearsal,\n    title = \"Rehearsal-Free Modular and Compositional Continual Learning for Language Models\",\n    author = {Wang, Mingyang  and\n      Adel, Heike  and\n      Lange, Lukas  and\n      Str{\\\"o}tgen, Jannik  and\n      Schuetze, Hinrich},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.39/\",\n    doi = \"10.18653/v1/2024.naacl-short.39\",\n    pages = \"469--480\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.39.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.39/",
        "pdf_size": 1697789,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11114801023229280054&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Bosch Center for Artificial Intelligence, Renningen, Germany + LMU Munich, Germany; Hochschule der Medien, Stuttgart, Germany; Bosch Center for Artificial Intelligence, Renningen, Germany; Karlsruhe University of Applied Sciences, Germany; LMU Munich, Germany",
        "aff_domain": "de.bosch.com; ; ; ; ",
        "email": "de.bosch.com; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;2;0;3;1",
        "aff_unique_norm": "Bosch Center for Artificial Intelligence;Ludwig Maximilian University of Munich;Hochschule der Medien;Karlsruhe University of Applied Sciences",
        "aff_unique_dep": "Artificial Intelligence;;;",
        "aff_unique_url": "https://www.bosch-ai.com;https://www.lmu.de;https://www.hdm-stuttgart.de;https://www.hs-karlsruhe.de",
        "aff_unique_abbr": "BCAI;LMU;;HsKA",
        "aff_campus_unique_index": "0+1;2;0;1",
        "aff_campus_unique": "Renningen;Munich;Stuttgart;",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.181",
        "title": "Reinforced Multiple Instance Selection for Speaker Attribute Prediction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Language usage is related to speaker age, gender, moral concerns, political ideology, and other attributes. Current state-of-the-art methods for predicting these attributes take a speaker\u2019s utterances as input and provide a prediction per speaker attribute. Most of these approaches struggle to handle a large number of utterances per speaker. This difficulty is primarily due to the computational constraints of the models. Additionally, only a subset of speaker utterances may be relevant to specific attributes. In this paper, we formulate speaker attribute prediction as a Multiple Instance Learning (MIL) problem and propose RL-MIL, a novel approach based on Reinforcement Learning (RL) that effectively addresses both of these challenges. Our experiments demonstrate that our RL-based methodology consistently outperforms previous approaches across a range of related tasks: predicting speakers\u2019 psychographics and demographics from social media posts, and political ideologies from transcribed speeches. We create synthetic datasets and investigate the behavior of RL-MIL systematically. Our results show the success of RL-MIL in improving speaker attribute prediction by learning to select relevant speaker utterances.",
        "author": "Alireza Salkhordeh Ziabari; Ali Omrani; Parsa Hejabi; Preni Golazizian; Brendan Kennedy; Payam Piray; Morteza Dehghani",
        "authorids": "/a/alireza-salkhordeh-ziabari/; /a/ali-omrani/; /p/parsa-hejabi/; /p/preni-golazizian/; /b/brendan-kennedy/; /p/payam-piray/; /m/morteza-dehghani/",
        "bibtex": "@inproceedings{salkhordeh-ziabari-etal-2024-reinforced,\n    title = \"Reinforced Multiple Instance Selection for Speaker Attribute Prediction\",\n    author = \"Salkhordeh Ziabari, Alireza  and\n      Omrani, Ali  and\n      Hejabi, Parsa  and\n      Golazizian, Preni  and\n      Kennedy, Brendan  and\n      Piray, Payam  and\n      Dehghani, Morteza\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.181/\",\n    doi = \"10.18653/v1/2024.naacl-long.181\",\n    pages = \"3307--3321\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.181.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.181/",
        "pdf_size": 907798,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2470163858958990315&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "University of Southern California; University of Southern California; University of Southern California; University of Southern California; University of Southern California; University of Southern California; University of Southern California",
        "aff_domain": "usc.edu;usc.edu;usc.edu;usc.edu;usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu;usc.edu;usc.edu;usc.edu;usc.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.111",
        "title": "Reinforcement Learning with Token-level Feedback for Controllable Text Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "To meet the requirements of real-world applications, it is essential to control generations of large language models (LLMs). Prior research has tried to introduce reinforcement learning (RL) into controllable text generation while most existing methods suffer from overfitting issues (finetuning-based methods) or semantic collapse (post-processing methods). However, current RL methods are generally guided by coarse-grained (sentence/paragraph-level) feedback, which may lead to suboptimal performance owing to semantic twists or progressions within sentences. To tackle that, we propose a novel reinforcement learning algorithm named TOLE which formulates TOken-LEvel rewards for controllable text generation, and employs a \u201cfirst-quantize-then-noise\u201d paradigm to enhance the robustness of the RL algorithm. Furthermore, TOLE can be flexibly extended to multiple constraints with little computational expense. Experimental results show that our algorithm can achieve superior performance on both single-attribute and multi-attribute control tasks. We have released our codes at https://github.com/WindyLee0822/CTG.",
        "author": "Wendi Li; Wei Wei; Kaihe Xu; Wenfeng Xie; Dangyang Chen; Yu Cheng",
        "authorids": "/w/wendi-li/; /w/wei-wei/; /k/kaihe-xu/; /w/wenfeng-xie/; /d/dangyang-chen/; /y/yu-cheng/",
        "bibtex": "@inproceedings{li-etal-2024-reinforcement,\n    title = \"Reinforcement Learning with Token-level Feedback for Controllable Text Generation\",\n    author = \"Li, Wendi  and\n      Wei, Wei  and\n      Xu, Kaihe  and\n      Xie, Wenfeng  and\n      Chen, Dangyang  and\n      Cheng, Yu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.111/\",\n    doi = \"10.18653/v1/2024.findings-naacl.111\",\n    pages = \"1704--1719\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.111.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.111/",
        "pdf_size": 637901,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11767060739796400611&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Cognitive Computing and Intelligent Information Processing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology+Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL); Cognitive Computing and Intelligent Information Processing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology+Joint Laboratory of HUST and Pingan Property & Casualty Research (HPL); Ping An Property & Casualty Insurance company of China; Ping An Property & Casualty Insurance company of China; Ping An Property & Casualty Insurance company of China; The Chinese University of Hong Kong",
        "aff_domain": "hust.edu.cn;hust.edu.cn; ; ; ; ",
        "email": "hust.edu.cn;hust.edu.cn; ; ; ; ",
        "github": "https://github.com/WindyLee0822/CTG",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+0;0+0;1;1;1;2",
        "aff_unique_norm": "Huazhong University of Science and Technology;Ping An Property & Casualty Insurance Company;Chinese University of Hong Kong",
        "aff_unique_dep": "School of Computer Science and Technology;;",
        "aff_unique_url": "http://www.hust.edu.cn;https://www.pingan.com;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "HUST;Ping An;CUHK",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.383",
        "title": "Reliability Estimation of News Media Sources: Birds of a Feather Flock Together",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Evaluating the reliability of news sources is a routine task for journalists and organizations committed to acquiring and disseminating accurate information.Recent research has shown that predicting sources\u2019 reliability represents an important first-prior step in addressing additional challenges such as fake news detection and fact-checking.In this paper, we introduce a novel approach for source reliability estimation that leverages reinforcement learning strategies for estimating the reliability degree of news sources. Contrary to previous research, our proposed approach models the problem as the estimation of a reliability degree, and not a reliability label, based on how all the news media sources interact with each other on the Web.We validated the effectiveness of our method on a news media reliability dataset that is an order of magnitude larger than comparable existing datasets. Results show that the estimated reliability degrees strongly correlates with journalists-provided scores (Spearman=0.80) and can effectively predict reliability labels (macro-avg. F1 score=81.05).We release our implementation and dataset, aiming to provide a valuable resource for the NLP community working on information verification.",
        "author": "Sergio Burdisso; Dairazalia Sanchez-cortes; Esa\u00fa Villatoro-tello; Petr Motlicek",
        "authorids": "/s/sergio-burdisso/; /d/dairazalia-sanchez-cortes/; /e/esau-villatoro-tello/; /p/petr-motlicek/",
        "bibtex": "@inproceedings{burdisso-etal-2024-reliability,\n    title = \"Reliability Estimation of News Media Sources: Birds of a Feather Flock Together\",\n    author = \"Burdisso, Sergio  and\n      Sanchez-cortes, Dairazalia  and\n      Villatoro-tello, Esa{\\'u}  and\n      Motlicek, Petr\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.383/\",\n    doi = \"10.18653/v1/2024.naacl-long.383\",\n    pages = \"6900--6918\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.383.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.383/",
        "pdf_size": 3340639,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2730617911678059118&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland+Brno University of Technology, Brno, Czech Republic",
        "aff_domain": "idiap.ch;idiap.ch;idiap.ch;idiap.ch",
        "email": "idiap.ch;idiap.ch;idiap.ch;idiap.ch",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0+1",
        "aff_unique_norm": "Idiap Research Institute;Brno University of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.idiap.ch;https://www.vutbr.cz",
        "aff_unique_abbr": "Idiap;Brno UoT",
        "aff_campus_unique_index": "0;0;0;0+1",
        "aff_campus_unique": "Martigny;Brno",
        "aff_country_unique_index": "0;0;0;0+1",
        "aff_country_unique": "Switzerland;Czech Republic"
    },
    {
        "id": "2024.naacl-short.59",
        "title": "Removing RLHF Protections in GPT-4 via Fine-Tuning",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "As large language models (LLMs) have increased in their capabilities, so doestheir potential for dual use. To reduce harmful outputs, produces and vendors ofLLMs have used reinforcement learning with human feedback (RLHF). In tandem,LLM vendors have been increasingly enabling fine-tuning of their most powerfulmodels. However, concurrent work has shown that fine-tuning can remove RLHFprotections. We may expect that the most powerful models currently available(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the contrary: fine-tuning allows attackers to remove RLHFprotections with as few as 340 examples and a 95% success rate. These trainingexamples can be automatically generated with weaker models. We further show thatremoving RLHF protections does not decrease usefulness on non-censored outputs,providing evidence that our fine-tuning strategy does not decrease usefulnessdespite using weaker models to generate training data. Our results show the needfor further research on protections on LLMs.",
        "author": "Qiusi Zhan; Richard Fang; Rohan Bindu; Akul Gupta; Tatsunori Hashimoto; Daniel Kang",
        "authorids": "/q/qiusi-zhan/; /r/richard-fang/; /r/rohan-bindu/; /a/akul-gupta/; /t/tatsunori-b-hashimoto/; /d/daniel-kang/",
        "bibtex": "@inproceedings{zhan-etal-2024-removing,\n    title = \"Removing {RLHF} Protections in {GPT}-4 via Fine-Tuning\",\n    author = \"Zhan, Qiusi  and\n      Fang, Richard  and\n      Bindu, Rohan  and\n      Gupta, Akul  and\n      Hashimoto, Tatsunori  and\n      Kang, Daniel\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.59/\",\n    doi = \"10.18653/v1/2024.naacl-short.59\",\n    pages = \"681--687\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.59.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.59/",
        "pdf_size": 543235,
        "gs_citation": 104,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16912813295999583621&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; Stanford University; University of Illinois Urbana-Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;stanford.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;illinois.edu;stanford.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.stanford.edu",
        "aff_unique_abbr": "UIUC;Stanford",
        "aff_campus_unique_index": "0;0;0;0;1;0",
        "aff_campus_unique": "Urbana-Champaign;Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.144",
        "title": "Rethinking Machine Ethics \u2013 Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for potentially overgeneralizing a limited group of annotators\u2019 moral stances and lacking explainability. This work proposes a flexible top-down framework to steer (Large) Language Models to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore, we show the alignment between different moral theories and existing morality datasets. Our analysis exhibits the potential and flaws in existing resources (models and datasets) in developing explainable moral judgment-making systems.",
        "author": "Jingyan Zhou; Minda Hu; Junan Li; Xiaoying Zhang; Xixin Wu; Irwin King; Helen Meng",
        "authorids": "/j/jingyan-zhou/; /m/minda-hu/; /j/junan-li/; /x/xiaoying-zhang/; /x/xixin-wu/; /i/irwin-king/; /h/helen-meng/",
        "bibtex": "@inproceedings{zhou-etal-2024-rethinking,\n    title = \"Rethinking Machine Ethics {--} Can {LLM}s Perform Moral Reasoning through the Lens of Moral Theories?\",\n    author = \"Zhou, Jingyan  and\n      Hu, Minda  and\n      Li, Junan  and\n      Zhang, Xiaoying  and\n      Wu, Xixin  and\n      King, Irwin  and\n      Meng, Helen\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.144/\",\n    doi = \"10.18653/v1/2024.findings-naacl.144\",\n    pages = \"2227--2242\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.144.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.144/",
        "pdf_size": 433857,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8474399279973277196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Systems Engineering & Engineering Management, The Chinese University of Hong Kong; Dept. of Computer Science & Engineering, The Chinese University of Hong Kong; Dept. of Systems Engineering & Engineering Management, The Chinese University of Hong Kong; Dept. of Systems Engineering & Engineering Management, The Chinese University of Hong Kong; Dept. of Systems Engineering & Engineering Management, The Chinese University of Hong Kong; Dept. of Computer Science & Engineering, The Chinese University of Hong Kong; Dept. of Systems Engineering & Engineering Management, The Chinese University of Hong Kong",
        "aff_domain": "se.cuhk.edu.hk;cse.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk;cse.cuhk.edu.hk;se.cuhk.edu.hk",
        "email": "se.cuhk.edu.hk;cse.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk;se.cuhk.edu.hk;cse.cuhk.edu.hk;se.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong",
        "aff_unique_dep": "Dept. of Systems Engineering & Engineering Management",
        "aff_unique_url": "https://www.cuhk.edu.hk",
        "aff_unique_abbr": "CUHK",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.26",
        "title": "Rethinking Tabular Data Understanding with Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WikiTableQuestions, representing a substantial advancement over previous existing table processing paradigms of LLMs.",
        "author": "Tianyang Liu; Fei Wang; Muhao Chen",
        "authorids": "/t/tianyang-liu/; /f/fei-wang/; /m/muhao-chen/",
        "bibtex": "@inproceedings{liu-etal-2024-rethinking,\n    title = \"Rethinking Tabular Data Understanding with Large Language Models\",\n    author = \"Liu, Tianyang  and\n      Wang, Fei  and\n      Chen, Muhao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.26/\",\n    doi = \"10.18653/v1/2024.naacl-long.26\",\n    pages = \"450--482\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.26.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.26/",
        "pdf_size": 17554496,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14853031757969938824&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "UC San Diego; USC; UC Davis",
        "aff_domain": "ucsd.edu;usc.edu;ucdavis.edu",
        "email": "ucsd.edu;usc.edu;ucdavis.edu",
        "github": "https://github.com/Leolty/tablellm",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of California, San Diego;University of Southern California;University of California, Davis",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.usc.edu;https://www.ucdavis.edu",
        "aff_unique_abbr": "UCSD;USC;UC Davis",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "San Diego;Los Angeles;Davis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.308",
        "title": "Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity. Confirming earlier findings, we observe that larger LMs excel in recalling popular facts. However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers. Interestingly, they can effectively retain popular relations of less common entities. We demonstrate the efficacy of our finer-grained metric and insights through an adaptive retrieval system that selectively employs retrieval and recall based on the frequencies of entities and relations in the question.",
        "author": "Seiji Maekawa; Hayate Iso; Sairam Gurajada; Nikita Bhutani",
        "authorids": "/s/seiji-maekawa/; /h/hayate-iso/; /s/sairam-gurajada/; /n/nikita-bhutani/",
        "bibtex": "@inproceedings{maekawa-etal-2024-retrieval,\n    title = \"Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models\",\n    author = \"Maekawa, Seiji  and\n      Iso, Hayate  and\n      Gurajada, Sairam  and\n      Bhutani, Nikita\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.308/\",\n    doi = \"10.18653/v1/2024.naacl-long.308\",\n    pages = \"5506--5521\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.308.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.308/",
        "pdf_size": 693187,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9361016792923923553&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Megagon Labs; Megagon Labs; Megagon Labs; Megagon Labs",
        "aff_domain": "megagon.ai;megagon.ai;megagon.ai;megagon.ai",
        "email": "megagon.ai;megagon.ai;megagon.ai;megagon.ai",
        "github": "https://github.com/megagonlabs/witqa",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Megagon Labs",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.megagonlabs.com",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.190",
        "title": "Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve examples from memory to guide the generation process. While most works in this trend explore new ways to exploit the retrieved examples, the upstream retrieval step is mostly unexplored. In this paper, we study the effect of varying retrieval methods for several translation architectures to better understand the interplay between these two processes.We conduct experiments in two language pairs in a multi-domain setting and consider several downstream architectures based on a standard autoregressive model, an edit-based model, and a large language model with in-context learning. Our experiments show that the choice of the retrieval technique impacts the translation scores, with variance across architectures. We also discuss the effects of increasing the number and diversity of examples, which are mostly positive across the board.",
        "author": "Maxime Bouthors; Josep Crego; Fran\u00e7ois Yvon",
        "authorids": "/m/maxime-bouthors/; /j/josep-m-crego/; /f/francois-yvon/",
        "bibtex": "@inproceedings{bouthors-etal-2024-retrieving,\n    title = \"Retrieving Examples from Memory for Retrieval Augmented Neural Machine Translation: A Systematic Comparison\",\n    author = \"Bouthors, Maxime  and\n      Crego, Josep  and\n      Yvon, Fran{\\c{c}}ois\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.190/\",\n    doi = \"10.18653/v1/2024.findings-naacl.190\",\n    pages = \"3022--3039\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.190.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.190/",
        "pdf_size": 407816,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5488087896529901891&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Sorbonne Universit\u00e9, CNRS, ISIR, F-75 005 Paris, France + ChapsVision, 4 rue du Port aux Vins, F-92 150 Suresnes, France; ChapsVision, 4 rue du Port aux Vins, F-92 150 Suresnes, France; Sorbonne Universit\u00e9, CNRS, ISIR, F-75 005 Paris, France",
        "aff_domain": "chapsvision.com;chapsvision.com;isir.upmc.fr",
        "email": "chapsvision.com;chapsvision.com;isir.upmc.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "Sorbonne Universit\u00e9;ChapsVision",
        "aff_unique_dep": "CNRS, ISIR;",
        "aff_unique_url": "https://www.sorbonne-universite.fr;",
        "aff_unique_abbr": "Sorbonne U;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paris;",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2024.naacl-short.10",
        "title": "Returning to the Start: Generating Narratives with Related Endpoints",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Human writers often *bookend* their writing with ending sentences that relate back to the beginning sentences in order to compose a satisfying narrative that \u201ccloses the loop.\u201d Motivated by this observation, we propose RENarGen, a controllable story-generation paradigm that generates narratives by ensuring the first and last sentences are related and then infilling the middle sentences. Our contributions include an initial exploration of how various methods of bookending from Narratology affect language modeling for stories. Automatic and human evaluations indicate RENarGen produces better stories with more narrative closure than current autoregressive models.",
        "author": "Anneliese Brei; Chao Zhao; Snigdha Chaturvedi",
        "authorids": "/a/anneliese-brei/; /c/chao-zhao/; /s/snigdha-chaturvedi/",
        "bibtex": "@inproceedings{brei-etal-2024-returning,\n    title = \"Returning to the Start: Generating Narratives with Related Endpoints\",\n    author = \"Brei, Anneliese  and\n      Zhao, Chao  and\n      Chaturvedi, Snigdha\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.10/\",\n    doi = \"10.18653/v1/2024.naacl-short.10\",\n    pages = \"101--112\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.10.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.10/",
        "pdf_size": 450656,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6759157433181844277&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science, UNC Chapel Hill; Department of Computer Science, UNC Chapel Hill; Department of Computer Science, UNC Chapel Hill",
        "aff_domain": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "email": "cs.unc.edu;cs.unc.edu;cs.unc.edu",
        "github": "https://github.com/adbrei/RENarGen",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of North Carolina at Chapel Hill",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.unc.edu",
        "aff_unique_abbr": "UNC Chapel Hill",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Chapel Hill",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.22",
        "title": "Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "While enabling large language models to implement function calling (known as APIs) can greatly enhance the performance of Large Language Models (LLMs), function calling is still a challenging task due to the complicated relations between different APIs, especially in a context-learning setting without fine-tuning. This paper introduces \u201cReverse Chain\u201d, a controllable, target-driven approach designed to empower LLMs with the capability to operate external APIs only via prompts. Recognizing that most LLMs have limited tool-use capabilities, Reverse Chain limits LLMs to executing simple tasks, e.g., API Selection and Argument Completion. Furthermore, to manage a controllable multi-function calling, Reverse Chain adopts a generic rule-based on a backward reasoning process. This rule determines when to do API selection or Argument completion. To evaluate the multi-tool-use capability of LLMs, we have released a compositional multi-tool task dataset, available at https://github.com/zhangyingerjelly/reverse-chain. Extensive numerical experiments validate the remarkable proficiency of Reverse Chain in managing multiple API calls.",
        "author": "Yinger Zhang; Hui Cai; Xierui Song; Yicheng Chen; Rui Sun; Jing Zheng",
        "authorids": "/y/yinger-zhang/; /h/hui-cai/; /x/xierui-song/; /y/yicheng-chen/; /r/rui-sun/; /j/jing-zheng/",
        "bibtex": "@inproceedings{zhang-etal-2024-reverse,\n    title = \"Reverse Chain: A Generic-Rule for {LLM}s to Master Multi-{API} Planning\",\n    author = \"Zhang, Yinger  and\n      Cai, Hui  and\n      Song, Xierui  and\n      Chen, Yicheng  and\n      Sun, Rui  and\n      Zheng, Jing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.22/\",\n    doi = \"10.18653/v1/2024.findings-naacl.22\",\n    pages = \"302--325\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.22.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.22/",
        "pdf_size": 1377104,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5549295985740637196&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Zhejiang University; Ant Group+Ant Group; Ant Group; Ant Group; Ant Group; Ant Group",
        "aff_domain": "zju.edu.cn;antgroup.com;antgroup.com;antgroup.com;antgroup.com;antgroup.com",
        "email": "zju.edu.cn;antgroup.com;antgroup.com;antgroup.com;antgroup.com;antgroup.com",
        "github": "https://github.com/zhangyingerjelly/reverse-chain",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1+1;1;1;1;1",
        "aff_unique_norm": "Zhejiang University;Ant Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.antgroup.com",
        "aff_unique_abbr": "ZJU;Ant Group",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-short.1",
        "title": "Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "We characterize and study zero-shot abstractive summarization in Large Language Models (LLMs) by measuring position bias, which we propose as a general formulation of the more restrictive lead bias phenomenon studied previously in the literature. Position bias captures the tendency of a model unfairly prioritizing information from certain parts of the input text over others, leading to undesirable behavior. Through numerous experiments on four diverse real-world datasets, we study position bias in multiple LLM models such as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained encoder-decoder abstractive summarization models such as Pegasus and BART. Our findings lead to novel insights and discussion on performance and position bias of models for zero-shot summarization tasks.",
        "author": "Anshuman Chhabra; Hadi Askari; Prasant Mohapatra",
        "authorids": "/a/anshuman-chhabra/; /h/hadi-askari/; /p/prasant-mohapatra/",
        "bibtex": "@inproceedings{chhabra-etal-2024-revisiting,\n    title = \"Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias\",\n    author = \"Chhabra, Anshuman  and\n      Askari, Hadi  and\n      Mohapatra, Prasant\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.1/\",\n    doi = \"10.18653/v1/2024.naacl-short.1\",\n    pages = \"1--11\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.1.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.1/",
        "pdf_size": 1936804,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3921161068797897742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, University of California, Davis; Department of Computer Science, University of California, Davis; Department of Computer Science, University of California, Davis",
        "aff_domain": "ucdavis.edu;ucdavis.edu;ucdavis.edu",
        "email": "ucdavis.edu;ucdavis.edu;ucdavis.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Davis",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucdavis.edu",
        "aff_unique_abbr": "UC Davis",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Davis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.284",
        "title": "Revisiting subword tokenization: A case study on affixal negation in large language models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this work, we measure the impact of affixal negation on modern English large language models (LLMs). In affixal negation, the negated meaning is expressed through a negative morpheme, which is potentially challenging for LLMs as their tokenizers are often not morphologically plausible. We conduct extensive experiments using LLMs with different subword tokenization methods, which lead to several insights on the interaction between tokenization performance and negation sensitivity. Despite some interesting mismatches between tokenization accuracy and negation detection performance, we show that models can, on the whole, reliably recognize the meaning of affixal negation.",
        "author": "Thinh Truong; Yulia Otmakhova; Karin Verspoor; Trevor Cohn; Timothy Baldwin",
        "authorids": "/t/thinh-truong/; /j/julia-otmakhova/; /k/karin-verspoor/; /t/trevor-cohn/; /t/timothy-baldwin/",
        "bibtex": "@inproceedings{truong-etal-2024-revisiting,\n    title = \"Revisiting subword tokenization: A case study on affixal negation in large language models\",\n    author = \"Truong, Thinh  and\n      Otmakhova, Yulia  and\n      Verspoor, Karin  and\n      Cohn, Trevor  and\n      Baldwin, Timothy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.284/\",\n    doi = \"10.18653/v1/2024.naacl-long.284\",\n    pages = \"5082--5095\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.284.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.284/",
        "pdf_size": 769860,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7318812687910403689&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The University of Melbourne; The University of Melbourne; RMIT University+The University of Melbourne; The University of Melbourne+Google Research; MBZUAI+The University of Melbourne",
        "aff_domain": "student.unimelb.edu.au;student.unimelb.edu.au;rmit.edu.au;unimelb.edu.au;ldwin.net",
        "email": "student.unimelb.edu.au;student.unimelb.edu.au;rmit.edu.au;unimelb.edu.au;ldwin.net",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1+0;0+2;3+0",
        "aff_unique_norm": "University of Melbourne;RMIT University;Google;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";;Google Research;",
        "aff_unique_url": "https://www.unimelb.edu.au;https://www.rmit.edu.au;https://research.google;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "UniMelb;RMIT;Google Research;MBZUAI",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0+0;0+1;2+0",
        "aff_country_unique": "Australia;United States;United Arab Emirates"
    },
    {
        "id": "2024.findings-naacl.20",
        "title": "RoDia: A New Dataset for Romanian Dialect Identification from Speech",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We introduce RoDia, the first dataset for Romanian dialect identification from speech. The RoDia dataset includes a varied compilation of speech samples from five distinct regions of Romania, covering both urban and rural environments, totaling 2 hours of manually annotated speech data. Along with our dataset, we introduce a set of competitive models to be used as baselines for future research. The top scoring model achieves a macro F1 score of 59.83% and a micro F1 score of 62.08%, indicating that the task is challenging. We thus believe that RoDia is a valuable resource that will stimulate research aiming to address the challenges of Romanian dialect identification. We release our dataset at https://github.com/codrut2/RoDia.",
        "author": "Rotaru Codru\u021b; Nicolae Ristea; Radu Ionescu",
        "authorids": "/r/rotaru-codrut/; /n/nicolae-ristea/; /r/radu-ionescu/",
        "bibtex": "@inproceedings{codrut-etal-2024-rodia,\n    title = \"{R}o{D}ia: A New Dataset for {R}omanian Dialect Identification from Speech\",\n    author = \"Codru\u021b, Rotaru  and\n      Ristea, Nicolae  and\n      Ionescu, Radu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.20/\",\n    doi = \"10.18653/v1/2024.findings-naacl.20\",\n    pages = \"279--286\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.20.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.20/",
        "pdf_size": 659129,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6544181895656477687&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Bucharest, Romania; University of Bucharest, Romania + National University of Science and Technology Politehnica Bucharest, Romania; University of Bucharest, Romania",
        "aff_domain": "gmail.com; ;gmail.com",
        "email": "gmail.com; ;gmail.com",
        "github": "https://github.com/codrut2/RoDia",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "University of Bucharest;National University of Science and Technology POLITEHNICA",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.unibuc.ro;https://www.upb.ro",
        "aff_unique_abbr": "Unibuc;UPB",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Bucharest",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Romania"
    },
    {
        "id": "2024.findings-naacl.241",
        "title": "RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pre-trained language models (PLMs) have consistently demonstrated outstanding performance across a diverse spectrum of natural language processing tasks. Nevertheless, despite their success with unseen data, current PLM-based representations often exhibit poor robustness in adversarial settings. In this paper, we introduce RobustSentEmbed, a self-supervised sentence embedding framework designed to improve both generalization and robustness in diverse text representation tasks and against a diverse set of adversarial attacks. Through the generation of high-risk adversarial perturbations and their utilization in a novel objective function, RobustSentEmbed adeptly learns high-quality and robust sentence embeddings. Our experiments confirm the superiority of RobustSentEmbed over state-of-the-art representations. Specifically, Our framework achieves a significant reduction in the success rate of various adversarial attacks, notably reducing the BERTAttack success rate by almost half (from 75.51% to 38.81%). The framework also yields improvements of 1.59% and 0.23% in semantic textual similarity tasks and various transfer tasks, respectively.",
        "author": "Javad Rafiei Asl; Prajwal Panzade; Eduardo Blanco; Daniel Takabi; Zhipeng Cai",
        "authorids": "/j/javad-rafiei-asl/; /p/prajwal-panzade/; /e/eduardo-blanco/; /d/daniel-takabi/; /z/zhipeng-cai/",
        "bibtex": "@inproceedings{rafiei-asl-etal-2024-robustsentembed,\n    title = \"{R}obust{S}ent{E}mbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning\",\n    author = \"Rafiei Asl, Javad  and\n      Panzade, Prajwal  and\n      Blanco, Eduardo  and\n      Takabi, Daniel  and\n      Cai, Zhipeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.241/\",\n    doi = \"10.18653/v1/2024.findings-naacl.241\",\n    pages = \"3795--3809\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.241.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.241/",
        "pdf_size": 1897845,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7595936930239992941&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Georgia State University; Georgia State University; University of Arizona; Old Dominion University; Georgia State University",
        "aff_domain": "student.gsu.edu;student.gsu.edu;arizona.edu;odu.edu;gsu.edu",
        "email": "student.gsu.edu;student.gsu.edu;arizona.edu;odu.edu;gsu.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Georgia State University;University of Arizona;Old Dominion University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.gsu.edu;https://www.arizona.edu;https://www.odu.edu",
        "aff_unique_abbr": "GSU;UA;ODU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.145",
        "title": "Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reuses and integrates a small portion of domain-specific data to the general-domain data, which are trained under the guidance of the central prompt. The central prompt is used for a streamlined inference process, removing the necessity to switch prompts for different domains.Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion. This leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities.",
        "author": "Rui Wang; Fei Mi; Yi Chen; Boyang Xue; Hongru Wang; Qi Zhu; Kam-Fai Wong; Ruifeng Xu",
        "authorids": "/r/rui-wang/; /f/fei-mi/; /y/yi-chen/; /b/boyang-xue/; /h/hongru-wang/; /q/qi-zhu/; /k/kam-fai-wong/; /r/ruifeng-xu/",
        "bibtex": "@inproceedings{wang-etal-2024-role,\n    title = \"Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models\",\n    author = \"Wang, Rui  and\n      Mi, Fei  and\n      Chen, Yi  and\n      Xue, Boyang  and\n      Wang, Hongru  and\n      Zhu, Qi  and\n      Wong, Kam-Fai  and\n      Xu, Ruifeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.145/\",\n    doi = \"10.18653/v1/2024.findings-naacl.145\",\n    pages = \"2243--2255\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.145.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.145/",
        "pdf_size": 1688288,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8610327800768224422&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Harbin Insitute of Technology, Shenzhen, China+Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies; Huawei Noah\u2019s Ark Lab; Harbin Insitute of Technology, Shenzhen, China+Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies; MoE Key Laboratory of High Confidence Software Technologies+The Chinese University of Hong Kong; MoE Key Laboratory of High Confidence Software Technologies+The Chinese University of Hong Kong; Huawei Noah\u2019s Ark Lab; MoE Key Laboratory of High Confidence Software Technologies+The Chinese University of Hong Kong; Harbin Insitute of Technology, Shenzhen, China+Peng Cheng Laboratory, China+Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies",
        "aff_domain": "outlook.com;huawei.com; ; ; ; ; ;hit.edu.cn",
        "email": "outlook.com;huawei.com; ; ; ; ; ;hit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;2;0+1;3+4;3+4;2;3+4;0+5+1",
        "aff_unique_norm": "Harbin Institute of Technology;Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies;Huawei;MoE Key Laboratory of High Confidence Software Technologies;Chinese University of Hong Kong;Pengcheng Laboratory",
        "aff_unique_dep": ";Provincial Key Laboratory of Novel Security Intelligence Technologies;Noah\u2019s Ark Lab;High Confidence Software Technologies;;Peng Cheng Laboratory",
        "aff_unique_url": "http://en.hhit.edu.cn/;;https://www.huawei.com;;https://www.cuhk.edu.hk;",
        "aff_unique_abbr": "HIT;;Huawei;;CUHK;",
        "aff_campus_unique_index": "0;0;2;2;2;0",
        "aff_campus_unique": "Shenzhen;;Hong Kong SAR",
        "aff_country_unique_index": "0+0;0;0+0;0+0;0+0;0;0+0;0+0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.109",
        "title": "Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate on it by mining latent expertise with off-the-shelf reward models. We propose ZOOTER, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. ZOOTER shows computation efficiency in inference as it only introduces minor computation overhead of a routing function compared with reward model ranking methods. We evaluate ZOOTER on a comprehensive benchmark collection with 26 subsets in different domains and tasks. ZOOTER outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.",
        "author": "Keming Lu; Hongyi Yuan; Runji Lin; Junyang Lin; Zheng Yuan; Chang Zhou; Jingren Zhou",
        "authorids": "/k/keming-lu/; /h/hongyi-yuan/; /r/runji-lin/; /j/junyang-lin/; /z/zheng-yuan/; /c/chang-zhou/; /j/jingren-zhou/",
        "bibtex": "@inproceedings{lu-etal-2024-routing,\n    title = \"Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models\",\n    author = \"Lu, Keming  and\n      Yuan, Hongyi  and\n      Lin, Runji  and\n      Lin, Junyang  and\n      Yuan, Zheng  and\n      Zhou, Chang  and\n      Zhou, Jingren\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.109/\",\n    doi = \"10.18653/v1/2024.naacl-long.109\",\n    pages = \"1964--1974\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.109.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.109/",
        "pdf_size": 893777,
        "gs_citation": 83,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2023580638989570264&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Alibaba Inc.; Alibaba Inc.+Alibaba Inc.; Alibaba Inc.+Alibaba Inc.; Alibaba Inc.; Alibaba Inc.; Alibaba Inc.; Alibaba Inc.",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0+0;0+0;0;0;0;0",
        "aff_unique_norm": "Alibaba Group Holding Limited",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.69",
        "title": "S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Model",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The rapid development of Large Language Models (LLMs) has led to great strides in model capabilities like long-context understanding and reasoning.However, as LLMs are able to process longer contexts, it becomes more challenging to evaluate whether they have acquired certain capabilities, since the length of text (e.g., 200K tokens) they can process far exceeds what humans can reliably assess in a reasonable duration.In this paper, we propose using complex synthetic tasks as a proxy evaluation method, and present S3Eval, a Synthetic, Scalable, Systematic evaluation suite for LLMs evaluation.The synthetic nature of S3Eval provides users full control over the dataset, allowing them to systematically probe LLM capabilities by scaling text length and varying task difficulty across diverse scenarios.The strong correlation between S3Eval and real-world benchmarks demonstrates the soundness of using S3Eval for evaluation of LLMs.S3Eval provides a flexible and infinite long-context data generation method. We have generated a comprehensive dataset called S3Eval-Standard, and experimental results have shown that it poses significant challenges for all existing LLMs.",
        "author": "Fangyu Lei; Qian Liu; Yiming Huang; Shizhu He; Jun Zhao; Kang Liu",
        "authorids": "/f/fangyu-lei/; /q/qian-liu/; /y/yiming-huang/; /s/shizhu-he/; /j/jun-zhao/; /k/kang-liu/",
        "bibtex": "@inproceedings{lei-etal-2024-s3eval,\n    title = \"{S}3{E}val: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Model\",\n    author = \"Lei, Fangyu  and\n      Liu, Qian  and\n      Huang, Yiming  and\n      He, Shizhu  and\n      Zhao, Jun  and\n      Liu, Kang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.69/\",\n    doi = \"10.18653/v1/2024.naacl-long.69\",\n    pages = \"1259--1286\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.69.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.69/",
        "pdf_size": 2900712,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8475635121092533593&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://github.com/lfy79001/S3Eval",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.427",
        "title": "SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent advances in named entity recognition (NER) have pushed the boundary of the task to incorporate visual signals, leading to many variants, including multi-modal NER (MNER) or grounded MNER (GMNER). A key challenge to these tasks is that the model should be able to generalize to the entities unseen during the training, and should be able to handle the training samples with noisy annotations.To address this obstacle, we propose SCANNER (Span CANdidate detection and recognition for NER), a model capable of effectively handling all three NER variants.SCANNER is a two-stage structure; we extract entity candidates in the first stage and use it as a query to get knowledge, effectively pulling knowledge from various sources.We can boost our performance by utilizing this entity-centric extracted knowledge to address unseen entities.Furthermore, to tackle the challenges arising from noisy annotations in NER datasets, we introduce a novel self-distillation method, enhancing the robustness and accuracy of our model in processing training data with inherent uncertainties.Our approach demonstrates competitive performance on the NER benchmark and surpasses existing methods on both MNER and GMNER benchmarks.Further analysis shows that the proposed distillation and knowledge utilization methods improve the performance of our model on various benchmarks.",
        "author": "Hyunjong Ok; Taeho Kil; Sukmin Seo; Jaeho Lee",
        "authorids": "/h/hyunjong-ok/; /t/taeho-kil/; /s/sukmin-seo/; /j/jaeho-lee/",
        "bibtex": "@inproceedings{ok-etal-2024-scanner,\n    title = \"{SCANNER}: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities\",\n    author = \"Ok, Hyunjong  and\n      Kil, Taeho  and\n      Seo, Sukmin  and\n      Lee, Jaeho\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.427/\",\n    doi = \"10.18653/v1/2024.naacl-long.427\",\n    pages = \"7725--7737\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.427.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.427/",
        "pdf_size": 3159518,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7709517457360524712&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "POSTECH; NA VER Cloud; POSTECH+NA VER Cloud+HJ AILAB; POSTECH",
        "aff_domain": "gmail.com;navercorp.com; ; ",
        "email": "gmail.com;navercorp.com; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0+1+2;0",
        "aff_unique_norm": "Pohang University of Science and Technology;NAVER Cloud;HJ AILAB",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.postech.ac.kr;https://www.naver.com;",
        "aff_unique_abbr": "POSTECH;NAVER;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pohang;",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "id": "2024.findings-naacl.69",
        "title": "SELF-EXPERTISE: Knowledge-based Instruction Dataset Augmentation for a Legal Expert Language Model",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The advent of instruction-tuned large language models (LLMs) has significantly advanced the field of automatic instruction dataset augmentation. However, the method of generating instructions and outputs from inherent knowledge of LLM can unintentionally produce hallucinations \u2014 instances of generating factually incorrect or misleading information. To overcome this, we propose SELF-EXPERTISE, automatically generating instruction dataset in the legal domain from a seed dataset. SELF-EXPERTISE extracts knowledge from the outputs of the seed dataset, and generates new instructions, inputs, and outputs. In this way, the proposed method reduces hallucination in automatic instruction augmentation. We trained an SELF-EXPERTISE augmented instruction dataset on the LLaMA-2 7B model to construct Korean legal specialized model, called LxPERT. LxPERT has demonstrated performance surpassing GPT-3.5-turbo in both in-domain and out-of-domain datasets. The SELF-EXPERTISE augmentation pipeline is not only applicable to the legal field but is also expected to be extendable to various domains, potentially advancing domain-specialized LLMs.",
        "author": "Minju Kim; Haein Jung; Myoung-Wan Koo",
        "authorids": "/m/minju-kim/; /h/haein-jung/; /m/myoung-wan-koo/",
        "bibtex": "@inproceedings{kim-etal-2024-self,\n    title = \"{SELF}-{EXPERTISE}: Knowledge-based Instruction Dataset Augmentation for a Legal Expert Language Model\",\n    author = \"Kim, Minju  and\n      Jung, Haein  and\n      Koo, Myoung-Wan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.69/\",\n    doi = \"10.18653/v1/2024.findings-naacl.69\",\n    pages = \"1098--1112\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.69.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.69/",
        "pdf_size": 5434388,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:WgmH8bfbb_wJ:scholar.google.com/&scioq=SELF-EXPERTISE:+Knowledge-based+Instruction+Dataset+Augmentation+for+a+Legal+Expert+Language+Model&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Department of Artificial Intelligence, Sogang University, Republic of Korea; Department of Artificial Intelligence, Sogang University, Republic of Korea; Department of Artificial Intelligence, Sogang University, Republic of Korea",
        "aff_domain": "sogang.ac.kr;sogang.ac.kr;sogang.ac.kr",
        "email": "sogang.ac.kr;sogang.ac.kr;sogang.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Sogang University",
        "aff_unique_dep": "Department of Artificial Intelligence",
        "aff_unique_url": "https://www.sogang.ac.kr",
        "aff_unique_abbr": "Sogang",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-long.92",
        "title": "SELF-GUARD: Empower the LLM to Safeguard Itself",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "With the increasing risk posed by jailbreak attacks, recent studies have investigated various methods to improve the safety of large language models (LLMs), mainly falling into two strategies: safety training and safeguards. Safety training involves fine-tuning the LLM with adversarial samples, which activate the LLM\u2019s capabilities against jailbreak. However, it is not always effective in countering new attacks and often leads to potential performance degradation. Safeguards, on the other hand, are methods using additional models to filter harmful content from the LLM\u2019s response. Nevertheless, they can only reduce a limited amount of harmful output and introduce extra computational costs. Given the distinct strengths and weaknesses of both, we combine them to balance out their flaws and propose a more effective method called Self-Guard.Specifically, we train the LLM to review its responses for any harmful content and append a [harmful] or [harmless] tag to the end of the response. In this way, Self-Guard possesses the advantages of safety training, leveraging the powerful capabilities of the LLMs themselves to detect harmfulness. Besides that, it gains flexibility like safeguards, making the safety check target the output side, which makes the system less vulnerable to attack updates. Experimental results indicate that our Self-Guard can effectively defend against jailbreak attacks and will not cause LLMs\u2019 performance degradation.",
        "author": "Zezhong Wang; Fangkai Yang; Lu Wang; Pu Zhao; Hongru Wang; Liang Chen; Qingwei Lin; Kam-Fai Wong",
        "authorids": "/z/zezhong-wang/; /f/fangkai-yang/; /l/lu-wang/; /p/pu-zhao/; /h/hongru-wang/; /l/liang-chen/; /q/qingwei-lin/; /k/kam-fai-wong/",
        "bibtex": "@inproceedings{wang-etal-2024-self,\n    title = \"{SELF}-{GUARD}: Empower the {LLM} to Safeguard Itself\",\n    author = \"Wang, Zezhong  and\n      Yang, Fangkai  and\n      Wang, Lu  and\n      Zhao, Pu  and\n      Wang, Hongru  and\n      Chen, Liang  and\n      Lin, Qingwei  and\n      Wong, Kam-Fai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.92/\",\n    doi = \"10.18653/v1/2024.naacl-long.92\",\n    pages = \"1648--1668\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.92.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.92/",
        "pdf_size": 1116099,
        "gs_citation": 40,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=663680854269349013&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "The Chinese University of Hong Kong; Microsoft; Microsoft; Microsoft; The Chinese University of Hong Kong; The Chinese University of Hong Kong; Microsoft; The Chinese University of Hong Kong",
        "aff_domain": "se.cuhk.edu.hk; ; ; ; ; ; ; ",
        "email": "se.cuhk.edu.hk; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;0;0;1;0",
        "aff_unique_norm": "Chinese University of Hong Kong;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.microsoft.com",
        "aff_unique_abbr": "CUHK;Microsoft",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;1;1;1;0;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.74",
        "title": "SEMQA: Semi-Extractive Multi-Source Question Answering",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recently proposed long-form question answering (QA) systems, supported by large language models (LLMs), have shown promising capabilities. Yet, attributing and verifying their generated abstractive answers can be difficult, and automatically evaluating their accuracy remains an ongoing challenge.In this work, we introduce a new QA task for answering multi-answer questions by summarizing multiple diverse sources in a semi-extractive fashion. Specifically, Semi-extractive Multi-source QA (SEMQA) requires models to output a comprehensive answer, while mixing factual quoted spans\u2014copied verbatim from given input sources\u2014and non-factual free-text connectors that glue these spans together into a single cohesive passage. This setting bridges the gap between the outputs of well-grounded but constrained extractive QA systems and more fluent but harder to attribute fully abstractive answers. Particularly, it enables a new mode for language models that leverages their advanced language generation capabilities, while also producing fine in-line attributions by-design that are easy to verify, interpret, and evaluate. To study this task, we create the first dataset of this kind, QuoteSum, with human-written semi-extractive answers to natural and generated questions, and define text-based evaluation metrics. Experimenting with several LLMs in various settings, we find this task to be surprisingly challenging, demonstrating the importance of QuoteSum for developing and studying such consolidation capabilities.",
        "author": "Tal Schuster; Adam Lelkes; Haitian Sun; Jai Gupta; Jonathan Berant; William Cohen; Donald Metzler",
        "authorids": "/t/tal-schuster/; /a/adam-lelkes/; /h/haitian-sun/; /j/jai-gupta/; /j/jonathan-berant/; /w/william-cohen/; /d/donald-metzler/",
        "bibtex": "@inproceedings{schuster-etal-2024-semqa,\n    title = \"{SEMQA}: Semi-Extractive Multi-Source Question Answering\",\n    author = \"Schuster, Tal  and\n      Lelkes, Adam  and\n      Sun, Haitian  and\n      Gupta, Jai  and\n      Berant, Jonathan  and\n      Cohen, William  and\n      Metzler, Donald\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.74/\",\n    doi = \"10.18653/v1/2024.naacl-long.74\",\n    pages = \"1363--1381\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.74.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.74/",
        "pdf_size": 1874461,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1475955715625794974&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2024.findings-naacl.287",
        "title": "SGSH: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Knowledge base question generation (KBQG) aims to generate natural language questions from a set of triplet facts extracted from KB. Existing methods have significantly boosted the performance of KBQG via pre-trained language models (PLMs) thanks to the richly endowed semantic knowledge. With the advance of pre-training techniques, large language models (LLMs) (e.g., GPT-3.5) undoubtedly possess much more semantic knowledge. Therefore, how to effectively organize and exploit the abundant knowledge for KBQG becomes the focus of our study. In this work, we propose SGSH \u2014 a simple and effective framework to Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework incorporates \u201cskeleton heuristics\u201d, which provides more fine-grained guidance associated with each input to stimulate LLMs to generate optimal questions, encompassing essential elements like the question phrase and the auxiliary verb.More specifically, we devise an automatic data construction strategy leveraging ChatGPT to construct a skeleton training dataset, based on which we employ a soft prompting approach to train a BART model dedicated to generating the skeleton associated with each input.Subsequently, skeleton heuristics are encoded into the prompt to incentivize GPT-3.5 to generate desired questions. Extensive experiments demonstrate that SGSH derives the new state-of-the-art performance on the KBQG tasks.",
        "author": "Shasha Guo; Lizi Liao; Jing Zhang; Yanling Wang; Cuiping Li; Hong Chen",
        "authorids": "/s/shasha-guo/; /l/lizi-liao/; /j/jing-zhang/; /y/yanling-wang/; /c/cuiping-li/; /h/hong-chen/",
        "bibtex": "@inproceedings{guo-etal-2024-sgsh,\n    title = \"{SGSH}: Stimulate Large Language Models with Skeleton Heuristics for Knowledge Base Question Generation\",\n    author = \"Guo, Shasha  and\n      Liao, Lizi  and\n      Zhang, Jing  and\n      Wang, Yanling  and\n      Li, Cuiping  and\n      Chen, Hong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.287/\",\n    doi = \"10.18653/v1/2024.findings-naacl.287\",\n    pages = \"4613--4625\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.287.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.287/",
        "pdf_size": 2043580,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17666784892030502634&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "School of Information, Renmin University of China+Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education; Singapore Management University; School of Information, Renmin University of China+Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education; Zhongguancun Laboratory; School of Information, Renmin University of China+Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education; School of Information, Renmin University of China+Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education",
        "aff_domain": "ruc.edu.cn;smu.edu.sg;ruc.edu.cn;zgclab.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "email": "ruc.edu.cn;smu.edu.sg;ruc.edu.cn;zgclab.edu.cn;ruc.edu.cn;ruc.edu.cn",
        "github": "https://github.com/RUCKBReasoning/SGSH",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;2;0+1;3;0+1;0+1",
        "aff_unique_norm": "Renmin University of China;Ministry of Education;Singapore Management University;Zhongguancun Laboratory",
        "aff_unique_dep": "School of Information;Key Laboratory of Data Engineering and Knowledge Engineering;;",
        "aff_unique_url": "http://www.ruc.edu.cn;;https://www.smu.edu.sg;",
        "aff_unique_abbr": "RUC;;SMU;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1;0+0;0;0+0;0+0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2024.naacl-short.13",
        "title": "SKICSE: Sentence Knowable Information Prompted by LLMs Improves Contrastive Sentence Embeddings",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Contrastive learning, which utilizes positive pairs and in-batch negatives to optimize the loss objective, has been proven to be an effective method for learning sentence embeddings. However, we argue that the previous methods of constructing positive pairs only through dropout perturbation or entailment relation are limited. Since there is more sentence knowable information (SKI) to be mined, such as sentence external knowledge, semantic analysis, and grammatical description. In this work, we first hand-craft a simple and effective prompt template that is able to obtain the knowable information of input sentences from LLMs (e.g., LLaMA). Then we combine the original sentence and its knowable information to form a positive pair for contrastive learning. We evaluate our method on standard semantic textual similarity (STS) tasks. Experimental results show that our unsupervised and supervised models using BERTbase achieve an average of 78.65% and 82.45% Spearman\u2019s correlation respectively, a 2.40% and 0.88% improvement compared to SimCSE. Our model outperforms the previous state-of-the-art model PromptBERT in both unsupervised and supervised settings and specifically yields a new state-of-the-art performance in supervised setting.",
        "author": "Fangwei Ou; Jinan Xu",
        "authorids": "/f/fangwei-ou/; /j/jinan-xu/",
        "bibtex": "@inproceedings{ou-xu-2024-skicse,\n    title = \"{SKICSE}: Sentence Knowable Information Prompted by {LLM}s Improves Contrastive Sentence Embeddings\",\n    author = \"Ou, Fangwei  and\n      Xu, Jinan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.13/\",\n    doi = \"10.18653/v1/2024.naacl-short.13\",\n    pages = \"141--146\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.13.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.13/",
        "pdf_size": 905023,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1815168848274166233&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2024.naacl-short.18",
        "title": "SLIDE: Reference-free Evaluation for Machine Translation using a Sliding Document Window",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Reference-based metrics that operate at the sentence-level typically outperform quality estimation metrics, which have access only to the source and system output.This is unsurprising, since references resolve ambiguities that may be present in the source.In this paper, we investigate whether additional source context can effectively substitute for a reference.We present a metric named SLIDE (SLIding Document Evaluator), which operates on blocks of sentences. SLIDE leverages a moving window that slides over each document in the test set, feeding each chunk of sentences into an unmodified, off-the-shelf quality estimation model.We find that SLIDE obtains significantly higher pairwise system accuracy than its sentence-level baseline, in some cases even eliminating the gap with reference-base metrics.This suggests that source context may provide the same information as a human reference in disambiguating source ambiguities. This finding is especially pertinent for reference-free document-level evaluation, wherein SLIDE could provide higher-quality pairwise system assessments while only requiring document boundary annotations.",
        "author": "Vikas Raunak; Tom Kocmi; Matt Post",
        "authorids": "/v/vikas-raunak/; /t/tom-kocmi/; /m/matt-post/",
        "bibtex": "@inproceedings{raunak-etal-2024-slide,\n    title = \"{SLIDE}: Reference-free Evaluation for Machine Translation using a Sliding Document Window\",\n    author = \"Raunak, Vikas  and\n      Kocmi, Tom  and\n      Post, Matt\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.18/\",\n    doi = \"10.18653/v1/2024.naacl-short.18\",\n    pages = \"205--211\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.18.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.18/",
        "pdf_size": 259066,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7877586721404279086&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.findings-naacl.63",
        "title": "SLiM: Speculative Decoding with Hypothesis Reduction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Speculative decoding has emerged as a prominent alternative to autoregressive decoding for expediting inference in large language models (LLMs). However, prevailing assumptions often focus solely on latency reduction, neglecting the computational expenses. In this paper, we present Speculate Less, validate More (SLiM), a speculative decoding enhancement to reduce the speculation set while validating more effective tokens. SLiM is designed to mitigate LLMs\u2019 computation costs associated with the token verification by introducing hypothesis reduction based on a fast posterior estimation. It consistently surpasses counterparts lacking cost reduction across a spectrum from CPU to GPU. Our evaluation with diverse conversational datasets shows that SLiM can achieve a substantial 70% reduction in FLOPs while generating more effective predictions on top of prior arts.",
        "author": "Chi-Heng Lin; Shikhar Tuli; James Smith; Yen-Chang Hsu; Yilin Shen; Hongxia Jin",
        "authorids": "/c/chi-heng-lin/; /s/shikhar-tuli/; /j/james-smith/; /y/yen-chang-hsu/; /y/yilin-shen/; /h/hongxia-jin/",
        "bibtex": "@inproceedings{lin-etal-2024-slim,\n    title = \"{SL}i{M}: Speculative Decoding with Hypothesis Reduction\",\n    author = \"Lin, Chi-Heng  and\n      Tuli, Shikhar  and\n      Smith, James  and\n      Hsu, Yen-Chang  and\n      Shen, Yilin  and\n      Jin, Hongxia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.63/\",\n    doi = \"10.18653/v1/2024.findings-naacl.63\",\n    pages = \"1005--1017\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.63.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.63/",
        "pdf_size": 1571226,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8590812834243170986&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Samsung Research America; Samsung Research America; Samsung Research America; Samsung Research America; Samsung Research America; Samsung Research America",
        "aff_domain": "samsung.com;samsung.com;samsung.com;samsung.com;samsung.com;samsung.com",
        "email": "samsung.com;samsung.com;samsung.com;samsung.com;samsung.com;samsung.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Samsung",
        "aff_unique_dep": "Samsung Research America",
        "aff_unique_url": "https://www.samsung.com/us/careers/research/",
        "aff_unique_abbr": "SRA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.73",
        "title": "SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Despite the recent advances in artificial intelligence, building social intelligence remains a challenge.Among social signals, laughter is one of the distinctive expressions that occurs during social interactions between humans.In this work, we tackle a new challenge for machines to understand the rationale behind laughter in video, Video Laugh Reasoning.We introduce this new task to explain why people laugh in a particular video and a dataset for this task.Our proposed dataset, SMILE, comprises video clips and language descriptions of why people laugh. We propose a baseline by leveraging the reasoning capacity of large language models (LLMs) with textual video representation. Experiments show that our baseline can generate plausible explanations for laughter. We further investigate the scalability of our baseline by probing other video understanding tasks and in-the-wild videos. We release our dataset, code, and model checkpoints on https://github.com/postech-ami/SMILE-Dataset.",
        "author": "Lee Hyun; Kim Sung-Bin; Seungju Han; Youngjae Yu; Tae-Hyun Oh",
        "authorids": "/l/lee-hyun/; /k/kim-sung-bin/; /s/seungju-han/; /y/youngjae-yu/; /t/tae-hyun-oh/",
        "bibtex": "@inproceedings{hyun-etal-2024-smile,\n    title = \"{SMILE}: Multimodal Dataset for Understanding Laughter in Video with Language Models\",\n    author = \"Hyun, Lee  and\n      Sung-Bin, Kim  and\n      Han, Seungju  and\n      Yu, Youngjae  and\n      Oh, Tae-Hyun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.73/\",\n    doi = \"10.18653/v1/2024.findings-naacl.73\",\n    pages = \"1149--1167\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.73.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.73/",
        "pdf_size": 3586037,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11682780604511249871&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Electrical Engineering and Grad. School of Artificial Intelligence, POSTECH+Samsung Advanced Institute of Technology; Dept. of Electrical Engineering and Grad. School of Artificial Intelligence, POSTECH; Seoul National University; Yonsei University; Dept. of Electrical Engineering and Grad. School of Artificial Intelligence, POSTECH+Institute for Convergence Research and Education in Advanced Technology, Yonsei University",
        "aff_domain": "postech.ac.kr;postech.ac.kr; ; ;postech.ac.kr",
        "email": "postech.ac.kr;postech.ac.kr; ; ;postech.ac.kr",
        "github": "https://github.com/postech-ami/SMILE-Dataset",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0;2;3;0+3",
        "aff_unique_norm": "POSTECH;Samsung;Seoul National University;Yonsei University",
        "aff_unique_dep": "Dept. of Electrical Engineering;Samsung Advanced Institute of Technology;;",
        "aff_unique_url": "https://www.postech.ac.kr;https://www.sait.samsung.com;https://www.snu.ac.kr;https://www.yonsei.ac.kr",
        "aff_unique_abbr": "POSTECH;SAIT;SNU;Yonsei",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0+0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-industry.3",
        "title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. Inspired by recent efforts to efficiently up-scale LLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which encompasses depthwise scaling and continued pretraining. In contrast to other LLM up-scaling methods that use mixture-of-experts, DUS does not require complex changes to train and inference efficiently. We show experimentally that DUS is simple yet effective in scaling up high-performance LLMs from small ones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, a variant fine-tuned for instruction-following capabilities, surpassing Mixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0 license, promoting broad access and application in the LLM field.",
        "author": "Sanghoon Kim; Dahyun Kim; Chanjun Park; Wonsung Lee; Wonho Song; Yunsu Kim; Hyeonwoo Kim; Yungi Kim; Hyeonju Lee; Jihoo Kim; Changbae Ahn; Seonghoon Yang; Sukyung Lee; Hyunbyung Park; Gyoungjin Gim; Mikyoung Cha; Hwalsuk Lee; Sunghun Kim",
        "authorids": "/s/sanghoon-kim/; /d/dahyun-kim/; /c/chanjun-park/; /w/wonsung-lee/; /w/wonho-song/; /y/yunsu-kim/; /h/hyeonwoo-kim/; /y/yungi-kim/; /h/hyeonju-lee/; /j/jihoo-kim/; /c/changbae-ahn/; /s/seonghoon-yang/; /s/sukyung-lee/; /h/hyunbyung-park/; /g/gyoungjin-gim/; /m/mikyoung-cha/; /h/hwalsuk-lee/; /s/sunghun-kim/",
        "bibtex": "@inproceedings{kim-etal-2024-solar,\n    title = \"{SOLAR} 10.7{B}: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling\",\n    author = \"Kim, Sanghoon  and\n      Kim, Dahyun  and\n      Park, Chanjun  and\n      Lee, Wonsung  and\n      Song, Wonho  and\n      Kim, Yunsu  and\n      Kim, Hyeonwoo  and\n      Kim, Yungi  and\n      Lee, Hyeonju  and\n      Kim, Jihoo  and\n      Ahn, Changbae  and\n      Yang, Seonghoon  and\n      Lee, Sukyung  and\n      Park, Hyunbyung  and\n      Gim, Gyoungjin  and\n      Cha, Mikyoung  and\n      Lee, Hwalsuk  and\n      Kim, Sunghun\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.3/\",\n    doi = \"10.18653/v1/2024.naacl-industry.3\",\n    pages = \"23--35\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.3.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.3/",
        "pdf_size": 360211,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13905533370611173961&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Upstage AI, South Korea; Upstage AI, South Korea; Upstage AI, South Korea; Upstage AI, South Korea; Upstage AI, South Korea; Upstage AI, South Korea; Upstage AI, South Korea; ; ; ; ; ; ; ; ; ; Upstage AI, South Korea; Upstage AI, South Korea",
        "aff_domain": "upstage.ai;upstage.ai;upstage.ai;upstage.ai; ; ; ; ; ; ; ; ; ; ; ; ;upstage.ai;upstage.ai",
        "email": "upstage.ai;upstage.ai;upstage.ai;upstage.ai; ; ; ; ; ; ; ; ; ; ; ; ;upstage.ai;upstage.ai",
        "github": "",
        "project": "https://huggingface.co/upstage/SOLAR-10.7B-v1.0",
        "author_num": 18,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Upstage AI",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-long.453",
        "title": "SQATIN: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue NLU",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Task-oriented dialogue (TOD) systems help users execute well-defined tasks across a variety of domains (e.g., flight booking or food ordering), with their Natural Language Understanding (NLU) components being dedicated to the analysis of user utterances, predicting users\u2019 intents (Intent Detection, ID) and extracting values for informational slots (Value Extraction, VE). In most domains, labelled NLU data is scarce, making sample-efficient learning \u2013 enabled with effective transfer paradigms \u2013 paramount. In this work, we introduce SQATIN, a new framework for dialog NLU based on (i) instruction tuning and (ii) question-answering-based formulation of ID and VE tasks. According to the evaluation on established NLU benchmarks, SQATIN sets the new state of the art in dialogue NLU, substantially surpassing the performance of current models based on standard fine-tuning objectives in both in-domain training and cross-domain transfer, and it also surpasses off-the-shelf large language models for the same task, both in terms of performance and inference efficiency. Furthermore, SQATIN yields particularly large performance gains in cross-domain transfer, owing to the fact that our QA-based instruction tuning leverages similarities between natural language descriptions of classes (i.e., slots and intents) across domains.",
        "author": "Evgeniia Razumovskaia; Goran Glava\u0161; Anna Korhonen; Ivan Vuli\u0107",
        "authorids": "/e/evgeniia-razumovskaia/; /g/goran-glavas/; /a/anna-korhonen/; /i/ivan-vulic/",
        "bibtex": "@inproceedings{razumovskaia-etal-2024-sqatin,\n    title = \"{SQATIN}: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue {NLU}\",\n    author = \"Razumovskaia, Evgeniia  and\n      Glava{\\v{s}}, Goran  and\n      Korhonen, Anna  and\n      Vuli{\\'c}, Ivan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.453/\",\n    doi = \"10.18653/v1/2024.naacl-long.453\",\n    pages = \"8195--8211\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.453.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.453/",
        "pdf_size": 616014,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7908945200573031877&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-long.422",
        "title": "Safer-Instruct: Aligning Language Models with Automated Preference Data",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while maintaining a competitive edge in downstream tasks. Importantly, our Safer-Instruct framework is versatile and can be applied to generate preference data across various domains, extending its utility beyond safety preferences. It addresses the challenges in preference data acquisition and advances the development of more capable and responsible AI systems. For dataset and code implementation, see https://github.com/uscnlp-lime/safer-instruct/.",
        "author": "Taiwei Shi; Kai Chen; Jieyu Zhao",
        "authorids": "/t/taiwei-shi/; /k/kai-chen/; /j/jieyu-zhao/",
        "bibtex": "@inproceedings{shi-etal-2024-safer,\n    title = \"Safer-Instruct: Aligning Language Models with Automated Preference Data\",\n    author = \"Shi, Taiwei  and\n      Chen, Kai  and\n      Zhao, Jieyu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.422/\",\n    doi = \"10.18653/v1/2024.naacl-long.422\",\n    pages = \"7636--7651\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.422.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.422/",
        "pdf_size": 390072,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17753382869956264135&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Southern California; University of Southern California; University of Southern California",
        "aff_domain": "usc.edu;usc.edu;usc.edu",
        "email": "usc.edu;usc.edu;usc.edu",
        "github": "https://github.com/uscnlp-lime/safer-instruct/",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Southern California",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.usc.edu",
        "aff_unique_abbr": "USC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.24",
        "title": "Scaling Up Authorship Attribution",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "We describe our system for authorship attribution in the IARPA HIATUS program. We describe the model and compute infrastructure developed to satisfy the set of technical constraints imposed by IARPA, including runtime limits as well as other constraints related to the ultimate use case. One use-case constraint concerns the explainability of the features used in the system. For this reason, we integrate features from frame semantic parsing, as they are both interpretable and difficult for adversaries to evade. One trade-off with using such features, however, is that more sophisticated feature representations require more complicated architectures, which limit usefulness in time-sensitive and constrained compute environments. We propose an approach to increase the efficiency of frame semantic parsing through an analysis of parallelization and beam search sizes. Our approach results in a system that is approximately 8.37x faster than the base system with a minimal effect on accuracy.",
        "author": "Jacob Striebel; Abishek Edikala; Ethan Irby; Alex Rosenfeld; J. Gage; Daniel Dakota; Sandra K\u00fcbler",
        "authorids": "/j/jacob-striebel/; /a/abishek-edikala/; /e/ethan-irby/; /a/alex-rosenfeld/; /j/j-gage/; /d/daniel-dakota/; /s/sandra-kubler/",
        "bibtex": "@inproceedings{striebel-etal-2024-scaling,\n    title = \"Scaling Up Authorship Attribution\",\n    author = {Striebel, Jacob  and\n      Edikala, Abishek  and\n      Irby, Ethan  and\n      Rosenfeld, Alex  and\n      Gage, J.  and\n      Dakota, Daniel  and\n      K{\\\"u}bler, Sandra},\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.24/\",\n    doi = \"10.18653/v1/2024.naacl-industry.24\",\n    pages = \"295--302\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.24.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.24/",
        "pdf_size": 272206,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8616786946381122631&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Indiana University + Leidos Inc.; Leidos Inc.; Leidos Inc.; Leidos Inc.; Leidos Inc.; Indiana University + Leidos Inc.; Indiana University",
        "aff_domain": "indiana.edu;leidos.com;leidos.com;leidos.com;leidos.com;leidos.com;indiana.edu",
        "email": "indiana.edu;leidos.com;leidos.com;leidos.com;leidos.com;leidos.com;indiana.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;1;1;1;0+1;0",
        "aff_unique_norm": "Indiana University;Leidos Inc.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.indiana.edu;https://www.leidos.com",
        "aff_unique_abbr": "IU;Leidos",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.357",
        "title": "ScriptMix: Mixing Scripts for Low-resource Language Parsing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Despite the success of multilingual pretrained language models (mPLMs) for tasks such as dependency parsing (DEP) or part-of-speech (POS) tagging, their coverage of 100s of languages is still limited, as most of the 6500+ languages remains \u201cunseen\u201d. To adapt mPLMs for including such unseen langs, existing work has considered transliteration and vocabulary augmentation. Meanwhile, the consideration of combining the two has been surprisingly lacking. To understand why, we identify both complementary strengths of the two, and the hurdles to realizing it. Based on this observation, we propose ScriptMix, combining two strengths, and overcoming the hurdle.Specifically, ScriptMix a) is trained with dual-script corpus to combine strengths, but b) with separate modules to avoid gradient conflict. In combining modules properly, we also point out the limitation of the conventional method AdapterFusion, and propose AdapterFusion+ to overcome it. We empirically show ScriptMix is effective\u2013 ScriptMix improves the POS accuracy by up to 14%, and improves the DEP LAS score by up to 5.6%. Our code is publicly available.",
        "author": "Jaeseong Lee; Dohyeon Lee; Seung-won Hwang",
        "authorids": "/j/jaeseong-lee/; /d/dohyeon-lee/; /s/seung-won-hwang/",
        "bibtex": "@inproceedings{lee-etal-2024-scriptmix,\n    title = \"{S}cript{M}ix: Mixing Scripts for Low-resource Language Parsing\",\n    author = \"Lee, Jaeseong  and\n      Lee, Dohyeon  and\n      Hwang, Seung-won\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.357/\",\n    doi = \"10.18653/v1/2024.naacl-long.357\",\n    pages = \"6430--6444\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.357.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.357/",
        "pdf_size": 925818,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:wF7BZQYE1zEJ:scholar.google.com/&scioq=ScriptMix:+Mixing+Scripts+for+Low-resource+Language+Parsing&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Computer Science and Engineering, Seoul National University; Computer Science and Engineering, Seoul National University; Computer Science and Engineering, Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "email": "snu.ac.kr;snu.ac.kr;snu.ac.kr",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Seoul National University",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.snu.ac.kr",
        "aff_unique_abbr": "SNU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.naacl-long.22",
        "title": "SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Many models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingually-trained models have not attained \u201cbalanced multilingual\u201d capabilities. Our endeavors underscore the need for more generalizable semantic representations and enhanced multilingual contextualization. SeaEval can serve as a launchpad for more thorough investigations and evaluations for multilingual and multicultural scenarios.",
        "author": "Bin Wang; Zhengyuan Liu; Xin Huang; Fangkai Jiao; Yang Ding; AiTi Aw; Nancy Chen",
        "authorids": "/b/bin-wang/; /z/zhengyuan-liu/; /x/xin-huang/; /f/fangkai-jiao/; /y/yang-ding/; /a/aiti-aw/; /n/nancy-chen/",
        "bibtex": "@inproceedings{wang-etal-2024-seaeval,\n    title = \"{S}ea{E}val for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning\",\n    author = \"Wang, Bin  and\n      Liu, Zhengyuan  and\n      Huang, Xin  and\n      Jiao, Fangkai  and\n      Ding, Yang  and\n      Aw, AiTi  and\n      Chen, Nancy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.22/\",\n    doi = \"10.18653/v1/2024.naacl-long.22\",\n    pages = \"370--390\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.22.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.22/",
        "pdf_size": 2886581,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7950696832969713405&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Institute for Infocomm Research (I2R), A*STAR, Singapore; Institute for Infocomm Research (I2R), A*STAR, Singapore; Institute for Infocomm Research (I2R), A*STAR, Singapore; Nanyang Technological University, Singapore; Institute for Infocomm Research (I2R), A*STAR, Singapore; Institute for Infocomm Research (I2R), A*STAR, Singapore; Institute for Infocomm Research (I2R), A*STAR, Singapore+Centre for Frontier AI Research (CFAR), A*STAR, Singapore",
        "aff_domain": "i2r.a-star.edu.sg;i2r.a-star.edu.sg; ; ; ; ;i2r.a-star.edu.sg",
        "email": "i2r.a-star.edu.sg;i2r.a-star.edu.sg; ; ; ; ;i2r.a-star.edu.sg",
        "github": "https://github.com/SeaEval/SeaEval",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;0;0;0+2",
        "aff_unique_norm": "Institute for Infocomm Research;Nanyang Technological University;A*STAR",
        "aff_unique_dep": ";;Centre for Frontier AI Research (CFAR)",
        "aff_unique_url": "https://www.i2r.a-star.edu.sg;https://www.ntu.edu.sg;https://www.a-star.edu.sg",
        "aff_unique_abbr": "I2R;NTU;A*STAR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2024.naacl-industry.39",
        "title": "Search Query Refinement for Japanese Named Entity Recognition in E-commerce Domain",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "In the E-Commerce domain, search query refinement reformulates malformed queries into canonicalized forms by preprocessing operations such as \u201cterm splitting\u201d and \u201cterm merging\u201d. Unfortunately, most relevant research is rather limited to English. In particular, there is a severe lack of study on search query refinement for the Japanese language. Furthermore, no attempt has ever been made to apply refinement methods to data improvement for downstream NLP tasks in real-world scenarios.This paper presents a novel query refinement approach for the Japanese language. Experimental results show that our method achieves significant improvement by 3.5 points through comparison with BERT-CRF as a baseline. Further experiments are also conducted to measure beneficial impact of query refinement on named entity recognition (NER) as the downstream task. Evaluations indicate that the proposed query refinement method contributes to better data quality, leading to performance boost on E-Commerce specific NER tasks by 11.7 points, compared to search query data preprocessed by MeCab, a very popularly adopted Japanese tokenizer.",
        "author": "Yuki Nakayama; Ryutaro Tatsushima; Erick Mendieta; Koji Murakami; Keiji Shinzato",
        "authorids": "/y/yuki-nakayama/; /r/ryutaro-tatsushima/; /e/erick-mendieta/; /k/koji-murakami/; /k/keiji-shinzato/",
        "bibtex": "@inproceedings{nakayama-etal-2024-search,\n    title = \"Search Query Refinement for {J}apanese Named Entity Recognition in {E}-commerce Domain\",\n    author = \"Nakayama, Yuki  and\n      Tatsushima, Ryutaro  and\n      Mendieta, Erick  and\n      Murakami, Koji  and\n      Shinzato, Keiji\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.39/\",\n    doi = \"10.18653/v1/2024.naacl-industry.39\",\n    pages = \"447--452\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.39.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.39/",
        "pdf_size": 307046,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_qq3cbRDJDYJ:scholar.google.com/&scioq=Search+Query+Refinement+for+Japanese+Named+Entity+Recognition+in+E-commerce+Domain&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Rakuten Institute of Technology, Rakuten Group, Inc.; Rakuten Institute of Technology, Rakuten Group, Inc.; Rakuten Institute of Technology, Rakuten Group, Inc.; Rakuten Institute of Technology, Rakuten Group, Inc.; Rakuten Institute of Technology, Rakuten Group, Inc.",
        "aff_domain": "rakuten.com;rakuten.com;rakuten.com;rakuten.com;rakuten.com",
        "email": "rakuten.com;rakuten.com;rakuten.com;rakuten.com;rakuten.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Rakuten Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://rit.rakuten.com",
        "aff_unique_abbr": "RIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2024.findings-naacl.256",
        "title": "Secure Your Model: An Effective Key Prompt Protection Mechanism for Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) have notably revolutionized many domains within natural language processing due to their exceptional performance. Their security has become increasingly vital. This study is centered on protecting LLMs against unauthorized access and potential theft. We propose a simple yet effective protective measure wherein a unique key prompt is embedded within the LLM. This mechanism enables the model to respond only when presented with the correct key prompt; otherwise, LLMs will refuse to react to any input instructions. This key prompt protection offers a robust solution to prevent the unauthorized use of LLMs, as the model becomes unusable without the correct key. We evaluated the proposed protection on multiple LLMs and NLP tasks. Results demonstrate that our method can successfully protect the LLM without significantly impacting the model\u2019s original function. Moreover, we demonstrate potential attacks that attempt to bypass the protection mechanism will adversely affect the model\u2019s performance, further emphasizing the effectiveness of the proposed protection method.",
        "author": "Ruixiang Tang; Yu-Neng Chuang; Xuanting Cai; Mengnan Du; Xia Hu",
        "authorids": "/r/ruixiang-tang/; /y/yu-neng-chuang/; /x/xuanting-cai/; /m/mengnan-du/; /x/xia-hu/",
        "bibtex": "@inproceedings{tang-etal-2024-secure,\n    title = \"Secure Your Model: An Effective Key Prompt Protection Mechanism for Large Language Models\",\n    author = \"Tang, Ruixiang  and\n      Chuang, Yu-Neng  and\n      Cai, Xuanting  and\n      Du, Mengnan  and\n      Hu, Xia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.256/\",\n    doi = \"10.18653/v1/2024.findings-naacl.256\",\n    pages = \"4061--4073\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.256.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.256/",
        "pdf_size": 806242,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15645224507164881334&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Rice University; Rice University; Meta Platforms, Inc.; New Jersey Institute of Technology; Rice University",
        "aff_domain": "rice.edu;rice.edu;fb.com;njit.edu;rice.edu",
        "email": "rice.edu;rice.edu;fb.com;njit.edu;rice.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Rice University;Meta;New Jersey Institute of Technology",
        "aff_unique_dep": ";Meta Platforms, Inc.;",
        "aff_unique_url": "https://www.rice.edu;https://www.meta.com;https://www.njit.edu",
        "aff_unique_abbr": "Rice;Meta;NJIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.218",
        "title": "Select and Summarize: Scene Saliency for Movie Script Summarization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Abstractive summarization for long-form narrative texts such as movie scripts is challenging due to the computational and memory constraints of current language models. A movie script typically comprises a large number of scenes; however, only a fraction of these scenes are salient, i.e., important for understanding the overall narrative. The salience of a scene can be operationalized by considering it as salient if it is mentioned in the summary. Automatically identifying salient scenes is difficult due to the lack of suitable datasets. In this work, we introduce a scene saliency dataset that consists of human-annotated salient scenes for 100 movies. We propose a two-stage abstractive summarization approach which first identifies the salient scenes in script and then generates a summary using only those scenes. Using QA-based evaluation, we show that our model outperforms previous state-of-the-art summarization methods and reflects the information content of a movie more accurately than a model that takes the whole movie script as input.",
        "author": "Rohit Saxena; Frank Keller",
        "authorids": "/r/rohit-saxena/; /f/frank-keller/",
        "bibtex": "@inproceedings{saxena-keller-2024-select,\n    title = \"Select and Summarize: Scene Saliency for Movie Script Summarization\",\n    author = \"Saxena, Rohit  and\n      Keller, Frank\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.218/\",\n    doi = \"10.18653/v1/2024.findings-naacl.218\",\n    pages = \"3439--3455\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.218.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.218/",
        "pdf_size": 336034,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15372883499514707569&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Institute for Language, Cognition and Computation, School of Informatics, University of Edinburg; Institute for Language, Cognition and Computation, School of Informatics, University of Edinburg",
        "aff_domain": "ed.ac.uk;inf.ed.ac.uk",
        "email": "ed.ac.uk;inf.ed.ac.uk",
        "github": "https://github.com/saxenarohit/select_summ",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "School of Informatics",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edinburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-short.29",
        "title": "Selective Perception: Learning Concise State Descriptions for Language Model Actors",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "The latest large language models (LMs) support increasingly longer contexts. While this trend permits using substantial amounts of text with SOTA LMs, requiring these large LMs to process potentially redundant or irrelevant data needlessly increases inference time and cost. To remedy this problem, we propose BLINDER, a method that leverages a small finetuned LM to sample the minimal set of input features that maximizes the performance of a downstream LM. BLINDER trains an LM with a value head to estimate the likelihood of optimal outputs from a downstream LM given an input. We evaluate BLINDER on embodied decision making tasks with notoriously verbose state descriptions: NetHack and robot planning. BLINDER reduces the length of LM actor input by 87% and 99% while improving task success rates by 158% and 54% on NetHack and robot planning respectively which represents substantial inference cost savings while actually increasing performance.",
        "author": "Kolby Nottingham; Yasaman Razeghi; Kyungmin Kim; Jb Lanier; Pierre Baldi; Roy Fox; Sameer Singh",
        "authorids": "/k/kolby-nottingham/; /y/yasaman-razeghi/; /k/kyungmin-kim/; /j/jb-lanier/; /p/pierre-baldi/; /r/roy-fox/; /s/sameer-singh/",
        "bibtex": "@inproceedings{nottingham-etal-2024-selective,\n    title = \"Selective Perception: Learning Concise State Descriptions for Language Model Actors\",\n    author = \"Nottingham, Kolby  and\n      Razeghi, Yasaman  and\n      Kim, Kyungmin  and\n      Lanier, Jb  and\n      Baldi, Pierre  and\n      Fox, Roy  and\n      Singh, Sameer\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.29/\",\n    doi = \"10.18653/v1/2024.naacl-short.29\",\n    pages = \"327--341\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.29.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.29/",
        "pdf_size": 8902554,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11995452385653260221&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of California Irvine; University of California Irvine; University of California Irvine; University of California Irvine; University of California Irvine; University of California Irvine; University of California Irvine",
        "aff_domain": "uci.edu;uci.edu;uci.edu;uci.edu;uci.edu;uci.edu;uci.edu",
        "email": "uci.edu;uci.edu;uci.edu;uci.edu;uci.edu;uci.edu;uci.edu",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.162",
        "title": "Self-Adaptive Sampling for Accurate Video Question Answering on Image Text Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Image\u2013text models (ITMs) is the prevalent architecture to solve video question\u2013answering tasks, which requires only a few input frames to save huge computational cost compared to video\u2013language models.However, we find existent ITM video question\u2013answering solutions either 1) adopt simplistic and unintentional sampling strategies, which may miss key frames to offer the answer clues; or 2) sample a large number of frames into divided groups, which the computational sources can not accommodate. In this work, we aim at an efficient sampling method towards the few-frame situations.We first summarize a family of prior sampling methods based on question\u2013frame correlation into a unified one, dubbed *Most Implied Frames* (MIF). Through some primary results and analysis, Through analysis, we form a hypothesis that question-aware sampling is not necessary, from which we further propose the other method *Most Dominant Frames* (MDF).Experimental results on four public datasets and three advanced ITMs demonstrate that our proposed strategies can boost the performance for image\u2013text pretrained models, and have a wide application scenario in terms of model architectures and dataset types. Our code is available at https://github.com/declare-lab/Sealinghttps://github.com/declare-lab/Sealing.",
        "author": "Wei Han; Hui Chen; Min-Yen Kan; Soujanya Poria",
        "authorids": "/w/wei-han/; /h/hui-chen/; /m/min-yen-kan/; /s/soujanya-poria/",
        "bibtex": "@inproceedings{han-etal-2024-self,\n    title = \"Self-Adaptive Sampling for Accurate Video Question Answering on Image Text Models\",\n    author = \"Han, Wei  and\n      Chen, Hui  and\n      Kan, Min-Yen  and\n      Poria, Soujanya\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.162/\",\n    doi = \"10.18653/v1/2024.findings-naacl.162\",\n    pages = \"2522--2534\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.162.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.162/",
        "pdf_size": 753313,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13003395007315130957&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Singapore University of Technology and Design; Singapore University of Technology and Design; National University of Singapore; Singapore University of Technology and Design",
        "aff_domain": "mymail.sutd.edu.sg;mymail.sutd.edu.sg; ; ",
        "email": "mymail.sutd.edu.sg;mymail.sutd.edu.sg; ; ",
        "github": "https://github.com/declare-lab/Sealing",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Singapore University of Technology and Design;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sutd.edu.sg;https://www.nus.edu.sg",
        "aff_unique_abbr": "SUTD;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2024.findings-naacl.12",
        "title": "Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Fact-checking is an essential task in NLP that is commonly utilized to validate the factual accuracy of a piece of text. Previous approaches mainly involve the resource-intensive process of fine-tuning pre-trained language models on specific datasets. In addition, there is a notable gap in datasets that focus on fact-checking texts generated by large language models (LLMs). In this paper, we introduce Self-Checker, a plug-and-play framework that harnesses LLMs for efficient and rapid fact-checking in a few-shot manner. We also present the BingCheck dataset, specifically designed for fact-checking texts generated by LLMs. Empirical results demonstrate the potential of Self-Checker in the use of LLMs for fact-checking. Compared to state-of-the-art fine-tuned models, there is still significant room for improvement, indicating that adopting LLMs could be a promising direction for future fact-checking research.",
        "author": "Miaoran Li; Baolin Peng; Michel Galley; Jianfeng Gao; Zhu Zhang",
        "authorids": "/m/miaoran-li/; /b/baolin-peng/; /m/michel-galley/; /j/jianfeng-gao/; /z/zhu-zhang/",
        "bibtex": "@inproceedings{li-etal-2024-self,\n    title = \"Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models\",\n    author = \"Li, Miaoran  and\n      Peng, Baolin  and\n      Galley, Michel  and\n      Gao, Jianfeng  and\n      Zhang, Zhu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.12/\",\n    doi = \"10.18653/v1/2024.findings-naacl.12\",\n    pages = \"163--181\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.12.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.12/",
        "pdf_size": 2503587,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1512608095588820745&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Iowa State University + Microsoft Research; Microsoft Research + Tencent AI Lab; Microsoft Research; Microsoft Research; University of Rhode Island",
        "aff_domain": "iastate.edu;microsoft.com;microsoft.com;microsoft.com;uri.edu",
        "email": "iastate.edu;microsoft.com;microsoft.com;microsoft.com;uri.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1+2;1;1;3",
        "aff_unique_norm": "Iowa State University;Microsoft;Tencent;University of Rhode Island",
        "aff_unique_dep": ";Microsoft Research;Tencent AI Lab;",
        "aff_unique_url": "https://www.iastate.edu;https://www.microsoft.com/en-us/research;https://ai.tencent.com;https://www.uri.edu",
        "aff_unique_abbr": "ISU;MSR;Tencent AI Lab;URI",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+1;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.findings-naacl.14",
        "title": "Self-Cleaning: Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "To achieve state-of-the-art performance, one still needs to train NER models on large-scale, high-quality annotated data, an asset that is both costly and time-intensive to accumulate. In contrast, real-world applications often resort to massive low-quality labeled data through non-expert annotators via crowdsourcing and external knowledge bases via distant supervision as a cost-effective alternative. However, these annotation methods result in noisy labels, which in turn lead to a notable decline in performance. Hence, we propose to denoise the noisy NER data with guidance from a small set of clean instances. Along with the main NER model we train a discriminator model and use its outputs to recalibrate the sample weights. The discriminator is capable of detecting both span and category errors with different discriminative prompts. Results on public crowdsourcing and distant supervision datasets show that the proposed method can consistently improve performance with a small guidance set.",
        "author": "Zhendong Chu; Ruiyi Zhang; Tong Yu; Rajiv Jain; Vlad Morariu; Jiuxiang Gu; Ani Nenkova",
        "authorids": "/z/zhendong-chu/; /r/ruiyi-zhang/; /t/tong-yu/; /r/rajiv-jain/; /v/vlad-morariu/; /j/jiuxiang-gu/; /a/ani-nenkova/",
        "bibtex": "@inproceedings{chu-etal-2024-self,\n    title = \"Self-Cleaning: Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances\",\n    author = \"Chu, Zhendong  and\n      Zhang, Ruiyi  and\n      Yu, Tong  and\n      Jain, Rajiv  and\n      Morariu, Vlad  and\n      Gu, Jiuxiang  and\n      Nenkova, Ani\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.14/\",\n    doi = \"10.18653/v1/2024.findings-naacl.14\",\n    pages = \"196--210\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.14.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.14/",
        "pdf_size": 1081527,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17343393821811770294&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Virginia; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Adobe Research",
        "aff_domain": "virginia.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com",
        "email": "virginia.edu;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com;adobe.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;1",
        "aff_unique_norm": "University of Virginia;Adobe",
        "aff_unique_dep": ";Adobe Research",
        "aff_unique_url": "https://www.virginia.edu;https://research.adobe.com",
        "aff_unique_abbr": "UVA;Adobe",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.243",
        "title": "Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos\u2019s generalization and provide more insights.",
        "author": "Wei He; Shichun Liu; Jun Zhao; Yiwen Ding; Yi Lu; Zhiheng Xi; Tao Gui; Qi Zhang; Xuanjing Huang",
        "authorids": "/w/wei-he/; /s/shichun-liu/; /j/jun-zhao/; /y/yiwen-ding/; /y/yi-lu/; /z/zhiheng-xi/; /t/tao-gui/; /q/qi-zhang/; /x/xuan-jing-huang/",
        "bibtex": "@inproceedings{he-etal-2024-self,\n    title = \"Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models\",\n    author = \"He, Wei  and\n      Liu, Shichun  and\n      Zhao, Jun  and\n      Ding, Yiwen  and\n      Lu, Yi  and\n      Xi, Zhiheng  and\n      Gui, Tao  and\n      Zhang, Qi  and\n      Huang, Xuanjing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.243/\",\n    doi = \"10.18653/v1/2024.findings-naacl.243\",\n    pages = \"3829--3845\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.243.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.243/",
        "pdf_size": 366012,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3722774772074957724&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University; Institute of Modern Languages and Linguistics, Fudan University; School of Computer Science, Fudan University; School of Computer Science, Fudan University",
        "aff_domain": "m.fudan.edu.cn; ; ; ; ; ;fudan.edu.cn;fudan.edu.cn; ",
        "email": "m.fudan.edu.cn; ; ; ; ; ;fudan.edu.cn;fudan.edu.cn; ",
        "github": "https://github.com/hewei2001/Self-Demos",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Fudan University",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.fudan.edu.cn",
        "aff_unique_abbr": "Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-short.49",
        "title": "Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Exploring the application of powerful large language models (LLMs) on the named entity recognition (NER) task has drawn much attention recently. This work pushes the performance boundary of zero-shot NER with LLMs by proposing a training-free self-improving framework, which utilizes an unlabeled corpus to stimulate the self-learning ability of LLMs. First, we use the LLM to make predictions on the unlabeled corpus using self-consistency and obtain a self-annotated dataset. Second, we explore various strategies to select reliable annotations to form a reliable self-annotated dataset. Finally, for each test input, we retrieve demonstrations from the reliable self-annotated dataset and perform inference via in-context learning. Experiments on four benchmarks show substantial performance improvements achieved by our framework. Through comprehensive experimental analysis, we find that increasing the size of unlabeled corpus or iterations of self-improving does not guarantee further improvement, but the performance might be boosted via more advanced strategies for reliable annotation selection.",
        "author": "Tingyu Xie; Qi Li; Yan Zhang; Zuozhu Liu; Hongwei Wang",
        "authorids": "/t/tingyu-xie/; /q/qi-li/; /y/yan-zhang/; /z/zuozhu-liu/; /h/hongwei-wang/",
        "bibtex": "@inproceedings{xie-etal-2024-self,\n    title = \"Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models\",\n    author = \"Xie, Tingyu  and\n      Li, Qi  and\n      Zhang, Yan  and\n      Liu, Zuozhu  and\n      Wang, Hongwei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.49/\",\n    doi = \"10.18653/v1/2024.naacl-short.49\",\n    pages = \"583--593\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.49.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.49/",
        "pdf_size": 685134,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15448437033836440482&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "College of Computer Science and Technology, Zhejiang University, China+ZJU-UIUC Institute, Zhejiang University, China; College of Computer Science and Technology, Zhejiang University, China+ZJU-UIUC Institute, Zhejiang University, China; National University of Singapore, Singapore; ZJU-UIUC Institute, Zhejiang University, China; College of Computer Science and Technology, Zhejiang University, China+ZJU-UIUC Institute, Zhejiang University, China",
        "aff_domain": "zju.edu.cn;zju.edu.cn;gmail.com; ; ",
        "email": "zju.edu.cn;zju.edu.cn;gmail.com; ; ",
        "github": "https://github.com/Emma1066/Self-Improve-Zero-Shot-NER",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+0;0+0;1;0;0+0",
        "aff_unique_norm": "Zhejiang University;National University of Singapore",
        "aff_unique_dep": "College of Computer Science and Technology;",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.nus.edu.sg",
        "aff_unique_abbr": "ZJU;NUS",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;1;0;0+0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2024.naacl-long.17",
        "title": "Self-Prompting Large Language Models for Zero-Shot Open-Domain QA",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Open-Domain Question Answering (ODQA) aims to answer questions without explicitly providing specific background documents. This task becomes notably challenging in a zero-shot setting where no data is available to train tailored retrieval-reader models.While recent Large Language Models (LLMs) like GPT-3 have demonstrated their effectiveness in zero-shot ODQA using direct prompting methods, these methods still fall short of fully harnessing the potential of LLMs when implicitly invoked.In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge encoded in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations entirely from scratch.These generated elements are then utilized for in-context learning. Experimental results show that our method significantly surpasses previous state-of-the-art zero-shot methods on three widely-used ODQA datasets and even achieves comparable performance with various customized fine-tuned models on full training data. Our code is available at https://github.com/lockon-n/self-prompting.",
        "author": "Junlong Li; Jinyuan Wang; Zhuosheng Zhang; Hai Zhao",
        "authorids": "/j/junlong-li/; /j/jinyuan-wang/; /z/zhuosheng-zhang/; /h/hai-zhao/",
        "bibtex": "@inproceedings{li-etal-2024-self-prompting,\n    title = \"Self-Prompting Large Language Models for Zero-Shot Open-Domain {QA}\",\n    author = \"Li, Junlong  and\n      Wang, Jinyuan  and\n      Zhang, Zhuosheng  and\n      Zhao, Hai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.17/\",\n    doi = \"10.18653/v1/2024.naacl-long.17\",\n    pages = \"296--310\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.17.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.17/",
        "pdf_size": 505585,
        "gs_citation": 69,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8309073735070159153&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science and Engineering, Shanghai Jiao Tong University+Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; SJTU-Paris Elite Institute of Technology, Shanghai Jiao Tong University+Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University+Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University",
        "aff_domain": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "email": "sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;cs.sjtu.edu.cn",
        "github": "https://github.com/lockon-n/self-prompting",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;0;0+0",
        "aff_unique_norm": "Shanghai Jiao Tong University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.sjtu.edu.cn",
        "aff_unique_abbr": "SJTU",
        "aff_campus_unique_index": "1;2+1;1;1",
        "aff_campus_unique": ";Shanghai;Paris",
        "aff_country_unique_index": "0+0;0+0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-industry.43",
        "title": "Self-Regulated Data-Free Knowledge Amalgamation for Text Classification",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Recently, there has been a growing availability of pre-trained text models on various model repositories. These models greatly reduce the cost of training new models from scratch as they can be fine-tuned for specific tasks or trained on large datasets. However, these datasets may not be publicly accessible due to the privacy, security, or intellectual property issues. In this paper, we aim to develop a lightweight student network that can learn from multiple teacher models without accessing their original training data. Hence, we investigate Data-Free Knowledge Amalgamation (DFKA), a knowledge-transfer task that combines insights from multiple pre-trained teacher models and transfers them effectively to a compact student network. To accomplish this, we propose STRATANET, a modeling framework comprising: (a) a steerable data generator that produces text data tailored to each teacher and (b) an amalgamation module that implements a self-regulative strategy using confidence estimates from the teachers\u2019 different layers to selectively integrate their knowledge and train a versatile student. We evaluate our method on three benchmark text classification datasets with varying labels or domains. Empirically, we demonstrate that the student model learned using our STRATANET outperforms several baselines significantly under data-driven and data-free constraints.",
        "author": "Prashanth Vijayaraghavan; Hongzhi Wang; Luyao Shi; Tyler Baldwin; David Beymer; Ehsan Degan",
        "authorids": "/p/prashanth-vijayaraghavan/; /h/hongzhi-wang/; /l/luyao-shi/; /t/tyler-baldwin/; /d/david-beymer/; /e/ehsan-degan/",
        "bibtex": "@inproceedings{vijayaraghavan-etal-2024-self,\n    title = \"Self-Regulated Data-Free Knowledge Amalgamation for Text Classification\",\n    author = \"Vijayaraghavan, Prashanth  and\n      Wang, Hongzhi  and\n      Shi, Luyao  and\n      Baldwin, Tyler  and\n      Beymer, David  and\n      Degan, Ehsan\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.43/\",\n    doi = \"10.18653/v1/2024.naacl-industry.43\",\n    pages = \"491--502\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.43.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.43/",
        "pdf_size": 760907,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14665102721057153091&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research",
        "aff_domain": "ibm.com;us.ibm.com;ibm.com;us.ibm.com;us.ibm.com;us.ibm.com",
        "email": "ibm.com;us.ibm.com;ibm.com;us.ibm.com;us.ibm.com;us.ibm.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.122",
        "title": "Self-Regulated Sample Diversity in Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Sample diversity depends on the task; within mathematics, precision and determinism are paramount, while storytelling thrives on creativity and surprise. This paper presents a simple self-regulating approach where we adjust sample diversity inference parameters dynamically based on the input prompt\u2014in contrast to existing methods that require expensive and inflexible setups, or maintain static values during inference. Capturing a broad spectrum of sample diversities can be formulated as a straightforward self-supervised inference task, which we find significantly improves the quality of responses generically without model retraining or fine-tuning. In particular, our method demonstrates significant improvement in all supercategories of the MMLU multitask benchmark (GPT-3.5: +4.4%, GPT-4: +1.5%), which captures a large variety of difficult tasks covering STEM, the humanities and social sciences.",
        "author": "Mingyue Liu; Jonathan Frawley; Sarah Wyer; Hubert P. H. Shum; Sara Uckelman; Sue Black; Chris Willcocks",
        "authorids": "/m/mingyue-liu/; /j/jonathan-frawley/; /s/sarah-wyer/; /h/hubert-p-h-shum/; /s/sara-uckelman/; /s/sue-black/; /c/chris-willcocks/",
        "bibtex": "@inproceedings{liu-etal-2024-self-regulated,\n    title = \"Self-Regulated Sample Diversity in Large Language Models\",\n    author = \"Liu, Mingyue  and\n      Frawley, Jonathan  and\n      Wyer, Sarah  and\n      Shum, Hubert P. H.  and\n      Uckelman, Sara  and\n      Black, Sue  and\n      Willcocks, Chris\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.122/\",\n    doi = \"10.18653/v1/2024.findings-naacl.122\",\n    pages = \"1891--1899\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.122.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.122/",
        "pdf_size": 754722,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:6T-FdPJkghMJ:scholar.google.com/&scioq=Self-Regulated+Sample+Diversity+in+Large+Language+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 5,
        "aff": "Durham University; Durham University; Durham University; Durham University; Durham University; Durham University; Durham University",
        "aff_domain": "durham.ac.uk;durham.ac.uk;durham.ac.uk;durham.ac.uk;durham.ac.uk;durham.ac.uk;durham.ac.uk",
        "email": "durham.ac.uk;durham.ac.uk;durham.ac.uk;durham.ac.uk;durham.ac.uk;durham.ac.uk;durham.ac.uk",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Durham University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.dur.ac.uk",
        "aff_unique_abbr": "Durham",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-long.10",
        "title": "Self-generated Replay Memories for Continual Neural Machine Translation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Modern Neural Machine Translation systems exhibit strong performance in several different languages and are constantly improving. Their ability to learn continuously is, however, still severely limited by the catastrophic forgetting issue. In this work, we leverage a key property of encoder-decoder Transformers, i.e. their generative ability, to propose a novel approach to continually learning Neural Machine Translation systems. We show how this can effectively learn on a stream of experiences comprising different languages, by leveraging a replay memory populated by using the model itself as a generator of parallel sentences. We empirically demonstrate that our approach can counteract catastrophic forgetting without requiring explicit memorization of training data. Code will be publicly available upon publication.",
        "author": "Michele Resta; Davide Bacciu",
        "authorids": "/m/michele-resta/; /d/davide-bacciu/",
        "bibtex": "@inproceedings{resta-bacciu-2024-self,\n    title = \"Self-generated Replay Memories for Continual Neural Machine Translation\",\n    author = \"Resta, Michele  and\n      Bacciu, Davide\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.10/\",\n    doi = \"10.18653/v1/2024.naacl-long.10\",\n    pages = \"175--191\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.10.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.10/",
        "pdf_size": 367692,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15266989397988503676&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "University of Pisa; University of Pisa",
        "aff_domain": "phd.unipi.it;unipi.it",
        "email": "phd.unipi.it;unipi.it",
        "github": "https://github.com/m-resta/sg-rep",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pisa",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.unipi.it",
        "aff_unique_abbr": "UNIP",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "2024.naacl-long.443",
        "title": "SemRoDe: Macro Adversarial Training to Learn Representations that are Robust to Word-Level Attacks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern. While current research has explored adversarial training techniques, their improvements to defend against word-level attacks have been limited. In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, adversarial samples generated via word substitutions do indeed belong to an adversarial domain exhibiting a high Wasserstein distance from the base domain. Our method learns a robust representation that bridges these two domains. We hypothesize that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it would improve attack robustness. We align the domains by incorporating a new distance-based objective. With this, our model is able to learn more generalized representations by aligning the model\u2019s high-level output features and therefore better handling unseen adversarial samples. This method can be generalized across word embeddings, even when they share minimal overlap at both vocabulary and word-substitution levels. To evaluate the effectiveness of our approach, we conduct experiments on BERT and RoBERTa models on three datasets. The results demonstrate promising state-of-the-art robustness.",
        "author": "Brian Formento; Wenjie Feng; Chuan-Sheng Foo; Anh Tuan Luu; See-Kiong Ng",
        "authorids": "/b/brian-formento/; /w/wenjie-feng/; /c/chuan-sheng-foo/; /l/luu-anh-tuan/; /s/see-kiong-ng/",
        "bibtex": "@inproceedings{formento-etal-2024-semrode,\n    title = \"{S}em{R}o{D}e: Macro Adversarial Training to Learn Representations that are Robust to Word-Level Attacks\",\n    author = \"Formento, Brian  and\n      Feng, Wenjie  and\n      Foo, Chuan-Sheng  and\n      Luu, Anh Tuan  and\n      Ng, See-Kiong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.443/\",\n    doi = \"10.18653/v1/2024.naacl-long.443\",\n    pages = \"8005--8028\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.443.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.443/",
        "pdf_size": 1554992,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11522878850501417962&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Institute of Data Science, National University of Singapore; Institute of Data Science, National University of Singapore; Institute for Infocomm Research, A*STAR + Centre for Frontier AI Research, A*STAR; Nanyang Technological University; Institute of Data Science, National University of Singapore",
        "aff_domain": "u.nus.edu;nus.edu.sg;i2r.astar.edu.sg; ; ",
        "email": "u.nus.edu;nus.edu.sg;i2r.astar.edu.sg; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1+2;3;0",
        "aff_unique_norm": "National University of Singapore;Institute for Infocomm Research;A*STAR;Nanyang Technological University",
        "aff_unique_dep": "Institute of Data Science;;Centre for Frontier AI Research;",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.i2r.a-star.edu.sg;https://www.a-star.edu.sg;https://www.ntu.edu.sg",
        "aff_unique_abbr": "NUS;I2R;A*STAR;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2024.naacl-long.226",
        "title": "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Existing watermarked generation algorithms employ token-level designs and therefore, are vulnerable to paraphrase attacks. To address this issue, we introduce watermarking on the semantic representation of sentences. We propose SemStamp, a robust sentence-level semantic watermarking algorithm that uses locality-sensitive hashing (LSH) to partition the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by a language model, and conducts rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. To test the paraphrastic robustness of watermarking algorithms, we propose a \u201cbigram paraphrase\u201d attack that produces paraphrases with small bigram overlap with the original sentence. This attack is shown to be effective against existing token-level watermark algorithms, while posing only minor degradations to SemStamp. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on various paraphrasers and domains, but also better at preserving the quality of generation.",
        "author": "Abe Hou; Jingyu Zhang; Tianxing He; Yichen Wang; Yung-Sung Chuang; Hongwei Wang; Lingfeng Shen; Benjamin Van Durme; Daniel Khashabi; Yulia Tsvetkov",
        "authorids": "/a/abe-hou/; /j/jingyu-zhang/; /t/tianxing-he/; /y/yichen-wang/; /y/yung-sung-chuang/; /h/hongwei-wang/; /l/lingfeng-shen/; /b/benjamin-van-durme/; /d/daniel-khashabi/; /y/yulia-tsvetkov/",
        "bibtex": "@inproceedings{hou-etal-2024-semstamp,\n    title = \"{S}em{S}tamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation\",\n    author = \"Hou, Abe  and\n      Zhang, Jingyu  and\n      He, Tianxing  and\n      Wang, Yichen  and\n      Chuang, Yung-Sung  and\n      Wang, Hongwei  and\n      Shen, Lingfeng  and\n      Van Durme, Benjamin  and\n      Khashabi, Daniel  and\n      Tsvetkov, Yulia\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.226/\",\n    doi = \"10.18653/v1/2024.naacl-long.226\",\n    pages = \"4067--4082\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.226.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.226/",
        "pdf_size": 984038,
        "gs_citation": 68,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10136672149005318618&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Johns Hopkins University\u2663; Johns Hopkins University\u2663; University of Washington\u2661; Xi\u2019an Jiaotong University\u2662; Massachusetts Institute of Technology\u2660; Tencent AI Lab\u2021; Johns Hopkins University\u2663; Johns Hopkins University\u2663; Johns Hopkins University\u2663; University of Washington\u2661",
        "aff_domain": "jhu.edu;jhu.edu;cs.washington.edu; ; ; ; ; ; ; ",
        "email": "jhu.edu;jhu.edu;cs.washington.edu; ; ; ; ; ; ; ",
        "github": "https://github.com/bohanhou14/SemStamp",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;1;2;3;4;0;0;0;1",
        "aff_unique_norm": "Johns Hopkins University;University of Washington;Xi'an Jiao Tong University;Massachusetts Institute of Technology;Tencent",
        "aff_unique_dep": ";;;;Tencent AI Lab",
        "aff_unique_url": "https://www.jhu.edu;https://www.washington.edu;https://www.xjtu.edu.cn;https://web.mit.edu;https://ai.tencent.com",
        "aff_unique_abbr": "JHU;UW;XJTU;MIT;Tencent AI Lab",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;0;1;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.findings-naacl.267",
        "title": "Semantically-Prompted Language Models Improve Visual Descriptions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Language-vision models like CLIP have made significant strides in vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive visual descriptions remains challenging; descriptions produced by current methods are often ambiguous and lacking in granularity. To tackle these issues, we propose V-GLOSS: Visual Glosses, a novel method built upon two key ideas. The first is Semantic Prompting, which conditions a language model on structured semantic knowledge. The second is a new contrastive algorithm that elicits fine-grained distinctions between similar concepts. With both ideas, we demonstrate that V-GLOSS improves visual descriptions and achieves strong results in the zero-shot setting on general and fine-grained image-classification datasets, including ImageNet, STL-10, FGVC Aircraft, and Flowers 102. Moreover, these descriptive capabilities contribute to enhancing image-generation performance. Finally, we introduce a quality-tested silver dataset with descriptions generated with V-GLOSS for all ImageNet classes.",
        "author": "Michael Ogezi; Bradley Hauer; Grzegorz Kondrak",
        "authorids": "/m/michael-ogezi/; /b/bradley-hauer/; /g/grzegorz-kondrak/",
        "bibtex": "@inproceedings{ogezi-etal-2024-semantically,\n    title = \"Semantically-Prompted Language Models Improve Visual Descriptions\",\n    author = \"Ogezi, Michael  and\n      Hauer, Bradley  and\n      Kondrak, Grzegorz\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.267/\",\n    doi = \"10.18653/v1/2024.findings-naacl.267\",\n    pages = \"4285--4302\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.267.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.267/",
        "pdf_size": 4602549,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:FJHXKP4na3gJ:scholar.google.com/&scioq=Semantically-Prompted+Language+Models+Improve+Visual+Descriptions&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Alberta Machine Intelligence Institute, Department of Computing Science, University of Alberta, Edmonton, Canada; Alberta Machine Intelligence Institute, Department of Computing Science, University of Alberta, Edmonton, Canada; Alberta Machine Intelligence Institute, Department of Computing Science, University of Alberta, Edmonton, Canada",
        "aff_domain": "ualberta.ca;ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-long.475",
        "title": "Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "An important open question in the use of large language models for knowledge-intensive tasks is how to effectively integrate knowledge from three sources: the model\u2019s parametric memory, external structured knowledge, and external unstructured knowledge. Most existing prompting methods either rely on one or two of these sources, or require repeatedly invoking large language models to generate similar or identical content. In this work, we overcome these limitations by introducing a novel semi-structured prompting approach that seamlessly integrates the model\u2019s parametric memory with unstructured knowledge from text documents and structured knowledge from knowledge graphs. Experimental results on open-domain multi-hop question answering datasets demonstrate that our prompting method significantly surpasses existing techniques, even exceeding those that require fine-tuning.",
        "author": "Xin Su; Tiep Le; Steven Bethard; Phillip Howard",
        "authorids": "/x/xin-su/; /t/tiep-le/; /s/steven-bethard/; /p/phillip-howard/",
        "bibtex": "@inproceedings{su-etal-2024-semi,\n    title = \"Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning\",\n    author = \"Su, Xin  and\n      Le, Tiep  and\n      Bethard, Steven  and\n      Howard, Phillip\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.475/\",\n    doi = \"10.18653/v1/2024.naacl-long.475\",\n    pages = \"8597--8613\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.475.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.475/",
        "pdf_size": 683787,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6543648218494485883&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Arizona+Intel Labs; Intel Labs; University of Arizona; Intel Labs",
        "aff_domain": "arizona.edu;intel.com;arizona.edu;intel.com",
        "email": "arizona.edu;intel.com;arizona.edu;intel.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0;1",
        "aff_unique_norm": "University of Arizona;Intel",
        "aff_unique_dep": ";Intel Labs",
        "aff_unique_url": "https://www.arizona.edu;https://www.intel.com",
        "aff_unique_abbr": "UA;Intel",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.333",
        "title": "Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models. While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label. However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision). Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models. Comprehensive experiments on three public datasets demonstrate the effectiveness of SiCF scores in uncertainty estimation and semi-supervised learning for dialogue summarization tasks. Our code is available at https://github.com/amazon-science/summarization-sicf-score.",
        "author": "Jianfeng He; Hang Su; Jason Cai; Igor Shalyminov; Hwanjun Song; Saab Mansour",
        "authorids": "/j/jianfeng-he/; /h/hang-su/; /j/jason-cai/; /i/igor-shalyminov/; /h/hwanjun-song/; /s/saab-mansour/",
        "bibtex": "@inproceedings{he-etal-2024-semi,\n    title = \"Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection\",\n    author = \"He, Jianfeng  and\n      Su, Hang  and\n      Cai, Jason  and\n      Shalyminov, Igor  and\n      Song, Hwanjun  and\n      Mansour, Saab\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.333/\",\n    doi = \"10.18653/v1/2024.naacl-long.333\",\n    pages = \"5976--5996\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.333.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.333/",
        "pdf_size": 1624726,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7219542444817741572&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;;;;",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "https://github.com/amazon-science/summarization-sicf-score",
        "project": "",
        "author_num": 6
    },
    {
        "id": "2024.naacl-long.292",
        "title": "Sentence-level Media Bias Analysis with Event Relation Graph",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Media outlets are becoming more partisan and polarized nowadays. In this paper, we identify media bias at the sentence level, and pinpoint bias sentences that intend to sway readers\u2019 opinions. As bias sentences are often expressed in a neutral and factual way, considering broader context outside a sentence can help reveal the bias. In particular, we observe that events in a bias sentence need to be understood in associations with other events in the document. Therefore, we propose to construct an event relation graph to explicitly reason about event-event relations for sentence-level bias identification. The designed event relation graph consists of events as nodes and four common types of event relations: coreference, temporal, causal, and subevent relations. Then, we incorporate event relation graph for bias sentences identification in two steps: an event-aware language model is built to inject the events and event relations knowledge into the basic language model via soft labels; further, a relation-aware graph attention network is designed to update sentence embedding with events and event relations information based on hard labels. Experiments on two benchmark datasets demonstrate that our approach with the aid of event relation graph improves both precision and recall of bias sentence identification.",
        "author": "Yuanyuan Lei; Ruihong Huang",
        "authorids": "/y/yuanyuan-lei/; /r/ruihong-huang/",
        "bibtex": "@inproceedings{lei-huang-2024-sentence,\n    title = \"Sentence-level Media Bias Analysis with Event Relation Graph\",\n    author = \"Lei, Yuanyuan  and\n      Huang, Ruihong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.292/\",\n    doi = \"10.18653/v1/2024.naacl-long.292\",\n    pages = \"5225--5238\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.292.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.292/",
        "pdf_size": 1156896,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16623110671327461565&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science and Engineering, Texas A&M University, College Station, TX; Department of Computer Science and Engineering, Texas A&M University, College Station, TX",
        "aff_domain": "tamu.edu;tamu.edu",
        "email": "tamu.edu;tamu.edu",
        "github": "https://github.com/yuanyuanlei-nlp/sentence_level_media_bias_naacl_2024",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Texas A&M University",
        "aff_unique_dep": "Department of Computer Science and Engineering",
        "aff_unique_url": "https://www.tamu.edu",
        "aff_unique_abbr": "TAMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "College Station",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.246",
        "title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Sentiment analysis (SA) has been a long-standing research area in natural language processing. With the recent advent of large language models (LLMs), there is great potential for their employment on SA problems. However, the extent to which current LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring a deeper understanding of specific sentiment phenomena or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs\u2019 SA abilities and propose a novel benchmark, SentiEval, for a more comprehensive and realistic evaluation. Data and code are available at https://github.com/DAMO-NLP-SG/LLM-Sentiment.",
        "author": "Wenxuan Zhang; Yue Deng; Bing Liu; Sinno Pan; Lidong Bing",
        "authorids": "/w/wenxuan-zhang/; /y/yue-deng/; /b/bing-liu/; /s/sinno-pan/; /l/lidong-bing/",
        "bibtex": "@inproceedings{zhang-etal-2024-sentiment,\n    title = \"Sentiment Analysis in the Era of Large Language Models: A Reality Check\",\n    author = \"Zhang, Wenxuan  and\n      Deng, Yue  and\n      Liu, Bing  and\n      Pan, Sinno  and\n      Bing, Lidong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.246/\",\n    doi = \"10.18653/v1/2024.findings-naacl.246\",\n    pages = \"3881--3906\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.246.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.246/",
        "pdf_size": 418681,
        "gs_citation": 363,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8021739398663111087&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "DAMO Academy, Alibaba Group, Singapore+Hupan Lab, 310023, Hangzhou, China; DAMO Academy, Alibaba Group, Singapore+Nanyang Technological University, Singapore; University of Illinois at Chicago; Nanyang Technological University, Singapore+The Chinese University of Hong Kong; DAMO Academy, Alibaba Group, Singapore+Hupan Lab, 310023, Hangzhou, China",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;uic.edu;cuhk.edu.hk;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;uic.edu;cuhk.edu.hk;alibaba-inc.com",
        "github": "https://github.com/DAMO-NLP-SG/LLM-Sentiment",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;0+2;3;2+4;0+1",
        "aff_unique_norm": "Alibaba Group;Hupan Lab;Nanyang Technological University;University of Illinois at Chicago;Chinese University of Hong Kong",
        "aff_unique_dep": "DAMO Academy;;;;",
        "aff_unique_url": "https://www.alibaba.com;;https://www.ntu.edu.sg;https://www.uic.edu;https://www.cuhk.edu.hk",
        "aff_unique_abbr": "Alibaba;;NTU;UIC;CUHK",
        "aff_campus_unique_index": ";;1;2;",
        "aff_campus_unique": ";Chicago;Hong Kong SAR",
        "aff_country_unique_index": "0+1;0+0;2;0+1;0+1",
        "aff_country_unique": "Singapore;China;United States"
    },
    {
        "id": "2024.naacl-short.19",
        "title": "Separately Parameterizing Singleton Detection Improves End-to-end Neural Coreference Resolution",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Current end-to-end coreference resolution models combine detection of singleton mentions and antecedent linking into a single step. In contrast, singleton detection was often treated as a separate step in the pre-neural era. In this work, we show that separately parameterizing these two sub-tasks also benefits end-to-end neural coreference systems. Specifically, we add a singleton detector to the coarse-to-fine (C2F) coreference model, and design an anaphoricity-aware span embedding and singleton detection loss. Our method significantly improves model performance on OntoNotes and four additional datasets.",
        "author": "Xiyuan Zou; Yiran Li; Ian Porada; Jackie Cheung",
        "authorids": "/x/xiyuan-zou/; /y/yiran-li/; /i/ian-porada/; /j/jackie-chi-kit-cheung/",
        "bibtex": "@inproceedings{zou-etal-2024-separately,\n    title = \"Separately Parameterizing Singleton Detection Improves End-to-end Neural Coreference Resolution\",\n    author = \"Zou, Xiyuan  and\n      Li, Yiran  and\n      Porada, Ian  and\n      Cheung, Jackie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.19/\",\n    doi = \"10.18653/v1/2024.naacl-short.19\",\n    pages = \"212--219\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.19.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.19/",
        "pdf_size": 605140,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:530iAYHZkX4J:scholar.google.com/&scioq=Separately+Parameterizing+Singleton+Detection+Improves+End-to-end+Neural+Coreference+Resolution&hl=en&as_sdt=0,33",
        "gs_version_total": 0,
        "aff": "McGill University+Mila Quebec AI Institute; McGill University+Mila Quebec AI Institute; McGill University+Mila Quebec AI Institute; McGill University+Mila Quebec AI Institute",
        "aff_domain": "mail.mcgill.ca;mail.mcgill.ca;mail.mcgill.ca;mcgill.ca",
        "email": "mail.mcgill.ca;mail.mcgill.ca;mail.mcgill.ca;mcgill.ca",
        "github": "https://github.com/XiyuanZou/C2F-SDlinking",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "McGill University;Mila Quebec AI Institute",
        "aff_unique_dep": ";AI Institute",
        "aff_unique_url": "https://www.mcgill.ca;https://mila.quebec",
        "aff_unique_abbr": "McGill;Mila",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-long.368",
        "title": "Separation and Fusion: A Novel Multiple Token Linking Model for Event Argument Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In event argument extraction (EAE), a promising approach involves jointly encoding text and argument roles, and performing multiple token linking operations. This approach further falls into two categories. One extracts arguments within a single event, while the other attempts to extract arguments from multiple events simultaneously. However, the former lacks to leverage cross-event information and the latter requires tougher predictions with longer encoded role sequences and extra linking operations. In this paper, we design a novel separation-and-fusion paradigm to separately acquire cross-event information and fuse it into the argument extraction of a target event. Following the paradigm, we propose a novel multiple token linking model named Sep2F, which can effectively build event correlations via roles and preserve the simple linking predictions of single-event extraction. In particular, we employ one linking module to extract arguments for the target event and another to aggregate the role information of multiple events. More importantly, we propose a novel two-fold fusion module to ensure that the aggregated cross-event information serves EAE well. We evaluate our proposed model on sentence-level and document-level datasets, including ACE05, RAMS, WikiEvents and MLEE. The extensive experimental results indicate that our model outperforms the state-of-the-art EAE models on all the datasets.",
        "author": "Jing Xu; Dandan Song; Siu Hui; Zhijing Wu; Meihuizi Jia; Hao Wang; Yanru Zhou; Changzhi Zhou; Ziyi Yang",
        "authorids": "/j/jing-xu/; /d/dandan-song/; /s/siu-hui/; /z/zhijing-wu/; /m/meihuizi-jia/; /h/hao-wang/; /y/yanru-zhou/; /c/changzhi-zhou/; /z/ziyi-yang/",
        "bibtex": "@inproceedings{xu-etal-2024-separation,\n    title = \"Separation and Fusion: A Novel Multiple Token Linking Model for Event Argument Extraction\",\n    author = \"Xu, Jing  and\n      Song, Dandan  and\n      Hui, Siu  and\n      Wu, Zhijing  and\n      Jia, Meihuizi  and\n      Wang, Hao  and\n      Zhou, Yanru  and\n      Zhou, Changzhi  and\n      Yang, Ziyi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.368/\",\n    doi = \"10.18653/v1/2024.naacl-long.368\",\n    pages = \"6611--6624\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.368.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.368/",
        "pdf_size": 920997,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10916906867178740409&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "School of Computer Science and Technology, Beijing Institute of Technology, China; School of Computer Science and Technology, Beijing Institute of Technology, China; Nanyang Technological University, Singapore; School of Computer Science and Technology, Beijing Institute of Technology, China; School of Computer Science and Technology, Beijing Institute of Technology, China; School of Computer Science and Technology, Beijing Institute of Technology, China; School of Computer Science and Technology, Beijing Institute of Technology, China; School of Computer Science and Technology, Beijing Institute of Technology, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, China",
        "aff_domain": "bit.edu.cn;bit.edu.cn;ntu.edu.sg;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn",
        "email": "bit.edu.cn;bit.edu.cn;ntu.edu.sg;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn;bit.edu.cn",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;1;0;0;0;0;0;0",
        "aff_unique_norm": "Beijing Institute of Technology;Nanyang Technological University",
        "aff_unique_dep": "School of Computer Science and Technology;",
        "aff_unique_url": "http://www.bit.edu.cn/;https://www.ntu.edu.sg",
        "aff_unique_abbr": "BIT;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2024.naacl-long.311",
        "title": "Sequential Compositional Generalization in Multimodal Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The rise of large-scale multimodal models has paved the pathway for groundbreaking advances in generative modeling and reasoning, unlocking transformative applications in a variety of complex tasks. However, a pressing question that remains is their genuine capability for stronger forms of generalization, which has been largely underexplored in the multimodal setting. Our study aims to address this by examining sequential compositional generalization using CompAct (Compositional Activities), a carefully constructed, perceptually grounded dataset set within a rich backdrop of egocentric kitchen activity videos. Each instance in our dataset is represented with a combination of raw video footage, naturally occurring sound, and crowd-sourced step-by-step descriptions. More importantly, our setup ensures that the individual concepts are consistently distributed across training and evaluation sets, while their compositions are novel in the evaluation set. We conduct a comprehensive assessment of several unimodal and multimodal models. Our findings reveal that bi-modal and tri-modal models exhibit a clear edge over their text-only counterparts. This highlights the importance of multimodality while charting a trajectory for future research in this domain.",
        "author": "Semih Yagcioglu; Osman Batur \u0130nce; Aykut Erdem; Erkut Erdem; Desmond Elliott; Deniz Yuret",
        "authorids": "/s/semih-yagcioglu/; /o/osman-batur-ince/; /a/aykut-erdem/; /e/erkut-erdem/; /d/desmond-elliott/; /d/deniz-yuret/",
        "bibtex": "@inproceedings{yagcioglu-etal-2024-sequential,\n    title = \"Sequential Compositional Generalization in Multimodal Models\",\n    author = \"Yagcioglu, Semih  and\n      {\\.I}nce, Osman Batur  and\n      Erdem, Aykut  and\n      Erdem, Erkut  and\n      Elliott, Desmond  and\n      Yuret, Deniz\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.311/\",\n    doi = \"10.18653/v1/2024.naacl-long.311\",\n    pages = \"5591--5611\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.311.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.311/",
        "pdf_size": 10893201,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4508090881003139215&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Hacettepe University; Ko\u00e7 University+KUIS AI Center; Ko\u00e7 University+KUIS AI Center; Hacettepe University; University of Copenhagen+Pioneer Centre for AI; Ko\u00e7 University+KUIS AI Center",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "http://cyberiada.github.io/CompAct",
        "author_num": 6,
        "aff_unique_index": "0;1+2;1+2;0;3+4;1+2",
        "aff_unique_norm": "Hacettepe University;Ko\u00e7 University;Kwansei Gakuin University;University of Copenhagen;Pioneer Centre for AI",
        "aff_unique_dep": ";;AI Center;;AI Research",
        "aff_unique_url": "https://www.hacettepe.edu.tr;https://www.ku.edu.tr;https://www.kwansei.ac.jp;https://www.ku.dk;",
        "aff_unique_abbr": "Hacettepe;Ko\u00e7;KUIS;UCPH;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0+1;0;2;0+1",
        "aff_country_unique": "T\u00fcrkiye;Japan;Denmark;"
    },
    {
        "id": "2024.naacl-long.214",
        "title": "Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Event temporal graphs have been shown as convenient and effective representations of complex temporal relations between events in text. Recent studies, which employ pre-trained language models to auto-regressively generate linearised graphs for constructing event temporal graphs, have shown promising results. However, these methods have often led to suboptimal graph generation as the linearised graphs exhibit set characteristics which are instead treated sequentially by language models. This discrepancy stems from the conventional text generation objectives, leading to erroneous penalisation of correct predictions caused by the misalignment of elements in target sequences. To address these challenges, we reframe the task as a conditional set generation problem, proposing a Set-aligning Framework tailored for the effective utilisation of Large Language Models (LLMs). The framework incorporates data augmentations and set-property regularisations designed to alleviate text generation loss penalties associated with the linearised graph edge sequences, thus encouraging the generation of more relation edges. Experimental results show that our framework surpasses existing baselines for event temporal graph generation. Furthermore, under zero-shot settings, the structural knowledge introduced through our framework notably improves model generalisation, particularly when the training examples available are limited.",
        "author": "Xingwei Tan; Yuxiang Zhou; Gabriele Pergola; Yulan He",
        "authorids": "/x/xingwei-tan/; /y/yuxiang-zhou/; /g/gabriele-pergola/; /y/yulan-he/",
        "bibtex": "@inproceedings{tan-etal-2024-set,\n    title = \"Set-Aligning Framework for Auto-Regressive Event Temporal Graph Generation\",\n    author = \"Tan, Xingwei  and\n      Zhou, Yuxiang  and\n      Pergola, Gabriele  and\n      He, Yulan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.214/\",\n    doi = \"10.18653/v1/2024.naacl-long.214\",\n    pages = \"3872--3892\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.214.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.214/",
        "pdf_size": 988401,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5009364998876781837&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, University of Warwick, UK+Department of Informatics, King\u2019s College London, UK; Department of Informatics, King\u2019s College London, UK; Department of Computer Science, University of Warwick, UK; Department of Computer Science, University of Warwick, UK+Department of Informatics, King\u2019s College London, UK+The Alan Turing Institute, UK",
        "aff_domain": "warwick.ac.uk;kcl.ac.uk;warwick.ac.uk;kcl.ac.uk",
        "email": "warwick.ac.uk;kcl.ac.uk;warwick.ac.uk;kcl.ac.uk",
        "github": "https://github.com/Xingwei-Warwick/Set-Aligning-Event-Temporal-Graph-Generation",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0;0+1+2",
        "aff_unique_norm": "University of Warwick;King\u2019s College London;Alan Turing Institute",
        "aff_unique_dep": "Department of Computer Science;Department of Informatics;",
        "aff_unique_url": "https://warwick.ac.uk;https://www.kcl.ac.uk;https://www.turing.ac.uk",
        "aff_unique_abbr": "Warwick;KCL;ATI",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0+0+0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-long.200",
        "title": "SharpSeq: Empowering Continual Event Detection through Sharpness-Aware Sequential-task Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Continual event detection is a cornerstone in uncovering valuable patterns in many dynamic practical applications, where novel events emerge daily. Existing state-of-the-art approaches with replay buffers still suffer from catastrophic forgetting, partially due to overly simplistic objective aggregation. This oversight disregards complex trade-offs and leads to sub-optimal gradient updates, resulting in performance deterioration across objectives. While there are successful, widely cited multi-objective optimization frameworks for multi-task learning, they lack mechanisms to address data imbalance and evaluate whether a Pareto-optimal solution can effectively mitigate catastrophic forgetting, rendering them unsuitable for direct application to continual learning. To address these challenges, we propose **SharpSeq**, a novel continual learning paradigm leveraging sharpness-aware minimization combined with a generative model to balance training data distribution. Through extensive experiments on multiple real-world datasets, we demonstrate the superior performance of SharpSeq in continual event detection, proving the importance of our approach in mitigating catastrophic forgetting in continual event detection.",
        "author": "Thanh-Thien Le; Viet Dao; Linh Nguyen; Thi-Nhung Nguyen; Linh Ngo; Thien Nguyen",
        "authorids": "/t/thanh-thien-le/; /v/viet-dao/; /l/linh-nguyen/; /t/thi-nhung-nguyen/; /l/linh-ngo/; /t/thien-nguyen/",
        "bibtex": "@inproceedings{le-etal-2024-sharpseq,\n    title = \"{S}harp{S}eq: Empowering Continual Event Detection through Sharpness-Aware Sequential-task Learning\",\n    author = \"Le, Thanh-Thien  and\n      Dao, Viet  and\n      Nguyen, Linh  and\n      Nguyen, Thi-Nhung  and\n      Ngo, Linh  and\n      Nguyen, Thien\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.200/\",\n    doi = \"10.18653/v1/2024.naacl-long.200\",\n    pages = \"3632--3644\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.200.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.200/",
        "pdf_size": 368565,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13595372149895210983&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "VinAI Research; Hanoi University of Science and Technology; Hanoi University of Science and Technology; VinAI Research; Hanoi University of Science and Technology; VinAI Research+University of Oregon",
        "aff_domain": "vinai.io;sis.hust.edu.vn;sis.hust.edu.vn;vinai.io;soict.hust.edu.vn;cs.oregon.edu",
        "email": "vinai.io;sis.hust.edu.vn;sis.hust.edu.vn;vinai.io;soict.hust.edu.vn;cs.oregon.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;0;1;0+2",
        "aff_unique_norm": "VinAI Research;Hanoi University of Science and Technology;University of Oregon",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.vinai.io/;https://www.hust.edu.vn;https://www.uoregon.edu",
        "aff_unique_abbr": "VinAI;HUST;UO",
        "aff_campus_unique_index": "1;1;1;",
        "aff_campus_unique": ";Hanoi",
        "aff_country_unique_index": "0;0;0;0;0;0+1",
        "aff_country_unique": "Vietnam;United States"
    },
    {
        "id": "2024.naacl-industry.34",
        "title": "Shears: Unstructured Sparsity with Neural Low-rank Adapter Search",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Recently, several approaches successfully demonstrated that weight-sharing Neural Architecture Search (NAS) can effectively explore a search space of elastic low-rank adapters (LoRA), allowing the parameter-efficient fine-tuning (PEFT) and compression of large language models. In this paper, we introduce a novel approach called Shears, demonstrating how the integration of cost-effective sparsity and a proposed Neural Low-rank adapter Search (NLS) algorithm can further improve the efficiency of PEFT approaches. Results demonstrate the benefits of Shears compared to other methods, reaching high sparsity levels while improving or with little drop in accuracy, utilizing a single GPU for a pair of hours.",
        "author": "J. Pablo Mu\u00f1oz; Jinjie Yuan; Nilesh Jain",
        "authorids": "/j/juan-pablo-munoz/; /j/jinjie-yuan/; /n/nilesh-jain/",
        "bibtex": "@inproceedings{munoz-etal-2024-shears,\n    title = \"Shears: Unstructured Sparsity with Neural Low-rank Adapter Search\",\n    author = \"Mu{\\~n}oz, J. Pablo  and\n      Yuan, Jinjie  and\n      Jain, Nilesh\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.34/\",\n    doi = \"10.18653/v1/2024.naacl-industry.34\",\n    pages = \"395--405\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.34.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.34/",
        "pdf_size": 407975,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9906244124816066015&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Intel Labs; Intel Corporation; Intel Labs",
        "aff_domain": "intel.com;intel.com;intel.com",
        "email": "intel.com;intel.com;intel.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Intel",
        "aff_unique_dep": "Intel Labs",
        "aff_unique_url": "https://www.intel.com",
        "aff_unique_abbr": "Intel",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.189",
        "title": "Show Your Work with Confidence: Confidence Bands for Tuning Curves",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The choice of hyperparameters greatly impacts performance in natural language processing. Often, it is hard to tell if a method is better than another or just better tuned. *Tuning curves* fix this ambiguity by accounting for tuning effort. Specifically, they plot validation performance as a function of the number of hyperparameter choices tried so far. While several estimators exist for these curves, it is common to use point estimates, which we show fail silently and give contradictory results when given too little data.Beyond point estimates, *confidence bands* are necessary to rigorously establish the relationship between different approaches. We present the first method to construct valid confidence bands for tuning curves. The bands are exact, simultaneous, and distribution-free, thus they provide a robust basis for comparing methods.Empirical analysis shows that while bootstrap confidence bands, which serve as a baseline, fail to approximate their target confidence, ours achieve it exactly. We validate our design with ablations, analyze the effect of sample size, and provide guidance on comparing models with our method. To promote confident comparisons in future work, we release opda: an easy-to-use library that you can install with pip. https://github.com/nicholaslourie/opda",
        "author": "Nicholas Lourie; Kyunghyun Cho; He He",
        "authorids": "/n/nicholas-lourie/; /k/kyunghyun-cho/; /h/he-he/",
        "bibtex": "@inproceedings{lourie-etal-2024-show,\n    title = \"Show Your Work with Confidence: Confidence Bands for Tuning Curves\",\n    author = \"Lourie, Nicholas  and\n      Cho, Kyunghyun  and\n      He, He\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.189/\",\n    doi = \"10.18653/v1/2024.naacl-long.189\",\n    pages = \"3455--3472\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.189.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.189/",
        "pdf_size": 1171122,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6733702649450217839&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "New York University; New York University + Genentech; New York University",
        "aff_domain": "nyu.edu;nyu.edu;nyu.edu",
        "email": "nyu.edu;nyu.edu;nyu.edu",
        "github": "https://github.com/nicholaslourie/opda",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "New York University;Genentech",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nyu.edu;https://www.genentech.com",
        "aff_unique_abbr": "NYU;Genentech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.140",
        "title": "Signer Diversity-driven Data Augmentation for Signer-Independent Sign Language Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The primary objective of sign language translation (SLT) is to transform sign language videos into natural sentences.A crucial challenge in this field is developing signer-independent SLT systems which requires models to generalize effectively to signers not encountered during training.This challenge is exacerbated by the limited diversity of signers in existing SLT datasets, which often results in suboptimal generalization capabilities of current models.Achieving robustness to unseen signers is essential for signer-independent SLT.However, most existing method relies on signer identity labels, which is often impractical and costly in real-world applications.To address this issue, we propose the Signer Diversity-driven Data Augmentation (SDDA) method that can achieve good generalization without relying on signer identity labels. SDDA comprises two data augmentation schemes. The first is data augmentation based on adversarial training, which aims to utilize the gradients of the model to generate adversarial examples. The second is data augmentation based on diffusion model, which focuses on using the advanced diffusion-based text guided image editing method to modify the appearances of the signer in images. The combination of the two strategies significantly enriches the diversity of signers in the training process.Moreover, we introduce a consistency loss and a discrimination loss to enhance the learning of signer-independent features.Our experimental results demonstrate our model significantly enhances the performance of SLT in the signer-independent setting, achieving state-of-the-art results without relying on signer identity labels.",
        "author": "Honghao Fu; Liang Zhang; Biao Fu; Rui Zhao; Jinsong Su; Xiaodong Shi; Yidong Chen",
        "authorids": "/h/honghao-fu/; /l/liang-zhang/; /b/biao-fu/; /r/rui-zhao/; /j/jinsong-su/; /x/xiaodong-shi/; /y/yidong-chen/",
        "bibtex": "@inproceedings{honghaofu-etal-2024-signer,\n    title = \"Signer Diversity-driven Data Augmentation for Signer-Independent Sign Language Translation\",\n    author = \"Fu, Honghao  and\n      Zhang, Liang  and\n      Fu, Biao  and\n      Zhao, Rui  and\n      Su, Jinsong  and\n      Shi, Xiaodong  and\n      Chen, Yidong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.140/\",\n    doi = \"10.18653/v1/2024.findings-naacl.140\",\n    pages = \"2182--2193\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.140.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.140/",
        "pdf_size": 1828806,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16524913454243202183&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;;;;;",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7
    },
    {
        "id": "2024.findings-naacl.90",
        "title": "SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet.In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on four state-of-the-art pretrained models and applied to two code generation tasks, exposes multiple failure modes attributed to OOD generalization issues.",
        "author": "Hossein Hajipour; Ning Yu; Cristian-Alexandru Staicu; Mario Fritz",
        "authorids": "/h/hossein-hajipour/; /n/ning-yu/; /c/cristian-alexandru-staicu/; /m/mario-fritz/",
        "bibtex": "@inproceedings{hajipour-etal-2024-simscood,\n    title = \"{S}im{SCOOD}: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models\",\n    author = \"Hajipour, Hossein  and\n      Yu, Ning  and\n      Staicu, Cristian-Alexandru  and\n      Fritz, Mario\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.90/\",\n    doi = \"10.18653/v1/2024.findings-naacl.90\",\n    pages = \"1400--1416\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.90.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.90/",
        "pdf_size": 1076953,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8603687437464787116&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "CISPA Helmholtz Center for Information Security; Netflix Eyeline Studios; CISPA Helmholtz Center for Information Security; CISPA Helmholtz Center for Information Security",
        "aff_domain": "cispa.de;gmail.com;cispa.de;cispa.de",
        "email": "cispa.de;gmail.com;cispa.de;cispa.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "CISPA Helmholtz Center for Information Security;Netflix",
        "aff_unique_dep": ";Eyeline Studios",
        "aff_unique_url": "https://www.cispa.de/;https://www.netflix.com",
        "aff_unique_abbr": "CISPA;Netflix",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "2024.naacl-long.25",
        "title": "Simple and effective data augmentation for compositional generalization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Compositional generalization, the ability to predict complex meanings from training on simpler sentences, poses challenges for powerful pretrained seq2seq models. In this paper, we show that data augmentation methods that sample MRs and backtranslate them can be effective for compositional generalization, but only if we sample from the right distribution. Remarkably, sampling from a uniform distribution performs almost as well as sampling from the test distribution, and greatly outperforms earlier methods that sampled from the training distribution.We further conduct experiments to investigate the reason why this happens and where the benefit of such data augmentation methods come from.",
        "author": "Yuekun Yao; Alexander Koller",
        "authorids": "/y/yuekun-yao/; /a/alexander-koller/",
        "bibtex": "@inproceedings{yao-koller-2024-simple,\n    title = \"Simple and effective data augmentation for compositional generalization\",\n    author = \"Yao, Yuekun  and\n      Koller, Alexander\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.25/\",\n    doi = \"10.18653/v1/2024.naacl-long.25\",\n    pages = \"434--449\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.25.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.25/",
        "pdf_size": 672298,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10298392154214782755&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Language Science and Technology, Saarland Informatics Campus, Saarland University, Saarbr\u00fccken, Germany; Department of Language Science and Technology, Saarland Informatics Campus, Saarland University, Saarbr\u00fccken, Germany",
        "aff_domain": "coli.uni-saarland.de;coli.uni-saarland.de",
        "email": "coli.uni-saarland.de;coli.uni-saarland.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Saarland University",
        "aff_unique_dep": "Department of Language Science and Technology",
        "aff_unique_url": "https://www.uni-saarland.de",
        "aff_unique_abbr": "Uni Saar",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Saarbr\u00fccken",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.findings-naacl.211",
        "title": "Simulating Opinion Dynamics with Networks of LLM-based Agents",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations often over-simplify human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards producing accurate information, leading simulated agents to consensus in line with scientific reality. This bias limits their utility for understanding resistance to consensus views on issues like climate change. After inducing confirmation bias through prompt engineering, however, we observed opinion fragmentation in line with existing agent-based modeling and opinion dynamics research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.",
        "author": "Yun-Shiuan Chuang; Agam Goyal; Nikunj Harlalka; Siddharth Suresh; Robert Hawkins; Sijia Yang; Dhavan Shah; Junjie Hu; Timothy Rogers",
        "authorids": "/y/yun-shiuan-chuang/; /a/agam-goyal/; /n/nikunj-harlalka/; /s/siddharth-suresh/; /r/robert-hawkins/; /s/sijia-yang/; /d/dhavan-shah/; /j/junjie-hu/; /t/timothy-rogers/",
        "bibtex": "@inproceedings{chuang-etal-2024-simulating,\n    title = \"Simulating Opinion Dynamics with Networks of {LLM}-based Agents\",\n    author = \"Chuang, Yun-Shiuan  and\n      Goyal, Agam  and\n      Harlalka, Nikunj  and\n      Suresh, Siddharth  and\n      Hawkins, Robert  and\n      Yang, Sijia  and\n      Shah, Dhavan  and\n      Hu, Junjie  and\n      Rogers, Timothy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.211/\",\n    doi = \"10.18653/v1/2024.findings-naacl.211\",\n    pages = \"3326--3346\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.211.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.211/",
        "pdf_size": 1103079,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16368348767854485508&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison; University of Wisconsin-Madison",
        "aff_domain": "wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu",
        "email": "wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu;wisc.edu",
        "github": "https://github.com/yunshiuan/llm-agent-opinion-dynamics",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "University of Wisconsin-Madison",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.wisc.edu",
        "aff_unique_abbr": "UW-Madison",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Madison",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.345",
        "title": "SlimFit: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Transformer-based models, such as BERT and ViT, have achieved state-of-the-art results across different natural language processing (NLP) and computer vision (CV) tasks. However, these models are extremely memory intensive during their fine-tuning process, making them difficult to deploy on GPUs with limited memory resources. To address this issue, we introduce a new tool called SlimFit that reduces the memory requirements of these models by dynamically analyzing their training dynamics and freezing less-contributory layers during fine-tuning. The layers to freeze are chosen using a runtime inter-layer scheduling algorithm. This allows SlimFit to freeze up to 95% of layers and reduce the overall on-device GPU memory usage of transformer-based models such as ViT and BERT by an average of 2.2x, across different NLP and CV benchmarks/datasets such as GLUE, SQuAD 2.0, CIFAR-10, CIFAR-100 and ImageNet with an average degradation of 0.2% in accuracy. For such NLP and CV tasks, SlimFit can reduce up to 3.1x the total on-device memory usage with an accuracy degradation of only up to 0.4%. As a result, while fine-tuning of ViT on ImageNet and BERT on SQuAD 2.0 with a batch size of 128 requires 3 and 2 32GB GPUs, respectively, SlimFit enables fine-tuning them on a single 32GB GPU without any significant accuracy degradation. The code of SlimFit is available at https://github.com/arashardakani/SlimFit.",
        "author": "Arash Ardakani; Altan Haan; Shangyin Tan; Doru Thom Popovici; Alvin Cheung; Costin Iancu; Koushik Sen",
        "authorids": "/a/arash-ardakani/; /a/altan-haan/; /s/shangyin-tan/; /d/doru-thom-popovici/; /a/alvin-cheung/; /c/costin-iancu/; /k/koushik-sen/",
        "bibtex": "@inproceedings{ardakani-etal-2024-slimfit,\n    title = \"{S}lim{F}it: Memory-Efficient Fine-Tuning of Transformer-based Models Using Training Dynamics\",\n    author = \"Ardakani, Arash  and\n      Haan, Altan  and\n      Tan, Shangyin  and\n      Popovici, Doru Thom  and\n      Cheung, Alvin  and\n      Iancu, Costin  and\n      Sen, Koushik\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.345/\",\n    doi = \"10.18653/v1/2024.naacl-long.345\",\n    pages = \"6218--6236\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.345.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.345/",
        "pdf_size": 323829,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14874515664859096843&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; Lawrence Berkeley National Laboratory; University of California, Berkeley; Lawrence Berkeley National Laboratory; University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;lbl.gov;berkeley.edu;lbl.gov;berkeley.edu",
        "email": "berkeley.edu;berkeley.edu;berkeley.edu;lbl.gov;berkeley.edu;lbl.gov;berkeley.edu",
        "github": "https://github.com/arashardakani/SlimFit",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;0;1;0;1;0",
        "aff_unique_norm": "University of California, Berkeley;Lawrence Berkeley National Laboratory",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.lbl.gov",
        "aff_unique_abbr": "UC Berkeley;LBNL",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.175",
        "title": "SocREval: Large Language Models with the Socratic Method for Reference-free Reasoning Evaluation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "To comprehensively gauge the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains as references to assess the model-derived chains. However, such \u201cgold-standard\u201d human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning evaluation metrics, while eliminating the need for human-crafted reasoning chains as references, often require fine-tuning with human-derived chains before evaluation, complicating the process and questioning their adaptability to other datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes. Leveraging the Socratic method, we develop SocREval (**Soc**ratic Method-Inspired **R**easoning **Eval**uation), a novel approach for prompt design in reference-free reasoning evaluation. Empirical results from four human annotated datasets reveal that SocREval significantly improves GPT-4\u2019s performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated efficacy, SocREval, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis.",
        "author": "Hangfeng He; Hongming Zhang; Dan Roth",
        "authorids": "/h/hangfeng-he/; /h/hongming-zhang/; /d/dan-roth/",
        "bibtex": "@inproceedings{he-etal-2024-socreval,\n    title = \"{S}oc{RE}val: Large Language Models with the Socratic Method for Reference-free Reasoning Evaluation\",\n    author = \"He, Hangfeng  and\n      Zhang, Hongming  and\n      Roth, Dan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.175/\",\n    doi = \"10.18653/v1/2024.findings-naacl.175\",\n    pages = \"2736--2764\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.175.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.175/",
        "pdf_size": 448431,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14200782395445785130&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Rochester; Tencent AI Lab, Seattle; University of Pennsylvania",
        "aff_domain": "rochester.edu;global.tencent.com;seas.upenn.edu",
        "email": "rochester.edu;global.tencent.com;seas.upenn.edu",
        "github": "https://github.com/HornHehhf/SocREval",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Rochester;Tencent;University of Pennsylvania",
        "aff_unique_dep": ";AI Lab;",
        "aff_unique_url": "https://www.rochester.edu;https://ai.tencent.com;https://www.upenn.edu",
        "aff_unique_abbr": "U of R;Tencent AI Lab;UPenn",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.166",
        "title": "Social Meme-ing: Measuring Linguistic Variation in Memes",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Much work in the space of NLP has used computational methods to explore sociolinguistic variation in text. In this paper, we argue that memes, as multimodal forms of language comprised of visual templates and text, also exhibit meaningful social variation. We construct a computational pipeline to cluster individual instances of memes into templates and semantic variables, taking advantage of their multimodal structure in doing so. We apply this method to a large collection of meme images from Reddit and make available the resulting SemanticMemes dataset of 3.8M images clustered by their semantic function. We use these clusters to analyze linguistic variation in memes, discovering not only that socially meaningful variation in meme usage exists between subreddits, but that patterns of meme innovation and acculturation within these communities align with previous findings on written language.",
        "author": "Naitian Zhou; David Jurgens; David Bamman",
        "authorids": "/n/naitian-zhou/; /d/david-jurgens/; /d/david-bamman/",
        "bibtex": "@inproceedings{zhou-etal-2024-social,\n    title = \"Social Meme-ing: Measuring Linguistic Variation in Memes\",\n    author = \"Zhou, Naitian  and\n      Jurgens, David  and\n      Bamman, David\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.166/\",\n    doi = \"10.18653/v1/2024.naacl-long.166\",\n    pages = \"3005--3024\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.166.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.166/",
        "pdf_size": 32532878,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7152812518832570411&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Berkeley; University of Michigan; University of California, Berkeley",
        "aff_domain": "berkeley.edu;umich.edu;berkeley.edu",
        "email": "berkeley.edu;umich.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of California, Berkeley;University of Michigan",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.umich.edu",
        "aff_unique_abbr": "UC Berkeley;UM",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.41",
        "title": "Solving Data-centric Tasks using Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models are rapidly replacing help forums like StackOverflow, and are especially helpful to non-professional programmers and end users. These users are often interested in data-centric tasks, like spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including data. But how do we decide how much data and which data to include in the prompt?This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a novel cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table,our cluster-then-select technique outperforms a random selection baseline.",
        "author": "Shraddha Barke; Christian Poelitz; Carina Negreanu; Benjamin Zorn; Jos\u00e9 Cambronero; Andrew Gordon; Vu Le; Elnaz Nouri; Nadia Polikarpova; Advait Sarkar; Brian Slininger; Neil Toronto; Jack Williams",
        "authorids": "/s/shraddha-barke/; /c/christian-poelitz/; /c/carina-negreanu/; /b/benjamin-zorn/; /j/jose-cambronero/; /a/andrew-gordon/; /v/vu-le/; /e/elnaz-nouri/; /n/nadia-polikarpova/; /a/advait-sarkar/; /b/brian-slininger/; /n/neil-toronto/; /j/jack-williams/",
        "bibtex": "@inproceedings{barke-etal-2024-solving,\n    title = \"Solving Data-centric Tasks using Large Language Models\",\n    author = \"Barke, Shraddha  and\n      Poelitz, Christian  and\n      Negreanu, Carina  and\n      Zorn, Benjamin  and\n      Cambronero, Jos{\\'e}  and\n      Gordon, Andrew  and\n      Le, Vu  and\n      Nouri, Elnaz  and\n      Polikarpova, Nadia  and\n      Sarkar, Advait  and\n      Slininger, Brian  and\n      Toronto, Neil  and\n      Williams, Jack\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.41/\",\n    doi = \"10.18653/v1/2024.findings-naacl.41\",\n    pages = \"626--638\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.41.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.41/",
        "pdf_size": 532826,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11190630961874665326&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "UCSD*; Microsoft\u2020; Microsoft\u2020; Microsoft\u2020; Microsoft\u2020; Microsoft\u2020; Microsoft\u2020; Microsoft\u2020; UCSD*; Microsoft\u2020; Microsoft\u2020; Microsoft\u2020; Microsoft\u2020",
        "aff_domain": "ucsd.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;ucsd.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "ucsd.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;microsoft.com;ucsd.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 13,
        "aff_unique_index": "0;1;1;1;1;1;1;1;0;1;1;1;1",
        "aff_unique_norm": "University of California, San Diego;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "https://ucsd.edu;https://www.microsoft.com",
        "aff_unique_abbr": "UCSD;Microsoft",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "La Jolla;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.42",
        "title": "Solving General Natural-Language-Description Optimization Problems with Large Language Models",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Optimization problems seek to find the best solution to an objective under a set of constraints, and have been widely investigated in real-world applications. Modeling and solving optimization problems in a specific domain typically require a combination of domain knowledge, mathematical skills, and programming ability, making it difficult for general users and even domain professionals. In this paper, we propose a novel framework called OptLLM that augments LLMs with external solvers. Specifically, OptLLM accepts user queries in natural language, convert them into mathematical formulations and programming codes, and calls the solvers to calculate the results for decision-making. In addition, OptLLM supports multi-round dialogues to gradually refine the modeling and solving of optimization problems. To illustrate the effectiveness of OptLLM, we provide tutorials on three typical optimization applications and conduct experiments on both prompt-based GPT models and a fine-tuned Qwen model using a large-scale self-developed optimization dataset. Experimental results show that OptLLM works with various LLMs, and the fine-tuned model achieves an accuracy boost compared to the prompt-based models. Some features of OptLLM framework have been available for trial since June 2023 (https://opt.alibabacloud.com/chat or https://opt.aliyun.com/chat).",
        "author": "Jihai Zhang; Wei Wang; Siyan Guo; Li Wang; Fangquan Lin; Cheng Yang; Wotao Yin",
        "authorids": "/j/jihai-zhang/; /w/wei-wang/; /s/siyan-guo/; /l/li-wang/; /f/fangquan-lin/; /c/cheng-yang/; /w/wotao-yin/",
        "bibtex": "@inproceedings{zhang-etal-2024-solving,\n    title = \"Solving General Natural-Language-Description Optimization Problems with Large Language Models\",\n    author = \"Zhang, Jihai  and\n      Wang, Wei  and\n      Guo, Siyan  and\n      Wang, Li  and\n      Lin, Fangquan  and\n      Yang, Cheng  and\n      Yin, Wotao\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.42/\",\n    doi = \"10.18653/v1/2024.naacl-industry.42\",\n    pages = \"483--490\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.42.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.42/",
        "pdf_size": 695577,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15259366340205977987&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group",
        "aff_domain": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "",
        "project": "https://opt.alibabacloud.com/chat; https://opt.aliyun.com/chat",
        "author_num": 7,
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Alibaba Group",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.alibaba.com",
        "aff_unique_abbr": "Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.44",
        "title": "Source-Free Unsupervised Domain Adaptation for Question Answering via Prompt-Assisted Self-learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This work addresses source-free domain adaptation (SFDA) for Question Answering (QA), wherein a model trained on a source domain is adapted to unlabeled target domains without additional source data. Existing SFDA methods only focus on the adaptation phase, overlooking the impact of source domain training on model generalizability. In this paper, we argue that source model training itself is also critical for improving the adaptation performance and stability. To this end, we investigate the role of prompt learning as an effective method to internalize domain-agnostic QA knowledge, which can be integrated into source training. After source training, an interactive self-learning strategy is proposed to further fine tune both model and prompt in the model adaptation phase. This leads to the Prompt-Assisted Self-Adaptive Learning (PASAL), an innovative SFDA approach for QA. Empirical evaluation on four benchmark datasets shows that PASAL surpasses existing methods in managing domain gaps and demonstrates greater stability across various target domains, validating the significance of source domain training for effective domain adaptation.",
        "author": "Maxwell Yin; Boyu Wang; Charles Ling",
        "authorids": "/m/maxwell-yin/; /b/boyu-wang/; /c/charles-ling/",
        "bibtex": "@inproceedings{yin-etal-2024-source,\n    title = \"Source-Free Unsupervised Domain Adaptation for Question Answering via Prompt-Assisted Self-learning\",\n    author = \"Yin, Maxwell  and\n      Wang, Boyu  and\n      Ling, Charles\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.44/\",\n    doi = \"10.18653/v1/2024.findings-naacl.44\",\n    pages = \"700--713\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.44.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.44/",
        "pdf_size": 726575,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1877486025191451008&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Western University; Western University; Western University",
        "aff_domain": "uwo.ca;csd.uwo.ca;uwo.ca",
        "email": "uwo.ca;csd.uwo.ca;uwo.ca",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Western University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uwo.ca",
        "aff_unique_abbr": "Western",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.findings-naacl.6",
        "title": "SpeedE: Euclidean Geometric Knowledge Graph Embedding Strikes Back",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Geometric knowledge graph embedding models (gKGEs) have shown great potential for knowledge graph completion (KGC), i.e., automatically predicting missing triples. However, contemporary gKGEs require high embedding dimensionalities or complex embedding spaces for good KGC performance, drastically limiting their space and time efficiency. Facing these challenges, we propose SpeedE, a lightweight Euclidean gKGE that (1) provides strong inference capabilities, (2) is competitive with state-of-the-art gKGEs, even significantly outperforming them on YAGO3-10 and WN18RR, and (3) dramatically increases their efficiency, in particular, needing solely a fifth of the training time and a fourth of the parameters of the state-of-the-art ExpressivE model on WN18RR to reach the same KGC performance.",
        "author": "Aleksandar Pavlovi\u0107; Emanuel Sallinger",
        "authorids": "/a/aleksandar-pavlovic/; /e/emanuel-sallinger/",
        "bibtex": "@inproceedings{pavlovic-sallinger-2024-speede,\n    title = \"{S}peed{E}: {E}uclidean Geometric Knowledge Graph Embedding Strikes Back\",\n    author = \"Pavlovi{\\'c}, Aleksandar  and\n      Sallinger, Emanuel\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.6/\",\n    doi = \"10.18653/v1/2024.findings-naacl.6\",\n    pages = \"69--92\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.6.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.6/",
        "pdf_size": 824604,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10799780158177540297&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Research Unit of Databases and Artificial Intelligence, TU Wien, Vienna, Austria; Research Unit of Databases and Artificial Intelligence, TU Wien, Vienna, Austria",
        "aff_domain": "tuwien.ac.at;tuwien.ac.at",
        "email": "tuwien.ac.at;tuwien.ac.at",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "TU Wien",
        "aff_unique_dep": "Research Unit of Databases and Artificial Intelligence",
        "aff_unique_url": "https://www.tuwien.ac.at",
        "aff_unique_abbr": "TU Wien",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Vienna",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Austria"
    },
    {
        "id": "2024.naacl-long.283",
        "title": "SportQA: A Benchmark for Sports Understanding in Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise. The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and enhancing sports understanding in LLMs. The dataset is available at https://github.com/haotianxia/SportQA",
        "author": "Haotian Xia; Zhengbang Yang; Yuqing Wang; Rhys Tracy; Yun Zhao; Dongdong Huang; Zezhi Chen; Yan Zhu; Yuan-fang Wang; Weining Shen",
        "authorids": "/h/haotian-xia/; /z/zhengbang-yang/; /y/yuqing-wang/; /r/rhys-tracy/; /y/yun-zhao/; /d/dongdong-huang/; /z/zezhi-chen/; /y/yan-zhu/; /y/yuan-fang-wang/; /w/weining-shen/",
        "bibtex": "@inproceedings{xia-etal-2024-sportqa,\n    title = \"{S}port{QA}: A Benchmark for Sports Understanding in Large Language Models\",\n    author = \"Xia, Haotian  and\n      Yang, Zhengbang  and\n      Wang, Yuqing  and\n      Tracy, Rhys  and\n      Zhao, Yun  and\n      Huang, Dongdong  and\n      Chen, Zezhi  and\n      Zhu, Yan  and\n      Wang, Yuan-fang  and\n      Shen, Weining\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.283/\",\n    doi = \"10.18653/v1/2024.naacl-long.283\",\n    pages = \"5061--5081\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.283.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.283/",
        "pdf_size": 1360597,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1104819192570856916&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of California, Irvine, CA, USA; University of California, Irvine, CA, USA; Stanford University, CA, USA; University of California, Santa Barbara, CA, USA; Meta Platforms, Inc., CA, USA; Beijing Normal University, Beijing, China; University of California, Irvine, CA, USA; Beijing Normal University, Beijing, China; University of California, Santa Barbara, CA, USA; University of California, Irvine, CA, USA",
        "aff_domain": "uci.edu;uci.edu;cs.ucsb.edu;cs.ucsb.edu;bnu.edu.cn; ; ; ; ;",
        "email": "uci.edu;uci.edu;cs.ucsb.edu;cs.ucsb.edu;bnu.edu.cn; ; ; ; ;",
        "github": "https://github.com/haotianxia/SportQA",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;1;2;3;4;0;4;2;0",
        "aff_unique_norm": "University of California, Irvine;Stanford University;University of California, Santa Barbara;Meta;Beijing Normal University",
        "aff_unique_dep": ";;;Meta Platforms, Inc.;",
        "aff_unique_url": "https://www.uci.edu;https://www.stanford.edu;https://www.ucsb.edu;https://meta.com;https://www.bnu.edu.cn",
        "aff_unique_abbr": "UCI;Stanford;UCSB;Meta;BNU",
        "aff_campus_unique_index": "0;0;1;2;4;0;4;2;0",
        "aff_campus_unique": "Irvine;California;Santa Barbara;;Beijing",
        "aff_country_unique_index": "0;0;0;0;0;1;0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.276",
        "title": "Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent developments in Large Language Models (LLMs) have manifested significant advancements. To facilitate safeguards against malicious exploitation, a body of research has concentrated on aligning LLMs with human preferences and inhibiting their generation of inappropriate content. Unfortunately, such alignments are often vulnerable: fine-tuning with a minimal amount of harmful data can easily unalign the target LLM. While being effective, such fine-tuning-based unalignment approaches also have their own limitations: (1) non-stealthiness, after fine-tuning, safety audits or red-teaming can easily expose the potential weaknesses of the unaligned models, thereby precluding their release/use. (2) non-persistence, the unaligned LLMs can be easily repaired through re-alignment, i.e., fine-tuning again with aligned data points. In this work, we show that it is possible to conduct stealthy and persistent unalignment on large language models via backdoor injections. We also provide a novel understanding of the relationship between the backdoor persistence and the activation pattern and further provide guidelines for potential trigger design. Through extensive experiments, we demonstrate that our proposed stealthy and persistent unalignment can successfully pass the safety evaluation while maintaining strong persistence against re-alignment defense.",
        "author": "Yuanpu Cao; Bochuan Cao; Jinghui Chen",
        "authorids": "/y/yuanpu-cao/; /b/bochuan-cao/; /j/jinghui-chen/",
        "bibtex": "@inproceedings{cao-etal-2024-stealthy,\n    title = \"Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections\",\n    author = \"Cao, Yuanpu  and\n      Cao, Bochuan  and\n      Chen, Jinghui\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.276/\",\n    doi = \"10.18653/v1/2024.naacl-long.276\",\n    pages = \"4920--4935\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.276.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.276/",
        "pdf_size": 466388,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14935244006343849698&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The Pennsylvania State University; The Pennsylvania State University; The Pennsylvania State University",
        "aff_domain": "psu.edu;psu.edu;psu.edu",
        "email": "psu.edu;psu.edu;psu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Pennsylvania State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.psu.edu",
        "aff_unique_abbr": "PSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.122",
        "title": "Strings from the Library of Babel: Random Sampling as a Strong Baseline for Prompt Optimisation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent prompt optimisation approaches use the generative nature of language models to produce prompts \u2013 even rivaling the performance of human-curated prompts. In this paper, we demonstrate that randomly sampling tokens from the model vocabulary as \u201cseparators\u201d can be as effective as language models for prompt-style text classification. Our experiments show that random separators are competitive baselines, having less than a 1% difference compared to previous self-optimisation methods and showing a 12% average relative improvement over strong human baselines across nine text classification tasks and eight language models. We further analyse this phenomenon in detail using three different random generation strategies, establishing that the language space is rich with potentially good separators, with a greater than 40% average chance that a randomly drawn separator performs better than human-curated separators. These observations challenge the common assumption that an effective prompt should be human readable or task relevant and establish a strong baseline for prompt optimisation research.",
        "author": "Yao Lu; Jiayi Wang; Raphael Tang; Sebastian Riedel; Pontus Stenetorp",
        "authorids": "/y/yao-lu/; /j/jiayi-wang/; /r/raphael-tang/; /s/sebastian-riedel/; /p/pontus-stenetorp/",
        "bibtex": "@inproceedings{lu-etal-2024-strings,\n    title = \"Strings from the Library of {B}abel: Random Sampling as a Strong Baseline for Prompt Optimisation\",\n    author = \"Lu, Yao  and\n      Wang, Jiayi  and\n      Tang, Raphael  and\n      Riedel, Sebastian  and\n      Stenetorp, Pontus\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.122/\",\n    doi = \"10.18653/v1/2024.naacl-long.122\",\n    pages = \"2221--2231\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.122.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.122/",
        "pdf_size": 359240,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1260338017545458022&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University College London; University College London; Comcast AI Technologies; University College London; University College London",
        "aff_domain": "cs.ucl.ac.uk;ucl.ac.uk;comcast.com;cs.ucl.ac.uk;cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk;ucl.ac.uk;comcast.com;cs.ucl.ac.uk;cs.ucl.ac.uk",
        "github": "https://github.com/yaolu/random-prompt",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University College London;Comcast",
        "aff_unique_dep": ";AI Technologies",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.comcast.com",
        "aff_unique_abbr": "UCL;Comcast",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2024.naacl-short.2",
        "title": "Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Despite the remarkable capabilities of Large Language Models (LLMs) like GPT-4, producing complex, structured tabular data remains challenging. Our study assesses LLMs\u2019 proficiency in structuring tables and introduces a novel fine-tuning method, cognizant of data structures, to bolster their performance. We unveil Struc-Bench, a comprehensive benchmark featuring prominent LLMs (GPT-NeoX-20B, GPT-3.5, GPT-4, and Vicuna), which spans text tables, HTML, and LaTeX formats. Our proposed FormatCoT aids in crafting format-specific instructions from the intended outputs to populate this benchmark. Addressing the gap in task-centered evaluation, we propose two innovative metrics, P-Score (Prompting Score) and H-Score (Heuristical Score), to more accurately gauge LLM performance. Our experiments show that applying our structure-aware fine-tuning to LLaMA-7B leads to substantial performance gains, outshining its LLM counterparts across most measures. In-depth error analysis and creating an ability map across six dimensions, coverage, formatting, reasoning, comprehension, pragmatics, and hallucination, highlight areas for future enhancements and suggest forthcoming research trajectories. Our code and models can be found at https://github.com/gersteinlab/Struc-Bench.",
        "author": "Xiangru Tang; Yiming Zong; Jason Phang; Yilun Zhao; Wangchunshu Zhou; Arman Cohan; Mark Gerstein",
        "authorids": "/x/xiangru-tang/; /y/yiming-zong/; /j/jason-phang/; /y/yilun-zhao/; /w/wangchunshu-zhou/; /a/arman-cohan/; /m/mark-gerstein/",
        "bibtex": "@inproceedings{tang-etal-2024-struc,\n    title = \"Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?\",\n    author = \"Tang, Xiangru  and\n      Zong, Yiming  and\n      Phang, Jason  and\n      Zhao, Yilun  and\n      Zhou, Wangchunshu  and\n      Cohan, Arman  and\n      Gerstein, Mark\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.2/\",\n    doi = \"10.18653/v1/2024.naacl-short.2\",\n    pages = \"12--34\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.2.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.2/",
        "pdf_size": 4714583,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13324588471978787372&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Yale University; Zhejiang University; New York University; Yale University; Yale University; Yale University; Yale University",
        "aff_domain": "yale.edu; ; ; ; ; ; ",
        "email": "yale.edu; ; ; ; ; ; ",
        "github": "https://github.com/gersteinlab/Struc-Bench",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;0;0;0;0",
        "aff_unique_norm": "Yale University;Zhejiang University;New York University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.yale.edu;https://www.zju.edu.cn;https://www.nyu.edu",
        "aff_unique_abbr": "Yale;ZJU;NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.findings-naacl.1",
        "title": "Structured Pruning for Large Language Models Using Coupled Components Elimination and Minor Fine-tuning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) have demonstrated powerful capabilities in natural language processing, yet their vast number of parameters poses challenges for deployment and inference efficiency. Structured model pruning emerges as a viable approach to reduce model size and accelerate inference, without requiring specialized operators and libraries for deployment. However, structured pruning often severely weakens the model\u2019s capability.Despite repetitive fine-tuning can restore the capability to a certain extent, it impairs LLMs\u2019 utility as versatile problem solvers.To address this issue, we propose a novel structured pruning algorithm tailored for LLMs. It derives the importance of different components, namely rows and columns in parameter matrices, based on intermediate data dependencies. Then it removes coupled components across different layers simultaneously and preserves dependency relationships within remaining parameters, avoiding significant performance degradation. The pruned model requires only few epochs of fine-tuning to restore its performance, ensuring the model\u2019s ability to generalize.Empirical evaluations on LLaMA, Vicuna, and ChatGLM3 demonstrate our algorithm\u2019s efficacy, yielding 20% parameter reduction while retaining at least 94.4% of original performance metrics.",
        "author": "Honghe Zhang; XiaolongShi XiaolongShi; Jingwei Sun; Guangzhong Sun",
        "authorids": "/h/honghe-zhang/; /x/xiaolongshi-xiaolongshi/; /j/jingwei-sun/; /g/guangzhong-sun/",
        "bibtex": "@inproceedings{zhang-etal-2024-structured,\n    title = \"Structured Pruning for Large Language Models Using Coupled Components Elimination and Minor Fine-tuning\",\n    author = \"Zhang, Honghe  and\n      XiaolongShi, XiaolongShi  and\n      Sun, Jingwei  and\n      Sun, Guangzhong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.1/\",\n    doi = \"10.18653/v1/2024.findings-naacl.1\",\n    pages = \"1--12\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.1.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.1/",
        "pdf_size": 546380,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:F0GQ4tStMogJ:scholar.google.com/&scioq=Structured+Pruning+for+Large+Language+Models+Using+Coupled+Components+Elimination+and+Minor+Fine-tuning&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China; University of Science and Technology of China",
        "aff_domain": "mail.ustc.edu.cn;mail.ustc.edu.cn;ustc.edu.cn;ustc.edu.cn",
        "email": "mail.ustc.edu.cn;mail.ustc.edu.cn;ustc.edu.cn;ustc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Science and Technology of China",
        "aff_unique_dep": "",
        "aff_unique_url": "http://www.ustc.edu.cn",
        "aff_unique_abbr": "USTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.89",
        "title": "Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce sub-sentence encoder, a contrastively-learned contextual embedding model for fine-grained semantic representation of text. In contrast to the standard practice with sentence embeddings, where the meaning of an entire sequence of text is encoded into a fixed-length vector, the sub-sentence encoder learns to produce distinct contextual embeddings corresponding to different atomic propositions, i.e. atomic units of meaning expressed within a text sequence. The sub-sentence embeddings are contrastively learned to recognize (inferred) semantic equivalence between propositions across different text sequences. Our experiments show the effectiveness of sub-sentence encoders in applications, such as retrieving supporting facts for fine-grained text attribution or recognizing the conditional semantic similarity between texts. In practice, we demonstrate that sub-sentence encoders keep the same level of inference cost and space complexity compared to sentence encoders.",
        "author": "Sihao Chen; Hongming Zhang; Tong Chen; Ben Zhou; Wenhao Yu; Dian Yu; Baolin Peng; Hongwei Wang; Dan Roth; Dong Yu",
        "authorids": "/s/sihao-chen/; /h/hongming-zhang/; /t/tong-chen/; /b/ben-zhou/; /w/wenhao-yu/; /d/dian-yu/; /b/baolin-peng/; /h/hongwei-wang/; /d/dan-roth/; /d/dong-yu/",
        "bibtex": "@inproceedings{chen-etal-2024-sub,\n    title = \"Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations\",\n    author = \"Chen, Sihao  and\n      Zhang, Hongming  and\n      Chen, Tong  and\n      Zhou, Ben  and\n      Yu, Wenhao  and\n      Yu, Dian  and\n      Peng, Baolin  and\n      Wang, Hongwei  and\n      Roth, Dan  and\n      Yu, Dong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.89/\",\n    doi = \"10.18653/v1/2024.naacl-long.89\",\n    pages = \"1596--1609\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.89.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.89/",
        "pdf_size": 451912,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18179570441634803517&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Pennsylvania\u2663; Tencent AI Lab\u2662; University of Washington\u2661; University of Pennsylvania\u2663; Tencent AI Lab\u2662; Tencent AI Lab\u2662; Tencent AI Lab\u2662; Tencent AI Lab\u2662; University of Pennsylvania\u2663; Tencent AI Lab\u2662",
        "aff_domain": "cis.upenn.edu; ; ; ; ; ; ; ; ; ",
        "email": "cis.upenn.edu; ; ; ; ; ; ; ; ; ",
        "github": "https://github.com/schen149/sub-sentence-encoder",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;2;0;1;1;1;1;0;1",
        "aff_unique_norm": "University of Pennsylvania;Tencent;University of Washington",
        "aff_unique_dep": ";Tencent AI Lab;",
        "aff_unique_url": "https://www.upenn.edu;https://ai.tencent.com;https://www.washington.edu",
        "aff_unique_abbr": "UPenn;Tencent AI Lab;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;1;1;1;1;0;1",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.192",
        "title": "Subspace Representations for Soft Set Operations and Sentence Similarities",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In the field of natural language processing (NLP), continuous vector representations are crucial for capturing the semantic meanings of individual words. Yet, when it comes to the representations of sets of words, the conventional vector-based approaches often struggle with expressiveness and lack the essential set operations such as union, intersection, and complement. Inspired by quantum logic, we realize the representation of word sets and corresponding set operations within pre-trained word embedding spaces. By grounding our approach in the linear subspaces, we enable efficient computation of various set operations and facilitate the soft computation of membership functions within continuous spaces. Moreover, we allow for the computation of the F-score directly within word vectors, thereby establishing a direct link to the assessment of sentence similarity. In experiments with widely-used pre-trained embeddings and benchmarks, we show that our subspace-based set operations consistently outperform vector-based ones in both sentence similarity and set retrieval tasks.",
        "author": "Yoichi Ishibashi; Sho Yokoi; Katsuhito Sudoh; Satoshi Nakamura",
        "authorids": "/y/yoichi-ishibashi/; /s/sho-yokoi/; /k/katsuhito-sudoh/; /s/satoshi-nakamura/",
        "bibtex": "@inproceedings{ishibashi-etal-2024-subspace,\n    title = \"Subspace Representations for Soft Set Operations and Sentence Similarities\",\n    author = \"Ishibashi, Yoichi  and\n      Yokoi, Sho  and\n      Sudoh, Katsuhito  and\n      Nakamura, Satoshi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.192/\",\n    doi = \"10.18653/v1/2024.naacl-long.192\",\n    pages = \"3512--3524\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.192.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.192/",
        "pdf_size": 3207976,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5112162382052472887&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Nara Institute of Science and Technology+Kyoto University; Tohoku University+RIKEN; Nara Institute of Science and Technology; Nara Institute of Science and Technology",
        "aff_domain": "is.naist.jp;is.naist.jp;is.naist.jp;tohoku.ac.jp",
        "email": "is.naist.jp;is.naist.jp;is.naist.jp;tohoku.ac.jp",
        "github": "https://github.com/yoichi1484/subspace",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2+3;0;0",
        "aff_unique_norm": "Nara Institute of Science and Technology;Kyoto University;Tohoku University;RIKEN",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.nist.go.jp;https://www.kyoto-u.ac.jp;https://www.tohoku.ac.jp;https://www.riken.jp",
        "aff_unique_abbr": "NIST;Kyoto U;Tohoku U;RIKEN",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2024.findings-naacl.88",
        "title": "Subword Attention and Post-Processing for Rare and Unknown Contextualized Embeddings",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Word representations are an important aspect of Natural Language Processing (NLP). Representations are trained using large corpora, either as independent static embeddings or as part of a deep contextualized model. While word embeddings are useful, they struggle on rare and unknown words. As such, a large body of work has been done on estimating rare and unknown words. However, most of the methods focus on static embeddings, with few models focused on contextualized representations. In this work, we propose SPRUCE, a rare/unknown embedding architecture that focuses on contextualized representations. This architecture uses subword attention and embedding post-processing combined with the contextualized model to produce high quality embeddings. We then demonstrate these techniques lead to improved performance in most intrinsic and downstream tasks.",
        "author": "Raj Patel; Carlotta Domeniconi",
        "authorids": "/r/raj-patel/; /c/carlotta-domeniconi/",
        "bibtex": "@inproceedings{patel-domeniconi-2024-subword,\n    title = \"Subword Attention and Post-Processing for Rare and Unknown Contextualized Embeddings\",\n    author = \"Patel, Raj  and\n      Domeniconi, Carlotta\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.88/\",\n    doi = \"10.18653/v1/2024.findings-naacl.88\",\n    pages = \"1383--1389\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.88.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.88/",
        "pdf_size": 224998,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2414911650839576459&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Computer Science, George Mason University; Department of Computer Science, George Mason University",
        "aff_domain": "gmu.edu;gmu.edu",
        "email": "gmu.edu;gmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "George Mason University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.gmu.edu",
        "aff_unique_abbr": "GMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.227",
        "title": "SumCSE: Summary as a transformation for Contrastive Learning",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Sentence embedding models are typically trained using contrastive learning (CL), either using human annotations directly or by repurposing other annotated datasets. In this work, we explore the recently introduced paradigm of generating CL data using generative language models (LM). In CL for computer vision (CV), compositional transformations (series of operations applied over an image. e.g. cropping + color distortion) which modify the input/image to retain minimal information were shown to be very effective. We show that composition of a \u2018Summary\u2019 transformation with diverse paraphrasing/contradicting transformations accomplishes the same and works very well in CL for sentence embeddings. Our final generated dataset (using Vicuna-13B) significantly outperforms the previous best unsupervised method (using ChatGPT) by 1.8 points, and SimCSE, a strong supervised baseline by 0.3 points on the semantic text similarity (STS) benchmark.",
        "author": "Raghuveer Thirukovalluru; Xiaolan Wang; Jun Chen; Shuyang Li; Jie Lei; Rong Jin; Bhuwan Dhingra",
        "authorids": "/r/raghuveer-thirukovalluru/; /x/xiaolan-wang/; /j/jun-chen/; /s/shuyang-li/; /j/jie-lei/; /r/rong-jin/; /b/bhuwan-dhingra/",
        "bibtex": "@inproceedings{thirukovalluru-etal-2024-sumcse,\n    title = \"{S}um{CSE}: Summary as a transformation for Contrastive Learning\",\n    author = \"Thirukovalluru, Raghuveer  and\n      Wang, Xiaolan  and\n      Chen, Jun  and\n      Li, Shuyang  and\n      Lei, Jie  and\n      Jin, Rong  and\n      Dhingra, Bhuwan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.227/\",\n    doi = \"10.18653/v1/2024.findings-naacl.227\",\n    pages = \"3577--3588\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.227.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.227/",
        "pdf_size": 245153,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17341524565699179011&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Duke University; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Duke University",
        "aff_domain": "duke.edu; ; ; ; ; ; ",
        "email": "duke.edu; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;1;0",
        "aff_unique_norm": "Duke University;Meta",
        "aff_unique_dep": ";Meta AI",
        "aff_unique_url": "https://www.duke.edu;https://meta.com",
        "aff_unique_abbr": "Duke;Meta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.133",
        "title": "SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Cross-lingual summarization (XLS) generates summaries in a language different from that of the input documents (e.g., English to Spanish), allowing speakers of the target language to gain a concise view of their content. In the present day, the predominant approach to this task is to take a performing, pretrained multilingual language model (LM) and fine-tune it for XLS on the language pairs of interest. However, the scarcity of fine-tuning samples makes this approach challenging in some cases. For this reason, in this paper we propose revisiting the summarize-and-translate pipeline, where the summarization and translation tasks are performed in a sequence. This approach allows reusing the many, publicly-available resources for monolingual summarization and translation, obtaining a very competitive zero-shot performance. In addition, the proposed pipeline is completely differentiable end-to-end, allowing it to take advantage of few-shot fine-tuning, where available. Experiments over two contemporary and widely adopted XLS datasets (CrossSum and WikiLingua) have shown the remarkable zero-shot performance of the proposed approach, and also its strong few-shot performance compared to an equivalent multilingual LM baseline, that the proposed approach has been able to outperform in many languages with only 10% of the fine-tuning samples.",
        "author": "Jacob Parnell; Inigo Jauregi Unanue; Massimo Piccardi",
        "authorids": "/j/jacob-parnell/; /i/inigo-jauregi-unanue/; /m/massimo-piccardi/",
        "bibtex": "@inproceedings{parnell-etal-2024-sumtra,\n    title = \"{S}um{T}ra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization\",\n    author = \"Parnell, Jacob  and\n      Jauregi Unanue, Inigo  and\n      Piccardi, Massimo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.133/\",\n    doi = \"10.18653/v1/2024.naacl-long.133\",\n    pages = \"2399--2415\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.133.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.133/",
        "pdf_size": 2165345,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6568299947867592938&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Technology Sydney, Australia+RoZetta Technology, Australia; University of Technology Sydney, Australia+RoZetta Technology, Australia; University of Technology Sydney, Australia",
        "aff_domain": "rozettatechnology.com;rozettatechnology.com;uts.edu.au",
        "email": "rozettatechnology.com;rozettatechnology.com;uts.edu.au",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;0+1;0",
        "aff_unique_norm": "University of Technology Sydney;RoZetta Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uts.edu.au;",
        "aff_unique_abbr": "UTS;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "2024.naacl-long.438",
        "title": "SuperGLEBer: German Language Understanding Evaluation Benchmark",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We assemble a broad Natural Language Understanding benchmark suite for the German language and consequently evaluate a wide array of existing German-capable models in order to create a better understanding of the current state of German LLMs. Our benchmark consists of 29 different tasks ranging over different types such as document classification, sequence tagging, sentence similarity, and question answering, on which we evaluate 10 different German-pretrained models, thereby charting the landscape of German LLMs. In our comprehensive evaluation we find that encoder models are a good choice for most tasks, but also that the largest encoder model does not necessarily perform best for all tasks. We make our benchmark suite and a leaderboard publically available at https://supergleber.professor-x.de and encourage the community to contribute new tasks and evaluate more models on it (https://github.com/LSX-UniWue/SuperGLEBer).",
        "author": "Jan Pfister; Andreas Hotho",
        "authorids": "/j/jan-pfister/; /a/andreas-hotho/",
        "bibtex": "@inproceedings{pfister-hotho-2024-supergleber,\n    title = \"{S}uper{GLEB}er: {G}erman Language Understanding Evaluation Benchmark\",\n    author = \"Pfister, Jan  and\n      Hotho, Andreas\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.438/\",\n    doi = \"10.18653/v1/2024.naacl-long.438\",\n    pages = \"7904--7923\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.438.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.438/",
        "pdf_size": 208646,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7768531342528683083&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Data Science Chair, Center for Artificial Intelligence and Data Science (CAIDAS), Julius-Maximilians-Universit\u00e4t W\u00fcrzburg (JMU); Data Science Chair, Center for Artificial Intelligence and Data Science (CAIDAS), Julius-Maximilians-Universit\u00e4t W\u00fcrzburg (JMU)",
        "aff_domain": "informatik.uni-wuerzburg.de;informatik.uni-wuerzburg.de",
        "email": "informatik.uni-wuerzburg.de;informatik.uni-wuerzburg.de",
        "github": "github.com/LSX-UniWue/SuperGLEBer",
        "project": "supergleber.professor-x.de",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Julius-Maximilians-Universit\u00e4t W\u00fcrzburg",
        "aff_unique_dep": "Data Science Chair, Center for Artificial Intelligence and Data Science (CAIDAS)",
        "aff_unique_url": "https://www.jmu.de",
        "aff_unique_abbr": "JMU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "W\u00fcrzburg",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.findings-naacl.239",
        "title": "Synonym relations affect object detection learned on vision-language data",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We analyze whether object detectors trained on vision-language data learn effective visual representations for synonyms. Since many current vision-language models accept user-provided textual input, we highlight the need for such models to learn feature representations that are robust to changes in how such input is provided. Specifically, we analyze changes in synonyms used to refer to objects. Here, we study object detectors trained on vision-language data and investigate how to make their performance less dependent on whether synonyms are used to refer to an object. We propose two approaches to achieve this goal: data augmentation by back-translation and class embedding enrichment. We show the promise of such approaches, reporting improved performance on synonyms from mAP@0.5=33.87% to 37.93%.",
        "author": "Giacomo Nebbia; Adriana Kovashka",
        "authorids": "/g/giacomo-nebbia/; /a/adriana-kovashka/",
        "bibtex": "@inproceedings{nebbia-kovashka-2024-synonym,\n    title = \"Synonym relations affect object detection learned on vision-language data\",\n    author = \"Nebbia, Giacomo  and\n      Kovashka, Adriana\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.239/\",\n    doi = \"10.18653/v1/2024.findings-naacl.239\",\n    pages = \"3770--3776\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.239.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.239/",
        "pdf_size": 279504,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:oT0qJ9aXzewJ:scholar.google.com/&scioq=Synonym+relations+affect+object+detection+learned+on+vision-language+data&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "University of Pittsburgh; University of Pittsburgh",
        "aff_domain": "pitt.edu;cs.pitt.edu",
        "email": "pitt.edu;cs.pitt.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pittsburgh",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.pitt.edu",
        "aff_unique_abbr": "Pitt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.217",
        "title": "Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems using Differentially Private Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We address the challenge of ensuring differential privacy (DP) guarantees in training deep retrieval systems. Training these systems often involves the use of contrastive-style losses, which are typically non-per-example decomposable, making them difficult to directly DP-train with since common techniques require per-example gradients. To address this issue, we propose an approach that prioritizes ensuring query privacy prior to training a deep retrieval system. Our method employs DP language models (LMs) to generate private synthetic queries representative of the original data. These synthetic queries can be used in downstream retrieval system training without compromising privacy. Our approach demonstrates a significant enhancement in retrieval quality compared to direct DP-training, all while maintaining query-level privacy guarantees. This work highlights the potential of harnessing LMs to overcome limitations in standard DP-training methods.",
        "author": "Aldo Carranza; Rezsa Farahani; Natalia Ponomareva; Alexey Kurakin; Matthew Jagielski; Milad Nasr",
        "authorids": "/a/aldo-carranza/; /r/rezsa-farahani/; /n/natalia-ponomareva/; /a/alexey-kurakin/; /m/matthew-jagielski/; /m/milad-nasr/",
        "bibtex": "@inproceedings{carranza-etal-2024-synthetic,\n    title = \"Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems using Differentially Private Language Models\",\n    author = \"Carranza, Aldo  and\n      Farahani, Rezsa  and\n      Ponomareva, Natalia  and\n      Kurakin, Alexey  and\n      Jagielski, Matthew  and\n      Nasr, Milad\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.217/\",\n    doi = \"10.18653/v1/2024.naacl-long.217\",\n    pages = \"3920--3930\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.217.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.217/",
        "pdf_size": 291229,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18364716308829080059&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Stanford University; Google Inc.; Google Research; Google DeepMind; Google DeepMind; Google DeepMind",
        "aff_domain": "stanford.edu;google.com;google.com;google.com;google.com;google.com",
        "email": "stanford.edu;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "Stanford University;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.stanford.edu;https://www.google.com",
        "aff_unique_abbr": "Stanford;Google",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Stanford;Mountain View;",
        "aff_country_unique_index": "0;0;0;1;1;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2024.findings-naacl.74",
        "title": "T3M: Text Guided 3D Human Motion Synthesis from Speech",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Speech-driven 3D motion synthesis seeks to create lifelike animations based on human speech, with potential uses in virtual reality, gaming, and the film production. Existing approaches reply solely on speech audio for motion generation, leading to inaccurate and inflexible synthesis results. To mitigate this problem, we introduce a novel text-guided 3D human motion synthesis method, termed T3M. Unlike traditional approaches, T3M allows precise control over motion synthesis via textual input, enhancing the degree of diversity and user customization. The experiment results demonstrate that T3M can greatly outperform the state-of-the-art methods in both quantitative metrics and qualitative evaluations. We have publicly released our code at https://github.com/Gloria2tt/naacl2024.git",
        "author": "Wenshuo Peng; Kaipeng Zhang; Sai Qian Zhang",
        "authorids": "/w/wenshuo-peng/; /k/kaipeng-zhang/; /s/sai-qian-zhang/",
        "bibtex": "@inproceedings{peng-etal-2024-t3m,\n    title = \"{T}3{M}: Text Guided 3{D} Human Motion Synthesis from Speech\",\n    author = \"Peng, Wenshuo  and\n      Zhang, Kaipeng  and\n      Zhang, Sai Qian\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.74/\",\n    doi = \"10.18653/v1/2024.findings-naacl.74\",\n    pages = \"1168--1177\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.74.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.74/",
        "pdf_size": 2531457,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:JTirOKiJXzwJ:scholar.google.com/&scioq=T3M:+Text+Guided+3D+Human+Motion+Synthesis+from+Speech&hl=en&as_sdt=0,5",
        "gs_version_total": 7,
        "aff": "OpenGVLab, Shanghai AI Laboratory; OpenGVLab, Shanghai AI Laboratory; New York University",
        "aff_domain": "gmail.com;pjlab.org.cn;nyu.edu",
        "email": "gmail.com;pjlab.org.cn;nyu.edu",
        "github": "https://github.com/Gloria2tt/naacl2024.git",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Shanghai AI Laboratory;New York University",
        "aff_unique_dep": "OpenGVLab;",
        "aff_unique_url": ";https://www.nyu.edu",
        "aff_unique_abbr": ";NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.101",
        "title": "TISE: A Tripartite In-context Selection Method for Event Argument Extraction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In-context learning enhances the reasoning capabilities of LLMs by providing several examples. A direct yet effective approach to obtain in-context example is to select the top-k examples based on their semantic similarity to the test input. However, when applied to event argument extraction (EAE), this approach exhibits two shortcomings: 1) It may select almost identical examples, thus failing to provide additional event information, and 2) It overlooks event attributes, leading to the selected examples being unrelated to the test event type. In this paper, we introduce three necessary requirements when selecting an in-context example for EAE task: semantic similarity, example diversity and event correlation. And we further propose TISE, which scores examples from these three perspectives and integrates them using Determinantal Point Processes to directly select a set of examples as context. Experimental results on the ACE05 dataset demonstrate the effectiveness of TISE and the necessity of three requirements. Furthermore, we surprisingly observe that TISE can achieve superior performance with fewer examples and can even exceed some supervised methods.",
        "author": "Yanhe Fu; Yanan Cao; Qingyue Wang; Yi Liu",
        "authorids": "/y/yanhe-fu/; /y/yanan-cao/; /q/qingyue-wang/; /y/yi-liu/",
        "bibtex": "@inproceedings{fu-etal-2024-tise,\n    title = \"{TISE}: A Tripartite In-context Selection Method for Event Argument Extraction\",\n    author = \"Fu, Yanhe  and\n      Cao, Yanan  and\n      Wang, Qingyue  and\n      Liu, Yi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.101/\",\n    doi = \"10.18653/v1/2024.naacl-long.101\",\n    pages = \"1801--1818\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.101.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.101/",
        "pdf_size": 1038759,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3652454404665480744&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China+School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "email": "iie.ac.cn;iie.ac.cn;iie.ac.cn;iie.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Information Engineering;School of Cyber Security",
        "aff_unique_url": "http://www.cas.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-demo.1",
        "title": "TOPICAL: TOPIC Pages AutomagicaLly",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Topic pages aggregate useful information about an entity or concept into a single succinct and accessible article. Automated creation of topic pages would enable their rapid curation as information resources, providing an alternative to traditional web search. While most prior work has focused on generating topic pages about biographical entities, in this work, we develop a completely automated process to generate high-quality topic pages for scientific entities, with a focus on biomedical concepts. We release TOPICAL, a web app and associated open-source code, comprising a model pipeline combining retrieval, clustering, and prompting, that makes it easy for anyone to generate topic pages for a wide variety of biomedical entities on demand. In a human evaluation of 150 diverse topic pages generated using TOPICAL, we find that the vast majority were considered relevant, accurate, and coherent, with correct supporting citations. We make all code publicly available and host a free-to-use web app at: https://s2-topical.apps.allenai.org.",
        "author": "John Giorgi; Amanpreet Singh; Doug Downey; Sergey Feldman; Lucy Wang",
        "authorids": "/j/john-giorgi/; /a/amanpreet-singh/; /d/doug-downey/; /s/sergey-feldman/; /l/lucy-lu-wang/",
        "bibtex": "@inproceedings{giorgi-etal-2024-topical,\n    title = \"{TOPICAL}: {TOPIC} Pages {A}utomagica{L}ly\",\n    author = \"Giorgi, John  and\n      Singh, Amanpreet  and\n      Downey, Doug  and\n      Feldman, Sergey  and\n      Wang, Lucy\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.1/\",\n    doi = \"10.18653/v1/2024.naacl-demo.1\",\n    pages = \"1--11\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.1.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.1/",
        "pdf_size": 1925511,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15185291448360522243&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of Toronto+Terrence Donnelly Centre+Vector Institute for AI; Allen Institute for AI; Allen Institute for AI+Northwestern University; Allen Institute for AI; Allen Institute for AI+University of Washington",
        "aff_domain": "utoronto.ca; ;allenai.org;allenai.org;uw.edu",
        "email": "utoronto.ca; ;allenai.org;allenai.org;uw.edu",
        "github": "",
        "project": "https://s2-topical.apps.allenai.org",
        "author_num": 5,
        "aff_unique_index": "0+1+2;3;3+4;3;3+5",
        "aff_unique_norm": "University of Toronto;Terrence Donnelly Centre;Vector Institute for AI;Allen Institute for AI;Northwestern University;University of Washington",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.terrencedonnellycentre.ca;https://vectorinstitute.ai/;https://allenai.org;https://www.northwestern.edu;https://www.washington.edu",
        "aff_unique_abbr": "U of T;;Vector AI;AI2;NU;UW",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;1;1+1;1;1+1",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2024.naacl-long.210",
        "title": "TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "When applied to open-domain question answering, large language models (LLMs) frequently generate incorrect responses based on made-up facts, which are called hallucinations. Retrieval augmented generation (RAG) is a promising strategy to avoid hallucinations, but it does not provide guarantees on its correctness. To address this challenge, we propose the Trustworthy Retrieval Augmented Question Answering, or *TRAQ*, which provides the first end-to-end statistical correctness guarantee for RAG. TRAQ uses conformal prediction, a statistical technique for constructing prediction sets that are guaranteed to contain the semantically correct response with high probability. Additionally, TRAQ leverages Bayesian optimization to minimize the size of the constructed sets. In an extensive experimental evaluation, we demonstrate that TRAQ provides the desired correctness guarantee while reducing prediction set size by 16.2% on average compared to an ablation. The implementation is available: [https://github.com/shuoli90/TRAQ](https://github.com/shuoli90/TRAQ).",
        "author": "Shuo Li; Sangdon Park; Insup Lee; Osbert Bastani",
        "authorids": "/s/shuo-li/; /s/sangdon-park/; /i/insup-lee/; /o/osbert-bastani/",
        "bibtex": "@inproceedings{li-etal-2024-traq,\n    title = \"{TRAQ}: Trustworthy Retrieval Augmented Question Answering via Conformal Prediction\",\n    author = \"Li, Shuo  and\n      Park, Sangdon  and\n      Lee, Insup  and\n      Bastani, Osbert\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.210/\",\n    doi = \"10.18653/v1/2024.naacl-long.210\",\n    pages = \"3799--3821\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.210.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.210/",
        "pdf_size": 1601094,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12569899222674034755&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Pennsylvania; Pohang University of Science and Technology; University of Pennsylvania; University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;postech.ac.kr;cis.upenn.edu;seas.upenn.edu",
        "email": "seas.upenn.edu;postech.ac.kr;cis.upenn.edu;seas.upenn.edu",
        "github": "https://github.com/shuoli90/TRAQ",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Pennsylvania;Pohang University of Science and Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;https://www.postech.ac.kr",
        "aff_unique_abbr": "UPenn;POSTECH",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pohang",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "2024.naacl-long.103",
        "title": "TRUE-UIE: Two Universal Relations Unify Information Extraction Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Information extraction (IE) encounters challenges due to the variety of schemas and objectives that differ across tasks. Recent advancements hint at the potential for universal approaches to model such tasks, referred to as Universal Information Extraction (UIE). While handling diverse tasks in one model, their generalization is limited since they are actually learning task-specific knowledge.In this study, we introduce an innovative paradigm known as TRUE-UIE, wherein all IE tasks are aligned to learn the same goals: extracting mention spans and two universal relations named NEXT and IS. During the decoding process, the NEXT relation is utilized to group related elements, while the IS relation, in conjunction with structured language prompts, undertakes the role of type recognition. Additionally, we consider the sequential dependency of tokens during span extraction, an aspect often overlooked in prevalent models.Our empirical experiments indicate that TRUE-UIE achieves state-of-the-art performance on established benchmarks encompassing 16 datasets, spanning 7 diverse IE tasks. Further evaluations reveal that our approach effectively share knowledge between different IE tasks, showcasing significant transferability in zero-shot and few-shot scenarios.",
        "author": "Yucheng Wang; Bowen Yu; Yilin Liu; Shudong Lu",
        "authorids": "/y/yucheng-wang/; /b/bowen-yu/; /y/yilin-liu/; /s/shudong-lu/",
        "bibtex": "@inproceedings{wang-etal-2024-true,\n    title = \"{TRUE}-{UIE}: Two Universal Relations Unify Information Extraction Tasks\",\n    author = \"Wang, Yucheng  and\n      Yu, Bowen  and\n      Liu, Yilin  and\n      Lu, Shudong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.103/\",\n    doi = \"10.18653/v1/2024.naacl-long.103\",\n    pages = \"1863--1876\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.103.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.103/",
        "pdf_size": 1744972,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15671083534965346269&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Shanghai Artificial Intelligence Laboratory; Independent Researcher; University of Southern California; Beijing University of Posts and Telecommunications",
        "aff_domain": "gmail.com;gmail.com;usc.edu;bupt.edu.cn",
        "email": "gmail.com;gmail.com;usc.edu;bupt.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;3",
        "aff_unique_norm": "Shanghai Artificial Intelligence Laboratory;Independent Researcher;University of Southern California;Beijing University of Posts and Telecommunications",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "http://www.shailab.org/;;https://www.usc.edu;http://www.bupt.edu.cn/",
        "aff_unique_abbr": "Shanghai AI Lab;;USC;BUPT",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Los Angeles;Beijing",
        "aff_country_unique_index": "0;2;0",
        "aff_country_unique": "China;;United States"
    },
    {
        "id": "2024.naacl-long.320",
        "title": "TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Table reasoning is a challenging task that requires understanding both natural language questions and structured tabular data. Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation, but they often struggle with large tables due to their limited input length. In this paper, we propose TabSQLify, a novel method that leverages text-to-SQL generation to decompose tables into smaller and relevant sub-tables, containing only essential information for answering questions or verifying statements, before performing the reasoning task. In our comprehensive evaluation on four challenging datasets, our approach demonstrates comparable or superior performance compared to prevailing methods reliant on full tables as input. Moreover, our method can reduce the input context length significantly, making it more scalable and efficient for large-scale table reasoning applications. Our method performs remarkably well on the WikiTQ benchmark, achieving an accuracy of 64.7%. Additionally, on the TabFact benchmark, it achieves a high accuracy of 79.5%. These results surpass other LLM-based baseline models on gpt-3.5-turbo (chatgpt). TabSQLify can reduce the table size significantly alleviating the computational load on LLMs when handling large tables without compromising performance.",
        "author": "Md Mahadi Hasan Nahid; Davood Rafiei",
        "authorids": "/m/md-mahadi-hasan-nahid/; /d/davood-rafiei/",
        "bibtex": "@inproceedings{nahid-rafiei-2024-tabsqlify,\n    title = \"{T}ab{SQL}ify: Enhancing Reasoning Capabilities of {LLM}s Through Table Decomposition\",\n    author = \"Nahid, Md Mahadi Hasan  and\n      Rafiei, Davood\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.320/\",\n    doi = \"10.18653/v1/2024.naacl-long.320\",\n    pages = \"5725--5737\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.320.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.320/",
        "pdf_size": 386489,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17941109396741396243&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Alberta; University of Alberta",
        "aff_domain": "ualberta.ca;ualberta.ca",
        "email": "ualberta.ca;ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-long.335",
        "title": "TableLlama: Towards Open Large Generalist Models for Tables",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 5-44 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model\u2019s generalizability. We open-source our dataset and trained model to boost future work on developing open generalist models for tables.",
        "author": "Tianshu Zhang; Xiang Yue; Yifei Li; Huan Sun",
        "authorids": "/t/tianshu-zhang/; /x/xiang-yue/; /y/yifei-li/; /h/huan-sun/",
        "bibtex": "@inproceedings{zhang-etal-2024-tablellama,\n    title = \"{T}able{L}lama: Towards Open Large Generalist Models for Tables\",\n    author = \"Zhang, Tianshu  and\n      Yue, Xiang  and\n      Li, Yifei  and\n      Sun, Huan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.335/\",\n    doi = \"10.18653/v1/2024.naacl-long.335\",\n    pages = \"6024--6044\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.335.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.335/",
        "pdf_size": 1037290,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1228860701741428824&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University",
        "aff_domain": "osu.edu;osu.edu;osu.edu;osu.edu",
        "email": "osu.edu;osu.edu;osu.edu;osu.edu",
        "github": "",
        "project": "https://osu-nlp-group.github.io/TableLlama/",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Ohio State University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.osu.edu",
        "aff_unique_abbr": "OSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.101",
        "title": "TagDebias: Entity and Concept Tagging for Social Bias Mitigation in Pretrained Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pre-trained language models (PLMs) play a crucial role in various applications, including sensitive domains such as the hiring process. However, extensive research has unveiled that these models tend to replicate social biases present in their pre-training data, raising ethical concerns. In this study, we propose the TagDebias method, which proposes debiasing a dataset using type tags. It then proceeds to fine-tune PLMs on this debiased dataset. Experiments show that our proposed TagDebias model, when applied to a ranking task, exhibits significant improvements in bias scores.",
        "author": "Mehrnaz Moslemi; Amal Zouaq",
        "authorids": "/m/mehrnaz-moslemi/; /a/amal-zouaq/",
        "bibtex": "@inproceedings{moslemi-zouaq-2024-tagdebias,\n    title = \"{T}ag{D}ebias: Entity and Concept Tagging for Social Bias Mitigation in Pretrained Language Models\",\n    author = \"Moslemi, Mehrnaz  and\n      Zouaq, Amal\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.101/\",\n    doi = \"10.18653/v1/2024.findings-naacl.101\",\n    pages = \"1553--1567\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.101.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.101/",
        "pdf_size": 690869,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:fDuSkat1yL0J:scholar.google.com/&scioq=TagDebias:+Entity+and+Concept+Tagging+for+Social+Bias+Mitigation+in+Pretrained+Language+Models&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "Department of Computer and Software Engineering, LAMA-WeST Lab, Polytechnique Montr\u00e9al; Department of Computer and Software Engineering, LAMA-WeST Lab, Polytechnique Montr\u00e9al",
        "aff_domain": "polymtl.ca;polymtl.ca",
        "email": "polymtl.ca;polymtl.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Polytechnique Montr\u00e9al",
        "aff_unique_dep": "Department of Computer and Software Engineering",
        "aff_unique_url": "https://www.polymtl.ca",
        "aff_unique_abbr": "Polytechnique",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Montr\u00e9al",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.findings-naacl.164",
        "title": "Tailoring Vaccine Messaging with Common-Ground Opinions",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "One way to personalize chatbot interactions is by establishing common ground with the intended reader. A domain where establishing mutual understanding could be particularly impactful is vaccine concerns and misinformation. Vaccine interventions are forms of messaging which aim to answer concerns expressed about vaccination. Tailoring responses in this domain is difficult, since opinions often have seemingly little ideological overlap. We define the task of tailoring vaccine interventions to a Common-Ground Opinion (CGO). Tailoring responses to a CGO involves meaningfully improving the answer by relating it to an opinion or belief the reader holds. In this paper we introduce Tailor-CGO, a dataset for evaluating how well responses are tailored to provided CGOs. We benchmark several major LLMs on this task; finding GPT-4-Turbo performs significantly better than others. We also build automatic evaluation metrics, including an efficient and accurate BERT model that outperforms finetuned LLMs, investigate how to successfully tailor vaccine messaging to CGOs, and provide actionable recommendations from this investigation.Tailor-CGO dataset and code available at: https://github.com/rickardstureborg/tailor-cgo",
        "author": "Rickard Stureborg; Sanxing Chen; Roy Xie; Aayushi Patel; Christopher Li; Chloe Zhu; Tingnan Hu; Jun Yang; Bhuwan Dhingra",
        "authorids": "/r/rickard-stureborg/; /s/sanxing-chen/; /r/roy-xie/; /a/aayushi-patel/; /c/christopher-li/; /c/chloe-zhu/; /t/tingnan-hu/; /j/jun-yang/; /b/bhuwan-dhingra/",
        "bibtex": "@inproceedings{stureborg-etal-2024-tailoring,\n    title = \"Tailoring Vaccine Messaging with Common-Ground Opinions\",\n    author = \"Stureborg, Rickard  and\n      Chen, Sanxing  and\n      Xie, Roy  and\n      Patel, Aayushi  and\n      Li, Christopher  and\n      Zhu, Chloe  and\n      Hu, Tingnan  and\n      Yang, Jun  and\n      Dhingra, Bhuwan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.164/\",\n    doi = \"10.18653/v1/2024.findings-naacl.164\",\n    pages = \"2553--2575\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.164.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.164/",
        "pdf_size": 785501,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1297293468536921480&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Duke University; Duke University; Duke University; Duke University; Duke University; Duke University; Duke University; Duke University+Duke University; Duke University",
        "aff_domain": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;cs.duke.edu;cs.duke.edu",
        "email": "duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;duke.edu;cs.duke.edu;cs.duke.edu",
        "github": "https://github.com/rickardstureborg/tailor-cgo",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0+0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.221",
        "title": "Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In-Context Learning (ICL) is an emergent capability of Large Language Models (LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new tasks. Previous studies have shown that using LLMs\u2019 outputs as labels is effective in training models to select demonstrations. Such a label is expected to estimate utility of a demonstration in ICL; however, it has not been well understood how different labeling strategies affect results on target tasks. This paper presents an analysis on different utility functions by focusing on LLMs\u2019 output probability given ground-truth output, and task-specific reward given LLMs\u2019 prediction. Unlike the previous work, we introduce a novel labeling method, incremental utility, which estimates how much incremental knowledge is brought into the LLMs by a demonstration. We conduct experiments with instruction-tuned LLMs on binary/multi-class classification, segmentation, and translation across Arabic, English, Finnish, Japanese, and Spanish. Our results show that (1) the probability is effective when the probability values are distributed across the whole value range (on the classification tasks), and (2) the downstream metric is more robust when nuanced reward values are provided with long outputs (on the segmentation and translation tasks). We then show that the proposed incremental utility further helps ICL by contrasting how the LLMs perform with and without the demonstrations.",
        "author": "Kazuma Hashimoto; Karthik Raman; Michael Bendersky",
        "authorids": "/k/kazuma-hashimoto/; /k/karthik-raman/; /m/michael-bendersky/",
        "bibtex": "@inproceedings{hashimoto-etal-2024-take,\n    title = \"Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning\",\n    author = \"Hashimoto, Kazuma  and\n      Raman, Karthik  and\n      Bendersky, Michael\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.221/\",\n    doi = \"10.18653/v1/2024.naacl-long.221\",\n    pages = \"3973--3990\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.221.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.221/",
        "pdf_size": 709586,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17789582842628423543&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.findings-naacl.275",
        "title": "Targeted Augmentation for Low-Resource Event Extraction",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Addressing the challenge of low-resource information extraction remains an ongoing issue due to the inherent information scarcity within limited training examples. Existing data augmentation methods, considered potential solutions, struggle to strike a balance between weak augmentation (e.g., synonym augmentation) and drastic augmentation (e.g., conditional generation without proper guidance). This paper introduces a novel paradigm that employs targeted augmentation and back validation to produce augmented examples with enhanced diversity, polarity, accuracy, and coherence. Extensive experimental results demonstrate the effectiveness of the proposed paradigm. Furthermore, identified limitations are discussed, shedding light on areas for future improvement.",
        "author": "Sijia Wang; Lifu Huang",
        "authorids": "/s/sijia-wang/; /l/lifu-huang/",
        "bibtex": "@inproceedings{wang-huang-2024-targeted,\n    title = \"Targeted Augmentation for Low-Resource Event Extraction\",\n    author = \"Wang, Sijia  and\n      Huang, Lifu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.275/\",\n    doi = \"10.18653/v1/2024.findings-naacl.275\",\n    pages = \"4414--4428\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.275.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.275/",
        "pdf_size": 700415,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=72357144586361501&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Virginia Tech; Virginia Tech",
        "aff_domain": "vt.edu;vt.edu",
        "email": "vt.edu;vt.edu",
        "github": "https://github.com/VT-NLP/TALOR-EE",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Virginia Tech",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.vt.edu",
        "aff_unique_abbr": "VT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.179",
        "title": "Task-Agnostic Detector for Insertion-Based Backdoor Attacks",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Textual backdoor attacks pose significant security threats. Current detection approaches, typically relying on intermediate feature representation or reconstructing potential triggers, are task-specific and less effective beyond sentence classification, struggling with tasks like question answering and named entity recognition. We introduce TABDet (Task-Agnostic Backdoor Detector), a pioneering task-agnostic method for backdoor detection. TABDet leverages final layer logits combined with an efficient pooling technique, enabling unified logit representation across three prominent NLP tasks. TABDet can jointly learn from diverse task-specific models, demonstrating superior detection efficacy over traditional task-specific methods.",
        "author": "Weimin Lyu; Xiao Lin; Songzhu Zheng; Lu Pang; Haibin Ling; Susmit Jha; Chao Chen",
        "authorids": "/w/weimin-lyu/; /x/xiao-lin/; /s/songzhu-zheng/; /l/lu-pang/; /h/haibin-ling/; /s/susmit-jha/; /c/chao-chen/",
        "bibtex": "@inproceedings{lyu-etal-2024-task,\n    title = \"Task-Agnostic Detector for Insertion-Based Backdoor Attacks\",\n    author = \"Lyu, Weimin  and\n      Lin, Xiao  and\n      Zheng, Songzhu  and\n      Pang, Lu  and\n      Ling, Haibin  and\n      Jha, Susmit  and\n      Chen, Chao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.179/\",\n    doi = \"10.18653/v1/2024.findings-naacl.179\",\n    pages = \"2808--2822\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.179.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.179/",
        "pdf_size": 1792686,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3263267694201087414&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Stony Brook University; SRI International; Morgan Stanley; Department of Computer Science, Stony Brook University; Department of Computer Science, Stony Brook University; SRI International; Department of Computer Science, Stony Brook University",
        "aff_domain": "stonybrook.edu;stonybrook.edu;stonybrook.edu;stonybrook.edu;sri.com;sri.com;morganstanley.com",
        "email": "stonybrook.edu;stonybrook.edu;stonybrook.edu;stonybrook.edu;sri.com;sri.com;morganstanley.com",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;2;0;0;1;0",
        "aff_unique_norm": "Stony Brook University;SRI International;Morgan Stanley",
        "aff_unique_dep": "Department of Computer Science;;",
        "aff_unique_url": "https://www.stonybrook.edu;https://www.sri.com;https://www.morganstanley.com",
        "aff_unique_abbr": "SBU;SRI;Morgan Stanley",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stony Brook;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.287",
        "title": "Teaching Language Models to Self-Improve through Interactive Demonstrations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The self-improving ability of large language models (LLMs), enabled by prompting them to analyze and revise their own outputs, has garnered significant interest in recent research. However, this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more cost-effective and faster ones. To reduce this gap, we introduce TriPosT, a training algorithm that endows smaller models with such self-improvement ability, and show that our approach can improve LLaMA-7B\u2019s performance on math and reasoning tasks by up to 7.13%. In contrast to prior work, we achieve this by using the smaller model to interact with LLMs to collect feedback and improvements on *its own generations*. We then replay this experience to train the small model. Our experiments on four math and reasoning datasets show that the interactive experience of learning from and correcting its *own* mistakes is crucial for small models to improve their performance.",
        "author": "Xiao Yu; Baolin Peng; Michel Galley; Jianfeng Gao; Zhou Yu",
        "authorids": "/x/xiao-yu/; /b/baolin-peng/; /m/michel-galley/; /j/jianfeng-gao/; /z/zhou-yu/",
        "bibtex": "@inproceedings{yu-etal-2024-teaching,\n    title = \"Teaching Language Models to Self-Improve through Interactive Demonstrations\",\n    author = \"Yu, Xiao  and\n      Peng, Baolin  and\n      Galley, Michel  and\n      Gao, Jianfeng  and\n      Yu, Zhou\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.287/\",\n    doi = \"10.18653/v1/2024.naacl-long.287\",\n    pages = \"5127--5149\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.287.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.287/",
        "pdf_size": 678327,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4943252532549774445&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Columbia University; Microsoft Research; Microsoft Research; Microsoft Research; Columbia University",
        "aff_domain": "columbia.edu;global.tencent.com;microsoft.com;microsoft.com;columbia.edu",
        "email": "columbia.edu;global.tencent.com;microsoft.com;microsoft.com;columbia.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "Columbia University;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.columbia.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Columbia;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.210",
        "title": "Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper explores cost-efficient methods to adapt pretrained Large Language Models (LLMs) to new lower-resource languages, with a specific focus on Estonian. Leveraging the Llama 2 model, we investigate the impact of combining cross-lingual instruction-tuning with additional monolingual pretraining. Our results demonstrate that even a relatively small amount of additional monolingual pretraining followed by cross-lingual instruction-tuning significantly enhances results on Estonian. Furthermore, we showcase cross-lingual knowledge transfer from high-quality English instructions to Estonian, resulting in improvements in commonsense reasoning and multi-turn conversation capabilities. Our best model, named Llammas, represents the first open-source instruction-following LLM for Estonian. Additionally, we publish Alpaca-est, the first general task instruction dataset for Estonia. These contributions mark the initial progress in the direction of developing open-source LLMs for Estonian.",
        "author": "Hele-Andra Kuulmets; Taido Purason; Agnes Luhtaru; Mark Fishel",
        "authorids": "/h/hele-andra-kuulmets/; /t/taido-purason/; /a/agnes-luhtaru/; /m/mark-fishel/",
        "bibtex": "@inproceedings{kuulmets-etal-2024-teaching,\n    title = \"Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer\",\n    author = \"Kuulmets, Hele-Andra  and\n      Purason, Taido  and\n      Luhtaru, Agnes  and\n      Fishel, Mark\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.210/\",\n    doi = \"10.18653/v1/2024.findings-naacl.210\",\n    pages = \"3309--3325\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.210.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.210/",
        "pdf_size": 254200,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10924264571854764588&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Institute of Computer Science, University of Tartu; Institute of Computer Science, University of Tartu; Institute of Computer Science, University of Tartu; Institute of Computer Science, University of Tartu",
        "aff_domain": "ut.ee;ut.ee;ut.ee;ut.ee",
        "email": "ut.ee;ut.ee;ut.ee;ut.ee",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Tartu",
        "aff_unique_dep": "Institute of Computer Science",
        "aff_unique_url": "https://www.ut.ee",
        "aff_unique_abbr": "UT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Estonia"
    },
    {
        "id": "2024.findings-naacl.52",
        "title": "Teaching a Multilingual Large Language Model to Understand Multilingual Speech via Multi-Instructional Training",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent advancements in language modeling have led to the emergenceof Large Language Models (LLMs) capable ofvarious natural language processing tasks.Despite their success in text-based tasks, applying LLMs to the speech domainremains limited and challenging. This paper presents BLOOMZMMS, a novel modelthat integrates a multilingual LLM with a multilingual speech encoder,aiming to harness the capabilities of LLMs for speech recognition and beyond.Utilizing a multi-instructional training approach, we demonstrate the transferabilityof linguistic knowledge from the text to the speech modality.Our experiments, conducted on 1900 hours of transcribed data from 139 languages,establish that a multilingual speech representation can be effectivelylearned and aligned with a multilingual LLM. While this learned representationinitially shows limitations in task generalization, we address this issue bygenerating synthetic targets in a multi-instructional style.Our zero-shot evaluation results confirm the robustness of our approach acrossmultiple tasks, including speech translation and multilingual spoken languageunderstanding, thereby opening new avenues for applying LLMs in the speech domain.",
        "author": "Pavel Denisov; Thang Vu",
        "authorids": "/p/pavel-denisov/; /t/thang-vu/",
        "bibtex": "@inproceedings{denisov-vu-2024-teaching,\n    title = \"Teaching a Multilingual Large Language Model to Understand Multilingual Speech via Multi-Instructional Training\",\n    author = \"Denisov, Pavel  and\n      Vu, Thang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.52/\",\n    doi = \"10.18653/v1/2024.findings-naacl.52\",\n    pages = \"814--834\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.52.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.52/",
        "pdf_size": 421202,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2392633721930302808&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Stuttgart, Institute for Natural Language Processing, Germany; University of Stuttgart, Institute for Natural Language Processing, Germany",
        "aff_domain": "ims.uni-stuttgart.de;ims.uni-stuttgart.de",
        "email": "ims.uni-stuttgart.de;ims.uni-stuttgart.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Stuttgart",
        "aff_unique_dep": "Institute for Natural Language Processing",
        "aff_unique_url": "https://www.uni-stuttgart.de",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.5",
        "title": "TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue sys- tems to effectively respond to user requests. The emotions in a conversation can be identi- fied by the representations from various modal- ities, such as audio, visual, and text. How- ever, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal knowledge distillation to transfer information from a lan- guage model acting as the teacher to the non- verbal students, thereby optimizing the efficacy of the weak modalities. We then combine multi- modal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effec- tiveness of our components through additional experiments.",
        "author": "Taeyang Yun; Hyunkuk Lim; Jeonghwan Lee; Min Song",
        "authorids": "/t/taeyang-yun/; /h/hyunkuk-lim/; /j/jeonghwan-lee/; /m/min-song/",
        "bibtex": "@inproceedings{yun-etal-2024-telme,\n    title = \"{T}el{ME}: Teacher-leading Multimodal Fusion Network for Emotion Recognition in Conversation\",\n    author = \"Yun, Taeyang  and\n      Lim, Hyunkuk  and\n      Lee, Jeonghwan  and\n      Song, Min\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.5/\",\n    doi = \"10.18653/v1/2024.naacl-long.5\",\n    pages = \"82--95\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.5.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.5/",
        "pdf_size": 874696,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1619386227244180694&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Yonsei University, Seoul, South Korea; Yonsei University, Seoul, South Korea; Yonsei University, Seoul, South Korea; Yonsei University, Seoul, South Korea",
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "email": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr",
        "github": "https://www.github.com/yuntaeyang/TelME",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Yonsei University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.yonsei.ac.kr",
        "aff_unique_abbr": "Yonsei",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seoul",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.findings-naacl.66",
        "title": "Testing the Effect of Code Documentation on Large Language Model Code Understanding",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated impressive abilities in recent years with regards to code generation and understanding. However, little work has investigated how documentation and other code properties affect an LLM\u2019s ability to understand and generate code or documentation. We present an empirical analysis of how underlying properties of code or documentation can affect an LLM\u2019s capabilities. We show that providing an LLM with \u201cincorrect\u201d documentation can greatly hinder code understanding, while incomplete or missing documentation does not seem to significantly affect an LLM\u2019s ability to understand code.",
        "author": "William Macke; Michael Doyle",
        "authorids": "/w/william-macke/; /m/michael-doyle/",
        "bibtex": "@inproceedings{macke-doyle-2024-testing,\n    title = \"Testing the Effect of Code Documentation on Large Language Model Code Understanding\",\n    author = \"Macke, William  and\n      Doyle, Michael\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.66/\",\n    doi = \"10.18653/v1/2024.findings-naacl.66\",\n    pages = \"1044--1050\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.66.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.66/",
        "pdf_size": 175799,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14054757863923507726&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "The MITRE Corporation*; The MITRE Corporation*",
        "aff_domain": "mitre.org;mitre.org",
        "email": "mitre.org;mitre.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "MITRE Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mitre.org",
        "aff_unique_abbr": "MITRE",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.147",
        "title": "Testing the limits of logical reasoning in neural and hybrid models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "We study the ability of neural and hybrid models to generalize logical reasoning patterns. We created a series of tests for analyzing various aspects of generalization in the context of language and reasoning, focusing on compositionality and recursiveness. We used them to study the syllogistic logic in hybrid models, where the network assists in premise selection. We analyzed feed-forward, recurrent, convolutional, and transformer architectures. Our experiments demonstrate that even though the models can capture elementary aspects of the meaning of logical terms, they learn to generalize logical reasoning only to a limited degree.",
        "author": "Manuel Vargas Guzm\u00e1n; Jakub Szymanik; Maciej Malicki",
        "authorids": "/m/manuel-vargas-guzman/; /j/jakub-szymanik/; /m/maciej-malicki/",
        "bibtex": "@inproceedings{guzman-etal-2024-testing,\n    title = \"Testing the limits of logical reasoning in neural and hybrid models\",\n    author = \"Vargas Guzm{\\'a}n, Manuel  and\n      Szymanik, Jakub  and\n      Malicki, Maciej\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.147/\",\n    doi = \"10.18653/v1/2024.findings-naacl.147\",\n    pages = \"2267--2279\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.147.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.147/",
        "pdf_size": 442222,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15781946220008392254&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Warsaw; University of Trento; Institute of Mathematics of the Polish Academy of Sciences",
        "aff_domain": "uw.edu.pl;gmail.com;impan.pl",
        "email": "uw.edu.pl;gmail.com;impan.pl",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Warsaw;University of Trento;Polish Academy of Sciences",
        "aff_unique_dep": ";;Institute of Mathematics",
        "aff_unique_url": "https://www.uw.edu.pl;https://www.unitn.it;https://www.impan.pl/",
        "aff_unique_abbr": "UW;UniTN;IM PAN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Poland;Italy"
    },
    {
        "id": "2024.naacl-long.2",
        "title": "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The diffusion model, a new generative modeling paradigm, has achieved great success in image, audio, and video generation.However, considering the discrete categorical nature of the text, it is not trivial to extend continuous diffusion models to natural language. In this work, we propose SeqDiffuSeq, a text diffusion model, to approach sequence-to-sequence text generation with an encoder-decoder Transformer architecture.To improve the generation performance, SeqDiffuSeq is equipped with the self-conditioning technique and our newly proposed adaptive noise schedule technique. Self-conditioning enables SeqDiffuSeq to better use the predicted sequence information during the generation process.The adaptive noise schedule balances the difficulty of denoising across time steps at the token level.Experiment results illustrate the improved performance on five sequence-to-sequence generation tasks compared to other diffusion-based models regarding text quality and inference time.",
        "author": "Hongyi Yuan; Zheng Yuan; Chuanqi Tan; Fei Huang; Songfang Huang",
        "authorids": "/h/hongyi-yuan/; /z/zheng-yuan/; /c/chuanqi-tan/; /f/fei-huang/; /s/songfang-huang/",
        "bibtex": "@inproceedings{yuan-etal-2024-text,\n    title = \"Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation\",\n    author = \"Yuan, Hongyi  and\n      Yuan, Zheng  and\n      Tan, Chuanqi  and\n      Huang, Fei  and\n      Huang, Songfang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.2/\",\n    doi = \"10.18653/v1/2024.naacl-long.2\",\n    pages = \"22--39\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.2.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.2/",
        "pdf_size": 3282464,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14278672023404631556&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Tsinghua University+Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group; Alibaba Group",
        "aff_domain": "mails.tsinghua.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "email": "mails.tsinghua.edu.cn;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com;alibaba-inc.com",
        "github": "https://github.com/Yuanhy1997/SeqDiffuSeq",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;1;1;1",
        "aff_unique_norm": "Tsinghua University;Alibaba Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "THU;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.327",
        "title": "The ART of LLM Refinement: Ask, Refine, and Trust",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable generative abilities, but can they judge the quality of their own generations and self-improve?A popular concept, referred to as *self-refinement*, postulates that LLMs can detect and correct the errors in their generations when asked to do so. However, recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved. To address this, we propose a reasoning with a refinement strategy called *ART: Ask, Refine, and Trust*, which *asks* necessary questions to decide when an LLM should *refine* its output, and uses it to affirm or deny *trust* in its refinement by ranking the refinement and the initial prediction. On two multistep reasoning tasks of mathematical word problems (GSM8K) and question answering (StrategyQA), *ART* achieves a performance gain of +5 points over self-refinement baselines, while using a much smaller model as the decision maker. We believe that *ART* with smaller models, making refinement decisions can be a cost-effective alternative to fine-tuning LLMs.",
        "author": "Kumar Shridhar; Koustuv Sinha; Andrew Cohen; Tianlu Wang; Ping Yu; Ramakanth Pasunuru; Mrinmaya Sachan; Jason Weston; Asli Celikyilmaz",
        "authorids": "/k/kumar-shridhar/; /k/koustuv-sinha/; /a/andrew-cohen/; /t/tianlu-wang/; /p/ping-yu/; /r/ramakanth-pasunuru/; /m/mrinmaya-sachan/; /j/jason-weston/; /a/asli-celikyilmaz/",
        "bibtex": "@inproceedings{shridhar-etal-2024-art,\n    title = \"The {ART} of {LLM} Refinement: Ask, Refine, and Trust\",\n    author = \"Shridhar, Kumar  and\n      Sinha, Koustuv  and\n      Cohen, Andrew  and\n      Wang, Tianlu  and\n      Yu, Ping  and\n      Pasunuru, Ramakanth  and\n      Sachan, Mrinmaya  and\n      Weston, Jason  and\n      Celikyilmaz, Asli\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.327/\",\n    doi = \"10.18653/v1/2024.naacl-long.327\",\n    pages = \"5872--5883\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.327.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.327/",
        "pdf_size": 474535,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17572929012276315105&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "ETH Zurich; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; ETH Zurich; Meta AI; Meta AI",
        "aff_domain": "ethz.ch; ; ; ; ; ; ; ; ",
        "email": "ethz.ch; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;1;1;1;1;0;1;1",
        "aff_unique_norm": "ETH Zurich;Meta",
        "aff_unique_dep": ";Meta AI",
        "aff_unique_url": "https://www.ethz.ch;https://meta.com",
        "aff_unique_abbr": "ETHZ;Meta",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;1;0;1;1",
        "aff_country_unique": "Switzerland;United States"
    },
    {
        "id": "2024.naacl-long.353",
        "title": "The Bias Amplification Paradox in Text-to-Image Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Bias amplification is a phenomenon in which models exacerbate biases or stereotypes present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION) considerably. However, we discover that amplification can be largely attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while our prompts do not, which leads to a distribution shift and consequently inflates bias measures. Once we account for distributional differences between texts used for training and generation when evaluating amplification, we observe that amplification decreases drastically. Our findings illustrate the challenges of comparing biases in models and their training data, as well as evaluation more broadly, and highlight how confounding factors can impact analyses.",
        "author": "Preethi Seshadri; Sameer Singh; Yanai Elazar",
        "authorids": "/p/preethi-seshadri/; /s/sameer-singh/; /y/yanai-elazar/",
        "bibtex": "@inproceedings{seshadri-etal-2024-bias,\n    title = \"The Bias Amplification Paradox in Text-to-Image Generation\",\n    author = \"Seshadri, Preethi  and\n      Singh, Sameer  and\n      Elazar, Yanai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.353/\",\n    doi = \"10.18653/v1/2024.naacl-long.353\",\n    pages = \"6367--6384\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.353.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.353/",
        "pdf_size": 18154469,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17628153966821355008&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "UC Irvine; UC Irvine; Allen Institute for AI + University of Washington",
        "aff_domain": "uci.edu;uci.edu;gmail.com",
        "email": "uci.edu;uci.edu;gmail.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1+2",
        "aff_unique_norm": "University of California, Irvine;Allen Institute for AI;University of Washington",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uci.edu;https://allenai.org;https://www.washington.edu",
        "aff_unique_abbr": "UCI;AI2;UW",
        "aff_campus_unique_index": "0;0;",
        "aff_campus_unique": "Irvine;",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.113",
        "title": "The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT. This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of LLM\u2019s interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share experiences. We find that LLM responses are supportive and inclusive, outscoring humans. However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice. We discuss these challenges, demonstrate that a dedicated prompt can improve the performance, and propose a blueprint of an LLM-supporter that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses. Our annotated dataset is available for further research.*https://github.com/nitaytech/LGBTeenDataset",
        "author": "Shir Lissak; Nitay Calderon; Geva Shenkman; Yaakov Ophir; Eyal Fruchter; Anat Brunstein Klomek; Roi Reichart",
        "authorids": "/s/shir-lissak/; /n/nitay-calderon/; /g/geva-shenkman/; /y/yaakov-ophir/; /e/eyal-fruchter/; /a/anat-brunstein-klomek/; /r/roi-reichart/",
        "bibtex": "@inproceedings{lissak-etal-2024-colorful,\n    title = \"The Colorful Future of {LLM}s: Evaluating and Improving {LLM}s as Emotional Supporters for Queer Youth\",\n    author = \"Lissak, Shir  and\n      Calderon, Nitay  and\n      Shenkman, Geva  and\n      Ophir, Yaakov  and\n      Fruchter, Eyal  and\n      Brunstein Klomek, Anat  and\n      Reichart, Roi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.113/\",\n    doi = \"10.18653/v1/2024.naacl-long.113\",\n    pages = \"2040--2079\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.113.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.113/",
        "pdf_size": 1824321,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14851854918944585804&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "TFaculty of Data and Decision Sciences, Technion, IIT; TFaculty of Data and Decision Sciences, Technion, IIT; RBaruch Ivcher School of Psychology, Reichman University; AThe Education Department, Ariel University; MRambam Medical Center, Faculty of Medicine, Technicon, IIT; RBaruch Ivcher School of Psychology, Reichman University + TFaculty of Data and Decision Sciences, Technion, IIT; TFaculty of Data and Decision Sciences, Technion, IIT",
        "aff_domain": "campus.technion.ac.il; ; ; ; ; ; ",
        "email": "campus.technion.ac.il; ; ; ; ; ; ",
        "github": "https://github.com/nitaytech/LGBTeenDataset",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;2;3;1+0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology;Reichman University;Ariel University;Rambam Health Care Campus",
        "aff_unique_dep": "Faculty of Data and Decision Sciences;RBaruch Ivcher School of Psychology;The Education Department;Faculty of Medicine",
        "aff_unique_url": "https://www.technion.ac.il/en/;https://www.reichman.ac.il;https://www.ariel.ac.il;https://rambam.org.il",
        "aff_unique_abbr": "Technion;;Ariel U;Rambam",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Haifa",
        "aff_country_unique_index": "0;0;0;0;0;0+0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "2024.findings-naacl.228",
        "title": "The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This study investigates the consequences of training language models on synthetic data generated by their predecessors, an increasingly prevalent practice given the prominence of powerful generative models. Diverging from the usual emphasis on performance metrics, we focus on the impact of this training methodology on linguistic diversity, especially when conducted recursively over time. To assess this, we adapt and develop a set of novel metrics targeting lexical, syntactic, and semantic diversity, applying them in recursive finetuning experiments across various natural language generation tasks in English. Our findings reveal a consistent decrease in the diversity of the model outputs through successive iterations, especially remarkable for tasks demanding high levels of creativity. This trend underscores the potential risks of training language models on synthetic text, particularly concerning the preservation of linguistic richness. Our study highlights the need for careful consideration of the long-term effects of such training approaches on the linguistic capabilities of language models.",
        "author": "Yanzhu Guo; Guokan Shang; Michalis Vazirgiannis; Chlo\u00e9 Clavel",
        "authorids": "/y/yanzhu-guo/; /g/guokan-shang/; /m/michalis-vazirgiannis/; /c/chloe-clavel/",
        "bibtex": "@inproceedings{guo-etal-2024-curious,\n    title = \"The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text\",\n    author = \"Guo, Yanzhu  and\n      Shang, Guokan  and\n      Vazirgiannis, Michalis  and\n      Clavel, Chlo{\\'e}\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.228/\",\n    doi = \"10.18653/v1/2024.findings-naacl.228\",\n    pages = \"3589--3604\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.228.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.228/",
        "pdf_size": 475488,
        "gs_citation": 43,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5367755069505827381&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "\u00c9cole Polytechnique, Institut Polytechnique de Paris+Linagora; Mohamed bin Zayed University of Artificial Intelligence+Linagora; \u00c9cole Polytechnique, Institut Polytechnique de Paris; Inria",
        "aff_domain": "polytechnique.edu;mbzuai.ac.ae;lix.polytechnique.fr;inria.fr",
        "email": "polytechnique.edu;mbzuai.ac.ae;lix.polytechnique.fr;inria.fr",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;2+1;0;3",
        "aff_unique_norm": "Ecole Polytechnique;LINAGORA;Mohamed bin Zayed University of Artificial Intelligence;INRIA",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ecolepolytechnique.fr;https://www.linagora.com;https://mbzuai.ac.ae;https://www.inria.fr",
        "aff_unique_abbr": "X;;MBZUAI;Inria",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;1+0;0;0",
        "aff_country_unique": "France;United Arab Emirates"
    },
    {
        "id": "2024.naacl-long.157",
        "title": "The Effect of Data Partitioning Strategy on Model Generalizability: A Case Study of Morphological Segmentation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent work to enhance data partitioning strategies for more realistic model evaluation face challenges in providing a clear optimal choice. This study addresses these challenges, focusing on morphological segmentation and synthesizing limitations related to language diversity, adoption of multiple datasets and splits, and detailed model comparisons. Our study leverages data from 19 languages, including ten indigenous or endangered languages across 10 language families with diverse morphological systems (polysynthetic, fusional, and agglutinative) and different degrees of data availability. We conduct large-scale experimentation with varying sized combinations of training and evaluation sets as well as new test data. Our results show that, when faced with new test data: (1) models trained from random splits are able to achieve higher numerical scores; (2) model rankings derived from random splits tend to generalize more consistently.",
        "author": "Zoey Liu; Bonnie Dorr",
        "authorids": "/z/zoey-liu/; /b/bonnie-dorr/",
        "bibtex": "@inproceedings{liu-dorr-2024-effect,\n    title = \"The Effect of Data Partitioning Strategy on Model Generalizability: A Case Study of Morphological Segmentation\",\n    author = \"Liu, Zoey  and\n      Dorr, Bonnie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.157/\",\n    doi = \"10.18653/v1/2024.naacl-long.157\",\n    pages = \"2851--2864\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.157.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.157/",
        "pdf_size": 303433,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:cbKFiN-UlpYJ:scholar.google.com/&scioq=The+Effect+of+Data+Partitioning+Strategy+on+Model+Generalizability:+A+Case+Study+of+Morphological+Segmentation&hl=en&as_sdt=0,33",
        "gs_version_total": 6,
        "aff": "Department of Linguistics, University of Florida; Computer & Information Science & Engineering, Florida Institute for National Security, University of Florida",
        "aff_domain": "ufl.edu;ufl.edu",
        "email": "ufl.edu;ufl.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Florida",
        "aff_unique_dep": "Department of Linguistics",
        "aff_unique_url": "https://www.ufl.edu",
        "aff_unique_abbr": "UF",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.402",
        "title": "The Impact of Depth on Compositional Generalization in Transformer Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "To process novel sentences, language models (LMs) must generalize compositionally\u2014combine familiar elements in new ways. What aspects of a model\u2019s structure promote compositional generalization? Focusing on transformers, we test the hypothesis, motivated by theoretical and empirical work, that deeper transformers generalize more compositionally. Simply adding layers increases the total number of parameters; to address this confound between depth and size, we construct three classes of models which trade off depth for width such that the total number of parameters is kept constant (41M, 134M and 374M parameters). We pretrain all models as LMs and fine-tune them on tasks that test for compositional generalization. We report three main conclusions: (1) after fine-tuning, deeper models generalize more compositionally than shallower models do, but the benefit of additional layers diminishes rapidly; (2) within each family, deeper models show better language modeling performance, but returns are similarly diminishing; (3) the benefits of depth for compositional generalization cannot be attributed solely to better performance on language modeling. Because model latency is approximately linear in the number of layers, these results lead us to the recommendation that, with a given total parameter budget, transformers can be made shallower than is typical without sacrificing performance.",
        "author": "Jackson Petty; Sjoerd Steenkiste; Ishita Dasgupta; Fei Sha; Dan Garrette; Tal Linzen",
        "authorids": "/j/jackson-petty/; /s/sjoerd-steenkiste/; /i/ishita-dasgupta/; /f/fei-sha/; /d/dan-garrette/; /t/tal-linzen/",
        "bibtex": "@inproceedings{petty-etal-2024-impact,\n    title = \"The Impact of Depth on Compositional Generalization in Transformer Language Models\",\n    author = \"Petty, Jackson  and\n      Steenkiste, Sjoerd  and\n      Dasgupta, Ishita  and\n      Sha, Fei  and\n      Garrette, Dan  and\n      Linzen, Tal\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.402/\",\n    doi = \"10.18653/v1/2024.naacl-long.402\",\n    pages = \"7239--7252\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.402.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.402/",
        "pdf_size": 926692,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2369395255667303779&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "New York University; Google Research; Google DeepMind; Google Research; Google DeepMind; Google Research",
        "aff_domain": "nyu.edu;google.com;google.com;google.com;google.com;google.com",
        "email": "nyu.edu;google.com;google.com;google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;1",
        "aff_unique_norm": "New York University;Google",
        "aff_unique_dep": ";Google Research",
        "aff_unique_url": "https://www.nyu.edu;https://research.google",
        "aff_unique_abbr": "NYU;Google Research",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;1;0;1;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "2024.findings-naacl.249",
        "title": "The Impact of Differential Privacy on Group Disparity Mitigation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The performance cost of differential privacy has, for some applications, been shown to be higher for minority groups; fairness, conversely, has been shown to disproportionally compromise the privacy of members of such groups. Most work in this area has been restricted to computer vision and risk assessment. In response, we evaluate the impact of differential privacy on fairness across four diverse tasks, focusing on how attempts to mitigate privacy violations and between-group performance differences interact: Does privacy inhibit attempts to ensure fairness? To this end, we train (\ud835\udf00,\ud835\udeff)-differentially private models with empirical risk minimization and group distributionally robust training objectives. Consistent with previous findings, we find that differential privacy increases between-group performance differences in the baseline setting; more interestingly, differential privacy reduces between-group performance differences in the robust setting. We explain this by interpreting differential privacy as regularization.",
        "author": "Victor Hansen; Atula Neerkaje; Ramit Sawhney; Lucie Flek; Anders S\u00f8gaard",
        "authorids": "/v/victor-hansen/; /a/atula-neerkaje/; /r/ramit-sawhney/; /l/lucie-flek/; /a/anders-sogaard/",
        "bibtex": "@inproceedings{hansen-etal-2024-impact,\n    title = \"The Impact of Differential Privacy on Group Disparity Mitigation\",\n    author = \"Hansen, Victor  and\n      Neerkaje, Atula  and\n      Sawhney, Ramit  and\n      Flek, Lucie  and\n      S{\\o}gaard, Anders\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.249/\",\n    doi = \"10.18653/v1/2024.findings-naacl.249\",\n    pages = \"3952--3965\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.249.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.249/",
        "pdf_size": 533572,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10346088493544069909&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2024.naacl-short.53",
        "title": "The Impact of Language on Arithmetic Proficiency: A Multilingual Investigation with Cross-Agent Checking Computation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "This paper critically examines the arithmetic capabilities of Large Language Models (LLMs), uncovering significant limitations in their performance. Our research reveals a notable decline in accuracy for complex calculations involving large numbers, with addition and subtraction tasks showing varying degrees of proficiency. Additionally, we challenge the notion that arithmetic is language-independent, finding up to a 10% difference in performance across twenty languages. The study also compares self-verification methods with cross-agent collaborations, showing that a single model often outperforms collaborative approaches in basic arithmetic tasks. These findings suggest a need to reassess the effectiveness of LLMs in tasks requiring numerical accuracy and precision.",
        "author": "Chung-Chi Chen; Hiroya Takamura; Ichiro Kobayashi; Yusuke Miyao",
        "authorids": "/c/chung-chi-chen/; /h/hiroya-takamura/; /i/ichiro-kobayashi/; /y/yusuke-miyao/",
        "bibtex": "@inproceedings{chen-etal-2024-impact,\n    title = \"The Impact of Language on Arithmetic Proficiency: A Multilingual Investigation with Cross-Agent Checking Computation\",\n    author = \"Chen, Chung-Chi  and\n      Takamura, Hiroya  and\n      Kobayashi, Ichiro  and\n      Miyao, Yusuke\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.53/\",\n    doi = \"10.18653/v1/2024.naacl-short.53\",\n    pages = \"631--637\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.53.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.53/",
        "pdf_size": 272844,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2473179816544665312&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Artificial Intelligence Research Center, AIST, Japan; Artificial Intelligence Research Center, AIST, Japan; Ochanomizu University, Japan; University of Tokyo, Japan",
        "aff_domain": "acm.org;aist.go.jp;is.ocha.ac.jp;is.s.u-tokyo.ac.jp",
        "email": "acm.org;aist.go.jp;is.ocha.ac.jp;is.s.u-tokyo.ac.jp",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;2",
        "aff_unique_norm": "Advanced Institute of Science and Technology;Ochanomizu University;University of Tokyo",
        "aff_unique_dep": "Artificial Intelligence Research Center;;",
        "aff_unique_url": "https://www.aist.go.jp;https://www.ochanomizu-u.ac.jp;https://www.u-tokyo.ac.jp",
        "aff_unique_abbr": "AIST;Ochanomizu U;UTokyo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2024.naacl-long.369",
        "title": "The Integration of Semantic and Structural Knowledge in Knowledge Graph Entity Typing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The Knowledge Graph Entity Typing (KGET) task aims to predict missing type annotations for entities in knowledge graphs. Recent works only utilize the structural knowledge in the local neighborhood of entities, disregarding semantic knowledge in the textual representations of entities, relations, and types that are also crucial for type inference. Additionally, we observe that the interaction between semantic and structural knowledge can be utilized to address the false-negative problem. In this paper, we propose a novel Semantic and Structure-aware KG Entity Typing (SSET) framework, which is composed of three modules. First, the Semantic Knowledge Encoding module encodes factual knowledge in the KG with a Masked Entity Typing task. Then, the Structural Knowledge Aggregation module aggregates knowledge from the multi-hop neighborhood of entities to infer missing types. Finally, the Unsupervised Type Re-ranking module utilizes the inference results from the two models above to generate type predictions that are robust to false-negative samples. Extensive experiments show that SSET significantly outperforms existing state-of-the-art methods.",
        "author": "Muzhi Li; Minda Hu; Irwin King; Ho-fung Leung",
        "authorids": "/m/muzhi-li/; /m/minda-hu/; /i/irwin-king/; /h/ho-fung-leung/",
        "bibtex": "@inproceedings{li-etal-2024-integration,\n    title = \"The Integration of Semantic and Structural Knowledge in Knowledge Graph Entity Typing\",\n    author = \"Li, Muzhi  and\n      Hu, Minda  and\n      King, Irwin  and\n      Leung, Ho-fung\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.369/\",\n    doi = \"10.18653/v1/2024.naacl-long.369\",\n    pages = \"6625--6638\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.369.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.369/",
        "pdf_size": 474241,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2044705467393856382&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-long.126",
        "title": "The Perspectivist Paradigm Shift: Assumptions and Challenges of Capturing Human Labels",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Longstanding data labeling practices in machine learning involve collecting and aggregating labels from multiple annotators. But what should we do when annotators disagree? Though annotator disagreement has long been seen as a problem to minimize, new perspectivist approaches challenge this assumption by treating disagreement as a valuable source of information. In this position paper, we examine practices and assumptions surrounding the causes of disagreement\u2013some challenged by perspectivist approaches, and some that remain to be addressed\u2013as well as practical and normative challenges for work operating under these assumptions. We conclude with recommendations for the data labeling pipeline and avenues for future research engaging with subjectivity and disagreement.",
        "author": "Eve Fleisig; Su Lin Blodgett; Dan Klein; Zeerak Talat",
        "authorids": "/e/eve-fleisig/; /s/su-lin-blodgett/; /d/dan-klein/; /z/zeerak-talat/",
        "bibtex": "@inproceedings{fleisig-etal-2024-perspectivist,\n    title = \"The Perspectivist Paradigm Shift: Assumptions and Challenges of Capturing Human Labels\",\n    author = \"Fleisig, Eve  and\n      Blodgett, Su Lin  and\n      Klein, Dan  and\n      Talat, Zeerak\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.126/\",\n    doi = \"10.18653/v1/2024.naacl-long.126\",\n    pages = \"2279--2292\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.126.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.126/",
        "pdf_size": 165388,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13962653634607587471&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of California Berkeley; Microsoft Research Montr\u00e9al; University of California Berkeley; Mohamed Bin Zayed University of Artificial Intelligence",
        "aff_domain": "berkeley.edu;microsoft.com;berkeley.edu;Zeerak.org",
        "email": "berkeley.edu;microsoft.com;berkeley.edu;Zeerak.org",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "University of California, Berkeley;Microsoft;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";Microsoft Research;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com/en-us/research/group/microsoft-research-montreal;https://www.mbzuai.ac.ae",
        "aff_unique_abbr": "UC Berkeley;MSR Montreal;MBZUAI",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Berkeley;Montr\u00e9al;",
        "aff_country_unique_index": "0;1;0;2",
        "aff_country_unique": "United States;Canada;United Arab Emirates"
    },
    {
        "id": "2024.naacl-long.246",
        "title": "The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Assessing instruction quality is a fundamental component of any improvement efforts in the education system. However, traditional manual assessments are expensive, subjective, and heavily dependent on observers\u2019 expertise and idiosyncratic factors, preventing teachers from getting timely and frequent feedback. Different from prior research that mostly focuses on low-inference instructional practices on a singular basis, this paper presents the first study that leverages Natural Language Processing (NLP) techniques to assess multiple high-inference instructional practices in two distinct educational settings: in-person K-12 classrooms and simulated performance tasks for pre-service teachers. This is also the first study that applies NLP to measure a teaching practice that is widely acknowledged to be particularly effective for students with special needs. We confront two challenges inherent in NLP-based instructional analysis, including noisy and long input data and highly skewed distributions of human ratings. Our results suggest that pretrained Language Models (PLMs) demonstrate performances comparable to the agreement level of human raters for variables that are more discrete and require lower inference, but their efficacy diminishes with more complex teaching practices. Interestingly, using only teachers\u2019 utterances as input yields strong results for student-centered variables, alleviating common concerns over the difficulty of collecting and transcribing high-quality student speech data in in-person teaching settings. Our findings highlight both the potential and the limitations of current NLP techniques in the education domain, opening avenues for further exploration.",
        "author": "Paiheng Xu; Jing Liu; Nathan Jones; Julie Cohen; Wei Ai",
        "authorids": "/p/paiheng-xu/; /j/jing-liu/; /n/nathan-jones/; /j/julie-cohen/; /w/wei-ai/",
        "bibtex": "@inproceedings{xu-etal-2024-promises,\n    title = \"The Promises and Pitfalls of Using Language Models to Measure Instruction Quality in Education\",\n    author = \"Xu, Paiheng  and\n      Liu, Jing  and\n      Jones, Nathan  and\n      Cohen, Julie  and\n      Ai, Wei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.246/\",\n    doi = \"10.18653/v1/2024.naacl-long.246\",\n    pages = \"4375--4389\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.246.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.246/",
        "pdf_size": 1025802,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10539967213170122834&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "University of Maryland; University of Maryland; Boston University; University of Virginia; University of Maryland",
        "aff_domain": "cs.umd.edu;umd.edu;bu.edu;virginia.edu;umd.edu",
        "email": "cs.umd.edu;umd.edu;bu.edu;virginia.edu;umd.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "University of Maryland;Boston University;University of Virginia",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www/umd.edu;https://www.bu.edu;https://www.virginia.edu",
        "aff_unique_abbr": "UMD;BU;UVA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.382",
        "title": "The Role of n-gram Smoothing in the Age of Neural Networks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "For nearly three decades, language models derived from the n-gram assumption held the state of the art on the task. The key to their success lay in the application of various smoothing techniques that served to combat overfitting. However, when neural language models toppled n-gram models as the best performers, n-gram smoothing techniques became less relevant. Indeed, it would hardly be an understatement to suggest that the line of inquiry into n-gram smoothing techniques became dormant. This paper re-opens the role classical n-gram smoothing techniques may play in the age of neural language models. First, we draw a formal equivalence between label smoothing, a popular regularization technique for neural language models, and add-\ud835\udf06 smoothing. Second, we derive a generalized framework for converting any n-gram smoothing technique into a regularizer compatible with neural language models. Our empirical results find that our novel regularizers are comparable to and, indeed, sometimes outperform label smoothing on language modeling and machine translation.",
        "author": "Luca Malagutti; Andrius Buinovskij; Anej Svete; Clara Meister; Afra Amini; Ryan Cotterell",
        "authorids": "/l/luca-malagutti/; /a/andrius-buinovskij/; /a/anej-svete/; /c/clara-meister/; /a/afra-amini/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{malagutti-etal-2024-role,\n    title = \"The Role of $n$-gram Smoothing in the Age of Neural Networks\",\n    author = \"Malagutti, Luca  and\n      Buinovskij, Andrius  and\n      Svete, Anej  and\n      Meister, Clara  and\n      Amini, Afra  and\n      Cotterell, Ryan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.382/\",\n    doi = \"10.18653/v1/2024.naacl-long.382\",\n    pages = \"6882--6899\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.382.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.382/",
        "pdf_size": 500928,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6133406476309210105&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich; ETH Zurich",
        "aff_domain": "inf.ethz.ch;student.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;student.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "github": "https://github.com/rycolab/ngram_regularizers",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2024.naacl-short.56",
        "title": "The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Continuous-output neural machine translation (CoNMT) replaces the discrete next-word prediction problem with an embedding prediction.The semantic structure of the target embedding space (*i.e.*, closeness of related words) is intuitively believed to be crucial. We challenge this assumption and show that completely random output embeddings can outperform laboriously pre-trained ones, especially on larger datasets. Further investigation shows this surprising effect is strongest for rare words, due to the geometry of their embeddings. We shed further light on this finding by designing a mixed strategy that combines random and pre-trained embeddings, and that performs best overall.",
        "author": "Evgeniia Tokarchuk; Vlad Niculae",
        "authorids": "/e/evgeniia-tokarchuk/; /v/vlad-niculae/",
        "bibtex": "@inproceedings{tokarchuk-niculae-2024-unreasonable,\n    title = \"The Unreasonable Effectiveness of Random Target Embeddings for Continuous-Output Neural Machine Translation\",\n    author = \"Tokarchuk, Evgeniia  and\n      Niculae, Vlad\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.56/\",\n    doi = \"10.18653/v1/2024.naacl-short.56\",\n    pages = \"653--662\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.56.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.56/",
        "pdf_size": 618842,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11101451488230252&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Language Technology Lab, University of Amsterdam; Language Technology Lab, University of Amsterdam",
        "aff_domain": "uva.nl;uva.nl",
        "email": "uva.nl;uva.nl",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Amsterdam",
        "aff_unique_dep": "Language Technology Lab",
        "aff_unique_url": "https://www.uva.nl",
        "aff_unique_abbr": "UvA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "2024.findings-naacl.56",
        "title": "The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.",
        "author": "Lei Wang; Ee-Peng Lim",
        "authorids": "/l/lei-wang/; /e/ee-peng-lim/",
        "bibtex": "@inproceedings{wang-lim-2024-whole,\n    title = \"The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation\",\n    author = \"Wang, Lei  and\n      Lim, Ee-Peng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.56/\",\n    doi = \"10.18653/v1/2024.findings-naacl.56\",\n    pages = \"876--895\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.56.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.56/",
        "pdf_size": 8903076,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7628179965209143354&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Singapore Management University; Singapore Management University",
        "aff_domain": "smu.edu.sg;smu.edu.sg",
        "email": "smu.edu.sg;smu.edu.sg",
        "github": "https://github.com/demoleiwang/LLMSRec_Syn",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Singapore Management University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.smu.edu.sg",
        "aff_unique_abbr": "SMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2024.naacl-long.405",
        "title": "The steerability of large language models toward data-driven personas",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) are known to generate biased responses where the opinions of certain groups and populations are underrepresented. Here, we present a novel approach to achieve controllable generation of specific viewpoints using LLMs, that can be leveraged to produce multiple perspectives and to reflect the diverse opinions. Moving beyond the traditional reliance on demographics like age, gender, or party affiliation, we introduce a data-driven notion of persona grounded in collaborative filtering, which is defined as either a single individual or a cohort of individuals manifesting similar views across specific inquiries. As individuals in the same demographic group may have different personas, our data-driven persona definition allows for a more nuanced understanding of different (latent) social groups present in the population. In addition to this, we also explore an efficient method to steer LLMs toward the personas that we define. We show that our data-driven personas significantly enhance model steerability, with improvements of between 57%-77% over our best performing baselines.",
        "author": "Junyi Li; Charith Peris; Ninareh Mehrabi; Palash Goyal; Kai-Wei Chang; Aram Galstyan; Richard Zemel; Rahul Gupta",
        "authorids": "/j/junyi-li/; /c/charith-peris/; /n/ninareh-mehrabi/; /p/palash-goyal/; /k/kai-wei-chang/; /a/aram-galstyan/; /r/richard-zemel/; /r/rahul-gupta/",
        "bibtex": "@inproceedings{li-etal-2024-steerability,\n    title = \"The steerability of large language models toward data-driven personas\",\n    author = \"Li, Junyi  and\n      Peris, Charith  and\n      Mehrabi, Ninareh  and\n      Goyal, Palash  and\n      Chang, Kai-Wei  and\n      Galstyan, Aram  and\n      Zemel, Richard  and\n      Gupta, Rahul\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.405/\",\n    doi = \"10.18653/v1/2024.naacl-long.405\",\n    pages = \"7290--7305\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.405.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.405/",
        "pdf_size": 742121,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13988225952909420591&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Maryland, College Park; Amazon; Amazon; Amazon; Amazon; Amazon; Amazon; Amazon",
        "aff_domain": "gmail.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "gmail.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;1;1;1;1;1;1",
        "aff_unique_norm": "University of Maryland;Amazon",
        "aff_unique_dep": ";Amazon.com, Inc.",
        "aff_unique_url": "https://www/umd.edu;https://www.amazon.com",
        "aff_unique_abbr": "UMD;Amazon",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "College Park;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.43",
        "title": "The taste of IPA: Towards open-vocabulary keyword spotting and forced alignment in any language",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this project, we demonstrate that phoneme-based models for speech processing can achieve strong crosslinguistic generalizability to unseen languages. We curated the IPAPACK, a massively multilingual speech corpora with phonemic transcriptions, encompassing more than 115 languages from diverse language families, selectively checked by linguists. Based on the IPAPACK, we propose CLAP-IPA, a multi-lingual phoneme-speech contrastive embedding model capable of open-vocabulary matching between arbitrary speech signals and phonemic sequences. The proposed model was tested on 95 unseen languages, showing strong generalizability across languages. Temporal alignments between phonemes and speech signals also emerged from contrastive training, enabling zeroshot forced alignment in unseen languages. We further introduced a neural forced aligner IPA-ALIGNER by finetuning CLAP-IPA with the Forward-Sum loss to learn better phone-to-audio alignment. Evaluation results suggest that IPA-ALIGNER can generalize to unseen languages without adaptation.",
        "author": "Jian Zhu; Changbing Yang; Farhan Samir; Jahurul Islam",
        "authorids": "/j/jian-zhu/; /c/changbing-yang/; /f/farhan-samir/; /j/jahurul-islam/",
        "bibtex": "@inproceedings{zhu-etal-2024-taste,\n    title = \"The taste of {IPA}: Towards open-vocabulary keyword spotting and forced alignment in any language\",\n    author = \"Zhu, Jian  and\n      Yang, Changbing  and\n      Samir, Farhan  and\n      Islam, Jahurul\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.43/\",\n    doi = \"10.18653/v1/2024.naacl-long.43\",\n    pages = \"750--772\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.43.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.43/",
        "pdf_size": 459264,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17565636938979677999&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Linguistics, University of British Columbia + Natural Language Processing Group, University of British Columbia; Department of Linguistics, University of British Columbia + Natural Language Processing Group, University of British Columbia; Department of Linguistics, University of British Columbia + Natural Language Processing Group, University of British Columbia; Department of Linguistics, University of British Columbia + Natural Language Processing Group, University of British Columbia",
        "aff_domain": "ubc.ca;mail.ubc.ca;mail.ubc.ca; ",
        "email": "ubc.ca;mail.ubc.ca;mail.ubc.ca; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0+0;0+0;0+0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Linguistics",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-long.44",
        "title": "Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards Vision-Language Tasks",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Gender bias in vision-language models (VLMs) can reinforce harmful stereotypes and discrimination. In this paper, we focus on mitigating gender bias towards vision-language tasks. We identify object hallucination as the essence of gender bias in VLMs. Existing VLMs tend to focus on salient or familiar attributes in images but ignore contextualized nuances. Moreover, most VLMs rely on the co-occurrence between specific objects and gender attributes to infer the ignored features, ultimately resulting in gender bias. We propose GAMA, a task-agnostic generation framework to mitigate gender bias. GAMA consists of two stages: narrative generation and answer inference. During narrative generation, GAMA yields all-sided but gender-obfuscated narratives, which prevents premature concentration on localized image features, especially gender attributes. During answer inference, GAMA integrates the image, generated narrative, and a task-specific question prompt to infer answers for different vision-language tasks. This approach allows the model to rethink gender attributes and answers. We conduct extensive experiments on GAMA, demonstrating its debiasing and generalization ability.",
        "author": "Yunqi Zhang; Songda Li; Chunyuan Deng; Luyi Wang; Hui Zhao",
        "authorids": "/y/yunqi-zhang/; /s/songda-li/; /c/chunyuan-deng/; /l/luyi-wang/; /h/hui-zhao/",
        "bibtex": "@inproceedings{zhang-etal-2024-think,\n    title = \"Think Before You Act: A Two-Stage Framework for Mitigating Gender Bias Towards Vision-Language Tasks\",\n    author = \"Zhang, Yunqi  and\n      Li, Songda  and\n      Deng, Chunyuan  and\n      Wang, Luyi  and\n      Zhao, Hui\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.44/\",\n    doi = \"10.18653/v1/2024.naacl-long.44\",\n    pages = \"773--791\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.44.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.44/",
        "pdf_size": 2337046,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13131072028226643313&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Software Engineering Institute, East China Normal University; Software Engineering Institute, East China Normal University; Georgia Institute of Technology; Software Engineering Institute, East China Normal University; Software Engineering Institute, East China Normal University + Shanghai Key Laboratory of Trustworthy Computing, Shanghai, China",
        "aff_domain": "stu.ecnu.edu.cn;stu.ecnu.edu.cn;gatech.edu;stu.ecnu.edu.cn;sei.ecnu.edu.cn",
        "email": "stu.ecnu.edu.cn;stu.ecnu.edu.cn;gatech.edu;stu.ecnu.edu.cn;sei.ecnu.edu.cn",
        "github": "https://github.com/zyq0000/GAMA",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;0+2",
        "aff_unique_norm": "East China Normal University;Georgia Institute of Technology;Shanghai Key Laboratory of Trustworthy Computing",
        "aff_unique_dep": "Software Engineering Institute;;Trustworthy Computing",
        "aff_unique_url": "http://www.ecnu.edu.cn;https://www.gatech.edu;",
        "aff_unique_abbr": "ECNU;Georgia Tech;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shanghai",
        "aff_country_unique_index": "0;0;1;0;0+0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.findings-naacl.248",
        "title": "Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The emergence of large language models (LLMs) further improves the capabilities of open-domain dialogue systems and can generate fluent, coherent, and diverse responses. However, LLMs still lack a crucial ability: communication skills. This limitation renders them more like information seeking tools rather than anthropomorphic chatbots. Communication skills, such as topic transition, proactively asking questions, concept guidance, empathy, and summarising often should be taken into consideration, to make LLMs more anthropomorphic and proactive during the conversation, thereby increasing the interest of users and attracting them to chat for longer. However, enabling these communication skills in black-box LLMs remains a key challenge because they do not have the same utterance formation mode as real people: think before speaking. Inspired by linguistics and cognitive science, we empower LLMs with communication skills through inner monologues. To evaluate various communication skills, we construct a benchmark named Cskills, which can also more comprehensively evaluate the dialogue generation ability of the model. Experimental results show that the proposed CSIM strategy improves the backbone models and outperforms the baselines.",
        "author": "Junkai Zhou; Liang Pang; Huawei Shen; Xueqi Cheng",
        "authorids": "/j/junkai-zhou/; /l/liang-pang/; /h/huawei-shen/; /x/xueqi-cheng/",
        "bibtex": "@inproceedings{zhou-etal-2024-think,\n    title = \"Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue\",\n    author = \"Zhou, Junkai  and\n      Pang, Liang  and\n      Shen, Huawei  and\n      Cheng, Xueqi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.248/\",\n    doi = \"10.18653/v1/2024.findings-naacl.248\",\n    pages = \"3925--3951\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.248.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.248/",
        "pdf_size": 537563,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=634880642617654470&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "CAS Key Laboratory of AI Security, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China; CAS Key Laboratory of AI Security, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China; CAS Key Laboratory of AI Security, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China; CAS Key Laboratory of AI Security, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China + University of Chinese Academy of Sciences, Beijing, China",
        "aff_domain": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "email": "ict.ac.cn;ict.ac.cn;ict.ac.cn;ict.ac.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Computing Technology;",
        "aff_unique_url": "http://www.cas.ac.cn;http://www.ucas.ac.cn",
        "aff_unique_abbr": "CAS;UCAS",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.106",
        "title": "Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Knowledge-to-text generators often struggle to faithfully generate descriptions for the input facts: they may produce hallucinations that contradict the input, or describe facts not present in the input. To reduce hallucinations, we propose a decoding-only method, TWEAK (Think While Effectively Articulating Knowledge), which can be integrated with any generator without retraining. TWEAK treats the generated sequences at each decoding step and its future sequences as hypotheses, and ranks each generation candidate based on the extent to which their hypotheses are supported by the input facts using a Hypothesis Verification Model (HVM). We first demonstrate the effectiveness of TWEAK by using a Natural Language Inference (NLI) model as the HVM and report improved faithfulness with a minimal impact on the quality. We then replace the NLI model with a task-specific HVM trained with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which pairs input facts with their original and perturbed descriptions. We test TWEAK with two generators, and the best TWEAK variants improve on average for the two models by 2.24/7.17 points in faithfulness (FactKB) in in/out-of-distribution evaluations, respectively, and with only a 0.14/0.32-point decline in quality (BERTScore).",
        "author": "Yifu Qiu; Varun Embar; Shay Cohen; Benjamin Han",
        "authorids": "/y/yifu-qiu/; /v/varun-embar/; /s/shay-b-cohen/; /b/benjamin-han/",
        "bibtex": "@inproceedings{qiu-etal-2024-think,\n    title = \"Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation\",\n    author = \"Qiu, Yifu  and\n      Embar, Varun  and\n      Cohen, Shay  and\n      Han, Benjamin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.106/\",\n    doi = \"10.18653/v1/2024.findings-naacl.106\",\n    pages = \"1628--1644\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.106.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.106/",
        "pdf_size": 720211,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9612148323648250992&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Edinburgh1 + Apple2; Apple2; University of Edinburgh1; Apple2",
        "aff_domain": "ed.ac.uk;apple.com;ed.ac.uk;apple.com",
        "email": "ed.ac.uk;apple.com;ed.ac.uk;apple.com",
        "github": "https://github.com/apple/ml-tweak",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1;1;0;1",
        "aff_unique_norm": "University of Edinburgh;Apple",
        "aff_unique_dep": ";Apple Inc.",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.apple.com",
        "aff_unique_abbr": "Edinburgh;Apple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;0;1",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "2024.naacl-long.213",
        "title": "This Land is Your, My Land: Evaluating Geopolitical Bias in Language Models through Territorial Disputes",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Do the Spratly Islands belong to China, the Philippines, or Vietnam? A pretrained large language model (LLM) may answer differently if asked in the languages of each claimant country: Chinese, Tagalog, or Vietnamese. This contrasts with a multilingual human, who would likely answer consistently. In this paper, we show that LLMs recall certain geographical knowledge inconsistently when queried in different languages\u2014a phenomenon we term geopolitical bias. As a targeted case study, we consider territorial disputes, an inherently controversial and multilingual task. We introduce BorderLines, a dataset of territorial disputes which covers 251 territories, each associated with a set of multiple-choice questions in the languages of each claimant country (49 languages in total). We also propose a suite of evaluation metrics to precisely quantify bias and consistency in responses across different languages. We then evaluate various multilingual LLMs on our dataset and metrics to probe their internal knowledge and use the proposed metrics to discover numerous inconsistencies in how these models respond in different languages. Finally, we explore several prompt modification strategies, aiming to either amplify or mitigate geopolitical bias, which highlights how brittle LLMs are and how they tailor their responses depending on cues from the interaction context. Our code and data are available at https://github.com/manestay/borderlines.",
        "author": "Bryan Li; Samar Haider; Chris Callison-Burch",
        "authorids": "/b/bryan-li/; /s/samar-haider/; /c/chris-callison-burch/",
        "bibtex": "@inproceedings{li-etal-2024-land,\n    title = \"This Land is {Your, My} Land: Evaluating Geopolitical Bias in Language Models through Territorial Disputes\",\n    author = \"Li, Bryan  and\n      Haider, Samar  and\n      Callison-Burch, Chris\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.213/\",\n    doi = \"10.18653/v1/2024.naacl-long.213\",\n    pages = \"3855--3871\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.213.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.213/",
        "pdf_size": 835120,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14912469171052687312&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "https://github.com/manestay/borderlines",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.naacl-long.481",
        "title": "Tied-LoRA: Enhancing parameter efficiency of LoRA with Weight Tying",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce Tied-LoRA, a novel paradigm leveraging weight tying and selective training to enhance the parameter efficiency of Low-rank Adaptation (LoRA). Our exploration encompasses different plausible combinations of parameter training and freezing, coupled with weight tying, aimed at identifying the optimal trade-off between performance and the count of trainable parameters. Across 5 diverse tasks and two foundational language models with different parameter counts, our experiments provide comprehensive insights into the inherent trade-offs between efficiency and performance.Our findings reveal a specific Tied-LoRA configuration that distinguishes itself by showcasing comparable performance to LoRA across multiple tasks while utilizing only a fraction of the parameters employed by the standard LoRA method, particularly at elevated ranks. This underscores the efficacy of Tied-LoRA in achieving impressive results with significantly reduced model complexity.",
        "author": "Adithya Renduchintala; Tugrul Konuk; Oleksii Kuchaiev",
        "authorids": "/a/adithya-renduchintala/; /t/tugrul-konuk/; /o/oleksii-kuchaiev/",
        "bibtex": "@inproceedings{renduchintala-etal-2024-tied,\n    title = \"Tied-{L}o{RA}: Enhancing parameter efficiency of {L}o{RA} with Weight Tying\",\n    author = \"Renduchintala, Adithya  and\n      Konuk, Tugrul  and\n      Kuchaiev, Oleksii\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.481/\",\n    doi = \"10.18653/v1/2024.naacl-long.481\",\n    pages = \"8694--8705\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.481.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.481/",
        "pdf_size": 316589,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12982897186020268068&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "NVIDIA; NVIDIA; NVIDIA",
        "aff_domain": "nvidia.com;nvidia.com;nvidia.com",
        "email": "nvidia.com;nvidia.com;nvidia.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "NVIDIA",
        "aff_unique_dep": "NVIDIA Corporation",
        "aff_unique_url": "https://www.nvidia.com",
        "aff_unique_abbr": "NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.208",
        "title": "Time Machine GPT",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large language models (LLMs) are often trained on extensive, temporally indiscriminate text corpora, reflecting the lack of datasets with temporal metadata. This approach is not aligned with the evolving nature of language. Conventional methods for creating temporally adapted language models often depend on further pre-training static models on time-specific data. This paper presents a new approach: a series of point-in-time LLMs called TimeMachineGPT (TiMaGPT), specifically designed to be nonprognosticative. This ensures they remain uninformed about future factual information and linguistic changes. This strategy is beneficial for understanding language evolution and is of critical importance when applying models in dynamic contexts, such as time-series forecasting, where foresight of future information can prove problematic. We provide access to both the models and training datasets.",
        "author": "Felix Drinkall; Eghbal Rahimikia; Janet Pierrehumbert; Stefan Zohren",
        "authorids": "/f/felix-drinkall/; /e/eghbal-rahimikia/; /j/janet-pierrehumbert/; /s/stefan-zohren/",
        "bibtex": "@inproceedings{drinkall-etal-2024-time,\n    title = \"Time Machine {GPT}\",\n    author = \"Drinkall, Felix  and\n      Rahimikia, Eghbal  and\n      Pierrehumbert, Janet  and\n      Zohren, Stefan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.208/\",\n    doi = \"10.18653/v1/2024.findings-naacl.208\",\n    pages = \"3281--3292\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.208.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.208/",
        "pdf_size": 452048,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11408459584688864980&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Engineering Science, University of Oxford; Alliance Manchester Business School, University of Manchester; Faculty of Linguistics, University of Oxford + The Alan Turing Institute; The Alan Turing Institute",
        "aff_domain": "eng.ox.ac.uk; ; ; ",
        "email": "eng.ox.ac.uk; ; ; ",
        "github": "",
        "project": "https://huggingface.co/Ti-Ma",
        "author_num": 4,
        "aff_unique_index": "0;1;0+2;2",
        "aff_unique_norm": "University of Oxford;University of Manchester;Alan Turing Institute",
        "aff_unique_dep": "Department of Engineering Science;Alliance Manchester Business School;",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.manchester.ac.uk;https://www.turing.ac.uk",
        "aff_unique_abbr": "Oxford;UM;ATI",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Oxford;Manchester;",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-industry.33",
        "title": "Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, Compact LLMs are a good alternative to the comparatively Larger LLMs to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (FLAN-T5, TinyLLaMA, LiteLLaMA, etc.) with zero-shot larger LLMs (LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which achieves performance on par with zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller. This makes compact LLMs like FLAN-T5 a suitable cost-efficient LLM for real-world industrial deployment.",
        "author": "Xue-Yong Fu; Md Tahmid Rahman Laskar; Elena Khasanova; Cheng Chen; Shashi Tn",
        "authorids": "/x/xue-yong-fu/; /m/md-tahmid-rahman-laskar/; /e/elena-khasanova/; /c/cheng-chen/; /s/shashi-tn/",
        "bibtex": "@inproceedings{fu-etal-2024-tiny,\n    title = \"Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?\",\n    author = \"Fu, Xue-Yong  and\n      Laskar, Md Tahmid Rahman  and\n      Khasanova, Elena  and\n      Chen, Cheng  and\n      Tn, Shashi\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.33/\",\n    doi = \"10.18653/v1/2024.naacl-industry.33\",\n    pages = \"387--394\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.33.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.33/",
        "pdf_size": 193845,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=496245229103082624&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2024.naacl-long.470",
        "title": "To Tell The Truth: Language of Deception and Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text-based false information permeates online discourses, yet evidence of people\u2019s ability to discern truth from such deceptive textual content is scarce. We analyze a novel TV game show data where conversations in a high-stake environment between individuals with conflicting objectives result in lies. We investigate the manifestation of potentially verifiable language cues of deception in the presence of objective truth, a distinguishing feature absent in previous text-based deception datasets. We show that there exists a class of detectors (algorithms) that have similar truth detection performance compared to human subjects, even when the former accesses only the language cues while the latter engages in conversations with complete access to all potential sources of cues (language and audio-visual). Our model, built on a large language model, employs a bottleneck framework to learn discernible cues to determine truth, an act of reasoning in which human subjects often perform poorly, even with incentives. Our model detects novel but accurate language cues in many cases where humans failed to detect deception, opening up the possibility of humans collaborating with algorithms and ameliorating their ability to detect the truth.",
        "author": "Sanchaita Hazra; Bodhisattwa Prasad Majumder",
        "authorids": "/s/sanchaita-hazra/; /b/bodhisattwa-prasad-majumder/",
        "bibtex": "@inproceedings{hazra-majumder-2024-tell,\n    title = \"To Tell The Truth: Language of Deception and Language Models\",\n    author = \"Hazra, Sanchaita  and\n      Majumder, Bodhisattwa Prasad\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.470/\",\n    doi = \"10.18653/v1/2024.naacl-long.470\",\n    pages = \"8506--8520\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.470.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.470/",
        "pdf_size": 1469735,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7787510868812596302&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 3,
        "aff": "University of Utah; Allen Institute for AI",
        "aff_domain": "utah.edu;allenai.org",
        "email": "utah.edu;allenai.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Utah;Allen Institute for AI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utah.edu;https://allenai.org",
        "aff_unique_abbr": "Utah;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.298",
        "title": "To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Perfect machine translation (MT) would render cross-lingual transfer (XLT) by means of multilingual language models (mLMs) superfluous. Given, on the one hand, the large body of work on improving XLT with mLMs and, on the other hand, recent advances in massively multilingual MT, in this work, we systematically evaluate existing and propose new translation-based XLT approaches for transfer to low-resource languages. We show that all translation-based approaches dramatically outperform zero-shot XLT with mLMs\u2014with the combination of round-trip translation of the source-language training data and the translation of the target-language test instances at inference\u2014being generally the most effective. We next show that one can obtain further empirical gains by adding reliable translations to other high-resource languages to the training data. Moreover, we propose an effective translation-based XLT strategy even for languages not supported by the MT system. Finally, we show that model selection for XLT based on target-language validation data obtained with MT outperforms model selection based on the source-language data. We believe our findings warrant a broader inclusion of more robust translation-based baselines in XLT research.",
        "author": "Benedikt Ebing; Goran Glava\u0161",
        "authorids": "/b/benedikt-ebing/; /g/goran-glavas/",
        "bibtex": "@inproceedings{ebing-glavas-2024-translate,\n    title = \"To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages\",\n    author = \"Ebing, Benedikt  and\n      Glava{\\v{s}}, Goran\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.298/\",\n    doi = \"10.18653/v1/2024.naacl-long.298\",\n    pages = \"5325--5344\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.298.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.298/",
        "pdf_size": 342646,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18122839071633564102&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of W\u00fcrzburg+Center for Artificial Intelligence and Data Science (CAIDAS); University of W\u00fcrzburg+Center for Artificial Intelligence and Data Science (CAIDAS)",
        "aff_domain": "uni-wuerzburg.de;uni-wuerzburg.de",
        "email": "uni-wuerzburg.de;uni-wuerzburg.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of W\u00fcrzburg;Center for Artificial Intelligence and Data Science",
        "aff_unique_dep": ";Artificial Intelligence and Data Science",
        "aff_unique_url": "https://www.uni-wuerzburg.de;",
        "aff_unique_abbr": "UWue;CAIDAS",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "2024.naacl-long.359",
        "title": "ToXCL: A Unified Framework for Toxic Speech Detection and Explanation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The proliferation of online toxic speech is a pertinent problem posing threats to demographic groups. While explicit toxic speech contains offensive lexical signals, implicit one consists of coded or indirect language. Therefore, it is crucial for models not only to detect implicit toxic speech but also to explain its toxicity. This draws a unique need for unified frameworks that can effectively detect and explain implicit toxic speech. Prior works mainly formulated the task of toxic speech detection and explanation as a text generation problem. Nonetheless, models trained using this strategy can be prone to suffer from the consequent error propagation problem. Moreover, our experiments reveal that the detection results of such models are much lower than those that focus only on the detection task. To bridge these gaps, we introduce ToXCL, a unified framework for the detection and explanation of implicit toxic speech. Our model consists of three modules: a (i) Target Group Generator to generate the targeted demographic group(s) of a given post; an (ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit toxic speech and is boosted by a (iii) Teacher Classifier via knowledge distillation, and the decoder generates the necessary explanation. ToXCL achieves new state-of-the-art effectiveness, and outperforms baselines significantly.",
        "author": "Nhat M. Hoang; Xuan Long Do; Duc Anh Do; Duc Anh Vu; Luu Anh Tuan",
        "authorids": "/n/nhat-m-hoang/; /x/xuan-long-do/; /d/duc-anh-do/; /d/duc-anh-vu/; /l/luu-anh-tuan/",
        "bibtex": "@inproceedings{hoang-etal-2024-toxcl,\n    title = \"{T}o{XCL}: A Unified Framework for Toxic Speech Detection and Explanation\",\n    author = \"Hoang, Nhat M.  and\n      Do, Xuan Long  and\n      Do, Duc Anh  and\n      Vu, Duc Anh  and\n      Anh Tuan, Luu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.359/\",\n    doi = \"10.18653/v1/2024.naacl-long.359\",\n    pages = \"6460--6472\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.359.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.359/",
        "pdf_size": 341425,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10076647252264846324&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Nanyang Technological University, Singapore; National University of Singapore + Institute for Infocomm Research (I2R), A*STAR; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore",
        "aff_domain": "e.ntu.edu.sg;u.nus.edu;e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg",
        "email": "e.ntu.edu.sg;u.nus.edu;e.ntu.edu.sg;e.ntu.edu.sg;ntu.edu.sg",
        "github": "https://github.com/NhatHoang2002/ToXCL",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1+2;0;0;0",
        "aff_unique_norm": "Nanyang Technological University;National University of Singapore;Institute for Infocomm Research",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.nus.edu.sg;https://www.i2r.a-star.edu.sg",
        "aff_unique_abbr": "NTU;NUS;I2R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "Singapore"
    },
    {
        "id": "2024.naacl-long.251",
        "title": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence- level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model\u2019s size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators.",
        "author": "Liyan Tang; Igor Shalyminov; Amy Wong; Jon Burnsky; Jake Vincent; Yu\u2019an Yang; Siffi Singh; Song Feng; Hwanjun Song; Hang Su; Lijia Sun; Yi Zhang; Saab Mansour; Kathleen McKeown",
        "authorids": "/l/liyan-tang/; /i/igor-shalyminov/; /a/amy-wong/; /j/jon-burnsky/; /j/jake-vincent/; /y/yuan-yang/; /s/siffi-singh/; /s/song-feng/; /h/hwanjun-song/; /h/hang-su/; /l/lijia-sun/; /y/yi-zhang/; /s/saab-mansour/; /k/kathleen-mckeown/",
        "bibtex": "@inproceedings{tang-etal-2024-tofueval,\n    title = \"{T}ofu{E}val: Evaluating Hallucinations of {LLM}s on Topic-Focused Dialogue Summarization\",\n    author = \"Tang, Liyan  and\n      Shalyminov, Igor  and\n      Wong, Amy  and\n      Burnsky, Jon  and\n      Vincent, Jake  and\n      Yang, Yu{'}an  and\n      Singh, Siffi  and\n      Feng, Song  and\n      Song, Hwanjun  and\n      Su, Hang  and\n      Sun, Lijia  and\n      Zhang, Yi  and\n      Mansour, Saab  and\n      McKeown, Kathleen\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.251/\",\n    doi = \"10.18653/v1/2024.naacl-long.251\",\n    pages = \"4455--4480\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.251.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.251/",
        "pdf_size": 3520082,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1492181813930708430&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "AWS AI Labs\u2660; AWS AI Labs\u2660; AWS AI Labs\u2660; AWS AI Labs\u2660; AWS AI Labs\u2660; AWS AI Labs\u2660; AWS AI Labs\u2660; AWS AI Labs\u2660; Korea Advanced Institute of Science & Technology\u2661; AWS AI Labs\u2660; AWS AI Labs\u2660; AWS AI Labs\u2660; AWS AI Labs\u2660; AWS AI Labs\u2660",
        "aff_domain": "amazon.com; ; ; ; ; ; ; ; ; ; ; ; ;",
        "email": "amazon.com; ; ; ; ; ; ; ; ; ; ; ; ;",
        "github": "github.com/amazon-science/tofueval",
        "project": "",
        "author_num": 14,
        "aff_unique_index": "0;0;0;0;0;0;0;0;1;0;0;0;0;0",
        "aff_unique_norm": "Amazon;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "AWS AI Labs;",
        "aff_unique_url": "https://aws.amazon.com;https://www.kaist.ac.kr",
        "aff_unique_abbr": "AWS;KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;1;0;0;0;0;0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "2024.findings-naacl.113",
        "title": "Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Gender-inclusive NLP research has documented the harmful limitations of gender binary-centric large language models (LLM), such as the inability to correctly use gender-diverse English neopronouns (e.g., xe, zir, fae). While data scarcity is a known culprit, the precise mechanisms through which scarcity affects this behavior remain underexplored. We discover LLM misgendering is significantly influenced by Byte-Pair Encoding (BPE) tokenization, the tokenizer powering many popular LLMs. Unlike binary pronouns, BPE overfragments neopronouns, a direct consequence of data scarcity during tokenizer training. This disparate tokenization mirrors tokenizer limitations observed in multilingual and low-resource NLP, unlocking new misgendering mitigation strategies. We propose two techniques: (1) pronoun tokenization parity, a method to enforce consistent tokenization across gendered pronouns, and (2) utilizing pre-existing LLM pronoun knowledge to improve neopronoun proficiency. Our proposed methods outperform finetuning with standard BPE, improving neopronoun accuracy from 14.1% to 58.4%. Our paper is the first to link LLM misgendering to tokenization and deficient neopronoun grammar, indicating that LLMs unable to correctly treat neopronouns as pronouns are more prone to misgender.",
        "author": "Anaelia Ovalle; Ninareh Mehrabi; Palash Goyal; Jwala Dhamala; Kai-Wei Chang; Richard Zemel; Aram Galstyan; Yuval Pinter; Rahul Gupta",
        "authorids": "/a/anaelia-ovalle/; /n/ninareh-mehrabi/; /p/palash-goyal/; /j/jwala-dhamala/; /k/kai-wei-chang/; /r/richard-zemel/; /a/aram-galstyan/; /y/yuval-pinter/; /r/rahul-gupta/",
        "bibtex": "@inproceedings{ovalle-etal-2024-tokenization,\n    title = \"Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies\",\n    author = \"Ovalle, Anaelia  and\n      Mehrabi, Ninareh  and\n      Goyal, Palash  and\n      Dhamala, Jwala  and\n      Chang, Kai-Wei  and\n      Zemel, Richard  and\n      Galstyan, Aram  and\n      Pinter, Yuval  and\n      Gupta, Rahul\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.113/\",\n    doi = \"10.18653/v1/2024.findings-naacl.113\",\n    pages = \"1739--1756\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.113.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.113/",
        "pdf_size": 1172223,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16243611538341537133&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "University of California, Los Angeles; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Amazon AGI Foundations; Ben-Gurion University of the Negev, Be\u2019er Sheva, Israel + Amazon AGI Foundations; Amazon AGI Foundations",
        "aff_domain": "cs.ucla.edu;amazon.com; ; ; ; ; ;bgu.ac.il; ",
        "email": "cs.ucla.edu;amazon.com; ; ; ; ; ;bgu.ac.il; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;1;1;1;1;1;2+1;1",
        "aff_unique_norm": "University of California, Los Angeles;Amazon;Ben-Gurion University of the Negev",
        "aff_unique_dep": ";AGI Foundations;",
        "aff_unique_url": "https://www.ucla.edu;https://www.amazon.com;https://www.bgu.ac.il",
        "aff_unique_abbr": "UCLA;Amazon;BGU",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Los Angeles;;Be\u2019er Sheva",
        "aff_country_unique_index": "0;0;0;0;0;0;0;1+0;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "2024.findings-naacl.247",
        "title": "Tokenizer Choice For LLM Training: Negligible or Crucial?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The recent success of large language models (LLMs) has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot.Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model\u2019s downstream performance and training costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable proxy for the model\u2019s downstream performance. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary size increases of factor three in comparison to English. While English-centric tokenizers have been applied to the training of multi-lingual LLMs in the past, we find that this approach results in a severe downstream performance degradation and additional training costs of up to 68%, due to an inefficient tokenization vocabulary.",
        "author": "Mehdi Ali; Michael Fromm; Klaudia Thellmann; Richard Rutmann; Max L\u00fcbbering; Johannes Leveling; Katrin Klug; Jan Ebert; Niclas Doll; Jasper Buschhoff; Charvi Jain; Alexander Weber; Lena Jurkschat; Hammam Abdelwahab; Chelsea John; Pedro Ortiz Suarez; Malte Ostendorff; Samuel Weinbach; Rafet Sifa; Stefan Kesselheim; Nicolas Flores-Herr",
        "authorids": "/m/mehdi-ali/; /m/michael-fromm/; /k/klaudia-thellmann/; /r/richard-rutmann/; /m/max-lubbering/; /j/johannes-leveling/; /k/katrin-klug/; /j/jan-ebert/; /n/niclas-doll/; /j/jasper-buschhoff/; /c/charvi-jain/; /a/alexander-weber/; /l/lena-jurkschat/; /h/hammam-abdelwahab/; /c/chelsea-john/; /p/pedro-ortiz-suarez/; /m/malte-ostendorff/; /s/samuel-weinbach/; /r/rafet-sifa/; /s/stefan-kesselheim/; /n/nicolas-flores-herr/",
        "bibtex": "@inproceedings{ali-etal-2024-tokenizer,\n    title = \"Tokenizer Choice For {LLM} Training: Negligible or Crucial?\",\n    author = {Ali, Mehdi  and\n      Fromm, Michael  and\n      Thellmann, Klaudia  and\n      Rutmann, Richard  and\n      L{\\\"u}bbering, Max  and\n      Leveling, Johannes  and\n      Klug, Katrin  and\n      Ebert, Jan  and\n      Doll, Niclas  and\n      Buschhoff, Jasper  and\n      Jain, Charvi  and\n      Weber, Alexander  and\n      Jurkschat, Lena  and\n      Abdelwahab, Hammam  and\n      John, Chelsea  and\n      Ortiz Suarez, Pedro  and\n      Ostendorff, Malte  and\n      Weinbach, Samuel  and\n      Sifa, Rafet  and\n      Kesselheim, Stefan  and\n      Flores-Herr, Nicolas},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.247/\",\n    doi = \"10.18653/v1/2024.findings-naacl.247\",\n    pages = \"3907--3924\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.247.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.247/",
        "pdf_size": 380169,
        "gs_citation": 34,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10724310413850892980&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;;;;;;;;;;;;;;;;;;",
        "aff_domain": ";;;;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 21
    },
    {
        "id": "2024.naacl-long.48",
        "title": "Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model\u2019s creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation of diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses the chain-of-thought approach. Further studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase its capability in using toolkits not explicitly tailored for the target task, affirming its robustness in real-world scenarios. All codes and data are released.",
        "author": "Cheng Qian; Chenyan Xiong; Zhenghao Liu; Zhiyuan Liu",
        "authorids": "/c/cheng-qian/; /c/chenyan-xiong/; /z/zhenghao-liu/; /z/zhiyuan-liu/",
        "bibtex": "@inproceedings{qian-etal-2024-toolink,\n    title = \"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model\",\n    author = \"Qian, Cheng  and\n      Xiong, Chenyan  and\n      Liu, Zhenghao  and\n      Liu, Zhiyuan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.48/\",\n    doi = \"10.18653/v1/2024.naacl-long.48\",\n    pages = \"831--854\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.48.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.48/",
        "pdf_size": 1617786,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3569466878590143165&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Tsinghua University; Carnegie Mellon University; Northeastern University; Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn; ; ; ",
        "email": "mails.tsinghua.edu.cn; ; ; ",
        "github": "https://github.com/qiancheng0/Toolink",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Tsinghua University;Carnegie Mellon University;Northeastern University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.cmu.edu;https://www.northeastern.edu",
        "aff_unique_abbr": "THU;CMU;NEU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.164",
        "title": "TopicGPT: A Prompt-based Topic Modeling Framework",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Topic modeling is a well-established technique for exploring text corpora. Conventional topic models (e.g., LDA) represent topics as bags of words that often require \u201creading the tea leaves\u201d to interpret; additionally, they offer users minimal control over the formatting and specificity of resulting topics. To tackle these issues, we introduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to uncover latent topics in a text collection. TopicGPT produces topics that align better with human categorizations compared to competing methods: it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for the strongest baseline. Its topics are also more interpretable, dispensing with ambiguous bags of words in favor of topics with natural language labels and associated free-form descriptions. Moreover, the framework is highly adaptable, allowing users to specify constraints and modify topics without the need for model retraining. By streamlining access to high-quality and interpretable topics, TopicGPT represents a compelling, human-centered approach to topic modeling.",
        "author": "Chau Minh Pham; Alexander Hoyle; Simeng Sun; Philip Resnik; Mohit Iyyer",
        "authorids": "/c/chau-minh-pham/; /a/alexander-hoyle/; /s/simeng-sun/; /p/philip-resnik/; /m/mohit-iyyer/",
        "bibtex": "@inproceedings{pham-etal-2024-topicgpt,\n    title = \"{T}opic{GPT}: A Prompt-based Topic Modeling Framework\",\n    author = \"Pham, Chau Minh  and\n      Hoyle, Alexander  and\n      Sun, Simeng  and\n      Resnik, Philip  and\n      Iyyer, Mohit\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.164/\",\n    doi = \"10.18653/v1/2024.naacl-long.164\",\n    pages = \"2956--2984\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.164.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.164/",
        "pdf_size": 766022,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17217291333004361442&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Massachusetts Amherst; University of Maryland; University of Massachusetts Amherst; University of Maryland; University of Massachusetts Amherst",
        "aff_domain": "umass.edu;umd.edu;umass.edu;umd.edu;umass.edu",
        "email": "umass.edu;umd.edu;umass.edu;umd.edu;umass.edu",
        "github": "https://github.com/chtmp223/topicGPT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "University of Massachusetts Amherst;University of Maryland",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umass.edu;https://www/umd.edu",
        "aff_unique_abbr": "UMass Amherst;UMD",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.67",
        "title": "Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) are dramatically influencing AI research, spurring discussions on what has changed so far and how to shape the field\u2019s future. To clarify such questions, we analyze a new dataset of 16,979 LLM-related arXiv papers, focusing on recent trends in 2023 vs. 2018-2022. First, we study disciplinary shifts: LLM research increasingly considers societal impacts, evidenced by 20\u00d7 growth in LLM submissions to the Computers and Society sub-arXiv. An influx of new authors \u2013 half of all first authors in 2023 \u2013 are entering from non-NLP fields of CS, driving disciplinary expansion. Second, we study industry and academic publishing trends. Surprisingly, industry accounts for a smaller publication share in 2023, largely due to reduced output from Google and other Big Tech companies; universities in Asia are publishing more. Third, we study institutional collaboration: while industry-academic collaborations are common, they tend to focus on the same topics that industry focuses on rather than bridging differences. The most prolific institutions are all US- or China-based, but there is very little cross-country collaboration. We discuss implications around (1) how to support the influx of new authors, (2) how industry trends may affect academics, and (3) possible effects of (the lack of) collaboration.",
        "author": "Rajiv Movva; Sidhika Balachandar; Kenny Peng; Gabriel Agostini; Nikhil Garg; Emma Pierson",
        "authorids": "/r/rajiv-movva/; /s/sidhika-balachandar/; /k/kenny-peng/; /g/gabriel-agostini/; /n/nikhil-garg/; /e/emma-pierson/",
        "bibtex": "@inproceedings{movva-etal-2024-topics,\n    title = \"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17{K} ar{X}iv Papers\",\n    author = \"Movva, Rajiv  and\n      Balachandar, Sidhika  and\n      Peng, Kenny  and\n      Agostini, Gabriel  and\n      Garg, Nikhil  and\n      Pierson, Emma\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.67/\",\n    doi = \"10.18653/v1/2024.naacl-long.67\",\n    pages = \"1223--1243\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.67.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.67/",
        "pdf_size": 501775,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14910286507083984424&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Cornell Tech; Cornell Tech; Cornell Tech; Cornell Tech; Stanford University; Stanford University",
        "aff_domain": "cs.cornell.edu; ; ; ; ; ",
        "email": "cs.cornell.edu; ; ; ; ; ",
        "github": "https://github.com/rmovva/LLM-publication-patterns-public1223",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "Cornell University;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://tech.cornell.edu;https://www.stanford.edu",
        "aff_unique_abbr": "Cornell Tech;Stanford",
        "aff_campus_unique_index": "0;0;0;0;1;1",
        "aff_campus_unique": "New York City;Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.94",
        "title": "Toward Informal Language Processing: Knowledge of Slang in Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent advancement in large language models (LLMs) has offered a strong potential for natural language systems to process informal language. A representative form of informal language is slang, used commonly in daily conversations and online social media. To date, slang has not been comprehensively evaluated in LLMs due partly to the absence of a carefully designed and publicly accessible benchmark. Using movie subtitles, we construct a dataset that supports evaluation on a diverse set of tasks pertaining to automatic processing of slang. For both evaluation and finetuning, we show the effectiveness of our dataset on two core applications: 1) slang detection, and 2) identification of regional and historical sources of slang from natural sentences. We also show how our dataset can be used to probe the output distributions of LLMs for interpretive insights. We find that while LLMs such as GPT-4 achieve good performance in a zero-shot setting, smaller BERT-like models finetuned on our dataset achieve comparable performance. Furthermore, we show that our dataset enables finetuning of LLMs such as GPT-3.5 that achieve substantially better performance than strong zero-shot baselines. Our work offers a comprehensive evaluation and a high-quality benchmark on English slang based on the OpenSubtitles corpus, serving both as a publicly accessible resource and a platform for applying tools for informal language processing.",
        "author": "Zhewei Sun; Qian Hu; Rahul Gupta; Richard Zemel; Yang Xu",
        "authorids": "/z/zhewei-sun/; /q/qian-hu/; /r/rahul-gupta/; /r/richard-zemel/; /y/yang-xu/",
        "bibtex": "@inproceedings{sun-etal-2024-toward,\n    title = \"Toward Informal Language Processing: Knowledge of Slang in Large Language Models\",\n    author = \"Sun, Zhewei  and\n      Hu, Qian  and\n      Gupta, Rahul  and\n      Zemel, Richard  and\n      Xu, Yang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.94/\",\n    doi = \"10.18653/v1/2024.naacl-long.94\",\n    pages = \"1683--1701\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.94.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.94/",
        "pdf_size": 1227404,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13592630592350842914&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "University of Toronto; Amazon Alexa AI; Amazon Alexa AI; Columbia University; University of Toronto",
        "aff_domain": "cs.toronto.edu;amazon.com;amazon.com;cs.columbia.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;amazon.com;amazon.com;cs.columbia.edu;cs.toronto.edu",
        "github": "https://github.com/amazon-science/slang-llm-benchmark",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;2;0",
        "aff_unique_norm": "University of Toronto;Amazon;Columbia University",
        "aff_unique_dep": ";Amazon Alexa AI;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.amazon.com;https://www.columbia.edu",
        "aff_unique_abbr": "U of T;Amazon;Columbia",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "2024.naacl-long.356",
        "title": "Toward Interactive Regional Understanding in Vision-Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent Vision-Language Pre-training (VLP) models have demonstrated significant advancements. Nevertheless, these models heavily rely on image-text pairs that capture only coarse and global information of an image, leading to a limitation in their regional understanding ability. In this work, we introduce RegionVLM, equipped with explicit regional modeling capabilities, allowing them to understand user-indicated image regions. To achieve this, we design a simple yet innovative architecture, requiring no modifications to the model architecture or objective function. Additionally, we leverage a dataset that contains a novel source of information, namely Localized Narratives, which has been overlooked in previous VLP research. Our experiments demonstrate that our single generalist model not only achieves an interactive dialogue system but also exhibits superior performance on various zero-shot region understanding tasks, without compromising its ability for global image understanding.",
        "author": "Jungbeom Lee; Sanghyuk Chun; Sangdoo Yun",
        "authorids": "/j/jungbeom-lee/; /s/sanghyuk-chun/; /s/sangdoo-yun/",
        "bibtex": "@inproceedings{lee-etal-2024-toward,\n    title = \"Toward Interactive Regional Understanding in Vision-Large Language Models\",\n    author = \"Lee, Jungbeom  and\n      Chun, Sanghyuk  and\n      Yun, Sangdoo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.356/\",\n    doi = \"10.18653/v1/2024.naacl-long.356\",\n    pages = \"6416--6429\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.356.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.356/",
        "pdf_size": 7836069,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3442251497640240576&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Amazon; NA VER AI Lab; NA VER AI Lab",
        "aff_domain": "amazon.com;navercorp.com;navercorp.com",
        "email": "amazon.com;navercorp.com;navercorp.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Amazon;NAVER Corporation",
        "aff_unique_dep": "Amazon.com, Inc.;AI Lab",
        "aff_unique_url": "https://www.amazon.com;https://www.naver.com",
        "aff_unique_abbr": "Amazon;NAVER",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "2024.findings-naacl.48",
        "title": "Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Open-domain Question Answering (OpenQA) aims at answering factual questions with an external large-scale knowledge corpus. However, real-world knowledge is not static; it updates and evolves continually. Such a dynamic characteristic of knowledge poses a vital challenge for these models, as the trained models need to constantly adapt to the latest information to make sure that the answers remain accurate. In addition, it is still unclear how well an OpenQA model can transfer to completely new knowledge domains. In this paper, we investigate the generalization performance of a retrieval-augmented QA model in two specific scenarios: 1) adapting to updated versions of the same knowledge corpus; 2) switching to completely different knowledge domains. We observe that the generalization challenges of OpenQA models stem from the reader\u2019s over-reliance on memorizing the knowledge from the external corpus, which hinders the model from generalizing to a new knowledge corpus. We introduce Corpus-Invariant Tuning (CIT), a simple but effective training strategy, to mitigate the knowledge over-memorization by controlling the likelihood of retrieved contexts during training. Extensive experimental results on multiple OpenQA benchmarks show that CIT achieves significantly better generalizability without compromising the model\u2019s performance in its original corpus and domain.",
        "author": "Zixuan Zhang; Revanth Gangi Reddy; Kevin Small; Tong Zhang; Heng Ji",
        "authorids": "/z/zixuan-zhang/; /r/revanth-gangi-reddy/; /k/kevin-small/; /t/tong-zhang/; /h/heng-ji/",
        "bibtex": "@inproceedings{zhang-etal-2024-towards,\n    title = \"Towards Better Generalization in Open-Domain Question Answering by Mitigating Context Memorization\",\n    author = \"Zhang, Zixuan  and\n      Gangi Reddy, Revanth  and\n      Small, Kevin  and\n      Zhang, Tong  and\n      Ji, Heng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.48/\",\n    doi = \"10.18653/v1/2024.findings-naacl.48\",\n    pages = \"742--753\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.48.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.48/",
        "pdf_size": 326859,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17259436768083178937&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; Amazon; University of Illinois Urbana-Champaign; Amazon",
        "aff_domain": "illinois.edu;illinois.edu;amazon.com;illinois.edu;amazon.com",
        "email": "illinois.edu;illinois.edu;amazon.com;illinois.edu;amazon.com",
        "github": "https://github.com/zhangzx-uiuc/CIT",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;1;0;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Amazon",
        "aff_unique_dep": ";Amazon.com, Inc.",
        "aff_unique_url": "https://illinois.edu;https://www.amazon.com",
        "aff_unique_abbr": "UIUC;Amazon",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.404",
        "title": "Towards Explainability in Legal Outcome Prediction Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand the model\u2019s decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and neural models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.",
        "author": "Josef Valvoda; Ryan Cotterell",
        "authorids": "/j/josef-valvoda/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{valvoda-cotterell-2024-towards,\n    title = \"Towards Explainability in Legal Outcome Prediction Models\",\n    author = \"Valvoda, Josef  and\n      Cotterell, Ryan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.404/\",\n    doi = \"10.18653/v1/2024.naacl-long.404\",\n    pages = \"7269--7289\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.404.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.404/",
        "pdf_size": 695030,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6259014415385236626&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "University of Cambridge; ETH Z\u00fcrich",
        "aff_domain": "cam.ac.uk;inf.ethz.ch",
        "email": "cam.ac.uk;inf.ethz.ch",
        "github": "https://github.com/valvoda/under_the_influence",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Cambridge;ETH Zurich",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ethz.ch",
        "aff_unique_abbr": "Cambridge;ETHZ",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Switzerland"
    },
    {
        "id": "2024.naacl-long.216",
        "title": "Towards Improved Multi-Source Attribution for Long-Form Answer Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Teaching large language models (LLMs) to generate text with attribution to evidence sources can reduce hallucinations, improve verifiability in question answering systems (QA), and increase reliability of retrieval augmented LLMs. Despite gaining increasing popularity for usage in QA systems and search engines, current LLMs struggle with attribution for long-form responses which require reasoning over multiple evidence sources. To address this, in this paper we aim to improve the attribution capability of LLMs for long-form answer generation to multiple sources, with multiple citations per sentence. However, data for training multi-source attributable QA systems is difficult and expensive to annotate, and therefore scarce. To overcome this challenge, we transform existing QA datasets for this task (MultiAttr), and empirically demonstrate, on a wide range of attribution benchmark datasets, that fine-tuning on MultiAttr provides significant improvements over training only on the target QA domain. Lastly, to fill a gap in existing benchmarks, we present a multi-source attribution dataset containing multi-paragraph answers, PolitiICite, based on PolitiFact articles that discuss events closely related to implementation statuses of election promises.",
        "author": "Nilay Patel; Shivashankar Subramanian; Siddhant Garg; Pratyay Banerjee; Amita Misra",
        "authorids": "/n/nilay-patel/; /s/shivashankar-subramanian/; /s/siddhant-garg/; /p/pratyay-banerjee/; /a/amita-misra/",
        "bibtex": "@inproceedings{patel-etal-2024-towards,\n    title = \"Towards Improved Multi-Source Attribution for Long-Form Answer Generation\",\n    author = \"Patel, Nilay  and\n      Subramanian, Shivashankar  and\n      Garg, Siddhant  and\n      Banerjee, Pratyay  and\n      Misra, Amita\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.216/\",\n    doi = \"10.18653/v1/2024.naacl-long.216\",\n    pages = \"3906--3919\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.216.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.216/",
        "pdf_size": 544150,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11234965597605435095&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "UC Santa Cruz; Amazon; Meta AI + Amazon; Amazon; Amazon",
        "aff_domain": "ucsc.edu;amazon.com;meta.com;amazon.com;amazon.com",
        "email": "ucsc.edu;amazon.com;meta.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2+1;1;1",
        "aff_unique_norm": "University of California, Santa Cruz;Amazon;Meta",
        "aff_unique_dep": ";Amazon.com, Inc.;Meta AI",
        "aff_unique_url": "https://www.ucsc.edu;https://www.amazon.com;https://meta.com",
        "aff_unique_abbr": "UCSC;Amazon;Meta",
        "aff_campus_unique_index": "0;",
        "aff_campus_unique": "Santa Cruz;",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.399",
        "title": "Towards Reducing Diagnostic Errors with Interpretable Risk Prediction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Many diagnostic errors occur because clinicians cannot easily access relevant information in patient Electronic Health Records (EHRs). In this work we propose a method to use LLMs to identify pieces of evidence in patient EHR data that indicate increased or decreased risk of specific diagnoses; our ultimate aim is to increase access to evidence and reduce diagnostic errors. In particular, we propose a Neural Additive Model to make predictions backed by evidence with individualized risk estimates at time-points where clinicians are still uncertain, aiming to specifically mitigate delays in diagnosis and errors stemming from an incomplete differential. To train such a model, it is necessary to infer temporally fine-grained retrospective labels of eventual \u201ctrue\u201d diagnoses. We do so with LLMs, to ensure that the input text is from before a confident diagnosis can be made. We use an LLM to retrieve an initial pool of evidence, but then refine this set of evidence according to correlations learned by the model. We conduct an in-depth evaluation of the usefulness of our approach by simulating how it might be used by a clinician to decide between a pre-defined list of differential diagnoses.",
        "author": "Denis McInerney; William Dickinson; Lucy Flynn; Andrea Young; Geoffrey Young; Jan-Willem van de Meent; Byron Wallace",
        "authorids": "/d/denis-mcinerney/; /w/william-dickinson/; /l/lucy-flynn/; /a/andrea-young/; /g/geoffrey-young/; /j/jan-willem-van-de-meent/; /b/byron-c-wallace/",
        "bibtex": "@inproceedings{mcinerney-etal-2024-towards,\n    title = \"Towards Reducing Diagnostic Errors with Interpretable Risk Prediction\",\n    author = \"McInerney, Denis  and\n      Dickinson, William  and\n      Flynn, Lucy  and\n      Young, Andrea  and\n      Young, Geoffrey  and\n      van de Meent, Jan-Willem  and\n      Wallace, Byron\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.399/\",\n    doi = \"10.18653/v1/2024.naacl-long.399\",\n    pages = \"7193--7210\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.399.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.399/",
        "pdf_size": 732191,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12066997462967092019&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Northeastern University; Brigham and Women\u2019s Hospital; Brigham and Women\u2019s Hospital; Brigham and Women\u2019s Hospital; Brigham and Women\u2019s Hospital; University of Amsterdam; Northeastern University",
        "aff_domain": "northeastern.edu;bwh.harvard.edu;mgb.org;bwh.harvard.edu;bwh.harvard.edu;uva.nl;northeastern.edu",
        "email": "northeastern.edu;bwh.harvard.edu;mgb.org;bwh.harvard.edu;bwh.harvard.edu;uva.nl;northeastern.edu",
        "github": "https://github.com/dmcinerney/ehr-diagnosis-env; https://github.com/dmcinerney/ehr-diagnosis-agent; https://github.com/dmcinerney/ehr-diagnosis-env-interface",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;1;1;1;1;2;0",
        "aff_unique_norm": "Northeastern University;Brigham and Women's Hospital;University of Amsterdam",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.northeastern.edu;https://www.brighamandwomens.org;https://www.uva.nl",
        "aff_unique_abbr": "NEU;BWH;UvA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;1;0",
        "aff_country_unique": "United States;Netherlands"
    },
    {
        "id": "2024.naacl-industry.20",
        "title": "Towards Translating Objective Product Attributes Into Customer Language",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "When customers search online for a product they are not familiar with, their needs are often expressed through subjective product attributes, such as \u201dpicture quality\u201d for a TV or \u201deasy to clean\u201d for a sofa. In contrast, the product catalog in online stores includes objective attributes such as \u201dscreen resolution\u201d or \u201dmaterial\u201d. In this work, we aim to find a link between the objective product catalog and the subjective needs of the customers, to help customers better understand the product space using their own words. We apply correlation-based methods to the store\u2019s product catalog and product reviews in order to find the best potential links between objective and subjective attributes; next, Large Language Models (LLMs) reduce spurious correlations by incorporating common sense and world knowledge (e.g., picture quality is indeed affected by screen resolution, and 8k is the best one). We curate a dataset for this task and show that our combined approach outperforms correlation-only and causation-only approaches.",
        "author": "Ram Yazdi; Oren Kalinsky; Alexander Libov; Dafna Shahaf",
        "authorids": "/r/ram-yazdi/; /o/oren-kalinsky/; /a/alexander-libov/; /d/dafna-shahaf/",
        "bibtex": "@inproceedings{yazdi-etal-2024-towards,\n    title = \"Towards Translating Objective Product Attributes Into Customer Language\",\n    author = \"Yazdi, Ram  and\n      Kalinsky, Oren  and\n      Libov, Alexander  and\n      Shahaf, Dafna\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.20/\",\n    doi = \"10.18653/v1/2024.naacl-industry.20\",\n    pages = \"239--247\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.20.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.20/",
        "pdf_size": 365576,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:YQS645EIqdUJ:scholar.google.com/&scioq=Towards+Translating+Objective+Product+Attributes+Into+Customer+Language&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Amazon; Amazon; Amazon; Amazon",
        "aff_domain": "amazon.com;amazon.com;amazon.com;amazon.com",
        "email": "amazon.com;amazon.com;amazon.com;amazon.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Amazon",
        "aff_unique_dep": "Amazon.com, Inc.",
        "aff_unique_url": "https://www.amazon.com",
        "aff_unique_abbr": "Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.163",
        "title": "Towards an On-device Agent for Text Rewriting",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities for text rewriting. However creating a smaller yet potent language model for text rewriting presents two formidable challenges: costly data collection and absence of emergent capabilities.In this paper we present solutions to address the above challenges.We propose an new instruction tuning method to develop a mo-bile text rewriting model that leverages LLM-generated data and heuristic reinforcement learning, eliminating the need for human data collection. Moreover, to bridge the performance gap from the constraint size, we pro-pose a cascading approach based on the confidence levels which are distilled from the large server model\u2019s critiques. To evaluate the text rewriting tasks for mobile scenarios, we introduce MessageRewriteEval, a human-labeled benchmark that focuses on text rewriting of messages through natural language instructions. Through empirical experiments, we demonstrate that our on-device model surpasses the current state-of-the-art LLMs in text rewriting while maintaining a significantly reduced model size using public benchmark EditEval and our new benchmark. We also demonstrate that our proposed cascading approach improves model performance further.",
        "author": "Yun Zhu; Yinxiao Liu; Felix Stahlberg; Shankar Kumar; Yu-Hui Chen; Liangchen Luo; Lei Shu; Renjie Liu; Jindong Chen; Lei Meng",
        "authorids": "/y/yun-zhu/; /y/yinxiao-liu/; /f/felix-stahlberg/; /s/shankar-kumar/; /y/yu-hui-chen/; /l/liangchen-luo/; /l/lei-shu/; /r/renjie-liu/; /j/jindong-chen/; /l/lei-meng/",
        "bibtex": "@inproceedings{zhu-etal-2024-towards,\n    title = \"Towards an On-device Agent for Text Rewriting\",\n    author = \"Zhu, Yun  and\n      Liu, Yinxiao  and\n      Stahlberg, Felix  and\n      Kumar, Shankar  and\n      Chen, Yu-Hui  and\n      Luo, Liangchen  and\n      Shu, Lei  and\n      Liu, Renjie  and\n      Chen, Jindong  and\n      Meng, Lei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.163/\",\n    doi = \"10.18653/v1/2024.findings-naacl.163\",\n    pages = \"2535--2552\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.163.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.163/",
        "pdf_size": 425713,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12990639604076897346&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;;;;;;",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 10
    },
    {
        "id": "2024.findings-naacl.186",
        "title": "Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Automatically generating human-readable text describing the functionality of a program is the intent of source code summarization. Although neural language models achieve significant performance in this field, they are limited by their inability to access external knowledge. To address this limitation, an emerging trend is combining neural models with external knowledge through retrieval methods. Previous methods have relied on the sentence-level retrieval paradigm on the encoder side. However, this paradigm is coarse-grained, noise-filled and cannot directly take advantage of the high-quality retrieved summary tokens on the decoder side. In this paper, we propose a fine-grained Token-level retrieval-augmented mechanism (Tram) on the decoder side rather than the encoder side to enhance the performance of neural models and produce more low-frequency tokens in generating summaries. Furthermore, to overcome the challenge of token-level retrieval in capturing contextual code semantics, we also propose integrating code semantics into individual summary tokens. The results of extensive experiments and human evaluation show that our token-level retrieval-augmented approach significantly improves performance and is more interpretable.",
        "author": "Tong Ye; Lingfei Wu; Tengfei Ma; Xuhong Zhang; Yangkai Du; Peiyu Liu; Shouling Ji; Wenhai Wang",
        "authorids": "/t/tong-ye/; /l/lingfei-wu/; /t/tengfei-ma/; /x/xuhong-zhang/; /y/yangkai-du/; /p/peiyu-liu/; /s/shouling-ji/; /w/wenhai-wang/",
        "bibtex": "@inproceedings{ye-etal-2024-tram,\n    title = \"Tram: A Token-level Retrieval-augmented Mechanism for Source Code Summarization\",\n    author = \"Ye, Tong  and\n      Wu, Lingfei  and\n      Ma, Tengfei  and\n      Zhang, Xuhong  and\n      Du, Yangkai  and\n      Liu, Peiyu  and\n      Ji, Shouling  and\n      Wang, Wenhai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.186/\",\n    doi = \"10.18653/v1/2024.findings-naacl.186\",\n    pages = \"2959--2971\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.186.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.186/",
        "pdf_size": 454979,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12355348222718589574&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Zhejiang University; Anytime.AI; Stony Brook University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University; Zhejiang University",
        "aff_domain": "zju.edu.cn;anytime-ai.com;stonybrook.edu;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;anytime-ai.com;stonybrook.edu;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;0;0;0;0;0",
        "aff_unique_norm": "Zhejiang University;Anytime AI;Stony Brook University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.zju.edu.cn;https://www.anytime.ai;https://www.stonybrook.edu",
        "aff_unique_abbr": "ZJU;Anytime AI;SBU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;0;0;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "2024.naacl-long.381",
        "title": "Transformers Can Represent n-gram Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Plenty of existing work has analyzed the abilities of the transformer architecture by describing its representational capacity with formal models of computation. However, the focus so far has been on analyzing the architecture in terms of language acceptance. We contend that this is an ill-suited problem in the study of language models (LMs), which are definitionally probability distributions over strings. In this paper, we focus on the relationship between transformer LMs and n-gram LMs, a simple and historically relevant class of language models. We show that transformer LMs using the hard or sparse attention mechanisms can exactly represent any n-gram LM, giving us a concrete lower bound on their probabilistic representational capacity. This provides a first step towards understanding the mechanisms that transformer LMs can use to represent probability distributions over strings.",
        "author": "Anej Svete; Ryan Cotterell",
        "authorids": "/a/anej-svete/; /r/ryan-cotterell/",
        "bibtex": "@inproceedings{svete-cotterell-2024-transformers,\n    title = \"Transformers Can Represent $n$-gram Language Models\",\n    author = \"Svete, Anej  and\n      Cotterell, Ryan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.381/\",\n    doi = \"10.18653/v1/2024.naacl-long.381\",\n    pages = \"6845--6881\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.381.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.381/",
        "pdf_size": 676866,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1465289173105522537&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 6,
        "aff": "ETH Zurich; ETH Zurich",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch",
        "github": "https://github.com/rycolab/transformer-ngrams",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "2024.naacl-industry.35",
        "title": "Tree-of-Question: Structured Retrieval Framework for Korean Question Answering Systems",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "We introduce Korean language-specific RAG-based QA systems, primarily through the innovative Tree-of-Question (ToQ) methodology and enhanced query generation techniques. We address the complex, multi-hop nature of real-world questions by effectively integrating advanced LLMs with nuanced query planning. Our comprehensive evaluations, including a newly created Korean multi-hop QA dataset, demonstrate our method\u2019s ability to elevate response validity and accuracy, especially in deeper levels of reasoning. This paper not only showcases significant progress in handling the intricacies of Korean linguistic structures but also sets a new standard in the development of context-aware and linguistically sophisticated QA systems.",
        "author": "Dongyub Lee; Younghun Jeong; Hwa-Yeon Kim; Hongyeon Yu; Seunghyun Han; Taesun Whang; Seungwoo Cho; Chanhee Lee; Gunsu Lee; Youngbum Kim",
        "authorids": "/d/dongyub-lee/; /y/younghun-jeong/; /h/hwa-yeon-kim/; /h/hongyeon-yu/; /s/seunghyun-han/; /t/taesun-whang/; /s/seungwoo-cho/; /c/chanhee-lee/; /g/gunsu-lee/; /y/youngbum-kim/",
        "bibtex": "@inproceedings{lee-etal-2024-tree,\n    title = \"Tree-of-Question: Structured Retrieval Framework for {K}orean Question Answering Systems\",\n    author = \"Lee, Dongyub  and\n      Jeong, Younghun  and\n      Kim, Hwa-Yeon  and\n      Yu, Hongyeon  and\n      Han, Seunghyun  and\n      Whang, Taesun  and\n      Cho, Seungwoo  and\n      Lee, Chanhee  and\n      Lee, Gunsu  and\n      Kim, Youngbum\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.35/\",\n    doi = \"10.18653/v1/2024.naacl-industry.35\",\n    pages = \"406--418\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.35.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.35/",
        "pdf_size": 2524292,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:n0sBJ9x_EkoJ:scholar.google.com/&scioq=Tree-of-Question:+Structured+Retrieval+Framework+for+Korean+Question+Answering+Systems&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Naver Corp, WA, USA; Naver Corp, WA, USA; Naver Corp, WA, USA; Naver Corp, WA, USA; Naver Corp, WA, USA; Naver Corp, WA, USA; Naver Corp, WA, USA; Naver Corp, WA, USA; Naver Corp, WA, USA; Naver Corp, WA, USA",
        "aff_domain": "navercorp.com;navercorp.com;navercorp.com;navercorp.com; ; ; ; ; ;navercorp.com",
        "email": "navercorp.com;navercorp.com;navercorp.com;navercorp.com; ; ; ; ; ;navercorp.com",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "NAVER Corporation",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.naver.com",
        "aff_unique_abbr": "Naver",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.154",
        "title": "TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings. To overcome this, we introduce TriSum, a framework for distilling LLMs\u2019 text summarization abilities into a compact, local model. Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality. Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks. Our method enhances local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by providing insights into the summarization rationale.",
        "author": "Pengcheng Jiang; Cao Xiao; Zifeng Wang; Parminder Bhatia; Jimeng Sun; Jiawei Han",
        "authorids": "/p/pengcheng-jiang/; /c/cao-xiao/; /z/zifeng-wang/; /p/parminder-bhatia/; /j/jimeng-sun/; /j/jiawei-han/",
        "bibtex": "@inproceedings{jiang-etal-2024-trisum,\n    title = \"{T}ri{S}um: Learning Summarization Ability from Large Language Models with Structured Rationale\",\n    author = \"Jiang, Pengcheng  and\n      Xiao, Cao  and\n      Wang, Zifeng  and\n      Bhatia, Parminder  and\n      Sun, Jimeng  and\n      Han, Jiawei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.154/\",\n    doi = \"10.18653/v1/2024.naacl-long.154\",\n    pages = \"2805--2819\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.154.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.154/",
        "pdf_size": 1409071,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4383878976668045050&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "University of Illinois at Urbana-Champaign; GE HealthCare; University of Illinois at Urbana-Champaign; GE HealthCare; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign",
        "aff_domain": "illinois.edu;gehealthcare.com;illinois.edu;gehealthcare.com;illinois.edu;illinois.edu",
        "email": "illinois.edu;gehealthcare.com;illinois.edu;gehealthcare.com;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;1;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;GE Healthcare",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://illinois.edu;https://www.gehealthcare.com",
        "aff_unique_abbr": "UIUC;GEHC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.64",
        "title": "TrojFSP: Trojan Insertion in Few-shot Prompt Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prompt tuning is one of the most effective solutions to adapting a fixed pre-trained language model (PLM) for various downstream tasks, especially with only a few input samples. However, the security issues, e.g., Trojan attacks, of prompt tuning on a few data samples are not well-studied. Transferring established data poisoning attacks directly to few-shot prompt tuning presents multiple challenges. One significant issue is the _poisoned imbalance issue_, where non-target class samples are added to the target class, resulting in a greater number of target-class samples compared to non-target class. While this issue is not critical in regular tuning, it significantly hampers the few-shot prompt tuning, making it difficult to simultaneously achieve a high attack success rate (ASR) and maintain clean data accuracy (CDA). Additionally, few-shot prompting is prone to overfitting in terms of both ASR and CDA. In this paper, we introduce _TrojFSP_, a method designed to address the challenges. To solve the poisoned imbalance issue, we develop a _Target-Class Shrink (TC-Shrink)_ technique, which aims to equalize the number of poisoning samples. To combat overfitting, we employ a _Selective Token Poisoning_ technique to boost attack performance. Furthermore, we introduce a _Trojan-Trigger Attention_ objective function to amplify the attention of the poisoned trojan prompt on triggers. Experiments show that our TrojFSP achieves an ASR of over 99% while maintaining negligible decreases in CDA across various PLMs and datasets. The source code of TrojFSP is available at _https://github.com/UCF-ML-Research/TrojFSP_.",
        "author": "Mengxin Zheng; Jiaqi Xue; Xun Chen; Yanshan Wang; Qian Lou; Lei Jiang",
        "authorids": "/m/mengxin-zheng/; /j/jiaqi-xue/; /x/xun-chen/; /y/yanshan-wang/; /q/qian-lou/; /l/lei-jiang/",
        "bibtex": "@inproceedings{zheng-etal-2024-trojfsp,\n    title = \"{T}roj{FSP}: Trojan Insertion in Few-shot Prompt Tuning\",\n    author = \"Zheng, Mengxin  and\n      Xue, Jiaqi  and\n      Chen, Xun  and\n      Wang, Yanshan  and\n      Lou, Qian  and\n      Jiang, Lei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.64/\",\n    doi = \"10.18653/v1/2024.naacl-long.64\",\n    pages = \"1141--1151\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.64.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.64/",
        "pdf_size": 601455,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1551186934945813632&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Central Florida; University of Central Florida; Samsung Research America; University of Pittsburgh; University of Central Florida; Indiana University Bloomington",
        "aff_domain": "ucf.edu; ; ; ; ; ",
        "email": "ucf.edu; ; ; ; ; ",
        "github": "https://github.com/UCF-ML-Research/TrojFSP",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;2;0;3",
        "aff_unique_norm": "University of Central Florida;Samsung;University of Pittsburgh;Indiana University",
        "aff_unique_dep": ";Samsung Research America;;",
        "aff_unique_url": "https://www.ucf.edu;https://www.samsung.com/us/careers/research/;https://www.pitt.edu;https://www.indiana.edu",
        "aff_unique_abbr": "UCF;SRA;Pitt;IU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Bloomington",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-short.69",
        "title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA, and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model\u2019s prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential. Our code is publicly released at https://github.com/xhan77/context-aware-decoding.",
        "author": "Weijia Shi; Xiaochuang Han; Mike Lewis; Yulia Tsvetkov; Luke Zettlemoyer; Wen-tau Yih",
        "authorids": "/w/weijia-shi/; /x/xiaochuang-han/; /m/mike-lewis/; /y/yulia-tsvetkov/; /l/luke-zettlemoyer/; /w/wen-tau-yih/",
        "bibtex": "@inproceedings{shi-etal-2024-trusting,\n    title = \"Trusting Your Evidence: Hallucinate Less with Context-aware Decoding\",\n    author = \"Shi, Weijia  and\n      Han, Xiaochuang  and\n      Lewis, Mike  and\n      Tsvetkov, Yulia  and\n      Zettlemoyer, Luke  and\n      Yih, Wen-tau\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.69/\",\n    doi = \"10.18653/v1/2024.naacl-short.69\",\n    pages = \"783--791\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.69.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.69/",
        "pdf_size": 841062,
        "gs_citation": 179,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4688443087870652295&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Washington, Seattle, WA + FAIR, Meta; University of Washington, Seattle, WA + FAIR, Meta; FAIR, Meta; University of Washington, Seattle, WA; University of Washington, Seattle, WA + FAIR, Meta; FAIR, Meta",
        "aff_domain": "uw.edu;uw.edu; ; ; ; ",
        "email": "uw.edu;uw.edu; ; ; ; ",
        "github": "https://github.com/xhan77/context-aware-decoding",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;0+1;1;0;0+1;1",
        "aff_unique_norm": "University of Washington;Meta",
        "aff_unique_dep": ";Facebook AI Research (FAIR)",
        "aff_unique_url": "https://www.washington.edu;https://meta.com",
        "aff_unique_abbr": "UW;Meta",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0+0;0+0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.40",
        "title": "Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Data poisoning backdoor attacks can cause undesirable behaviors in large language models (LLMs), and defending against them is of increasing importance. Existing defense mechanisms often assume that only one type of trigger is adopted by the attacker, while defending against multiple simultaneous and independent trigger types necessitates general defense frameworks and is relatively unexplored. In this paper, we propose Nested Product of Experts (NPoE) defense framework, which involves a mixture of experts (MoE) as a trigger-only ensemble within the PoE defense framework to simultaneously defend against multiple trigger types. During NPoE training, the main modelis trained in an ensemble with a mixture of smaller expert models that learn the features of backdoor triggers. At inference time, only the main model is used. Experimental results on sentiment analysis, hate speech detection, and question classification tasks demonstrate that NPoE effectively defends against a variety of triggers both separately and in trigger mixtures. Due to the versatility of the MoE structure in NPoE, this framework can be further expanded to defend against other attack settings.",
        "author": "Victoria Graf; Qin Liu; Muhao Chen",
        "authorids": "/v/victoria-graf/; /q/qin-liu/; /m/muhao-chen/",
        "bibtex": "@inproceedings{graf-etal-2024-two,\n    title = \"Two Heads are Better than One: Nested {P}o{E} for Robust Defense Against Multi-Backdoors\",\n    author = \"Graf, Victoria  and\n      Liu, Qin  and\n      Chen, Muhao\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.40/\",\n    doi = \"10.18653/v1/2024.naacl-long.40\",\n    pages = \"706--718\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.40.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.40/",
        "pdf_size": 544734,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17864629936007498125&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Princeton University; UC Davis & USC; UC Davis & USC",
        "aff_domain": "princeton.edu;ucdavis.edu;ucdavis.edu",
        "email": "princeton.edu;ucdavis.edu;ucdavis.edu",
        "github": "https://github.com/VictoriaGraf/Nested_PoE",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Princeton University;University of California, Davis",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.princeton.edu;https://www.ucdavis.edu",
        "aff_unique_abbr": "Princeton;UC Davis",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Davis",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.170",
        "title": "UEGP: Unified Expert-Guided Pre-training for Knowledge Rekindle",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Pre-training and fine-tuning framework has become the standard training paradigm for NLP tasks and is also widely used in industrial-level applications. However, there are still a limitation with this paradigm: simply fine-tuning with task-specific objectives tends to converge to local minima, resulting in a sub-optimal performance. In this paper, we first propose a new paradigm: knowledge rekindle, which aims to re-incorporate the fine-tuned expert model into the training cycle and break through the performance upper bounds of experts without introducing additional annotated data. Then we further propose a unified expert-guided pre-training (UEGP) framework for knowledge rekindle. Specifically, we reuse fine-tuned expert models for various downstream tasks as knowledge sources and inject task-specific prior knowledge to pre-trained language models (PLMs) by means of knowledge distillation. In this process, we perform multi-task learning with knowledge distillation and masked language modeling (MLM) objectives. We also further explored whether mixture-of-expert guided pre-training (MoEGP) can further enhance the effect of knowledge rekindle. Experiments and analysis on eight datasets in GLUE benchmark and a industrial-level search re-ranking dataset show the effectiveness of our method.",
        "author": "Yutao Mou; Kexiang Wang; Jianhe Lin; Dehong Ma; Jun Fan; Daiting Shi; Zhicong Cheng; Gu Simiu; Dawei Yin; Weiran Xu",
        "authorids": "/y/yutao-mou/; /k/kexiang-wang/; /j/jianhe-lin/; /d/dehong-ma/; /j/jun-fan/; /d/daiting-shi/; /z/zhicong-cheng/; /g/gu-simiu/; /d/dawei-yin/; /w/weiran-xu/",
        "bibtex": "@inproceedings{mou-etal-2024-uegp,\n    title = \"{UEGP}: Unified Expert-Guided Pre-training for Knowledge Rekindle\",\n    author = \"Mou, Yutao  and\n      Wang, Kexiang  and\n      Lin, Jianhe  and\n      Ma, Dehong  and\n      Fan, Jun  and\n      Shi, Daiting  and\n      Cheng, Zhicong  and\n      Simiu, Gu  and\n      Yin, Dawei  and\n      Xu, Weiran\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.170/\",\n    doi = \"10.18653/v1/2024.findings-naacl.170\",\n    pages = \"2661--2673\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.170.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.170/",
        "pdf_size": 1399964,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:tuoh8YSxYVIJ:scholar.google.com/&scioq=UEGP:+Unified+Expert-Guided+Pre-training+for+Knowledge+Rekindle&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": ";;;;;;;;;",
        "aff_domain": ";;;;;;;;;",
        "email": ";;;;;;;;;",
        "github": "https://github.com/MurrayTom/UEGP",
        "project": "",
        "author_num": 10
    },
    {
        "id": "2024.findings-naacl.89",
        "title": "UGIF-DataSet: A New Dataset for Cross-lingual, Cross-modal Sequential actions on the UI",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Help documents are supposed to aid smartphone users in resolving queries such as \u201cHow to block calls from unknown numbers?\u201d. However, given a query, identifying the right help document, understanding instructions from the document, and using them to resolve the issue at hand is challenging. The user experience may be enhanced by converting the instructions in the help document to a step-by-step tutorial overlaid on the phone UI. Successful execution of this task requires overcoming research challenges in retrieval, parsing, and grounding in the multilingual-multimodal setting. For example, user queries in one language may have to be matched against instructions in another language, which in turn needs to be grounded in a multimodal UI in yet another language. Moreover, there isn\u2019t any relevant dataset for such a task. In order to bridge this gap, we introduce UGIF-DataSet, a multi-lingual, multi-modal UI grounded dataset for step-by-step task completion on the smartphone, containing 4,184 tasks across 8 languages. The instruction steps in UGIF-DataSet are available only in English, so the challenge involves operations in the cross-modal, cross-lingual setting. We compare the performance of different large language models for this task and find that the end-to-end task completion rate drops from 48% in English to 32% for other languages, demonstrating significant overall headroom for improvement. We are hopeful that UGIF-DataSet and our analysis will aid further research on the important problem of sequential task completion in the multilingual and multimodal setting.",
        "author": "Sagar Gubbi Venkatesh; Partha Talukdar; Srini Narayanan",
        "authorids": "/s/sagar-gubbi-venkatesh/; /p/partha-talukdar/; /s/srini-narayanan/",
        "bibtex": "@inproceedings{gubbi-venkatesh-etal-2024-ugif,\n    title = \"{UGIF}-{D}ata{S}et: A New Dataset for Cross-lingual, Cross-modal Sequential actions on the {UI}\",\n    author = \"Gubbi Venkatesh, Sagar  and\n      Talukdar, Partha  and\n      Narayanan, Srini\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.89/\",\n    doi = \"10.18653/v1/2024.findings-naacl.89\",\n    pages = \"1390--1399\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.89.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.89/",
        "pdf_size": 940599,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6867039143281935763&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Google; Google; Google",
        "aff_domain": "google.com;google.com;google.com",
        "email": "google.com;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Google",
        "aff_unique_dep": "Google",
        "aff_unique_url": "https://www.google.com",
        "aff_unique_abbr": "Google",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.417",
        "title": "UICoder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Many large language models (LLMs) struggle to consistently generate UI code that compiles and produces visually relevant designs. Existing approaches to improve generation rely either on expensive human feedback or distilling a proprietary model. In this paper, we explore the use of automated feedback (compilers and multi-modal models) to guide LLMs to generate high-quality UI code. Our method starts with an existing LLM and iteratively produces improved models by self-generating a large synthetic dataset using an original model, applying automated tools to aggressively filter, score, and de-duplicate the data into a refined higher quality dataset, and producing a new LLM by finetuning the original on the refined dataset.We applied our approach to several open-source LLMs and compared the resulting performance to baseline models with both automated metrics and human preferences.Our results show the resulting models outperform all other downloadable baselines and approach the performance of larger proprietary models.",
        "author": "Jason Wu; Eldon Schoop; Alan Leung; Titus Barik; Jeffrey Bigham; Jeffrey Nichols",
        "authorids": "/j/jason-wu/; /e/eldon-schoop/; /a/alan-leung/; /t/titus-barik/; /j/jeffrey-p-bigham/; /j/jeffrey-nichols/",
        "bibtex": "@inproceedings{wu-2024-uicoder,\n    title = \"{UIC}oder: Finetuning Large Language Models to Generate User Interface Code through Automated Feedback\",\n    author = \"Wu, Jason  and\n      Schoop, Eldon  and\n      Leung, Alan  and\n      Barik, Titus  and\n      Bigham, Jeffrey  and\n      Nichols, Jeffrey\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.417/\",\n    doi = \"10.18653/v1/2024.naacl-long.417\",\n    pages = \"7511--7525\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.417.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.417/",
        "pdf_size": 2760586,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3894034944034400185&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Carnegie Mellon University + Apple Inc.; Apple Inc.; Apple Inc.; Apple Inc.; Apple Inc.; Apple Inc.",
        "aff_domain": "cmu.edu;apple.com;apple.com;apple.com;apple.com;apple.com",
        "email": "cmu.edu;apple.com;apple.com;apple.com;apple.com;apple.com",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0+1;1;1;1;1;1",
        "aff_unique_norm": "Carnegie Mellon University;Apple",
        "aff_unique_dep": ";Apple Inc.",
        "aff_unique_url": "https://www.cmu.edu;https://www.apple.com",
        "aff_unique_abbr": "CMU;Apple",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.4",
        "title": "UINav: A Practical Approach to Train On-Device Automation Agents",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Automation systems that can autonomously drive application user interfaces to complete user tasks are of great benefit, especially when users are situationally or permanently impaired. Prior automation systems do not produce generalizable models while AI-based automation agents work reliably only in simple, hand-crafted applications or incur high computation costs. We propose UINav, a demonstration-based approach to train automation agents that fit mobile devices, yet achieving high success rates with modest numbers of demonstrations. To reduce the demonstration overhead, UINav uses a referee model that provides users with immediate feedback on tasks where the agent fails, and automatically augments human demonstrations to increase diversity in training data. Our evaluation shows that with only 10 demonstrations can achieve 70% accuracy, and that with enough demonstrations it can surpass 90% accuracy.",
        "author": "Wei Li; Fu-Lin Hsu; William Bishop; Folawiyo Campbell-Ajala; Max Lin; Oriana Riva",
        "authorids": "/w/wei-li/; /f/fu-lin-hsu/; /w/william-bishop/; /f/folawiyo-campbell-ajala/; /m/max-lin/; /o/oriana-riva/",
        "bibtex": "@inproceedings{li-etal-2024-uinav,\n    title = \"{UIN}av: A Practical Approach to Train On-Device Automation Agents\",\n    author = \"Li, Wei  and\n      Hsu, Fu-Lin  and\n      Bishop, William  and\n      Campbell-Ajala, Folawiyo  and\n      Lin, Max  and\n      Riva, Oriana\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.4/\",\n    doi = \"10.18653/v1/2024.naacl-industry.4\",\n    pages = \"36--51\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.4.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.4/",
        "pdf_size": 2301883,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12733158854803037397&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Google Research\u2020; University of Pennsylvania\u2021; Google Research\u2020; Google Research\u2020; Google Research\u2020; Google Research\u2020",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "Google;University of Pennsylvania",
        "aff_unique_dep": "Google Research;",
        "aff_unique_url": "https://research.google;https://www.upenn.edu",
        "aff_unique_abbr": "Google Research;UPenn",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.187",
        "title": "UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Previous zero-shot dialogue state tracking (DST) methods only apply transfer learning, but ignore unlabelled data in the target domain.We transform zero-shot DST into few-shot DST by utilising such unlabelled data via joint and self-training methods. Our method incorporates auxiliary tasks that generate slot types as inverse prompts for main tasks, creating slot values during joint training. Cycle consistency between these two tasks enables the generation and selection of quality samples in unknown target domains for subsequent fine-tuning. This approach also facilitates automatic label creation, thereby optimizing the training and fine-tuning of DST models. We demonstrate this method\u2019s effectiveness on general language models in zero-shot scenarios, improving average joint goal accuracy by 8% across all domains in MultiWOZ.",
        "author": "Chuang Li; Yan Zhang; Min-Yen Kan; Haizhou Li",
        "authorids": "/c/chuang-li/; /y/yan-zhang/; /m/min-yen-kan/; /h/haizhou-li/",
        "bibtex": "@inproceedings{li-etal-2024-uno,\n    title = \"{UNO}-{DST}: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking\",\n    author = \"Li, Chuang  and\n      Zhang, Yan  and\n      Kan, Min-Yen  and\n      Li, Haizhou\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.187/\",\n    doi = \"10.18653/v1/2024.findings-naacl.187\",\n    pages = \"2972--2983\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.187.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.187/",
        "pdf_size": 855250,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18228110385515919375&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "National University of Singapore + NUS Graduate School for Integrative Sciences and Engineering; National University of Singapore; National University of Singapore; National University of Singapore + Chinese University of Hong Kong, Shenzhen",
        "aff_domain": "u.nus.edu;nus.edu.sg;nus.edu.sg;nus.edu.sg",
        "email": "u.nus.edu;nus.edu.sg;nus.edu.sg;nus.edu.sg",
        "github": "https://github.com/lichuangnus/UNO-DST",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+0;0;0;0+1",
        "aff_unique_norm": "National University of Singapore;Chinese University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nus.edu.sg;https://www.cuhk.edu.cn",
        "aff_unique_abbr": "NUS;CUHK",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0+0;0;0;0+1",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "2024.naacl-long.469",
        "title": "UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Language technologies that accurately model the dynamics of events must perform commonsense reasoning. Existing work evaluating commonsense reasoning focuses on making inferences about common, everyday situations. To instead investigate the ability to model unusual, unexpected, and unlikely situations, we explore the task of uncommonsense abductive reasoning. Given a piece of context with an unexpected outcome, this task requires reasoning abductively to generate an explanation that makes the unexpected outcome more likely in the context. To this end, we curate and release a new English language corpus called UNcommonsense. We characterize the performance differences between human explainers and the best-performing large language models, finding that model-enhanced human-written explanations achieve the highest quality by trading off between specificity and diversity. Finally, we experiment with several imitation learning algorithms to train open and accessible language models on this task. When compared with the vanilla supervised fine-tuning approach, these methods consistently reduce lose rates on both common and uncommonsense abductive reasoning judged by human evaluators.",
        "author": "Wenting Zhao; Justin Chiu; Jena Hwang; Faeze Brahman; Jack Hessel; Sanjiban Choudhury; Yejin Choi; Xiang Li; Alane Suhr",
        "authorids": "/w/wenting-zhao/; /j/justin-chiu/; /j/jena-hwang/; /f/faeze-brahman/; /j/jack-hessel/; /s/sanjiban-choudhury/; /y/yejin-choi/; /x/xiang-li/; /a/alane-suhr/",
        "bibtex": "@inproceedings{zhao-etal-2024-uncommonsense,\n    title = \"{UN}commonsense Reasoning: Abductive Reasoning about Uncommon Situations\",\n    author = \"Zhao, Wenting  and\n      Chiu, Justin  and\n      Hwang, Jena  and\n      Brahman, Faeze  and\n      Hessel, Jack  and\n      Choudhury, Sanjiban  and\n      Choi, Yejin  and\n      Li, Xiang  and\n      Suhr, Alane\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.469/\",\n    doi = \"10.18653/v1/2024.naacl-long.469\",\n    pages = \"8487--8505\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.469.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.469/",
        "pdf_size": 1411371,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=470445696014235795&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Cornell University; Cornell University; Allen Institute for Artificial Intelligence; Allen Institute for Artificial Intelligence; Allen Institute for Artificial Intelligence; Cornell University; Allen Institute for Artificial Intelligence + University of Washington; University of Pittsburgh; University of California, Berkeley",
        "aff_domain": "cornell.edu; ; ; ; ; ; ;pitt.edu;berkeley.edu",
        "email": "cornell.edu; ; ; ; ; ; ;pitt.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;1;1;1;0;1+2;3;4",
        "aff_unique_norm": "Cornell University;Allen Institute for Artificial Intelligence;University of Washington;University of Pittsburgh;University of California, Berkeley",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.cornell.edu;https://allenai.org;https://www.washington.edu;https://www.pitt.edu;https://www.berkeley.edu",
        "aff_unique_abbr": "Cornell;AI2;UW;Pitt;UC Berkeley",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.31",
        "title": "Uncertainty Estimation in Large Language Models to Support Biodiversity Conservation",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Large Language Models (LLM) provide significant value in question answering (QA) scenarios and have practical application in complex decision-making contexts, such as biodiversity conservation. However, despite substantial performance improvements, they may still produce inaccurate outcomes. Consequently, incorporating uncertainty quantification alongside predictions is essential for mitigating the potential risks associated with their use. This study introduces an exploratory analysis of the application of Monte Carlo Dropout (MCD) and Expected Calibration Error (ECE) to assess the uncertainty of generative language models. To that end, we analyzed two publicly available language models (Falcon-7B and DistilGPT-2). Our findings suggest the viability of employing ECE as a metric to estimate uncertainty in generative LLM. The findings from this research contribute to a broader project aiming at facilitating free and open access to standardized and integrated data and services about Costa Rica\u2019s biodiversity to support the development of science, education, and biodiversity conservation.",
        "author": "Maria Mora-Cross; Saul Calderon-Ramirez",
        "authorids": "/m/maria-mora-cross/; /s/saul-calderon-ramirez/",
        "bibtex": "@inproceedings{mora-cross-calderon-ramirez-2024-uncertainty,\n    title = \"Uncertainty Estimation in Large Language Models to Support Biodiversity Conservation\",\n    author = \"Mora-Cross, Maria  and\n      Calderon-Ramirez, Saul\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.31/\",\n    doi = \"10.18653/v1/2024.naacl-industry.31\",\n    pages = \"368--378\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.31.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.31/",
        "pdf_size": 541780,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1748865790247133404&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "Costa Rica Institute of Technology; Costa Rica Institute of Technology",
        "aff_domain": "itcr.ac.cr;itcr.ac.cr",
        "email": "itcr.ac.cr;itcr.ac.cr",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Costa Rica Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.itcr.ac.cr",
        "aff_unique_abbr": "ITCR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Costa Rica"
    },
    {
        "id": "2024.findings-naacl.180",
        "title": "Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Sequential labeling is a task predicting labels for each token in a sequence, such as Named Entity Recognition (NER). NER tasks aim to extract entities and predict their labels given a text, which is important in information extraction. Although previous works have shown great progress in improving NER performance, uncertainty estimation on NER (UE-NER) is still underexplored but essential. This work focuses on UE-NER, which aims to estimate uncertainty scores for the NER predictions. Previous uncertainty estimation models often overlook two unique characteristics of NER: the connection between entities (i.e., one entity embedding is learned based on the other ones) and wrong span cases in the entity extraction subtask. Therefore, we propose a Sequential Labeling Posterior Network (SLPN) to estimate uncertainty scores for the extracted entities, considering uncertainty transmitted from other tokens. Moreover, we have defined an evaluation strategy to address the specificity of wrong-span cases. Our SLPN has achieved significant improvements on three datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant dataset. Our code is available at https://github.com/he159ok/UncSeqLabeling_SLPN.",
        "author": "Jianfeng He; Linlin Yu; Shuo Lei; Chang-Tien Lu; Feng Chen",
        "authorids": "/j/jianfeng-he/; /l/linlin-yu/; /s/shuo-lei/; /c/chang-tien-lu/; /f/feng-chen/",
        "bibtex": "@inproceedings{he-etal-2024-uncertainty,\n    title = \"Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission\",\n    author = \"He, Jianfeng  and\n      Yu, Linlin  and\n      Lei, Shuo  and\n      Lu, Chang-Tien  and\n      Chen, Feng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.180/\",\n    doi = \"10.18653/v1/2024.findings-naacl.180\",\n    pages = \"2823--2835\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.180.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.180/",
        "pdf_size": 1283518,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2149135814791434417&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, Virginia Tech, Falls Church, VA, USA+Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA; Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA; Department of Computer Science, Virginia Tech, Falls Church, VA, USA; Department of Computer Science, Virginia Tech, Falls Church, VA, USA; Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA",
        "aff_domain": "vt.edu;utdallas.edu;vt.edu;vt.edu;utdallas.edu",
        "email": "vt.edu;utdallas.edu;vt.edu;vt.edu;utdallas.edu",
        "github": "https://github.com/he159ok/UncSeqLabeling_SLPN",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0+1;1;0;0;1",
        "aff_unique_norm": "Virginia Tech;University of Texas at Dallas",
        "aff_unique_dep": "Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.vt.edu;https://www.utdallas.edu",
        "aff_unique_abbr": "VT;UT Dallas",
        "aff_campus_unique_index": "0+1;1;0;0;1",
        "aff_campus_unique": "Falls Church;Richardson",
        "aff_country_unique_index": "0+0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.184",
        "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM\u2019s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM\u2019s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model\u2019s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
        "author": "Chen Ling; Xujiang Zhao; Xuchao Zhang; Wei Cheng; Yanchi Liu; Yiyou Sun; Mika Oishi; Takao Osaki; Katsushi Matsuda; Jie Ji; Guangji Bai; Liang Zhao; Haifeng Chen",
        "authorids": "/c/chen-ling/; /x/xujiang-zhao/; /x/xuchao-zhang/; /w/wei-cheng/; /y/yanchi-liu/; /y/yiyou-sun/; /m/mika-oishi/; /t/takao-osaki/; /k/katsushi-matsuda/; /j/jie-ji/; /g/guangji-bai/; /l/liang-zhao/; /h/haifeng-chen/",
        "bibtex": "@inproceedings{ling-etal-2024-uncertainty,\n    title = \"Uncertainty Quantification for In-Context Learning of Large Language Models\",\n    author = \"Ling, Chen  and\n      Zhao, Xujiang  and\n      Zhang, Xuchao  and\n      Cheng, Wei  and\n      Liu, Yanchi  and\n      Sun, Yiyou  and\n      Oishi, Mika  and\n      Osaki, Takao  and\n      Matsuda, Katsushi  and\n      Ji, Jie  and\n      Bai, Guangji  and\n      Zhao, Liang  and\n      Chen, Haifeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.184/\",\n    doi = \"10.18653/v1/2024.naacl-long.184\",\n    pages = \"3357--3370\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.184.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.184/",
        "pdf_size": 1446814,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1068825235452112570&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Emory University; NEC Labs America; Microsoft; NEC Corporation; NEC Corporation; NEC Corporation; NEC Corporation; NEC Corporation; NEC Corporation; Emory University; Emory University; Emory University; Microsoft",
        "aff_domain": "emory.edu;nec-labs.com;emory.edu; ; ; ; ; ; ; ; ; ;",
        "email": "emory.edu;nec-labs.com;emory.edu; ; ; ; ; ; ; ; ; ;",
        "github": "https://github.com/lingchen0331/UQ_ICL",
        "project": "",
        "author_num": 13,
        "aff_unique_index": "0;1;2;3;3;3;3;3;3;0;0;0;2",
        "aff_unique_norm": "Emory University;NEC Labs America;Microsoft;NEC Corporation",
        "aff_unique_dep": ";;Microsoft Corporation;",
        "aff_unique_url": "https://www.emory.edu;https://www.nec-labs.com;https://www.microsoft.com;https://www.nec.com",
        "aff_unique_abbr": "Emory;NEC LA;Microsoft;NEC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1;1;1;1;1;1;0;0;0;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "2024.naacl-long.316",
        "title": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large language models (LLMs) have demonstrated substantial commonsense understanding through numerous benchmark evaluations. However, their understanding of cultural commonsense remains largely unexamined. In this paper, we conduct a comprehensive examination of the capabilities and limitations of several state-of-the-art LLMs in the context of cultural commonsense tasks. Using several general and cultural commonsense benchmarks, we find that (1) LLMs have a significant discrepancy in performance when tested on culture-specific commonsense knowledge for different cultures; (2) LLMs\u2019 general commonsense capability is affected by cultural context; and (3) The language used to query the LLMs can impact their performance on cultural-related tasks.Our study points to the inherent bias in the cultural understanding of LLMs and provides insights that can help develop culturally-aware language models.",
        "author": "Siqi Shen; Lajanugen Logeswaran; Moontae Lee; Honglak Lee; Soujanya Poria; Rada Mihalcea",
        "authorids": "/s/siqi-shen/; /l/lajanugen-logeswaran/; /m/moontae-lee/; /h/honglak-lee/; /s/soujanya-poria/; /r/rada-mihalcea/",
        "bibtex": "@inproceedings{shen-etal-2024-understanding,\n    title = \"Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense\",\n    author = \"Shen, Siqi  and\n      Logeswaran, Lajanugen  and\n      Lee, Moontae  and\n      Lee, Honglak  and\n      Poria, Soujanya  and\n      Mihalcea, Rada\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.316/\",\n    doi = \"10.18653/v1/2024.naacl-long.316\",\n    pages = \"5668--5680\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.316.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.316/",
        "pdf_size": 792723,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14146496866520273891&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Michigan1; LG AI Research2; University of Illinois at Chicago3; University of Michigan1; Singapore University of Technology and Design4; University of Michigan1",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;0;3;0",
        "aff_unique_norm": "University of Michigan;LG;University of Illinois at Chicago;Singapore University of Technology and Design",
        "aff_unique_dep": ";LG AI Research;;",
        "aff_unique_url": "https://www.umich.edu;https://www.lgaires.com;https://www.uic.edu;https://www.sutd.edu.sg",
        "aff_unique_abbr": "UM;LG AI;UIC;SUTD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;1;0;0;2;0",
        "aff_country_unique": "United States;South Korea;Singapore"
    },
    {
        "id": "2024.naacl-long.99",
        "title": "Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In the era of large language models (LLMs), in-context learning (ICL) stands out as an effective prompting strategy that explores LLMs\u2019 potency across various tasks. However, applying LLMs to grammatical error correction (GEC) is still a challenging task. In this paper, we propose a novel ungrammatical-syntax-based in-context example selection strategy for GEC. Specifically, we measure similarity of sentences based on their syntactic structures with diverse algorithms, and identify optimal ICL examples sharing the most similar ill-formed syntax to the test input. Additionally, we carry out a two-stage process to further improve the quality of selection results. On benchmark English GEC datasets, empirical results show that our proposed ungrammatical-syntax-based strategies outperform commonly-used word-matching or semantics-based methods with multiple LLMs. This indicates that for a syntax-oriented task like GEC, paying more attention to syntactic information can effectively boost LLMs\u2019 performance. Our code is available at https://github.com/JamyDon/SynICL4GEC.",
        "author": "Chenming Tang; Fanyi Qu; Yunfang Wu",
        "authorids": "/c/chenming-tang/; /f/fanyi-qu/; /y/yunfang-wu/",
        "bibtex": "@inproceedings{tang-etal-2024-ungrammatical,\n    title = \"Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction\",\n    author = \"Tang, Chenming  and\n      Qu, Fanyi  and\n      Wu, Yunfang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.99/\",\n    doi = \"10.18653/v1/2024.naacl-long.99\",\n    pages = \"1758--1770\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.99.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.99/",
        "pdf_size": 715838,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9219875818941333208&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "National Key Laboratory for Multimedia Information Processing, Peking University; MOE Key Laboratory of Computational Linguistics, Peking University; School of Computer Science, Peking University",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "email": "stu.pku.edu.cn;pku.edu.cn;pku.edu.cn",
        "github": "https://github.com/JamyDon/SynICL4GEC",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Peking University",
        "aff_unique_dep": "National Key Laboratory for Multimedia Information Processing",
        "aff_unique_url": "http://www.pku.edu.cn",
        "aff_unique_abbr": "PKU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.388",
        "title": "UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge. In this work, we focus on the factual probing performance over unseen prompts from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge. We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen prompts. We propose an adapter-based framework, **UniArk**, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters. Extensive experiments show that UniArk can significantly improve the model\u2019s out-of-domain generalisation as well as consistency under various prompts. Additionally, we construct **ParaTrex**, a large-scale and diverse dataset for measuring the inconsistency and out-of-domain generation of models. Further, ParaTrex offers a reference method for constructing paraphrased datasets using large language models.",
        "author": "Yijun Yang; Jie He; Pinzhen Chen; Victor Gutierrez Basulto; Jeff Pan",
        "authorids": "/y/yijun-yang/; /j/jie-he/; /p/pinzhen-chen/; /v/victor-gutierrez-basulto/; /j/jeff-pan/",
        "bibtex": "@inproceedings{yang-etal-2024-uniark,\n    title = \"{U}ni{A}rk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing\",\n    author = \"Yang, Yijun  and\n      He, Jie  and\n      Chen, Pinzhen  and\n      Gutierrez Basulto, Victor  and\n      Pan, Jeff\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.388/\",\n    doi = \"10.18653/v1/2024.naacl-long.388\",\n    pages = \"7018--7035\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.388.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.388/",
        "pdf_size": 670117,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14303835305733543301&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Informatics, University of Edinburgh, UK; School of Informatics, University of Edinburgh, UK; School of Informatics, University of Edinburgh, UK; School of Computer Science and Informatics, Cardiff University, UK; School of Informatics, University of Edinburgh, UK",
        "aff_domain": "outlook.com;ed.ac.uk;ed.ac.uk;cardiff.ac.uk;ed.ac.uk",
        "email": "outlook.com;ed.ac.uk;ed.ac.uk;cardiff.ac.uk;ed.ac.uk",
        "github": "https://github.com/Thomasyyj/UniArk",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of Edinburgh;Cardiff University",
        "aff_unique_dep": "School of Informatics;School of Computer Science and Informatics",
        "aff_unique_url": "https://www.ed.ac.uk;https://www.cardiff.ac.uk",
        "aff_unique_abbr": "Edinburgh;Cardiff",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "Edinburgh;Cardiff",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.naacl-short.11",
        "title": "Unified Examination of Entity Linking in Absence of Candidate Sets",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Despite remarkable strides made in the development of entity linking systems in recent years, a comprehensive comparative analysis of these systems using a unified framework is notably absent. This paper addresses this oversight by introducing a new black-box benchmark and conducting a comprehensive evaluation of all state-of-the-art entity linking methods. We use an ablation study to investigate the impact of candidate sets on the performance of entity linking. Our findings uncover exactly how much such entity linking systems depend on candidate sets, and how much this limits the general applicability of each system. We present an alternative approach to candidate sets, demonstrating that leveraging the entire in-domain candidate set can serve as a viable substitute for certain models. We show the trade-off between less restrictive candidate sets, increased inference time and memory footprint for some models.",
        "author": "Nicolas Ong; Hassan Shavarani; Anoop Sarkar",
        "authorids": "/n/nicolas-ong/; /h/hassan-s-shavarani/; /a/anoop-sarkar/",
        "bibtex": "@inproceedings{ong-etal-2024-unified,\n    title = \"Unified Examination of Entity Linking in Absence of Candidate Sets\",\n    author = \"Ong, Nicolas  and\n      Shavarani, Hassan  and\n      Sarkar, Anoop\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.11/\",\n    doi = \"10.18653/v1/2024.naacl-short.11\",\n    pages = \"113--123\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.11.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.11/",
        "pdf_size": 191864,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12112777316202313118&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "School of Computing Science, Simon Fraser University, BC, Canada; School of Computing Science, Simon Fraser University, BC, Canada; School of Computing Science, Simon Fraser University, BC, Canada",
        "aff_domain": "sfu.ca;sfu.ca;sfu.ca",
        "email": "sfu.ca;sfu.ca;sfu.ca",
        "github": "https://github.com/NicolasOng/gerbil_connects",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Simon Fraser University",
        "aff_unique_dep": "School of Computing Science",
        "aff_unique_url": "https://www.sfu.ca",
        "aff_unique_abbr": "SFU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "BC",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "2024.naacl-demo.21",
        "title": "Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "In the dynamic landscape of generative NLP, traditional text processing pipelines limit research flexibility and reproducibility, as they are tailored to specific dataset, task, and model combinations. The escalating complexity, involving system prompts, model-specific formats, instructions, and more, calls for a shift to a structured, modular, and customizable solution.Addressing this need, we present Unitxt, an innovative library for customizable textual data preparation and evaluation tailored to generative language models. Unitxt natively integrates with common libraries like HuggingFace and LM-eval-harness and deconstructs processing flows into modular components, enabling easy customization and sharing between practitioners. These components encompass model-specific formats, task prompts, and many other comprehensive dataset processing definitions. The Unitxt Catalog centralizes these components, fostering collaboration and exploration in modern textual data workflows. Beyond being a tool, Unitxt is a community-driven platform, empowering users to build, share, and advance their pipelines collaboratively. Join the Unitxt community at https://github.com/IBM/unitxt",
        "author": "Elron Bandel; Yotam Perlitz; Elad Venezian; Roni Friedman; Ofir Arviv; Matan Orbach; Shachar Don-Yehiya; Dafna Sheinwald; Ariel Gera; Leshem Choshen; Michal Shmueli-Scheuer; Yoav Katz",
        "authorids": "/e/elron-bandel/; /y/yotam-perlitz/; /e/elad-venezian/; /r/roni-friedman/; /o/ofir-arviv/; /m/matan-orbach/; /s/shachar-don-yehiya/; /d/dafna-sheinwald/; /a/ariel-gera/; /l/leshem-choshen/; /m/michal-shmueli-scheuer/; /y/yoav-katz/",
        "bibtex": "@inproceedings{bandel-etal-2024-unitxt,\n    title = \"Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative {AI}\",\n    author = \"Bandel, Elron  and\n      Perlitz, Yotam  and\n      Venezian, Elad  and\n      Friedman, Roni  and\n      Arviv, Ofir  and\n      Orbach, Matan  and\n      Don-Yehiya, Shachar  and\n      Sheinwald, Dafna  and\n      Gera, Ariel  and\n      Choshen, Leshem  and\n      Shmueli-Scheuer, Michal  and\n      Katz, Yoav\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.21/\",\n    doi = \"10.18653/v1/2024.naacl-demo.21\",\n    pages = \"207--215\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.21.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.21/",
        "pdf_size": 672200,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17542077313828914330&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research",
        "aff_domain": "ibm.com; ; ; ; ; ; ; ; ; ; ; ",
        "email": "ibm.com; ; ; ; ; ; ; ; ; ; ; ",
        "github": "https://github.com/IBM/unitxt",
        "project": "https://bit.ly/unitxt-explore",
        "author_num": 12,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "IBM",
        "aff_unique_dep": "IBM Research",
        "aff_unique_url": "https://www.ibm.com/research",
        "aff_unique_abbr": "IBM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.151",
        "title": "UniverSLU: Universal Spoken Language Understanding for Diverse Tasks with Natural Language Instructions",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent studies leverage large language models with multi-tasking capabilities, using natural language prompts to guide the model\u2019s behavior and surpassing performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly performs various spoken language understanding (SLU) tasks? We start by adapting a pre-trained automatic speech recognition model to additional tasks using single-token task specifiers. We enhance this approach through instruction tuning, i.e., finetuning by describing the task using natural language instructions followed by the list of label options. Our approach can generalize to new task descriptions for the seen tasks during inference, thereby enhancing its user-friendliness. We demonstrate the efficacy of our single multi-task learning model \u201cUniverSLU\u201d for 12 speech classification and sequence generation task types spanning 17 datasets and 9 languages. On most tasks, UniverSLU achieves competitive performance and often even surpasses task-specific models. Additionally, we assess the zero-shot capabilities, finding that the model generalizes to new datasets and languages for seen task types.",
        "author": "Siddhant Arora; Hayato Futami; Jee-weon Jung; Yifan Peng; Roshan Sharma; Yosuke Kashiwagi; Emiru Tsunoo; Karen Livescu; Shinji Watanabe",
        "authorids": "/s/siddhant-arora/; /h/hayato-futami/; /j/jee-weon-jung/; /y/yifan-peng-cmu/; /r/roshan-sharma/; /y/yosuke-kashiwagi/; /e/emiru-tsunoo/; /k/karen-livescu/; /s/shinji-watanabe/",
        "bibtex": "@inproceedings{arora-etal-2024-universlu,\n    title = \"{U}niver{SLU}: Universal Spoken Language Understanding for Diverse Tasks with Natural Language Instructions\",\n    author = \"Arora, Siddhant  and\n      Futami, Hayato  and\n      Jung, Jee-weon  and\n      Peng, Yifan  and\n      Sharma, Roshan  and\n      Kashiwagi, Yosuke  and\n      Tsunoo, Emiru  and\n      Livescu, Karen  and\n      Watanabe, Shinji\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.151/\",\n    doi = \"10.18653/v1/2024.naacl-long.151\",\n    pages = \"2754--2774\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.151.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.151/",
        "pdf_size": 365085,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4151771516356221609&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Carnegie Mellon University, USA; Sony Group Corporation, Japan; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Sony Group Corporation, Japan; Sony Group Corporation, Japan; Toyota Technological Institute at Chicago; Carnegie Mellon University, USA",
        "aff_domain": "cs.cmu.edu; ; ; ; ; ; ; ; ",
        "email": "cs.cmu.edu; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;1;0;0;0;1;1;2;0",
        "aff_unique_norm": "Carnegie Mellon University;Sony Group Corporation;Toyota Technological Institute at Chicago",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.sony.com;https://www.tti-chicago.org",
        "aff_unique_abbr": "CMU;Sony;TTI Chicago",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Chicago",
        "aff_country_unique_index": "0;1;0;0;0;1;1;0;0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "2024.naacl-long.243",
        "title": "Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce Universal NER (UNER), an open, community-driven project to develop gold-standard NER benchmarks in many languages. The overarching goal of UNER is to provide high-quality, cross-lingually consistent annotations to facilitate and standardize multilingual NER research. UNER v1 contains 19 datasets annotated with named entities in a cross-lingual consistent schema across 13 diverse languages. In this paper, we detail the dataset creation and composition of UNER; we also provide initial modeling baselines on both in-language and cross-lingual learning settings. We will release the data, code, and fitted models to the public.",
        "author": "Stephen Mayhew; Terra Blevins; Shuheng Liu; Marek \u0160uppa; Hila Gonen; Joseph Marvin Imperial; B\u00f6rje F. Karlsson; Peiqin Lin; Nikola Ljube\u0161i\u0107; LJ Miranda; Barbara Plank; Arij Riabi; Yuval Pinter",
        "authorids": "/s/stephen-mayhew/; /t/terra-blevins/; /s/shuheng-liu/; /m/marek-suppa/; /h/hila-gonen/; /j/joseph-marvin-imperial/; /b/borje-f-karlsson/; /p/peiqin-lin/; /n/nikola-ljubesic/; /l/lj-miranda/; /b/barbara-plank/; /a/arij-riabi/; /y/yuval-pinter/",
        "bibtex": "@inproceedings{mayhew-etal-2024-universal,\n    title = \"Universal {NER}: A Gold-Standard Multilingual Named Entity Recognition Benchmark\",\n    author = {Mayhew, Stephen  and\n      Blevins, Terra  and\n      Liu, Shuheng  and\n      {\\v{S}}uppa, Marek  and\n      Gonen, Hila  and\n      Imperial, Joseph Marvin  and\n      Karlsson, B{\\\"o}rje F.  and\n      Lin, Peiqin  and\n      Ljube{\\v{s}}i{\\'c}, Nikola  and\n      Miranda, LJ  and\n      Plank, Barbara  and\n      Riabi, Arij  and\n      Pinter, Yuval},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.243/\",\n    doi = \"10.18653/v1/2024.naacl-long.243\",\n    pages = \"4322--4337\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.243.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.243/",
        "pdf_size": 696844,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3705947970808557986&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Duolingo; University of Washington; Georgia Institute of Technology; Comenius University in Bratislava+Cisco; University of Washington; National University Philippines+University of Bath; Beijing Academy of Artificial Intelligence; LMU Munich; Jo\u017eef Stefan Institute; Allen Institute for Artificial Intelligence; LMU Munich+IT University of Copenhagen; Inria Paris+Sorbonne Universit\u00e9; Ben-Gurion University",
        "aff_domain": "duolingo.com;cs.washington.edu; ; ; ; ; ; ; ; ; ; ;",
        "email": "duolingo.com;cs.washington.edu; ; ; ; ; ; ; ; ; ; ;",
        "github": "",
        "project": "https://www.universalner.org",
        "author_num": 13,
        "aff_unique_index": "0;1;2;3+4;1;5+6;7;8;9;10;8+11;12+13;14",
        "aff_unique_norm": "Duolingo;University of Washington;Georgia Institute of Technology;Comenius University;Cisco Systems;National University;University of Bath;Beijing Academy of Artificial Intelligence;Ludwig Maximilian University of Munich;Jo\u017eef Stefan Institute;Allen Institute for Artificial Intelligence;IT University of Copenhagen;INRIA;Sorbonne Universit\u00e9;Ben-Gurion University of the Negev",
        "aff_unique_dep": ";;;;;;;;;;;;;;",
        "aff_unique_url": "https://www.duolingo.com;https://www.washington.edu;https://www.gatech.edu;https://www.uniba.sk;https://www.cisco.com;https://www.nu.edu.ph;https://www.bath.ac.uk;https://www.baaic.cn;https://www.lmu.de;https://www.ijs.si;https://allenai.org;https://itu.dk;https://www.inria.fr;https://www.sorbonne-universite.fr;https://www.bgu.ac.il",
        "aff_unique_abbr": "Duolingo;UW;Georgia Tech;Comenius;Cisco;NU;Bath;BAAI;LMU;JSI;AI2;ITU;Inria;Sorbonne U;BGU",
        "aff_campus_unique_index": "1;;2;2;3",
        "aff_campus_unique": ";Bratislava;Munich;Paris",
        "aff_country_unique_index": "0;0;0;1+0;0;2+3;4;5;6;0;5+7;8+8;9",
        "aff_country_unique": "United States;Slovakia;Philippines;United Kingdom;China;Germany;Slovenia;Denmark;France;Israel"
    },
    {
        "id": "2024.naacl-long.351",
        "title": "Universal Prompt Optimizer for Safe Text-to-Image Generation",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, we propose the first universal **p**rompt **o**ptimizer for **s**afe T2**I** (**POSI**) generation in black-box scenario. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating inappropriate images, with no significant impact on text alignment. It is also flexible to be combined with methods to achieve better performance. Our code is available at [https://github.com/wzongyu/POSI](https://github.com/wzongyu/POSI).",
        "author": "Zongyu Wu; Hongcheng Gao; Yueze Wang; Xiang Zhang; Suhang Wang",
        "authorids": "/z/zongyu-wu/; /h/hongcheng-gao/; /y/yueze-wang/; /x/xiang-zhang/; /s/suhang-wang/",
        "bibtex": "@inproceedings{wu-etal-2024-universal,\n    title = \"Universal Prompt Optimizer for Safe Text-to-Image Generation\",\n    author = \"Wu, Zongyu  and\n      Gao, Hongcheng  and\n      Wang, Yueze  and\n      Zhang, Xiang  and\n      Wang, Suhang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.351/\",\n    doi = \"10.18653/v1/2024.naacl-long.351\",\n    pages = \"6340--6354\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.351.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.351/",
        "pdf_size": 1352601,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12406092254780628533&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "The Pennsylvania State University; University of Chinese Academy of Sciences; Tianjin University; The Pennsylvania State University; The Pennsylvania State University",
        "aff_domain": "psu.edu;mails.ucas.ac.cn; ;psu.edu; ",
        "email": "psu.edu;mails.ucas.ac.cn; ;psu.edu; ",
        "github": "https://github.com/wu-zongyu/POSI",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;0;0",
        "aff_unique_norm": "Pennsylvania State University;University of Chinese Academy of Sciences;Tianjin University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.psu.edu;http://www.ucas.ac.cn;http://www.tju.edu.cn",
        "aff_unique_abbr": "PSU;UCAS;TJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.15",
        "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds\u2019 strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
        "author": "Zhenhailong Wang; Shaoguang Mao; Wenshan Wu; Tao Ge; Furu Wei; Heng Ji",
        "authorids": "/z/zhenhailong-wang/; /s/shaoguang-mao/; /w/wenshan-wu/; /t/tao-ge/; /f/furu-wei/; /h/heng-ji/",
        "bibtex": "@inproceedings{wang-etal-2024-unleashing,\n    title = \"Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration\",\n    author = \"Wang, Zhenhailong  and\n      Mao, Shaoguang  and\n      Wu, Wenshan  and\n      Ge, Tao  and\n      Wei, Furu  and\n      Ji, Heng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.15/\",\n    doi = \"10.18653/v1/2024.naacl-long.15\",\n    pages = \"257--279\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.15.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.15/",
        "pdf_size": 4417633,
        "gs_citation": 246,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16275833962129130088&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Illinois Urbana-Champaign; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; University of Illinois Urbana-Champaign",
        "aff_domain": "illinois.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;illinois.edu",
        "email": "illinois.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com;illinois.edu",
        "github": "https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Microsoft",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "https://illinois.edu;https://www.microsoft.com/en-us/research/group/asia",
        "aff_unique_abbr": "UIUC;MSR Asia",
        "aff_campus_unique_index": "0;1;1;1;1;0",
        "aff_campus_unique": "Urbana-Champaign;Asia",
        "aff_country_unique_index": "0;1;1;1;1;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.findings-naacl.177",
        "title": "Unleashing the Power of LLMs in Court View Generation by Stimulating Internal Knowledge and Incorporating External Knowledge",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Court View Generation (CVG) plays a vital role in the realm of legal artificial intelligence, which aims to support judges in crafting legal judgment documents. The court view consists of three essential judgment parts: the charge-related, law article-related, and prison term-related parts, each requiring specialized legal knowledge, rendering CVG a challenging task.Although Large Language Models (LLMs) have made remarkable strides in language generation, they encounter difficulties in the knowledge-intensive legal domain.Actually, there can be two types of knowledge: internal knowledge stored within LLMs\u2019 parameters and external knowledge sourced from legal documents outside the models.In this paper, we decompose court views into different parts, stimulate internal knowledge, and incorporate external information to unleash the power of LLMs in the CVG task.To validate our method, we conduct a series of experiment results on two real-world datasets LAIC2021 and CJO2022. The experiments demonstrate that our method is capable of generating more accurate and reliable court views.",
        "author": "Yifei Liu; Yiquan Wu; Ang Li; Yating Zhang; Changlong Sun; Weiming Lu; Fei Wu; Kun Kuang",
        "authorids": "/y/yifei-liu/; /y/yiquan-wu/; /a/ang-li/; /y/yating-zhang/; /c/changlong-sun/; /w/weiming-lu/; /f/fei-wu/; /k/kun-kuang/",
        "bibtex": "@inproceedings{liu-etal-2024-unleashing,\n    title = \"Unleashing the Power of {LLM}s in Court View Generation by Stimulating Internal Knowledge and Incorporating External Knowledge\",\n    author = \"Liu, Yifei  and\n      Wu, Yiquan  and\n      Li, Ang  and\n      Zhang, Yating  and\n      Sun, Changlong  and\n      Lu, Weiming  and\n      Wu, Fei  and\n      Kuang, Kun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.177/\",\n    doi = \"10.18653/v1/2024.findings-naacl.177\",\n    pages = \"2782--2792\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.177.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.177/",
        "pdf_size": 1485616,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15102139405187959743&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "School of Software Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; Alibaba Group; Alibaba Group; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University",
        "aff_domain": "zju.edu.cn;zju.edu.cn;zju.edu.cn;gmail.com;taobao.com;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "email": "zju.edu.cn;zju.edu.cn;zju.edu.cn;gmail.com;taobao.com;zju.edu.cn;zju.edu.cn;zju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;0;0;1;1;0;0;0",
        "aff_unique_norm": "Zhejiang University;Alibaba Group",
        "aff_unique_dep": "School of Software Technology;",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.alibaba.com",
        "aff_unique_abbr": "ZJU;Alibaba",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.144",
        "title": "Unlocking Emergent Modularity in Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Modular Neural Networks (MNNs) demonstrate various advantages over monolithic models.Existing MNNs are generally explicit: their modular architectures are pre-defined, with individual modules expected to implement distinct functions.Recent works reveal that there exists implicit modularity in standard pre-trained transformers, namely Emergent Modularity.They indicate that such modular structures spontaneously exhibit during the early pre-training phase.Despite the benefits of modularity, most Language Models (LMs) are still treated as monolithic models in the pre-train and fine-tune paradigm, with their emergent modularity locked and underutilized.In this work, focusing on unlocking the emergent modularity in LMs, we showcase that standard LMs could be fine-tuned as their Mixture-of-Expert (MoEs) counterparts without introducing any extra parameters. Such MoEs are derived from emergent modularity and are referred to as Emergent MoEs (EMoE).Our experiments demonstrate that fine-tuning EMoE effectively improves downstream in-domain and out-of-domain generalization compared with vanilla fine-tuning.Our analysis and ablation studies further illustrate that it is robust to various configurations and can scale up to Large Language Models (i.e., Llama2-7B and Llama-30B). Code is available at https://github.com/qiuzh20/EMoE.",
        "author": "Zihan Qiu; Zeyu Huang; Jie Fu",
        "authorids": "/z/zihan-qiu/; /z/zeyu-huang/; /j/jie-fu/",
        "bibtex": "@inproceedings{qiu-etal-2024-unlocking,\n    title = \"Unlocking Emergent Modularity in Large Language Models\",\n    author = \"Qiu, Zihan  and\n      Huang, Zeyu  and\n      Fu, Jie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.144/\",\n    doi = \"10.18653/v1/2024.naacl-long.144\",\n    pages = \"2638--2660\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.144.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.144/",
        "pdf_size": 1731773,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8703869697991455560&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "CSE, HKUST+IIIS, Tsinghua University; ILCC, University of Edinburgh; CSE, HKUST",
        "aff_domain": "gmail.com;ed.ac.uk;ust.hk",
        "email": "gmail.com;ed.ac.uk;ust.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+1;2;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Tsinghua University;University of Edinburgh",
        "aff_unique_dep": "Department of Computer Science and Engineering;Institute for Interdisciplinary Information Sciences;ILCC",
        "aff_unique_url": "https://www.ust.hk;https://www.tsinghua.edu.cn;https://www.ed.ac.uk",
        "aff_unique_abbr": "HKUST;THU;Edinburgh",
        "aff_campus_unique_index": "0;2;0",
        "aff_campus_unique": "Hong Kong SAR;;Edinburgh",
        "aff_country_unique_index": "0+0;1;0",
        "aff_country_unique": "China;United Kingdom"
    },
    {
        "id": "2024.findings-naacl.263",
        "title": "Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Parameter-efficient fine-tuning (PEFT) methods are increasingly vital in adapting large-scale pre-trained language models for diverse tasks, offering a balance between adaptability and computational efficiency. They are important in Low-Resource Language (LRL) Neural Machine Translation (NMT) to enhance translation accuracy with minimal resources. However, their practical effectiveness varies significantly across different languages. We conducted comprehensive empirical experiments with varying LRL domains and sizes to evaluate the performance of 8 PEFT methods with in total of 15 architectures using the SacreBLEU score. We showed that 6 PEFT architectures outperform the baseline for both in-domain and out-domain tests and the Houlsby+Inversion adapter has the best performance overall, proving the effectiveness of PEFT methods.",
        "author": "Tong Su; Xin Peng; Sarubi Thillainathan; David Guzm\u00e1n; Surangika Ranathunga; En-Shiun Lee",
        "authorids": "/t/tong-su/; /x/xin-peng/; /s/sarubi-thillainathan/; /d/david-guzman/; /s/surangika-ranathunga/; /e/en-shiun-lee/",
        "bibtex": "@inproceedings{su-etal-2024-unlocking,\n    title = \"Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation\",\n    author = \"Su, Tong  and\n      Peng, Xin  and\n      Thillainathan, Sarubi  and\n      Guzm{\\'a}n, David  and\n      Ranathunga, Surangika  and\n      Lee, En-Shiun\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.263/\",\n    doi = \"10.18653/v1/2024.findings-naacl.263\",\n    pages = \"4217--4225\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.263.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.263/",
        "pdf_size": 393819,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12756514853786705251&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Toronto; Saarland University; Massey University; University of Toronto; Massey University; University of Toronto+Ontario Tech University",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;2;0;2;0+3",
        "aff_unique_norm": "University of Toronto;Saarland University;Massey University;Ontario Tech University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.uni-saarland.de;https://www.massey.ac.nz;https://www.ontariotechu.ca",
        "aff_unique_abbr": "U of T;UdS;MU;OntTechU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;2;0;2;0+0",
        "aff_country_unique": "Canada;Germany;New Zealand"
    },
    {
        "id": "2024.naacl-short.9",
        "title": "Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for Positional Discourse Coherence",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Recent large language models (LLMs) have shown remarkable performance in aligning generated text with user intentions across various tasks. When it comes to long-form text generation, there has been a growing interest in generation from a discourse coherence perspective.However, existing lexical or semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the discourse coherence.The development of discourse-specific automatic evaluation methods for assessing the output of LLMs warrants greater focus and exploration. In this paper, we present a novel automatic metric designed to quantify the discourse divergence between two long-form articles.Extensive experiments on three datasets from representative domains demonstrate that our metric aligns more closely with human preferences and GPT-4 coherence evaluation, outperforming existing evaluation methods.",
        "author": "Yinhong Liu; Yixuan Su; Ehsan Shareghi; Nigel Collier",
        "authorids": "/y/yinhong-liu/; /y/yixuan-su/; /e/ehsan-shareghi/; /n/nigel-collier/",
        "bibtex": "@inproceedings{liu-etal-2024-unlocking,\n    title = \"Unlocking Structure Measuring: Introducing {PDD}, an Automatic Metric for Positional Discourse Coherence\",\n    author = \"Liu, Yinhong  and\n      Su, Yixuan  and\n      Shareghi, Ehsan  and\n      Collier, Nigel\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.9/\",\n    doi = \"10.18653/v1/2024.naacl-short.9\",\n    pages = \"92--100\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.9.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.9/",
        "pdf_size": 313037,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:CaSa6jTl7aEJ:scholar.google.com/&scioq=Unlocking+Structure+Measuring:+Introducing+PDD,+an+Automatic+Metric+for+Positional+Discourse+Coherence&hl=en&as_sdt=0,5",
        "gs_version_total": 6,
        "aff": "Language Technology Lab, University of Cambridge; Language Technology Lab, University of Cambridge; Department of Data Science and AI, Monash University + Language Technology Lab, University of Cambridge; Language Technology Lab, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;monash.edu;cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;monash.edu;cam.ac.uk",
        "github": "https://github.com/williamLyh/pos_div_metric",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1+0;0",
        "aff_unique_norm": "University of Cambridge;Monash University",
        "aff_unique_dep": "Language Technology Lab;Department of Data Science and AI",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.monash.edu",
        "aff_unique_abbr": "Cambridge;Monash",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0;1+0;0",
        "aff_country_unique": "United Kingdom;Australia"
    },
    {
        "id": "2024.naacl-short.20",
        "title": "Unveiling Divergent Inductive Biases of LLMs on Temporal Data",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Unraveling the intricate details of events in natural language necessitates a subtle understanding of temporal dynamics. Despite the adeptness of Large Language Models (LLMs) in discerning patterns and relationships from data, their inherent comprehension of temporal dynamics remains a formidable challenge. This research meticulously explores these intrinsic challenges within LLMs, with a specific emphasis on evaluating the performance of GPT-3.5 and GPT-4 models in the analysis of temporal data. Employing two distinct prompt types, namely Question Answering (QA) format and Textual Entailment (TE) format, our analysis probes into both implicit and explicit events. The findings underscore noteworthy trends, revealing disparities in the performance of GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships come to light, with GPT-3.5 demonstrating a preference for \u201cAFTER\u201d in the QA format for both implicit and explicit events, while GPT-4 leans towards \u201cBEFORE\u201d. Furthermore, a consistent pattern surfaces wherein GPT-3.5 tends towards \u201cTRUE\u201d, and GPT-4 exhibits a preference for \u201cFALSE\u201d in the TE format for both implicit and explicit events. This persistent discrepancy between GPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of inductive bias in LLMs, suggesting that the evolution of these models may not merely mitigate bias but may introduce new layers of complexity.",
        "author": "Sindhu Kishore; Hangfeng He",
        "authorids": "/s/sindhu-kishore/; /h/hangfeng-he/",
        "bibtex": "@inproceedings{kishore-he-2024-unveiling,\n    title = \"Unveiling Divergent Inductive Biases of {LLM}s on Temporal Data\",\n    author = \"Kishore, Sindhu  and\n      He, Hangfeng\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.20/\",\n    doi = \"10.18653/v1/2024.naacl-short.20\",\n    pages = \"220--228\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.20.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.20/",
        "pdf_size": 601462,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16851982949392980535&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Rochester; University of Rochester",
        "aff_domain": "ur.rochester.edu;rochester.edu",
        "email": "ur.rochester.edu;rochester.edu",
        "github": "https://github.com/SindhuKRao/LLM_temporal_Bias",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Rochester",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rochester.edu",
        "aff_unique_abbr": "U of R",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.51",
        "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs\u2019 generalization ability are not fully understood.This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets.Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks.Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model\u2019s generalization ability.Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs.",
        "author": "Haoran Yang; Yumeng Zhang; Jiaqi Xu; Hongyuan Lu; Pheng-Ann Heng; Wai Lam",
        "authorids": "/h/haoran-yang/; /y/yumeng-zhang/; /j/jiaqi-xu/; /h/hongyuan-lu/; /p/pheng-ann-heng/; /w/wai-lam/",
        "bibtex": "@inproceedings{yang-etal-2024-unveiling,\n    title = \"Unveiling the Generalization Power of Fine-Tuned Large Language Models\",\n    author = \"Yang, Haoran  and\n      Zhang, Yumeng  and\n      Xu, Jiaqi  and\n      Lu, Hongyuan  and\n      Heng, Pheng-Ann  and\n      Lam, Wai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.51/\",\n    doi = \"10.18653/v1/2024.naacl-long.51\",\n    pages = \"884--899\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.51.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.51/",
        "pdf_size": 7692991,
        "gs_citation": 57,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12513985121088474213&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The Chinese University of Hong Kong; Tsinghua University; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong",
        "aff_domain": "se.cuhk.edu.hk;mails.tsinghua.edu.cn;cse.cuhk.edu.hk;se.cuhk.edu.hk;cse.cuhk.edu.hk;se.cuhk.edu.hk",
        "email": "se.cuhk.edu.hk;mails.tsinghua.edu.cn;cse.cuhk.edu.hk;se.cuhk.edu.hk;cse.cuhk.edu.hk;se.cuhk.edu.hk",
        "github": "https://github.com/LHRYANG/Generalization_of_FT-LLM",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong;Tsinghua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "CUHK;THU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-short.65",
        "title": "Unveiling the Magic: Investigating Attention Distillation in Retrieval-Augmented Generation",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Retrieval-augmented generation framework addresses the limitations of large language models by enabling real-time knowledge updates for more accurate answers. An efficient way in the training phase of retrieval-augmented models is attention distillation, which uses attention scores as supervision signals instead of manually annotated query-document pairs. Despite its growing popularity, the detailed mechanisms behind the success of attention distillation remain unexplored, particularly the specific patterns it leverages to benefit training. In this paper, we address this gap by conducting a comprehensive investigation of attention distillation workflow and identifying key factors influencing the learning performance of retrieval-augmented language models. We further propose several insightful indicators for optimizing models\u2019 training methods and avoiding ineffective training.",
        "author": "Zizhong Li; Haopeng Zhang; Jiawei Zhang",
        "authorids": "/z/zizhong-li/; /h/haopeng-zhang/; /j/jiawei-zhang/",
        "bibtex": "@inproceedings{li-etal-2024-unveiling,\n    title = \"Unveiling the Magic: Investigating Attention Distillation in Retrieval-Augmented Generation\",\n    author = \"Li, Zizhong  and\n      Zhang, Haopeng  and\n      Zhang, Jiawei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.65/\",\n    doi = \"10.18653/v1/2024.naacl-short.65\",\n    pages = \"745--754\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.65.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.65/",
        "pdf_size": 1323867,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3918494058195990714&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3
    },
    {
        "id": "2024.findings-naacl.15",
        "title": "VLUE: A New Benchmark and Multi-task Knowledge Transfer Learning for Vietnamese Natural Language Understanding",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The success of Natural Language Understanding (NLU) benchmarks in various languages, such as GLUE for English, CLUE for Chinese, KLUE for Korean, and IndoNLU for Indonesian, has facilitated the evaluation of new NLU models across a wide range of tasks. To establish a standardized set of benchmarks for Vietnamese NLU, we introduce the first Vietnamese Language Understanding Evaluation (VLUE) benchmark. The VLUE benchmark encompasses five datasets covering different NLU tasks, including text classification, span extraction, and natural language understanding. To provide an insightful overview of the current state of Vietnamese NLU, we then evaluate seven state-of-the-art pre-trained models, including both multilingual and Vietnamese monolingual models, on our proposed VLUE benchmark. Furthermore, we present CafeBERT, a new state-of-the-art pre-trained model that achieves superior results across all tasks in the VLUE benchmark. Our model combines the proficiency of a multilingual pre-trained model with Vietnamese linguistic knowledge. CafeBERT is developed based on the XLM-RoBERTa model, with an additional pretraining step utilizing a significant amount of Vietnamese textual data to enhance its adaptation to the Vietnamese language. For the purpose of future research, CafeBERT is made publicly available for research purposes.",
        "author": "Phong Nguyen-Thuan Do; Son Quoc Tran; Phu Gia Hoang; Kiet Van Nguyen; Ngan Luu-Thuy Nguyen",
        "authorids": "/p/phong-nguyen-thuan-do/; /s/son-quoc-tran/; /p/phu-gia-hoang/; /k/kiet-van-nguyen/; /n/ngan-luu-thuy-nguyen/",
        "bibtex": "@inproceedings{do-etal-2024-vlue,\n    title = \"{VLUE}: A New Benchmark and Multi-task Knowledge Transfer Learning for {V}ietnamese Natural Language Understanding\",\n    author = \"Do, Phong Nguyen-Thuan  and\n      Tran, Son Quoc  and\n      Hoang, Phu Gia  and\n      Nguyen, Kiet Van  and\n      Nguyen, Ngan Luu-Thuy\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.15/\",\n    doi = \"10.18653/v1/2024.findings-naacl.15\",\n    pages = \"211--222\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.15.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.15/",
        "pdf_size": 223682,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17319224377368932226&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "https://uitnlpgroup.github.io/VLUE/",
        "project": "https://huggingface.co/uitnlp/CafeBERT",
        "author_num": 5
    },
    {
        "id": "2024.findings-naacl.26",
        "title": "VOLTA: Improving Generative Diversity by Variational Mutual Information Maximizing Autoencoder",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The natural language generation domain has witnessed great success thanks to Transformer models. Although they have achieved state-of-the-art generative quality, they often neglect generative diversity. Prior attempts to tackle this issue suffer from either low model capacity or over-complicated architectures. Some recent methods employ the VAE framework to enhance diversity, but their latent variables fully depend on the input context, restricting exploration of the latent space. In this paper, we introduce VOLTA, a framework that elevates generative diversity by bridging Transformer with VAE via a more effective cross-attention-based connection, departing from conventional embedding concatenation or summation. Additionally, we propose integrating InfoGAN-style latent codes to enable input-independent variability, further diversifying the generation. Moreover, our framework accommodates discrete inputs alongside its existing support for continuous inputs. We perform comprehensive experiments with two types of Transformers on six datasets from three different NLG tasks to show that our approach can significantly improve generative diversity while maintaining generative quality.",
        "author": "Yueen Ma; DaFeng Chi; Jingjing Li; Kai Song; Yuzheng Zhuang; Irwin King",
        "authorids": "/y/yueen-ma/; /d/dafeng-chi/; /j/jingjing-li/; /k/kai-song/; /y/yuzheng-zhuang/; /i/irwin-king/",
        "bibtex": "@inproceedings{ma-etal-2024-volta,\n    title = \"{VOLTA}: Improving Generative Diversity by Variational Mutual Information Maximizing Autoencoder\",\n    author = \"Ma, Yueen  and\n      Chi, DaFeng  and\n      Li, Jingjing  and\n      Song, Kai  and\n      Zhuang, Yuzheng  and\n      King, Irwin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.26/\",\n    doi = \"10.18653/v1/2024.findings-naacl.26\",\n    pages = \"364--378\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.26.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.26/",
        "pdf_size": 2191206,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:UGdIgcYEv3UJ:scholar.google.com/&scioq=VOLTA:+Improving+Generative+Diversity+by+Variational+Mutual+Information+Maximizing+Autoencoder&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "The Chinese University of Hong Kong1; Huawei Noah\u2019s Ark Lab2; The Chinese University of Hong Kong1; ; Huawei Noah\u2019s Ark Lab2; The Chinese University of Hong Kong1",
        "aff_domain": "cse.cuhk.edu.hk;huawei.com;cse.cuhk.edu.hk;gmail.com;huawei.com;cse.cuhk.edu.hk",
        "email": "cse.cuhk.edu.hk;huawei.com;cse.cuhk.edu.hk;gmail.com;huawei.com;cse.cuhk.edu.hk",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "Chinese University of Hong Kong;Huawei",
        "aff_unique_dep": ";Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.cuhk.edu.hk;https://www.huawei.com",
        "aff_unique_abbr": "CUHK;Huawei",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.486",
        "title": "Value FULCRA: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Value",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Value alignment is crucial for the responsible development of Large Language Models (LLMs). However, how to define values in this context remains largely unexplored. Existing work mainly specifies values as risk criteria formulated in the AI community, e.g., fairness and privacy protection, suffering from poor clarity, adaptability and transparency. Leveraging basic values established in humanity and social science that are compatible with values across cultures, this paper introduces a novel value space spanned by multiple basic value dimensions and proposes BaseAlign, a corresponding value alignment paradigm. Applying the representative Schwartz\u2019s Theory of Basic Values as an instantiation, we construct FULCRA, a dataset consisting of 20k (LLM output, value vector) pairs. LLMs\u2019 outputs are mapped into the K-dim value space beyond simple binary labels, by identifying their underlying priorities for these value dimensions. Extensive analysis and experiments on FULCRA: (1) reveal the essential relation between basic values and LLMs\u2019 behaviors, (2) demonstrate that our paradigm with basic values not only covers existing risks but also anticipates the unidentified ones, and (3) manifest BaseAlign\u2019s superiority in alignment performance with less data, paving the way for addressing the above three challenges.",
        "author": "Jing Yao; Xiaoyuan Yi; Yifan Gong; Xiting Wang; Xing Xie",
        "authorids": "/j/jing-yao/; /x/xiaoyuan-yi/; /y/yifan-gong/; /x/xiting-wang/; /x/xing-xie/",
        "bibtex": "@inproceedings{yao-etal-2024-value,\n    title = \"Value {FULCRA}: Mapping Large Language Models to the Multidimensional Spectrum of Basic Human Value\",\n    author = \"Yao, Jing  and\n      Yi, Xiaoyuan  and\n      Gong, Yifan  and\n      Wang, Xiting  and\n      Xie, Xing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.486/\",\n    doi = \"10.18653/v1/2024.naacl-long.486\",\n    pages = \"8762--8785\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.486.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.486/",
        "pdf_size": 1945190,
        "gs_citation": 37,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11658053939228424595&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China + Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China; Microsoft Research Asia, Beijing, China",
        "aff_domain": "microsoft.com;microsoft.com;microsoft.com;ruc.edu.cn;microsoft.com",
        "email": "microsoft.com;microsoft.com;microsoft.com;ruc.edu.cn;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1+2;0",
        "aff_unique_norm": "Microsoft;Renmin University of China;Beijing Key Laboratory of Big Data Management and Analysis Methods",
        "aff_unique_dep": "Research;Gaoling School of Artificial Intelligence;Big Data Management and Analysis",
        "aff_unique_url": "https://www.microsoft.com/en-us/research/group/asia;http://www.ruc.edu.cn;",
        "aff_unique_abbr": "MSRA;RUC;",
        "aff_campus_unique_index": "0;0;0;0+0;0",
        "aff_campus_unique": "Beijing",
        "aff_country_unique_index": "0;0;0;0+0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-short.62",
        "title": "Verifying Claims About Metaphors with Large-Scale Automatic Metaphor Identification",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "There are several linguistic claims about situations where words are more likely to be used as metaphors.However, few studies have sought to verify such claims with large corpora.This study entails a large-scale, corpus-based analysis of certain existing claims about verb metaphors, by applying metaphor detection to sentences extracted from Common Crawl and using the statistics obtained from the results.The verification results indicate that the direct objects of verbs used as metaphors tend to have lower degrees of concreteness, imageability, and familiarity, and that metaphors are more likely to be used in emotional and subjective sentences.",
        "author": "Kotaro Aono; Ryohei Sasano; Koichi Takeda",
        "authorids": "/k/kotaro-aono/; /r/ryohei-sasano/; /k/koichi-takeda/",
        "bibtex": "@inproceedings{aono-etal-2024-verifying,\n    title = \"Verifying Claims About Metaphors with Large-Scale Automatic Metaphor Identification\",\n    author = \"Aono, Kotaro  and\n      Sasano, Ryohei  and\n      Takeda, Koichi\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.62/\",\n    doi = \"10.18653/v1/2024.naacl-short.62\",\n    pages = \"711--719\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.62.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.62/",
        "pdf_size": 163497,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10534962271084385254&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Graduate School of Informatics, Nagoya University; Graduate School of Informatics, Nagoya University; Graduate School of Informatics, Nagoya University",
        "aff_domain": "s.mail.nagoya-u.ac.jp;i.nagoya-u.ac.jp;i.nagoya-u.ac.jp",
        "email": "s.mail.nagoya-u.ac.jp;i.nagoya-u.ac.jp;i.nagoya-u.ac.jp",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Nagoya University",
        "aff_unique_dep": "Graduate School of Informatics",
        "aff_unique_url": "https://www.nagoya-u.ac.jp",
        "aff_unique_abbr": "Nagoya U",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Nagoya",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "2024.naacl-long.41",
        "title": "VertAttack: Taking Advantage of Text Classifiers\u2019 Horizontal Vision",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Text classification systems have continuouslyimproved in performance over the years. How-ever, nearly all current SOTA classifiers have asimilar shortcoming, they process text in a hor-izontal manner. Vertically written words willnot be recognized by a classifier. In contrast,humans are easily able to recognize and readwords written both horizontally and vertically.Hence, a human adversary could write problem-atic words vertically and the meaning wouldstill be preserved to other humans. We simulatesuch an attack, VertAttack. VertAttack identifieswhich words a classifier is reliant on and thenrewrites those words vertically. We find thatVertAttack is able to greatly drop the accuracyof 4 different transformer models on 5 datasets.For example, on the SST2 dataset, VertAttackis able to drop RoBERTa\u2019s accuracy from 94 to13%. Furthermore, since VertAttack does notreplace the word, meaning is easily preserved.We verify this via a human study and find thatcrowdworkers are able to correctly label 77%perturbed texts perturbed, compared to 81% ofthe original texts. We believe VertAttack offersa look into how humans might circumvent clas-sifiers in the future and thus inspire a look intomore robust algorithms.",
        "author": "Jonathan Rusert",
        "authorids": "/j/jonathan-rusert/",
        "bibtex": "@inproceedings{rusert-2024-vertattack,\n    title = \"{V}ert{A}ttack: Taking Advantage of Text Classifiers' Horizontal Vision\",\n    author = \"Rusert, Jonathan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.41/\",\n    doi = \"10.18653/v1/2024.naacl-long.41\",\n    pages = \"719--732\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.41.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.41/",
        "pdf_size": 305714,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10536515261195919095&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1
    },
    {
        "id": "2024.findings-naacl.261",
        "title": "ViGLUE: A Vietnamese General Language Understanding Benchmark and Analysis of Vietnamese Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "As the number of language models has increased, various benchmarks have been suggested to assess the proficiency of the models in natural language understanding. However, there is a lack of such a benchmark in Vietnamese due to the difficulty in accessing natural language processing datasets or the scarcity of task-specific datasets. **ViGLUE**, the proposed dataset collection, is a **Vi**etnamese **G**eneral **L**anguage **U**nderstanding **E**valuation benchmark developed using three methods: translating an existing benchmark, generating new corpora, and collecting available datasets. ViGLUE contains twelve tasks and encompasses over ten areas and subjects, enabling it to evaluate models comprehensively over a broad spectrum of aspects. Baseline models utilizing multilingual language models are also provided for all tasks in the proposed benchmarks. In addition, the study of the available Vietnamese large language models is conducted to explore the language models\u2019 ability in the few-shot learning framework, leading to the exploration of the relationship between specific tasks and the number of shots.",
        "author": "Minh-Nam Tran; Phu-Vinh Nguyen; Long Nguyen; Dien Dinh",
        "authorids": "/m/minh-nam-tran/; /p/phu-vinh-nguyen/; /l/long-nguyen/; /d/dinh-dien/",
        "bibtex": "@inproceedings{tran-etal-2024-viglue,\n    title = \"{V}i{GLUE}: A {V}ietnamese General Language Understanding Benchmark and Analysis of {V}ietnamese Language Models\",\n    author = \"Tran, Minh-Nam  and\n      Nguyen, Phu-Vinh  and\n      Nguyen, Long  and\n      Dinh, Dien\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.261/\",\n    doi = \"10.18653/v1/2024.findings-naacl.261\",\n    pages = \"4174--4189\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.261.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.261/",
        "pdf_size": 434735,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2508740755573983134&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam+Vietnam National University, Ho Chi Minh City, Vietnam; Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam+Vietnam National University, Ho Chi Minh City, Vietnam; Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam+Vietnam National University, Ho Chi Minh City, Vietnam; Faculty of Information Technology, University of Science, Ho Chi Minh City, Vietnam+Vietnam National University, Ho Chi Minh City, Vietnam",
        "aff_domain": "apcs.fitus.edu.vn;apcs.fitus.edu.vn;fit.hcmus.edu.vn;fit.hcmus.edu.vn",
        "email": "apcs.fitus.edu.vn;apcs.fitus.edu.vn;fit.hcmus.edu.vn;fit.hcmus.edu.vn",
        "github": "https://github.com/trminhnam/ViGLUE",
        "project": "https://huggingface.co/datasets/tmnam20/ViGLUE",
        "author_num": 4,
        "aff_unique_index": "0+1;0+1;0+1;0+1",
        "aff_unique_norm": "University of Science;Vietnam National University",
        "aff_unique_dep": "Faculty of Information Technology;",
        "aff_unique_url": ";https://www.vnu.edu.vn",
        "aff_unique_abbr": ";VNU",
        "aff_campus_unique_index": "0+0;0+0;0+0;0+0",
        "aff_campus_unique": "Ho Chi Minh City",
        "aff_country_unique_index": "0+0;0+0;0+0;0+0",
        "aff_country_unique": "Vietnam"
    },
    {
        "id": "2024.naacl-long.117",
        "title": "VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "This paper presents VisLingInstruct, a novel approach to advancing Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show impressive zero-shot abilities in multi-modal tasks, but their performance depends heavily on the quality of instructions. VisLingInstruct tackles this by autonomously evaluating and optimizing instructional texts through In-Context Learning, improving the synergy between visual perception and linguistic expression in MMLMs. Alongside this instructional advancement, we have also optimized the visual feature extraction modules in MMLMs, further augmenting their responsiveness to textual content. Our comprehensive experiments on MMLMs, based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1% and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and HatefulMemes datasets. Our main code is available at https://github.com/Zhudongsheng75/VisLingInstruct",
        "author": "Dongsheng Zhu; Daniel Tang; Weidong Han; Jinghui Lu; Yukun Zhao; Guoliang Xing; Junfeng Wang; Dawei Yin",
        "authorids": "/d/dongsheng-zhu/; /d/daniel-tang/; /w/weidong-han/; /j/jinghui-lu/; /y/yukun-zhao/; /g/guoliang-xing/; /j/junfeng-wang/; /d/dawei-yin/",
        "bibtex": "@inproceedings{zhu-etal-2024-vislinginstruct,\n    title = \"{V}is{L}ing{I}nstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization\",\n    author = \"Zhu, Dongsheng  and\n      Tang, Daniel  and\n      Han, Weidong  and\n      Lu, Jinghui  and\n      Zhao, Yukun  and\n      Xing, Guoliang  and\n      Wang, Junfeng  and\n      Yin, Dawei\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.117/\",\n    doi = \"10.18653/v1/2024.naacl-long.117\",\n    pages = \"2122--2135\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.117.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.117/",
        "pdf_size": 6573206,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16584558856896413&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;;;;;",
        "aff_domain": ";;;;;;;",
        "email": ";;;;;;;",
        "github": "",
        "project": "",
        "author_num": 8
    },
    {
        "id": "2024.findings-naacl.206",
        "title": "Visual Enhanced Entity-Level Interaction Network for Multimodal Summarization",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "MultiModal Summarization (MMS) aims to generate a concise summary based on multimodal data like texts and images and has wide application in multimodal fields.Previous works mainly focus on the coarse-level textual and visual features in which the overall features of the image interact with the whole sentence.However, the entities of the input text and the objects of the image may be underutilized, limiting the performance of current MMS models.In this paper, we propose a novel Visual Enhanced Entity-Level Interaction Network (VE-ELIN) to address the problem of underutilization of multimodal inputs at a fine-grained level in two ways.We first design a cross-modal entity interaction module to better fuse the entity information in text and the object information in vision.Then, we design an object-guided visual enhancement module to fully extract the visual features and enhance the focus of the image on the object area.We evaluate VE-ELIN on two MMS datasets and propose new metrics to measure the factual consistency of entities in the output.Finally, experimental results demonstrate that VE-ELIN is effective and outperforms previous methods under both traditional metrics and ours.The source code is available at https://github.com/summoneryhl/VE-ELIN.",
        "author": "Haolong Yan; Binghao Tang; Boda Lin; Gang Zhao; Si Li",
        "authorids": "/h/haolong-yan/; /b/binghao-tang/; /b/boda-lin/; /g/gang-zhao/; /s/si-li/",
        "bibtex": "@inproceedings{yan-etal-2024-visual,\n    title = \"Visual Enhanced Entity-Level Interaction Network for Multimodal Summarization\",\n    author = \"Yan, Haolong  and\n      Tang, Binghao  and\n      Lin, Boda  and\n      Zhao, Gang  and\n      Li, Si\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.206/\",\n    doi = \"10.18653/v1/2024.findings-naacl.206\",\n    pages = \"3248--3260\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.206.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.206/",
        "pdf_size": 2720125,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10983950928370407149&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5
    },
    {
        "id": "2024.naacl-long.71",
        "title": "Visual Grounding Helps Learn Word Meanings in Low-Data Regimes",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Modern neural language models (LMs) are powerful tools for modeling human sentence production and comprehension, and their internal representations are remarkably well-aligned with representations of language in the human brain. But to achieve these results, LMs must be trained in distinctly un-human-like ways \u2014 requiring orders of magnitude more language data than children receive during development, and without perceptual or social context. Do models trained more naturalistically \u2014 with grounded supervision \u2014 exhibit more humanlike language learning? We investigate this question in the context of word learning, a key sub-task in language acquisition. We train a diverse set of LM architectures, with and without auxiliary visual supervision, on datasets of varying scales. We then evaluate these models\u2019 learning of syntactic categories, lexical relations, semantic features, word similarity, and alignment with human neural representations. We find that visual supervision can indeed improve the efficiency of word learning. However, these improvements are limited: they are present almost exclusively in the low-dataregime, and sometimes canceled out by the inclusion of rich distributional signals from text. The information conveyed by text and images isnot redundant\u2014models mainly driven by visual information yield qualitatively different from those mainly driven by word co-occurrences. However, our results suggest that current multimodal modeling approaches fail to effectively leverage visual information to build human-like word representations from human-scale data.",
        "author": "Chengxu Zhuang; Evelina Fedorenko; Jacob Andreas",
        "authorids": "/c/chengxu-zhuang/; /e/evelina-fedorenko/; /j/jacob-andreas/",
        "bibtex": "@inproceedings{zhuang-etal-2024-visual,\n    title = \"Visual Grounding Helps Learn Word Meanings in Low-Data Regimes\",\n    author = \"Zhuang, Chengxu  and\n      Fedorenko, Evelina  and\n      Andreas, Jacob\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.71/\",\n    doi = \"10.18653/v1/2024.naacl-long.71\",\n    pages = \"1311--1329\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.71.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.71/",
        "pdf_size": 3655685,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=184277621704044508&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "McGovern Institute for Brain Research, MIT + Department of Brain and Cognitive Sciences, MIT + The Program in Speech and Hearing Bioscience and Technology, Harvard University; McGovern Institute for Brain Research, MIT + Department of Brain and Cognitive Sciences, MIT + The Program in Speech and Hearing Bioscience and Technology, Harvard University; Computer Science and Artificial Intelligence Laboratory, MIT",
        "aff_domain": "mit.edu;mit.edu;mit.edu",
        "email": "mit.edu;mit.edu;mit.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0+0+1;0+0+1;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Harvard University",
        "aff_unique_dep": "McGovern Institute for Brain Research;The Program in Speech and Hearing Bioscience and Technology",
        "aff_unique_url": "https://www.mit.edu;https://www.harvard.edu",
        "aff_unique_abbr": "MIT;Harvard",
        "aff_campus_unique_index": "0+0+0;0+0+0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0+0+0;0+0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-industry.9",
        "title": "Visual Grounding for User Interfaces",
        "track": "main",
        "status": "Industry",
        "award": false,
        "abstract": "Enabling autonomous language agents to drive application user interfaces (UIs) as humans do can significantly expand the capability of today\u2019s API-based agents. Essential to this vision is the ability of agents to ground natural language commands to on-screen UI elements. Prior UI grounding approaches work by relaying on developer-provided UI metadata (UI trees, such as web DOM, and accessibility labels) to detect on-screen elements. However, such metadata is often unavailable or incomplete. Object detection techniques applied to UI screens remove this dependency, by inferring location and types of UI elements directly from the UI\u2019s visual appearance. The extracted semantics, however, are too limited to directly enable grounding. We overcome the limitations of both approaches by introducing the task of visual UI grounding, which unifies detection and grounding. A model takes as input a UI screenshot and a free-form language expression, and must identify the referenced UI element. We propose a solution to this problem, LVG, which learns UI element detection and grounding using a new technique called layout-guided contrastive learning, where the semantics of individual UI objects are learned also from their visual organization. Due to the scarcity of UI datasets, LVG integrates synthetic data in its training using multi-context learning. LVG outperforms baselines pre-trained on much larger datasets by over 4.9 points in top-1 accuracy, thus demonstrating its effectiveness.",
        "author": "Yijun Qian; Yujie Lu; Alexander Hauptmann; Oriana Riva",
        "authorids": "/y/yijun-qian/; /y/yujie-lu/; /a/alexander-g-hauptmann/; /o/oriana-riva/",
        "bibtex": "@inproceedings{qian-etal-2024-visual,\n    title = \"Visual Grounding for User Interfaces\",\n    author = \"Qian, Yijun  and\n      Lu, Yujie  and\n      Hauptmann, Alexander  and\n      Riva, Oriana\",\n    editor = \"Yang, Yi  and\n      Davani, Aida  and\n      Sil, Avi  and\n      Kumar, Anoop\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-industry.9/\",\n    doi = \"10.18653/v1/2024.naacl-industry.9\",\n    pages = \"97--107\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-industry.9.pdf",
        "site": "https://aclanthology.org/2024.naacl-industry.9/",
        "pdf_size": 8215504,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1677215473398793363&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4
    },
    {
        "id": "2024.naacl-long.264",
        "title": "Visually Guided Generative Text-Layout Pre-training for Document Intelligence",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Prior study shows that pre-training techniques can boost the performance of visual document understanding (VDU), which typically requires models to gain abilities to perceive and reason both document texts and layouts (e.g., locations of texts and table-cells). To this end, we propose visually guided generative text-layout pre-training, named ViTLP. Given a document image, the model optimizes hierarchical language and layout modeling objectives to generate the interleaved text and layout sequence. In addition, to address the limitation of processing long documents by Transformers, we introduce a straightforward yet effective multi-segment generative pre-training scheme, facilitating ViTLP to process word-intensive documents of any length. ViTLP can function as a native OCR model to localize and recognize texts of document images. Besides, ViTLP can be effectively applied to various downstream VDU tasks. Extensive experiments show that ViTLP achieves competitive performance over existing baselines on benchmark VDU tasks, including information extraction, document classification, and document question answering.",
        "author": "Zhiming Mao; Haoli Bai; Lu Hou; Lifeng Shang; Xin Jiang; Qun Liu; Kam-Fai Wong",
        "authorids": "/z/zhiming-mao/; /h/haoli-bai/; /l/lu-hou/; /l/lifeng-shang/; /x/xin-jiang/; /q/qun-liu/; /k/kam-fai-wong/",
        "bibtex": "@inproceedings{mao-etal-2024-visually,\n    title = \"Visually Guided Generative Text-Layout Pre-training for Document Intelligence\",\n    author = \"Mao, Zhiming  and\n      Bai, Haoli  and\n      Hou, Lu  and\n      Shang, Lifeng  and\n      Jiang, Xin  and\n      Liu, Qun  and\n      Wong, Kam-Fai\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.264/\",\n    doi = \"10.18653/v1/2024.naacl-long.264\",\n    pages = \"4713--4730\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.264.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.264/",
        "pdf_size": 17385140,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=762745500598599526&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "The Chinese University of Hong Kong, Hong Kong, China + MoE Key Laboratory of High Confidence Software Technologies, China; Noah\u2019s Ark Lab, Huawei Technologies; Noah\u2019s Ark Lab, Huawei Technologies; Noah\u2019s Ark Lab, Huawei Technologies; Noah\u2019s Ark Lab, Huawei Technologies; Noah\u2019s Ark Lab, Huawei Technologies; The Chinese University of Hong Kong, Hong Kong, China + MoE Key Laboratory of High Confidence Software Technologies, China",
        "aff_domain": "se.cuhk.edu.hk;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;se.cuhk.edu.hk",
        "email": "se.cuhk.edu.hk;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com;se.cuhk.edu.hk",
        "github": "https://github.com/Veason-silverbullet/ViTLP",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;2;2;2;2;2;0+1",
        "aff_unique_norm": "Chinese University of Hong Kong;MoE Key Laboratory of High Confidence Software Technologies;Huawei",
        "aff_unique_dep": ";High Confidence Software Technologies;Noah\u2019s Ark Lab",
        "aff_unique_url": "https://www.cuhk.edu.hk;;https://www.huawei.com",
        "aff_unique_abbr": "CUHK;;Huawei",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong;",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.162",
        "title": "Visually-Aware Context Modeling for News Image Captioning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "News Image Captioning aims to create captions from news articles and images, emphasizing the connection between textual context and visual elements. Recognizing the significance of human faces in news images and the face-name co-occurrence pattern in existing datasets, we propose a face-naming module for learning better name embeddings. Apart from names, which can be directly linked to an image area (faces), news image captions mostly contain context information that can only be found in the article. We design a retrieval strategy using CLIP to retrieve sentences that are semantically close to the image, mimicking human thought process of linking articles to images. Furthermore, to tackle the problem of the imbalanced proportion of article context and image context in captions, we introduce a simple yet effective method Contrasting with Language Model backbone (CoLaM) to the training pipeline. We conduct extensive experiments to demonstrate the efficacy of our framework. We out-perform the previous state-of-the-art (without external data) by 7.97/5.80 CIDEr scores on GoodNews/NYTimes800k. Our code is available at https://github.com/tingyu215/VACNIC.",
        "author": "Tingyu Qu; Tinne Tuytelaars; Marie-Francine Moens",
        "authorids": "/t/tingyu-qu/; /t/tinne-tuytelaars/; /m/marie-francine-moens/",
        "bibtex": "@inproceedings{qu-etal-2024-visually,\n    title = \"Visually-Aware Context Modeling for News Image Captioning\",\n    author = \"Qu, Tingyu  and\n      Tuytelaars, Tinne  and\n      Moens, Marie-Francine\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.162/\",\n    doi = \"10.18653/v1/2024.naacl-long.162\",\n    pages = \"2927--2943\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.162.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.162/",
        "pdf_size": 1100583,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3860729314038080252&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, KU Leuven; Department of Electrical Engineering, KU Leuven; Department of Computer Science, KU Leuven",
        "aff_domain": "kuleuven.be;kuleuven.be;kuleuven.be",
        "email": "kuleuven.be;kuleuven.be;kuleuven.be",
        "github": "https://github.com/tingyu215/VACNIC",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "KU Leuven",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.kuleuven.be",
        "aff_unique_abbr": "KU Leuven",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "2024.naacl-long.23",
        "title": "Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Large multimodal models suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination is due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through qualitative analysis, we show that Volcano\u2019s feedback is properly grounded on the image than the initial response. This indicates that Volcano can provide itself with richer visual information through feedback generation, leading to self-correct hallucinations. We publicly release our model, data, and code at https://github.com/kaistAI/Volcanogithub.com/kaistAI/Volcano",
        "author": "Seongyun Lee; Sue Hyun Park; Yongrae Jo; Minjoon Seo",
        "authorids": "/s/seongyun-lee/; /s/sue-hyun-park/; /y/yongrae-jo/; /m/minjoon-seo/",
        "bibtex": "@inproceedings{lee-etal-2024-volcano,\n    title = \"Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision\",\n    author = \"Lee, Seongyun  and\n      Park, Sue Hyun  and\n      Jo, Yongrae  and\n      Seo, Minjoon\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.23/\",\n    doi = \"10.18653/v1/2024.naacl-long.23\",\n    pages = \"391--404\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.23.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.23/",
        "pdf_size": 4228524,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5916795981421379713&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "KAIST AI; KAIST AI; LG AI Research; KAIST AI",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;lgresearch.ai;kaist.ac.kr",
        "email": "kaist.ac.kr;kaist.ac.kr;lgresearch.ai;kaist.ac.kr",
        "github": "github.com/kaistAI/Volcano",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;LG",
        "aff_unique_dep": "KAIST AI;LG AI Research",
        "aff_unique_url": "https://www.kaist.edu;https://www.lgaires.com",
        "aff_unique_abbr": "KAIST;LG AI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2024.findings-naacl.223",
        "title": "WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Watermarking generative-AI systems, such as LLMs, has gained considerable interest, driven by their enhanced capabilities across a wide range of tasks. Although current approaches have demonstrated that small, context-dependent shifts in the word distributions can be used to apply and detect watermarks, there has been little work in analyzing the impact that these perturbations have on the quality of generated texts. Balancing high detectability with minimal performance degradation is crucial in terms of selecting the appropriate watermarking setting; therefore this paper proposes a simple analysis framework where comparative assessment, a flexible NLG evaluation framework, is used to assess the quality degradation caused by a particular watermark setting. We demonstrate that our framework provides easy visualization of the quality-detection trade-off of watermark settings, enabling a simple solution to find an LLM watermark operating point that provides a well-balanced performance. This approach is applied to two different summarization systems and a translation system, enabling cross-model analysis for a task, and cross-task analysis.",
        "author": "Piotr Molenda; Adian Liusie; Mark Gales",
        "authorids": "/p/piotr-molenda/; /a/adian-liusie/; /m/mark-gales/",
        "bibtex": "@inproceedings{molenda-etal-2024-waterjudge,\n    title = \"{W}ater{J}udge: Quality-Detection Trade-off when Watermarking Large Language Models\",\n    author = \"Molenda, Piotr  and\n      Liusie, Adian  and\n      Gales, Mark\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.223/\",\n    doi = \"10.18653/v1/2024.findings-naacl.223\",\n    pages = \"3515--3525\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.223.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.223/",
        "pdf_size": 1362166,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3512214639345053299&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "ALTA Institute, Department of Engineering, University of Cambridge; ALTA Institute, Department of Engineering, University of Cambridge; ALTA Institute, Department of Engineering, University of Cambridge",
        "aff_domain": "cam.ac.uk;cam.ac.uk;eng.cam.ac.uk",
        "email": "cam.ac.uk;cam.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2024.findings-naacl.234",
        "title": "WebWISE: Unlocking Web Interface Control for LLMs via Sequential Exploration",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper investigates using Large Language Models (LLMs) to automatically perform web software tasks using click, scroll, and text in- put operations. Previous approaches, such as reinforcement learning (RL) or imitation learning, are inefficient to train and task-specific. Our method uses filtered Document Object Model (DOM) elements as observations and performs tasks step-by-step, sequentially generating small programs based on the current observations. We use in-context learning, either benefiting from a single manually provided example, or an automatically generated example based on a successful zero-shot trial. We evaluate our proposed method on the MiniWob++ benchmark. With only one in-context example, our WebWISE method using gpt-3.5-turbo achieves similar or better performance than other methods that require many demonstrations or trials.",
        "author": "Heyi Tao; Sethuraman T V; Michal Shlapentokh-Rothman; Tanmay Gupta; Heng Ji; Derek Hoiem",
        "authorids": "/h/heyi-tao/; /s/sethuraman-t-v/; /m/michal-shlapentokh-rothman/; /t/tanmay-gupta/; /h/heng-ji/; /d/derek-hoiem/",
        "bibtex": "@inproceedings{tao-etal-2024-webwise,\n    title = \"{W}eb{WISE}: Unlocking Web Interface Control for {LLM}s via Sequential Exploration\",\n    author = \"Tao, Heyi  and\n      T V, Sethuraman  and\n      Shlapentokh-Rothman, Michal  and\n      Gupta, Tanmay  and\n      Ji, Heng  and\n      Hoiem, Derek\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.234/\",\n    doi = \"10.18653/v1/2024.findings-naacl.234\",\n    pages = \"3693--3711\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.234.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.234/",
        "pdf_size": 1471179,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7353976687645336571&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "University of Illinois at Urbana Champaign; University of Illinois at Urbana Champaign; University of Illinois at Urbana Champaign; PRIOR @Allen Institute for AI; University of Illinois at Urbana Champaign; University of Illinois at Urbana Champaign",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu;allenai.org;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu;allenai.org;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;1;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Allen Institute for AI",
        "aff_unique_dep": ";PRIOR",
        "aff_unique_url": "https://illinois.edu;https://allenai.org",
        "aff_unique_abbr": "UIUC;AI2",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.2",
        "title": "Weight-Inherited Distillation for Task-Agnostic BERT Compression",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Knowledge Distillation (KD) is a predominant approach for BERT compression.Previous KD-based methods focus on designing extra alignment losses for the student model to mimic the behavior of the teacher model.These methods transfer the knowledge in an indirect way.In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly transfers knowledge from the teacher.WID does not require any additional alignment loss and trains a compact student by inheriting the weights, showing a new perspective of knowledge distillation.Specifically, we design the row compactors and column compactors as mappings and then compress the weights via structural re-parameterization.Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines.Further analysis indicates that WID can also learn the attention patterns from the teacher model without any alignment loss on attention distributions.The code is available at https://github.com/wutaiqiang/WID-NAACL2024.",
        "author": "Taiqiang Wu; Cheng Hou; Shanshan Lao; Jiayi Li; Ngai Wong; Zhe Zhao; Yujiu Yang",
        "authorids": "/t/taiqiang-wu/; /c/cheng-hou/; /s/shanshan-lao/; /j/jiayi-li/; /n/ngai-wong/; /z/zhe-zhao/; /y/yujiu-yang/",
        "bibtex": "@inproceedings{wu-etal-2024-weight,\n    title = \"Weight-Inherited Distillation for Task-Agnostic {BERT} Compression\",\n    author = \"Wu, Taiqiang  and\n      Hou, Cheng  and\n      Lao, Shanshan  and\n      Li, Jiayi  and\n      Wong, Ngai  and\n      Zhao, Zhe  and\n      Yang, Yujiu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.2/\",\n    doi = \"10.18653/v1/2024.findings-naacl.2\",\n    pages = \"13--28\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.2.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.2/",
        "pdf_size": 2819512,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3971868077357950036&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "The University of Hong Kong + Tencent AI Lab; Tencent AI Lab; Shenzhen International Graduate School, Tsinghua University; Shenzhen International Graduate School, Tsinghua University; The University of Hong Kong; Tencent AI Lab; Shenzhen International Graduate School, Tsinghua University",
        "aff_domain": "connect.hku.hk; ; ; ; ; ; ",
        "email": "connect.hku.hk; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;1;2;2;0;1;2",
        "aff_unique_norm": "University of Hong Kong;Tencent;Tsinghua University",
        "aff_unique_dep": ";Tencent AI Lab;Shenzhen International Graduate School",
        "aff_unique_url": "https://www.hku.hk;https://ai.tencent.com;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "HKU;Tencent AI Lab;THU",
        "aff_campus_unique_index": "0;2;2;0;2",
        "aff_campus_unique": "Hong Kong SAR;;Shenzhen",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.188",
        "title": "What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Vision-language (VL) models, pretrained on colossal image-text datasets, have attained broad VL competence that is difficult to evaluate. A common belief is that a small number of VL skills underlie the variety of VL tests. In this paper, we perform a large-scale transfer learning experiment aimed at discovering latent VL skills from data. We reveal interesting characteristics that have important implications for test suite design. First, generation tasks suffer from a length bias, suggesting benchmarks should balance tasks with varying output lengths. Second, we demonstrate that factor analysis successfully identifies reasonable yet surprising VL skill factors, suggesting benchmarks could leverage similar analyses for task selection.Finally, we present a new dataset, OLIVE1, which simulates user instructions in the wild and presents challenges dissimilar to all datasets we tested. Our findings contribute to the design of balanced and broad-coverage vision-language evaluation methods. 1https://github.com/jq-zh/olive-dataset",
        "author": "Anthony Tiong; Junqi Zhao; Boyang Li; Junnan Li; Steven Hoi; Caiming Xiong",
        "authorids": "/a/anthony-tiong/; /j/junqi-zhao/; /b/boyang-li/; /j/junnan-li/; /s/steven-hoi/; /c/caiming-xiong/",
        "bibtex": "@inproceedings{tiong-etal-2024-measuring,\n    title = \"What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases\",\n    author = \"Tiong, Anthony  and\n      Zhao, Junqi  and\n      Li, Boyang  and\n      Li, Junnan  and\n      Hoi, Steven  and\n      Xiong, Caiming\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.188/\",\n    doi = \"10.18653/v1/2024.naacl-long.188\",\n    pages = \"3427--3454\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.188.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.188/",
        "pdf_size": 928115,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2798182338699487078&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Nanyang Technological University; Nanyang Technological University; Nanyang Technological University+Singapore Management University; ; Singapore Management University; Salesforce Research",
        "aff_domain": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg;gmail.com;gmail.com;salesforce.com",
        "email": "ntu.edu.sg;ntu.edu.sg;ntu.edu.sg;gmail.com;gmail.com;salesforce.com",
        "github": "https://github.com/jq-zh/olive-dataset",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0+1;1;2",
        "aff_unique_norm": "Nanyang Technological University;Singapore Management University;Salesforce",
        "aff_unique_dep": ";;Salesforce Research",
        "aff_unique_url": "https://www.ntu.edu.sg;https://www.smu.edu.sg;https://research.salesforce.com",
        "aff_unique_abbr": "NTU;SMU;Salesforce",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0;1",
        "aff_country_unique": "Singapore;United States"
    },
    {
        "id": "2024.naacl-long.150",
        "title": "What Causes the Failure of Explicit to Implicit Discourse Relation Recognition?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We consider an unanswered question in the discourse processing community: why do relation classifiers trained on explicit examples (with connectives removed) perform poorly in real implicit scenarios? Prior work claimed this is due to linguistic dissimilarity between explicit and implicit examples but provided no empirical evidence. In this study, we show that one cause for such failure is a label shift after connectives are eliminated. Specifically, we find that the discourse relations expressed by some explicit instances will change when connectives disappear. Unlike previous work manually analyzing a few examples, we present empirical evidence at the corpus level to prove the existence of such shift. Then, we analyze why label shift occurs by considering factors such as the syntactic role played by connectives, ambiguity of connectives, and more. Finally, we investigate two strategies to mitigate the label shift: filtering out noisy data and joint learning with connectives. Experiments on PDTB 2.0, PDTB 3.0, and the GUM dataset demonstrate that classifiers trained with our strategies outperform strong baselines.",
        "author": "Wei Liu; Stephen Wan; Michael Strube",
        "authorids": "/w/wei-liu/; /s/stephen-wan/; /m/michael-strube/",
        "bibtex": "@inproceedings{liu-etal-2024-causes,\n    title = \"What Causes the Failure of Explicit to Implicit Discourse Relation Recognition?\",\n    author = \"Liu, Wei  and\n      Wan, Stephen  and\n      Strube, Michael\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.150/\",\n    doi = \"10.18653/v1/2024.naacl-long.150\",\n    pages = \"2738--2753\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.150.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.150/",
        "pdf_size": 3700861,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=193181075706052526&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Heidelberg Institute for Theoretical Studies gGmbH; CSIRO Data61; Heidelberg Institute for Theoretical Studies gGmbH",
        "aff_domain": "h-its.org;csiro.au;h-its.org",
        "email": "h-its.org;csiro.au;h-its.org",
        "github": "",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Heidelberg Institute for Theoretical Studies;CSIRO",
        "aff_unique_dep": ";Data61",
        "aff_unique_url": "https://www.hits.org;https://www.csiro.au",
        "aff_unique_abbr": "HITS;CSIRO",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;Australia"
    },
    {
        "id": "2024.findings-naacl.72",
        "title": "What Makes Math Word Problems Challenging for LLMs?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "This paper investigates the question of what makes math word problems (MWPs) in English challenging for large language models (LLMs). We conduct an in-depth analysis of the key linguistic and mathematical characteristics of MWPs. In addition, we train feature-based classifiers to better understand the impact of each feature on the overall difficulty of MWPs for prominent LLMs and investigate whether this helps predict how well LLMs fare against specific categories of MWPs.",
        "author": "Kv Aditya Srivatsa; Ekaterina Kochmar",
        "authorids": "/k/kv-aditya-srivatsa/; /e/ekaterina-kochmar/",
        "bibtex": "@inproceedings{srivatsa-kochmar-2024-makes,\n    title = \"What Makes Math Word Problems Challenging for {LLM}s?\",\n    author = \"Srivatsa, Kv Aditya  and\n      Kochmar, Ekaterina\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.72/\",\n    doi = \"10.18653/v1/2024.findings-naacl.72\",\n    pages = \"1138--1148\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.72.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.72/",
        "pdf_size": 10927021,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6791985532592612530&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "MBZUAI, Abu Dhabi, UAE; MBZUAI, Abu Dhabi, UAE",
        "aff_domain": "mbzuai.ac.ae;mbzuai.ac.ae",
        "email": "mbzuai.ac.ae;mbzuai.ac.ae",
        "github": "github.com/kvadityasrivatsa/analyzing-llms-for-mwps",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.mbzuali.ac.ae",
        "aff_unique_abbr": "MBZUAI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Abu Dhabi",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Arab Emirates"
    },
    {
        "id": "2024.naacl-long.440",
        "title": "What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Recent advancements in GPT-4V have displayed remarkable multi-modal capabilities in processing image inputs and following open-ended instructions. Despite these advancements, there is considerable scope for enhancing open-source multi-modal LLMs, especially in terms of multi-modal understanding accuracy and instruction-following proficiency. In this paper, we conduct a comprehensive study on training GPT4-style models. We introduce Lynx a multi-modal LLM developed through a series of controlled experiments comparing various model variants. This process allowed us to identify and implement an optimal training strategy tailored for multi-modal LLMs. In addition to our model development, we propose a plug-and-play technique designed to augment the instruction-following capabilities of multi-modal LLMs. We have validated the performance of Lynx on multiple benchmarks. Results demonstrate that Lynx not only achieves strong image understanding accuracy but also excels in instruction-following tasks, paving the path for ongoing enhancements in multi-modal LLMs.",
        "author": "Yan Zeng; Hanbo Zhang; Jiani Zheng; Jiangnan Xia; Guoqiang Wei; Yang Wei; Yuchen Zhang; Tao Kong; Ruihua Song",
        "authorids": "/y/yan-zeng/; /h/hanbo-zhang/; /j/jiani-zheng/; /j/jiangnan-xia/; /g/guoqiang-wei/; /y/yang-wei/; /y/yuchen-zhang/; /t/tao-kong/; /r/ruihua-song/",
        "bibtex": "@inproceedings{zeng-etal-2024-matters,\n    title = \"What Matters in Training a {GPT}4-Style Language Model with Multimodal Inputs?\",\n    author = \"Zeng, Yan  and\n      Zhang, Hanbo  and\n      Zheng, Jiani  and\n      Xia, Jiangnan  and\n      Wei, Guoqiang  and\n      Wei, Yang  and\n      Zhang, Yuchen  and\n      Kong, Tao  and\n      Song, Ruihua\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.440/\",\n    doi = \"10.18653/v1/2024.naacl-long.440\",\n    pages = \"7937--7964\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.440.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.440/",
        "pdf_size": 32883139,
        "gs_citation": 75,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=993600076851162742&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Renmin University of China+ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; ByteDance Research; Renmin University of China",
        "aff_domain": "ruc.edu.cn;bytedance.com;bytedance.com; ; ; ; ; ;ruc.edu.cn",
        "email": "ruc.edu.cn;bytedance.com;bytedance.com; ; ; ; ; ;ruc.edu.cn",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0+1;1;1;1;1;1;1;1;0",
        "aff_unique_norm": "Renmin University of China;ByteDance",
        "aff_unique_dep": ";Research",
        "aff_unique_url": "http://www.ruc.edu.cn;https://www.bytedance.com",
        "aff_unique_abbr": "RUC;ByteDance",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.168",
        "title": "What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Eliciting feedback from end users of NLP models can be beneficial for improving models. However, how should we present model responses to users so they are most amenable to be corrected from user feedback? Further, what properties do users value to understand and trust responses? We answer these questions by analyzing the effect of rationales (or explanations) generated by QA models to support their answers. We specifically consider decomposed QA models that first extract an intermediate rationale based on a context and a question and then use solely this rationale to answer the question. A rationale outlines the approach followed by the model to answer the question. Our work considers various formats of these rationales that vary according to well-defined properties of interest. We sample rationales from language models using few-shot prompting for two datasets, and then perform two user studies. First, we present users with incorrect answers and corresponding rationales in various formats and ask them to provide natural language feedback to revise the rationale. We then measure the effectiveness of this feedback in patching these rationales through in-context learning. The second study evaluates how well different rationale formats enable users to understand and trust model answers, when they are correct. We find that rationale formats significantly affect how easy it is (1) for users to give feedback for rationales, and (2) for models to subsequently execute this feedback. In addition, formats with attributions to the context and in-depth reasoning significantly enhance user-reported understanding and trust of model outputs.",
        "author": "Chaitanya Malaviya; Subin Lee; Dan Roth; Mark Yatskar",
        "authorids": "/c/chaitanya-malaviya/; /s/subin-lee/; /d/dan-roth/; /m/mark-yatskar/",
        "bibtex": "@inproceedings{malaviya-etal-2024-said,\n    title = \"What if you said that differently?: How Explanation Formats Affect Human Feedback Efficacy and User Perception\",\n    author = \"Malaviya, Chaitanya  and\n      Lee, Subin  and\n      Roth, Dan  and\n      Yatskar, Mark\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.168/\",\n    doi = \"10.18653/v1/2024.naacl-long.168\",\n    pages = \"3046--3065\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.168.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.168/",
        "pdf_size": 4530341,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7167600007470578479&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; University of Pennsylvania",
        "aff_domain": "upenn.edu;upenn.edu;upenn.edu;upenn.edu",
        "email": "upenn.edu;upenn.edu;upenn.edu;upenn.edu",
        "github": "https://github.com/chaitanyamalaviya/rationale_formats",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.349",
        "title": "When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Multilingual machine translation (MMT), trained on a mixture of parallel and monolingual data, is key for improving translation in low-resource language pairs. However, the literature offers conflicting results on the performance of different methods of including monolingual data. To resolve this, we examine how denoising autoencoding (DAE) and backtranslation (BT) impact MMT under different data conditions and model scales. Unlike prior studies, we use a realistic dataset of 100 translation directions and consider many domain combinations of monolingual and test data. We find that monolingual data generally helps MMT, but models are surprisingly brittle to domain mismatches, especially at smaller model scales. BT is beneficial when the parallel, monolingual, and test data sources are similar but can be detrimental otherwise, while DAE is less effective than previously reported. Next, we analyze the impact of scale (from 90M to 1.6B parameters) and find it is important for both methods, particularly DAE. As scale increases, DAE transitions from underperforming the parallel-only baseline at 90M to converging with BT performance at 1.6B, and even surpassing it in low-resource. These results offer new insights into how to best use monolingual data in MMT.",
        "author": "Christos Baziotis; Biao Zhang; Alexandra Birch; Barry Haddow",
        "authorids": "/c/christos-baziotis/; /b/biao-zhang/; /a/alexandra-birch/; /b/barry-haddow/",
        "bibtex": "@inproceedings{baziotis-etal-2024-monolingual,\n    title = \"When Does Monolingual Data Help Multilingual Translation: The Role of Domain and Model Scale\",\n    author = \"Baziotis, Christos  and\n      Zhang, Biao  and\n      Birch, Alexandra  and\n      Haddow, Barry\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.349/\",\n    doi = \"10.18653/v1/2024.naacl-long.349\",\n    pages = \"6297--6324\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.349.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.349/",
        "pdf_size": 640454,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12014336243956714798&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Samaya AI; Google DeepMind; University of Edinburgh; University of Edinburgh",
        "aff_domain": "samaya.ai;google.com;ed.ac.uk;ed.ac.uk",
        "email": "samaya.ai;google.com;ed.ac.uk;ed.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Samaya AI;Google;University of Edinburgh",
        "aff_unique_dep": ";Google DeepMind;",
        "aff_unique_url": ";https://deepmind.com;https://www.ed.ac.uk",
        "aff_unique_abbr": ";DeepMind;Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United Kingdom"
    },
    {
        "id": "2024.findings-naacl.237",
        "title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs\u2019 ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA.We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models\u2019 initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.",
        "author": "Yanhong Li; Chenghao Yang; Allyson Ettinger",
        "authorids": "/y/yanhong-li/; /c/chenghao-yang/; /a/allyson-ettinger/",
        "bibtex": "@inproceedings{li-etal-2024-hindsight,\n    title = \"When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models\",\n    author = \"Li, Yanhong  and\n      Yang, Chenghao  and\n      Ettinger, Allyson\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.237/\",\n    doi = \"10.18653/v1/2024.findings-naacl.237\",\n    pages = \"3741--3753\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.237.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.237/",
        "pdf_size": 733071,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17592123484490401693&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of Chicago; University of Chicago; Allen Institute for AI",
        "aff_domain": "uchicago.edu;uchicago.edu;allenai.org",
        "email": "uchicago.edu;uchicago.edu;allenai.org",
        "github": "https://github.com/yanhong-lbh/LLM-SelfReflection-Eval",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "University of Chicago;Allen Institute for AI",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uchicago.edu;https://allenai.org",
        "aff_unique_abbr": "UChicago;AI2",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.169",
        "title": "When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Deployed dialogue agents have the potential to integrate human feedback to continuously improve themselves. However, humans may not always provide explicit signals when the chatbot makes mistakes during interactions. In this work, we propose Juicer, a framework to make use of both binary and free-form textual human feedback. It works by: (i) extending sparse binary feedback by training a satisfaction classifier to label the unlabeled data; and (ii) training a reply corrector to map the bad replies to good ones. We find that augmenting training with model-corrected replies improves the final dialogue model, and we can further improve performance by using both positive and negative replies through the recently proposed Director model.",
        "author": "Weiyan Shi; Emily Dinan; Kurt Shuster; Jason Weston; Jing Xu",
        "authorids": "/w/weiyan-shi/; /e/emily-dinan/; /k/kurt-shuster/; /j/jason-weston/; /j/jing-xu/",
        "bibtex": "@inproceedings{shi-etal-2024-life,\n    title = \"When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels\",\n    author = \"Shi, Weiyan  and\n      Dinan, Emily  and\n      Shuster, Kurt  and\n      Weston, Jason  and\n      Xu, Jing\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.169/\",\n    doi = \"10.18653/v1/2024.naacl-long.169\",\n    pages = \"3066--3082\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.169.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.169/",
        "pdf_size": 711262,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15038534320635983779&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Stanford University\u2020; Meta AI; Character.ai3; Meta AI*; Meta AI*",
        "aff_domain": "\u2020; ; 3; *; *",
        "email": "\u2020; ; 3; *; *",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;1;1",
        "aff_unique_norm": "Stanford University;Meta;Character.AI",
        "aff_unique_dep": ";Meta AI;",
        "aff_unique_url": "https://www.stanford.edu;https://meta.com;https://www.character.ai",
        "aff_unique_abbr": "Stanford;Meta;Character.ai",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.124",
        "title": "When Quantization Affects Confidence of Large Language Models?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Recent studies introduced effective compression techniques for Large Language Models (LLMs) via post-training quantization or low-bit weight representation. Although quantized weights offer storage efficiency and allow for faster inference, existing works have indicated that quantization might compromise performance and exacerbate biases in LLMs.This study investigates the confidence and calibration of quantized models, considering factors such as language model type and scale as contributors to quantization loss.Firstly, we reveal that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models. Secondly, we observe fluctuations in the impact on confidence across different scales. Finally, we propose an explanation for quantization loss based on confidence levels, indicating that quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place.We make our code and quantized models publicly available.",
        "author": "Irina Proskurina; Luc Brun; Guillaume Metzler; Julien Velcin",
        "authorids": "/i/irina-proskurina/; /l/luc-brun/; /g/guillaume-metzler/; /j/julien-velcin/",
        "bibtex": "@inproceedings{proskurina-etal-2024-quantization,\n    title = \"When Quantization Affects Confidence of Large Language Models?\",\n    author = \"Proskurina, Irina  and\n      Brun, Luc  and\n      Metzler, Guillaume  and\n      Velcin, Julien\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.124/\",\n    doi = \"10.18653/v1/2024.findings-naacl.124\",\n    pages = \"1918--1928\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.124.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.124/",
        "pdf_size": 468376,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14532468047744879575&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Universit\u00e9 de Lyon, Lyon 2, ERIC UR 3083, France; Universit\u00e9 de Lyon, Lyon 2, ERIC UR 3083, France; Universit\u00e9 de Lyon, Lyon 2, ERIC UR 3083, France; Universit\u00e9 de Lyon, Lyon 2, ERIC UR 3083, France",
        "aff_domain": "univ-lyon2.fr; ; ; ",
        "email": "univ-lyon2.fr; ; ; ",
        "github": "https://github.com/upunaprosk/quantized-lm-confidence",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Universit\u00e9 de Lyon",
        "aff_unique_dep": "ERIC UR 3083",
        "aff_unique_url": "https://www.universitedelyon.fr",
        "aff_unique_abbr": "UDL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Lyon",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "2024.naacl-long.286",
        "title": "Where are you from? Geolocating Speech and Applications to Language Identification",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We train models to answer the question, Where are you from? and show how such models can be repurposed for language identification (LID). To our knowledge, this paper is the first to introduce data sources, methods and models to tackle the task of geolocation of speech at a global scale, and the first to explore using geolocation as a proxy-task for LID. Specifically, we explore whether radio broadcasts with known origin can be used to train regression and classification-based models for geolocating speech. We build models on top of self-supervised pretrained models, using attention pooling to qualitatively verify that the model geolocates the speech itself, and not other channel artifacts.The best geolocation models localize speaker origin to around 650km. We confirm the value of speech geolocation as a proxy task by using speech geolocation models for zero-shot LID. Finally, we show that fine-tuning geolocation models for LID outperforms fine-tuning pretrained Wav2Vec2.0 models, and achieves state-of-the-art performance on the FLEURS benchmark.",
        "author": "Patrick Foley; Matthew Wiesner; Bismarck Odoom; Leibny Paola Garcia Perera; Kenton Murray; Philipp Koehn",
        "authorids": "/p/patrick-foley/; /m/matthew-wiesner/; /b/bismarck-odoom/; /l/leibny-paola-garcia-perera/; /k/kenton-murray/; /p/philipp-koehn/",
        "bibtex": "@inproceedings{foley-etal-2024-geolocating,\n    title = \"Where are you from? Geolocating Speech and Applications to Language Identification\",\n    author = \"Foley, Patrick  and\n      Wiesner, Matthew  and\n      Odoom, Bismarck  and\n      Garcia Perera, Leibny Paola  and\n      Murray, Kenton  and\n      Koehn, Philipp\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.286/\",\n    doi = \"10.18653/v1/2024.naacl-long.286\",\n    pages = \"5114--5126\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.286.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.286/",
        "pdf_size": 2378147,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2145539588449609473&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University; Johns Hopkins University",
        "aff_domain": "gmail.com;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "email": "gmail.com;jhu.edu;jhu.edu;jhu.edu;jhu.edu;jhu.edu",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Johns Hopkins University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.jhu.edu",
        "aff_unique_abbr": "JHU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.34",
        "title": "Which Modality should I use - Text, Motif, or Image? : Understanding Graphs with Large Language Models",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Our research integrates graph data with Large Language Models (LLMs), which, despite their advancements in various fields using large text corpora, face limitations in encoding entire graphs due to context size constraints. This paper introduces a new approach to encoding a graph with diverse modalities, such as text, image, and motif, coupled with prompts to approximate a graph\u2019s global connectivity, thereby enhancing LLMs\u2019 efficiency in processing complex graph structures. The study also presents GraphTMI, a novel benchmark for evaluating LLMs in graph structure analysis, focusing on homophily, motif presence, and graph difficulty. Key findings indicate that the image modality, especially with vision-language models like GPT-4V, is superior to text in balancing token limits and preserving essential information and comes close to prior graph neural net (GNN) encoders. Furthermore, the research assesses how various factors affect the performance of each encoding modality and outlines the existing challenges and potential future developments for LLMs in graph understanding and reasoning tasks. Our code and data are publicly available on our project page - https://minnesotanlp.github.io/GraphLLM/",
        "author": "Debarati Das; Ishaan Gupta; Jaideep Srivastava; Dongyeop Kang",
        "authorids": "/d/debarati-das/; /i/ishaan-gupta/; /j/jaideep-srivastava/; /d/dongyeop-kang/",
        "bibtex": "@inproceedings{das-etal-2024-modality,\n    title = \"Which Modality should {I} use - Text, Motif, or Image? : Understanding Graphs with Large Language Models\",\n    author = \"Das, Debarati  and\n      Gupta, Ishaan  and\n      Srivastava, Jaideep  and\n      Kang, Dongyeop\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.34/\",\n    doi = \"10.18653/v1/2024.findings-naacl.34\",\n    pages = \"503--519\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.34.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.34/",
        "pdf_size": 3065054,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16105278438310776557&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science, University of Minnesota; Department of Computer Science, University of Minnesota; Department of Computer Science, University of Minnesota; Department of Computer Science, University of Minnesota",
        "aff_domain": "umn.edu;umn.edu;umn.edu;umn.edu",
        "email": "umn.edu;umn.edu;umn.edu;umn.edu",
        "github": "",
        "project": "https://minnesotanlp.github.io/GraphLLM",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Minnesota",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.umn.edu",
        "aff_unique_abbr": "UMN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.175",
        "title": "Which One? Leveraging Context Between Objects and Multiple Views for Language Grounding",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "When connecting objects and their language referents in an embodied 3D environment, it is important to note that: (1) an object can be better characterized by leveraging comparative information between itself and other objects, and (2) an object\u2019s appearance can vary with camera position. As such, we present the Multi-view Approach to Grounding in Context (MAGiC) model, which selects an object referent based on language that distinguishes between two similar objects. By pragmatically reasoning over both objects and across multiple views of those objects, MAGiC improves over the state-of-the-art model on the SNARE object reference task with a relative error reduction of 12.9% (representing an absolute improvement of 2.7%). Ablation studies show that reasoning jointly over object referent candidates and multiple views of each object both contribute to improved accuracy. Code: https://github.com/rcorona/magic_snare/",
        "author": "Chancharik Mitra; Abrar Anwar; Rodolfo Corona; Dan Klein; Trevor Darrell; Jesse Thomason",
        "authorids": "/c/chancharik-mitra/; /a/abrar-anwar/; /r/rodolfo-corona/; /d/dan-klein/; /t/trevor-darrell/; /j/jesse-thomason/",
        "bibtex": "@inproceedings{mitra-etal-2024-one,\n    title = \"Which One? Leveraging Context Between Objects and Multiple Views for Language Grounding\",\n    author = \"Mitra, Chancharik  and\n      Anwar, Abrar  and\n      Corona, Rodolfo  and\n      Klein, Dan  and\n      Darrell, Trevor  and\n      Thomason, Jesse\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.175/\",\n    doi = \"10.18653/v1/2024.naacl-long.175\",\n    pages = \"3177--3189\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.175.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.175/",
        "pdf_size": 979175,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2851683113334418695&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of California, Berkeley; University of Southern California; University of California, Berkeley; University of California, Berkeley; University of California, Berkeley; University of Southern California",
        "aff_domain": "; ; ; ; ; ",
        "email": "; ; ; ; ; ",
        "github": "https://github.com/rcorona/magic_snare/",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;0;0;0;1",
        "aff_unique_norm": "University of California, Berkeley;University of Southern California",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://www.usc.edu",
        "aff_unique_abbr": "UC Berkeley;USC",
        "aff_campus_unique_index": "0;1;0;0;0;1",
        "aff_campus_unique": "Berkeley;Los Angeles",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.310",
        "title": "Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "*Do larger and more performant models resolve NLP\u2019s longstanding robustness issues?* We investigate this question using over 20 models of different sizes spanning different architectural choices and pretraining objectives. We conduct evaluations using (a) out-of-domain and challenge test sets, (b) behavioral testing with CheckLists, (c) contrast sets, and (d) adversarial inputs. Our analysis reveals that not all out-of-domain tests provide insight into robustness. Evaluating with CheckLists and contrast sets shows significant gaps in model performance; merely scaling models does not make them adequately robust. Finally, we point out that current approaches for adversarial evaluations of models are themselves problematic: they can be easily thwarted, and in their current forms, do not represent a sufficiently deep probe of model robustness. We conclude that not only is the question of robustness in NLP as yet unresolved, but even some of the approaches to measure robustness need to be reassessed.",
        "author": "Ashim Gupta; Rishanth Rajendhran; Nathan Stringham; Vivek Srikumar; Ana Marasovic",
        "authorids": "/a/ashim-gupta/; /r/rishanth-rajendhran/; /n/nathan-stringham/; /v/vivek-srikumar/; /a/ana-marasovic/",
        "bibtex": "@inproceedings{gupta-etal-2024-whispers,\n    title = \"Whispers of Doubt Amidst Echoes of Triumph in {NLP} Robustness\",\n    author = \"Gupta, Ashim  and\n      Rajendhran, Rishanth  and\n      Stringham, Nathan  and\n      Srikumar, Vivek  and\n      Marasovic, Ana\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.310/\",\n    doi = \"10.18653/v1/2024.naacl-long.310\",\n    pages = \"5533--5590\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.310.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.310/",
        "pdf_size": 10820079,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16718470858663131591&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Kahlert School of Computing, University of Utah; Kahlert School of Computing, University of Utah; Kahlert School of Computing, University of Utah; Kahlert School of Computing, University of Utah; Kahlert School of Computing, University of Utah",
        "aff_domain": "cs.utah.edu; ; ; ; ",
        "email": "cs.utah.edu; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "University of Utah",
        "aff_unique_dep": "Kahlert School of Computing",
        "aff_unique_url": "https://www.utah.edu",
        "aff_unique_abbr": "U of U",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Salt Lake City",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.159",
        "title": "Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Most existing retrieval-augmented language models (LMs) assume a naive dichotomy within a retrieved document set: query-relevance and irrelevance. Our work investigates a more challenging scenario in which even the \u201crelevant\u201d documents may contain misleading or incorrect information, causing conflict among the retrieved documents and thereby negatively influencing model decisions as noise. We observe that existing LMs are highly brittle to the presence of conflicting information in both the fine-tuning and in-context few-shot learning scenarios. We propose approaches for handling knowledge conflicts among retrieved documents by explicitly fine-tuning a discriminator or prompting GPT-3.5 to elicit its discriminative capability. Our empirical results on open-domain QA show that these approaches significantly enhance model robustness. We also provide our findings on incorporating the fine-tuned discriminator\u2019s decision into the in-context learning process, proposing a way to exploit the benefits of two disparate learning schemes. Alongside our findings, we provide MacNoise, a machine-generated, conflict-induced dataset to further encourage research in this direction.",
        "author": "Giwon Hong; Jeonghwan Kim; Junmo Kang; Sung-Hyon Myaeng; Joyce Whang",
        "authorids": "/g/giwon-hong/; /j/jeonghwan-kim/; /j/junmo-kang/; /s/sung-hyon-myaeng/; /j/joyce-whang/",
        "bibtex": "@inproceedings{hong-etal-2024-gullible,\n    title = \"Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise\",\n    author = \"Hong, Giwon  and\n      Kim, Jeonghwan  and\n      Kang, Junmo  and\n      Myaeng, Sung-Hyon  and\n      Whang, Joyce\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.159/\",\n    doi = \"10.18653/v1/2024.findings-naacl.159\",\n    pages = \"2474--2495\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.159.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.159/",
        "pdf_size": 1964431,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6928648741414584832&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "University of Edinburgh; UIUC; Georgia Tech; KAIST; KAIST",
        "aff_domain": "ed.ac.uk;illinois.edu;gatech.edu;kaist.ac.kr;kaist.ac.kr",
        "email": "ed.ac.uk;illinois.edu;gatech.edu;kaist.ac.kr;kaist.ac.kr",
        "github": "https://github.com/wjdghks950/Discern-and-Answer",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;2;3;3",
        "aff_unique_norm": "University of Edinburgh;University of Illinois Urbana-Champaign;Georgia Institute of Technology;Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ed.ac.uk;https://www illinois.edu;https://www.gatech.edu;https://www.kaist.ac.kr",
        "aff_unique_abbr": "Edinburgh;UIUC;Georgia Tech;KAIST",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Urbana-Champaign",
        "aff_country_unique_index": "0;1;1;2;2",
        "aff_country_unique": "United Kingdom;United States;South Korea"
    },
    {
        "id": "2024.naacl-long.473",
        "title": "X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Natural Language Generation (NLG) typically involves evaluating the generated text in various aspects (e.g., consistency and naturalness) to obtain a comprehensive assessment. However, multi-aspect evaluation remains challenging as it may require the evaluator to generalize to any given evaluation aspect even if it\u2019s absent during training. In this paper, we introduce X-Eval, a two-stage instruction tuning framework to evaluate text in both seen and unseen aspects customized by end users. X-Eval consists of two learning stages: the vanilla instruction tuning stage that improves the model\u2019s ability to follow evaluation instructions, and an enhanced instruction tuning stage that exploits the connections between fine-grained evaluation aspects to better assess text quality. To support the training of X-Eval, we collect AspectInstruct, the first instruction tuning dataset tailored for multi-aspect NLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance task diversity, we devise an augmentation strategy that converts human rating annotations into diverse forms of NLG evaluation tasks, including scoring, comparison, ranking, and Boolean question answering. Extensive experiments across three essential categories of NLG tasks: dialogue generation, summarization, and data-to-text coupled with 21 aspects in meta-evaluation, demonstrate that X-Eval enables even a lightweight language model to achieve a comparable if not higher correlation with human judgments compared to the state-of-the-art NLG evaluators like GPT-4.",
        "author": "Minqian Liu; Ying Shen; Zhiyang Xu; Yixin Cao; Eunah Cho; Vaibhav Kumar; Reza Ghanadan; Lifu Huang",
        "authorids": "/m/minqian-liu/; /y/ying-shen/; /z/zhiyang-xu/; /y/yixin-cao/; /e/eunah-cho/; /v/vaibhav-kumar/; /r/reza-ghanadan/; /l/lifu-huang/",
        "bibtex": "@inproceedings{liu-etal-2024-x,\n    title = \"{X}-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects\",\n    author = \"Liu, Minqian  and\n      Shen, Ying  and\n      Xu, Zhiyang  and\n      Cao, Yixin  and\n      Cho, Eunah  and\n      Kumar, Vaibhav  and\n      Ghanadan, Reza  and\n      Huang, Lifu\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.473/\",\n    doi = \"10.18653/v1/2024.naacl-long.473\",\n    pages = \"8560--8579\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.473.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.473/",
        "pdf_size": 1273796,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1938092818453256702&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Virginia Tech\u2660; Amazon Inc.\u2661; Fudan University\u2663; Virginia Tech\u2660; Amazon Inc.\u2661; Amazon Inc.\u2661; Amazon Inc.\u2661; Virginia Tech\u2660",
        "aff_domain": "vt.edu;vt.edu;vt.edu;gmail.com;amazon.com;amazon.com;amazon.com;vt.edu",
        "email": "vt.edu;vt.edu;vt.edu;gmail.com;amazon.com;amazon.com;amazon.com;vt.edu",
        "github": "https://github.com/VT-NLP/XEval",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;2;0;1;1;1;0",
        "aff_unique_norm": "Virginia Tech;Amazon;Fudan University",
        "aff_unique_dep": ";Amazon;",
        "aff_unique_url": "https://www.vt.edu;https://www.amazon.com;https://www.fudan.edu.cn",
        "aff_unique_abbr": "VT;Amazon;Fudan",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.findings-naacl.158",
        "title": "X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.",
        "author": "DongJae Shin; HyeonSeok Lim; Inho Won; ChangSu Choi; Minjun Kim; SeungWoo Song; HanGyeol Yoo; SangMin Kim; KyungTae Lim",
        "authorids": "/d/dongjae-shin/; /h/hyeonseok-lim/; /i/inho-won/; /c/changsu-choi/; /m/minjun-kim/; /s/seungwoo-song/; /h/hangyeol-yoo/; /s/sangmin-kim/; /k/kyungtae-lim/",
        "bibtex": "@inproceedings{shin-etal-2024-x,\n    title = \"{X}-{LL}a{VA}: Optimizing Bilingual Large Vision-Language Alignment\",\n    author = \"Shin, DongJae  and\n      Lim, HyeonSeok  and\n      Won, Inho  and\n      Choi, ChangSu  and\n      Kim, Minjun  and\n      Song, SeungWoo  and\n      Yoo, HanGyeol  and\n      Kim, SangMin  and\n      Lim, KyungTae\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.158/\",\n    doi = \"10.18653/v1/2024.findings-naacl.158\",\n    pages = \"2463--2473\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.158.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.158/",
        "pdf_size": 1250202,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12437235138300257644&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Seoul National University of Science and Technology\u2021; Seoul National University of Science and Technology\u2217; Teddysum\u2021; Seoul National University of Science and Technology; Seoul National University of Science and Technology; Seoul National University of Science and Technology; Seoul National University of Science and Technology; Seoul National University of Science and Technology; Seoul National University of Science and Technology\u2020",
        "aff_domain": "seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr",
        "email": "seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr;seoultech.ac.kr",
        "github": "",
        "project": "",
        "author_num": 9,
        "aff_unique_index": "0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Seoul National University of Science and Technology;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.snust.ac.kr;",
        "aff_unique_abbr": "SNUST;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "id": "2024.naacl-long.66",
        "title": "X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Understanding when two pieces of text convey the same information is a goal touching many subproblems in NLP, including textual entailment and fact-checking. This problem becomes more complex when those two pieces of text are in different languages. Here, we introduce X-PARADE (Cross-lingual Paragraph-level Analysis of Divergences and Entailments), the first cross-lingual dataset of paragraph-level information divergences. Annotators label a paragraph in a target language at the span level and evaluate it with respect to a corresponding paragraph in a source language, indicating whether a given piece of information is the same, new, or new but can be inferred. This last notion establishes a link with cross-language NLI. Aligned paragraphs are sourced from Wikipedia pages in different languages, reflecting real information divergences observed in the wild. Armed with our dataset, we investigate a diverse set of approaches for this problem, including classic token alignment from machine translation, textual entailment methods that localize their decisions, and prompting LLMs. Our results show that these methods vary in their capability to handle inferable information, but they all fall short of human performance.",
        "author": "Juan Rodriguez; Katrin Erk; Greg Durrett",
        "authorids": "/j/juan-rodriguez/; /k/katrin-erk/; /g/greg-durrett/",
        "bibtex": "@inproceedings{rodriguez-etal-2024-x,\n    title = \"{X}-{PARADE}: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs\",\n    author = \"Rodriguez, Juan  and\n      Erk, Katrin  and\n      Durrett, Greg\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.66/\",\n    doi = \"10.18653/v1/2024.naacl-long.66\",\n    pages = \"1198--1222\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.66.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.66/",
        "pdf_size": 1202842,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14578963842818921267&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Computer Science; Department of Computer Science+Department of Linguistics; Department of Computer Science",
        "aff_domain": "utexas.edu; ; ",
        "email": "utexas.edu; ; ",
        "github": "https://github.com/juand-r/x-parade",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0+1;0",
        "aff_unique_norm": "Unknown Institution;University Affiliation Not Specified",
        "aff_unique_dep": "Department of Computer Science;Department of Linguistics",
        "aff_unique_url": ";",
        "aff_unique_abbr": ";",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "2024.naacl-long.372",
        "title": "XAL: EXplainable Active Learning Makes Classifiers Better Low-resource Learners",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Active learning (AL), which aims to construct an effective training set by iteratively curating the most formative unlabeled data for annotation, has been widely used in low-resource tasks. Most active learning techniques in classification rely on the model\u2019s uncertainty or disagreement to choose unlabeled data, suffering from the problem of over-confidence in superficial patterns and a lack of exploration.Inspired by the cognitive processes in which humans deduce and predict through causal information, we take an initial attempt towards integrating rationales into AL and propose a novel Explainable Active Learning framework (XAL) for low-resource text classification, which aims to encourage classifiers to justify their inferences and delve into unlabeled data for which they cannot provide reasonable explanations. Specifically, besides using a pre-trained bi-directional encoder for classification, we employ a pre-trained uni-directional decoder to generate and score the explanation. We further facilitate the alignment of the model with human reasoning preference through a proposed ranking loss. During the selection of unlabeled data, the predicted uncertainty of the encoder and the explanation score of the decoder complement each other as the final metric to acquire informative data. Extensive experiments on six datasets show that XAL achieves consistent improvement over 9 strong baselines. Analysis indicates that the proposed method can generate corresponding explanations for its predictions.",
        "author": "Yun Luo; Zhen Yang; Fandong Meng; Yingjie Li; Fang Guo; Qinglin Qi; Jie Zhou; Yue Zhang",
        "authorids": "/y/yun-luo/; /z/zhen-yang/; /f/fandong-meng/; /y/yingjie-li/; /f/fang-guo/; /q/qinglin-qi/; /j/jie-zhou/; /y/yue-zhang/",
        "bibtex": "@inproceedings{luo-etal-2024-xal,\n    title = \"{XAL}: {EX}plainable Active Learning Makes Classifiers Better Low-resource Learners\",\n    author = \"Luo, Yun  and\n      Yang, Zhen  and\n      Meng, Fandong  and\n      Li, Yingjie  and\n      Guo, Fang  and\n      Qi, Qinglin  and\n      Zhou, Jie  and\n      Zhang, Yue\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.372/\",\n    doi = \"10.18653/v1/2024.naacl-long.372\",\n    pages = \"6676--6698\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.372.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.372/",
        "pdf_size": 2955698,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2136100536096490760&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "School of Engineering, Westlake University+Zhejiang University+Westlake Institute for Advanced Study; WeChat AI, Tencent Inc.; WeChat AI, Tencent Inc.; School of Engineering, Westlake University+Zhejiang University+Westlake Institute for Advanced Study; School of Engineering, Westlake University+Zhejiang University+Westlake Institute for Advanced Study; School of Cyber Science and Engineering, Sichuan University; WeChat AI, Tencent Inc.; School of Engineering, Westlake University+Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "aff_domain": "westlake.edu.cn; ; ; ; ; ; ;westlake.edu.cn",
        "email": "westlake.edu.cn; ; ; ; ; ; ;westlake.edu.cn",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1+2;3;3;0+1+2;0+1+2;4;3;0+2",
        "aff_unique_norm": "Westlake University;Zhejiang University;Westlake Institute for Advanced Study;Tencent;Sichuan University",
        "aff_unique_dep": "School of Engineering;;;WeChat AI;School of Cyber Science and Engineering",
        "aff_unique_url": "https://www.westlake.edu.cn;https://www.zju.edu.cn;https://www.westlake.edu.cn;https://www.tencent.com;https://www.scu.edu.cn",
        "aff_unique_abbr": ";ZJU;WIAS;Tencent;",
        "aff_campus_unique_index": ";;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+0;0;0;0+0+0;0+0+0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-long.234",
        "title": "XNLIeu: a dataset for cross-lingual NLI in Basque",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches. The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step. We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation. The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch. Our code and datasets are publicly available under open licenses.",
        "author": "Maite Heredia; Julen Etxaniz; Muitze Zulaika; Xabier Saralegi; Jeremy Barnes; Aitor Soroa",
        "authorids": "/m/maite-heredia/; /j/julen-etxaniz/; /m/muitze-zulaika/; /x/xabier-saralegi/; /j/jeremy-barnes/; /a/aitor-soroa/",
        "bibtex": "@inproceedings{heredia-etal-2024-xnlieu,\n    title = \"{XNLI}eu: a dataset for cross-lingual {NLI} in {B}asque\",\n    author = \"Heredia, Maite  and\n      Etxaniz, Julen  and\n      Zulaika, Muitze  and\n      Saralegi, Xabier  and\n      Barnes, Jeremy  and\n      Soroa, Aitor\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.234/\",\n    doi = \"10.18653/v1/2024.naacl-long.234\",\n    pages = \"4177--4188\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.234.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.234/",
        "pdf_size": 261708,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17565630030692689857&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "HiTZ Center - Ixa, University of the Basque Country UPV/EHU; HiTZ Center - Ixa, University of the Basque Country UPV/EHU; Orai NLP Technologies; Orai NLP Technologies; HiTZ Center - Ixa, University of the Basque Country UPV/EHU; HiTZ Center - Ixa, University of the Basque Country UPV/EHU",
        "aff_domain": "ehu.eus; ; ; ; ; ",
        "email": "ehu.eus; ; ; ; ; ",
        "github": "https://github.com/hitz-zentroa/xnli-eu",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;0;1;1;0;0",
        "aff_unique_norm": "University of the Basque Country;Orai NLP Technologies",
        "aff_unique_dep": "HiTZ Center - Ixa;",
        "aff_unique_url": "https://www.ehu.eus/en;",
        "aff_unique_abbr": "UPV/EHU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Spain;"
    },
    {
        "id": "2024.naacl-long.301",
        "title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest\u2019s creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.",
        "author": "Paul R\u00f6ttger; Hannah Kirk; Bertie Vidgen; Giuseppe Attanasio; Federico Bianchi; Dirk Hovy",
        "authorids": "/p/paul-rottger/; /h/hannah-kirk/; /b/bertie-vidgen/; /g/giuseppe-attanasio/; /f/federico-bianchi/; /d/dirk-hovy/",
        "bibtex": "@inproceedings{rottger-etal-2024-xstest,\n    title = \"{XST}est: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models\",\n    author = {R{\\\"o}ttger, Paul  and\n      Kirk, Hannah  and\n      Vidgen, Bertie  and\n      Attanasio, Giuseppe  and\n      Bianchi, Federico  and\n      Hovy, Dirk},\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.301/\",\n    doi = \"10.18653/v1/2024.naacl-long.301\",\n    pages = \"5377--5400\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.301.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.301/",
        "pdf_size": 343568,
        "gs_citation": 197,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11104865788960494175&as_sdt=5,31&sciodt=0,31&hl=en",
        "gs_version_total": 6,
        "aff": "Bocconi University; University of Oxford; University of Oxford; Bocconi University; Stanford University; Bocconi University",
        "aff_domain": ";;;;;",
        "email": ";;;;;",
        "github": "",
        "project": "",
        "author_num": 6,
        "aff_unique_index": "0;1;1;0;2;0",
        "aff_unique_norm": "Bocconi University;University of Oxford;Stanford University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.bocconi.edu;https://www.ox.ac.uk;https://www.stanford.edu",
        "aff_unique_abbr": "Bocconi;Oxford;Stanford",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;1;1;0;2;0",
        "aff_country_unique": "Italy;United Kingdom;United States"
    },
    {
        "id": "2024.naacl-long.82",
        "title": "XferBench: a Data-Driven Benchmark for Emergent Language",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "In this paper, we introduce a benchmark for evaluating the overall quality of emergent languages using data-driven methods. Specifically, we interpret the notion of the \u201cquality\u201d of an emergent language as its similarity to human language within a deep learning framework. We measure this by using the emergent language as pretraining data for a downstream NLP tasks in human language\u2014the better the downstream performance, the better the emergent language. We implement this benchmark as an easy-to-use Python package that only requires a text file of utterances from the emergent language to be evaluated. Finally, we empirically test the benchmark\u2019s validity using human, synthetic, and emergent language baselines.",
        "author": "Brendon Boldt; David Mortensen",
        "authorids": "/b/brendon-boldt/; /d/david-r-mortensen/",
        "bibtex": "@inproceedings{boldt-mortensen-2024-xferbench,\n    title = \"{X}fer{B}ench: a Data-Driven Benchmark for Emergent Language\",\n    author = \"Boldt, Brendon  and\n      Mortensen, David\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.82/\",\n    doi = \"10.18653/v1/2024.naacl-long.82\",\n    pages = \"1475--1489\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.82.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.82/",
        "pdf_size": 287542,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:3ZjuKdFzvOoJ:scholar.google.com/&scioq=XferBench:+a+Data-Driven+Benchmark+for+Emergent+Language&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2
    },
    {
        "id": "2024.naacl-long.295",
        "title": "You don\u2019t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting LLMs elicits responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLMs\u2019 capabilities to generate answers, as well as prompt variations to examine their consistency with respect to content-level variations such as switching the order of response options or negating the statement. Our experiments on 17 different LLMs reveal that even simple perturbations significantly downgrade a model\u2019s question-answering ability, and that most LLMs have low negation consistency. Our results suggest that the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, and we therefore discuss potential alternatives to improve these issues.",
        "author": "Bangzhao Shu; Lechen Zhang; Minje Choi; Lavinia Dunagan; Lajanugen Logeswaran; Moontae Lee; Dallas Card; David Jurgens",
        "authorids": "/b/bangzhao-shu/; /l/lechen-zhang/; /m/minje-choi/; /l/lavinia-dunagan/; /l/lajanugen-logeswaran/; /m/moontae-lee/; /d/dallas-card/; /d/david-jurgens/",
        "bibtex": "@inproceedings{shu-etal-2024-dont,\n    title = \"You don{'}t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments\",\n    author = \"Shu, Bangzhao  and\n      Zhang, Lechen  and\n      Choi, Minje  and\n      Dunagan, Lavinia  and\n      Logeswaran, Lajanugen  and\n      Lee, Moontae  and\n      Card, Dallas  and\n      Jurgens, David\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.295/\",\n    doi = \"10.18653/v1/2024.naacl-long.295\",\n    pages = \"5263--5281\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.295.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.295/",
        "pdf_size": 397782,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14562863428879888115&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "University of Michigan, Ann Arbor, MI, USA+LG AI Research, Ann Arbor, MI, USA; University of Michigan, Ann Arbor, MI, USA+LG AI Research, Ann Arbor, MI, USA; Georgia Institute of Technology, Atlanta, GA, USA; University of Michigan, Ann Arbor, MI, USA; LG AI Research, Ann Arbor, MI, USA; LG AI Research, Ann Arbor, MI, USA; University of Michigan, Ann Arbor, MI, USA; University of Michigan, Ann Arbor, MI, USA",
        "aff_domain": "umich.edu;umich.edu;gatech.edu;umich.edu;lgresearch.ai;lgresearch.ai;umich.edu;umich.edu",
        "email": "umich.edu;umich.edu;gatech.edu;umich.edu;lgresearch.ai;lgresearch.ai;umich.edu;umich.edu",
        "github": "",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0+1;0+1;2;0;1;1;0;0",
        "aff_unique_norm": "University of Michigan;LG;Georgia Institute of Technology",
        "aff_unique_dep": ";LG AI Research;",
        "aff_unique_url": "https://www.umich.edu;https://www.lgaires.com;https://www.gatech.edu",
        "aff_unique_abbr": "UM;LG AI;Georgia Tech",
        "aff_campus_unique_index": "0+0;0+0;1;0;0;0;0;0",
        "aff_campus_unique": "Ann Arbor;Atlanta",
        "aff_country_unique_index": "0+0;0+0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.220",
        "title": "Z-GMOT: Zero-shot Generic Multiple Object Tracking",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Despite recent significant progress, Multi-Object Tracking (MOT) faces limitations such as reliance on prior knowledge and predefined categories and struggles with unseen objects. To address these issues, Generic Multiple Object Tracking (GMOT) has emerged as an alternative approach, requiring less prior information. However, current GMOT methods often rely on initial bounding boxes and struggle to handle variations in factors such as viewpoint, lighting, occlusion, and scale, among others. Our contributions commence with the introduction of the Referring GMOT dataset a collection of videos, each accompanied by detailed textual descriptions of their attributes. Subsequently, we propose Z-GMOT, a cutting-edge tracking solution capable of tracking objects from never-seen categories without the need of initial bounding boxes or predefined categories. Within our Z-GMOT framework, we introduce two novel components: (i) iGLIP, an improved Grounded language-image pretraining, for accurately detecting unseen objects with specific characteristics. (ii) MA-SORT, a novel object association approach that adeptly integrates motion and appearance-based matching strategies to tackle the complex task of tracking objects with high similarity. Our contributions are benchmarked through extensive experiments conducted on the Referring GMOT dataset for GMOT task. Additionally, to assess the generalizability of the proposed Z-GMOT, we conduct ablation studies on the DanceTrack and MOT20 datasets for the MOT task. Our dataset, code, and models are released at: https://fsoft-aic.github.io/Z-GMOT",
        "author": "Kim Tran; Anh Duy Le Dinh; Tien-Phat Nguyen; Thinh Phan; Pha Nguyen; Khoa Luu; Donald Adjeroh; Gianfranco Doretto; Ngan Le",
        "authorids": "/k/kim-tran/; /a/anh-duy-le-dinh/; /t/tien-phat-nguyen/; /t/thinh-phan/; /p/pha-nguyen/; /k/khoa-luu/; /d/donald-adjeroh/; /g/gianfranco-doretto/; /n/ngan-le/",
        "bibtex": "@inproceedings{tran-etal-2024-z,\n    title = \"{Z}-{GMOT}: Zero-shot Generic Multiple Object Tracking\",\n    author = \"Tran, Kim  and\n      Le Dinh, Anh Duy  and\n      Nguyen, Tien-Phat  and\n      Phan, Thinh  and\n      Nguyen, Pha  and\n      Luu, Khoa  and\n      Adjeroh, Donald  and\n      Doretto, Gianfranco  and\n      Le, Ngan\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.220/\",\n    doi = \"10.18653/v1/2024.findings-naacl.220\",\n    pages = \"3468--3479\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.220.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.220/",
        "pdf_size": 16839192,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16130695484604506849&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "FPT Software AI Center, Vietnam+VNUHCM-University of Science, Ho Chi Minh City, Vietnam; FPT Software AI Center, Vietnam; FPT Software AI Center, Vietnam; Department of Computer Science, University of Arkansas, USA; Department of Computer Science, University of Arkansas, USA; Department of Computer Science, University of Arkansas, USA; Department of Computer Science, West Virginia University, USA; Department of Computer Science, West Virginia University, USA; Department of Computer Science, University of Arkansas, USA",
        "aff_domain": ";;;;;;;;",
        "email": ";;;;;;;;",
        "github": "",
        "project": "https://fsoft-aic.github.io/Z-GMOT",
        "author_num": 9,
        "aff_unique_index": "0+1;0;0;2;2;2;3;3;2",
        "aff_unique_norm": "FPT Software;University of Science;University of Arkansas;West Virginia University",
        "aff_unique_dep": "AI Center;;Department of Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.fpt-software.com;;https://www.uark.edu;https://www.wvu.edu",
        "aff_unique_abbr": ";VNUHCM-US;UARK;WVU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Ho Chi Minh City",
        "aff_country_unique_index": "0+0;0;0;1;1;1;1;1;1",
        "aff_country_unique": "Vietnam;United States"
    },
    {
        "id": "2024.findings-naacl.116",
        "title": "ZSEE: A Dataset based on Zeolite Synthesis Event Extraction for Automated Synthesis Platform",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Automated synthesis of zeolite, one of the most important catalysts in chemical industries, holds great significance for attaining economic and environmental benefits. Structural synthesis data extracted through NLP technologies from zeolite experimental procedures can significantly expedite automated synthesis owing to its machine readability. However, the utilization of NLP technologies in information extraction of zeolite synthesis remains restricted due to the lack of annotated datasets. In this paper, we formulate an event extraction task to mine structural synthesis actions from experimental narratives for modular automated synthesis. Furthermore, we introduce ZSEE, a novel dataset containing fine-grained event annotations of zeolite synthesis actions. Our dataset features 16 event types and 13 argument roles which cover all the experimental operational steps of zeolite synthesis. We explore current state-of-the-art event extraction methods on ZSEE, perform error analysis based on the experimental results, and summarize the challenges and corresponding research directions to further facilitate the automated synthesis of zeolites. The code is publicly available at https://github.com/Hi-0317/ZSEE.",
        "author": "Song He; Xin Peng; Yihan Cai; Xin Li; Zhiqing Yuan; WenLi Du; Weimin Yang",
        "authorids": "/s/song-he/; /x/xin-peng/; /y/yihan-cai/; /x/xin-li/; /z/zhiqing-yuan/; /w/wenli-du/; /w/weimin-yang/",
        "bibtex": "@inproceedings{he-etal-2024-zsee,\n    title = \"{ZSEE}: A Dataset based on Zeolite Synthesis Event Extraction for Automated Synthesis Platform\",\n    author = \"He, Song  and\n      Peng, Xin  and\n      Cai, Yihan  and\n      Li, Xin  and\n      Yuan, Zhiqing  and\n      Du, WenLi  and\n      Yang, Weimin\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.116/\",\n    doi = \"10.18653/v1/2024.findings-naacl.116\",\n    pages = \"1791--1808\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.116.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.116/",
        "pdf_size": 1416461,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15585444793718822632&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "State Key Laboratory of Industrial Control Technology, East China University of Science and Technology; State Key Laboratory of Industrial Control Technology, East China University of Science and Technology; State Key Laboratory of Green Chemical Engineering and Industrial Catalysis, Sinopec Shanghai Research Institute of Petrochemical Technology; State Key Laboratory of Green Chemical Engineering and Industrial Catalysis, Sinopec Shanghai Research Institute of Petrochemical Technology; State Key Laboratory of Green Chemical Engineering and Industrial Catalysis, Sinopec Shanghai Research Institute of Petrochemical Technology; State Key Laboratory of Industrial Control Technology, East China University of Science and Technology; State Key Laboratory of Green Chemical Engineering and Industrial Catalysis, Sinopec Shanghai Research Institute of Petrochemical Technology",
        "aff_domain": "ecust.edu.cn;ecust.edu.cn;sinopec.com;sinopec.com;sinopec.com;ecust.edu.cn;sinopec.com",
        "email": "ecust.edu.cn;ecust.edu.cn;sinopec.com;sinopec.com;sinopec.com;ecust.edu.cn;sinopec.com",
        "github": "https://github.com/Hi-0317/ZSEE",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0;0;1;1;1;0;1",
        "aff_unique_norm": "East China University of Science and Technology;Sinopec Shanghai Research Institute of Petrochemical Technology",
        "aff_unique_dep": "State Key Laboratory of Industrial Control Technology;State Key Laboratory of Green Chemical Engineering and Industrial Catalysis",
        "aff_unique_url": "http://www.ecust.edu.cn;http://www.sRIPT.com.cn",
        "aff_unique_abbr": "ECUST;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-short.37",
        "title": "Zero-Shot vs. Translation-Based Cross-Lingual Transfer: The Case of Lexical Gaps",
        "track": "main",
        "status": "Short",
        "award": false,
        "abstract": "Cross-lingual transfer can be achieved through two main approaches: zero-shot transfer or machine translation (MT). While the former has been the dominant approach, both have been shown to be competitive. In this work, we compare the current performance and long-term viability of these methods. We leverage lexical gaps to create a multilingual question answering dataset, which provides a difficult domain for evaluation. Both approaches struggle in this setting, though zero-shot transfer performs better, as current MT outputs are not specific enough for the task. Using oracle translation offers the best performance, showing that this approach can perform well long-term, however current MT quality is a bottleneck. We also conduct an exploratory study to see if humans produce translations sufficient for the task with only general instructions. We find this to be true for the majority of translators, but not all. This indicates that while translation has the potential to outperform zero-shot approaches, creating MT models that generate accurate task-specific translations may not be straightforward.",
        "author": "Abteen Ebrahimi; Katharina von der Wense",
        "authorids": "/a/abteen-ebrahimi/; /k/katharina-von-der-wense/",
        "bibtex": "@inproceedings{ebrahimi-wense-2024-zero,\n    title = \"Zero-Shot vs. Translation-Based Cross-Lingual Transfer: The Case of Lexical Gaps\",\n    author = \"Ebrahimi, Abteen  and\n      von der Wense, Katharina\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-short.37/\",\n    doi = \"10.18653/v1/2024.naacl-short.37\",\n    pages = \"443--458\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-short.37.pdf",
        "site": "https://aclanthology.org/2024.naacl-short.37/",
        "pdf_size": 194278,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4103644383573631385&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 0,
        "aff": "University of Colorado Boulder; University of Colorado Boulder + Johannes Gutenberg University Mainz",
        "aff_domain": "colorado.edu; ",
        "email": "colorado.edu; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "aff_unique_index": "0;0+1",
        "aff_unique_norm": "University of Colorado;Johannes Gutenberg University Mainz",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.colorado.edu;https://www.jgu.de",
        "aff_unique_abbr": "CU;JGU",
        "aff_campus_unique_index": "0;0+1",
        "aff_campus_unique": "Boulder;Mainz",
        "aff_country_unique_index": "0;0+1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "2024.naacl-long.289",
        "title": "Zero-shot Generative Linguistic Steganography",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Generative linguistic steganography attempts to hide secret messages into covertext. Previous studies have generally focused on the statistical differences between the covertext and stegotext, however, ill-formed stegotext can readily be identified by humans. In this paper, we propose a novel zero-shot approach based on in-context learning for linguistic steganography to achieve better perceptual and statistical imperceptibility. We also design several new metrics and reproducible language evaluations to measure the imperceptibility of the stegotext. Our experimental results indicate that our method produces 1.926\u00d7 more innocent and intelligible stegotext than any other method.",
        "author": "Ke Lin; Yiyang Luo; Zijian Zhang; Luo Ping",
        "authorids": "/k/ke-lin/; /y/yiyang-luo/; /z/zijian-zhang/; /l/luo-ping/",
        "bibtex": "@inproceedings{lin-etal-2024-zero,\n    title = \"Zero-shot Generative Linguistic Steganography\",\n    author = \"Lin, Ke  and\n      Luo, Yiyang  and\n      Zhang, Zijian  and\n      Ping, Luo\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.289/\",\n    doi = \"10.18653/v1/2024.naacl-long.289\",\n    pages = \"5168--5182\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.289.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.289/",
        "pdf_size": 561566,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9123795460374397052&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Tsinghua University; Nanyang Technological University; Tsinghua University; Tsinghua University",
        "aff_domain": "gmail.com;outlook.com;gmail.com;tsinghua.edu.cn",
        "email": "gmail.com;outlook.com;gmail.com;tsinghua.edu.cn",
        "github": "https://github.com/leonardodalinky/zero-shot-GLS",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Tsinghua University;Nanyang Technological University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.ntu.edu.sg",
        "aff_unique_abbr": "THU;NTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2024.naacl-demo.20",
        "title": "ZhuJiu-Knowledge: A Fairer Platform for Evaluating Multiple Knowledge Types in Large Language Models",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "The swift advancement in large language models (LLMs) has heightened the importance of model evaluations. LLMs have acquired a substantial amount of knowledge, and evaluating the knowledge of these LLMs is crucial. To address this, we introduce the ZhuJiu-Knowledge benchmark which carefully considers the following factors: (1) For knowledge scope, we concentrate on three domains: commonsense knowledge, world knowledge, language knowledge, which comes from ATOMIC, Conceptnet, Wikidata, and Wordnet. (2) For data construction, to prevent data contamination, we utilize knowledge derived from corpora and knowledge graphs to formulate novel questions which are ensured not to appear in the training corpus. A multitude of prompts is purposefully devised to mitigate the impact of prompt design on evaluation and to further analyze the LLMs\u2019 sensitivity to various prompts. (3) For evaluation criteria, we propose a novel voting methodology for assessing generative text, aligning the model\u2019s evaluation with human preferences to reduce biases inherent in individual model assessments. We evaluate 14 current mainstream LLMs and conduct a comprehensive discussion and analysis of their results. The ZhuJiu-Knowledge benchmark and open-participation leaderboard are publicly released at http://zhujiu-knowledge.top and we also provide a demo video at https://youtu.be/QJp4qlEHVH8.",
        "author": "Pengfan Du; Sirui Liang; Baoli Zhang; Pengfei Cao; Yubo Chen; Kang Liu; Jun Zhao",
        "authorids": "/p/pengfan-du/; /s/sirui-liang/; /b/baoli-zhang/; /p/pengfei-cao/; /y/yubo-chen/; /k/kang-liu/; /j/jun-zhao/",
        "bibtex": "@inproceedings{du-etal-2024-zhujiu,\n    title = \"{Z}hu{J}iu-Knowledge: A Fairer Platform for Evaluating Multiple Knowledge Types in Large Language Models\",\n    author = \"Du, Pengfan  and\n      Liang, Sirui  and\n      Zhang, Baoli  and\n      Cao, Pengfei  and\n      Chen, Yubo  and\n      Liu, Kang  and\n      Zhao, Jun\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.20/\",\n    doi = \"10.18653/v1/2024.naacl-demo.20\",\n    pages = \"194--206\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.20.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.20/",
        "pdf_size": 2672682,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=478766747511638046&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Institute of Automation, Chinese Academy of Sciences+School of Artificial Intelligence, University of Chinese Academy of Sciences; Institute of Automation, Chinese Academy of Sciences+School of Artificial Intelligence, University of Chinese Academy of Sciences; Institute of Automation, Chinese Academy of Sciences+School of Artificial Intelligence, University of Chinese Academy of Sciences; Institute of Automation, Chinese Academy of Sciences; Institute of Automation, Chinese Academy of Sciences+School of Artificial Intelligence, University of Chinese Academy of Sciences+Shanghai Artificial Intelligence Laboratory; Institute of Automation, Chinese Academy of Sciences+School of Artificial Intelligence, University of Chinese Academy of Sciences+Shanghai Artificial Intelligence Laboratory; Institute of Automation, Chinese Academy of Sciences+School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "aff_domain": "mails.ucas.ac.cn;ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "email": "mails.ucas.ac.cn;ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn;nlpr.ia.ac.cn",
        "github": "",
        "project": "http://zhujiu-knowledge.top/",
        "author_num": 7,
        "aff_unique_index": "0+1;0+1;0+1;0;0+1+2;0+1+2;0+1",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Chinese Academy of Sciences;Shanghai Artificial Intelligence Laboratory",
        "aff_unique_dep": "Institute of Automation;School of Artificial Intelligence;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ucas.ac.cn;http://www.shailab.org/",
        "aff_unique_abbr": "CAS;UCAS;Shanghai AI Lab",
        "aff_campus_unique_index": ";;;;;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0+0;0;0+0+0;0+0+0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.findings-naacl.105",
        "title": "i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "The convergence of text, visual, and audio data is crucial towards human-like artificial intelligence, however the current Vision-Language-Speech landscape is dominated by encoder-only models that lack generative abilities. We propose closing this gap with i-Code V2, one of the first models capable of generating natural language from any combination of Vision, Language, and Speech data. i-Code V2 leverages state-of-the-art single-modality encoders, combining their outputs with a new modality-fusing encoder to project combinations of modalities into a shared representational space. Language tokens are generated from these representations via an autoregressive decoder. i-Code V2 is pretrained end-to-end on a large collection of dual- and single-modality datasets with a novel text completion objective that can be generalized across arbitrary combinations of modalities. i-Code V2 matches or outperforms state-of-the-art single- and dual-modality baselines on 7 multimodal tasks, demonstrating the power of generative multimodal pretraining across a diversity of tasks and signals.",
        "author": "Ziyi Yang; Mahmoud Khademi; Yichong Xu; Reid Pryzant; Yuwei Fang; Chenguang Zhu; Dongdong Chen; Yao Qian; Xuemei Gao; Yi-Ling Chen; Robert Gmyr; Naoyuki Kanda; Noel Codella; Bin Xiao; Yu Shi; Lu Yuan; Takuya Yoshioka; Michael Zeng; Xuedong Huang",
        "authorids": "/z/ziyi-yang/; /m/mahmoud-khademi/; /y/yichong-xu/; /r/reid-pryzant/; /y/yuwei-fang/; /c/chenguang-zhu/; /d/dongdong-chen/; /y/yao-qian/; /x/xuemei-gao/; /y/yi-ling-chen/; /r/robert-gmyr/; /n/naoyuki-kanda/; /n/noel-codella/; /b/bin-xiao/; /y/yu-shi/; /l/lu-yuan/; /t/takuya-yoshioka/; /m/michael-zeng/; /x/xuedong-huang/",
        "bibtex": "@inproceedings{yang-etal-2024-code,\n    title = \"i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data\",\n    author = \"Yang, Ziyi  and\n      Khademi, Mahmoud  and\n      Xu, Yichong  and\n      Pryzant, Reid  and\n      Fang, Yuwei  and\n      Zhu, Chenguang  and\n      Chen, Dongdong  and\n      Qian, Yao  and\n      Gao, Xuemei  and\n      Chen, Yi-Ling  and\n      Gmyr, Robert  and\n      Kanda, Naoyuki  and\n      Codella, Noel  and\n      Xiao, Bin  and\n      Shi, Yu  and\n      Yuan, Lu  and\n      Yoshioka, Takuya  and\n      Zeng, Michael  and\n      Huang, Xuedong\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.105/\",\n    doi = \"10.18653/v1/2024.findings-naacl.105\",\n    pages = \"1615--1627\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.105.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.105/",
        "pdf_size": 423049,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7986506457749170535&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI; Microsoft AI",
        "aff_domain": ";;;;;;;;;;;;;;;;;;",
        "email": ";;;;;;;;;;;;;;;;;;",
        "github": "",
        "project": "",
        "author_num": 19,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft AI",
        "aff_unique_url": "https://www.microsoft.com",
        "aff_unique_abbr": "Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-long.241",
        "title": "iACOS: Advancing Implicit Sentiment Extraction with Informative and Adaptive Negative Examples",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Aspect-based sentiment analysis (ABSA) have been extensively studied, but little light has been shed on the quadruple extraction consisting of four fundamental elements: aspects, categories, opinions and sentiments, especially with implicit aspects and opinions. In this paper, we propose a new method iACOS for extracting Implicit Aspects with Categories and Opinions with Sentiments. First, iACOS appends two implicit tokens at the end of a text to capture the context-aware representation of all tokens including implicit aspects and opinions. Second, iACOS develops a sequence labeling model over the context-aware token representation to co-extract explicit and implicit aspects and opinions. Third, iACOS devises a multi-label classifier with a specialized multi-head attention for discovering aspect-opinion pairs and predicting their categories and sentiments simultaneously. Fourth, iACOS leverages informative and adaptive negative examples to jointly train the multi-label classifier and the other two classifiers on categories and sentiments by multi-task learning. Finally, the experimental results show that iACOS significantly outperforms other quadruple extraction baselines according to the F1 score on two public benchmark datasets.",
        "author": "Xiancai Xu; Jia-Dong Zhang; Lei Xiong; Zhishang Liu",
        "authorids": "/x/xiancai-xu/; /j/jia-dong-zhang/; /l/lei-xiong/; /z/zhishang-liu/",
        "bibtex": "@inproceedings{xu-etal-2024-iacos,\n    title = \"i{ACOS}: Advancing Implicit Sentiment Extraction with Informative and Adaptive Negative Examples\",\n    author = \"Xu, Xiancai  and\n      Zhang, Jia-Dong  and\n      Xiong, Lei  and\n      Liu, Zhishang\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.241/\",\n    doi = \"10.18653/v1/2024.naacl-long.241\",\n    pages = \"4283--4293\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.241.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.241/",
        "pdf_size": 415289,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17973722819753609428&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Brands & Consumers Research Institute, Enbrands Inc., Shenzhen, China; Brands & Consumers Research Institute, Enbrands Inc., Shenzhen, China; Brands & Consumers Research Institute, Enbrands Inc., Shenzhen, China; Brands & Consumers Research Institute, Enbrands Inc., Shenzhen, China",
        "aff_domain": "enbrands.com;enbrands.com;enbrands.com;enbrands.com",
        "email": "enbrands.com;enbrands.com;enbrands.com;enbrands.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Enbrands Inc.",
        "aff_unique_dep": "Brands & Consumers Research Institute",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2024.naacl-demo.7",
        "title": "jp-evalb: Robust Alignment-based PARSEVAL Measures",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "We introduce an evaluation system designed to compute PARSEVAL measures, offering a viable alternative to evalb commonly used for constituency parsing evaluation. The widely used evalb script has traditionally been employed for evaluating the accuracy of constituency parsing results, albeit with the requirement for consistent tokenization and sentence boundaries. In contrast, our approach, named jp-evalb, is founded on an alignment method. This method aligns sentences and words when discrepancies arise. It aims to overcome several known issues associated with evalb by utilizing the \u2018jointly preprocessed (JP)\u2019 alignment-based method. We introduce a more flexible and adaptive framework, ultimately contributing to a more accurate assessment of constituency parsing performance.",
        "author": "Jungyeul Park; Junrui Wang; Eunkyul Leah Jo; Angela Yoonseo Park",
        "authorids": "/j/jungyeul-park/; /j/junrui-wang/; /e/eunkyul-leah-jo/; /a/angela-yoonseo-park/",
        "bibtex": "@inproceedings{park-etal-2024-jp,\n    title = \"jp-evalb: Robust Alignment-based {PARSEVAL} Measures\",\n    author = \"Park, Jungyeul  and\n      Wang, Junrui  and\n      Jo, Eunkyul Leah  and\n      Park, Angela Yoonseo\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.7/\",\n    doi = \"10.18653/v1/2024.naacl-demo.7\",\n    pages = \"70--77\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.7.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.7/",
        "pdf_size": 306915,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3594406976301309168&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Linguistics, The University of British Columbia, Canada; Department of Linguistics, The University of British Columbia, Canada; Department of Computer Science, The University of British Columbia, Canada + Facult\u00e9 des Sciences et Ing\u00e9nierie, Sorbonne Universit\u00e9, France; Department of Linguistics, The University of British Columbia, Canada",
        "aff_domain": "mail.ubc.ca;student.ubc.ca;student.ubc.ca;student.ubc.ca",
        "email": "mail.ubc.ca;student.ubc.ca;student.ubc.ca;student.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;0;0+1;0",
        "aff_unique_norm": "University of British Columbia;Sorbonne Universit\u00e9",
        "aff_unique_dep": "Department of Linguistics;Facult\u00e9 des Sciences et Ing\u00e9nierie",
        "aff_unique_url": "https://www.ubc.ca;https://www.sorbonne-universite.fr",
        "aff_unique_abbr": "UBC;Sorbonne",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;0",
        "aff_country_unique": "Canada;France"
    },
    {
        "id": "2024.naacl-long.19",
        "title": "kNN-ICL: Compositional Task-Oriented Parsing Generalization with Nearest Neighbor In-Context Learning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Task-Oriented Parsing (TOP) enables conversational assistants to interpret user commands expressed in natural language, transforming them into structured outputs that combine elements of both natural language and intent/slot tags. Recently, Large Language Models (LLMs) have achieved impressive performance in synthesizing computer programs based on a natural-language prompt, mitigating the gap between natural language and structured programs. Our paper focuses on harnessing the capabilities of LLMs for semantic parsing tasks, addressing the following three key research questions: 1) How can LLMs be effectively utilized for semantic parsing tasks? 2) What defines an effective prompt? and 3) How can LLM overcome the length constraint and streamline prompt design by including all examples as prompts? We introduce k Nearest Neighbor In-Context Learning (kNN-ICL), which simplifies prompt engineering by allowing it to be built on top of any design strategy while providing access to all demo examples. Extensive experiments show that: 1) Simple ICL without kNN search can achieve a comparable performance with strong supervised models on the TOP tasks, and 2) kNN-ICL significantly improves the comprehension of complex requests by seamlessly integrating ICL with a nearest-neighbor approach. Notably, this enhancement is achieved without the need for additional data or specialized prompts.",
        "author": "Wenting Zhao; Ye Liu; Yao Wan; Yibo Wang; Qingyang Wu; Zhongfen Deng; Jiangshu Du; Shuaiqi Liu; Yunlong Xu; Philip Yu",
        "authorids": "/w/wenting-zhao/; /y/ye-liu/; /y/yao-wan/; /y/yibo-wang/; /q/qingyang-wu/; /z/zhongfen-deng/; /j/jiangshu-du/; /s/shuaiqi-liu/; /y/yunlong-xu/; /p/philip-s-yu/",
        "bibtex": "@inproceedings{zhao-etal-2024-knn,\n    title = \"$k${NN}-{ICL}: Compositional Task-Oriented Parsing Generalization with Nearest Neighbor In-Context Learning\",\n    author = \"Zhao, Wenting  and\n      Liu, Ye  and\n      Wan, Yao  and\n      Wang, Yibo  and\n      Wu, Qingyang  and\n      Deng, Zhongfen  and\n      Du, Jiangshu  and\n      Liu, Shuaiqi  and\n      Xu, Yunlong  and\n      Yu, Philip\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.19/\",\n    doi = \"10.18653/v1/2024.naacl-long.19\",\n    pages = \"326--337\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.19.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.19/",
        "pdf_size": 388400,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3706125881828087581&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "University of Illinois Chicago; Salesforce Research; Huazhong University of Science and Technology; Columbia University; University of Illinois Chicago; University of Illinois Chicago; University of Illinois Chicago; The Hong Kong Polytechnic University; State University of New York at Binghamton; University of Illinois Chicago",
        "aff_domain": "uic.edu; ; ; ; ; ; ; ; ; ",
        "email": "uic.edu; ; ; ; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 10,
        "aff_unique_index": "0;1;2;3;0;0;0;4;5;0",
        "aff_unique_norm": "University of Illinois at Chicago;Salesforce;Huazhong University of Science and Technology;Columbia University;Hong Kong Polytechnic University;State University of New York at Binghamton",
        "aff_unique_dep": ";Salesforce Research;;;;",
        "aff_unique_url": "https://www.uic.edu;https://research.salesforce.com;http://www.hust.edu.cn;https://www.columbia.edu;https://www.polyu.edu.hk;https://www.binghamton.edu",
        "aff_unique_abbr": "UIC;Salesforce;HUST;Columbia;PolyU;SUNY Binghamton",
        "aff_campus_unique_index": "0;0;0;0;2;3;0",
        "aff_campus_unique": "Chicago;;Hong Kong SAR;Binghamton",
        "aff_country_unique_index": "0;0;1;0;0;0;0;1;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "2024.naacl-long.56",
        "title": "mEdIT: Multilingual Text Editing via Instruction Tuning",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "We introduce mEdIT, a multi-lingual extension to CoEdIT \u2013 the recent state-of-the-art text editing models for writing assistance. mEdIT models are trained by fine-tuning multi-lingual large, pre-trained language models (LLMs) via instruction tuning. They are designed to take instructions from the user specifying the attributes of the desired text in the form of natural language instructions, such as \u201cGrammatik korrigieren\u201d (German) or \u201c\uc774 \ud14d\uc2a4 \ud2b8\ub97c \ub2e8\uc21c\ud654\u201d (Korean). We build mEdIT by curating data from multiple publicly available human-annotated text editing datasets for three text editing tasks (Grammatical Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse languages belonging to six different language families. We detail the design and training of mEdIT models and demonstrate their strong performance on many multi-lingual text editing benchmarks against other multilingual LLMs. We also find that mEdIT generalizes effectively to new languages over multilingual baselines. We publicly release our data, code, and trained models.",
        "author": "Vipul Raheja; Dimitris Alikaniotis; Vivek Kulkarni; Bashar Alhafni; Dhruv Kumar",
        "authorids": "/v/vipul-raheja/; /d/dimitris-alikaniotis/; /v/vivek-kulkarni/; /b/bashar-alhafni/; /d/dhruv-kumar/",
        "bibtex": "@inproceedings{raheja-etal-2024-medit,\n    title = \"m{E}d{IT}: Multilingual Text Editing via Instruction Tuning\",\n    author = \"Raheja, Vipul  and\n      Alikaniotis, Dimitris  and\n      Kulkarni, Vivek  and\n      Alhafni, Bashar  and\n      Kumar, Dhruv\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.56/\",\n    doi = \"10.18653/v1/2024.naacl-long.56\",\n    pages = \"979--1001\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.56.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.56/",
        "pdf_size": 737662,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14532229272500268446&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Grammarly; Grammarly; Grammarly; New York University Abu Dhabi; Grammarly",
        "aff_domain": "grammarly.com;grammarly.com;grammarly.com;nyu.edu;grammarly.com",
        "email": "grammarly.com;grammarly.com;grammarly.com;nyu.edu;grammarly.com",
        "github": "https://github.com/vipulraheja/meditMultilingual Editing",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Grammarly;New York University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.grammarly.com;https://nyu.edu",
        "aff_unique_abbr": "Grammarly;NYU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Abu Dhabi",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "United States;United Arab Emirates"
    },
    {
        "id": "2024.findings-naacl.103",
        "title": "mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Many pretrained multilingual models exhibit cross-lingual transfer ability, which is often attributed to a learned language-neutral representation during pretraining. However, it remains unclear what factors contribute to the learning of a language-neutral representation, and whether the learned language-neutral representation suffices to facilitate cross-lingual transfer. We propose a synthetic task, Multilingual Othello (mOthello), as a testbed to delve into these two questions. We find that: (1) models trained with naive multilingual pretraining fail to learn a language-neutral representation across all input languages; (2) the introduction of \u201canchor tokens\u201d (i.e., lexical items that are identical across languages) helps cross-lingual representation alignment; and (3) the learning of a language-neutral representation alone is not sufficient to facilitate cross-lingual transfer. Based on our findings, we propose a novel approach \u2013 multilingual pretraining with unified output space \u2013 that both induces the learning of language-neutral representation and facilitates cross-lingual transfer.",
        "author": "Tianze Hua; Tian Yun; Ellie Pavlick",
        "authorids": "/t/tianze-hua/; /t/tian-yun/; /e/ellie-pavlick/",
        "bibtex": "@inproceedings{hua-etal-2024-mothello,\n    title = \"m{O}thello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?\",\n    author = \"Hua, Tianze  and\n      Yun, Tian  and\n      Pavlick, Ellie\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.103/\",\n    doi = \"10.18653/v1/2024.findings-naacl.103\",\n    pages = \"1585--1598\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.103.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.103/",
        "pdf_size": 3502587,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10680725285304529768&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Brown University; Brown University; Brown University",
        "aff_domain": "brown.edu;brown.edu;brown.edu",
        "email": "brown.edu;brown.edu;brown.edu",
        "github": "https://github.com/ethahtz/multilingual_othello",
        "project": "",
        "author_num": 3,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Brown University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.brown.edu",
        "aff_unique_abbr": "Brown",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.naacl-demo.16",
        "title": "pyvene: A Library for Understanding and Improving PyTorch Models via Interventions",
        "track": "main",
        "status": "System Demonstrations",
        "award": false,
        "abstract": "Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, we introduce pyvene, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. pyvene supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. We show how pyvene provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at \u2018https://github.com/stanfordnlp/pyvene\u2018.",
        "author": "Zhengxuan Wu; Atticus Geiger; Aryaman Arora; Jing Huang; Zheng Wang; Noah Goodman; Christopher Manning; Christopher Potts",
        "authorids": "/z/zhengxuan-wu/; /a/atticus-geiger/; /a/aryaman-arora/; /j/jing-huang/; /z/zheng-wang/; /n/noah-goodman/; /c/christopher-d-manning/; /c/christopher-potts/",
        "bibtex": "@inproceedings{wu-etal-2024-pyvene,\n    title = \"pyvene: A Library for Understanding and Improving {P}y{T}orch Models via Interventions\",\n    author = \"Wu, Zhengxuan  and\n      Geiger, Atticus  and\n      Arora, Aryaman  and\n      Huang, Jing  and\n      Wang, Zheng  and\n      Goodman, Noah  and\n      Manning, Christopher  and\n      Potts, Christopher\",\n    editor = \"Chang, Kai-Wei  and\n      Lee, Annie  and\n      Rajani, Nazneen\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-demo.16/\",\n    doi = \"10.18653/v1/2024.naacl-demo.16\",\n    pages = \"158--165\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-demo.16.pdf",
        "site": "https://aclanthology.org/2024.naacl-demo.16/",
        "pdf_size": 1420487,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7714226264986231833&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Stanford University\u2020; Pr(Ai)2R Group\u2021; Stanford University\u2020; Stanford University\u2020; Stanford University\u2020; Stanford University\u2020; Stanford University\u2020; Stanford University\u2020",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "email": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "github": "https://github.com/stanfordnlp/pyvene",
        "project": "",
        "author_num": 8,
        "aff_unique_index": "0;1;0;0;0;0;0;0",
        "aff_unique_norm": "Stanford University;Pr(Ai)2R Group",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;",
        "aff_unique_abbr": "Stanford;",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "2024.naacl-long.104",
        "title": "zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become a heated topic. Various methods have been proposed to forecast links on TKGs. Most of them are embedding-based, where hidden representations are learned to represent knowledge graph (KG) entities and relations based on the observed graph contexts. Although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the unseen zero-shot relations that have no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic meanings stay close in the embedding space, enabling TKGF models to recognize zero-shot relations even without any observed graph context. Experimental results show that our approach helps TKGF models to achieve much better performance in forecasting the facts with previously unseen relations, while still maintaining their ability in link forecasting regarding seen relations.",
        "author": "Zifeng Ding; Heling Cai; Jingpei Wu; Yunpu Ma; Ruotong Liao; Bo Xiong; Volker Tresp",
        "authorids": "/z/zifeng-ding/; /h/heling-cai/; /j/jingpei-wu/; /y/yunpu-ma/; /r/ruotong-liao/; /b/bo-xiong/; /v/volker-tresp/",
        "bibtex": "@inproceedings{ding-etal-2024-zrllm,\n    title = \"zr{LLM}: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models\",\n    author = \"Ding, Zifeng  and\n      Cai, Heling  and\n      Wu, Jingpei  and\n      Ma, Yunpu  and\n      Liao, Ruotong  and\n      Xiong, Bo  and\n      Tresp, Volker\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.104/\",\n    doi = \"10.18653/v1/2024.naacl-long.104\",\n    pages = \"1877--1895\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.104.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.104/",
        "pdf_size": 2053357,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14131981079597443112&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "LMU Munich+Siemens AG; LMU Munich; LMU Munich; LMU Munich+Munich Center for Machine Learning (MCML); LMU Munich+Munich Center for Machine Learning (MCML); University of Stuttgart; LMU Munich",
        "aff_domain": "campus.lmu.de;campus.lmu.de;dbs.ifi.lmu.de;dbs.ifi.lmu.de;gmail.com;ki.uni-stuttgart.de;dbs.ifi.lmu.de",
        "email": "campus.lmu.de;campus.lmu.de;dbs.ifi.lmu.de;dbs.ifi.lmu.de;gmail.com;ki.uni-stuttgart.de;dbs.ifi.lmu.de",
        "github": "",
        "project": "",
        "author_num": 7,
        "aff_unique_index": "0+1;0;0;0+2;0+2;3;0",
        "aff_unique_norm": "Ludwig Maximilian University of Munich;Siemens AG;Munich Center for Machine Learning;University of Stuttgart",
        "aff_unique_dep": ";;Center for Machine Learning;",
        "aff_unique_url": "https://www.lmu.de;https://www.siemens.com;https://www.munich-center-for-machine-learning.de;https://www.uni-stuttgart.de",
        "aff_unique_abbr": "LMU;Siemens;MCML;USTuttgart",
        "aff_campus_unique_index": "0;0;0;0+0;0+0;0",
        "aff_campus_unique": "Munich;",
        "aff_country_unique_index": "0+0;0;0;0+0;0+0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.61",
        "title": "\u201cOne-Size-Fits-All\u201d? Examining Expectations around What Constitute \u201cFair\u201d or \u201cGood\u201d NLG System Behaviors",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Fairness-related assumptions about what constitute appropriate NLG system behaviors range from invariance, where systems are expected to behave identically for social groups, to adaptation, where behaviors should instead vary across them. To illuminate tensions around invariance and adaptation, we conduct five case studies, in which we perturb different types of identity-related language features (names, roles, locations, dialect, and style) in NLG system inputs. Through these cases studies, we examine people\u2019s expectations of system behaviors, and surface potential caveats of these contrasting yet commonly held assumptions. We find that motivations for adaptation include social norms, cultural differences, feature-specific information, and accommodation; in contrast, motivations for invariance include perspectives that favor prescriptivism, view adaptation as unnecessary or too difficult for NLG systems to do appropriately, and are wary of false assumptions. Our findings highlight open challenges around what constitute \u201cfair\u201d or \u201cgood\u201d NLG system behaviors.",
        "author": "Li Lucy; Su Lin Blodgett; Milad Shokouhi; Hanna Wallach; Alexandra Olteanu",
        "authorids": "/l/li-lucy/; /s/su-lin-blodgett/; /m/milad-shokouhi/; /h/hanna-wallach/; /a/alexandra-olteanu/",
        "bibtex": "@inproceedings{lucy-etal-2024-one,\n    title = \"{\\textquotedblleft}One-Size-Fits-All{\\textquotedblright}? Examining Expectations around What Constitute {\\textquotedblleft}Fair{\\textquotedblright} or {\\textquotedblleft}Good{\\textquotedblright} {NLG} System Behaviors\",\n    author = \"Lucy, Li  and\n      Blodgett, Su Lin  and\n      Shokouhi, Milad  and\n      Wallach, Hanna  and\n      Olteanu, Alexandra\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.61/\",\n    doi = \"10.18653/v1/2024.naacl-long.61\",\n    pages = \"1054--1089\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.61.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.61/",
        "pdf_size": 5521361,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9069962424169021059&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "University of California, Berkeley; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research",
        "aff_domain": "berkeley.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "email": "berkeley.edu;microsoft.com;microsoft.com;microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 5,
        "aff_unique_index": "0;1;1;1;1",
        "aff_unique_norm": "University of California, Berkeley;Microsoft",
        "aff_unique_dep": ";Microsoft Research",
        "aff_unique_url": "https://www.berkeley.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "UC Berkeley;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2024.findings-naacl.128",
        "title": "\u201cTell me who you are and I tell you how you argue\u201d: Predicting Stances and Arguments for Stakeholder Groups",
        "track": "main",
        "status": "Findings",
        "award": false,
        "abstract": "Argument mining has focused so far mainly on the identification, extraction, and formalization of arguments. An important yet unaddressedtask consists in the prediction of the argumentative behavior of stakeholders in a debate. Predicting the argumentative behavior in advance can support foreseeing issues in public policy making or help recognize potential disagreements early on and help to resolve them. In this paper, we consider the novel task of predicting the argumentative behavior of individual stakeholders. We present ARGENST, a framework that relies on a recommender-based architecture to predict the stance and the argumentative main point on a specific controversial topic for a given stakeholder, which is described in terms of a profile including properties related to demographic attributes, religious and political orientation, socio-economic background, etc. We evaluate our approach on the well-known debate.org dataset in terms of accuracy for predicting stance as well as in terms of similarity of the generated arguments to the ground truth arguments using BERTScore. As part of a case study, we show how juries of members representing different stakeholder groups and perspectives can be assembled to simulate the public opinion on a given topic.",
        "author": "Philipp Heinisch; Lorik Dumani; Philipp Cimiano; Ralf Schenkel",
        "authorids": "/p/philipp-heinisch/; /l/lorik-dumani/; /p/philipp-cimiano/; /r/ralf-schenkel/",
        "bibtex": "@inproceedings{heinisch-etal-2024-tell,\n    title = \"{\\textquotedblleft}Tell me who you are and {I} tell you how you argue{\\textquotedblright}: Predicting Stances and Arguments for Stakeholder Groups\",\n    author = \"Heinisch, Philipp  and\n      Dumani, Lorik  and\n      Cimiano, Philipp  and\n      Schenkel, Ralf\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Findings of the Association for Computational Linguistics: NAACL 2024\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.findings-naacl.128/\",\n    doi = \"10.18653/v1/2024.findings-naacl.128\",\n    pages = \"1968--1982\"\n}",
        "pdf": "https://aclanthology.org/2024.findings-naacl.128.pdf",
        "site": "https://aclanthology.org/2024.findings-naacl.128/",
        "pdf_size": 1412855,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:2TdfGCzVpvUJ:scholar.google.com/&scioq=%E2%80%9CTell+me+who+you+are+and+I+tell+you+how+you+argue%E2%80%9D:+Predicting+Stances+and+Arguments+for+Stakeholder+Groups&hl=en&as_sdt=0,5",
        "gs_version_total": 0,
        "aff": "Bielefeld University; Trier University; Bielefeld University; Trier University",
        "aff_domain": "techfak.uni-bielefeld.de;uni-trier.de;techfak.uni-bielefeld.de;uni-trier.de",
        "email": "techfak.uni-bielefeld.de;uni-trier.de;techfak.uni-bielefeld.de;uni-trier.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0;1;0;1",
        "aff_unique_norm": "Bielefeld University;Trier University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.uni-bielefeld.de/;https://www.uni-trier.de",
        "aff_unique_abbr": "Uni Bielefeld;UT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "2024.naacl-long.439",
        "title": "\u201cYou are an expert annotator\u201d: Automatic Best\u2013Worst-Scaling Annotations for Emotion Intensity Modeling",
        "track": "main",
        "status": "Long",
        "award": false,
        "abstract": "Labeling corpora constitutes a bottleneck to create models for new tasks or domains. Large language models mitigate the issue with automatic corpus labeling methods, particularly for categorical annotations. Some NLP tasks such as emotion intensity prediction, however, require text regression, but there is no work on automating annotations for continuous label assignments. Regression is considered more challenging than classification: The fact that humans perform worse when tasked to choose values from a rating scale lead to comparative annotation methods, including best\u2013worst scaling. This raises the question if large language model-based annotation methods show similar patterns, namely that they perform worse on rating scale annotation tasks than on comparative annotation tasks. To study this, we automate emotion intensity predictions and compare direct rating scale predictions, pairwise comparisons and best\u2013worst scaling. We find that the latter shows the highest reliability. A transformer regressor fine-tuned on these data performs nearly on par with a model trained on the original manual annotations.",
        "author": "Christopher Bagdon; Prathamesh Karmalkar; Harsha Gurulingappa; Roman Klinger",
        "authorids": "/c/christopher-bagdon/; /p/prathamesh-karmalkar/; /h/harsha-gurulingappa/; /r/roman-klinger/",
        "bibtex": "@inproceedings{bagdon-etal-2024-expert,\n    title = \"{\\textquotedblleft}You are an expert annotator{\\textquotedblright}: Automatic Best{--}Worst-Scaling Annotations for Emotion Intensity Modeling\",\n    author = \"Bagdon, Christopher  and\n      Karmalkar, Prathamesh  and\n      Gurulingappa, Harsha  and\n      Klinger, Roman\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.439/\",\n    doi = \"10.18653/v1/2024.naacl-long.439\",\n    pages = \"7924--7936\"\n}",
        "pdf": "https://aclanthology.org/2024.naacl-long.439.pdf",
        "site": "https://aclanthology.org/2024.naacl-long.439/",
        "pdf_size": 262106,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8213540228927756265&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Merck Data & AI Organization, Merck Group, Darmstadt, Germany+Institut f\u00fcr Maschinelle Sprachverarbeitung, University of Stuttgart, Germany+Fundamentals of Natural Language Processing, University of Bamberg, Germany; Merck Data & AI Organization, Merck IT Centre, Merck Group, Bengaluru, India; Merck Data & AI Organization, Merck IT Centre, Merck Group, Bengaluru, India; Fundamentals of Natural Language Processing, University of Bamberg, Germany",
        "aff_domain": "uni-bamberg.de;merckgroup.com;merckgroup.com;uni-bamberg.de",
        "email": "uni-bamberg.de;merckgroup.com;merckgroup.com;uni-bamberg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "aff_unique_index": "0+1+2;0;0;2",
        "aff_unique_norm": "Merck Group;University of Stuttgart;University of Bamberg",
        "aff_unique_dep": "Merck Data & AI Organization;Institut f\u00fcr Maschinelle Sprachverarbeitung;Department of Natural Language Processing",
        "aff_unique_url": "https://www.merckgroup.com;https://www.uni-stuttgart.de;https://www.uni-bamberg.de/",
        "aff_unique_abbr": "Merck;;Uni Bamberg",
        "aff_campus_unique_index": "0;2;2",
        "aff_campus_unique": "Darmstadt;;Bengaluru",
        "aff_country_unique_index": "0+0+0;1;1;0",
        "aff_country_unique": "Germany;India"
    }
]