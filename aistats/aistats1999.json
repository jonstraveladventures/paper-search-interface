[
    {
        "id": "25b6d29cfa",
        "title": "A Note on the Comparison of Polynomial Selection Methods",
        "site": "https://proceedings.mlr.press/r2/viswanathan99a.html",
        "author": "Murlikrishna Viswanathan; Chris S. Wallace",
        "abstract": "Minimum Message Length (MML) and Structural Risk Minimisation (SRM) are two computational learning principles that have achieved wide acclaim in recent years. Whereas the former is based on Bayesian learning and the latter on the classical theory of VC-dimension, they are similar in their attempt to define a trade-off between model complexity and goodness of fit to the data. A recent empirical study by Wallace compared the performance of standard model selection methods in a one-dimensional polynomial regression framework. The results from this study provided strong evidence in support of the MML and SRM based methods over the other standard approaches. In this paper we present a detailed empirical evaluation of three model selection methods which include an MML based approach and two SRM based methods. Results from our analysis and experimental evaluation suggest that the MML-based approach in general has higher predictive accuracy and also raise questions on the inductive capabilities of the Structural Risk Minimization Principle.",
        "bibtex": "@InProceedings{pmlr-vR2-viswanathan99a,\n  title = \t {A Note on the Comparison of Polynomial Selection Methods},\n  author =       {Viswanathan, Murlikrishna and Wallace, Chris S.},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/viswanathan99a/viswanathan99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/viswanathan99a.html},\n  abstract = \t {Minimum Message Length (MML) and Structural Risk Minimisation (SRM) are two computational learning principles that have achieved wide acclaim in recent years. Whereas the former is based on Bayesian learning and the latter on the classical theory of VC-dimension, they are similar in their attempt to define a trade-off between model complexity and goodness of fit to the data. A recent empirical study by Wallace compared the performance of standard model selection methods in a one-dimensional polynomial regression framework. The results from this study provided strong evidence in support of the MML and SRM based methods over the other standard approaches. In this paper we present a detailed empirical evaluation of three model selection methods which include an MML based approach and two SRM based methods. Results from our analysis and experimental evaluation suggest that the MML-based approach in general has higher predictive accuracy and also raise questions on the inductive capabilities of the Structural Risk Minimization Principle.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/viswanathan99a/viswanathan99a.pdf",
        "supp": "",
        "pdf_size": 0,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10440282549026774138&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "a2493352ff",
        "title": "An experiment in causal discovery using a pneumonia database",
        "site": "https://proceedings.mlr.press/r2/spirtes99a.html",
        "author": "Peter Spirtes; Gregory F. Cooper",
        "abstract": "We tested a causal discovery algorithm on a database of pneumonia patients. The output of the causal discovery algorithm is a list of statements \"A causes B\", where A and B are variables in the database, and a score indicating the degree of confidence in the statement. We compared the output of the algorithm with the opinions of physicians about whether A caused B or not. We found that the doctors opinions were independent of the output of the algorithm. However, an examination of the output of results suggested a simple, well motivated modification of the algorithm which would bring the output of the algorithm into high agreement with the physicians opinions.",
        "bibtex": "@InProceedings{pmlr-vR2-spirtes99a,\n  title = \t {An experiment in causal discovery using a pneumonia database},\n  author =       {Spirtes, Peter and Cooper, Gregory F.},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/spirtes99a/spirtes99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/spirtes99a.html},\n  abstract = \t {We tested a causal discovery algorithm on a database of pneumonia patients. The output of the causal discovery algorithm is a list of statements \"A causes B\", where A and B are variables in the database, and a score indicating the degree of confidence in the statement. We compared the output of the algorithm with the opinions of physicians about whether A caused B or not. We found that the doctors opinions were independent of the output of the algorithm. However, an examination of the output of results suggested a simple, well motivated modification of the algorithm which would bring the output of the algorithm into high agreement with the physicians opinions.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/spirtes99a/spirtes99a.pdf",
        "supp": "",
        "pdf_size": 175527,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8460558381284393830&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Philosophy, Carnegie Mellon University; Department of Medical Informatics, University of Pittsburgh",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;University of Pittsburgh",
        "aff_unique_dep": "Department of Philosophy;Department of Medical Informatics",
        "aff_unique_url": "https://www.cmu.edu;https://www.pitt.edu",
        "aff_unique_abbr": "CMU;Pitt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a29bd4ea7f",
        "title": "Bayesian Graphical Models, Intention-to-Treat, and the Rubin Causal Model",
        "site": "https://proceedings.mlr.press/r2/madigan99a.html",
        "author": "David Madigan",
        "abstract": "In clinical trials with significant noncompliance the standard intention-to-treat analyses sometimes mislead. Rubin\u2019s causal model provides an alternative method of analysis that can shed extra light on clinical trial data. Formulating the Rubin Causal Model as a Bayesian graphical model facilitates model communication and computation.",
        "bibtex": "@InProceedings{pmlr-vR2-madigan99a,\n  title = \t {Bayesian Graphical Models, Intention-to-Treat, and the Rubin Causal Model},\n  author =       {Madigan, David},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/madigan99a/madigan99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/madigan99a.html},\n  abstract = \t {In clinical trials with significant noncompliance the standard intention-to-treat analyses sometimes mislead. Rubin\u2019s causal model provides an alternative method of analysis that can shed extra light on clinical trial data. Formulating the Rubin Causal Model as a Bayesian graphical model facilitates model communication and computation.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/madigan99a/madigan99a.pdf",
        "supp": "",
        "pdf_size": 509890,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Department of Statistics, University of Washington",
        "aff_domain": "stat.washington.edu",
        "email": "stat.washington.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3ee49c9f06",
        "title": "Boosting Methodology for Regression Problems",
        "site": "https://proceedings.mlr.press/r2/ridgeway99a.html",
        "author": "Greg Ridgeway; David Madigan; Thomas S. Richardson",
        "abstract": "Classification problems have dominated research on boosting to date. The application of boosting to regression problems, on the other hand, has received little investigation. In this paper we develop a new boosting method for regression problems. We cast the regression problem as a classification problem and apply an interpretable form of the boosted na\u00efve Bayes classifier. This induces a regression model that we show to be expressible as an additive model for which we derive estimators and discuss computational issues. We compare the performance of our boosted na\u00efve Bayes regression model with other interpretable multivariate regression procedures.",
        "bibtex": "@InProceedings{pmlr-vR2-ridgeway99a,\n  title = \t {Boosting Methodology for Regression Problems},\n  author =       {Ridgeway, Greg and Madigan, David and Richardson, Thomas S.},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/ridgeway99a/ridgeway99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/ridgeway99a.html},\n  abstract = \t {Classification problems have dominated research on boosting to date. The application of boosting to regression problems, on the other hand, has received little investigation. In this paper we develop a new boosting method for regression problems. We cast the regression problem as a classification problem and apply an interpretable form of the boosted na\u00efve Bayes classifier. This induces a regression model that we show to be expressible as an additive model for which we derive estimators and discuss computational issues. We compare the performance of our boosted na\u00efve Bayes regression model with other interpretable multivariate regression procedures.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/ridgeway99a/ridgeway99a.pdf",
        "supp": "",
        "pdf_size": 147538,
        "gs_citation": 159,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3182223566423201022&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Box 354322, Department of Statistics, University of Washington, Seattle, WA 98195; Box 354322, Department of Statistics, University of Washington, Seattle, WA 98195; Box 354322, Department of Statistics, University of Washington, Seattle, WA 98195",
        "aff_domain": "stat.washington.edu;stat.washington.edu;stat.washington.edu",
        "email": "stat.washington.edu;stat.washington.edu;stat.washington.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "39350d6465",
        "title": "Causal Mechanisms and Classification Trees for Predicting Chemical Carcinogens",
        "site": "https://proceedings.mlr.press/r2/cox99a.html",
        "author": "Louis Anthony Cox Jr",
        "abstract": "Classification trees, usually used as a nonlinear, nonparametric classification method, can also provide a powerful framework for comparing, assessing, and combining information from different expert systems, by treating their predictions as the independent variables in a classification tree analysis. This paper discusses the applied problem of classifying chemicals as human carcinogens. It shows how classification trees can be used to compare the information provided by ten different carcinogen classification expert systems, construct an improved \"hybrid\" classification system from them, and identify cost-effective combinations of assays (the inputs to the expert systems) to use in classifying chemicals in future.",
        "bibtex": "@InProceedings{pmlr-vR2-cox99a,\n  title = \t {Causal Mechanisms and Classification Trees for Predicting Chemical Carcinogens},\n  author =       {Cox, Jr, Louis Anthony},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {18--26},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/cox99a/cox99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/cox99a.html},\n  abstract = \t {Classification trees, usually used as a nonlinear, nonparametric classification method, can also provide a powerful framework for comparing, assessing, and combining information from different expert systems, by treating their predictions as the independent variables in a classification tree analysis. This paper discusses the applied problem of classifying chemicals as human carcinogens. It shows how classification trees can be used to compare the information provided by ten different carcinogen classification expert systems, construct an improved \"hybrid\" classification system from them, and identify cost-effective combinations of assays (the inputs to the expert systems) to use in classifying chemicals in future.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/cox99a/cox99a.pdf",
        "supp": "",
        "pdf_size": 254957,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:wjj9I-0FrjYJ:scholar.google.com/&scioq=Causal+Mechanisms+and+Classification+Trees+for+Predicting+Chemical+Carcinogens&hl=en&as_sdt=0,33",
        "gs_version_total": 3,
        "aff": "Cox Associates",
        "aff_domain": "cox-associates.com",
        "email": "cox-associates.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Cox Associates",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "44e372da1f",
        "title": "Conditional products: An alternative approach to conditional independence",
        "site": "https://proceedings.mlr.press/r2/dawid99a.html",
        "author": "A. Philip Dawid; Milan Studen\u00fd",
        "abstract": "We introduce a new abstract approach to the study of conditional independence, founded on a concept analogous to the factorization properties of probabilistic independence, rather than the separation properties of a graph. The basic ingredient is the \"conditional product\", which provides a way of combining the basic objects under consideration while preserving as much independence as possible. We introduce an appropriate axiom system for conditional product, and show how, when these axioms are obeyed, they induce a derived concept of conditional independence which obeys the usual semi-graphoid axioms. The general structure is used to throw light on three specific areas: the familiar probabilistic framework (both the discrete and the general case); a set-theoretic framework related to \"variation independence\"; and a variety of graphical frameworks.",
        "bibtex": "@InProceedings{pmlr-vR2-dawid99a,\n  title = \t {Conditional products: An alternative approach to conditional independence},\n  author =       {Dawid, A. Philip and Studen{\\'{y}}, Milan},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {27--35},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/dawid99a/dawid99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/dawid99a.html},\n  abstract = \t {We introduce a new abstract approach to the study of conditional independence, founded on a concept analogous to the factorization properties of probabilistic independence, rather than the separation properties of a graph. The basic ingredient is the \"conditional product\", which provides a way of combining the basic objects under consideration while preserving as much independence as possible. We introduce an appropriate axiom system for conditional product, and show how, when these axioms are obeyed, they induce a derived concept of conditional independence which obeys the usual semi-graphoid axioms. The general structure is used to throw light on three specific areas: the familiar probabilistic framework (both the discrete and the general case); a set-theoretic framework related to \"variation independence\"; and a variety of graphical frameworks.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/dawid99a/dawid99a.pdf",
        "supp": "",
        "pdf_size": 204859,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10592458809461843119&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Statistical Science, University College London, Gower Street, London WC1E 6BT, UK; Academy of Sciences of Czech Republic and University of Economics Prague, Pod vodarenskou vezi 4, Prague 18208, CZ",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;Academy of Sciences of the Czech Republic",
        "aff_unique_dep": "Department of Statistical Science;",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.cas.cz",
        "aff_unique_abbr": "UCL;ASCR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Czech Republic"
    },
    {
        "id": "99cf92f34c",
        "title": "Efficient Mining of Statistical Dependencies",
        "site": "https://proceedings.mlr.press/r2/oates99a.html",
        "author": "Tim Oates; Matthew D. Schmill; Paul R. Cohen; Casey Durfee",
        "abstract": "The Multi-Stream Dependency Detection algorithm finds rules that capture statistical dependencies between patterns in multivariate time series of categorical data. Rule strength is measured by the G statistic, and an upper bound on the value of G for the descendants of a node allows MSDD\u2019s search space to be pruned. However, in the worst case, the algorithm will explore exponentially many rules. This paper presents and empirically evaluates two ways of addressing this problem. The first is a set of three methods for reducing the size of MSDD\u2019s search space based on information collected during the search process. Second, we discuss an implementation of MSDD that distributes its computations over multiple machines on a network.",
        "bibtex": "@InProceedings{pmlr-vR2-oates99a,\n  title = \t {Efficient Mining of Statistical Dependencies},\n  author =       {Oates, Tim and Schmill, Matthew D. and Cohen, Paul R. and Durfee, Casey},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/oates99a/oates99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/oates99a.html},\n  abstract = \t {The Multi-Stream Dependency Detection algorithm finds rules that capture statistical dependencies between patterns in multivariate time series of categorical data. Rule strength is measured by the G statistic, and an upper bound on the value of G for the descendants of a node allows MSDD\u2019s search space to be pruned. However, in the worst case, the algorithm will explore exponentially many rules. This paper presents and empirically evaluates two ways of addressing this problem. The first is a set of three methods for reducing the size of MSDD\u2019s search space based on information collected during the search process. Second, we discuss an implementation of MSDD that distributes its computations over multiple machines on a network.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/oates99a/oates99a.pdf",
        "supp": "",
        "pdf_size": 0,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1137019785113004177&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ed7325d50f",
        "title": "Efficient learning using constrained sufficient statistics",
        "site": "https://proceedings.mlr.press/r2/friedman99a.html",
        "author": "Nir Friedman; Lise Getoor",
        "abstract": "Learning Bayesian networks is a central problem for pattern recognition, density estimation and classification. In this paper, we propose a new method for speeding up the computational process of learning Bayesian network",
        "bibtex": "@InProceedings{pmlr-vR2-friedman99a,\n  title = \t {Efficient learning using constrained sufficient statistics},\n  author =       {Friedman, Nir and Getoor, Lise},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/friedman99a/friedman99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/friedman99a.html},\n  abstract = \t {Learning Bayesian networks is a central problem for pattern recognition, density estimation and classification. In this paper, we propose a new method for speeding up the computational process of learning Bayesian network",
        "pdf": "http://proceedings.mlr.press/r2/friedman99a/friedman99a.pdf",
        "supp": "",
        "pdf_size": 220166,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=331436310778368520&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 13,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/ai-research-paper-summarizer",
        "project": "https://ai-research-paper-summarizer.project.com",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e5fa704075",
        "title": "Geometric modeling of a nuclear environment",
        "site": "https://proceedings.mlr.press/r2/geeter99a.html",
        "author": "Jan De Geeter; Marc Decr\u00e9ton; Joris De Schutter; Herman Bruyninckx; Hendrik Van Brussel",
        "abstract": "This paper is about the task-directed updating of an incomplete and inaccurate geometric model of a nuclear environment, using only robust radiation-resistant sensors installed on a robot that is remotely controlled by a human operator. In this problem, there are many sources of uncertainty and ambiguity. This paper proposes a probabilistic solution under Gaussian assumptions. Uncertainty is reduced with an estimator based on a Kalman filter. Ambiguity on the measurement-feature association is resolved by running a bank of those estimators in parallel, one for each plausible association. The residual errors of these estimators are used for hypothesis testing and for the calculation of a probability distribution over the remaining hypotheses. The best next sensing action is calculated as a Bayes decision with respect to a loss function that takes into account both the uncertainty on the current estimate, and the variance/precision required by the task.",
        "bibtex": "@InProceedings{pmlr-vR2-geeter99a,\n  title = \t {Geometric modeling of a nuclear environment},\n  author =       {Geeter, Jan De and Decr{\\'{e}}ton, Marc and Schutter, Joris De and Bruyninckx, Herman and Brussel, Hendrik Van},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/geeter99a/geeter99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/geeter99a.html},\n  abstract = \t {This paper is about the task-directed updating of an incomplete and inaccurate geometric model of a nuclear environment, using only robust radiation-resistant sensors installed on a robot that is remotely controlled by a human operator. In this problem, there are many sources of uncertainty and ambiguity. This paper proposes a probabilistic solution under Gaussian assumptions. Uncertainty is reduced with an estimator based on a Kalman filter. Ambiguity on the measurement-feature association is resolved by running a bank of those estimators in parallel, one for each plausible association. The residual errors of these estimators are used for hypothesis testing and for the calculation of a probability distribution over the remaining hypotheses. The best next sensing action is calculated as a Bayes decision with respect to a loss function that takes into account both the uncertainty on the current estimate, and the variance/precision required by the task.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/geeter99a/geeter99a.pdf",
        "supp": "",
        "pdf_size": 301922,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:j07-zbng4NoJ:scholar.google.com/&scioq=Geometric+modeling+of+a+nuclear+environment&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "SCK/CEN Research Centre for Nuclear Energy; K.U.Leuven, Dept. of Mechanical Engineering, Div. PMA; K.U.Leuven, Dept. of Mechanical Engineering, Div. PMA; K.U.Leuven, Dept. of Mechanical Engineering, Div. PMA; SCK/CEN Research Centre for Nuclear Energy",
        "aff_domain": "sckcen.be; ; ; ; ",
        "email": "sckcen.be; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;1;0",
        "aff_unique_norm": "SCK\u2022CEN - Belgian Nuclear Research Centre;KU Leuven",
        "aff_unique_dep": "Research Centre for Nuclear Energy;Department of Mechanical Engineering",
        "aff_unique_url": "https://www.sckcen.be;https://www.kuleuven.be",
        "aff_unique_abbr": "SCK\u2022CEN;KU Leuven",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Belgium"
    },
    {
        "id": "b2f90de0f9",
        "title": "Hierarchical IFA Belief Networks",
        "site": "https://proceedings.mlr.press/r2/attias99a.html",
        "author": "Hagai Attias",
        "abstract": "We introduce a new real-valued belief network, which is a multilayer generalization of independent factor analysis (IFA). At each level, this network extracts real-valued latent variables that are non-linear functions of the input data with a highly adaptive functional form, resulting in a hierarchical distributed representation of these data. The network is based on a probabilistic generative model, constructed by cascading single-layer IFA models. Whereas exact maximum-likelihood learning for this model is intractable, we present and demonstrate an algorithm that maximizes a lower bound on the likelihood. This algorithm is developed by formulating a variational approach to hierarchical IFA networks.",
        "bibtex": "@InProceedings{pmlr-vR2-attias99a,\n  title = \t {Hierarchical {IFA} Belief Networks},\n  author =       {Attias, Hagai},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {1--9},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/attias99a/attias99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/attias99a.html},\n  abstract = \t {We introduce a new real-valued belief network, which is a multilayer generalization of independent factor analysis (IFA). At each level, this network extracts real-valued latent variables that are non-linear functions of the input data with a highly adaptive functional form, resulting in a hierarchical distributed representation of these data. The network is based on a probabilistic generative model, constructed by cascading single-layer IFA models. Whereas exact maximum-likelihood learning for this model is intractable, we present and demonstrate an algorithm that maximizes a lower bound on the likelihood. This algorithm is developed by formulating a variational approach to hierarchical IFA networks.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/attias99a/attias99a.pdf",
        "supp": "",
        "pdf_size": 227354,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12419375960002715924&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "https://github.com/example-paper",
        "project": "https://example-project.com",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9599dc89e6",
        "title": "Hierarchical Mixtures-of-Experts for Generalized Linear Models: Some Results on Denseness and Consistency",
        "site": "https://proceedings.mlr.press/r2/jiang99a.html",
        "author": "Wenxin Jiang; Martin A. Tanner",
        "abstract": "We investigate a class of hierarchical mixtures-of-experts (HME) models where exponential family regression models with generalized linear mean functions of the form $\\psi(a+x^T b)$ are mixed. Here $\\psi(\\cdot)$ is the inverse link function. Suppose the true response $y$ follows an exponential family regression model with mean function belonging to a class of smooth functions of the form $\\psi(h(x))$ where $h \\in W_{2;K_0}^\\infty$ (a Sobolev class over $[0,1]^{s}$). It is shown that the HME mean functions can approximate the true mean function, at a rate of $O(m^{-2/s})$ in $L_p$ norm. Moreover, the HME probability density functions can approximate the true density, at a rate of $O(m^{-2/s})$ in Hellinger distance, and at a rate of $O(m^{-4/s})$ in Kullback-Leibler divergence. These rates can be achieved within the family of HME structures with a tree of binary splits, or within the family of structures with a single layer of experts. Here $s$ is the dimension of the predictor $x$. It is also shown that likelihood-based inference based on HME is consistent in recovering the truth, in the sense that as the sample size $n$ and the number of experts $m$ both increase, the mean square error of the estimated mean response goes to zero. Conditions for such results to hold are stated and discussed.",
        "bibtex": "@InProceedings{pmlr-vR2-jiang99a,\n  title = \t {Hierarchical Mixtures-of-Experts for Generalized Linear Models: Some Results on Denseness and Consistency},\n  author =       {Jiang, Wenxin and Tanner, Martin A.},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/jiang99a/jiang99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/jiang99a.html},\n  abstract = \t {We investigate a class of hierarchical mixtures-of-experts (HME) models where exponential family regression models with generalized linear mean functions of the form $\\psi(a+x^T b)$ are mixed. Here $\\psi(\\cdot)$ is the inverse link function. Suppose the true response $y$ follows an exponential family regression model with mean function belonging to a class of smooth functions of the form $\\psi(h(x))$ where $h \\in W_{2;K_0}^\\infty$ (a Sobolev class over $[0,1]^{s}$). It is shown that the HME mean functions can approximate the true mean function, at a rate of $O(m^{-2/s})$ in $L_p$ norm. Moreover, the HME probability density functions can approximate the true density, at a rate of $O(m^{-2/s})$ in Hellinger distance, and at a rate of $O(m^{-4/s})$ in Kullback-Leibler divergence. These rates can be achieved within the family of HME structures with a tree of binary splits, or within the family of structures with a single layer of experts. Here $s$ is the dimension of the predictor $x$. It is also shown that likelihood-based inference based on HME is consistent in recovering the truth, in the sense that as the sample size $n$ and the number of experts $m$ both increase, the mean square error of the estimated mean response goes to zero. Conditions for such results to hold are stated and discussed.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/jiang99a/jiang99a.pdf",
        "supp": "",
        "pdf_size": 272738,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17091252672380441424&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Statistics, Northwestern University; Department of Statistics, Northwestern University",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Northwestern University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.northwestern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6e41828bc7",
        "title": "Mean field inference in a general probabilistic setting",
        "site": "https://proceedings.mlr.press/r2/haft99a.html",
        "author": "Michael Haft; Reimar Hofmann; Volker Tresp",
        "abstract": "We present a systematic, model-independent formulation of  mean field theory (MFT) as an inference method in probabilistic models.  \"Model-independent\" means that we do not assume a particular type of dependency among the variables of a domain but instead work in a general probabilistic setting. In a Bayesian network, for example, you may use arbitrary tables to specify conditional dependencies and thus run MFT in any Bayesian network. Furthermore, the general mean field equations derived here shed a light on the essence of MFT. MFT can be interpreted as a",
        "bibtex": "@InProceedings{pmlr-vR2-haft99a,\n  title = \t {Mean field inference in a general probabilistic setting},\n  author =       {Haft, Michael and Hofmann, Reimar and Tresp, Volker},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/haft99a/haft99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/haft99a.html},\n  abstract = \t {We present a systematic, model-independent formulation of  mean field theory (MFT) as an inference method in probabilistic models.  \"Model-independent\" means that we do not assume a particular type of dependency among the variables of a domain but instead work in a general probabilistic setting. In a Bayesian network, for example, you may use arbitrary tables to specify conditional dependencies and thus run MFT in any Bayesian network. Furthermore, the general mean field equations derived here shed a light on the essence of MFT. MFT can be interpreted as a",
        "pdf": "http://proceedings.mlr.press/r2/haft99a/haft99a.pdf",
        "supp": "",
        "pdf_size": 8805312,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17643083079256689792&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4c3725f0c6",
        "title": "Model choice: A minimum posterior predictive loss approach",
        "site": "https://proceedings.mlr.press/r2/ghosh99a.html",
        "author": "Sujit Kumar Ghosh; Alan E. Gelfand",
        "abstract": "Model choice is a fundamental activity in the analysis of data sets, an activity which has become increasingly more important as computational advances enable the fitting of increasingly complex models. Such complexity typically arises through hierarchical structure which requires specification at each stage of probabilistic mechanisms, mean and dispersion forms, explanatory variables, etc. Nonnested hierarchical models introducing random effects may not be handled by classical methods. Bayesian approaches using predictive distributions can be used though the FORMAL solution, which includes Bayes factors as a special case, can be criticized. It seems natural to evaluate model performance by comparing what it predicts with what has been observed. Most classical criteria utilize such comparison. We propose a predictive criterion where the goal is good prediction of a replicate of the observed data but tempered by fidelity to the observed values. We obtain this criterion by minimizing posterior loss for a given model and then, for models under consideration, selecting the one which minimizes this criterion. For a version of log scoring loss we can do the minimization explicitly, obtaining an expression which can be interpreted as a penalized deviance criterion. We illustrate its performance with an application to a large data set involving residential property transactions.",
        "bibtex": "@InProceedings{pmlr-vR2-ghosh99a,\n  title = \t {Model choice: {A} minimum posterior predictive loss approach},\n  author =       {Ghosh, Sujit Kumar and Gelfand, Alan E.},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/ghosh99a/ghosh99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/ghosh99a.html},\n  abstract = \t {Model choice is a fundamental activity in the analysis of data sets, an activity which has become increasingly more important as computational advances enable the fitting of increasingly complex models. Such complexity typically arises through hierarchical structure which requires specification at each stage of probabilistic mechanisms, mean and dispersion forms, explanatory variables, etc. Nonnested hierarchical models introducing random effects may not be handled by classical methods. Bayesian approaches using predictive distributions can be used though the FORMAL solution, which includes Bayes factors as a special case, can be criticized. It seems natural to evaluate model performance by comparing what it predicts with what has been observed. Most classical criteria utilize such comparison. We propose a predictive criterion where the goal is good prediction of a replicate of the observed data but tempered by fidelity to the observed values. We obtain this criterion by minimizing posterior loss for a given model and then, for models under consideration, selecting the one which minimizes this criterion. For a version of log scoring loss we can do the minimization explicitly, obtaining an expression which can be interpreted as a penalized deviance criterion. We illustrate its performance with an application to a large data set involving residential property transactions.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/ghosh99a/ghosh99a.pdf",
        "supp": "",
        "pdf_size": 0,
        "gs_citation": 1014,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4943830660128098389&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "21197f15e5",
        "title": "Modeling decision tree performance with the power law",
        "site": "https://proceedings.mlr.press/r2/frey99a.html",
        "author": "Lewis J. Frey; Douglas H. Fisher",
        "abstract": "This paper discusses the use of a power law to predict decision tree performance. Power laws are fit to learning curves of decision trees trained on data sets from the UCI repository. The learning curves are generated by training C4.5 on different size training sets. The power law predicts diminishing returns in terms of error rate as training set size increase. By characterizing the learning curve with a power law, the error rate for a given size training set can be projected. This projection can be used in estimating the amount of data needed to achieve an acceptable error rate, and the cost effectiveness of further data collection.",
        "bibtex": "@InProceedings{pmlr-vR2-frey99a,\n  title = \t {Modeling decision tree performance with the power law},\n  author =       {Frey, Lewis J. and Fisher, Douglas H.},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/frey99a/frey99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/frey99a.html},\n  abstract = \t {This paper discusses the use of a power law to predict decision tree performance. Power laws are fit to learning curves of decision trees trained on data sets from the UCI repository. The learning curves are generated by training C4.5 on different size training sets. The power law predicts diminishing returns in terms of error rate as training set size increase. By characterizing the learning curve with a power law, the error rate for a given size training set can be projected. This projection can be used in estimating the amount of data needed to achieve an acceptable error rate, and the cost effectiveness of further data collection.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/frey99a/frey99a.pdf",
        "supp": "",
        "pdf_size": 466109,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13484746039068774229&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1bdd8037be",
        "title": "On the geometry of DAG models with hidden variables",
        "site": "https://proceedings.mlr.press/r2/geiger99a.html",
        "author": "Dan Geiger; David Heckerman; Henry King; Christopher Meek",
        "abstract": "We prove that many graphical models with hidden variables are not curved exponential families. This result, together with the fact that some graphical models are curved and not linear, implies that the hierarchy of graphical models, as linear, curved, and stratified, is non-collapsing; each level in the hierarchy is strictly contained in the larger levels. This result is discussed in the context of model selection of graphical models.",
        "bibtex": "@InProceedings{pmlr-vR2-geiger99a,\n  title = \t {On the geometry of {DAG} models with hidden variables},\n  author =       {Geiger, Dan and Heckerman, David and King, Henry and Meek, Christopher},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/geiger99a/geiger99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/geiger99a.html},\n  abstract = \t {We prove that many graphical models with hidden variables are not curved exponential families. This result, together with the fact that some graphical models are curved and not linear, implies that the hierarchy of graphical models, as linear, curved, and stratified, is non-collapsing; each level in the hierarchy is strictly contained in the larger levels. This result is discussed in the context of model selection of graphical models.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/geiger99a/geiger99a.pdf",
        "supp": "",
        "pdf_size": 366221,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2644542387223745583&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": "Microsoft Research, WA, USA; Computer Science Department, Technion, Israel; Math Department, University of Maryland, USA; Microsoft Research, WA, USA",
        "aff_domain": "microsoft.com;microsoft.com;math.umd.edu;microsoft.com",
        "email": "microsoft.com;microsoft.com;math.umd.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Microsoft;Technion;University of Maryland",
        "aff_unique_dep": "Research;Computer Science Department;Math Department",
        "aff_unique_url": "https://www.microsoft.com/en-us/research;https://www.technion.ac.il;https://www/umd.edu",
        "aff_unique_abbr": "MSR;Technion;UMD",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Redmond;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Israel"
    },
    {
        "id": "ed464e8d87",
        "title": "Pattern Discovery via Entropy Minimization",
        "site": "https://proceedings.mlr.press/r2/brand99a.html",
        "author": "Matthew Brand",
        "abstract": "We propose a framework for learning hidden-variable models by optimizing entropies, in which entropy minimization, posterior maximization, and free energy minimization are all equivalent. Solutions for the maximum",
        "bibtex": "@InProceedings{pmlr-vR2-brand99a,\n  title = \t {Pattern Discovery via Entropy Minimization},\n  author =       {Brand, Matthew},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {10--17},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/brand99a/brand99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/brand99a.html},\n  abstract = \t {We propose a framework for learning hidden-variable models by optimizing entropies, in which entropy minimization, posterior maximization, and free energy minimization are all equivalent. Solutions for the maximum",
        "pdf": "http://proceedings.mlr.press/r2/brand99a/brand99a.pdf",
        "supp": "",
        "pdf_size": 1343981,
        "gs_citation": 103,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4360176387168914454&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "b49e54117b",
        "title": "Probabilistic Kernel Regression Models",
        "site": "https://proceedings.mlr.press/r2/jaakkola99a.html",
        "author": "Tommi S. Jaakkola; David Haussler",
        "abstract": "We introduce a class of flexible conditional probability models and techniques for classification/regression problems. Many existing methods such as generalized linear models and support vector machines are subsumed under this class. The flexibility of this class of techniques comes from the use of kernel functions as in support vector machines, and the generality from dual formulations of standard regression models.",
        "bibtex": "@InProceedings{pmlr-vR2-jaakkola99a,\n  title = \t {Probabilistic kernel regression models},\n  author =       {Jaakkola, Tommi S. and Haussler, David},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/jaakkola99a/jaakkola99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/jaakkola99a.html},\n  abstract = \t {We introduce a class of flexible conditional probability models and techniques for classification/regression problems. Many existing methods such as generalized linear models and support vector machines are subsumed under this class. The flexibility of this class of techniques comes from the use of kernel functions as in support vector machines, and the generality from dual formulations of standard regression models.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/jaakkola99a/jaakkola99a.pdf",
        "supp": "",
        "pdf_size": 239195,
        "gs_citation": 354,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15977526603267048449&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9a8ba4d145",
        "title": "Process-oriented evaluation: The next step",
        "site": "https://proceedings.mlr.press/r2/domingos99a.html",
        "author": "Pedro M. Domingos",
        "abstract": "Methods to avoid overfitting fall into two broad categories: data-oriented (using separate data for validation) and representation-oriented (penalizing complexity in the model). Both have limitations that are hard to overcome. We argue that fully adequate model evaluation is only possible if the search process by which models are obtained is also taken into account. To this end, we recently proposed a method for process-oriented evaluation (POE), and successfully applied it to rule induction (Domingos, 1998). However, for the sake of simplicity this treatment made two rather artificial assumptions. In this paper the assumptions are removed, and a simple formula for model evaluation is obtained. Empirical trials show the new, better-founded form of POE to be as accurate as the previous one, while further reducing theory sizes.",
        "bibtex": "@InProceedings{pmlr-vR2-domingos99a,\n  title = \t {Process-oriented evaluation: The next step},\n  author =       {Domingos, Pedro M.},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/domingos99a/domingos99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/domingos99a.html},\n  abstract = \t {Methods to avoid overfitting fall into two broad categories: data-oriented (using separate data for validation) and representation-oriented (penalizing complexity in the model). Both have limitations that are hard to overcome. We argue that fully adequate model evaluation is only possible if the search process by which models are obtained is also taken into account. To this end, we recently proposed a method for process-oriented evaluation (POE), and successfully applied it to rule induction (Domingos, 1998). However, for the sake of simplicity this treatment made two rather artificial assumptions. In this paper the assumptions are removed, and a simple formula for model evaluation is obtained. Empirical trials show the new, better-founded form of POE to be as accurate as the previous one, while further reducing theory sizes.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/domingos99a/domingos99a.pdf",
        "supp": "",
        "pdf_size": 182330,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=896642040979897461&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Artificial Intelligence Group, Instituto Superior Tecnico, Lisbon, Portugal",
        "aff_domain": "gia.ist.utl.pt",
        "email": "gia.ist.utl.pt",
        "github": "",
        "project": "http://www.gia.ist.utl.pt/",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Instituto Superior T\u00e9cnico",
        "aff_unique_dep": "Artificial Intelligence Group",
        "aff_unique_url": "https://www.ist.utl.pt",
        "aff_unique_abbr": "IST",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Lisbon",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Portugal"
    },
    {
        "id": "d9c2f11ca2",
        "title": "Stochastic Local Search for Bayesian Network",
        "site": "https://proceedings.mlr.press/r2/kask99a.html",
        "author": "Kalev Kask; Rina Dechter",
        "abstract": "The paper evaluates empirically the suitability of Stochastic Local Search algorithms (SLS) for finding most probable explanations in Bayesian networks. SLS algorithms (e.g., GSAT, WSAT [16]) have recently proven to be highly effective in solving complex constraint-satisfaction and satisfiability problems which cannot be solved by traditional search schemes. Our experiments investigate the applicability of this scheme to probabilistic optimization problems. Specifically, we show that algorithms combining hill-climbing steps with stochastic steps (guided by the network\u2019s probability distribution) called",
        "bibtex": "@InProceedings{pmlr-vR2-kask99a,\n  title = \t {Stochastic Local Search for Bayesian Network},\n  author =       {Kask, Kalev and Dechter, Rina},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/kask99a/kask99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/kask99a.html},\n  abstract = \t {The paper evaluates empirically the suitability of Stochastic Local Search algorithms (SLS) for finding most probable explanations in Bayesian networks. SLS algorithms (e.g., GSAT, WSAT [16]) have recently proven to be highly effective in solving complex constraint-satisfaction and satisfiability problems which cannot be solved by traditional search schemes. Our experiments investigate the applicability of this scheme to probabilistic optimization problems. Specifically, we show that algorithms combining hill-climbing steps with stochastic steps (guided by the network\u2019s probability distribution) called",
        "pdf": "http://proceedings.mlr.press/r2/kask99a/kask99a.pdf",
        "supp": "",
        "pdf_size": 231190,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8428176709067069887&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/alice-bob-carol-dave/SLSS-inference",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "922a7d71e2",
        "title": "Tractable Structure Search in the Presence of Latent Variables",
        "site": "https://proceedings.mlr.press/r2/richardson99a.html",
        "author": "Thomas Richardson; Heiko Bailer; Moulinath Banarjees",
        "abstract": "The problem of learning the structure of a DAG model in the presence of latent variables presents many formidable challenges. In particular there are an infinite number of latent variable models to consider, and these models possess features which make them hard to work with. We describe a class of graphical models which can represent the conditional independence structure induced by a latent variable model over the observed margin. We give a parametrization of the set of Gaussian distributions with conditional independence structure given by a MAG model. The models are illustrated via a simple example. Different estimation techniques are discussed in the context of Zellner\u2019s Seemingly Unrelated Regression (SUR) models.",
        "bibtex": "@InProceedings{pmlr-vR2-richardson99a,\n  title = \t {Tractable Structure Search in the Presence of Latent Variables},\n  author =       {Richardson, Thomas and Bailer, Heiko and Banarjees, Moulinath},\n  booktitle = \t {Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics},\n  year = \t {1999},\n  editor = \t {Heckerman, David and Whittaker, Joe},\n  volume = \t {R2},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {03--06 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r2/richardson99a/richardson99a.pdf},\n  url = \t {https://proceedings.mlr.press/r2/richardson99a.html},\n  abstract = \t {The problem of learning the structure of a DAG model in the presence of latent variables presents many formidable challenges. In particular there are an infinite number of latent variable models to consider, and these models possess features which make them hard to work with. We describe a class of graphical models which can represent the conditional independence structure induced by a latent variable model over the observed margin. We give a parametrization of the set of Gaussian distributions with conditional independence structure given by a MAG model. The models are illustrated via a simple example. Different estimation techniques are discussed in the context of Zellner\u2019s Seemingly Unrelated Regression (SUR) models.},\n  note =         {Reissued by PMLR on 20 August 2020.}\n}",
        "pdf": "http://proceedings.mlr.press/r2/richardson99a/richardson99a.pdf",
        "supp": "",
        "pdf_size": 196187,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16734266131171936486&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    }
]