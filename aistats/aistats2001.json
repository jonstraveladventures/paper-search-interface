[
    {
        "id": "7f55c567c7",
        "title": "A Kernel Approach for Vector Quantization with Guaranteed Distortion Bounds",
        "site": "https://proceedings.mlr.press/r3/tipping01a.html",
        "author": "Michael E. Tipping; Bernhard Sch\u00f6lkopf",
        "abstract": "We propose a kernel method for vector quantization and clustering. Our approach allows a priori specification of the maximally allowed distortion, and it automatically finds a sufficient representative subset of the data to act as codebook vectors (or cluster centres). It does not find the minimal number of such vectors, which would amount to a combinatorial problem; however, we find a \u2019good\u2019 quantization through linear programming.",
        "bibtex": "@InProceedings{pmlr-vR3-tipping01a,\n  title = \t {A Kernel Approach for Vector Quantization with Guaranteed Distortion Bounds},\n  author =       {Tipping, Michael E. and Sch{\\\"{o}}lkopf, Bernhard},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {298--303},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/tipping01a/tipping01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/tipping01a.html},\n  abstract = \t {We propose a kernel method for vector quantization and clustering. Our approach allows a priori specification of the maximally allowed distortion, and it automatically finds a sufficient representative subset of the data to act as codebook vectors (or cluster centres). It does not find the minimal number of such vectors, which would amount to a combinatorial problem; however, we find a \u2019good\u2019 quantization through linear programming.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/tipping01a/tipping01a.pdf",
        "supp": "",
        "pdf_size": 576969,
        "gs_citation": 39,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10243367413149601994&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "https://github.com/nlp-deep-learning",
        "project": "https://nlp-deep-learning-project.org",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d3c526a2c2",
        "title": "A Non-Parametric EM-Style Algorithm for Imputing Missing Values",
        "site": "https://proceedings.mlr.press/r3/caruana01a.html",
        "author": "Rich Caruana",
        "abstract": "We present an iterative non-parametric algorithm for imputing missing values. The algorithm is similar to EM except that it uses non-parametric models such as k-nearest neighbor or kernel regression instead of the parametric models used with EM. An interesting feature of the algorithm is that the E and M steps collapse into a single step because the data being filled in is the model - updating the filled-in values updates the model at the same time. The main advantages of this approach compared to parametric EM methods are that: 1) it is more efficient for moderate size data sets, and 2) it is less susceptible to errors that parametric methods make when the parametric models do not fit the data well. The robustness to model failure makes the non-parametric method more accurate when models of the data are not known apriori and cannot be determined reliably. We evaluate the method using a real medical data set that has many missing values.",
        "bibtex": "@InProceedings{pmlr-vR3-caruana01a,\n  title = \t {A Non-Parametric EM-Style Algorithm for Imputing Missing Values},\n  author =       {Caruana, Rich},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {35--40},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/caruana01a/caruana01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/caruana01a.html},\n  abstract = \t {We present an iterative non-parametric algorithm for imputing missing values. The algorithm is similar to EM except that it uses non-parametric models such as k-nearest neighbor or kernel regression instead of the parametric models used with EM. An interesting feature of the algorithm is that the E and M steps collapse into a single step because the data being filled in is the model - updating the filled-in values updates the model at the same time. The main advantages of this approach compared to parametric EM methods are that: 1) it is more efficient for moderate size data sets, and 2) it is less susceptible to errors that parametric methods make when the parametric models do not fit the data well. The robustness to model failure makes the non-parametric method more accurate when models of the data are not known apriori and cannot be determined reliably. We evaluate the method using a real medical data set that has many missing values.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/caruana01a/caruana01a.pdf",
        "supp": "",
        "pdf_size": 128741,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13516320111362105515&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 3,
        "aff": "Center for Automated Learning and Discovery, Carnegie-Mellon University",
        "aff_domain": "cs.cmu.edu",
        "email": "cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Center for Automated Learning and Discovery",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "af36022284",
        "title": "A Random Walks View of Spectral Segmentation",
        "site": "https://proceedings.mlr.press/r3/meila01a.html",
        "author": "Marina Meil\u0103; Jianbo Shi",
        "abstract": "We present a new view of clustering and segmentation by pairwise similarities. We interpret the similarities as edge flows in a Markov random walk and study the eigenvalues and eigenvectors of the walk\u2019s transition matrix. This view shows that spectral methods for clustering and segmentation have a probabilistic foundation. We prove that the Normalized Cut method arises naturally from our framework and we provide a complete characterization of the cases when the Normalized Cut algorithm is exact. Then we discuss other spectral segmentation and clustering methods showing that several of them are essentially the same as NCut.",
        "bibtex": "@InProceedings{pmlr-vR3-meila01a,\n  title = \t {A Random Walks View of Spectral Segmentation},\n  author =       {Meil\\u{a}, Marina and Shi, Jianbo},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {203--208},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/meila01a/meila01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/meila01a.html},\n  abstract = \t {We present a new view of clustering and segmentation by pairwise similarities. We interpret the similarities as edge flows in a Markov random walk and study the eigenvalues and eigenvectors of the walk\u2019s transition matrix. This view shows that spectral methods for clustering and segmentation have a probabilistic foundation. We prove that the Normalized Cut method arises naturally from our framework and we provide a complete characterization of the cases when the Normalized Cut algorithm is exact. Then we discuss other spectral segmentation and clustering methods showing that several of them are essentially the same as NCut.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/meila01a/meila01a.pdf",
        "supp": "",
        "pdf_size": 212612,
        "gs_citation": 933,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17518377671244049072&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f787e4d73c",
        "title": "A Simulation Study of Three Related Causal Data Mining Algorithms",
        "site": "https://proceedings.mlr.press/r3/mani01a.html",
        "author": "Subramani Mani; Gregory F. Cooper",
        "abstract": "In all scientific domains causality plays a significant role. This study focused on evaluating and refining efficient algorithms to learn causal relationships from observational data. Evaluation of learned causal output is difficult, due to lack of a gold standard in real-world domains. Therefore, we used simulated data from a known causal network in a medical domain-the Alarm network. For causal discovery we used three variants of the Local Causal Discovery (LCD) algorithms, that are referred to as LCDa, LCDb and LCDc. These algorithms use the framework of causal Bayesian Networks to represent causal relationships among model variables. LCDa, LCDb and LCDe take as input a dataset and a partial node ordering, and output purported causes of the form variable $Y$ causally influences variable $Z$. Using the simulated Alarm dataset as input, LCDa had a false positive rate of $0.09$, LCDb $0.08$ and LCDc 0.04. All the algorithms had a true positive rate of about 0.27 . Most of the false positives occurred when a causal relationship was confounded. LCDc output as causal only those causally confounded pairs that had very weak confounding. We identify and discuss the causally confounded relationships that often seem to induce false positive results.",
        "bibtex": "@InProceedings{pmlr-vR3-mani01a,\n  title = \t {A Simulation Study of Three Related Causal Data Mining Algorithms},\n  author =       {Mani, Subramani and Cooper, Gregory F.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {184--191},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/mani01a/mani01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/mani01a.html},\n  abstract = \t {In all scientific domains causality plays a significant role. This study focused on evaluating and refining efficient algorithms to learn causal relationships from observational data. Evaluation of learned causal output is difficult, due to lack of a gold standard in real-world domains. Therefore, we used simulated data from a known causal network in a medical domain-the Alarm network. For causal discovery we used three variants of the Local Causal Discovery (LCD) algorithms, that are referred to as LCDa, LCDb and LCDc. These algorithms use the framework of causal Bayesian Networks to represent causal relationships among model variables. LCDa, LCDb and LCDe take as input a dataset and a partial node ordering, and output purported causes of the form variable $Y$ causally influences variable $Z$. Using the simulated Alarm dataset as input, LCDa had a false positive rate of $0.09$, LCDb $0.08$ and LCDc 0.04. All the algorithms had a true positive rate of about 0.27 . Most of the false positives occurred when a causal relationship was confounded. LCDc output as causal only those causally confounded pairs that had very weak confounding. We identify and discuss the causally confounded relationships that often seem to induce false positive results.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/mani01a/mani01a.pdf",
        "supp": "",
        "pdf_size": 406869,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17424635165362234666&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Center for Biomedical Informatics and Intelligent Systems Program, University of Pittsburgh PA; Center for Biomedical Informatics and Intelligent Systems Program, University of Pittsburgh PA",
        "aff_domain": "cbmi.upmc.edu;cbmi.upmc.edu",
        "email": "cbmi.upmc.edu;cbmi.upmc.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pittsburgh",
        "aff_unique_dep": "Center for Biomedical Informatics and Intelligent Systems Program",
        "aff_unique_url": "https://www.pitt.edu",
        "aff_unique_abbr": "Pitt",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "4e4e4aee76",
        "title": "An Anytime Algorithm for Causal Inference",
        "site": "https://proceedings.mlr.press/r3/spirtes01a.html",
        "author": "Peter Spirtes",
        "abstract": "The Fast Casual Inference (FCI) algorithm searches for features common to observationally equivalent sets of causal directed acyclic graphs. It is correct in the large sample limit with probability one even if there is a possibility of hidden variables and selection bias. In the worst case, the number of conditional independence tests performed by the algorithm grows exponentially with the number of variables in the data set. This affects both the speed of the algorithm and the accuracy of the algorithm on small samples, because tests of independence conditional on large numbers of variables have very low power. In this paper, I prove that the FCI algorithm can be interrupted at any stage and asked for output. The output from the interrupted algorithm is still correct with probability one in the large sample limit, although possibly less informative (in the sense that it answers \"Can\u2019t tell\" for a larger number of questions) than if the FCI algorithm had been allowed to continue uninterrupted.",
        "bibtex": "@InProceedings{pmlr-vR3-spirtes01a,\n  title = \t {An Anytime Algorithm for Causal Inference},\n  author =       {Spirtes, Peter},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {278--285},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/spirtes01a/spirtes01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/spirtes01a.html},\n  abstract = \t {The Fast Casual Inference (FCI) algorithm searches for features common to observationally equivalent sets of causal directed acyclic graphs. It is correct in the large sample limit with probability one even if there is a possibility of hidden variables and selection bias. In the worst case, the number of conditional independence tests performed by the algorithm grows exponentially with the number of variables in the data set. This affects both the speed of the algorithm and the accuracy of the algorithm on small samples, because tests of independence conditional on large numbers of variables have very low power. In this paper, I prove that the FCI algorithm can be interrupted at any stage and asked for output. The output from the interrupted algorithm is still correct with probability one in the large sample limit, although possibly less informative (in the sense that it answers \"Can\u2019t tell\" for a larger number of questions) than if the FCI algorithm had been allowed to continue uninterrupted.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/spirtes01a/spirtes01a.pdf",
        "supp": "",
        "pdf_size": 134098,
        "gs_citation": 200,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16832129407013346668&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Philosophy, Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu",
        "email": "andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Philosophy",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "115477f460",
        "title": "An improved training algorithm for kernel Fisher discriminants",
        "site": "https://proceedings.mlr.press/r3/mika01a.html",
        "author": "Sebastian Mika; Alexander J. Smola; Bernhard Sch\u00f6lkopf",
        "abstract": "We present a fast training algorithm for the kernel Fisher discriminant classifier. It uses a greedy approximation technique and has an empirical scaling behavior which improves upon the state of the art by more than an order of magnitude, thus rendering the kernel Fisher algorithm a viable option also for large datasets.",
        "bibtex": "@InProceedings{pmlr-vR3-mika01a,\n  title = \t {An improved training algorithm for kernel Fisher discriminants},\n  author =       {Mika, Sebastian and Smola, Alexander J. and Sch{\\\"{o}}lkopf, Bernhard},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {209--215},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/mika01a/mika01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/mika01a.html},\n  abstract = \t {We present a fast training algorithm for the kernel Fisher discriminant classifier. It uses a greedy approximation technique and has an empirical scaling behavior which improves upon the state of the art by more than an order of magnitude, thus rendering the kernel Fisher algorithm a viable option also for large datasets.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/mika01a/mika01a.pdf",
        "supp": "",
        "pdf_size": 222581,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14231881277107453673&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "GMD FIRST.IDA; Australian National University; Microsoft Research",
        "aff_domain": "first.gmd.de;anu.edu.au;scientist.com",
        "email": "first.gmd.de;anu.edu.au;scientist.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "GMD FIRST.IDA;Australian National University;Microsoft",
        "aff_unique_dep": ";;Microsoft Research",
        "aff_unique_url": ";https://www.anu.edu.au;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": ";ANU;MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2",
        "aff_country_unique": ";Australia;United States"
    },
    {
        "id": "b2385e76de",
        "title": "Another look at sensitivity of Bayesian networks to imprecise probabilities",
        "site": "https://proceedings.mlr.press/r3/kipersztok01a.html",
        "author": "Oscar Kipersztok; Haiqin Wang",
        "abstract": "Empirical study of sensitivity analysis on a Bayesian network examines the effects of varying the network\u2019s probability parameters on the posterior probabilities of the true hypothesis. One appealing approach to modeling the uncertainty of the probability parameters is to add normal noise to the log-odds of the nominal probabilities. However, the paper argues that differences in sensitivities found on true hypothesis may only be valid in the range of standard deviations where the log-odds normal distribution is unimodal. The paper also shows that using average posterior probabilities as criterion to measure the sensitivity may not be the most indicative, especially when the distribution is very asymmetric as is the case at nominal values close to zero or one. It is proposed, instead, to use the partial ordering of the most probable causes of diagnosis, measured by a suitable lower confidence bound. The paper also presents the preliminary results of our sensitivity analysis experiments with three Bayesian networks built for diagnosis of airplane systems. Our results show that some networks are more sensitive to imprecision in probabilities than previously believed.",
        "bibtex": "@InProceedings{pmlr-vR3-kipersztok01a,\n  title = \t {Another look at sensitivity of Bayesian networks to imprecise probabilities},\n  author =       {Kipersztok, Oscar and Wang, Haiqin},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {149--155},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/kipersztok01a/kipersztok01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/kipersztok01a.html},\n  abstract = \t {Empirical study of sensitivity analysis on a Bayesian network examines the effects of varying the network\u2019s probability parameters on the posterior probabilities of the true hypothesis. One appealing approach to modeling the uncertainty of the probability parameters is to add normal noise to the log-odds of the nominal probabilities. However, the paper argues that differences in sensitivities found on true hypothesis may only be valid in the range of standard deviations where the log-odds normal distribution is unimodal. The paper also shows that using average posterior probabilities as criterion to measure the sensitivity may not be the most indicative, especially when the distribution is very asymmetric as is the case at nominal values close to zero or one. It is proposed, instead, to use the partial ordering of the most probable causes of diagnosis, measured by a suitable lower confidence bound. The paper also presents the preliminary results of our sensitivity analysis experiments with three Bayesian networks built for diagnosis of airplane systems. Our results show that some networks are more sensitive to imprecision in probabilities than previously believed.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/kipersztok01a/kipersztok01a.pdf",
        "supp": "",
        "pdf_size": 294922,
        "gs_citation": 33,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13984815075667978181&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Mathematics and Computing Technology, Phantom Works, The Boeing Company; Decision Systems Laboratory, Intelligent Systems Program, University of Pittsburgh",
        "aff_domain": "boeing.com;pitt.edu",
        "email": "boeing.com;pitt.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Boeing Company;University of Pittsburgh",
        "aff_unique_dep": "Mathematics and Computing Technology;Decision Systems Laboratory, Intelligent Systems Program",
        "aff_unique_url": "https://www.boeing.com;https://www.pitt.edu",
        "aff_unique_abbr": "Boeing;Pitt",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "876eaeb844",
        "title": "Are they really neighbors? A statistical analysis of the SOM algorithm output",
        "site": "https://proceedings.mlr.press/r3/bodt01a.html",
        "author": "Eric de Bodt; Marie Cottrell; Michel Verleysen",
        "abstract": "One of the attractive features of Self-Organizing Maps (SOM) is the so-called \"topological preservation property\": observations that are close to each other in the input space (at least locally) remain close to each other in the SOM. In this work, we propose the use of a bootstrap scheme to construct a statistical significance test of the observed proximity among individuals in the SOM.",
        "bibtex": "@InProceedings{pmlr-vR3-bodt01a,\n  title = \t {Are they really neighbors? {A} statistical analysis of the {SOM} algorithm output},\n  author =       {de Bodt, Eric and Cottrell, Marie and Verleysen, Michel},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {87--92},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/bodt01a/bodt01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/bodt01a.html},\n  abstract = \t {One of the attractive features of Self-Organizing Maps (SOM) is the so-called \"topological preservation property\": observations that are close to each other in the input space (at least locally) remain close to each other in the SOM. In this work, we propose the use of a bootstrap scheme to construct a statistical significance test of the observed proximity among individuals in the SOM.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/bodt01a/bodt01a.pdf",
        "supp": "",
        "pdf_size": 589286,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10906285083256858071&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "da79223596",
        "title": "Bagging and the Bayesian Bootstrap",
        "site": "https://proceedings.mlr.press/r3/clyde01a.html",
        "author": "Merlise Clyde; Herbert Lee",
        "abstract": "Bagging is a method of obtaining more robust predictions when the model class under consideration is unstable with respect to the data, i.e., small changes in the data can cause the predicted values to change significantly. In this paper, we introduce a Bayesian version of bagging based on the Bayesian bootstrap. The Bayesian bootstrap resolves a theoretical problem with ordinary bagging and often results in more efficient estimators. We show how model averaging can be combined within the Bayesian bootstrap and illustrate the procedure with several examples.",
        "bibtex": "@InProceedings{pmlr-vR3-clyde01a,\n  title = \t {Bagging and the Bayesian Bootstrap},\n  author =       {Clyde, Merlise and Lee, Herbert},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {57--62},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/clyde01a/clyde01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/clyde01a.html},\n  abstract = \t {Bagging is a method of obtaining more robust predictions when the model class under consideration is unstable with respect to the data, i.e., small changes in the data can cause the predicted values to change significantly. In this paper, we introduce a Bayesian version of bagging based on the Bayesian bootstrap. The Bayesian bootstrap resolves a theoretical problem with ordinary bagging and often results in more efficient estimators. We show how model averaging can be combined within the Bayesian bootstrap and illustrate the procedure with several examples.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/clyde01a/clyde01a.pdf",
        "supp": "",
        "pdf_size": 104759,
        "gs_citation": 65,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3672010222944171495&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Institute of Statistics & Decision Sciences, Duke University; Institute of Statistics & Decision Sciences, Duke University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Institute of Statistics & Decision Sciences",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "92be3f9fbc",
        "title": "Bayesian Support Vector Regression",
        "site": "https://proceedings.mlr.press/r3/law01a.html",
        "author": "Martin H. C. Law; James Tin-Yau Kwok",
        "abstract": "We show that the Bayesian evidence framework can be applied to both $\\epsilon$-support vector regression ($\\epsilon$-SVR) and $\\nu$-support vector regression ($\\nu$-SVR) algorithms. Standard SVR training can be regarded as performing level one inference of the evidence framework, while levels two and three allow automatic adjustments of the regularization and kernel parameters respectively, without the need of a validation set.",
        "bibtex": "@InProceedings{pmlr-vR3-law01a,\n  title = \t {Bayesian Support Vector Regression},\n  author =       {Law, Martin H. C. and Kwok, James Tin-Yau},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {162--167},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/law01a/law01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/law01a.html},\n  abstract = \t {We show that the Bayesian evidence framework can be applied to both $\\epsilon$-support vector regression ($\\epsilon$-SVR) and $\\nu$-support vector regression ($\\nu$-SVR) algorithms. Standard SVR training can be regarded as performing level one inference of the evidence framework, while levels two and three allow automatic adjustments of the regularization and kernel parameters respectively, without the need of a validation set.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/law01a/law01a.pdf",
        "supp": "",
        "pdf_size": 200264,
        "gs_citation": 90,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9628464172108408698&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "Department of Computer Science, Hong Kong University of Science and Technology; Department of Computer Science, Hong Kong University of Science and Technology",
        "aff_domain": "cs.ust.hk;cs.ust.hk",
        "email": "cs.ust.hk;cs.ust.hk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "7cd6715d8e",
        "title": "Can the Computer Learn to Play Music Expressively?",
        "site": "https://proceedings.mlr.press/r3/raphael01a.html",
        "author": "Christopher Raphael",
        "abstract": "A computer system is described that provides a real-time musical accompaniment for a live soloist in a piece of non-improvised music. A Bayesian belief network is developed that represents the joint distribution on the times at which the solo and accompaniment notes are played as well as many hidden variables. The network models several important sources of information including the information contained in the score and the rhythmic interpretations of the soloist and accompaniment which are learned from examples. The network is used to provide a computationally efficient decision-making engine that utilizes all available information while producing a flexible and musical accompaniment.",
        "bibtex": "@InProceedings{pmlr-vR3-raphael01a,\n  title = \t {Can the Computer Learn to Play Music Expressively?},\n  author =       {Raphael, Christopher},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {251--258},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/raphael01a/raphael01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/raphael01a.html},\n  abstract = \t {A computer system is described that provides a real-time musical accompaniment for a live soloist in a piece of non-improvised music. A Bayesian belief network is developed that represents the joint distribution on the times at which the solo and accompaniment notes are played as well as many hidden variables. The network models several important sources of information including the information contained in the score and the rhythmic interpretations of the soloist and accompaniment which are learned from examples. The network is used to provide a computationally efficient decision-making engine that utilizes all available information while producing a flexible and musical accompaniment.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/raphael01a/raphael01a.pdf",
        "supp": "",
        "pdf_size": 191524,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7412776652690840588&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Mathematics and Statistics, University of Massachusetts at Amherst, Amherst, MA 01003-4515",
        "aff_domain": "math.umass.edu",
        "email": "math.umass.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Massachusetts Amherst",
        "aff_unique_dep": "Department of Mathematics and Statistics",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass Amherst",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f1516ec72c",
        "title": "Comparing Prequential Model Selection Criteria in Supervised Learning of Mixture Models",
        "site": "https://proceedings.mlr.press/r3/kontkanen01a.html",
        "author": "Petri Kontkanen; Petri Myllym\u00e4ki; Henry Tirri",
        "abstract": "In this paper we study prequential model selection criteria in supervised learning domains. The main problem with this approach is the fact that the criterion is sensitive to the ordering the data is processed with. We discuss several approaches for addressing the ordering problem, and compare empirically their performance in real-world supervised model selection tasks. The empirical results demonstrate that with the prequential approach it is quite easy to find predictive models that are significantly more accurate classifiers than the models found by the standard unsupervised marginal likelihood criterion. The results also suggest that averaging over random orderings may be a more sensible strategy for solving the ordering problem than trying to find the ordering optimizing the prequential model selection criterion.",
        "bibtex": "@InProceedings{pmlr-vR3-kontkanen01a,\n  title = \t {Comparing Prequential Model Selection Criteria in Supervised Learning of Mixture Models},\n  author =       {Kontkanen, Petri and Myllym{\\\"{a}}ki, Petri and Tirri, Henry},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {156--161},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/kontkanen01a/kontkanen01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/kontkanen01a.html},\n  abstract = \t {In this paper we study prequential model selection criteria in supervised learning domains. The main problem with this approach is the fact that the criterion is sensitive to the ordering the data is processed with. We discuss several approaches for addressing the ordering problem, and compare empirically their performance in real-world supervised model selection tasks. The empirical results demonstrate that with the prequential approach it is quite easy to find predictive models that are significantly more accurate classifiers than the models found by the standard unsupervised marginal likelihood criterion. The results also suggest that averaging over random orderings may be a more sensible strategy for solving the ordering problem than trying to find the ordering optimizing the prequential model selection criterion.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/kontkanen01a/kontkanen01a.pdf",
        "supp": "",
        "pdf_size": 148937,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6765766833270883236&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "http://www.cs.Helsinki.FI/research/cosco/",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "52dc3974db",
        "title": "Discriminant Analysis on Dissimilarity Data : a New Fast Gaussian like Algorithm",
        "site": "https://proceedings.mlr.press/r3/guerin-dugue01a.html",
        "author": "Anne Gu\u00e9rin-Dugu\u00e9; Gilles Celeux",
        "abstract": "Classifying objects according to their proximity is the fundamental task of pattern recognition and arises as a classification problem or discriminant analysis in experimental sciences. Here we consider a particular point of view on discriminant analysis from a dissimilarity data table. We develop a new approach, inspired from the Gaussian model in discriminant analysis, which defines a set a decision rules from simple statistics on the dissimilarity matrix between observations. This matrix can be only sparse dealing with huge databases. Numerical experiments on artificial and real data (proteins classification) show interesting behaviour compared to a $K$NN classifier, (i) equivalent error rate, (ii) dramatically lower CPU times and (iii) more robustness with sparse dissimilarity structure up to $40 %$ of actual dissimilarity measures.",
        "bibtex": "@InProceedings{pmlr-vR3-guerin-dugue01a,\n  title = \t {Discriminant Analysis on Dissimilarity Data : a New Fast Gaussian like Algorithm},\n  author =       {Gu{\\'{e}}rin{-}Dugu{\\'{e}}, Anne and Celeux, Gilles},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {117--122},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/guerin-dugue01a/guerin-dugue01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/guerin-dugue01a.html},\n  abstract = \t {Classifying objects according to their proximity is the fundamental task of pattern recognition and arises as a classification problem or discriminant analysis in experimental sciences. Here we consider a particular point of view on discriminant analysis from a dissimilarity data table. We develop a new approach, inspired from the Gaussian model in discriminant analysis, which defines a set a decision rules from simple statistics on the dissimilarity matrix between observations. This matrix can be only sparse dealing with huge databases. Numerical experiments on artificial and real data (proteins classification) show interesting behaviour compared to a $K$NN classifier, (i) equivalent error rate, (ii) dramatically lower CPU times and (iii) more robustness with sparse dissimilarity structure up to $40 %$ of actual dissimilarity measures.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/guerin-dugue01a/guerin-dugue01a.pdf",
        "supp": "",
        "pdf_size": 219443,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16280937435887396328&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "50ad985b7d",
        "title": "Dual perturb and combine algorithm",
        "site": "https://proceedings.mlr.press/r3/geurts01a.html",
        "author": "Pierre Geurts",
        "abstract": "In this paper, a dual perturb and combine algorithm is proposed which consists in producing the perturbed predictions at the prediction stage using only one model. To this end, the attribute vector of a test case is perturbed several times by an additive random noise, the model is applied to each of these perturbed vectors and the resulting predictions are aggregated. An analytical version of this algorithm is described in the context of decision tree induction. From experiments on several datasets, it appears that this simple algorithm yields significant improvements on several problems, sometimes comparable to those obtained with bagging. When combined with decision tree bagging, this algorithm also improves accuracy in many problems.",
        "bibtex": "@InProceedings{pmlr-vR3-geurts01a,\n  title = \t {Dual perturb and combine algorithm},\n  author =       {Geurts, Pierre},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {106--111},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/geurts01a/geurts01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/geurts01a.html},\n  abstract = \t {In this paper, a dual perturb and combine algorithm is proposed which consists in producing the perturbed predictions at the prediction stage using only one model. To this end, the attribute vector of a test case is perturbed several times by an additive random noise, the model is applied to each of these perturbed vectors and the resulting predictions are aggregated. An analytical version of this algorithm is described in the context of decision tree induction. From experiments on several datasets, it appears that this simple algorithm yields significant improvements on several problems, sometimes comparable to those obtained with bagging. When combined with decision tree bagging, this algorithm also improves accuracy in many problems.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/geurts01a/geurts01a.pdf",
        "supp": "",
        "pdf_size": 183796,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=236949180389338634&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "db6d033bf6",
        "title": "Dynamic Positional Trees for Structural Image Analysis",
        "site": "https://proceedings.mlr.press/r3/storkey01a.html",
        "author": "Amos J. Storkey; Christopher K. I. Williams",
        "abstract": "Dynamic positional trees are a significant extension of dynamic trees, incorporating movable nodes. This addition makes sequence tracking viable within the model, but requires a new formulation to incorporate the prior over positions. The model is implemented using a structured variational procedure, and is illustrated on synthetic raytraced images and image sequences.",
        "bibtex": "@InProceedings{pmlr-vR3-storkey01a,\n  title = \t {Dynamic Positional Trees for Structural Image Analysis},\n  author =       {Storkey, Amos J. and Williams, Christopher K. I.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {286--292},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/storkey01a/storkey01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/storkey01a.html},\n  abstract = \t {Dynamic positional trees are a significant extension of dynamic trees, incorporating movable nodes. This addition makes sequence tracking viable within the model, but requires a new formulation to incorporate the prior over positions. The model is implemented using a structured variational procedure, and is illustrated on synthetic raytraced images and image sequences.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/storkey01a/storkey01a.pdf",
        "supp": "",
        "pdf_size": 270633,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2133839517903644029&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c48a617a41",
        "title": "Finding a path is harder than finding a tree",
        "site": "https://proceedings.mlr.press/r3/meek01a.html",
        "author": "Christopher Meek",
        "abstract": "This note shows that the problem of learning an optimal chain graphical model from data is NP-hard for the Bayesian, maximum likelihood, and minimum description length approaches. This hardness result holds despite the fact that the problem is a restriction of the polynomially solvable problem of finding the optimal tree graphical model.",
        "bibtex": "@InProceedings{pmlr-vR3-meek01a,\n  title = \t {Finding a path is harder than finding a tree},\n  author =       {Meek, Christopher},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {192--195},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/meek01a/meek01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/meek01a.html},\n  abstract = \t {This note shows that the problem of learning an optimal chain graphical model from data is NP-hard for the Bayesian, maximum likelihood, and minimum description length approaches. This hardness result holds despite the fact that the problem is a restriction of the polynomially solvable problem of finding the optimal tree graphical model.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/meek01a/meek01a.pdf",
        "supp": "",
        "pdf_size": 131034,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10881600552706546333&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "23037c645f",
        "title": "Geographical Clustering of Cancer Incidence by Means of Bayesian Networks and Conditional Gaussian Networks",
        "site": "https://proceedings.mlr.press/r3/pena01a.html",
        "author": "Jos\u00e9 M. Pe\u00f1a; I. Izarzugaza; Jos\u00e9 Antonio Lozano; E. Aldasoro; Pedro Larra\u00f1aga",
        "abstract": "With the aim of improving knowledge on the geographical distribution and characterization of malignant tumors in the Autonomous Community of the Basque Country (Spain), age-standardized cancer incidence rates of the 6 most frequent cancer types for patients of each sex between 1986 and 1994 are analyzed, in relation to the towns of the Community. Concretely, we perform a geographical clustering of the towns of the Community by means of Bayesian networks and conditional Gaussian networks. We present several maps that show the clusterings encoded by the learnt models. In addition to this, we outline the cancer incidence profile for each of the obtained clusters.",
        "bibtex": "@InProceedings{pmlr-vR3-pena01a,\n  title = \t {Geographical Clustering of Cancer Incidence by Means of {B}ayesian Networks and Conditional {G}aussian Networks},\n  author =       {Pe{\\~{n}}a, Jos{\\'{e}} M. and Izarzugaza, I. and Lozano, Jos{\\'{e}} Antonio and Aldasoro, E. and Larra{\\~{n}}aga, Pedro},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {237--242},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/pena01a/pena01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/pena01a.html},\n  abstract = \t {With the aim of improving knowledge on the geographical distribution and characterization of malignant tumors in the Autonomous Community of the Basque Country (Spain), age-standardized cancer incidence rates of the 6 most frequent cancer types for patients of each sex between 1986 and 1994 are analyzed, in relation to the towns of the Community. Concretely, we perform a geographical clustering of the towns of the Community by means of Bayesian networks and conditional Gaussian networks. We present several maps that show the clusterings encoded by the learnt models. In addition to this, we outline the cancer incidence profile for each of the obtained clusters.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/pena01a/pena01a.pdf",
        "supp": "",
        "pdf_size": 661618,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=827804265928899417&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Computer Science and Artificial Intelligence, University of the Basque Country, Donostia-San Sebasti\u00e1n, Spain; Basque Health Service, Basque Government, Vitoria-Gasteiz, Spain; Dept. of Computer Science and Artificial Intelligence, University of the Basque Country, Donostia-San Sebasti\u00e1n, Spain; Basque Health Service, Basque Government, Vitoria-Gasteiz, Spain; Dept. of Computer Science and Artificial Intelligence, University of the Basque Country, Donostia-San Sebasti\u00e1n, Spain",
        "aff_domain": "si.ehu.es;si.ehu.es;si.ehu.es;ej-gv.es;ej-gv.es",
        "email": "si.ehu.es;si.ehu.es;si.ehu.es;ej-gv.es;ej-gv.es",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "University of the Basque Country;Basque Health Service",
        "aff_unique_dep": "Dept. of Computer Science and Artificial Intelligence;",
        "aff_unique_url": "https://www.ehu.eus/en;",
        "aff_unique_abbr": "UPV/EHU;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Donostia-San Sebasti\u00e1n;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "a09044f6a4",
        "title": "Handling Missing and Unreliable Information in Speech Recognition",
        "site": "https://proceedings.mlr.press/r3/green01a.html",
        "author": "Phil D. Green; Jon Barker; Martin Cooke; Ljubomir Josifovski",
        "abstract": "In this work, techniques for classification with missing or unreliable data are applied to the problem of noise-robustness in Automatic Speech Recognition (ASR). The primary advantage of this viewpoint is that it makes minimal assumptions about any noise background. As motivation, we review evidence that the auditory system is capable of dealing with incomplete data and, indeed, does so in normal listening conditions. We formulate the unreliable classification problem and show how it can be expressed in the framework of Continuous Density Hidden Markov Models for statistical ASR. We describe experiments on connected digit recognition in noise in which encouraging results are obtained. Results are improved by \u2019softening\u2019 the missing data decision. We argue that if the noise background is unpredictable it is necessary to integrate primitive processes which identify coherent spectraltemporal regions likely to be dominated by a single source with a generalised recognition decode which searches for the best sub-set of regions which match a speech source. We describe an implementation of a multi-source decoder using missing data recognition and show how it improves recognition results for non-stationary noises.",
        "bibtex": "@InProceedings{pmlr-vR3-green01a,\n  title = \t {Handling Missing and Unreliable Information in Speech Recognition},\n  author =       {Green, Phil D. and Barker, Jon and Cooke, Martin and Josifovski, Ljubomir},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {112--116},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/green01a/green01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/green01a.html},\n  abstract = \t {In this work, techniques for classification with missing or unreliable data are applied to the problem of noise-robustness in Automatic Speech Recognition (ASR). The primary advantage of this viewpoint is that it makes minimal assumptions about any noise background. As motivation, we review evidence that the auditory system is capable of dealing with incomplete data and, indeed, does so in normal listening conditions. We formulate the unreliable classification problem and show how it can be expressed in the framework of Continuous Density Hidden Markov Models for statistical ASR. We describe experiments on connected digit recognition in noise in which encouraging results are obtained. Results are improved by \u2019softening\u2019 the missing data decision. We argue that if the noise background is unpredictable it is necessary to integrate primitive processes which identify coherent spectraltemporal regions likely to be dominated by a single source with a generalised recognition decode which searches for the best sub-set of regions which match a speech source. We describe an implementation of a multi-source decoder using missing data recognition and show how it improves recognition results for non-stationary noises.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/green01a/green01a.pdf",
        "supp": "",
        "pdf_size": 210032,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8413947238491432172&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f1c83b777d",
        "title": "Hyperparameters for Soft Bayesian Model Selection",
        "site": "https://proceedings.mlr.press/r3/corduneanu01a.html",
        "author": "Adrian Corduneanu; Christopher M. Bishop",
        "abstract": "Mixture models, in which a probability distribution is represented as a linear superposition of component distributions, are widely used in statistical modeling and pattern recognition. One of the key tasks in the application of mixture models is the determination of a suitable number of components. Conventional approaches based on cross-validation are computationally expensive, are wasteful of data, and give noisy estimates for the optimal number of components. A fully Bayesian treatment, based on Markov chain Monte Carlo methods for instance, will return a posterior distribution over the number of components. However, in practical applications it is generally convenient, or even computationally essential, to select a single, most appropriate model. Recently it has been shown, in the context of linear latent variable models, that the use of hierarchical priors governed by continuous hyperparameters whose values are set by typeII maximum likelihood, can be used to optimize model complexity. In this paper we extend this framework to mixture distributions by considering the classical task of density estimation using mixtures of Gaussians. We show that, by setting the mixing coefficients to maximize the marginal log-likelihood, unwanted components can be suppressed, and the appropriate number of components for the mixture can be determined in a single training run without recourse to crossvalidation. Our approach uses a variational treatment based on a factorized approximation to the posterior distribution.",
        "bibtex": "@InProceedings{pmlr-vR3-corduneanu01a,\n  title = \t {Hyperparameters for Soft Bayesian Model Selection},\n  author =       {Corduneanu, Adrian and Bishop, Christopher M.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {63--70},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/corduneanu01a/corduneanu01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/corduneanu01a.html},\n  abstract = \t {Mixture models, in which a probability distribution is represented as a linear superposition of component distributions, are widely used in statistical modeling and pattern recognition. One of the key tasks in the application of mixture models is the determination of a suitable number of components. Conventional approaches based on cross-validation are computationally expensive, are wasteful of data, and give noisy estimates for the optimal number of components. A fully Bayesian treatment, based on Markov chain Monte Carlo methods for instance, will return a posterior distribution over the number of components. However, in practical applications it is generally convenient, or even computationally essential, to select a single, most appropriate model. Recently it has been shown, in the context of linear latent variable models, that the use of hierarchical priors governed by continuous hyperparameters whose values are set by typeII maximum likelihood, can be used to optimize model complexity. In this paper we extend this framework to mixture distributions by considering the classical task of density estimation using mixtures of Gaussians. We show that, by setting the mixing coefficients to maximize the marginal log-likelihood, unwanted components can be suppressed, and the appropriate number of components for the mixture can be determined in a single training run without recourse to crossvalidation. Our approach uses a variational treatment based on a factorized approximation to the posterior distribution.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/corduneanu01a/corduneanu01a.pdf",
        "supp": "",
        "pdf_size": 228350,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=760306673068915908&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "MIT AI Lab; Microsoft Research",
        "aff_domain": "mit.edu;microsoft.com",
        "email": "mit.edu;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Microsoft",
        "aff_unique_dep": "Artificial Intelligence Laboratory;Microsoft Research",
        "aff_unique_url": "http://www.ai.mit.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MIT AI Lab;MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b70abcd12e",
        "title": "Information-Theoretic Advisors in Invisible Chess",
        "site": "https://proceedings.mlr.press/r3/bud01a.html",
        "author": "Ariel E. Bud; David W. Albrecht; Ann E. Nicholson; Ingrid Zukerman",
        "abstract": "Making decisions under uncertainty remains a central problem in AI research. Unfortunately, most uncertain real-world problems are so complex that progress in them is extremely difficult. Games model some elements of the real world, and offer a more controlled environment for exploring methods for dealing with uncertainty. Chess and chesslike games have long been used as a strategically complex test-bed for general AI research, and we extend that tradition by introducing an imperfect information variant of chess with some useful properties such as the ability to scale the amount of uncertainty in the game. We discuss the complexity of this game which we call invisible chess, and present results outlining the basic game. We motivate and describe the implementation and application of two information-theoretic advisors, and describe our decision-theoretic approach to combining these information-theoretic advisors with a basic strategic advisor. Finally we discuss promising preliminary results that we have obtained with these advisors.",
        "bibtex": "@InProceedings{pmlr-vR3-bud01a,\n  title = \t {Information-Theoretic Advisors in Invisible Chess},\n  author =       {Bud, Ariel E. and Albrecht, David W. and Nicholson, Ann E. and Zukerman, Ingrid},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {29--34},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/bud01a/bud01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/bud01a.html},\n  abstract = \t {Making decisions under uncertainty remains a central problem in AI research. Unfortunately, most uncertain real-world problems are so complex that progress in them is extremely difficult. Games model some elements of the real world, and offer a more controlled environment for exploring methods for dealing with uncertainty. Chess and chesslike games have long been used as a strategically complex test-bed for general AI research, and we extend that tradition by introducing an imperfect information variant of chess with some useful properties such as the ability to scale the amount of uncertainty in the game. We discuss the complexity of this game which we call invisible chess, and present results outlining the basic game. We motivate and describe the implementation and application of two information-theoretic advisors, and describe our decision-theoretic approach to combining these information-theoretic advisors with a basic strategic advisor. Finally we discuss promising preliminary results that we have obtained with these advisors.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/bud01a/bud01a.pdf",
        "supp": "",
        "pdf_size": 134295,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8455334646017772287&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science and Software Engineering, Monash University; School of Computer Science and Software Engineering, Monash University; School of Computer Science and Software Engineering, Monash University; School of Computer Science and Software Engineering, Monash University",
        "aff_domain": "csse.monash.edu.au; ; ; ",
        "email": "csse.monash.edu.au; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Monash University",
        "aff_unique_dep": "School of Computer Science and Software Engineering",
        "aff_unique_url": "https://www.monash.edu",
        "aff_unique_abbr": "Monash",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "00fddbbba5",
        "title": "Is regularization unnecessary for boosting?",
        "site": "https://proceedings.mlr.press/r3/jiang01a.html",
        "author": "Wenxin Jiang",
        "abstract": "Boosting algorithms are often observed to be resistant to overfitting, to a degree that one may wonder whether it is harmless to run the algorithms forever, and whether regularization in on way or another is unnecessary [see, e.g., Schapire (1999); Friedman, Hastie and Tibshirani (1999); Grove and Schuurmans (1998); Mason, Baxter, Bartlett and Frean (1999)]. One may also wonder whether it is possible to adapt the boosting ideas to regression, and whether or not it is possible to avoid the need of regularization by just adopting the boosting device. In this paper we present examples where \u2019boosting forever\u2019 leads to suboptimal predictions; while some regularization method, on the other hand, can achieve asymptotic optimality, at least in theory. We conjecture that this can be true in more general situations, and for some other regularization methods as well. Therefore the emerging literature on regularized variants of boosting is not unnecessary, but should be encouraged instead. The results of this paper are obtained from an analogy between some boosting algorithms that are used in regression and classification.",
        "bibtex": "@InProceedings{pmlr-vR3-jiang01a,\n  title = \t {Is regularization unnecessary for boosting?},\n  author =       {Jiang, Wenxin},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {129--136},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/jiang01a/jiang01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/jiang01a.html},\n  abstract = \t {Boosting algorithms are often observed to be resistant to overfitting, to a degree that one may wonder whether it is harmless to run the algorithms forever, and whether regularization in on way or another is unnecessary [see, e.g., Schapire (1999); Friedman, Hastie and Tibshirani (1999); Grove and Schuurmans (1998); Mason, Baxter, Bartlett and Frean (1999)]. One may also wonder whether it is possible to adapt the boosting ideas to regression, and whether or not it is possible to avoid the need of regularization by just adopting the boosting device. In this paper we present examples where \u2019boosting forever\u2019 leads to suboptimal predictions; while some regularization method, on the other hand, can achieve asymptotic optimality, at least in theory. We conjecture that this can be true in more general situations, and for some other regularization methods as well. Therefore the emerging literature on regularized variants of boosting is not unnecessary, but should be encouraged instead. The results of this paper are obtained from an analogy between some boosting algorithms that are used in regression and classification.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/jiang01a/jiang01a.pdf",
        "supp": "",
        "pdf_size": 203677,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11402641944848838150&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Statistics, Northwestern University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Northwestern University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.northwestern.edu",
        "aff_unique_abbr": "NU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "03b278ae2e",
        "title": "Learning Bayesian networks with mixed variables",
        "site": "https://proceedings.mlr.press/r3/bottcher01a.html",
        "author": "Susanne Bottcher",
        "abstract": "The paper considers conditional Gaussian networks. As conjugate local priors, we use the Dirichlet distribution for discrete variables and the Gaussian-inverse Gamma distribution for continuous variables, given a configuration of the discrete parents. We assume parameter independence and complete data. Further, the network-score is calculated. We then develop a local master prior procedure, for deriving parameter priors in CG networks. The local master procedure satisfies parameter independence, parameter modularity and likelihood equivalence.",
        "bibtex": "@InProceedings{pmlr-vR3-bottcher01a,\n  title = \t {Learning Bayesian networks with mixed variables},\n  author =       {Bottcher, Susanne},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {13--20},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/bottcher01a/bottcher01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/bottcher01a.html},\n  abstract = \t {The paper considers conditional Gaussian networks. As conjugate local priors, we use the Dirichlet distribution for discrete variables and the Gaussian-inverse Gamma distribution for continuous variables, given a configuration of the discrete parents. We assume parameter independence and complete data. Further, the network-score is calculated. We then develop a local master prior procedure, for deriving parameter priors in CG networks. The local master procedure satisfies parameter independence, parameter modularity and likelihood equivalence.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/bottcher01a/bottcher01a.pdf",
        "supp": "",
        "pdf_size": 175424,
        "gs_citation": 98,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8180506894801121051&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4f8af5447f",
        "title": "Learning in high dimensions: Modular Mixture Models",
        "site": "https://proceedings.mlr.press/r3/attias01a.html",
        "author": "Hagai Attias",
        "abstract": "We present a new approach to learning prob- abilistic models for high dimensional data. This approach divides the data dimensions into low dimensional subspaces, and learns a separate mixture model for each subspace. The models combine in a principled manner to form a flexible modular network that pro- duces a total density estimate. We derive and demonstrate an iterative learning algorithm that uses only local information.",
        "bibtex": "@InProceedings{pmlr-vR3-attias01a,\n  title = \t {Learning in high dimensions: Modular Mixture Models},\n  author =       {Attias, Hagai},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {8--12},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/attias01a/attias01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/attias01a.html},\n  abstract = \t {We present a new approach to learning prob- abilistic models for high dimensional data. This approach divides the data dimensions into low dimensional subspaces, and learns a separate mixture model for each subspace. The models combine in a principled manner to form a flexible modular network that pro- duces a total density estimate. We derive and demonstrate an iterative learning algorithm that uses only local information.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/attias01a/attias01a.pdf",
        "supp": "",
        "pdf_size": 74880,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4875605775091752360&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Microsoft Research, USA",
        "aff_domain": "microsoft.com",
        "email": "microsoft.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9f264ea4ce",
        "title": "Learning mixtures of smooth, nonuniform deformation models for probabilistic image matching",
        "site": "https://proceedings.mlr.press/r3/jojic01a.html",
        "author": "Nebojsa Jojic; Patrice Y. Simard; Brendan J. Frey; David Heckerman",
        "abstract": "By representing images and image prototypes by linear subspaces spanned by \"tangent vectors\" (derivatives of an image with respect to translation, rotation, etc.), impressive invariance to known types of uniform distortion can be built into feedforward discriminators. We describe a new probability model that can jointly cluster data and learn mixtures of nonuniform, smooth deformation fields. Our fields are based on low-frequency wavelets, so they use very few parameters to model a wide range of smooth deformations (unlike, e.g., factor analysis, which uses a large number of parameters to model deformations). We give results on handwritten digit recognition and face recognition.",
        "bibtex": "@InProceedings{pmlr-vR3-jojic01a,\n  title = \t {Learning mixtures of smooth, nonuniform deformation models for probabilistic image matching},\n  author =       {Jojic, Nebojsa and Simard, Patrice Y. and Frey, Brendan J. and Heckerman, David},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {137--142},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/jojic01a/jojic01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/jojic01a.html},\n  abstract = \t {By representing images and image prototypes by linear subspaces spanned by \"tangent vectors\" (derivatives of an image with respect to translation, rotation, etc.), impressive invariance to known types of uniform distortion can be built into feedforward discriminators. We describe a new probability model that can jointly cluster data and learn mixtures of nonuniform, smooth deformation fields. Our fields are based on low-frequency wavelets, so they use very few parameters to model a wide range of smooth deformations (unlike, e.g., factor analysis, which uses a large number of parameters to model deformations). We give results on handwritten digit recognition and face recognition.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/jojic01a/jojic01a.pdf",
        "supp": "",
        "pdf_size": 281015,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=77750425669764098&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ffcbd61b86",
        "title": "Managing Multiple Models",
        "site": "https://proceedings.mlr.press/r3/chipman01a.html",
        "author": "Hugh A. Chipman; Edward I. George; Robert E. McCulloch",
        "abstract": "Recent research in model selection and adaptive modeling has produced an embarrassment of riches. By using any one of several different techniques, an analyst is able to generate a number of models that describe the same data set well. Examples include multiple tree models generated by bootstrapping or stochastic searches, and different subsets of variables in linear regression models identified by stochastic or exhaustive searches. While model averaging can use these models to improve prediction accuracy, interpretation of the resultant models becomes difficult. We seek a compromise, developing measures of dissimilarity between different models and using these to select good models which may reveal different aspects of the data. Data on housing prices in Boston are used to illustrate this in the context of treed regression models.",
        "bibtex": "@InProceedings{pmlr-vR3-chipman01a,\n  title = \t {Managing Multiple Models},\n  author =       {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {41--48},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/chipman01a/chipman01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/chipman01a.html},\n  abstract = \t {Recent research in model selection and adaptive modeling has produced an embarrassment of riches. By using any one of several different techniques, an analyst is able to generate a number of models that describe the same data set well. Examples include multiple tree models generated by bootstrapping or stochastic searches, and different subsets of variables in linear regression models identified by stochastic or exhaustive searches. While model averaging can use these models to improve prediction accuracy, interpretation of the resultant models becomes difficult. We seek a compromise, developing measures of dissimilarity between different models and using these to select good models which may reveal different aspects of the data. Data on housing prices in Boston are used to illustrate this in the context of treed regression models.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/chipman01a/chipman01a.pdf",
        "supp": "",
        "pdf_size": 153292,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=146602511417717723&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "dfe0e234c8",
        "title": "Message Length as an Effective Ockham\u2019s Razor in Decision Tree Induction",
        "site": "https://proceedings.mlr.press/r3/needham01a.html",
        "author": "Scott Needham; David L. Dowe",
        "abstract": "The validity of the Ockham\u2019s Razor principle is a topic of much debate. A series of empirical investigations have sought to discredit the principle by the application of decision trees to learning tasks using node cardinality as the objective function. As a response to these papers, we suggest that the message length of a hypothesis can be used as an effective interpretation of Ockham\u2019s Razor, resulting in positive empirical support for the principle. The theoretical justification for this Bayesian interpretation is also investigated.",
        "bibtex": "@InProceedings{pmlr-vR3-needham01a,\n  title = \t {Message Length as an Effective Ockham\u2019s Razor in Decision Tree Induction},\n  author =       {Needham, Scott and Dowe, David L.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {216--223},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/needham01a/needham01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/needham01a.html},\n  abstract = \t {The validity of the Ockham\u2019s Razor principle is a topic of much debate. A series of empirical investigations have sought to discredit the principle by the application of decision trees to learning tasks using node cardinality as the objective function. As a response to these papers, we suggest that the message length of a hypothesis can be used as an effective interpretation of Ockham\u2019s Razor, resulting in positive empirical support for the principle. The theoretical justification for this Bayesian interpretation is also investigated.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/needham01a/needham01a.pdf",
        "supp": "",
        "pdf_size": 166901,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9824960627182669624&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6404fa794c",
        "title": "Models for Conditional Probability Tables in Educational Assessment",
        "site": "https://proceedings.mlr.press/r3/almond01a.html",
        "author": "Russell G. Almond; Lou DiBello; Frank Jenkins; Deniz Senturk; Robert J. Mislevy; Linda S. Steinberg; Duanli Yan",
        "abstract": "Experts in educational assessment can often identify the skills needed to provide a solution for a test item and which patterns of those skills pro duce better expected performance. The method described here combines judgements about the structure of the conditional probability table (e.g., conjunctive or compensatory) with Item Response Theory methods for partial credit scoring (Samejima, 1969) to produce a conditional probability table or a prior distribution for a learning algorithm. The structural judgements induce a projection of each configuration of parent skill variables onto a single latent response-propensity $\\theta$. This is then used to calculate a probability for each cell in the table.",
        "bibtex": "@InProceedings{pmlr-vR3-almond01a,\n  title = \t {Models for Conditional Probability Tables in Educational Assessment},\n  author =       {Almond, Russell G. and DiBello, Lou and Jenkins, Frank and Senturk, Deniz and Mislevy, Robert J. and Steinberg, Linda S. and Yan, Duanli},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {1--7},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/almond01a/almond01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/almond01a.html},\n  abstract = \t {Experts in educational assessment can often identify the skills needed to provide a solution for a test item and which patterns of those skills pro duce better expected performance. The method described here combines judgements about the structure of the conditional probability table (e.g., conjunctive or compensatory) with Item Response Theory methods for partial credit scoring (Samejima, 1969) to produce a conditional probability table or a prior distribution for a learning algorithm. The structural judgements induce a projection of each configuration of parent skill variables onto a single latent response-propensity $\\theta$. This is then used to calculate a probability for each cell in the table.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/almond01a/almond01a.pdf",
        "supp": "",
        "pdf_size": 185755,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6324300692935472417&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Educational Testing Service, Princeton, NJ; Educational Testing Service, Princeton, NJ; Educational Testing Service, Princeton, NJ; Educational Testing Service, Princeton, NJ; Educational Testing Service, Princeton, NJ; Educational Testing Service, Princeton, NJ; Educational Testing Service, Princeton, NJ",
        "aff_domain": "ets.org; ; ; ; ; ; ",
        "email": "ets.org; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;0",
        "aff_unique_norm": "Educational Testing Service",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ets.org",
        "aff_unique_abbr": "ETS",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Princeton",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5371073f32",
        "title": "Monte-Carlo Algorithms for the Improvement of Finite-State Stochastic Controllers: Application to Bayes-Adaptive Markov Decision Processes",
        "site": "https://proceedings.mlr.press/r3/duff01a.html",
        "author": "Michael O. Duff",
        "abstract": "We consider the problem of \"optimal learning\" for Markov decision processes with uncertain transition probabilities. Motivated by the correspondence between these processes and partially-observable Markov decision processes, we adopt policies expressed as finite-state stochastic automata, and we propose policy improvement algorithms that utilize Monte-Carlo techniques for gradient estimation and ascent.",
        "bibtex": "@InProceedings{pmlr-vR3-duff01a,\n  title = \t {Monte-Carlo Algorithms for the Improvement of Finite-State Stochastic Controllers: Application to Bayes-Adaptive Markov Decision Processes},\n  author =       {Duff, Michael O.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {93--97},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/duff01a/duff01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/duff01a.html},\n  abstract = \t {We consider the problem of \"optimal learning\" for Markov decision processes with uncertain transition probabilities. Motivated by the correspondence between these processes and partially-observable Markov decision processes, we adopt policies expressed as finite-state stochastic automata, and we propose policy improvement algorithms that utilize Monte-Carlo techniques for gradient estimation and ascent.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/duff01a/duff01a.pdf",
        "supp": "",
        "pdf_size": 147614,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12873414194011633430&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "https://github.com/ai-model-paper-summarization",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "291720dcd4",
        "title": "On Parameter Priors for Discrete DAG Models",
        "site": "https://proceedings.mlr.press/r3/rusakov01a.html",
        "author": "Dmitry Rusakov; Dan Geiger",
        "abstract": "We investigate parameter priors for discrete DAG models. It was shown in previous works that a Dirichlet prior on the parameters of a discrete DAG model is inevitable assuming global and local parameter independence for all possible complete DAG structures. A similar result for Gaussian DAG models hinted that the assumption of local independence may be redundant. Herein, we prove that the local independence assumption is necessary in order to dictate a Dirichlet prior on the parameters of a discrete DAG model. We explicate the minimal set of assumptions needed to dictate a Dirichlet prior, and we derive the functional form of prior distributions that arise under the global independence assumption alone.",
        "bibtex": "@InProceedings{pmlr-vR3-rusakov01a,\n  title = \t {On Parameter Priors for Discrete {DAG} Models},\n  author =       {Rusakov, Dmitry and Geiger, Dan},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {259--264},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/rusakov01a/rusakov01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/rusakov01a.html},\n  abstract = \t {We investigate parameter priors for discrete DAG models. It was shown in previous works that a Dirichlet prior on the parameters of a discrete DAG model is inevitable assuming global and local parameter independence for all possible complete DAG structures. A similar result for Gaussian DAG models hinted that the assumption of local independence may be redundant. Herein, we prove that the local independence assumption is necessary in order to dictate a Dirichlet prior on the parameters of a discrete DAG model. We explicate the minimal set of assumptions needed to dictate a Dirichlet prior, and we derive the functional form of prior distributions that arise under the global independence assumption alone.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/rusakov01a/rusakov01a.pdf",
        "supp": "",
        "pdf_size": 159199,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9419611447427086114&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1cb0618cad",
        "title": "On searching for optimal classifiers among Bayesian networks",
        "site": "https://proceedings.mlr.press/r3/cowell01a.html",
        "author": "Robert G. Cowell",
        "abstract": "There is much interest in constructing from datasets Bayesian networks which are efficient, or even optimal, for classification purposes. Most search strategies usually discriminate between networks by comparing their marginal likelihood score, but recently it has been suggested that search strategies for classifiers should instead select among models using alternative scores. This paper contributes to this discussion by presenting the results of simulations on the sets of all directed acyclic graphs on four and five nodes. Our results add evidence to earlier indications that the marginal likelihood is likely to be a poor criterion to use for classifier selection.",
        "bibtex": "@InProceedings{pmlr-vR3-cowell01a,\n  title = \t {On searching for optimal classifiers among Bayesian networks},\n  author =       {Cowell, Robert G.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {71--76},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/cowell01a/cowell01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/cowell01a.html},\n  abstract = \t {There is much interest in constructing from datasets Bayesian networks which are efficient, or even optimal, for classification purposes. Most search strategies usually discriminate between networks by comparing their marginal likelihood score, but recently it has been suggested that search strategies for classifiers should instead select among models using alternative scores. This paper contributes to this discussion by presenting the results of simulations on the sets of all directed acyclic graphs on four and five nodes. Our results add evidence to earlier indications that the marginal likelihood is likely to be a poor criterion to use for classifier selection.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/cowell01a/cowell01a.pdf",
        "supp": "",
        "pdf_size": 297108,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15612697485367284821&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "c40e8b8b55",
        "title": "On the effectiveness of the skew divergence for statistical language analysis",
        "site": "https://proceedings.mlr.press/r3/lee01a.html",
        "author": "Lillian Lee",
        "abstract": "Estimating word co-occurrence probabilities is a problem underlying many applications in statistical natural language processing. Distance-weighted (or similarityweighted) averaging has been shown to be a promising approach to the analysis of novel co-occurrences. Many measures of distributional similarity have been proposed for use in the distance-weighted averaging framework; here, we empirically study their stability properties, finding that similarity-based estimation appears to make more efficient use of more reliable portions of the training data. We also investigate properties of the skew divergence, a weighted version of the KullbackLeibler (KL) divergence; our results indicate that the skew divergence yields better results than the KL divergence even when the KL divergence is applied to more sophisticated probability estimates.",
        "bibtex": "@InProceedings{pmlr-vR3-lee01a,\n  title = \t {On the effectiveness of the skew divergence for statistical language analysis},\n  author =       {Lee, Lillian},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {176--183},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/lee01a/lee01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/lee01a.html},\n  abstract = \t {Estimating word co-occurrence probabilities is a problem underlying many applications in statistical natural language processing. Distance-weighted (or similarityweighted) averaging has been shown to be a promising approach to the analysis of novel co-occurrences. Many measures of distributional similarity have been proposed for use in the distance-weighted averaging framework; here, we empirically study their stability properties, finding that similarity-based estimation appears to make more efficient use of more reliable portions of the training data. We also investigate properties of the skew divergence, a weighted version of the KullbackLeibler (KL) divergence; our results indicate that the skew divergence yields better results than the KL divergence even when the KL divergence is applied to more sophisticated probability estimates.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/lee01a/lee01a.pdf",
        "supp": "",
        "pdf_size": 155823,
        "gs_citation": 239,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10831209197196183144&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, Cornell University, Ithaca, NY 14853 USA",
        "aff_domain": "cs.cornell.edu",
        "email": "cs.cornell.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Cornell University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.cornell.edu",
        "aff_unique_abbr": "Cornell",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Ithaca",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c9a367f198",
        "title": "Online Bagging and Boosting",
        "site": "https://proceedings.mlr.press/r3/oza01a.html",
        "author": "Nikunj C. Oza; Stuart J. Russell",
        "abstract": "Bagging and boosting are well-known ensemble learning methods. They combine multiple learned base models with the aim of improving generalization performance. To date, they have been used primarily in batch mode, and no effective online versions have been proposed. We present simple online bagging and boosting algorithms that we claim perform as well as their batch counterparts.",
        "bibtex": "@InProceedings{pmlr-vR3-oza01a,\n  title = \t {Online Bagging and Boosting},\n  author =       {Oza, Nikunj C. and Russell, Stuart J.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {229--236},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/oza01a/oza01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/oza01a.html},\n  abstract = \t {Bagging and boosting are well-known ensemble learning methods. They combine multiple learned base models with the aim of improving generalization performance. To date, they have been used primarily in batch mode, and no effective online versions have been proposed. We present simple online bagging and boosting algorithms that we claim perform as well as their batch counterparts.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/oza01a/oza01a.pdf",
        "supp": "",
        "pdf_size": 221148,
        "gs_citation": 1326,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7341114427768725305&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e1a685c63c",
        "title": "Piecewise Linear Instrumental Variable Estimation of Causal Influence",
        "site": "https://proceedings.mlr.press/r3/scheines01a.html",
        "author": "Richard Scheines; Gregory F. Cooper; Changwon Yoo; Tianjiao Chu",
        "abstract": "Instrumental Variable (IV) estimation is a powerful strategy for estimating causal  influence, even in the presence of confounding. Standard IV estimation requires that the relationships between variables is linear. Here we relax the linearity requirement by constructing a piecewise linear IV estimator. Simulation studies show that when the causal influence of $X$ on $Y$ is non-linear, the piecewise linear is an improvement.",
        "bibtex": "@InProceedings{pmlr-vR3-scheines01a,\n  title = \t {Piecewise Linear Instrumental Variable Estimation of Causal Influence},\n  author =       {Scheines, Richard and Cooper, Gregory F. and Yoo, Changwon and Chu, Tianjiao},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {265--271},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/scheines01a/scheines01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/scheines01a.html},\n  abstract = \t {Instrumental Variable (IV) estimation is a powerful strategy for estimating causal  influence, even in the presence of confounding. Standard IV estimation requires that the relationships between variables is linear. Here we relax the linearity requirement by constructing a piecewise linear IV estimator. Simulation studies show that when the causal influence of $X$ on $Y$ is non-linear, the piecewise linear is an improvement.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/scheines01a/scheines01a.pdf",
        "supp": "",
        "pdf_size": 62130,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15826901927034476408&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5b4dcbf6ce",
        "title": "Predicting with Variables Constructed from Temporal Sequences",
        "site": "https://proceedings.mlr.press/r3/kayaalp01a.html",
        "author": "Mehmet Kayaalp; Gregory F. Cooper; Gilles Clermont",
        "abstract": "In this study, we applied the local learning paradigm and conditional independence assumptions to control the rapid growth of the dimensionality introduced by multivariate time series. We also combined various univariate time series with different stationary assumptions in temporal models. These techniques are applied to learn simple Bayesian networks from temporal data and to predict survival probabilities of ICU patients on every day of their ICU stay.",
        "bibtex": "@InProceedings{pmlr-vR3-kayaalp01a,\n  title = \t {Predicting with Variables Constructed from Temporal Sequences},\n  author =       {Kayaalp, Mehmet and Cooper, Gregory F. and Clermont, Gilles},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {143--148},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/kayaalp01a/kayaalp01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/kayaalp01a.html},\n  abstract = \t {In this study, we applied the local learning paradigm and conditional independence assumptions to control the rapid growth of the dimensionality introduced by multivariate time series. We also combined various univariate time series with different stationary assumptions in temporal models. These techniques are applied to learn simple Bayesian networks from temporal data and to predict survival probabilities of ICU patients on every day of their ICU stay.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/kayaalp01a/kayaalp01a.pdf",
        "supp": "",
        "pdf_size": 186026,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14603855873755013454&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "Center for Biomedical Informatics, Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15213; Center for Biomedical Informatics, Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15213; Department of Anesthesiology, School of Medicine, University of Pittsburgh, Pittsburgh, PA 15213",
        "aff_domain": "acm.org;cbmi.upmc.edu;anes.upmc.edu",
        "email": "acm.org;cbmi.upmc.edu;anes.upmc.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Pittsburgh",
        "aff_unique_dep": "Center for Biomedical Informatics, Intelligent Systems Program",
        "aff_unique_url": "https://www.pitt.edu",
        "aff_unique_abbr": "Pitt",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f247e28f51",
        "title": "Products of Hidden Markov Models",
        "site": "https://proceedings.mlr.press/r3/brown01a.html",
        "author": "Andrew D. Brown; Geoffrey E. Hinton",
        "abstract": "We present products of hidden Markov models (PoHMM\u2019s), a way of combining HMM\u2019s to form a distributed state time series model. Inference in a PoHMM is tractable and efficient. Learning of the parameters, although intractable, can be effectively done using the Product of Experts learning rule. The distributed state helps the model to explain data which has multiple causes, and the fact that each model need only explain part of the data means a PoHMM can capture longer range structure than an HMM is capable of. We show some results on modelling character strings, a simple language task and the symbolic family trees problem, which highlight these advantages.",
        "bibtex": "@InProceedings{pmlr-vR3-brown01a,\n  title = \t {Products of Hidden Markov Models},\n  author =       {Brown, Andrew D. and Hinton, Geoffrey E.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {21--28},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/brown01a/brown01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/brown01a.html},\n  abstract = \t {We present products of hidden Markov models (PoHMM\u2019s), a way of combining HMM\u2019s to form a distributed state time series model. Inference in a PoHMM is tractable and efficient. Learning of the parameters, although intractable, can be effectively done using the Product of Experts learning rule. The distributed state helps the model to explain data which has multiple causes, and the fact that each model need only explain part of the data means a PoHMM can capture longer range structure than an HMM is capable of. We show some results on modelling character strings, a simple language task and the symbolic family trees problem, which highlight these advantages.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/brown01a/brown01a.pdf",
        "supp": "",
        "pdf_size": 186149,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1070836033653757009&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "23f164fa4b",
        "title": "Profile Likelihood in Directed Graphical Models from BUGS Output",
        "site": "https://proceedings.mlr.press/r3/hojbjerre01a.html",
        "author": "Malene H\u00f8jbjerre",
        "abstract": "This paper presents a method for using output of the computer program BUGS to obtain approximate profile likelihood functions of parameters or functions of parameters in directed graphical models with incomplete data. The method also provides a tool to approximate integrated likelihood functions. The prior distributions specified in BUGS do not have a significant impact on the profile likelihood functions and we consider the method as a desirable supplement to BUGS that enables us to do both Bayesian and likelihood based analyses in directed graphical models.",
        "bibtex": "@InProceedings{pmlr-vR3-hojbjerre01a,\n  title = \t {Profile Likelihood in Directed Graphical Models from {BUGS} Output},\n  author =       {H{\\o}jbjerre, Malene},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {123--128},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/hojbjerre01a/hojbjerre01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/hojbjerre01a.html},\n  abstract = \t {This paper presents a method for using output of the computer program BUGS to obtain approximate profile likelihood functions of parameters or functions of parameters in directed graphical models with incomplete data. The method also provides a tool to approximate integrated likelihood functions. The prior distributions specified in BUGS do not have a significant impact on the profile likelihood functions and we consider the method as a desirable supplement to BUGS that enables us to do both Bayesian and likelihood based analyses in directed graphical models.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/hojbjerre01a/hojbjerre01a.pdf",
        "supp": "",
        "pdf_size": 146841,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16909118395967366454&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Mathematical Sciences, Aalborg University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Aalborg University",
        "aff_unique_dep": "Department of Mathematical Sciences",
        "aff_unique_url": "https://www.aau.dk",
        "aff_unique_abbr": "AAU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Denmark"
    },
    {
        "id": "884a0b437a",
        "title": "Solving Hidden-Mode Markov Decision Problems",
        "site": "https://proceedings.mlr.press/r3/choi01a.html",
        "author": "Samuel Ping-Man Choi; Nevin Lianwen Zhang; Dit-Yan Yeung",
        "abstract": "Markov decision processes (HM-MDPs) are a novel mathematical framework for a subclass of nonstationary reinforcement learning problems where environment dynamics change over time according to a Markov process. HM-MDPs are a special case of partially observable Markov decision processes (POMDPs), and therefore nonstationary problems of this type can in principle be addressed indirectly via existing POMDP algorithms. However, previous research has shown that such an indirect approach is inefficient compared with a direct HM-MDP approach in terms of the model learning time. In this paper, we investigate how to solve HM-MDP problems efficiently by using a direct approach. We exploit the HM-MDP structure and derive an equation for dynamic programming update. Our equation decomposes the value function into a number of components and as a result, substantially reduces the amount of computations in finding optimal policies. Based on the incremental pruning and point-based improvement techniques, a value iteration algorithm is also implemented. Empirical results show that the HM-MDP approach outperforms the POMDP one several order of magnitude with respect to both space requirement and speed.",
        "bibtex": "@InProceedings{pmlr-vR3-choi01a,\n  title = \t {Solving Hidden-Mode Markov Decision Problems},\n  author =       {Choi, Samuel Ping-Man and Zhang, Nevin Lianwen and Yeung, Dit-Yan},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {49--56},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/choi01a/choi01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/choi01a.html},\n  abstract = \t {Markov decision processes (HM-MDPs) are a novel mathematical framework for a subclass of nonstationary reinforcement learning problems where environment dynamics change over time according to a Markov process. HM-MDPs are a special case of partially observable Markov decision processes (POMDPs), and therefore nonstationary problems of this type can in principle be addressed indirectly via existing POMDP algorithms. However, previous research has shown that such an indirect approach is inefficient compared with a direct HM-MDP approach in terms of the model learning time. In this paper, we investigate how to solve HM-MDP problems efficiently by using a direct approach. We exploit the HM-MDP structure and derive an equation for dynamic programming update. Our equation decomposes the value function into a number of components and as a result, substantially reduces the amount of computations in finding optimal policies. Based on the incremental pruning and point-based improvement techniques, a value iteration algorithm is also implemented. Empirical results show that the HM-MDP approach outperforms the POMDP one several order of magnitude with respect to both space requirement and speed.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/choi01a/choi01a.pdf",
        "supp": "",
        "pdf_size": 193279,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9615927510037588097&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Department of Computer Science, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, China; Department of Computer Science, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, China; Department of Computer Science, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, China",
        "aff_domain": "cs.ust.hk;cs.ust.hk;cs.ust.hk",
        "email": "cs.ust.hk;cs.ust.hk;cs.ust.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Clear Water Bay",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "af70a48fee",
        "title": "Some variations on variation independence.",
        "site": "https://proceedings.mlr.press/r3/dawid01a.html",
        "author": "A. Philip Dawid",
        "abstract": "Variation independence of functions is a simple natural \u2019irrelevance\u2019 property arising in a number of applications in Artificial Intelligence and Statistics. We show how it can be alternatively expressed in terms of two other representations of the same underlying structure: equivalence relations and $\\tau$ -fields.",
        "bibtex": "@InProceedings{pmlr-vR3-dawid01a,\n  title = \t {Some variations on variation independence.},\n  author =       {Dawid, A. Philip},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {83--86},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/dawid01a/dawid01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/dawid01a.html},\n  abstract = \t {Variation independence of functions is a simple natural \u2019irrelevance\u2019 property arising in a number of applications in Artificial Intelligence and Statistics. We show how it can be alternatively expressed in terms of two other representations of the same underlying structure: equivalence relations and $\\tau$ -fields.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/dawid01a/dawid01a.pdf",
        "supp": "",
        "pdf_size": 116864,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3883645849148717307&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5e3e87f817",
        "title": "Statistical Aspects of Stochastic Logic Programs",
        "site": "https://proceedings.mlr.press/r3/cussens01a.html",
        "author": "James Cussens",
        "abstract": "Stochastic logic programs (SLPs) and the various distributions they define are presented with a stress on their characterisation in terms of Markov chains. Sampling, parameter estimation and structure learning for SLPs are discussed. The application of SLPs to Bayesian learning, computational linguistics and computational biology are considered. Lafferty\u2019s Gibbs-Markov models are compared and contrasted with SLPs.",
        "bibtex": "@InProceedings{pmlr-vR3-cussens01a,\n  title = \t {Statistical Aspects of Stochastic Logic Programs},\n  author =       {Cussens, James},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {77--82},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/cussens01a/cussens01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/cussens01a.html},\n  abstract = \t {Stochastic logic programs (SLPs) and the various distributions they define are presented with a stress on their characterisation in terms of Markov chains. Sampling, parameter estimation and structure learning for SLPs are discussed. The application of SLPs to Bayesian learning, computational linguistics and computational biology are considered. Lafferty\u2019s Gibbs-Markov models are compared and contrasted with SLPs.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/cussens01a/cussens01a.pdf",
        "supp": "",
        "pdf_size": 161619,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11555992581626657621&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "581130be71",
        "title": "Stochastic System Monitoring and Control",
        "site": "https://proceedings.mlr.press/r3/provan01a.html",
        "author": "Gregory M. Provan",
        "abstract": "In this article we propose a new technique for efficiently solving a specialized instance of a finite state sequential decision process. This specialized task requires keeping a system within a set of nominal states, introducing control actions only when forbidden states are entered. Instead of assuming that the process evolves only due to control actions, we assume that system evolution occurs due to both internal system dynamics and control actions, referred to as endogenous and exogenous evolution respectively. Since controls are needed only for exogenous evolution, we separate inference for the case of endogenous and exogenous evolution, obtaining an inference method that is computationally simpler than using a standard POMDP framework for solving this task. We summarize the problem framework and the algorithm for performing sequential decision-making.",
        "bibtex": "@InProceedings{pmlr-vR3-provan01a,\n  title = \t {Stochastic System Monitoring and Control},\n  author =       {Provan, Gregory M.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {243--250},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/provan01a/provan01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/provan01a.html},\n  abstract = \t {In this article we propose a new technique for efficiently solving a specialized instance of a finite state sequential decision process. This specialized task requires keeping a system within a set of nominal states, introducing control actions only when forbidden states are entered. Instead of assuming that the process evolves only due to control actions, we assume that system evolution occurs due to both internal system dynamics and control actions, referred to as endogenous and exogenous evolution respectively. Since controls are needed only for exogenous evolution, we separate inference for the case of endogenous and exogenous evolution, obtaining an inference method that is computationally simpler than using a standard POMDP framework for solving this task. We summarize the problem framework and the algorithm for performing sequential decision-making.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/provan01a/provan01a.pdf",
        "supp": "",
        "pdf_size": 320297,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff": "Rockwell Science Center",
        "aff_domain": "rsc.rockwell.com",
        "email": "rsc.rockwell.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Rockwell Science Center",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.rockwellcollins.com/About-Us/Our-Company/Rockwell-Science-Center",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "33d36ec2de",
        "title": "Temporal Matching under Uncertainty",
        "site": "https://proceedings.mlr.press/r3/tawfik01a.html",
        "author": "Ahmed Y. Tawfik; Greg Scott",
        "abstract": "Temporal matching is the problem of matching observations to predefined temporal patterns or templates. This problem arises in many applications including medical and model-based diagnosis, plan-recognition, and temporal databases. This work examines the sources of uncertainty in temporal matching and presents a probabilistic technique to perform temporal matching under uncertainty. This technique is then applied to the problem of finding the onset of infection with \\emph{Toxoplasma Gondii}.",
        "bibtex": "@InProceedings{pmlr-vR3-tawfik01a,\n  title = \t {Temporal Matching under Uncertainty},\n  author =       {Tawfik, Ahmed Y. and Scott, Greg},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {293--297},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/tawfik01a/tawfik01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/tawfik01a.html},\n  abstract = \t {Temporal matching is the problem of matching observations to predefined temporal patterns or templates. This problem arises in many applications including medical and model-based diagnosis, plan-recognition, and temporal databases. This work examines the sources of uncertainty in temporal matching and presents a probabilistic technique to perform temporal matching under uncertainty. This technique is then applied to the problem of finding the onset of infection with \\emph{Toxoplasma Gondii}.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/tawfik01a/tawfik01a.pdf",
        "supp": "",
        "pdf_size": 121660,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13481188837798564683&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "School of Computer Science, University of Windsor, Windsor, ON N9B 3P4, Canada; University of Prince Edward Island, Charlottetown, PE C1A 9J8, Canada",
        "aff_domain": "uwindsor.ca;upei.ca",
        "email": "uwindsor.ca;upei.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Windsor;University of Prince Edward Island",
        "aff_unique_dep": "School of Computer Science;",
        "aff_unique_url": "https://www.uwindsor.ca;https://www.upei.ca",
        "aff_unique_abbr": "UWindsor;UPEI",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Windsor;Charlottetown",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "6081de612f",
        "title": "The Efficient Propagation of Arbitrary Subsets of Beliefs in Discrete-Valued Bayesian Networks",
        "site": "https://proceedings.mlr.press/r3/smith01a.html",
        "author": "Duncan Smith",
        "abstract": "The paper describes an approach for propagating arbitrary subsets of beliefs in Bayesian Belief Networks. The method is based on a multiple message passing scheme in junction trees. A hybrid tree structure is introduced, both for the propagation of evidence and as an efficiently permutable representation of a decomposable graph. The use of maximal prime subgraph decompositions and tree permutations to reduce computational cost is demonstrated.",
        "bibtex": "@InProceedings{pmlr-vR3-smith01a,\n  title = \t {The Efficient Propagation of Arbitrary Subsets of Beliefs in Discrete-Valued Bayesian Networks},\n  author =       {Smith, Duncan},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {272--277},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/smith01a/smith01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/smith01a.html},\n  abstract = \t {The paper describes an approach for propagating arbitrary subsets of beliefs in Bayesian Belief Networks. The method is based on a multiple message passing scheme in junction trees. A hybrid tree structure is introduced, both for the propagation of evidence and as an efficiently permutable representation of a decomposable graph. The use of maximal prime subgraph decompositions and tree permutations to reduce computational cost is demonstrated.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/smith01a/smith01a.pdf",
        "supp": "",
        "pdf_size": 104543,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6333299027840003738&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 0,
        "aff": "Department of Physics, Astronomy and Mathematics, University of Central Lancashire",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Central Lancashire",
        "aff_unique_dep": "Department of Physics, Astronomy and Mathematics",
        "aff_unique_url": "https://www.uclan.ac.uk",
        "aff_unique_abbr": "UCLan",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "dd6791b1e1",
        "title": "The Learning Curve Method Applied to Clustering",
        "site": "https://proceedings.mlr.press/r3/meek01b.html",
        "author": "Christopher Meek; Bo Thiesson; David Heckerman",
        "abstract": "We describe novel fast learning curve methods\u2014methods for scaling inductive methods to large data sets\u2014and their application to clustering. We describe the decision theoretic underpinnings of the approach and demonstrate significant performance gains on two real-world data sets.",
        "bibtex": "@InProceedings{pmlr-vR3-meek01b,\n  title = \t {The Learning Curve Method Applied to Clustering},\n  author =       {Meek, Christopher and Thiesson, Bo and Heckerman, David},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {196--202},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/meek01b/meek01b.pdf},\n  url = \t {https://proceedings.mlr.press/r3/meek01b.html},\n  abstract = \t {We describe novel fast learning curve methods\u2014methods for scaling inductive methods to large data sets\u2014and their application to clustering. We describe the decision theoretic underpinnings of the approach and demonstrate significant performance gains on two real-world data sets.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/meek01b/meek01b.pdf",
        "supp": "",
        "pdf_size": 178022,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5151072210638390354&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d42dc73e9b",
        "title": "Using Unsupervised Learning to Guide Resampling in Imbalanced Data Sets",
        "site": "https://proceedings.mlr.press/r3/nickerson01a.html",
        "author": "Adam Nickerson; Nathalie Japkowicz; Evangelos E. Milios",
        "abstract": "The class imbalance problem causes a classifier to over-fit the data belonging to the class with the greatest number of training examples. The purpose of this paper is to argue that methods that equalize class membership are not as effective as possible when applied blindly and that improvements can be obtained by adjusting for the within-class imbalance. A guided resampling technique is proposed and tested within a simpler letter recognition domain and a more difficult text classification domain. A fast unsupervised clustering technique, Principal Direction Divisive Partitioning (PDDP), is used to determine the internal characteristics of each class. The performance improvement in categories that suffer from a large between-class imbalance (few positive examples) are shown to be improved when using the guided resampling method.",
        "bibtex": "@InProceedings{pmlr-vR3-nickerson01a,\n  title = \t {Using Unsupervised Learning to Guide Resampling in Imbalanced Data Sets},\n  author =       {Nickerson, Adam and Japkowicz, Nathalie and Milios, Evangelos E.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {224--228},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/nickerson01a/nickerson01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/nickerson01a.html},\n  abstract = \t {The class imbalance problem causes a classifier to over-fit the data belonging to the class with the greatest number of training examples. The purpose of this paper is to argue that methods that equalize class membership are not as effective as possible when applied blindly and that improvements can be obtained by adjusting for the within-class imbalance. A guided resampling technique is proposed and tested within a simpler letter recognition domain and a more difficult text classification domain. A fast unsupervised clustering technique, Principal Direction Divisive Partitioning (PDDP), is used to determine the internal characteristics of each class. The performance improvement in categories that suffer from a large between-class imbalance (few positive examples) are shown to be improved when using the guided resampling method.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/nickerson01a/nickerson01a.pdf",
        "supp": "",
        "pdf_size": 164207,
        "gs_citation": 108,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8584883180092341395&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "University of Southern California; MIT; Stanford University",
        "aff_domain": "usc.edu;mit.edu;stanford.edu",
        "email": "usc.edu;mit.edu;stanford.edu",
        "github": "https://github.com/ai-model-paper-summarization",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Southern California;Massachusetts Institute of Technology;Stanford University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.usc.edu;https://web.mit.edu;https://www.stanford.edu",
        "aff_unique_abbr": "USC;MIT;Stanford",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Los Angeles;;Stanford",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1a55c61fda",
        "title": "Variational Learning for Multi-Layer Networks of Linear Threshold Units",
        "site": "https://proceedings.mlr.press/r3/lawrence01a.html",
        "author": "Neil D. Lawrence",
        "abstract": "Linear threshold units (LTUs) were originally proposed as models of biological neurons. They were widely studied in the context of the perceptron (Rosenblatt, 1962). Due to the difficulties of finding a general algorithm for networks with hidden nodes, they never passed into general use. In this work we derive an algorithm in the context of probabilistic models and show how it may be applied in multi-layer networks of LTUs. We demonstrate the performance of the algorithm on three data-sets.",
        "bibtex": "@InProceedings{pmlr-vR3-lawrence01a,\n  title = \t {Variational Learning for Multi-Layer Networks of Linear Threshold Units},\n  author =       {Lawrence, Neil D.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {168--175},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/lawrence01a/lawrence01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/lawrence01a.html},\n  abstract = \t {Linear threshold units (LTUs) were originally proposed as models of biological neurons. They were widely studied in the context of the perceptron (Rosenblatt, 1962). Due to the difficulties of finding a general algorithm for networks with hidden nodes, they never passed into general use. In this work we derive an algorithm in the context of probabilistic models and show how it may be applied in multi-layer networks of LTUs. We demonstrate the performance of the algorithm on three data-sets.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/lawrence01a/lawrence01a.pdf",
        "supp": "",
        "pdf_size": 226680,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2440542957294561444&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Microsoft Research, St George House, 1 Guildhall Street, Cambridge, CB2 3NH, U.K.",
        "aff_domain": "microsoft.com",
        "email": "microsoft.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "9b51ab577e",
        "title": "Why Averaging Classifiers can Protect Against Overfitting",
        "site": "https://proceedings.mlr.press/r3/freund01a.html",
        "author": "Yoav Freund; Yishay Mansour; Robert E. Schapire",
        "abstract": "We study a simple learning algorithm for binary classification. Instead of predicting with the best hypothesis in the hypothesis class, this algorithm predicts with a weighted average of all hypotheses, weighted exponentially with respect to their training error. We show that the prediction of this algorithm is much more stable than the prediction of an algorithm that predicts with the best hypothesis. By allowing the algorithm to abstain from predicting on some examples, we show that the predictions it makes when it does not abstain are very reliable. Finally, we show that the probability that the algorithm abstains is comparable to the generalization error of the best hypothesis in the class.",
        "bibtex": "@InProceedings{pmlr-vR3-freund01a,\n  title = \t {Why Averaging Classifiers can Protect Against Overfitting},\n  author =       {Freund, Yoav and Mansour, Yishay and Schapire, Robert E.},\n  booktitle = \t {Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {98--105},\n  year = \t {2001},\n  editor = \t {Richardson, Thomas S. and Jaakkola, Tommi S.},\n  volume = \t {R3},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r3/freund01a/freund01a.pdf},\n  url = \t {https://proceedings.mlr.press/r3/freund01a.html},\n  abstract = \t {We study a simple learning algorithm for binary classification. Instead of predicting with the best hypothesis in the hypothesis class, this algorithm predicts with a weighted average of all hypotheses, weighted exponentially with respect to their training error. We show that the prediction of this algorithm is much more stable than the prediction of an algorithm that predicts with the best hypothesis. By allowing the algorithm to abstain from predicting on some examples, we show that the predictions it makes when it does not abstain are very reliable. Finally, we show that the probability that the algorithm abstains is comparable to the generalization error of the best hypothesis in the class.},\n  note =         {Reissued by PMLR on 31 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r3/freund01a/freund01a.pdf",
        "supp": "",
        "pdf_size": 153627,
        "gs_citation": 61,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14009883788152943091&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "AT&T Labs /BnZrResearch, Shannon Laboratory, 180 Park Avenue, Room A205, Florham Park, NJ 07932 + Computer Science Dept., Tel\u00adAviv University, ISRAEL; Computer Science Dept., Tel\u00adAviv University, ISRAEL; AT&T Labs /BnZrResearch, Shannon Laboratory, 180 Park Avenue, Room A203, Florham Park, NJ 07932",
        "aff_domain": "research.att.com;math.tau.ac.il;research.att.com",
        "email": "research.att.com;math.tau.ac.il;research.att.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;0",
        "aff_unique_norm": "AT&T Labs;Tel Aviv University",
        "aff_unique_dep": "Shannon Laboratory;Computer Science Dept.",
        "aff_unique_url": "https://www.att.com/labs;https://www.tau.ac.il",
        "aff_unique_abbr": "AT&T Labs;TAU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;1;0",
        "aff_country_unique": "United States;Israel"
    }
]