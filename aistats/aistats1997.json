[
    {
        "id": "d2d9d02ac7",
        "title": "A Bayesian approach to CART",
        "site": "https://proceedings.mlr.press/r1/chipman97a.html",
        "author": "Hugh Chipman; Edward I. George; Robert E. McCulloch",
        "abstract": "A Bayesian approach for finding classification and regression tree (CART) models is proposed. By putting an appropriate prior distribution on the space of CART models, the resulting posterior will put higher probability on the more \"promising trees\". In particular, priors are proposed which penalize complexity by putting higher probability on trees with fewer nodes. Metropolis-Hastings algorithms are used to rapidly grow trees in such a way that the high posterior probability trees are more likely to be obtained. In effect, the algorithm performs a stochastic search for promising trees. Examples are used to illustrate the potential superiority of this approach over conventional greedy methods.",
        "bibtex": "@InProceedings{pmlr-vR1-chipman97a,\n  title = \t {A {B}ayesian approach to {CART}},\n  author =       {Chipman, Hugh and George, Edward I. and McCulloch, Robert E.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {91--102},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/chipman97a/chipman97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/chipman97a.html},\n  abstract = \t {A Bayesian approach for finding classification and regression tree (CART) models is proposed. By putting an appropriate prior distribution on the space of CART models, the resulting posterior will put higher probability on the more \"promising trees\". In particular, priors are proposed which penalize complexity by putting higher probability on trees with fewer nodes. Metropolis-Hastings algorithms are used to rapidly grow trees in such a way that the high posterior probability trees are more likely to be obtained. In effect, the algorithm performs a stochastic search for promising trees. Examples are used to illustrate the potential superiority of this approach over conventional greedy methods.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/chipman97a/chipman97a.pdf",
        "supp": "",
        "pdf_size": 6464265,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14049941543671633192&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "1fdbe586cc",
        "title": "A Characterization of Bayesian Network Structures and its Application to Leaming",
        "site": "https://proceedings.mlr.press/r1/forbes97a.html",
        "author": "James I. G. Forbes",
        "abstract": "We present an analysis of the minimal I-map relation between Bayesian network structures and dependency models. This includes a partial order characterisation of the structures, and the connection between the relation and the arc reversal operation. Two applications of this analysis are presented. The first is a simple condition for identifying equivalence between Bayesian network structures, and the second is an exact learning algorithm based on the partial order characterisation.",
        "bibtex": "@InProceedings{pmlr-vR1-forbes97a,\n  title = \t {A Characterization of Bayesian Network Structures and its Application to Leaming},\n  author =       {Forbes, James I. G.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {203--210},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/forbes97a/forbes97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/forbes97a.html},\n  abstract = \t {We present an analysis of the minimal I-map relation between Bayesian network structures and dependency models. This includes a partial order characterisation of the structures, and the connection between the relation and the arc reversal operation. Two applications of this analysis are presented. The first is a simple condition for identifying equivalence between Bayesian network structures, and the second is an exact learning algorithm based on the partial order characterisation.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/forbes97a/forbes97a.pdf",
        "supp": "",
        "pdf_size": 4654351,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15374220416412834567&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Computer Science Department, Monash University, Clayton, Victoria, 3168, AUSTRALIA",
        "aff_domain": "cs.monash.edu.au",
        "email": "cs.monash.edu.au",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Monash University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.monash.edu",
        "aff_unique_abbr": "Monash",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Clayton",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "63934cdfc5",
        "title": "A Comparison of Decision Trees, Influence Diagrams and Valuation Networks for Asymmetric Decision Problems",
        "site": "https://proceedings.mlr.press/r1/bielza97b.html",
        "author": "Concha Bielza; Prakash P. Shenoy",
        "abstract": "We compare three graphical techniques for representation and solution of asymmetric decision problems- decision trees, influence diagrams, and valuation networks. We solve a modified version of Covaliu and Oliver\u2019s Reactor problem using each of the three techniques. For each technique, we highlight the strengths, intrinsic weaknesses, and shortcomings that perhaps can be overcome by further research.",
        "bibtex": "@InProceedings{pmlr-vR1-bielza97b,\n  title = \t {A Comparison of Decision Trees, Influence Diagrams and Valuation Networks for Asymmetric Decision Problems},\n  author =       {Bielza, Concha and Shenoy, Prakash P.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {39--46},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/bielza97b/bielza97b.pdf},\n  url = \t {https://proceedings.mlr.press/r1/bielza97b.html},\n  abstract = \t {We compare three graphical techniques for representation and solution of asymmetric decision problems- decision trees, influence diagrams, and valuation networks. We solve a modified version of Covaliu and Oliver\u2019s Reactor problem using each of the three techniques. For each technique, we highlight the strengths, intrinsic weaknesses, and shortcomings that perhaps can be overcome by further research.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/bielza97b/bielza97b.pdf",
        "supp": "",
        "pdf_size": 4961638,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10180823869479330816&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Facultad de Informatica, Universidad Politecnica de Madrid; School of Business, University of Kansas",
        "aff_domain": "fi.upm.es;ukans.edu",
        "email": "fi.upm.es;ukans.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universidad Politecnica de Madrid;University of Kansas",
        "aff_unique_dep": "Facultad de Informatica;School of Business",
        "aff_unique_url": "https://www.upm.es;https://business.ku.edu",
        "aff_unique_abbr": "UPM;KU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Lawrence",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Spain;United States"
    },
    {
        "id": "4129a1112f",
        "title": "A Comparison of Scientific and Engineering Criteria for Bayesian Model Selection",
        "site": "https://proceedings.mlr.press/r1/heckerman97a.html",
        "author": "David Heckerman; David Maxwell Chickering",
        "abstract": "Given a set of possible model structures for variables $\\mathbf{X}$ and a set of possible parameters for each structure, the Bayesian \"estimate\" of the probability distribution for $\\mathbf{X}$ given observed data is obtained by averaging over the possible model structures and their parameters. An often-used approximation for this estimate is obtained by selecting a single model structure and averaging over its parameters. The approximation is useful because it is computationally efficient, and because it provides a model that facilitates understanding\u00b7 of the domain. A common criterion for model selection is the posterior probability of the model. Another criterion for model selection, proposed by San Martini and Spezzafari (1984) , is the predictive performance of a model for the next observation to be seen. From the standpoint of domain understanding, both criteria are useful, because one identifies the model that is most likely, whereas the other identifies the model that is the best predictor of the next observation. To highlight the difference, we refer to the posterior-probability and alternative criteria as the \\emph{scientific criterion} (SC) and \\emph{engineering criterion} (EC), respectively. When we are interested in predicting the next observation, the model-averaged estimate is at least as good as that produced by EC, which itself is at least as good as the estimate produced by SC. We show experimentally that, for Bayesian-network models containing discrete variables only, differences in predictive performance between the model-averaged estimate and EC and between EC and SC can be substantial.",
        "bibtex": "@InProceedings{pmlr-vR1-heckerman97a,\n  title = \t {A Comparison of Scientific and Engineering Criteria for {B}ayesian Model Selection},\n  author =       {Heckerman, David and Chickering, David Maxwell},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {275--282},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/heckerman97a/heckerman97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/heckerman97a.html},\n  abstract = \t {Given a set of possible model structures for variables $\\mathbf{X}$ and a set of possible parameters for each structure, the Bayesian \"estimate\" of the probability distribution for $\\mathbf{X}$ given observed data is obtained by averaging over the possible model structures and their parameters. An often-used approximation for this estimate is obtained by selecting a single model structure and averaging over its parameters. The approximation is useful because it is computationally efficient, and because it provides a model that facilitates understanding\u00b7 of the domain. A common criterion for model selection is the posterior probability of the model. Another criterion for model selection, proposed by San Martini and Spezzafari (1984) , is the predictive performance of a model for the next observation to be seen. From the standpoint of domain understanding, both criteria are useful, because one identifies the model that is most likely, whereas the other identifies the model that is the best predictor of the next observation. To highlight the difference, we refer to the posterior-probability and alternative criteria as the \\emph{scientific criterion} (SC) and \\emph{engineering criterion} (EC), respectively. When we are interested in predicting the next observation, the model-averaged estimate is at least as good as that produced by EC, which itself is at least as good as the estimate produced by SC. We show experimentally that, for Bayesian-network models containing discrete variables only, differences in predictive performance between the model-averaged estimate and EC and between EC and SC can be substantial.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/heckerman97a/heckerman97a.pdf",
        "supp": "",
        "pdf_size": 3404175,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1447501170657986393&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Microsoft Research; Microsoft Research",
        "aff_domain": "microsoft.com;microsoft.com",
        "email": "microsoft.com;microsoft.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Microsoft",
        "aff_unique_dep": "Microsoft Research",
        "aff_unique_url": "https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "MSR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "614f7164b8",
        "title": "A Distance Metric for Classification Trees",
        "site": "https://proceedings.mlr.press/r1/shannon97a.html",
        "author": "William D. Shannon; David Banks",
        "abstract": "CART is an unstable classifier resulting in significant changes in tree structure for small changes in the learning set (Breiman, Friedman et al. 1984; Breiman 1994). To address this problem, research into combining classifiers has increased significantly in the last few years (Breiman 1992; Wolpert 1992; Breiman 1994). These methods are of two basic types: concatenation uses the output from one classifier as input to the next classifier; parallel classifiers work on the same input data with the output from each classifier combined using regression or vote-counting techniques (Schurman 1996). These strategies greatly improve the predictive power of unstable classifiers. However, when the goal of the statistical analysis is to learn about the relationship between outcome and predictors, these strategies for combining classifiers are unacceptable since they produce a large number of trees, making interpretation difficult. We present a new method for combining classification trees which results in a single, interpretable tree. We begin by defining a distance metric between two trees based on the amount of rearrangement needed so that the structure of the two trees is identical. Using this distance metric, we develop the concept of the central, or median, tree structure and estimate it using a consensus rule. This tree is seen to be more centrally located than the tree fit to all the data. We finish by discussing future work including alternative methods for estimating the median tree, probability models, uses in data mining and meta-analysis, and performance measurements of the median tree.",
        "bibtex": "@InProceedings{pmlr-vR1-shannon97a,\n  title = \t {A Distance Metric for Classification Trees},\n  author =       {Shannon, William D. and Banks, David},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {457--464},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/shannon97a/shannon97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/shannon97a.html},\n  abstract = \t {CART is an unstable classifier resulting in significant changes in tree structure for small changes in the learning set (Breiman, Friedman et al. 1984; Breiman 1994). To address this problem, research into combining classifiers has increased significantly in the last few years (Breiman 1992; Wolpert 1992; Breiman 1994). These methods are of two basic types: concatenation uses the output from one classifier as input to the next classifier; parallel classifiers work on the same input data with the output from each classifier combined using regression or vote-counting techniques (Schurman 1996). These strategies greatly improve the predictive power of unstable classifiers. However, when the goal of the statistical analysis is to learn about the relationship between outcome and predictors, these strategies for combining classifiers are unacceptable since they produce a large number of trees, making interpretation difficult. We present a new method for combining classification trees which results in a single, interpretable tree. We begin by defining a distance metric between two trees based on the amount of rearrangement needed so that the structure of the two trees is identical. Using this distance metric, we develop the concept of the central, or median, tree structure and estimate it using a consensus rule. This tree is seen to be more centrally located than the tree fit to all the data. We finish by discussing future work including alternative methods for estimating the median tree, probability models, uses in data mining and meta-analysis, and performance measurements of the median tree.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/shannon97a/shannon97a.pdf",
        "supp": "",
        "pdf_size": 7415975,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5570153727904880059&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Washington University School of Medicine, Division of General Medical Sciences; Department of Statistics, Carnegie Mellon University",
        "aff_domain": "osler.wustl.edu; ",
        "email": "osler.wustl.edu; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Washington University School of Medicine;Carnegie Mellon University",
        "aff_unique_dep": "Division of General Medical Sciences;Department of Statistics",
        "aff_unique_url": "https://medicine.wustl.edu;https://www.cmu.edu",
        "aff_unique_abbr": "WUSM;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "5b59aaac07",
        "title": "A Family of Algorithms for Finding Temporal Structure in Data",
        "site": "https://proceedings.mlr.press/r1/oates97a.html",
        "author": "Tim Oates; Matthew J. Schmill; David Jensen; Paul R. Cohen",
        "abstract": "Finding patterns in temporally structured data is an important and difficult problem. Examples of temporally structured data include time series of economic indicators, distributed network status reports, and continuous streams such as flight recorder data. We have developed a family of algorithms for finding structure in multivariate, discrete-valued time series data (Oates & Cohen 1996b; Oates, Schmill, & Cohen 1996; Oates et al. 1995). In this paper, we introduce a new member of that family for handling event-based data, and offer an empirical characterization of a time series based algorithm.",
        "bibtex": "@InProceedings{pmlr-vR1-oates97a,\n  title = \t {A Family of Algorithms for Finding Temporal Structure in Data},\n  author =       {Oates, Tim and Schmill, Matthew J. and Jensen, David and Cohen, Paul R.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {371--378},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/oates97a/oates97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/oates97a.html},\n  abstract = \t {Finding patterns in temporally structured data is an important and difficult problem. Examples of temporally structured data include time series of economic indicators, distributed network status reports, and continuous streams such as flight recorder data. We have developed a family of algorithms for finding structure in multivariate, discrete-valued time series data (Oates & Cohen 1996b; Oates, Schmill, & Cohen 1996; Oates et al. 1995). In this paper, we introduce a new member of that family for handling event-based data, and offer an empirical characterization of a time series based algorithm.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/oates97a/oates97a.pdf",
        "supp": "",
        "pdf_size": 4436086,
        "gs_citation": 64,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6227368156381656935&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "30d77b505f",
        "title": "A Forward Monte Carlo Method for Solving Influence Diagrams using local Computation",
        "site": "https://proceedings.mlr.press/r1/charnes97a.html",
        "author": "John M. Charnes; Prakash P. Shenoy",
        "abstract": "The main goal of this paper is to describe a Monte Carlo method for solving influence diagrams using local computation. The forward Monte Carlo sampling technique draws independent and identically distributed observations. Methods that have been proposed in this spirit sample from the entire distribution. However, when the number of variables is large, the state space of all variables is exponentially large, and the sample size required for good estimates is too large to be practical. In the forward Monte Carlo method we generate observations from a subset of chance variables for each decision node in the influence diagram. We use methods developed for exact solution of influence diagrams to limit the number of chance variables sampled at any time. Because influence diagrams model each chance variable with a conditional probability distribution, the forward Monte Carlo solution method lends itself very well to influence-diagram representations.",
        "bibtex": "@InProceedings{pmlr-vR1-charnes97a,\n  title = \t {A Forward {M}onte {C}arlo Method for Solving Influence Diagrams using local Computation},\n  author =       {Charnes, John M. and Shenoy, Prakash P.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {75--82},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/charnes97a/charnes97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/charnes97a.html},\n  abstract = \t {The main goal of this paper is to describe a Monte Carlo method for solving influence diagrams using local computation. The forward Monte Carlo sampling technique draws independent and identically distributed observations. Methods that have been proposed in this spirit sample from the entire distribution. However, when the number of variables is large, the state space of all variables is exponentially large, and the sample size required for good estimates is too large to be practical. In the forward Monte Carlo method we generate observations from a subset of chance variables for each decision node in the influence diagram. We use methods developed for exact solution of influence diagrams to limit the number of chance variables sampled at any time. Because influence diagrams model each chance variable with a conditional probability distribution, the forward Monte Carlo solution method lends itself very well to influence-diagram representations.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/charnes97a/charnes97a.pdf",
        "supp": "",
        "pdf_size": 4749845,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2146992024220769018&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "School of Business University of Kansas; School of Business University of Kansas",
        "aff_domain": "ukans.edu;uka ns.ed u",
        "email": "ukans.edu;uka ns.ed u",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Kansas",
        "aff_unique_dep": "School of Business",
        "aff_unique_url": "https://business.ku.edu",
        "aff_unique_abbr": "KU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Lawrence",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7c5c436896",
        "title": "A Note on Cyclic Graphs and Dynamical Feedback Systems",
        "site": "https://proceedings.mlr.press/r1/richardson97b.html",
        "author": "Thomas S. Richardson; Peter Spirtes; Clark Glymour",
        "abstract": "Directed acyclic graphical (DAG) models were motivated in large part by the desire to have a general formalism to represent causal hypotheses and the restrictions on probability distributions they imply. DAG models exploited a fundamental kinship in a variety of statistical formalisms often treated as distinct \"models\": factor models, structural equation models, regression models, logistic regression models, survival models, etc. The fundamental connection is through either of two equivalent (for DAGs) properties, a \"local\" Markov condition, or the property of d-separation, sometimes called the \"global\" Markov condition. (Pearl 1988, Lauritzen et al. 1990). In much the same spirit, directed cyclic graphs (DCGs) have been introduced to represent the causal \u00b7structure of feedback processes and the restrictions on probability distributions those structures imply. Developments in our understanding of DCGs have proceeded so rapidly that it is appropriate to consider the prospects and limitations of cyclic representations of feedback systems. (For an alternative approach to extending graphical models to the temporal domain see Aliferis and Cooper, 1996)",
        "bibtex": "@InProceedings{pmlr-vR1-richardson97b,\n  title = \t {A Note on Cyclic Graphs and Dynamical Feedback Systems},\n  author =       {Richardson, Thomas S. and Spirtes, Peter and Glymour, Clark},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {421--428},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/richardson97b/richardson97b.pdf},\n  url = \t {https://proceedings.mlr.press/r1/richardson97b.html},\n  abstract = \t {Directed acyclic graphical (DAG) models were motivated in large part by the desire to have a general formalism to represent causal hypotheses and the restrictions on probability distributions they imply. DAG models exploited a fundamental kinship in a variety of statistical formalisms often treated as distinct \"models\": factor models, structural equation models, regression models, logistic regression models, survival models, etc. The fundamental connection is through either of two equivalent (for DAGs) properties, a \"local\" Markov condition, or the property of d-separation, sometimes called the \"global\" Markov condition. (Pearl 1988, Lauritzen et al. 1990). In much the same spirit, directed cyclic graphs (DCGs) have been introduced to represent the causal \u00b7structure of feedback processes and the restrictions on probability distributions those structures imply. Developments in our understanding of DCGs have proceeded so rapidly that it is appropriate to consider the prospects and limitations of cyclic representations of feedback systems. (For an alternative approach to extending graphical models to the temporal domain see Aliferis and Cooper, 1996)},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/richardson97b/richardson97b.pdf",
        "supp": "",
        "pdf_size": 3263303,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4403699158934180160&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "University of Washington; Carnegie Mellon University; University of California, San Diego + Carnegie Mellon University",
        "aff_domain": "stat.washington.edu;andrew.cmu.edu;andrew.cmu.edu",
        "email": "stat.washington.edu;andrew.cmu.edu;andrew.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2+1",
        "aff_unique_norm": "University of Washington;Carnegie Mellon University;University of California, San Diego",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.washington.edu;https://www.cmu.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "UW;CMU;UCSD",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "70683235b5",
        "title": "A Polynomial Time Algorithm for Determining DAG Equivalence in the Presence of Latent Variables and Selection Bias",
        "site": "https://proceedings.mlr.press/r1/spirtes97b.html",
        "author": "Peter Spirtes; Thomas S. Richardson",
        "abstract": "",
        "bibtex": "@InProceedings{pmlr-vR1-spirtes97b,\n  title = \t {A Polynomial Time Algorithm for Determining DAG Equivalence in the Presence of Latent Variables and Selection Bias},\n  author =       {Spirtes, Peter and Richardson, Thomas S.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {489--500},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/spirtes97b/spirtes97b.pdf},\n  url = \t {https://proceedings.mlr.press/r1/spirtes97b.html},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/spirtes97b/spirtes97b.pdf",
        "supp": "",
        "pdf_size": 8620586,
        "gs_citation": 77,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10101449712726692653&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Philosophy, Carnegie Mellon University; Department of Statistics, University of Washington",
        "aff_domain": "andrew.cmu.edu; ",
        "email": "andrew.cmu.edu; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;University of Washington",
        "aff_unique_dep": "Department of Philosophy;Department of Statistics",
        "aff_unique_url": "https://www.cmu.edu;https://www.washington.edu",
        "aff_unique_abbr": "CMU;UW",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "799e5cd39b",
        "title": "A Variational Approach to Bayesian Logistic Regression Models and their Extensions",
        "site": "https://proceedings.mlr.press/r1/jaakkola97a.html",
        "author": "Tommi S. Jaakkola; Michael I. Jordan",
        "abstract": "We consider a logistic regression model with a Gaussian prior distribution over the parameters. We show that accurate variational techniques can be used to obtain a closed form posterior distribution over the parameters given the data thereby yielding a posterior predictive model. The results are readily extended to (binary) belief networks. For belief networks we also derive closed form posteriors in the presence of missing values. Finally, we show that the dual of the regression problem gives a latent variable density model, the variational formulation of which leads to exactly solvable EM updates.",
        "bibtex": "@InProceedings{pmlr-vR1-jaakkola97a,\n  title = \t {A Variational Approach to {B}ayesian Logistic Regression Models and their Extensions},\n  author =       {Jaakkola, Tommi S. and Jordan, Michael I.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {283--294},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/jaakkola97a/jaakkola97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/jaakkola97a.html},\n  abstract = \t {We consider a logistic regression model with a Gaussian prior distribution over the parameters. We show that accurate variational techniques can be used to obtain a closed form posterior distribution over the parameters given the data thereby yielding a posterior predictive model. The results are readily extended to (binary) belief networks. For belief networks we also derive closed form posteriors in the presence of missing values. Finally, we show that the dual of the regression problem gives a latent variable density model, the variational formulation of which leads to exactly solvable EM updates.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/jaakkola97a/jaakkola97a.pdf",
        "supp": "",
        "pdf_size": 4793579,
        "gs_citation": 337,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4615612703475250937&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology; Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology",
        "aff_domain": "psyche.mit.edu;psyche.mit.edu",
        "email": "psyche.mit.edu;psyche.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Department of Brain and Cognitive Sciences",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7a6cabe85f",
        "title": "Adjusting for Multiple Testing in Decision Tree Pruning",
        "site": "https://proceedings.mlr.press/r1/jensen97a.html",
        "author": "David Jensen",
        "abstract": "Overfitting is a widely observed pathology of induction algorithms. Overfitted models contain unnecessary structure that reflects nothing more than random variation in the data sample used to construct the model. Portions of these models are literally wrong, and can mislead users. Overfitted models are less efficient to store and use than their correctly-sized counterparts. Finally, overfitting can reduce the accuracy of induced models on new data [14, 7]. For induction algorithms that build decision trees [1, 13, 15], pruning is a common approach to correct overfitting. Pruning techniques take an induced tree, examine individual subtrees, and remove those subtrees deemed to be unnecessary. While pruning techniques can differ in several respects, they primarily differ in the criterion used to judge subtrees. Many criteria have been proposed, including statistical significance tests [13], corrected error estimates [15], and minimum description length calculations [12]. Most common pruning techniques, however, do not account for one potentially important factor - multiple testing. Multiple testing occurs whenever an induction algorithm examines several candidate models and selects the one that best accords with the data. Any search process necessarily involves multiple testing, and most common induction algorithms involve implicit or explicit search through a space of candidate models. In the case of decision trees, search involves examining many possible subtrees and selecting the best one. Pruning techniques need to account for the number of subtrees examined, because such multiple testing affects the apparent accuracy of models on training data [8]. This paper examines the importance of adjusting for multiple testing. Specifically, it examines the effectiveness of one particular pruning method - \\emph{bonferroni pruning}. Bonferroni pruning adjusts the results of a standard significance test to account for the number of subtrees examined at a particular node of a decision tree. Evidence that bonferroni pruning leads to better models supports the hypothesis that multiple testing is an important cause of overfitting.",
        "bibtex": "@InProceedings{pmlr-vR1-jensen97a,\n  title = \t {Adjusting for Multiple Testing in Decision Tree Pruning},\n  author =       {Jensen, David},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {295--302},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/jensen97a/jensen97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/jensen97a.html},\n  abstract = \t {Overfitting is a widely observed pathology of induction algorithms. Overfitted models contain unnecessary structure that reflects nothing more than random variation in the data sample used to construct the model. Portions of these models are literally wrong, and can mislead users. Overfitted models are less efficient to store and use than their correctly-sized counterparts. Finally, overfitting can reduce the accuracy of induced models on new data [14, 7]. For induction algorithms that build decision trees [1, 13, 15], pruning is a common approach to correct overfitting. Pruning techniques take an induced tree, examine individual subtrees, and remove those subtrees deemed to be unnecessary. While pruning techniques can differ in several respects, they primarily differ in the criterion used to judge subtrees. Many criteria have been proposed, including statistical significance tests [13], corrected error estimates [15], and minimum description length calculations [12]. Most common pruning techniques, however, do not account for one potentially important factor - multiple testing. Multiple testing occurs whenever an induction algorithm examines several candidate models and selects the one that best accords with the data. Any search process necessarily involves multiple testing, and most common induction algorithms involve implicit or explicit search through a space of candidate models. In the case of decision trees, search involves examining many possible subtrees and selecting the best one. Pruning techniques need to account for the number of subtrees examined, because such multiple testing affects the apparent accuracy of models on training data [8]. This paper examines the importance of adjusting for multiple testing. Specifically, it examines the effectiveness of one particular pruning method - \\emph{bonferroni pruning}. Bonferroni pruning adjusts the results of a standard significance test to account for the number of subtrees examined at a particular node of a decision tree. Evidence that bonferroni pruning leads to better models supports the hypothesis that multiple testing is an important cause of overfitting.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/jensen97a/jensen97a.pdf",
        "supp": "",
        "pdf_size": 4396421,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18037853821163373624&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "e8c15108d5",
        "title": "An Algorithm for Bayesian Network Construction from Data",
        "site": "https://proceedings.mlr.press/r1/cheng97a.html",
        "author": "Jie Cheng; David A. Bell; Weiru Liu",
        "abstract": "This paper presents an efficient algorithm for constructing Bayesian belief networks from databases. The algorithm takes a database and an attributes ordering (i.e., the causal attributes of an attribute should appear earlier in the order) as input and constructs a belief network structure as output. The construction process is based on the computation of mutual information and cross entropy of attribute pairs. This algorithm guarantees that the \\emph{minimal Independent map} [1] of the underlying dependency model is generated, and at the same time, enjoys the time complexity of $O(N^2)$ on conditional independence (Cl) tests. To evaluate this algorithm, we present the experimental results on three versions of the well-known ALARM network database, which has 37 attributes and 10,000 records. The correctness proof and the analysis of computational complexity are also presented. We also discuss the features ofour work and relate it to previous works.",
        "bibtex": "@InProceedings{pmlr-vR1-cheng97a,\n  title = \t {An Algorithm for Bayesian Network Construction from Data},\n  author =       {Cheng, Jie and Bell, David A. and Liu, Weiru},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {83--90},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/cheng97a/cheng97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/cheng97a.html},\n  abstract = \t {This paper presents an efficient algorithm for constructing Bayesian belief networks from databases. The algorithm takes a database and an attributes ordering (i.e., the causal attributes of an attribute should appear earlier in the order) as input and constructs a belief network structure as output. The construction process is based on the computation of mutual information and cross entropy of attribute pairs. This algorithm guarantees that the \\emph{minimal Independent map} [1] of the underlying dependency model is generated, and at the same time, enjoys the time complexity of $O(N^2)$ on conditional independence (Cl) tests. To evaluate this algorithm, we present the experimental results on three versions of the well-known ALARM network database, which has 37 attributes and 10,000 records. The correctness proof and the analysis of computational complexity are also presented. We also discuss the features ofour work and relate it to previous works.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/cheng97a/cheng97a.pdf",
        "supp": "",
        "pdf_size": 4539921,
        "gs_citation": 362,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14570643927310487090&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Information and Software Engineering, University of Ulster at Jordanstown, Northern Ireland, UK, BT37 OQB; School of Information and Software Engineering, University of Ulster at Jordanstown, Northern Ireland, UK, BT37 OQB; School of Information and Software Engineering, University of Ulster at Jordanstown, Northern Ireland, UK, BT37 OQB",
        "aff_domain": "ulst.ac.uk;ulst.ac.uk;ulst.ac.uk",
        "email": "ulst.ac.uk;ulst.ac.uk;ulst.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Ulster",
        "aff_unique_dep": "School of Information and Software Engineering",
        "aff_unique_url": "https://www.ulster.ac.uk",
        "aff_unique_abbr": "UU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Jordanstown",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "e27c4f2fe6",
        "title": "An Incremental Construction of a Nonparametric Regression Model",
        "site": "https://proceedings.mlr.press/r1/smid97a.html",
        "author": "Jan Smid; Petr Volf",
        "abstract": "",
        "bibtex": "@InProceedings{pmlr-vR1-smid97a,\n  title = \t {An Incremental Construction of a Nonparametric Regression Model},\n  author =       {Smid, Jan and Volf, Petr},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {465--472},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/smid97a/smid97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/smid97a.html},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/smid97a/smid97a.pdf",
        "supp": "",
        "pdf_size": 3655229,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=171339994561875759&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department. of Mathematics, Morgan State University; UTIA, The Czech Academy of Sciences",
        "aff_domain": "gsfc.nasa.gov;utia.cas.cz",
        "email": "gsfc.nasa.gov;utia.cas.cz",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Morgan State University;Czech Academy of Sciences",
        "aff_unique_dep": "Department of Mathematics;UTIA",
        "aff_unique_url": "https://www.morganstate.edu;https://www.cas.cz",
        "aff_unique_abbr": "MSU;CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Czech Republic"
    },
    {
        "id": "f0d26f7619",
        "title": "An Objective Function for Belief Net Triangulation",
        "site": "https://proceedings.mlr.press/r1/meila97a.html",
        "author": "Marina Meil\u0103; Michael I. Jordan",
        "abstract": "This paper presents a new approach to the triangulation of belief networks. Triangulation is a combinatorial optimization problem; our idea is to embed its discrete domain into a continuous domain e. Then, by suitably extending the objective function over e, we can make use of continuous optimization techniques to do the minimization. We used an upper bound of the total junction tree weight as the cost function. The appropriateness of this choice is discussed and explored by simulations.",
        "bibtex": "@InProceedings{pmlr-vR1-meila97a,\n  title = \t {An Objective Function for Belief Net Triangulation},\n  author =       {Meil\\u{a}, Marina and Jordan, Michael I.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {355--362},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/meila97a/meila97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/meila97a.html},\n  abstract = \t {This paper presents a new approach to the triangulation of belief networks. Triangulation is a combinatorial optimization problem; our idea is to embed its discrete domain into a continuous domain e. Then, by suitably extending the objective function over e, we can make use of continuous optimization techniques to do the minimization. We used an upper bound of the total junction tree weight as the cost function. The appropriateness of this choice is discussed and explored by simulations.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/meila97a/meila97a.pdf",
        "supp": "",
        "pdf_size": 2623003,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17898711088615683879&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "add9ff5c47",
        "title": "Applying a Gaussian-Bernoulli Mixture Model Network to Binary and Continuous Missing Data in Medicine",
        "site": "https://proceedings.mlr.press/r1/rosen97a.html",
        "author": "David B. Rosen; Harry B. Burke",
        "abstract": "We wish to train a feedforward projective-sigmoidal neural network (MLP) on breast cancer outcomes data missing both binary and continuous input variable values. A Gaussian-Bernoulli mixture model is trained on the data (using EM). It then performs stochastic imputation (filling in) of the missing values, as a preprocessor to the MLP. In order to compare predictive accuracy when the training data are complete vs. incomplete/imputed, we use only complete cases from a natural data set, but artificially remove 80% of their input data values. Very little difference is observed in the comparison, suggesting that the mixture model is quite effective here, despite the fact that more than 99% of the casesfmstances had had some missing value(s). The mixture model can be used both for output/outcome prediction by a trained MLP and for the training process itself.",
        "bibtex": "@InProceedings{pmlr-vR1-rosen97a,\n  title = \t {Applying a {G}aussian-{B}ernoulli Mixture Model Network to Binary and Continuous Missing Data in Medicine},\n  author =       {Rosen, David B. and Burke, Harry B.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {429--436},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/rosen97a/rosen97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/rosen97a.html},\n  abstract = \t {We wish to train a feedforward projective-sigmoidal neural network (MLP) on breast cancer outcomes data missing both binary and continuous input variable values. A Gaussian-Bernoulli mixture model is trained on the data (using EM). It then performs stochastic imputation (filling in) of the missing values, as a preprocessor to the MLP. In order to compare predictive accuracy when the training data are complete vs. incomplete/imputed, we use only complete cases from a natural data set, but artificially remove 80% of their input data values. Very little difference is observed in the comparison, suggesting that the mixture model is quite effective here, despite the fact that more than 99% of the casesfmstances had had some missing value(s). The mixture model can be used both for output/outcome prediction by a trained MLP and for the training process itself. },\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/rosen97a/rosen97a.pdf",
        "supp": "",
        "pdf_size": 3209446,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12952644843751102293&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Medicine, New York Medical College, Valhalla, New York 10595 USA; Department of Medicine, New York Medical College, Valhalla, New York 10595 USA",
        "aff_domain": "ieee.org;unr.edu",
        "email": "ieee.org;unr.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "New York Medical College",
        "aff_unique_dep": "Department of Medicine",
        "aff_unique_url": "https://nymc.edu",
        "aff_unique_abbr": "NYMC",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Valhalla",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "8231671412",
        "title": "Approximate Inference and Forecast Algorithms in Graphical Models for Partially Observed Dynamic Systems",
        "site": "https://proceedings.mlr.press/r1/lekuona97a.html",
        "author": "Alberto Lekuona; Beatrix Lacruz; Pilar Lasala",
        "abstract": "From a statistical point of view , modelling stochastic temporal processes by graphical models is a suitable choice, specially when certain standard assumptions in classical modelling cannot be assumed. Focusing the discussion on partially observed domains, it is important to design algorithms which provide probability distributions over the current and future states of the non-observable components of the domain, using the information stored in the observable components. In this paper, we present a simulation algorithm for approximating the exact probability distributions associated with such inference and forecast processes. This algorithm uses both the probabilities built at the previous time step and the new evidence obtained to propose new probability distributions associated with current and future states of the domain. To validate the algorithm, a case study of equipment maintenance is considered.",
        "bibtex": "@InProceedings{pmlr-vR1-lekuona97a,\n  title = \t {Approximate Inference and Forecast Algorithms in Graphical Models for Partially Observed Dynamic Systems},\n  author =       {Lekuona, Alberto and Lacruz, Beatrix and Lasala, Pilar},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {319--319},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/lekuona97a/lekuona97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/lekuona97a.html},\n  abstract = \t {From a statistical point of view , modelling stochastic temporal processes by graphical models is a suitable choice, specially when certain standard assumptions in classical modelling cannot be assumed. Focusing the discussion on partially observed domains, it is important to design algorithms which provide probability distributions over the current and future states of the non-observable components of the domain, using the information stored in the observable components. In this paper, we present a simulation algorithm for approximating the exact probability distributions associated with such inference and forecast processes. This algorithm uses both the probabilities built at the previous time step and the new evidence obtained to propose new probability distributions associated with current and future states of the domain. To validate the algorithm, a case study of equipment maintenance is considered.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/lekuona97a/lekuona97a.pdf",
        "supp": "",
        "pdf_size": 3266374,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3037987984580100705&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Departamento de Metodos Estadisticos, Universidad de Zaragoza (Spain); Departamento de Metodos Estadisticos, Universidad de Zaragoza (Spain); Departamento de Metodos Estadisticos, Universidad de Zaragoza (Spain)",
        "aff_domain": "posta.unizar.es;posta.unizar.es;posta.unizar.es",
        "email": "posta.unizar.es;posta.unizar.es;posta.unizar.es",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Universidad de Zaragoza",
        "aff_unique_dep": "Departamento de Metodos Estadisticos",
        "aff_unique_url": "https://www.unizar.es",
        "aff_unique_abbr": "UZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Spain"
    },
    {
        "id": "4d5f8f2066",
        "title": "Asessing and Improving Classification Rules",
        "site": "https://proceedings.mlr.press/r1/hand97a.html",
        "author": "David J. Hand; Kerning Yu; Niall Ada",
        "abstract": "The last few years have witnessed a resurgence of research effort aimed at developing improved techniques for supervised classification problems. In a large part this resurgence of interest has been stimulated by the novelty of multi-layer feedforward neural networks (Hertz et al, 1991; Ripley, 1996) and similar complex and flexible models such as MARS (Friedman, 1991), projection pursuit regression (Friedman and Stuetzle, 1981), and additive models in general (Hastie and Tibshirani, 1990)). The flexibility of these models is in striking contrast to the simplicity of models such as simple linear discriminant analysis, perceptrons, and logistic discriminant analysis, which assume highly restricted forms of decision surface. The merit of the flexibility of neural networks is countered by the dangers that they will overfit the design data. This relationship between model flexibility and the danger of overfitting has long been understood within the statistical community. For example, in the 1960s the optimistic bias of resubstitution error rate became widely recognised and it was replaced by the leave-one-out method as the method of choice. (Later, in the 1980s, an apparently large variance of the latter led to its being abandoned in favour of bootstrap methods, in particular the 632 bootstrap.) Early work on neural networks also fell into this trap, producing inflated claims of the performance of such models .derived from optimistic performance measures based on overfitting the design set. In recent years the risk has been recognised, and some sophisticated proposals have been made for overcoming the problem. They are based on ideas such as penalising the goodness of fit measure (by combining it with a measure of model complexity), restricting the form of the model (to few nodes in a network, for example), shrinking an overfitted model .(by weight decay, for example), or even by adding randomly perturbed replicates to the design set. The problem with all such methods is \\emph{how} to strike the optimum compromise between modelling the design data and overfitting.",
        "bibtex": "@InProceedings{pmlr-vR1-hand97a,\n  title = \t {Asessing and Improving Classification Rules},\n  author =       {Hand, David J. and Yu, Kerning and Ada, Niall},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {243--254},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/hand97a/hand97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/hand97a.html},\n  abstract = \t {The last few years have witnessed a resurgence of research effort aimed at developing improved techniques for supervised classification problems. In a large part this resurgence of interest has been stimulated by the novelty of multi-layer feedforward neural networks (Hertz et al, 1991; Ripley, 1996) and similar complex and flexible models such as MARS (Friedman, 1991), projection pursuit regression (Friedman and Stuetzle, 1981), and additive models in general (Hastie and Tibshirani, 1990)). The flexibility of these models is in striking contrast to the simplicity of models such as simple linear discriminant analysis, perceptrons, and logistic discriminant analysis, which assume highly restricted forms of decision surface. The merit of the flexibility of neural networks is countered by the dangers that they will overfit the design data. This relationship between model flexibility and the danger of overfitting has long been understood within the statistical community. For example, in the 1960s the optimistic bias of resubstitution error rate became widely recognised and it was replaced by the leave-one-out method as the method of choice. (Later, in the 1980s, an apparently large variance of the latter led to its being abandoned in favour of bootstrap methods, in particular the 632 bootstrap.) Early work on neural networks also fell into this trap, producing inflated claims of the performance of such models .derived from optimistic performance measures based on overfitting the design set. In recent years the risk has been recognised, and some sophisticated proposals have been made for overcoming the problem. They are based on ideas such as penalising the goodness of fit measure (by combining it with a measure of model complexity), restricting the form of the model (to few nodes in a network, for example), shrinking an overfitted model .(by weight decay, for example), or even by adding randomly perturbed replicates to the design set. The problem with all such methods is \\emph{how} to strike the optimum compromise between modelling the design data and overfitting.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/hand97a/hand97a.pdf",
        "supp": "",
        "pdf_size": 5087214,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5903659380511382222&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Statistics, The Open University; Department of Statistics, The Open University; Department of Statistics, The Open University",
        "aff_domain": "open.ac.uk;open.ac.uk;open.ac.uk",
        "email": "open.ac.uk;open.ac.uk;open.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Open University",
        "aff_unique_dep": "Department of Statistics",
        "aff_unique_url": "https://www.open.ac.uk",
        "aff_unique_abbr": "OU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "97d9b5367c",
        "title": "Bayesian Information Retrieval: Preliminary Evaluation",
        "site": "https://proceedings.mlr.press/r1/keim97a.html",
        "author": "Michelle Keim; David D. Lewis; David Madigan",
        "abstract": "Given a database of documents and a user\u2019s query, how can we locate those documents that meet the user\u2019s information needs? Because there is no precise definition of which documents in the database match the user\u2019s query, uncertainty is inherent in the information retrieval process. Therefore, probability theory is a natural tool for formalizing the retrieval task. In this paper, we propose a Bayesian approach to one of the conventional probabilistic information retrieval models. We discuss the motivation for such a model, describe its implementation, and present some experimental results.",
        "bibtex": "@InProceedings{pmlr-vR1-keim97a,\n  title = \t {Bayesian Information Retrieval: Preliminary Evaluation},\n  author =       {Keim, Michelle and Lewis, David D. and Madigan, David},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {303--318},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/keim97a/keim97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/keim97a.html},\n  abstract = \t {Given a database of documents and a user\u2019s query, how can we locate those documents that meet the user\u2019s information needs? Because there is no precise definition of which documents in the database match the user\u2019s query, uncertainty is inherent in the information retrieval process. Therefore, probability theory is a natural tool for formalizing the retrieval task. In this paper, we propose a Bayesian approach to one of the conventional probabilistic information retrieval models. We discuss the motivation for such a model, describe its implementation, and present some experimental results.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/keim97a/keim97a.pdf",
        "supp": "",
        "pdf_size": 4998961,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1738504326290268533&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 4,
        "aff": "Dept. of Statistics, University of Washington; AT&T Labs - Research; Dept. of Statistics, University of Washington",
        "aff_domain": "stat.washington.edu;research.att.com;stat.washington.edu",
        "email": "stat.washington.edu;research.att.com;stat.washington.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Washington;AT&T Labs",
        "aff_unique_dep": "Department of Statistics;Research",
        "aff_unique_url": "https://www.washington.edu;https://www.att.com/labs/research",
        "aff_unique_abbr": "UW;AT&T Labs",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2007375c35",
        "title": "Bayesian Model Averaging in Rule Induction",
        "site": "https://proceedings.mlr.press/r1/domingos97a.html",
        "author": "Pedro Domingos",
        "abstract": "Bayesian model averaging (BMA) can be seen as the optimal approach to any induction task. It can reduce error by accounting for model uncertainty in a principled way, and its usefulness in several areas has been empirically verified. However, few attempts to apply it to rule induction have been made. This paper reports a series of experiments designed to test the utility of BMA in this field. BMA is applied to combining multiple rule sets learned from different subsets of the training data, to combining multiple rules covering a test example, to inducing technical rules for foreign exchange trading, and to inducing conjunctive concepts. In the first two cases, BMA is observed to produce lower accuracies than the ad hoc methods it is compared with. In the last two cases, BMA is observed to typically produce the same result as simply using the best (maximum-likelihood) rule, even though averaging is performed over all possible rules in the space, the domains are highly noisy, and the samples are medium- to small-sized. In all cases, this is observed to be due to BMA\u2019s consistent tendency to assign highly asymmetric weights to different models, even when their accuracy differs by little, with most models (often all but one) effectively having no influence on the outcome. Thus the effective number of models being averaged is much smaller for BMA than for common ad hoc methods, leading to a smaller reduction in variance. This suggests that the success of the multiple models approach to rule induction is primarily due to this variance reduction, and not to its being a closer approximation to the Bayesian ideal.",
        "bibtex": "@InProceedings{pmlr-vR1-domingos97a,\n  title = \t {Bayesian Model Averaging in Rule Induction},\n  author =       {Domingos, Pedro},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {157--164},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/domingos97a/domingos97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/domingos97a.html},\n  abstract = \t {Bayesian model averaging (BMA) can be seen as the optimal approach to any induction task. It can reduce error by accounting for model uncertainty in a principled way, and its usefulness in several areas has been empirically verified. However, few attempts to apply it to rule induction have been made. This paper reports a series of experiments designed to test the utility of BMA in this field. BMA is applied to combining multiple rule sets learned from different subsets of the training data, to combining multiple rules covering a test example, to inducing technical rules for foreign exchange trading, and to inducing conjunctive concepts. In the first two cases, BMA is observed to produce lower accuracies than the ad hoc methods it is compared with. In the last two cases, BMA is observed to typically produce the same result as simply using the best (maximum-likelihood) rule, even though averaging is performed over all possible rules in the space, the domains are highly noisy, and the samples are medium- to small-sized. In all cases, this is observed to be due to BMA\u2019s consistent tendency to assign highly asymmetric weights to different models, even when their accuracy differs by little, with most models (often all but one) effectively having no influence on the outcome. Thus the effective number of models being averaged is much smaller for BMA than for common ad hoc methods, leading to a smaller reduction in variance. This suggests that the success of the multiple models approach to rule induction is primarily due to this variance reduction, and not to its being a closer approximation to the Bayesian ideal.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/domingos97a/domingos97a.pdf",
        "supp": "",
        "pdf_size": 4194337,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1601873995271959646&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Information and Computer Science, University of California, Irvine",
        "aff_domain": "ics.uci.edu",
        "email": "ics.uci.edu",
        "github": "",
        "project": "http://www.ics.uci.edu/-pedrod",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of California, Irvine",
        "aff_unique_dep": "Department of Information and Computer Science",
        "aff_unique_url": "https://www.uci.edu",
        "aff_unique_abbr": "UCI",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Irvine",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2a77028575",
        "title": "Building an EDA Assistant: A Progress Report",
        "site": "https://proceedings.mlr.press/r1/amant97a.html",
        "author": "Robert St. Amant; Paul R. Cohen",
        "abstract": "Since 1993 we have been working on a system to help people with exploratory data analysis (EDA). AIDE, an Assistant for Intelligent Data Exploration, is a knowledge-based planning system that incrementally explores a dataset, guided by user directives and its own evaluation of indications in the data. Its plan library contains strategies for generating and interpreting indications in data, selecting techniques to build appropriate descriptions of data, carrying out relevant procedures, and combining individual results into a coherent larger picture. The system is mixed-initiative, autonomously pursuing high- and low-level goals while still allowing the user to inform or override its decisions. Elsewhere we have described AIDE\u2019s operations and primitive data structures [22], its planning representation [23], its user interface [25, 24], and the system as a whole [21]. This progress report discusses a recent evaluation we conducted with AIDE and explains why we believe that this line of research is important to AI and statistics researchers. We will begin with a very brief overview of the system. The bulk of the paper describes the evaluation, our analysis of the results, and the lessons we learned through the experience of building and evaluating AIDE. We end with a discussion of the generality of our results and the potential for future work.",
        "bibtex": "@InProceedings{pmlr-vR1-amant97a,\n  title = \t {Building an EDA Assistant: A Progress Report},\n  author =       {Amant, Robert St. and Cohen, Paul R.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {501--512},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/amant97a/amant97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/amant97a.html},\n  abstract = \t {Since 1993 we have been working on a system to help people with exploratory data analysis (EDA). AIDE, an Assistant for Intelligent Data Exploration, is a knowledge-based planning system that incrementally explores a dataset, guided by user directives and its own evaluation of indications in the data. Its plan library contains strategies for generating and interpreting indications in data, selecting techniques to build appropriate descriptions of data, carrying out relevant procedures, and combining individual results into a coherent larger picture. The system is mixed-initiative, autonomously pursuing high- and low-level goals while still allowing the user to inform or override its decisions. Elsewhere we have described AIDE\u2019s operations and primitive data structures [22], its planning representation [23], its user interface [25, 24], and the system as a whole [21]. This progress report discusses a recent evaluation we conducted with AIDE and explains why we believe that this line of research is important to AI and statistics researchers. We will begin with a very brief overview of the system. The bulk of the paper describes the evaluation, our analysis of the results, and the lessons we learned through the experience of building and evaluating AIDE. We end with a discussion of the generality of our results and the potential for future work.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/amant97a/amant97a.pdf",
        "supp": "",
        "pdf_size": 7060127,
        "gs_citation": 3,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=944543308315689536&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Computer Science, North Carolina State University; Computer Science Department, LGRC, University of Massachusetts",
        "aff_domain": "csc.ncsu.edu;cs.umass.edu",
        "email": "csc.ncsu.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "North Carolina State University;University of Massachusetts",
        "aff_unique_dep": "Department of Computer Science;Computer Science Department",
        "aff_unique_url": "https://www.ncsu.edu;https://www.umass.edu",
        "aff_unique_abbr": "NCSU;UMass",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Amherst",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7c6a82463b",
        "title": "Case-based Probability Factoring in Bayesian Belief Networks",
        "site": "https://proceedings.mlr.press/r1/portinale97a.html",
        "author": "Luigi Portinale",
        "abstract": "Bayesian network inference can be formulated as a combinatorial optimization problem, concerning in the computation of an optimal factoring for the distribution represented in the net. Since the determination of an optimal factoring is a computationally hard problem, heuristic greedy strategies able to find approximations of the optimal factoring are usually adopted. In the present paper we investigate an alternative approach based on a combination of genetic algorithms (GA) and case-based reasoning (CBR). We show how the use of genetic algorithms can improve the quality of the computed factoring in case a static strategy is used (as for the MPE computation), while the combination of GA and CBR can still provide advantages in the case of dynamic strategies. Some preliminary results on different kinds of nets are then reported.",
        "bibtex": "@InProceedings{pmlr-vR1-portinale97a,\n  title = \t {Case-based Probability Factoring in Bayesian Belief Networks},\n  author =       {Portinale, Luigi},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {391--398},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/portinale97a/portinale97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/portinale97a.html},\n  abstract = \t {Bayesian network inference can be formulated as a combinatorial optimization problem, concerning in the computation of an optimal factoring for the distribution represented in the net. Since the determination of an optimal factoring is a computationally hard problem, heuristic greedy strategies able to find approximations of the optimal factoring are usually adopted. In the present paper we investigate an alternative approach based on a combination of genetic algorithms (GA) and case-based reasoning (CBR). We show how the use of genetic algorithms can improve the quality of the computed factoring in case a static strategy is used (as for the MPE computation), while the combination of GA and CBR can still provide advantages in the case of dynamic strategies. Some preliminary results on different kinds of nets are then reported.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/portinale97a/portinale97a.pdf",
        "supp": "",
        "pdf_size": 4171106,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:n6-4nlN0KE0J:scholar.google.com/&scioq=Case-based+Probability+Factoring+in+Bayesian+Belief+Networks&hl=en&as_sdt=0,33",
        "gs_version_total": 9,
        "aff": "Dipartimento di Informatica - Universita' di Torino",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Universita' di Torino",
        "aff_unique_dep": "Dipartimento di Informatica",
        "aff_unique_url": "https://www.unito.it",
        "aff_unique_abbr": "Unito",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Torino",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Italy"
    },
    {
        "id": "570c4a0d25",
        "title": "Combining Neural Network Regression Estimates Using Principal Components",
        "site": "https://proceedings.mlr.press/r1/merz97a.html",
        "author": "Christopher J. Merz; Michael J. Pazzani",
        "abstract": "Combining a set of learned models1 to improve classification and regression estimates has been an area ofmuch research in machine learning and neural net- works ([Wolpert92, Merz95 , PerroneCooper92 , LeblancTibshirani93, Breiman92, Meir95, Krogh95, Tresp95, ChanStolfo95]). The challenge of this problem is to decide which models to rely on for prediction and how much weight to give each. The goal of combining learned models is to obtain a more accurate predic- tion than can be obtained from any single source alone. One major issue in combining a set of learned models is redundancy. Redundancy refers to the amount of agreement or linear dependence between models when making a set of predictions\u00a0 The more the set agrees, the more redundancy is present. In statistical terms, this is referred to as the multicollinearity problem. The focus of this paper is to describe and evaluate an approach for combining regression estimates based on principal components regression. The method, called PCR*, is then evaluated on several real-world domains to demonstrate its robustness versus a collection of existing techniques.",
        "bibtex": "@InProceedings{pmlr-vR1-merz97a,\n  title = \t {Combining Neural Network Regression Estimates Using Principal Components},\n  author =       {Merz, Christopher J. and Pazzani, Michael J.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {363--370},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/merz97a/merz97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/merz97a.html},\n  abstract = \t {Combining a set of learned models1 to improve classification and regression estimates has been an area ofmuch research in machine learning and neural net- works ([Wolpert92, Merz95 , PerroneCooper92 , LeblancTibshirani93, Breiman92, Meir95, Krogh95, Tresp95, ChanStolfo95]). The challenge of this problem is to decide which models to rely on for prediction and how much weight to give each. The goal of combining learned models is to obtain a more accurate predic- tion than can be obtained from any single source alone. One major issue in combining a set of learned models is redundancy. Redundancy refers to the amount of agreement or linear dependence between models when making a set of predictions\u00a0 The more the set agrees, the more redundancy is present. In statistical terms, this is referred to as the multicollinearity problem. The focus of this paper is to describe and evaluate an approach for combining regression estimates based on principal components regression. The method, called PCR*, is then evaluated on several real-world domains to demonstrate its robustness versus a collection of existing techniques.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/merz97a/merz97a.pdf",
        "supp": "",
        "pdf_size": 2483931,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3243938878744126736&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 2,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "bdff447e7e",
        "title": "Comparing Predictive Inference Methods for Discrete Domains",
        "site": "https://proceedings.mlr.press/r1/kontkanen97a.html",
        "author": "Petri Kontkanen; Petri Myllym\u00e4ki; Tomi Silander; Henry Tirri; Peter Gr\u00fcnwald",
        "abstract": "Predictive inference is seen here as the process of determining the predictive distribution of a discrete variable, given a data set of training examples and the values for the other problem domain variables. We consider three approaches for computing this predictive distribution, and assume that the joint probability distribution for the variables belongs to a set of distributions determined by a set of parametric models. In the simplest case, the predictive distribution is computed by using the model with the \\emph{maximum a posteriori (MAP)} posterior probability. In the \\emph{evidence} approach, the predictive distribution is obtained by averaging over all the individual models.in the model family. In the third case, we define the predictive distribution by using Rissanen\u2019s new definition of \\emph{stochastic complexity}. Our experiments performed with the family of Naive Bayes models suggest that when using all the data available, the stochastic complexity approach produces the most accurate predictions in the log-score sense. However, when the amount of available training data is decreased, the evidence approach clearly outperforms the two other approaches. The MAP predictive distribution is clearly inferior in the log-score sense to the two more sophisticated approaches, but for the 0/1-score the MAP approach may still in some cases produce the best results.",
        "bibtex": "@InProceedings{pmlr-vR1-kontkanen97a,\n  title = \t {Comparing Predictive Inference Methods for Discrete Domains},\n  author =       {Kontkanen, Petri and Myllym\\\"aki, Petri and Silander, Tomi and Tirri, Henry and Gr\\\"unwald, Peter},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {311--318},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/kontkanen97a/kontkanen97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/kontkanen97a.html},\n  abstract = \t {Predictive inference is seen here as the process of determining the predictive distribution of a discrete variable, given a data set of training examples and the values for the other problem domain variables. We consider three approaches for computing this predictive distribution, and assume that the joint probability distribution for the variables belongs to a set of distributions determined by a set of parametric models. In the simplest case, the predictive distribution is computed by using the model with the \\emph{maximum a posteriori (MAP)} posterior probability. In the \\emph{evidence} approach, the predictive distribution is obtained by averaging over all the individual models.in the model family. In the third case, we define the predictive distribution by using Rissanen\u2019s new definition of \\emph{stochastic complexity}. Our experiments performed with the family of Naive Bayes models suggest that when using all the data available, the stochastic complexity approach produces the most accurate predictions in the log-score sense. However, when the amount of available training data is decreased, the evidence approach clearly outperforms the two other approaches. The MAP predictive distribution is clearly inferior in the log-score sense to the two more sophisticated approaches, but for the 0/1-score the MAP approach may still in some cases produce the best results.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/kontkanen97a/kontkanen97a.pdf",
        "supp": "",
        "pdf_size": 4265173,
        "gs_citation": 86,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15009628774049288966&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Complex Systems Computation Group (CoSCo)*; Complex Systems Computation Group (CoSCo)*; Complex Systems Computation Group (CoSCo)*; Complex Systems Computation Group (CoSCo)*; CWI, Department of Algorithms and Architectures",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "http://www.cs.Helsinki.FI/research/cosc o/",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "Complex Systems Computation Group;Centrum Wiskunde & Informatica",
        "aff_unique_dep": "Complex Systems Computation;Department of Algorithms and Architectures",
        "aff_unique_url": ";https://www.cwi.nl",
        "aff_unique_abbr": "CoSCo;CWI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";Netherlands"
    },
    {
        "id": "2cc079a4c7",
        "title": "Comparing Tree-Simplification Procedures",
        "site": "https://proceedings.mlr.press/r1/breslow97a.html",
        "author": "Leonard A. Breslow; David W. Aha",
        "abstract": "Induced decision trees are frequently used by researchers in the machine learning and statistics communities to solve classification tasks (Breiman et al. 1984; Quinlan 1993). However, their practical utility is limited by difficulties users have in comprehending them due to their size and complexity. Many methods have been proposed to simplify decision trees, but their relative capabilities are largely unknown; their evaluation is usually limited to comparisons with \"bench-mark\" systems (e.g., C4.5, CART). This paper presents a categoriZation framework for tree-simplification methods and focuses on the empirical comparison of selected methods.",
        "bibtex": "@InProceedings{pmlr-vR1-breslow97a,\n  title = \t {Comparing Tree-Simplification Procedures},\n  author =       {Breslow, Leonard A. and Aha, David W.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {67--74},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/breslow97a/breslow97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/breslow97a.html},\n  abstract = \t {Induced decision trees are frequently used by researchers in the machine learning and statistics communities to solve classification tasks (Breiman et al. 1984; Quinlan 1993). However, their practical utility is limited by difficulties users have in comprehending them due to their size and complexity. Many methods have been proposed to simplify decision trees, but their relative capabilities are largely unknown; their evaluation is usually limited to comparisons with \"bench-mark\" systems (e.g., C4.5, CART). This paper presents a categoriZation framework for tree-simplification methods and focuses on the empirical comparison of selected methods.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/breslow97a/breslow97a.pdf",
        "supp": "",
        "pdf_size": 3611390,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6245264521577508768&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Navy Center for Applied Research in Artificial Intelligence; Navy Center for Applied Research in Artificial Intelligence",
        "aff_domain": "aic.nrl.navy.mil;aic.nrl.navy.mil",
        "email": "aic.nrl.navy.mil;aic.nrl.navy.mil",
        "github": "",
        "project": "http://www.aic.nrl.navy.mil/",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Navy Center for Applied Research in Artificial Intelligence",
        "aff_unique_dep": "Artificial Intelligence",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f5513e16c3",
        "title": "Conceptual Clustering with Numeric-and-Nominal Mixed Data - A New Similarity Based System",
        "site": "https://proceedings.mlr.press/r1/li97a.html",
        "author": "Cen Li; Gautam Biswas",
        "abstract": "This paper presents a new Similarity Based Agglomerative Clustering (SBAC) algorithm that works well for data with mixed numeric and nominal features. A similarity measure, proposed by Goodall for biological taxonomy[13], that gives greater weight to uncommon feature-value matches in similarity computations and makes no assumptions of the underlying distributions of the feature-values, is adopted to define the similarity measure between pairs of objects. An agglomerative algorithm is employed to construct a concept tree, and a simple distinctness heuristic is used to extract a partition of the data. The performance of SBAC has been studied on artificially generated data sets. Results demonstrate the effectiveness of this algorithm in unsupervised discovery tasks. Comparisons with other schemes illustrate the superior performance of the algorithm.",
        "bibtex": "@InProceedings{pmlr-vR1-li97a,\n  title = \t {Conceptual Clustering with Numeric-and-Nominal Mixed Data - A New Similarity Based System},\n  author =       {Li, Cen and Biswas, Gautam},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {327--346},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/li97a/li97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/li97a.html},\n  abstract = \t {This paper presents a new Similarity Based Agglomerative Clustering (SBAC) algorithm that works well for data with mixed numeric and nominal features. A similarity measure, proposed by Goodall for biological taxonomy[13], that gives greater weight to uncommon feature-value matches in similarity computations and makes no assumptions of the underlying distributions of the feature-values, is adopted to define the similarity measure between pairs of objects. An agglomerative algorithm is employed to construct a concept tree, and a simple distinctness heuristic is used to extract a partition of the data. The performance of SBAC has been studied on artificially generated data sets. Results demonstrate the effectiveness of this algorithm in unsupervised discovery tasks. Comparisons with other schemes illustrate the superior performance of the algorithm.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/li97a/li97a.pdf",
        "supp": "",
        "pdf_size": 10764731,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7483969076021751531&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 4,
        "aff": "Department of Computer Science Box 1679, Station B, Vanderbilt University, Nashville, TN 37235; Department of Computer Science Box 1679, Station B, Vanderbilt University, Nashville, TN 37235",
        "aff_domain": "vuse.vanderbilt.edu;vuse.vanderbilt.edu",
        "email": "vuse.vanderbilt.edu;vuse.vanderbilt.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Vanderbilt University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.vanderbilt.edu",
        "aff_unique_abbr": "Vanderbilt",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Nashville",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "88c6a4cc10",
        "title": "Cross-validated Likelihood for Model Selection in Unsupervised Learning",
        "site": "https://proceedings.mlr.press/r1/smyth97a.html",
        "author": "Padhraic Smyth",
        "abstract": "Cross-validation is a well-known technique in supervised learning to select a model from a family of candidate models. Examples include selecting the best classification tree using cross-validated classification error (Breiman et al., 1984) and variable selection in linear regression using cross-validated predictive squared error (Hjort, 1995). Cross-validation is less seldomly used in \\emph{unsupervised} learning such as clustering. It is popular in kernel density estimation for choosing the smoothing parameter (the kernel bandwidth). However, it does not appear to have been used for the problem of determining cluster structure in clustering problems, i.e., solving the problem of how many clusters to fit to a given data set. This is the problem addressed in this paper. Cross-validated likelihood can be viewed as an appropriate metric for model selection in probabilistic clustering, in particular for finite mixture models. In this paper, the use of cross-validated likelihood for clustering is investigated a.nd the method is applied to a real problem where \"truth\" is unknown, i.e., determining the number of intrinsic \"regimes\" in records of upper atmosphere pressure taken daily since 1948 over the Northern Hemisphere.",
        "bibtex": "@InProceedings{pmlr-vR1-smyth97a,\n  title = \t {Cross-validated Likelihood for Model Selection in Unsupervised Learning},\n  author =       {Smyth, Padhraic},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {473--480},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/smyth97a/smyth97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/smyth97a.html},\n  abstract = \t {Cross-validation is a well-known technique in supervised learning to select a model from a family of candidate models. Examples include selecting the best classification tree using cross-validated classification error (Breiman et al., 1984) and variable selection in linear regression using cross-validated predictive squared error (Hjort, 1995). Cross-validation is less seldomly used in \\emph{unsupervised} learning such as clustering. It is popular in kernel density estimation for choosing the smoothing parameter (the kernel bandwidth). However, it does not appear to have been used for the problem of determining cluster structure in clustering problems, i.e., solving the problem of how many clusters to fit to a given data set. This is the problem addressed in this paper. Cross-validated likelihood can be viewed as an appropriate metric for model selection in probabilistic clustering, in particular for finite mixture models. In this paper, the use of cross-validated likelihood for clustering is investigated a.nd the method is applied to a real problem where \"truth\" is unknown, i.e., determining the number of intrinsic \"regimes\" in records of upper atmosphere pressure taken daily since 1948 over the Northern Hemisphere.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/smyth97a/smyth97a.pdf",
        "supp": "",
        "pdf_size": 3492574,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1601174574967932368&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "Department of Information and Computer Science, University of California, Irvine + Jet Propulsion Laboratory, California Institute of Technology, Pasadena",
        "aff_domain": "ics.uci.edu",
        "email": "ics.uci.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1",
        "aff_unique_norm": "University of California, Irvine;California Institute of Technology",
        "aff_unique_dep": "Department of Information and Computer Science;Jet Propulsion Laboratory",
        "aff_unique_url": "https://www.uci.edu;https://www.caltech.edu",
        "aff_unique_abbr": "UCI;Caltech",
        "aff_campus_unique_index": "0+1",
        "aff_campus_unique": "Irvine;Pasadena",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "f010785895",
        "title": "Dataset Cataloging Metadata for Machine Learning Applications Research",
        "site": "https://proceedings.mlr.press/r1/cunningham97a.html",
        "author": "Sally Jo Cunningham",
        "abstract": "As the field of machine learning (ML) matures, two types of data archives are developing: collections of benchmark data sets used to test the performance of new algorithms, and data stores to which machine learning/data mining algorithms are applied to create scientific or commercial applications. At present, the catalogs of these archives are ad hoc and not tailored to machine learning analysis. This paper considers the cataloging metadata required to support these two types of repositories, and discusses the organizational support necessary for archive catalog maintenance.",
        "bibtex": "@InProceedings{pmlr-vR1-cunningham97a,\n  title = \t {Dataset Cataloging Metadata for Machine Learning Applications Research},\n  author =       {Cunningham, Sally Jo},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {139--146},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/cunningham97a/cunningham97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/cunningham97a.html},\n  abstract = \t {As the field of machine learning (ML) matures, two types of data archives are developing: collections of benchmark data sets used to test the performance of new algorithms, and data stores to which machine learning/data mining algorithms are applied to create scientific or commercial applications. At present, the catalogs of these archives are ad hoc and not tailored to machine learning analysis. This paper considers the cataloging metadata required to support these two types of repositories, and discusses the organizational support necessary for archive catalog maintenance.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/cunningham97a/cunningham97a.pdf",
        "supp": "",
        "pdf_size": 4467953,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3580040291376593267&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computer Science, University of Waikato, Hamilton, New Zealand",
        "aff_domain": "cs.waikato.ac.nz",
        "email": "cs.waikato.ac.nz",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Waikato",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.waikato.ac.nz",
        "aff_unique_abbr": "UoW",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Hamilton",
        "aff_country_unique_index": "0",
        "aff_country_unique": "New Zealand"
    },
    {
        "id": "dd75bda13a",
        "title": "Estimating Latent Causal Inferences: Tetrad II model selection and Bayesian parameter estimation",
        "site": "https://proceedings.mlr.press/r1/scheines97a.html",
        "author": "Richard Scheines",
        "abstract": "The statistical evidence for the detrimental effect of low level lead exposure on the cognitive capacities of children has been debated for several decades. In this paper I describe how two techniques from artificial intelligence and statistics proved crucial in making the statistical evidence for the accepted epidemiological conclusion seem decisive. The first is a variable-selection routine in TETRAD II, and the second a Bayesian estimation of the parameter reflecting the causal influence of Actual Lead Exposure, a latent variable, on the measured IQ score of middle class suburban children.",
        "bibtex": "@InProceedings{pmlr-vR1-scheines97a,\n  title = \t {Estimating Latent Causal Inferences: Tetrad II model selection and Bayesian parameter estimation},\n  author =       {Scheines, Richard},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {445--456},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/scheines97a/scheines97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/scheines97a.html},\n  abstract = \t {The statistical evidence for the detrimental effect of low level lead exposure on the cognitive capacities of children has been debated for several decades. In this paper I describe how two techniques from artificial intelligence and statistics proved crucial in making the statistical evidence for the accepted epidemiological conclusion seem decisive. The first is a variable-selection routine in TETRAD II, and the second a Bayesian estimation of the parameter reflecting the causal influence of Actual Lead Exposure, a latent variable, on the measured IQ score of middle class suburban children.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/scheines97a/scheines97a.pdf",
        "supp": "",
        "pdf_size": 5375010,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14552444531109830375&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Philosophy, Carnegie Mellon University",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "http://hss.cmu.edu/philosophy/TETRAD/tetrad.html",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Department of Philosophy",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1f97f1d150",
        "title": "Extensions of Undirected and Acyclic, Directed Graphical Models",
        "site": "https://proceedings.mlr.press/r1/richardson97a.html",
        "author": "Thomas S. Richardson",
        "abstract": "The use of acyclic, directed graphs (often called \u2019DAG\u2019s) to simultaneously represent causal hypotheses and to encode independence and conditional independence constraints associated with those hypotheses has proved fruitful in the construction of expert systems, in the development of efficient updating algorithms (Pearl, 1988, Lauritzen et al. 1988), and in inferring causal structure (Pearl and Verma, 1991; Cooper and Herskovits 1992; Spirtes, Glymour and Scheines, 1993). In section 1 I will survey a number of extensions of the DAG framework based on directed graphs and chain graphs (Lauritzen and Wermuth 1989; Frydenberg 1990; Koster 1996; Andersson, Madigan and Perlman 1996). Those based on directed graphs include models based on directed cyclic and acyclic graphs, possibly including latent variables and/or selection bias (Pearl, 1988; Spirtes, Glymour and Scheines 1993; Spirtes 1995; Spirtes, Meek, and Richardson 1995; Richardson 1996a, 1996b; Koster 1996; Pearl and Dechter 1996; Cox and Wermuth, 1996). In section 2 I state two properties, motivated by causal and spatial intuitions, that the set of conditional independencies entailed by a graphical model might satisfy. I proceed to show that the sets of independencies entailed by (i) an undirected graph via separation, and (ii) a (cyclic or acyclic) directed graph (possibly with latent and/or selection variables) via ct-separation, satisfy both properties. By contrast neither of these properties, in general, will hold in a chain graph under the Lauritzen-Wermuth-Frydenberg (LWF) interpretation. One property holds for chain graphs under the Andersson-Madigan-Perlman (AMP) interpretation, the other does not. The examination of these properties and others like them may provide insight into the current vigorous debate concerning the applicability of chain graphs under different global Markov properties.",
        "bibtex": "@InProceedings{pmlr-vR1-richardson97a,\n  title = \t {Extensions of Undirected and Acyclic, Directed Graphical Models},\n  author =       {Richardson, Thomas S.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {407--420},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/richardson97a/richardson97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/richardson97a.html},\n  abstract = \t {The use of acyclic, directed graphs (often called \u2019DAG\u2019s) to simultaneously represent causal hypotheses and to encode independence and conditional independence constraints associated with those hypotheses has proved fruitful in the construction of expert systems, in the development of efficient updating algorithms (Pearl, 1988, Lauritzen et al. 1988), and in inferring causal structure (Pearl and Verma, 1991; Cooper and Herskovits 1992; Spirtes, Glymour and Scheines, 1993). In section 1 I will survey a number of extensions of the DAG framework based on directed graphs and chain graphs (Lauritzen and Wermuth 1989; Frydenberg 1990; Koster 1996; Andersson, Madigan and Perlman 1996). Those based on directed graphs include models based on directed cyclic and acyclic graphs, possibly including latent variables and/or selection bias (Pearl, 1988; Spirtes, Glymour and Scheines 1993; Spirtes 1995; Spirtes, Meek, and Richardson 1995; Richardson 1996a, 1996b; Koster 1996; Pearl and Dechter 1996; Cox and Wermuth, 1996). In section 2 I state two properties, motivated by causal and spatial intuitions, that the set of conditional independencies entailed by a graphical model might satisfy. I proceed to show that the sets of independencies entailed by (i) an undirected graph via separation, and (ii) a (cyclic or acyclic) directed graph (possibly with latent and/or selection variables) via ct-separation, satisfy both properties. By contrast neither of these properties, in general, will hold in a chain graph under the Lauritzen-Wermuth-Frydenberg (LWF) interpretation. One property holds for chain graphs under the Andersson-Madigan-Perlman (AMP) interpretation, the other does not. The examination of these properties and others like them may provide insight into the current vigorous debate concerning the applicability of chain graphs under different global Markov properties.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/richardson97a/richardson97a.pdf",
        "supp": "",
        "pdf_size": 6251738,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17564057042318461042&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 5,
        "aff": "Statistics Department, University of Washington",
        "aff_domain": "stat.washington.edu",
        "email": "stat.washington.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Statistics Department",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2e3dd8ff77",
        "title": "Finding Overlapping Distributions with MML",
        "site": "https://proceedings.mlr.press/r1/baxter97a.html",
        "author": "Rohan A. Baxter; Jonathan J. Oliver",
        "abstract": "This paper considers an aspect of mixture modelling. Previous studies have shown minimum message length (MML) estimation to perform well in a wide variety of mixture modelling problems, including determining the number of com- ponents which best describes some data. In this paper, we focus on the difficult problem of overlapping components. An advantage of the probabilistic mixture modelling approach is its ability to identify models where the components overlap and data items can belong prob- abilistically to more than one component. Significantly overlapping distributions require more data for their parameters to be accurately estimated than well sep- arated distributions. For example, two Gaussian distributions are considered to significantly overlap when their means are within three standard deviations of each other. If insufficient data is available, only a single component distribution will be estimated, although the data originates from two component distributions. In this paper, we quantify this difficulty in terms of the number of data items needed for the MML criterion to \u2019discover\u2019 two overlapping components. First, we perform experiments which compare the MML criterion\u2019s performance relative to other Bayesian criteria based on MCMC sampling. Second, we make two alterations to the existing MML estimates in order to improve its performance on overlapping distributions. Experiments are performed with the new estimates to confirm that they are effective.",
        "bibtex": "@InProceedings{pmlr-vR1-baxter97a,\n  title = \t {Finding Overlapping Distributions with MML},\n  author =       {Baxter, Rohan A. and Oliver, Jonathan J.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {23--30},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/baxter97a/baxter97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/baxter97a.html},\n  abstract = \t {This paper considers an aspect of mixture modelling. Previous studies have shown minimum message length (MML) estimation to perform well in a wide variety of mixture modelling problems, including determining the number of com- ponents which best describes some data. In this paper, we focus on the difficult problem of overlapping components. An advantage of the probabilistic mixture modelling approach is its ability to identify models where the components overlap and data items can belong prob- abilistically to more than one component. Significantly overlapping distributions require more data for their parameters to be accurately estimated than well sep- arated distributions. For example, two Gaussian distributions are considered to significantly overlap when their means are within three standard deviations of each other. If insufficient data is available, only a single component distribution will be estimated, although the data originates from two component distributions. In this paper, we quantify this difficulty in terms of the number of data items needed for the MML criterion to \u2019discover\u2019 two overlapping components. First, we perform experiments which compare the MML criterion\u2019s performance relative to other Bayesian criteria based on MCMC sampling. Second, we make two alterations to the existing MML estimates in order to improve its performance on overlapping distributions. Experiments are performed with the new estimates to confirm that they are effective.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/baxter97a/baxter97a.pdf",
        "supp": "",
        "pdf_size": 3255865,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5888647649988116672&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Computer Science Department, Monash University, Clayton, Victoria, 3168, AUSTRALIA; Computer Science Department, Monash University, Clayton, Victoria, 3168, AUSTRALIA",
        "aff_domain": "cs.monash.edu.au;cs.monash.edu.au",
        "email": "cs.monash.edu.au;cs.monash.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Monash University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.monash.edu",
        "aff_unique_abbr": "Monash",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Clayton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "9ec1517f73",
        "title": "Graphical Model Based Computer Adaptive Testing",
        "site": "https://proceedings.mlr.press/r1/almond97b.html",
        "author": "Russell G. Almond; Robert J. Mislevy",
        "abstract": "This paper synthesizes ideas from the fields of graphical modelling and eductational testing, particularly Item Response Theory (IRT) applied to Computerized Adaptive Testing (CAT). Graphical modelling can offer IRT a language for describing multifaceted skills and knowledge and disentangling evidence from com- plex performances. IRT-CA T can offer graphical modellers several ways of treating sources of variability other than including more variables in the model. In particular, variables can enter into the modelling pro- cess at several levels: (1) in validity studies (but not in the ordinarily used model), (2) in task construction (in particular, in defining link parameters), (3) in test or model assembly (blocking and randomization con- straints in selecting tasks or other model pieces), (4) in response characterization (i.e. as part of task models which characterize a response) or (5) in the main (student) model. The paper describes an implementation of these ideas in a fielded application: HYDRIVE, a tutor for hydraulics diagnosis",
        "bibtex": "@InProceedings{pmlr-vR1-almond97b,\n  title = \t {Graphical Model Based Computer Adaptive Testing},\n  author =       {Almond, Russell G. and Mislevy, Robert J.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {11--22},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/almond97b/almond97b.pdf},\n  url = \t {https://proceedings.mlr.press/r1/almond97b.html},\n  abstract = \t {This paper synthesizes ideas from the fields of graphical modelling and eductational testing, particularly Item Response Theory (IRT) applied to Computerized Adaptive Testing (CAT). Graphical modelling can offer IRT a language for describing multifaceted skills and knowledge and disentangling evidence from com- plex performances. IRT-CA T can offer graphical modellers several ways of treating sources of variability other than including more variables in the model. In particular, variables can enter into the modelling pro- cess at several levels: (1) in validity studies (but not in the ordinarily used model), (2) in task construction (in particular, in defining link parameters), (3) in test or model assembly (blocking and randomization con- straints in selecting tasks or other model pieces), (4) in response characterization (i.e. as part of task models which characterize a response) or (5) in the main (student) model. The paper describes an implementation of these ideas in a fielded application: HYDRIVE, a tutor for hydraulics diagnosis},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/almond97b/almond97b.pdf",
        "supp": "",
        "pdf_size": 6977835,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:q3x2SXPqqaIJ:scholar.google.com/&scioq=Graphical+Model+Based+Computer+Adaptive+Testing&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": "Educational Testing Service; Educational Testing Service",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Educational Testing Service",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ets.org",
        "aff_unique_abbr": "ETS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6d48b5e726",
        "title": "Heuristic Greedy Search Algorithms for Latent Variable Models",
        "site": "https://proceedings.mlr.press/r1/spirtes97a.html",
        "author": "Peter Spirtes; Thomas S.Richardson; Christopher Meek",
        "abstract": "There has recently been significant progress in the development of algorithms for learning the directed acyclic graph (DAG) part of a Bayesian network without latent variables from data and optional background knowledge. However, the problem of learning the DAG part of a Bayesian network with latent (unmeasured) variables is much more difficult for two reasons: first the number of possible models is infinite, and second, calculating scores for latent variables models is generally much slower than calculating scores for models without latent variables. In this paper we will describe how to extend search algorithms developed for non-latent variable DAG models to the case of DAG models with latent variables. We will introduce two generalizations of DAGs, called mixed ancestor graphs (or MAGs) and partial ancestor graphs (or PAGs), and briefly describe how they can be used to search for latent variable DAG models, to classify, and to predict the effects of interventions in causal systems.",
        "bibtex": "@InProceedings{pmlr-vR1-spirtes97a,\n  title = \t {Heuristic Greedy Search Algorithms for Latent Variable Models},\n  author =       {Spirtes, Peter and S.Richardson, Thomas and Meek, Christopher},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {481--488},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/spirtes97a/spirtes97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/spirtes97a.html},\n  abstract = \t {There has recently been significant progress in the development of algorithms for learning the directed acyclic graph (DAG) part of a Bayesian network without latent variables from data and optional background knowledge. However, the problem of learning the DAG part of a Bayesian network with latent (unmeasured) variables is much more difficult for two reasons: first the number of possible models is infinite, and second, calculating scores for latent variables models is generally much slower than calculating scores for models without latent variables. In this paper we will describe how to extend search algorithms developed for non-latent variable DAG models to the case of DAG models with latent variables. We will introduce two generalizations of DAGs, called mixed ancestor graphs (or MAGs) and partial ancestor graphs (or PAGs), and briefly describe how they can be used to search for latent variable DAG models, to classify, and to predict the effects of interventions in causal systems.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/spirtes97a/spirtes97a.pdf",
        "supp": "",
        "pdf_size": 11054052,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11475460453044615021&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Philosophy, Carnegie Mellon University; Department of Statistics, University of Washington; Microsoft",
        "aff_domain": "andrew.cmu.edu; ; ",
        "email": "andrew.cmu.edu; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Carnegie Mellon University;University of Washington;Microsoft",
        "aff_unique_dep": "Department of Philosophy;Department of Statistics;Microsoft Corporation",
        "aff_unique_url": "https://www.cmu.edu;https://www.washington.edu;https://www.microsoft.com",
        "aff_unique_abbr": "CMU;UW;Microsoft",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "eb01fa69b6",
        "title": "How to Find Big-Oh in Your Data Set (and How Not To)",
        "site": "https://proceedings.mlr.press/r1/mcgeoch97a.html",
        "author": "C. C. McGeoch; P. R. Cohen",
        "abstract": "The \\emph{empirical curve bounding problem} is defined as follows. Suppose data vectors $X$, $Y$ are presented such that $E(Y[i]) = \\bar{f}(X[i])$ where $\\bar{f}(x)$ is an unknown function. The problem is to analyze $X$, $Y$ and obtain complexity bounds $O(g_u(x))$ and $\\Omega(g_l(x))$ on the function $\\bar{f}(x)$. As no algorithm for empirical curve bounding can be guaranteed correct, we consider heuristics. Five heuristic algorithms are presented here, together with analytical results guaranteeing correctness for certain families of functions. Experimental evaluations of the correctness and tightness of bounds obtained by the rules for several constructed functions $f(x)$ and real datasets are described.",
        "bibtex": "@InProceedings{pmlr-vR1-mcgeoch97a,\n  title = \t {How to Find Big-Oh in Your Data Set (and How Not To)},\n  author =       {McGeoch, C. C. and Cohen, P. R.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {347--354},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/mcgeoch97a/mcgeoch97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/mcgeoch97a.html},\n  abstract = \t {The \\emph{empirical curve bounding problem} is defined as follows. Suppose data vectors $X$, $Y$ are presented such that $E(Y[i]) = \\bar{f}(X[i])$ where $\\bar{f}(x)$ is an unknown function. The problem is to analyze $X$, $Y$ and obtain complexity bounds $O(g_u(x))$ and $\\Omega(g_l(x))$ on the function $\\bar{f}(x)$. As no algorithm for empirical curve bounding can be guaranteed correct, we consider heuristics. Five heuristic algorithms are presented here, together with analytical results guaranteeing correctness for certain families of functions. Experimental evaluations of the correctness and tightness of bounds obtained by the rules for several constructed functions $f(x)$ and real datasets are described.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/mcgeoch97a/mcgeoch97a.pdf",
        "supp": "",
        "pdf_size": 3882181,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8336639110342619278&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 21,
        "aff": "Department of Mathematics and Computer Science, Amherst College, Amherst, MA 01002; Department of Computer Science, University of Massachussetts Amherst, MA 01003",
        "aff_domain": "ccmCcs.amherst.edu; cohenCcs.umass.edu",
        "email": "ccmCcs.amherst.edu; cohenCcs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Amherst College;University of Massachusetts Amherst",
        "aff_unique_dep": "Department of Mathematics and Computer Science;Department of Computer Science",
        "aff_unique_url": "https://www.amherst.edu;https://www.umass.edu",
        "aff_unique_abbr": "Amherst College;UMass Amherst",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "523c01d9a6",
        "title": "Inductive Inference of First-Order Models from Numeric-Symbolic Data",
        "site": "https://proceedings.mlr.press/r1/esposito97a.html",
        "author": "Floriana Esposito; Sergio Caggese; Donato Malerba; Giovanni Semeraro",
        "abstract": "A factor common to statistical techniques of data analysis is the adopted representation formalism: A tabular (zeroth-order) model with almost exclusively numerical features . On the contrary, several studies on machine learning concern the induction of first-order models from symbolic data, but are inadequate for continuous data. In the paper, we face the problem of handling both numerical and symbolic data in first-order models. distinguishing the moment of model generation from examples (induction) from the moment of model recognition by means of a flexible. probabilistic subsumption test. We demonstrate the proposed solutions on a problem in document understanding where the objective is to induce the models of the logical structure of some real business letters.",
        "bibtex": "@InProceedings{pmlr-vR1-esposito97a,\n  title = \t {Inductive Inference of First-Order Models from Numeric-Symbolic Data},\n  author =       {Esposito, Floriana and Caggese, Sergio and Malerba, Donato and Semeraro, Giovanni},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {173--182},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/esposito97a/esposito97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/esposito97a.html},\n  abstract = \t {A factor common to statistical techniques of data analysis is the adopted representation formalism: A tabular (zeroth-order) model with almost exclusively numerical features . On the contrary, several studies on machine learning concern the induction of first-order models from symbolic data, but are inadequate for continuous data. In the paper, we face the problem of handling both numerical and symbolic data in first-order models. distinguishing the moment of model generation from examples (induction) from the moment of model recognition by means of a flexible. probabilistic subsumption test. We demonstrate the proposed solutions on a problem in document understanding where the objective is to induce the models of the logical structure of some real business letters.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/esposito97a/esposito97a.pdf",
        "supp": "",
        "pdf_size": 5389858,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:LrDjk2EW11gJ:scholar.google.com/&scioq=Inductive+Inference+of+First-Order+Models+from+Numeric-Symbolic+Data&hl=en&as_sdt=0,5",
        "gs_version_total": 3,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "8814b51cf6",
        "title": "Inference using Probabilistic Concept Trees",
        "site": "https://proceedings.mlr.press/r1/fisher97a.html",
        "author": "Doug Fisher; Doug Talbert",
        "abstract": "Discussions of \u2019probabilistic reasoning systems\u2019 often presuppose a belief network, which represents the joint probability distribution of a domain, as the primary knowledge structure. However, another common knowledge structure from which the joint probability distribution can be recovered is a hierarchical probabilistic clustering or probabilistic concept tree (Fisher, 1987). Probabilistic concept trees are a target structure for a number of clustering systems from machine learning such as COBWEB (Fisher, 1987) and systems by Hadzikadik and Yun (1989), Gennari, Langley, and Fisher (1989), Decaestecker (1991), Anderson and Matessa (1991), Reich and Fenves (1991), Biswas, Weinberg, and Li (1994), De Alte Da Veiga (1994), Kilander (1994) Ketterlin, Gan{\\c}arski, and Korczak (1995), and Nevins (1995). Related probabilistic structures are produced by systems such as AUTOCLASS (Cheeseman, Kelly, Self, Stutz, Taylor, &Freeman, 1988), SNOB (Wallace &Boulton, 1968; Wallace & Dowe, 1994) , and systems by Hanson and Bauer (1989) and Martin and Billman (1994). These systems can be easily adapted to form probabilistic concept trees of the type we describe. This paper will not focus on clustering systems \\emph{per se}, but on characteristics and capabilities of probabilistic concept trees, particularly as they relate to inference tasks often associated with belief networks. As \u2019object-centered\u2019 knowledge structures, probabilistic concept trees nicely complement the \u2019variable-centered\u2019, belief network structure.",
        "bibtex": "@InProceedings{pmlr-vR1-fisher97a,\n  title = \t {Inference using Probabilistic Concept Trees},\n  author =       {Fisher, Doug and Talbert, Doug},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {191--202},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/fisher97a/fisher97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/fisher97a.html},\n  abstract = \t {Discussions of \u2019probabilistic reasoning systems\u2019 often presuppose a belief network, which represents the joint probability distribution of a domain, as the primary knowledge structure. However, another common knowledge structure from which the joint probability distribution can be recovered is a hierarchical probabilistic clustering or probabilistic concept tree (Fisher, 1987). Probabilistic concept trees are a target structure for a number of clustering systems from machine learning such as COBWEB (Fisher, 1987) and systems by Hadzikadik and Yun (1989), Gennari, Langley, and Fisher (1989), Decaestecker (1991), Anderson and Matessa (1991), Reich and Fenves (1991), Biswas, Weinberg, and Li (1994), De Alte Da Veiga (1994), Kilander (1994) Ketterlin, Gan{\\c}arski, and Korczak (1995), and Nevins (1995). Related probabilistic structures are produced by systems such as AUTOCLASS (Cheeseman, Kelly, Self, Stutz, Taylor, &Freeman, 1988), SNOB (Wallace &Boulton, 1968; Wallace & Dowe, 1994) , and systems by Hanson and Bauer (1989) and Martin and Billman (1994). These systems can be easily adapted to form probabilistic concept trees of the type we describe. This paper will not focus on clustering systems \\emph{per se}, but on characteristics and capabilities of probabilistic concept trees, particularly as they relate to inference tasks often associated with belief networks. As \u2019object-centered\u2019 knowledge structures, probabilistic concept trees nicely complement the \u2019variable-centered\u2019, belief network structure.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/fisher97a/fisher97a.pdf",
        "supp": "",
        "pdf_size": 4436807,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12768423653088579852&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "cebd9440da",
        "title": "Integrating Signal and Language Context to Improve Handwritten Phrase Recognition: Alternative Approaches",
        "site": "https://proceedings.mlr.press/r1/bouchaffra97a.html",
        "author": "Djamel Bouchaffra; Eugene Koontz; V. Krpasundar; Rohini K. Srihari; Sargur N. Srihari",
        "abstract": "Handwritten phrase recognition is an important and difficult task. Recent research in this area has fo- cussed on utilising language context to improve recognition performance, without taking the information from the input signal itself into proper account. In this paper, we adopt a Bayesian approach to solving this problem. The Bayesian framework allows us to integrate signal-level information from the actual input with the linguistic context usually used in post-processing the recogniser\u2019s output. We demonstrate the validity of a statistical approach to integrating these two sources of information. We also analyse the need for improvement in performance through innovative estimation of informative priors, and describe our method for obtaining agreement from multiple experts for this task. We compare the performance of our integrated signal-language model against existing \"language-only\" models.",
        "bibtex": "@InProceedings{pmlr-vR1-bouchaffra97a,\n  title = \t {Integrating Signal and Language Context to Improve Handwritten Phrase Recognition: Alternative Approaches},\n  author =       {Bouchaffra, Djamel and Koontz, Eugene and Krpasundar, V. and Srihari, Rohini K. and Srihari, Sargur N.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {47--54},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/bouchaffra97a/bouchaffra97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/bouchaffra97a.html},\n  abstract = \t {Handwritten phrase recognition is an important and difficult task. Recent research in this area has fo- cussed on utilising language context to improve recognition performance, without taking the information from the input signal itself into proper account. In this paper, we adopt a Bayesian approach to solving this problem. The Bayesian framework allows us to integrate signal-level information from the actual input with the linguistic context usually used in post-processing the recogniser\u2019s output. We demonstrate the validity of a statistical approach to integrating these two sources of information. We also analyse the need for improvement in performance through innovative estimation of informative priors, and describe our method for obtaining agreement from multiple experts for this task. We compare the performance of our integrated signal-language model against existing \"language-only\" models.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/bouchaffra97a/bouchaffra97a.pdf",
        "supp": "",
        "pdf_size": 3984574,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9322317839569857017&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Center of Excellence for Document Analysis and Recognition (CEDAR); Center of Excellence for Document Analysis and Recognition (CEDAR); Center of Excellence for Document Analysis and Recognition (CEDAR); Center of Excellence for Document Analysis and Recognition (CEDAR); Center of Excellence for Document Analysis and Recognition (CEDAR)",
        "aff_domain": "cedar.buffalo.edu;cedar.buffalo.edu;cedar.buffalo.edu;cedar.buffalo.edu;cedar.buffalo.edu",
        "email": "cedar.buffalo.edu;cedar.buffalo.edu;cedar.buffalo.edu;cedar.buffalo.edu;cedar.buffalo.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Center of Excellence for Document Analysis and Recognition",
        "aff_unique_dep": "Document Analysis and Recognition",
        "aff_unique_url": "",
        "aff_unique_abbr": "CEDAR",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a4bf4502e3",
        "title": "Intelligent Assistant for Computational Scientists: Integrated Modelling, Experimentation and Analysis",
        "site": "https://proceedings.mlr.press/r1/gregory97a.html",
        "author": "Dawn E. Gregory; Paul R. Cohen",
        "abstract": "Computing technology has changed the way scientists work. Among the contributions of this new paradigm are the computational sciences, which involve the study of computer simulations rather than physical systems. This transition to a simulated world carries with it an important scientific advantage: the opportunity to run experiments that are expensive, dangerous, or impossible in the real world. Unfortunately, such experiments are often too easy, and the scientist is overwhelmed with empirical data. The fields of Artificial Intelligence (AI) and Statistics are concerned with modelling and analyzing such large bodies of data. AI employs heuristic reasoning and knowledge to select potential models, and statistical analysis verifies a proposed model. The combinatio\u00a0 of knowledge-based, heuristic, and statistical techniques is quite successful at modelling experiment data (e.g. [11]). Our goal is to provide an intelligent, integrated environment for scientific modelling, experimentation , and analysis, called the Scientist \u2019s Empirical Assistant (SEA). SEA is an assistant to human scientists: it automates model generation and verification, experiment design and data collection, but also relies on a human user for guidance, domain knowledge, and decision-making. SEA designs and runs prospective experiments with a simulator, allowing it to draw stronger conclusions than with post-hoc data analysis alone. SEA employs a variety of techniques from both AI and Statistics. It uses heuristic and knowledge-based reasoning to propose models, design experiments, and select analyses. It applies statistical techniques to verify models against experiment data. It develops plans to direct its course of action, and learns which plans are most successful based on past experience. Finally, it models the knowledge of the user to ensure its suggestions and decisions are appropriate.",
        "bibtex": "@InProceedings{pmlr-vR1-gregory97a,\n  title = \t {Intelligent Assistant for Computational Scientists: Integrated Modelling, Experimentation and Analysis},\n  author =       {Gregory, Dawn E. and Cohen, Paul R.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {231--238},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/gregory97a/gregory97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/gregory97a.html},\n  abstract = \t {Computing technology has changed the way scientists work. Among the contributions of this new paradigm are the computational sciences, which involve the study of computer simulations rather than physical systems. This transition to a simulated world carries with it an important scientific advantage: the opportunity to run experiments that are expensive, dangerous, or impossible in the real world. Unfortunately, such experiments are often too easy, and the scientist is overwhelmed with empirical data. The fields of Artificial Intelligence (AI) and Statistics are concerned with modelling and analyzing such large bodies of data. AI employs heuristic reasoning and knowledge to select potential models, and statistical analysis verifies a proposed model. The combinatio\u00a0 of knowledge-based, heuristic, and statistical techniques is quite successful at modelling experiment data (e.g. [11]). Our goal is to provide an intelligent, integrated environment for scientific modelling, experimentation , and analysis, called the Scientist \u2019s Empirical Assistant (SEA). SEA is an assistant to human scientists: it automates model generation and verification, experiment design and data collection, but also relies on a human user for guidance, domain knowledge, and decision-making. SEA designs and runs prospective experiments with a simulator, allowing it to draw stronger conclusions than with post-hoc data analysis alone. SEA employs a variety of techniques from both AI and Statistics. It uses heuristic and knowledge-based reasoning to propose models, design experiments, and select analyses. It applies statistical techniques to verify models against experiment data. It develops plans to direct its course of action, and learns which plans are most successful based on past experience. Finally, it models the knowledge of the user to ensure its suggestions and decisions are appropriate.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/gregory97a/gregory97a.pdf",
        "supp": "",
        "pdf_size": 3864314,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10964791312674675358&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4540c7724e",
        "title": "Intelligent Support of Secondary Data Analysis",
        "site": "https://proceedings.mlr.press/r1/almond97a.html",
        "author": "Russell G. Almond",
        "abstract": "",
        "bibtex": "@InProceedings{pmlr-vR1-almond97a,\n  title = \t {Intelligent Support of Secondary Data Analysis},\n  author =       {Almond, Russell G.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {1--10},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/almond97a/almond97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/almond97a.html},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/almond97a/almond97a.pdf",
        "supp": "",
        "pdf_size": 4875696,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:BCynJ8PW6qcJ:scholar.google.com/&scioq=Intelligent+Support+of+Secondary+Data+Analysis&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Educational Testing Service",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Educational Testing Service",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ets.org",
        "aff_unique_abbr": "ETS",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c6e6651556",
        "title": "Leaming Influence Diagram from Data",
        "site": "https://proceedings.mlr.press/r1/ezawa97a.html",
        "author": "Kazuo J. Ezawa; Narendra K. Gupta",
        "abstract": "There are many cases where decisions are made (and actions are taken) repeatedly under uncertainty, and consequences (results) ofthose decisions are available. For example, in telecommunications industry repeatedly decisions are made every day for fraud detection and account treatment. Indicators (variables) that have large uncertainties are used to make these decisions. Furthermore, the consequences of such decisions are recorded for later analysis. Similarly, in the financial industry, stocks or currencies are traded based on some indicators (variables). The consequences of these trade can be found. Similarly in the medicine, the patient treatment decisions are made on the basis of the patient information, and the consequences of these decisions to the patients can be found. These data sets contain uncertain variables, decision variables, and value lottery (final outcomes). Furthermore these decisions may not be made not by a single decision maker, but by many decision makers. In contrast to a typical decision analysis, in these environments decisions are made repeatedly. This paper addresses the discovery of knowledge bearing on these decisions in the form of influence diagrams (normative decision models) using a novel supervised machine learning method that constructs Bayesian network models with decisions. Algorithms presented in this paper exploit the goal oriented characteristics o f influence diagrams and generate a specific form of influence diagrams that are efficient, both to learn and evaluate. For this reason they are called \"efficient\" influence diagrams.",
        "bibtex": "@InProceedings{pmlr-vR1-ezawa97a,\n  title = \t {Leaming Influence Diagram from Data},\n  author =       {Ezawa, Kazuo J. and Gupta, Narendra K.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {183--190},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/ezawa97a/ezawa97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/ezawa97a.html},\n  abstract = \t {There are many cases where decisions are made (and actions are taken) repeatedly under uncertainty, and consequences (results) ofthose decisions are available. For example, in telecommunications industry repeatedly decisions are made every day for fraud detection and account treatment. Indicators (variables) that have large uncertainties are used to make these decisions. Furthermore, the consequences of such decisions are recorded for later analysis. Similarly, in the financial industry, stocks or currencies are traded based on some indicators (variables). The consequences of these trade can be found. Similarly in the medicine, the patient treatment decisions are made on the basis of the patient information, and the consequences of these decisions to the patients can be found. These data sets contain uncertain variables, decision variables, and value lottery (final outcomes). Furthermore these decisions may not be made not by a single decision maker, but by many decision makers. In contrast to a typical decision analysis, in these environments decisions are made repeatedly. This paper addresses the discovery of knowledge bearing on these decisions in the form of influence diagrams (normative decision models) using a novel supervised machine learning method that constructs Bayesian network models with decisions. Algorithms presented in this paper exploit the goal oriented characteristics o f influence diagrams and generate a specific form of influence diagrams that are efficient, both to learn and evaluate. For this reason they are called \"efficient\" influence diagrams.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/ezawa97a/ezawa97a.pdf",
        "supp": "",
        "pdf_size": 4054911,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:XYifu8uDXiIJ:scholar.google.com/&scioq=Leaming+Influence+Diagram+from+Data&hl=en&as_sdt=0,33",
        "gs_version_total": 2,
        "aff": "AT&T Laboratories; AT&T Laboratories",
        "aff_domain": "ulysses.att.com;ulysses.att.com",
        "email": "ulysses.att.com;ulysses.att.com",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "AT&T Laboratories",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.att.com/labs",
        "aff_unique_abbr": "AT&T Labs",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0d724eb297",
        "title": "MML Mixture Modelling of Multi-state, Poisson, vonMises circular and Gaussian Distributions",
        "site": "https://proceedings.mlr.press/r1/wallace97a.html",
        "author": "Chris S. Wallace; David L. Dowe",
        "abstract": "Minimum Message Length (MML) is an invariant Bayesian point estimation technique which is also consistent and efficient. We provide a brief overview of MML inductive inference (Wallace and Boulton (1968) , Wallace and Freeman (1987)), and how it has both an information-theoretic and a Bayesian interpretation. We then outline how MML is used for statistical parameter estimation, and how the MML mixture modelling program, Snob (Wallace and Boulton (1968), Wallace (1986), Wallace and Dowe (1994)) uses the message lengths from various parameter estimates to enable it to combine parameter estimation with selection of the num- ber of components. The message length is (to within a constant) the logarithm of the posterior probability of the theory. So, the MML theory can also be re- garded as the theory with the highest posterior probability. Snob currently assumes that variables are uncorrelated, and permits multi-variate data from Gaussian, discrete multi-state, Poisson and von Mises circular distributions.",
        "bibtex": "@InProceedings{pmlr-vR1-wallace97a,\n  title = \t {MML Mixture Modelling of Multi-state, Poisson, vonMises circular and Gaussian Distributions},\n  author =       {Wallace, Chris S. and Dowe, David L.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {529--536},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/wallace97a/wallace97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/wallace97a.html},\n  abstract = \t {Minimum Message Length (MML) is an invariant Bayesian point estimation technique which is also consistent and efficient. We provide a brief overview of MML inductive inference (Wallace and Boulton (1968) , Wallace and Freeman (1987)), and how it has both an information-theoretic and a Bayesian interpretation. We then outline how MML is used for statistical parameter estimation, and how the MML mixture modelling program, Snob (Wallace and Boulton (1968), Wallace (1986), Wallace and Dowe (1994)) uses the message lengths from various parameter estimates to enable it to combine parameter estimation with selection of the num- ber of components. The message length is (to within a constant) the logarithm of the posterior probability of the theory. So, the MML theory can also be re- garded as the theory with the highest posterior probability. Snob currently assumes that variables are uncorrelated, and permits multi-variate data from Gaussian, discrete multi-state, Poisson and von Mises circular distributions.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/wallace97a/wallace97a.pdf",
        "supp": "",
        "pdf_size": 5283902,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17861760589947736589&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "Department of Computer Science, Monash University; Department of Computer Science, Monash University",
        "aff_domain": "cs.monash.edu.au;cs.monash.edu.au",
        "email": "cs.monash.edu.au;cs.monash.edu.au",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Monash University",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.monash.edu",
        "aff_unique_abbr": "Monash",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Australia"
    },
    {
        "id": "95fa0ad863",
        "title": "Markov chain Monte Carlo methods for decision analysis",
        "site": "https://proceedings.mlr.press/r1/bielza97a.html",
        "author": "Concha Bielza; Peter Muller; David Rios Insua",
        "abstract": "This paper considers an aspect of mixture modelling. Previous studies have shown minimum message length (MML) estimation to perform well in a wide variety of mixture modelling problems, including determining the number of components which best describes some data. In this paper, we focus on the difficult problem of overlapping components. An advantage of the probabilistic mixture modelling approach is its ability to identify models where the components overlap and data items can belong probabilistically to more than one component. Significantly overlapping distributions require more data for their parameters to be accurately estimated than well separated distributions. For example, two Gaussian distributions are considered to significantly overlap when their means are within three standard deviations of each other. If insufficient data is available, only a single component distribution will be estimated, although the data originates from two component distributions. In this paper, we quantify this difficulty in terms of the number of data items needed for the MML criterion to \u2019discover\u2019 two overlapping components. First, we perform experiments which compare the MML criterion\u2019s performance relative to other Bayesian criteria based on MCMC sampling. Second, we make two alterations to the existing MML estimates in order to improve its performance on overlapping distributions. Experiments are performed with the new estimates to confirm that they are effective.",
        "bibtex": "@InProceedings{pmlr-vR1-bielza97a,\n  title = \t {Markov chain {M}onte {C}arlo methods for decision analysis},\n  author =       {Bielza, Concha and Muller, Peter and Insua, David Rios},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {31--38},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/bielza97a/bielza97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/bielza97a.html},\n  abstract = \t {This paper considers an aspect of mixture modelling. Previous studies have shown minimum message length (MML) estimation to perform well in a wide variety of mixture modelling problems, including determining the number of components which best describes some data. In this paper, we focus on the difficult problem of overlapping components. An advantage of the probabilistic mixture modelling approach is its ability to identify models where the components overlap and data items can belong probabilistically to more than one component. Significantly overlapping distributions require more data for their parameters to be accurately estimated than well separated distributions. For example, two Gaussian distributions are considered to significantly overlap when their means are within three standard deviations of each other. If insufficient data is available, only a single component distribution will be estimated, although the data originates from two component distributions. In this paper, we quantify this difficulty in terms of the number of data items needed for the MML criterion to \u2019discover\u2019 two overlapping components. First, we perform experiments which compare the MML criterion\u2019s performance relative to other Bayesian criteria based on MCMC sampling. Second, we make two alterations to the existing MML estimates in order to improve its performance on overlapping distributions. Experiments are performed with the new estimates to confirm that they are effective.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/bielza97a/bielza97a.pdf",
        "supp": "",
        "pdf_size": 3581822,
        "gs_citation": 1,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17921021935642081430&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "623c0de89c",
        "title": "Memory Based Stochastic Optimization for Validation and Tuning of Function Approximators",
        "site": "https://proceedings.mlr.press/r1/dubrawski97a.html",
        "author": "Artur Dubrawski; Jeff Schneider",
        "abstract": "This paper focuses on the optimization of hyper-parameters for function approximators. We describe a kind of racing algorithm for continuous optimization problems that spends less time evaluating poor parameter settings and more time honing its estimates in the most promising regions of the parameter space. The algorithm is able to automatically optimize the parameters of a function approximator with less computation time. We demonstrate the algorithm on the problem of finding good parameters for a memory based learner and show the tradeoffs involved in choosing the right amount of computation to spend on each evaluation.",
        "bibtex": "@InProceedings{pmlr-vR1-dubrawski97a,\n  title = \t {Memory Based Stochastic Optimization for Validation and Tuning of Function Approximators},\n  author =       {Dubrawski, Artur and Schneider, Jeff},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {165--172},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/dubrawski97a/dubrawski97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/dubrawski97a.html},\n  abstract = \t {This paper focuses on the optimization of hyper-parameters for function approximators. We describe a kind of racing algorithm for continuous optimization problems that spends less time evaluating poor parameter settings and more time honing its estimates in the most promising regions of the parameter space. The algorithm is able to automatically optimize the parameters of a function approximator with less computation time. We demonstrate the algorithm on the problem of finding good parameters for a memory based learner and show the tradeoffs involved in choosing the right amount of computation to spend on each evaluation.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/dubrawski97a/dubrawski97a.pdf",
        "supp": "",
        "pdf_size": 3917547,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16709308331884478459&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 6,
        "aff": "The Robotics Institute, Carnegie Mellon University; The Robotics Institute, Carnegie Mellon University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "The Robotics Institute",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d8f4059fc9",
        "title": "Mixed Memory Markov Models",
        "site": "https://proceedings.mlr.press/r1/saul97a.html",
        "author": "Lawrence K. Saul; Michael I. Jordan",
        "abstract": "We consider how to parameterize Markov models with prohibitively large state spaces. This is done by representing the transition matrix as a convex combination-or mixtureof simpler dynamical models. The parameters in these models admit a simple probabilistic interpretation and can be fitted iteratively by an Expectation-Maximization (EM) procedure. We give examples where these models may be a faithful and/or useful representation of the underlying dynamics. We also derive a set of generalized Baum-Welch updates for hidden Markov models (HMMs) that make use of this parameterization. Because these models decompose the hidden state as the Cartesian product of two or more random variables, they are well suited to the modeling of coupled time series.",
        "bibtex": "@InProceedings{pmlr-vR1-saul97a,\n  title = \t {Mixed Memory {M}arkov Models},\n  author =       {Saul, Lawrence K. and Jordan, Michael I.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {437--444},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/saul97a/saul97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/saul97a.html},\n  abstract = \t {We consider how to parameterize Markov models with prohibitively large state spaces. This is done by representing the transition matrix as a convex combination-or mixtureof simpler dynamical models. The parameters in these models admit a simple probabilistic interpretation and can be fitted iteratively by an Expectation-Maximization (EM) procedure. We give examples where these models may be a faithful and/or useful representation of the underlying dynamics. We also derive a set of generalized Baum-Welch updates for hidden Markov models (HMMs) that make use of this parameterization. Because these models decompose the hidden state as the Cartesian product of two or more random variables, they are well suited to the modeling of coupled time series.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/saul97a/saul97a.pdf",
        "supp": "",
        "pdf_size": 4813637,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11824523229513557990&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Speech and Image Processing Services, AT&T Labs - Research; Center for Biological and Computational Learning, Massachusetts Institute of Technology",
        "aff_domain": "lsaul~research.att.com; jordan~psyche.mit.edu",
        "email": "lsaul~research.att.com; jordan~psyche.mit.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "AT&T Labs - Research;Massachusetts Institute of Technology",
        "aff_unique_dep": "Speech and Image Processing Services;Center for Biological and Computational Learning",
        "aff_unique_url": "https://www.research.att.com;https://www.mit.edu",
        "aff_unique_abbr": "AT&T Labs;MIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e2247ece3b",
        "title": "Multivariate Density Factorization for Independent Component Analysis: An Unsupervised Artificial Neural Network Approach",
        "site": "https://proceedings.mlr.press/r1/girolami97a.html",
        "author": "Mark Girolami; Colin Fyfe",
        "abstract": "We propose a novel homogenous nonlinear self-organising network which employs solely computationally simple hebbian and anti-hebbian learning, in approximating a linear independent component analysis (ICA). The learning algorithms diagonalise the transformed data covariance matrix and approximate an orthogonal rotation which maximises the sum offourth order cumulants. This provides factorisation of the input multivariate density into the individual independent latent marginal densities. We apply this network to linear mixtures of data, which are inherently non-gaussian and have both Laplacian and bi-modal probability densities. We show that the proposed network is capable of factorising multivariate densities which are linear mixtures of independent latent playkurtic, leptokurtic and uniform distributions.",
        "bibtex": "@InProceedings{pmlr-vR1-girolami97a,\n  title = \t {Multivariate Density Factorization for Independent Component Analysis: An Unsupervised Artificial Neural Network Approach},\n  author =       {Girolami, Mark and Fyfe, Colin},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {223--230},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/girolami97a/girolami97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/girolami97a.html},\n  abstract = \t {We propose a novel homogenous nonlinear self-organising network which employs solely computationally simple hebbian and anti-hebbian learning, in approximating a linear independent component analysis (ICA). The learning algorithms diagonalise the transformed data covariance matrix and approximate an orthogonal rotation which maximises the sum offourth order cumulants. This provides factorisation of the input multivariate density into the individual independent latent marginal densities. We apply this network to linear mixtures of data, which are inherently non-gaussian and have both Laplacian and bi-modal probability densities. We show that the proposed network is capable of factorising multivariate densities which are linear mixtures of independent latent playkurtic, leptokurtic and uniform distributions.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/girolami97a/girolami97a.pdf",
        "supp": "",
        "pdf_size": 3636949,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17683149465067419515&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Computing and Information Systems, University of Paisley, High Street, Paisley, Scotland, PAl 2BE; Department of Computing and Information Systems, University of Paisley, High Street, Paisley, Scotland, PAl 2BE",
        "aff_domain": "paisley.ac.uk;paisley.ac.uk",
        "email": "paisley.ac.uk;paisley.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Paisley",
        "aff_unique_dep": "Department of Computing and Information Systems",
        "aff_unique_url": "https://www.uop.ac.uk",
        "aff_unique_abbr": "UoP",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Paisley",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "c1b064beee",
        "title": "On Predictive Classification of Binary Vectors",
        "site": "https://proceedings.mlr.press/r1/gyllenberg97a.html",
        "author": "Mats Gyllenberg; Timo Koski",
        "abstract": "The problem of rational classification of a database of binary vectors is analyzed by means of a family of Bayesian predictive distributions on the binary hypercube. The general notion of predictive classification was probably first discussed by S. Geisser. The predictive distributions are expressed in terms of a finite number observables based on a given set of binary vectors (predictors or centroids) representing a system of classes and an entropy-maximizing family of probability distributions. We derive the (non-probabilistic) criterion of maximal predictive classification due to J . Gower (1974) as a special case of a Bayesian predictive classification. The notion of a predictive distribution will be related to stochastic complexity of a set of data with respect to a family of statistical distributions. An application to bacterial identification will be presented using a database of Enterobacteriaceae as in Gyllenberg (1996 c). A framework for the analysis is provided by a theorem about the merging of opinions due to Blackwell and Dubins (1962). We prove certain results about the asymptotic properties of the predictive learning process.",
        "bibtex": "@InProceedings{pmlr-vR1-gyllenberg97a,\n  title = \t {On Predictive Classification of Binary Vectors},\n  author =       {Gyllenberg, Mats and Koski, Timo},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {239--242},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/gyllenberg97a/gyllenberg97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/gyllenberg97a.html},\n  abstract = \t {The problem of rational classification of a database of binary vectors is analyzed by means of a family of Bayesian predictive distributions on the binary hypercube. The general notion of predictive classification was probably first discussed by S. Geisser. The predictive distributions are expressed in terms of a finite number observables based on a given set of binary vectors (predictors or centroids) representing a system of classes and an entropy-maximizing family of probability distributions. We derive the (non-probabilistic) criterion of maximal predictive classification due to J . Gower (1974) as a special case of a Bayesian predictive classification. The notion of a predictive distribution will be related to stochastic complexity of a set of data with respect to a family of statistical distributions. An application to bacterial identification will be presented using a database of Enterobacteriaceae as in Gyllenberg (1996 c). A framework for the analysis is provided by a theorem about the merging of opinions due to Blackwell and Dubins (1962). We prove certain results about the asymptotic properties of the predictive learning process.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/gyllenberg97a/gyllenberg97a.pdf",
        "supp": "",
        "pdf_size": 1397728,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:_y42nbzizAYJ:scholar.google.com/&scioq=On+Predictive+Classification+of+Binary+Vectors&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Department of Mathematics, University of Turku, Turku, Finland + Department of Mathematics, Royal Institute of Technology, Stockholm, Sweden; Department of Mathematics, University of Turku, Turku, Finland + Department of Mathematics, Royal Institute of Technology, Stockholm, Sweden",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0+1",
        "aff_unique_norm": "University of Turku;Royal Institute of Technology",
        "aff_unique_dep": "Department of Mathematics;Department of Mathematics",
        "aff_unique_url": "https://www.utu.fi;https://www.kth.se",
        "aff_unique_abbr": ";KTH",
        "aff_campus_unique_index": "0+1;0+1",
        "aff_campus_unique": "Turku;Stockholm",
        "aff_country_unique_index": "0+1;0+1",
        "aff_country_unique": "Finland;Sweden"
    },
    {
        "id": "32287759e0",
        "title": "On the Error Probability of Model Selection for Classification",
        "site": "https://proceedings.mlr.press/r1/suzuki97a.html",
        "author": "Joe Suzuki",
        "abstract": "We consider model selection based on information criteria for classification. The information criterion is expressed in the form of the empirical entropy plus a compensation term $(k(g)/2)d(n)$, where $k(g)$ is the number of independent parameters in a model $g$, $d(n)$ is a function of $n$, and $n$ is the number of examples. First of all, we derive for arbitrary $d(\\cdot)$ the asymptotically exact error probabilities in model selection. Although it was known for linear/autoregression processes that $d(n) = \\log \\log n$ is the minimum function of n such that the model selection satisfies strong consistency, the problem whether the same thing holds for classification has been open. We solve this problem affirmatively. Additionally, we derive for arbitrary $d(\\cdot)$ the expected Kullback-leibler divergence between a true conditional probability and the conditional probability estimated by the model selection and the Laplace estimators. The derived value is $k(g^*)/(2n)$, where $g^*$ is a true model, and the accumulated value over $n$ time instances is $(k(g*)/2) \\log n + 0(1)$, which implies the optimality of a predictive coding based on the model selection. Keywords: model selection, error probability, strong con- sistency, Kullback-Leibler divergence, minimum description length principle, Hannan/Quinn\u2019s procedure, unseparated/separated models, Kolmogorov\u2019s law of the iterated logarithm.",
        "bibtex": "@InProceedings{pmlr-vR1-suzuki97a,\n  title = \t {On the Error Probability of Model Selection for Classification},\n  author =       {Suzuki, Joe},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {513--520},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/suzuki97a/suzuki97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/suzuki97a.html},\n  abstract = \t {We consider model selection based on information criteria for classification. The information criterion is expressed in the form of the empirical entropy plus a compensation term $(k(g)/2)d(n)$, where $k(g)$ is the number of independent parameters in a model $g$, $d(n)$ is a function of $n$, and $n$ is the number of examples. First of all, we derive for arbitrary $d(\\cdot)$ the asymptotically exact error probabilities in model selection. Although it was known for linear/autoregression processes that $d(n) = \\log \\log n$ is the minimum function of n such that the model selection satisfies strong consistency, the problem whether the same thing holds for classification has been open. We solve this problem affirmatively. Additionally, we derive for arbitrary $d(\\cdot)$ the expected Kullback-leibler divergence between a true conditional probability and the conditional probability estimated by the model selection and the Laplace estimators. The derived value is $k(g^*)/(2n)$, where $g^*$ is a true model, and the accumulated value over $n$ time instances is $(k(g*)/2) \\log n + 0(1)$, which implies the optimality of a predictive coding based on the model selection. Keywords: model selection, error probability, strong con- sistency, Kullback-Leibler divergence, minimum description length principle, Hannan/Quinn\u2019s procedure, unseparated/separated models, Kolmogorov\u2019s law of the iterated logarithm.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/suzuki97a/suzuki97a.pdf",
        "supp": "",
        "pdf_size": 3470193,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:nn3oMzl_76MJ:scholar.google.com/&scioq=On+the+Error+Probability+of+Model+Selection+for+Classification&hl=en&as_sdt=0,33",
        "gs_version_total": 5,
        "aff": "Information Systems Laboratory, Stanford University",
        "aff_domain": "isl.stanford.edu",
        "email": "isl.stanford.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "Information Systems Laboratory",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e72a8980ba",
        "title": "Overfitting Explained",
        "site": "https://proceedings.mlr.press/r1/cohen97a.html",
        "author": "Paul R. Cohen; David Jensen",
        "abstract": "Overfitting arises when model components are evaluated against the wrong reference distribution. Most modeling algorithms iteratively find the best of several components and then test whether this component is good enough to add to the model. We show that for independently distributed random variables, the reference distribution for any one variable underestimates the reference distribution for the the highest-valued variable; thus variate values will appear significant when they are not, and model components will be added when they should not be added. We relate this problem to the well-known statistical theory of multiple comparisons or simultaneous inference.",
        "bibtex": "@InProceedings{pmlr-vR1-cohen97a,\n  title = \t {Overfitting Explained},\n  author =       {Cohen, Paul R. and Jensen, David},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {115--122},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/cohen97a/cohen97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/cohen97a.html},\n  abstract = \t {Overfitting arises when model components are evaluated against the wrong reference distribution. Most modeling algorithms iteratively find the best of several components and then test whether this component is good enough to add to the model. We show that for independently distributed random variables, the reference distribution for any one variable underestimates the reference distribution for the the highest-valued variable; thus variate values will appear significant when they are not, and model components will be added when they should not be added. We relate this problem to the well-known statistical theory of multiple comparisons or simultaneous inference.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/cohen97a/cohen97a.pdf",
        "supp": "",
        "pdf_size": 3324749,
        "gs_citation": 81,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1257327751531629951&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "6351e538cf",
        "title": "PAC Learning with Constant-Partition Classification Noise and Applications to Decision Tree Induction",
        "site": "https://proceedings.mlr.press/r1/decatur97a.html",
        "author": "Scott E. Decatur",
        "abstract": "We consider the problem of concept learning in Valiant\u2019s PAC learning model in which the data used for learning is noisy. Specifically, we introduce a new model of noise called \\emph{constant-partition classification noise} (CPCN) which generalizes the standard model of classification noise to allow different examples to have different rates of random misclassification. One example of CPCN type noise is data with differing rates of false positives and false negatives. We then show how to learn in the presense of CPCN for any concept class learnable by statistical queries. This set of classes includes every concept class known to be learnable in the presense of standard classification noise. Our model is the first such non-uniform generalization of the standard classification noise model that allows efficient learning of this wide range of concept classes. We then examine standard methods of decision tree induction in the context of noisy data. We observe that the core of commonly used algorithms such as ID3, CART and c4.5 are not robust to CPCN noise, or even to standard classification noise. We therefore propose a simple modification to these algorithms in order to make them robust against CPCN. The modification is based on the statistical query techniques for CPCN described above.",
        "bibtex": "@InProceedings{pmlr-vR1-decatur97a,\n  title = \t {PAC Learning with Constant-Partition Classification Noise and Applications to Decision Tree Induction},\n  author =       {Decatur, Scott E.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {147--156},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/decatur97a/decatur97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/decatur97a.html},\n  abstract = \t {We consider the problem of concept learning in Valiant\u2019s PAC learning model in which the data used for learning is noisy. Specifically, we introduce a new model of noise called \\emph{constant-partition classification noise} (CPCN) which generalizes the standard model of classification noise to allow different examples to have different rates of random misclassification. One example of CPCN type noise is data with differing rates of false positives and false negatives. We then show how to learn in the presense of CPCN for any concept class learnable by statistical queries. This set of classes includes every concept class known to be learnable in the presense of standard classification noise. Our model is the first such non-uniform generalization of the standard classification noise model that allows efficient learning of this wide range of concept classes. We then examine standard methods of decision tree induction in the context of noisy data. We observe that the core of commonly used algorithms such as ID3, CART and c4.5 are not robust to CPCN noise, or even to standard classification noise. We therefore propose a simple modification to these algorithms in order to make them robust against CPCN. The modification is based on the statistical query techniques for CPCN described above.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/decatur97a/decatur97a.pdf",
        "supp": "",
        "pdf_size": 5027220,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9774849602010043722&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 3,
        "aff": "",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "5cf0682208",
        "title": "Preface",
        "site": "https://proceedings.mlr.press/r1/madigan97a.html",
        "author": "David Madigan; Padhraic Smyth",
        "abstract": "",
        "bibtex": "@InProceedings{pmlr-vR1-madigan97a,\n  title = \t {Preface},\n  author =       {Madigan, David and Smyth, Padhraic},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {i--xiii},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/madigan97a/madigan97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/madigan97a.html},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/madigan97a/madigan97a.pdf",
        "supp": "",
        "pdf_size": 2783513,
        "gs_citation": 96,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14141043608347785016&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "97db67078c",
        "title": "Robust Interpretation of Neural Network models",
        "site": "https://proceedings.mlr.press/r1/intrator97a.html",
        "author": "Oma Intrator; Nathan Intrator",
        "abstract": "Artificial Neural Network seem very promising for regression and classification, especially for large covariate spaces. These methods represent a non-linear function as a composition of low dimensional ridge functions and therefore appear to be less sensitive to the dimensionality of the covariate space. However, due to non uniqueness of a global minimum and the existence of (possibly) many local minima, the model revealed by the network is non stable. We introduce a method to interpret neural network results which uses novel robustification techniques. This results in a robust interpretation of the model employed by the network. Simulated data from known models is used to demonstrate the interpretability results and to demonstrate the effects of different regularization methods on the robustness of the model. Graphical methods are introduced to present the interpretation results. We further demonstrate how interaction between covariates can be revealed. From this study we conclude that the interpretation method works well, but that NN models may sometimes be misinterpreted, especially if the approximations to the true model are less robust.",
        "bibtex": "@InProceedings{pmlr-vR1-intrator97a,\n  title = \t {Robust Interpretation of Neural Network models},\n  author =       {Intrator, Oma and Intrator, Nathan},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {255--262},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/intrator97a/intrator97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/intrator97a.html},\n  abstract = \t {Artificial Neural Network seem very promising for regression and classification, especially for large covariate spaces. These methods represent a non-linear function as a composition of low dimensional ridge functions and therefore appear to be less sensitive to the dimensionality of the covariate space. However, due to non uniqueness of a global minimum and the existence of (possibly) many local minima, the model revealed by the network is non stable. We introduce a method to interpret neural network results which uses novel robustification techniques. This results in a robust interpretation of the model employed by the network. Simulated data from known models is used to demonstrate the interpretability results and to demonstrate the effects of different regularization methods on the robustness of the model. Graphical methods are introduced to present the interpretation results. We further demonstrate how interaction between covariates can be revealed. From this study we conclude that the interpretation method works well, but that NN models may sometimes be misinterpreted, especially if the approximations to the true model are less robust.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/intrator97a/intrator97a.pdf",
        "supp": "",
        "pdf_size": 3752838,
        "gs_citation": 4,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=912463315981239552&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Hebrew Universit y + Brown University; Tel-Aviv University + Brown University",
        "aff_domain": "Orna-1ntrator\u00a9brown . edu; nin\u00a9cns.brown.edu",
        "email": "Orna-1ntrator\u00a9brown . edu; nin\u00a9cns.brown.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2+1",
        "aff_unique_norm": "Hebrew University of Jerusalem;Brown University;Tel Aviv University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.huji.ac.il;https://www.brown.edu;https://www.tau.ac.il",
        "aff_unique_abbr": "HUJI;Brown;TAU",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;0+1",
        "aff_country_unique": "Israel;United States"
    },
    {
        "id": "71ba11dfe0",
        "title": "Robust Parameter Learning in Bayesian Networks with Missing Data",
        "site": "https://proceedings.mlr.press/r1/ramoni97a.html",
        "author": "Marco Ramoni; Paola Sebastiani",
        "abstract": "Bayesian Belief Networks (BBNs) are a powerful formalism for knowledge representation and reasoning under uncertainty. During the past few years, Artificial Intelligence met\u00b7Statistics in the quest to develop effective methods to learn BBNs directly from databases. Unfortunately, real-world databases include missing and/or unreported data whose presence challenges traditional learning techniques, from both the theoretical and computational point of view. This paper introduces a new method to learn the probabilities defining a BBNs from databases with missing data. The intuition behind this method is close to the robust sensitivity analysis interpretation of probability: the method computes the extreme points of the set of possible distributions consistent with the available information and proceeds by refining this set as more information becomes available. This paper outlines the description of this method and presents some experimental results comparing this approach to the Gibbs Samplings.",
        "bibtex": "@InProceedings{pmlr-vR1-ramoni97a,\n  title = \t {Robust Parameter Learning in {B}ayesian Networks with Missing Data},\n  author =       {Ramoni, Marco and Sebastiani, Paola},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {399--406},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/ramoni97a/ramoni97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/ramoni97a.html},\n  abstract = \t {Bayesian Belief Networks (BBNs) are a powerful formalism for knowledge representation and reasoning under uncertainty. During the past few years, Artificial Intelligence met\u00b7Statistics in the quest to develop effective methods to learn BBNs directly from databases. Unfortunately, real-world databases include missing and/or unreported data whose presence challenges traditional learning techniques, from both the theoretical and computational point of view. This paper introduces a new method to learn the probabilities defining a BBNs from databases with missing data. The intuition behind this method is close to the robust sensitivity analysis interpretation of probability: the method computes the extreme points of the set of possible distributions consistent with the available information and proceeds by refining this set as more information becomes available. This paper outlines the description of this method and presents some experimental results comparing this approach to the Gibbs Samplings.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/ramoni97a/ramoni97a.pdf",
        "supp": "",
        "pdf_size": 3608257,
        "gs_citation": 10,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1348544733331901534&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Knowledge Media Institute The Open University; Department of Actuarial Science and Statistics City University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Open University;City University",
        "aff_unique_dep": "Knowledge Media Institute;Department of Actuarial Science and Statistics",
        "aff_unique_url": "https://www.open.ac.uk;https://www.city.ac.uk",
        "aff_unique_abbr": "OU;City U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "c58a742e6b",
        "title": "Statistical Aspects of Classification in Drifting Populations",
        "site": "https://proceedings.mlr.press/r1/taylor97a.html",
        "author": "C. C. Taylor; G. Nakhaeizadeh; G. Kunisch",
        "abstract": "This paper discusses ideas for adaptive learning which can capture dynamic aspects of real-world datasets. Broadly, we explore two approaches. The first examines ways o f updating the classification rule as suggested by some monitoring process (similar to those used in a quality control problem), and this is applied to linear, logistic and quadratic discriminant. The second approach examines nonparametric classifiers based explicitly on the data and ways in which the data can be dynamically adapted to improve the performance. These methods are tried out on simulated data and real data from the credit industry.",
        "bibtex": "@InProceedings{pmlr-vR1-taylor97a,\n  title = \t {Statistical Aspects of Classification in Drifting Populations},\n  author =       {Taylor, C. C. and Nakhaeizadeh, G. and Kunisch, G.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {521--528},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/taylor97a/taylor97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/taylor97a.html},\n  abstract = \t {This paper discusses ideas for adaptive learning which can capture dynamic aspects of real-world datasets. Broadly, we explore two approaches. The first examines ways o f updating the classification rule as suggested by some monitoring process (similar to those used in a quality control problem), and this is applied to linear, logistic and quadratic discriminant. The second approach examines nonparametric classifiers based explicitly on the data and ways in which the data can be dynamically adapted to improve the performance. These methods are tried out on simulated data and real data from the credit industry.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/taylor97a/taylor97a.pdf",
        "supp": "",
        "pdf_size": 3980895,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4192402246000427540&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Department of Statistics, University of Leeds, Leeds LS2 9JT, UK; Daimler-Benz Forschung und Technik, Postfach 2360, 89013 Ulm DE; Department of Stochastics, University of Ulm, Ulm DE",
        "aff_domain": "amsta.leeds.ac.uk;dbag.ulm.DaimlerBenz.com; ",
        "email": "amsta.leeds.ac.uk;dbag.ulm.DaimlerBenz.com; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of Leeds;Daimler-Benz Forschung und Technik;University of Ulm",
        "aff_unique_dep": "Department of Statistics;;Department of Stochastics",
        "aff_unique_url": "https://www.leeds.ac.uk;https://www.daimler.com;https://www.uni-ulm.de",
        "aff_unique_abbr": "Leeds;;Ulm",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Leeds;;Ulm",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United Kingdom;Germany"
    },
    {
        "id": "1007e6d080",
        "title": "Strategies for Model Mixing in Generalized Linear Models",
        "site": "https://proceedings.mlr.press/r1/clyde97a.html",
        "author": "Merlise Clyde",
        "abstract": "In linear regression models and generalized linear regression models (GLMs), there is often substantial uncertainty about the choice of covariates to include in the model. Both classical and Bayesian approaches that involve selecting a subset of covariates and making inferences conditional on that model choice ignore a major component of uncertainty in the problem. One approach for incorporating this form of model uncertainty into the analysis is by directly building into the model a vector of indicator variables $Y$ that reflects which covariates are included in the model ...",
        "bibtex": "@InProceedings{pmlr-vR1-clyde97a,\n  title = \t {Strategies for Model Mixing in Generalized Linear Models},\n  author =       {Clyde, Merlise},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {103--114},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/clyde97a/clyde97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/clyde97a.html},\n  abstract = \t {In linear regression models and generalized linear regression models (GLMs), there is often substantial uncertainty about the choice of covariates to include in the model. Both classical and Bayesian approaches that involve selecting a subset of covariates and making inferences conditional on that model choice ignore a major component of uncertainty in the problem. One approach for incorporating this form of model uncertainty into the analysis is by directly building into the model a vector of indicator variables $Y$ that reflects which covariates are included in the model ...},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/clyde97a/clyde97a.pdf",
        "supp": "",
        "pdf_size": 4837710,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:blapEYI__NIJ:scholar.google.com/&scioq=Strategies+for+Model+Mixing+in+Generalized+Linear+Models&hl=en&as_sdt=0,5",
        "gs_version_total": 4,
        "aff": "Institute of Statistics and Decision Sciences, Box 90251 Duke University, Durham, NC 27708-0251",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Institute of Statistics and Decision Sciences",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Durham",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "270741582a",
        "title": "The Effects of Training Set Size on Decision Tree Complexity",
        "site": "https://proceedings.mlr.press/r1/oates97b.html",
        "author": "Tim Oates; David Jensen",
        "abstract": "This paper presents experiments with 19 datasets and 5 decision tree pruning algorithms that show that increasing training set size often results in a linear increase in tree size, even when that additional complexity results in no significant increase in classification accuracy. Said differently, removing randomly selected training instances often results in trees that are substantially smaller and just as accurate as those built on all available training instances. This implies that decreases in tree size obtained by more sophisticated data reduction techniques should be decomposed into two parts: that which is due to reduction of training set size, and the remainder, which is due to how the method selects instances to discard. We perform this decomposition for one recent data reduction technique, John\u2019s ROBUST-c4.5 (John 1995), and show that a large percentage of its effect on tree size is attributable to the fact that it simply reduces the size of the training set. We conclude that random data reduction is a baseline against which more sophisticated data reduction techniques should be compared.",
        "bibtex": "@InProceedings{pmlr-vR1-oates97b,\n  title = \t {The Effects of Training Set Size on Decision Tree Complexity},\n  author =       {Oates, Tim and Jensen, David},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {379--390},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/oates97b/oates97b.pdf},\n  url = \t {https://proceedings.mlr.press/r1/oates97b.html},\n  abstract = \t {This paper presents experiments with 19 datasets and 5 decision tree pruning algorithms that show that increasing training set size often results in a linear increase in tree size, even when that additional complexity results in no significant increase in classification accuracy. Said differently, removing randomly selected training instances often results in trees that are substantially smaller and just as accurate as those built on all available training instances. This implies that decreases in tree size obtained by more sophisticated data reduction techniques should be decomposed into two parts: that which is due to reduction of training set size, and the remainder, which is due to how the method selects instances to discard. We perform this decomposition for one recent data reduction technique, John\u2019s ROBUST-c4.5 (John 1995), and show that a large percentage of its effect on tree size is attributable to the fact that it simply reduces the size of the training set. We conclude that random data reduction is a baseline against which more sophisticated data reduction techniques should be compared.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/oates97b/oates97b.pdf",
        "supp": "",
        "pdf_size": 5729176,
        "gs_citation": 239,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15697310219175642744&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 7,
        "aff": "Computer Science Department, LGRC University of Massachusetts; Computer Science Department, LGRC University of Massachusetts",
        "aff_domain": "cs.umass.edu;cs.umass.edu",
        "email": "cs.umass.edu;cs.umass.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Massachusetts",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.umass.edu",
        "aff_unique_abbr": "UMass",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Amherst",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "35f6a1da74",
        "title": "Using Classification Trees to Improve Causal Inferences in Observational Studies",
        "site": "https://proceedings.mlr.press/r1/cox97a.html",
        "author": "Louis Anthony Cox Jr",
        "abstract": "Much of the recent literature on AI and statistics has focused on how to use causal knowledge to enrich the set of valid causal inferences that can be drawn from available data and applied to practical problems such as decision-making, probabilistic diagnosis, and cost-effective control of systems. This paper examines classical problems of valid causal inference in observational studies, using epidemiological studies on the association between exposure to diesel exhaust (DE) and risk of lung cancer as a case study. It shows that one of the main applied computational tools of AI and statistics, classification tree analysis, can be adapted to help control or avoid many of the usual statistical threats to valid causal inference, and links this new use of classification trees to an established older literature on techniques for causal inference in social statistics based on elimination of competing (non-causal) explanations for observed associations. A strong link is then forged between an extension of classification tree analysis and modem AI and statistics approaches to causal modeling and inference based in directed acyclic graph (DAG) causal models and influence diagrams. This new link is based on the observation that classification tree analysis can be adapted to test the local Markov conditions that provide the critical defining structure of DAG models, as well as to quantify the conditional distributions of variables given the values of their parents - the key numerical information needed to quantify an influence diagram model. Finally, these insights are applied to available data on DE and lung cancer risks and are used to conclude that there is no evidence of a causal relation between them.",
        "bibtex": "@InProceedings{pmlr-vR1-cox97a,\n  title = \t {Using Classification Trees to Improve Causal Inferences in Observational Studies},\n  author =       {Cox, Jr, Louis Anthony},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {123--138},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/cox97a/cox97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/cox97a.html},\n  abstract = \t {Much of the recent literature on AI and statistics has focused on how to use causal knowledge to enrich the set of valid causal inferences that can be drawn from available data and applied to practical problems such as decision-making, probabilistic diagnosis, and cost-effective control of systems. This paper examines classical problems of valid causal inference in observational studies, using epidemiological studies on the association between exposure to diesel exhaust (DE) and risk of lung cancer as a case study. It shows that one of the main applied computational tools of AI and statistics, classification tree analysis, can be adapted to help control or avoid many of the usual statistical threats to valid causal inference, and links this new use of classification trees to an established older literature on techniques for causal inference in social statistics based on elimination of competing (non-causal) explanations for observed associations. A strong link is then forged between an extension of classification tree analysis and modem AI and statistics approaches to causal modeling and inference based in directed acyclic graph (DAG) causal models and influence diagrams. This new link is based on the observation that classification tree analysis can be adapted to test the local Markov conditions that provide the critical defining structure of DAG models, as well as to quantify the conditional distributions of variables given the values of their parents - the key numerical information needed to quantify an influence diagram model. Finally, these insights are applied to available data on DE and lung cancer risks and are used to conclude that there is no evidence of a causal relation between them.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/cox97a/cox97a.pdf",
        "supp": "",
        "pdf_size": 8494127,
        "gs_citation": 0,
        "gs_cited_by_link": "https://scholar.google.com/scholar?q=related:NeSdKaPAN-gJ:scholar.google.com/&scioq=Using+Classification+Trees+to+Improve+Causal+Inferences+in+Observational+Studies&hl=en&as_sdt=0,5",
        "gs_version_total": 2,
        "aff": "Cox Associates and Cornerstone Consulting Group",
        "aff_domain": "aol.com",
        "email": "aol.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Cox Associates",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "b70a397e9e",
        "title": "Using Prediction to Improve Combinatorial Optimization Search",
        "site": "https://proceedings.mlr.press/r1/boyan97a.html",
        "author": "Justin A. Boyan; Andrew W. Moore",
        "abstract": "This paper describes a statistical approach to improving the performance of stochastic search algorithms for optimization. Given a search algorithm $A$, we learn to predict the outcome of $A$ as a function of state features along a search trajectory. Predictions are made by a function approximator such as global or locally-weighted polynomial regression; training data is collected by Monte-Carlo simulation. Extrapolating from this data produces a new evaluation function which can bias future search trajectories toward better optima. Our implementation of this idea, STAGE, has produced very promising results on two large-scale domains.",
        "bibtex": "@InProceedings{pmlr-vR1-boyan97a,\n  title = \t {Using Prediction to Improve Combinatorial Optimization Search},\n  author =       {Boyan, Justin A. and Moore, Andrew W.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {55--66},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/boyan97a/boyan97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/boyan97a.html},\n  abstract = \t {This paper describes a statistical approach to improving the performance of stochastic search algorithms for optimization. Given a search algorithm $A$, we learn to predict the outcome of $A$ as a function of state features along a search trajectory. Predictions are made by a function approximator such as global or locally-weighted polynomial regression; training data is collected by Monte-Carlo simulation. Extrapolating from this data produces a new evaluation function which can bias future search trajectories toward better optima. Our implementation of this idea, STAGE, has produced very promising results on two large-scale domains.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/boyan97a/boyan97a.pdf",
        "supp": "",
        "pdf_size": 4052557,
        "gs_citation": 38,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3767478906987895706&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Computer Science Department; Carnegie Mellon University",
        "aff_domain": "jab\u00a9ks.cmu.edu; awm\u00a9ks.cmu.edu",
        "email": "jab\u00a9ks.cmu.edu; awm\u00a9ks.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Computer Science Department;Carnegie Mellon University",
        "aff_unique_dep": "Computer Science;",
        "aff_unique_url": ";https://www.cmu.edu",
        "aff_unique_abbr": ";CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "5fa81b5f35",
        "title": "Variational Inference for continuous Sigmoidal Bayesian Networks",
        "site": "https://proceedings.mlr.press/r1/frey97a.html",
        "author": "Brendan J. Frey",
        "abstract": "Latent random variables can be useful for modelling covariance relationships between observed variables. The choice of whether specific latent variables ought to be continuous or discrete is often an arbitrary one. In a previous paper, I presented a \"unit\" that could adapt to be continuous or binary, as appropriate for the current problem, and showed how a Markov chain Monte Carlo method could be used for inference and parameter estimation in Bayesian networks of these units. In this paper, I develop a variational inference technique in the hope that it will prove to be more computationally efficient than Monte Carlo methods. After presenting promising \\emph{inference} results on a toy problem, I discuss why the variational technique does not work well for \\emph{parameter estimation} as compared to Monte Carlo.",
        "bibtex": "@InProceedings{pmlr-vR1-frey97a,\n  title = \t {Variational Inference for continuous Sigmoidal Bayesian Networks},\n  author =       {Frey, Brendan J.},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {211--222},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/frey97a/frey97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/frey97a.html},\n  abstract = \t {Latent random variables can be useful for modelling covariance relationships between observed variables. The choice of whether specific latent variables ought to be continuous or discrete is often an arbitrary one. In a previous paper, I presented a \"unit\" that could adapt to be continuous or binary, as appropriate for the current problem, and showed how a Markov chain Monte Carlo method could be used for inference and parameter estimation in Bayesian networks of these units. In this paper, I develop a variational inference technique in the hope that it will prove to be more computationally efficient than Monte Carlo methods. After presenting promising \\emph{inference} results on a toy problem, I discuss why the variational technique does not work well for \\emph{parameter estimation} as compared to Monte Carlo.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/frey97a/frey97a.pdf",
        "supp": "",
        "pdf_size": 5278264,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10615087786193365370&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Computer Science, University of Toronto",
        "aff_domain": "",
        "email": "",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "84948b7e1c",
        "title": "WWW Cache Layout to Ease Network Overload",
        "site": "https://proceedings.mlr.press/r1/yoshida97a.html",
        "author": "Kenichi Yoshida",
        "abstract": "The GBI (graph-based induction) concept learning method is applied to extract typical access patterns of WWW data. By interpreting extracted patterns as the cache site layout we can reduce the total network data flow by implementing a distributed cache system which is adapted to the WWW access patterns. Although the huge WWW data flow causes the overflow of the conventional hierarchical cache system, the layout created by the GBI method eases this problem. The traffic reduction ratio of this distributed cache system is 2.5 times higher than that of the conventional hierarchical cache system. Our results suggest the importance of the data analyzing methods which can handle structured data. By analyzing regularity in graph structures, the GBI method can reduce the network data flow; The statistical criteria contribute to the analysis of promising patterns",
        "bibtex": "@InProceedings{pmlr-vR1-yoshida97a,\n  title = \t {WWW Cache Layout to Ease Network Overload},\n  author =       {Yoshida, Kenichi},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {537--548},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/yoshida97a/yoshida97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/yoshida97a.html},\n  abstract = \t {The GBI (graph-based induction) concept learning method is applied to extract typical access patterns of WWW data. By interpreting extracted patterns as the cache site layout we can reduce the total network data flow by implementing a distributed cache system which is adapted to the WWW access patterns. Although the huge WWW data flow causes the overflow of the conventional hierarchical cache system, the layout created by the GBI method eases this problem. The traffic reduction ratio of this distributed cache system is 2.5 times higher than that of the conventional hierarchical cache system. Our results suggest the importance of the data analyzing methods which can handle structured data. By analyzing regularity in graph structures, the GBI method can reduce the network data flow; The statistical criteria contribute to the analysis of promising patterns},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/yoshida97a/yoshida97a.pdf",
        "supp": "",
        "pdf_size": 5881639,
        "gs_citation": 2,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17987895442614497870&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 2,
        "aff": "Advanced Research Laboratory, Hitachi, Ltd.",
        "aff_domain": "hitachi.co.jp",
        "email": "hitachi.co.jp",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Hitachi, Ltd.",
        "aff_unique_dep": "Advanced Research Laboratory",
        "aff_unique_url": "https://www.hitachi.com",
        "aff_unique_abbr": "Hitachi",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Japan"
    },
    {
        "id": "1fde4350a5",
        "title": "Wavelet based Random Densities",
        "site": "https://proceedings.mlr.press/r1/insua97a.html",
        "author": "David Rios Insua; Brani Vidakovic",
        "abstract": "In this paper we describe theoretical properties of wavelet-based random densities and give algorithms for their generation. We exhibit random densities subject to some standard constraints: smoothness, modality, and skewness. We also give a relevant example of use of random densities.",
        "bibtex": "@InProceedings{pmlr-vR1-insua97a,\n  title = \t {Wavelet based Random Densities},\n  author =       {Insua, David Rios and Vidakovic, Brani},\n  booktitle = \t {Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics},\n  pages = \t {263--274},\n  year = \t {1997},\n  editor = \t {Madigan, David and Smyth, Padhraic},\n  volume = \t {R1},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {04--07 Jan},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/r1/insua97a/insua97a.pdf},\n  url = \t {https://proceedings.mlr.press/r1/insua97a.html},\n  abstract = \t {In this paper we describe theoretical properties of wavelet-based random densities and give algorithms for their generation. We exhibit random densities subject to some standard constraints: smoothness, modality, and skewness. We also give a relevant example of use of random densities.},\n  note =         {Reissued by PMLR on 30 March 2021.}\n}",
        "pdf": "http://proceedings.mlr.press/r1/insua97a/insua97a.pdf",
        "supp": "",
        "pdf_size": 4188440,
        "gs_citation": 6,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7779562119738216676&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Universidad Politecnica de Madrid; Duke University",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Universidad Politecnica de Madrid;Duke University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upm.es;https://www.duke.edu",
        "aff_unique_abbr": "UPM;Duke",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "Spain;United States"
    }
]