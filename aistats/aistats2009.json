[
    {
        "id": "57aeb49028",
        "title": "A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation",
        "site": "https://proceedings.mlr.press/v5/wood09a.html",
        "author": "Frank Wood; Yee Whye Teh",
        "abstract": "In this paper we present a doubly hierarchical Pitman-Yor process language model. Its bottom layer of hierarchy consists of multiple hierarchical Pitman-Yor process language models, one each for some number of domains. The novel top layer of hierarchy consists of a mechanism to couple together multiple language models such that they share statistical strength. Intuitively this sharing results in the \u201cadaptation\u201d of a latent shared language model to each domain. We introduce a general formalism capable of describing the overall model which we call the graphical Pitman-Yor process and explain how to perform Bayesian inference in it. We present encouraging language model domain adaptation results that both illustrate the potential benefits of our new model and suggest new avenues of inquiry.",
        "bibtex": "@InProceedings{pmlr-v5-wood09a,\n  title = \t {A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation},\n  author = \t {Wood, Frank and Teh, Yee Whye},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {607--614},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/wood09a/wood09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/wood09a.html},\n  abstract = \t {In this paper we present a doubly hierarchical Pitman-Yor process language model. Its bottom layer of hierarchy consists of multiple hierarchical Pitman-Yor process language models, one each for some number of domains. The novel top layer of hierarchy consists of a mechanism to couple together multiple language models such that they share statistical strength. Intuitively this sharing results in the \u201cadaptation\u201d of a latent shared language model to each domain. We introduce a general formalism capable of describing the overall model which we call the graphical Pitman-Yor process and explain how to perform Bayesian inference in it. We present encouraging language model domain adaptation results that both illustrate the potential benefits of our new model and suggest new avenues of inquiry.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/wood09a/wood09a.pdf",
        "supp": "",
        "pdf_size": 444568,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10184693171962588331&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Gatsby Computational Neuroscience Unit, University College London; Gatsby Computational Neuroscience Unit, University College London",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Gatsby Computational Neuroscience Unit",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "ed4e7a9c55",
        "title": "A New Perspective for Information Theoretic Feature Selection",
        "site": "https://proceedings.mlr.press/v5/brown09a.html",
        "author": "Gavin Brown",
        "abstract": "Feature Filters are among the simplest and  fastest approaches to feature selection. A \u201cfilter\u201d  defines a statistical criterion, used to rank  features on how useful they are expected to  be for classification. The highest ranking features are retained, and the lowest ranking can  be discarded. A common approach is to use  the Mutual Information between the features and class label. This area has seen a recent  flurry of activity, resulting in a confusing variety  of heuristic criteria all based on mutual  information, and a lack of a principled way  to understand or relate them. The contribution  of this paper is a unifying theoretical  understanding of such filters. In contrast to current  methods which manually construct filter criteria  with particular properties, we show how  to naturally derive a space of possible ranking  criteria. We will show that several recent  contributions in the feature selection literature  are points within this space, and that  there exist many points that have never been  explored.",
        "bibtex": "@InProceedings{pmlr-v5-brown09a,\n  title = \t {A New Perspective for Information Theoretic Feature Selection},\n  author = \t {Brown, Gavin},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {49--56},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/brown09a/brown09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/brown09a.html},\n  abstract = \t {Feature Filters are among the simplest and  fastest approaches to feature selection. A \u201cfilter\u201d  defines a statistical criterion, used to rank  features on how useful they are expected to  be for classification. The highest ranking features are retained, and the lowest ranking can  be discarded. A common approach is to use  the Mutual Information between the features and class label. This area has seen a recent  flurry of activity, resulting in a confusing variety  of heuristic criteria all based on mutual  information, and a lack of a principled way  to understand or relate them. The contribution  of this paper is a unifying theoretical  understanding of such filters. In contrast to current  methods which manually construct filter criteria  with particular properties, we show how  to naturally derive a space of possible ranking  criteria. We will show that several recent  contributions in the feature selection literature  are points within this space, and that  there exist many points that have never been  explored.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/brown09a/brown09a.pdf",
        "supp": "",
        "pdf_size": 760925,
        "gs_citation": 243,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2636133116172325280&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": "School of Computer Science, University of Manchester",
        "aff_domain": "manchester.ac.uk",
        "email": "manchester.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Manchester",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.manchester.ac.uk",
        "aff_unique_abbr": "UoM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Manchester",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "9e8e8bc416",
        "title": "A kernel method for unsupervised structured network inference",
        "site": "https://proceedings.mlr.press/v5/lippert09a.html",
        "author": "Christoph Lippert; Oliver Stegle; Zoubin Ghahramani; Karsten Borgwardt",
        "abstract": "Network inference is the problem of inferring edges between a set of  real-world objects, for instance, interactions between pairs of  proteins in bioinformatics.   Current kernel-based approaches to this problem share a set of   common features:   (i) they are supervised and hence require labeled training data; (ii)  edges in the network are treated as mutually independent   and hence topological properties are largely ignored; (iii) they lack  a statistical interpretation.   We argue that these common assumptions are often   undesirable for network inference, and propose (i) an   unsupervised kernel method (ii) that takes the global structure of the   network into account and (iii) is statistically motivated.   We show that our approach can explain commonly used heuristics in  statistical terms.  In experiments on social networks, different variants of our  method demonstrate appealing predictive performance.",
        "bibtex": "@InProceedings{pmlr-v5-lippert09a,\n  title = \t {A kernel method for unsupervised structured network inference},\n  author = \t {Lippert, Christoph and Stegle, Oliver and Ghahramani, Zoubin and Borgwardt, Karsten},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {368--375},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/lippert09a/lippert09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/lippert09a.html},\n  abstract = \t {Network inference is the problem of inferring edges between a set of  real-world objects, for instance, interactions between pairs of  proteins in bioinformatics.   Current kernel-based approaches to this problem share a set of   common features:   (i) they are supervised and hence require labeled training data; (ii)  edges in the network are treated as mutually independent   and hence topological properties are largely ignored; (iii) they lack  a statistical interpretation.   We argue that these common assumptions are often   undesirable for network inference, and propose (i) an   unsupervised kernel method (ii) that takes the global structure of the   network into account and (iii) is statistically motivated.   We show that our approach can explain commonly used heuristics in  statistical terms.  In experiments on social networks, different variants of our  method demonstrate appealing predictive performance.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/lippert09a/lippert09a.pdf",
        "supp": "",
        "pdf_size": 861925,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15259807325674126974&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "MPI for Biol. Cybernetics + MPI for Dev. Biology, T\u00a8 ubingen, Germany; Cavendish Laboratory, University of Cambridge, Cambridge, UK; Department of Engineering, University of Cambridge, Cambridge, UK; MPI for Biol. Cybernetics + MPI for Dev. Biology, T\u00a8 ubingen, Germany",
        "aff_domain": "tuebingen.mpg.de;cam.ac.uk;eng.cam.ac.uk;tuebingen.mpg.de",
        "email": "tuebingen.mpg.de;cam.ac.uk;eng.cam.ac.uk;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;2;0+1",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;Max Planck Institute for Developmental Biology;University of Cambridge",
        "aff_unique_dep": "Biological Cybernetics;Developmental Biology;Cavendish Laboratory",
        "aff_unique_url": "https://www.biolcyb.mpg.de;https://www.mpi-biochem.mpg.de;https://www.cam.ac.uk",
        "aff_unique_abbr": "MPIB;MPI for Dev. Biol.;Cambridge",
        "aff_campus_unique_index": "1;2;2;1",
        "aff_campus_unique": ";T\u00fcbingen;Cambridge",
        "aff_country_unique_index": "0+0;1;1;0+0",
        "aff_country_unique": "Germany;United Kingdom"
    },
    {
        "id": "7754e2bba3",
        "title": "Active Learning as Non-Convex Optimization",
        "site": "https://proceedings.mlr.press/v5/guillory09a.html",
        "author": "Andrew Guillory; Erick Chastain; Jeff Bilmes",
        "abstract": "We propose a new view of active learning algorithms as optimization. We show that many online active learning algorithms can be viewed as stochastic gradient descent on non-convex objective functions. Variations of some of these algorithms and objective functions have been previously proposed without noting this connection.  We also point out a connection between the standard min-margin offline active learning algorithm and non-convex losses.  Finally, we discuss and show empirically how viewing active learning as non-convex loss minimization helps explain two previously observed phenomena: certain active learning algorithms achieve better generalization error than passive learning algorithms on certain data sets and on other data sets many active learning algorithms are prone to local minima.",
        "bibtex": "@InProceedings{pmlr-v5-guillory09a,\n  title = \t {Active Learning as Non-Convex Optimization},\n  author = \t {Guillory, Andrew and Chastain, Erick and Bilmes, Jeff},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {201--208},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/guillory09a/guillory09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/guillory09a.html},\n  abstract = \t {We propose a new view of active learning algorithms as optimization. We show that many online active learning algorithms can be viewed as stochastic gradient descent on non-convex objective functions. Variations of some of these algorithms and objective functions have been previously proposed without noting this connection.  We also point out a connection between the standard min-margin offline active learning algorithm and non-convex losses.  Finally, we discuss and show empirically how viewing active learning as non-convex loss minimization helps explain two previously observed phenomena: certain active learning algorithms achieve better generalization error than passive learning algorithms on certain data sets and on other data sets many active learning algorithms are prone to local minima.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/guillory09a/guillory09a.pdf",
        "supp": "",
        "pdf_size": 1244552,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14627226265879521465&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "Computer Science and Engineering, University of Washington; Neurobiology and Behavior, University of Washington; Electrical Engineering, University of Washington",
        "aff_domain": "cs.washington.edu;u.washington.edu;ee.washington.edu",
        "email": "cs.washington.edu;u.washington.edu;ee.washington.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Computer Science and Engineering",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ac3d186e64",
        "title": "Active Sensing",
        "site": "https://proceedings.mlr.press/v5/yu09a.html",
        "author": "Shipeng Yu; Balaji Krishnapuram; Romer Rosales; R. Bharat Rao",
        "abstract": "Labels are often expensive to get, and this motivates active learning which chooses the most informative samples for label  acquisition. In this paper we study active sensing in a multi-view setting, motivated from many problems where grouped features are also expensive to obtain and need to be acquired (or sensed) actively (e.g., in cancer diagnosis each patient might  go through many tests such as CT, Ultrasound and MRI to get valuable  features). The strength of this model is that one actively sensed (sample, view) pair would improve the joint multi-view  classification on all the samples. For this purpose we extend the  Bayesian co-training framework such that it can handle missing views  in a principled way, and introduce two criteria for view acquisition.  Experiments on one toy data and two real-world medical problems show  the effectiveness of this model.",
        "bibtex": "@InProceedings{pmlr-v5-yu09a,\n  title = \t {Active Sensing},\n  author = \t {Yu, Shipeng and Krishnapuram, Balaji and Rosales, Romer and Rao, R. Bharat},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {639--646},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/yu09a/yu09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/yu09a.html},\n  abstract = \t {Labels are often expensive to get, and this motivates active learning which chooses the most informative samples for label  acquisition. In this paper we study active sensing in a multi-view setting, motivated from many problems where grouped features are also expensive to obtain and need to be acquired (or sensed) actively (e.g., in cancer diagnosis each patient might  go through many tests such as CT, Ultrasound and MRI to get valuable  features). The strength of this model is that one actively sensed (sample, view) pair would improve the joint multi-view  classification on all the samples. For this purpose we extend the  Bayesian co-training framework such that it can handle missing views  in a principled way, and introduce two criteria for view acquisition.  Experiments on one toy data and two real-world medical problems show  the effectiveness of this model.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/yu09a/yu09a.pdf",
        "supp": "",
        "pdf_size": 788583,
        "gs_citation": 45,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9595399411546510541&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": "CAD and Knowledge Solutions, Siemens Medical Solutions USA, Inc.; CAD and Knowledge Solutions, Siemens Medical Solutions USA, Inc.; CAD and Knowledge Solutions, Siemens Medical Solutions USA, Inc.; CAD and Knowledge Solutions, Siemens Medical Solutions USA, Inc.",
        "aff_domain": "siemens.com;siemens.com;siemens.com;siemens.com",
        "email": "siemens.com;siemens.com;siemens.com;siemens.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Siemens Medical Solutions USA, Inc.",
        "aff_unique_dep": "CAD and Knowledge Solutions",
        "aff_unique_url": "https://www.siemens-healthineers.com/",
        "aff_unique_abbr": "Siemens Med",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2404354ed2",
        "title": "An Expectation Maximization Algorithm for Continuous Markov Decision Processes with Arbitrary Reward",
        "site": "https://proceedings.mlr.press/v5/hoffman09a.html",
        "author": "Matthew Hoffman; Nando Freitas; Arnaud Doucet; Jan Peters",
        "abstract": "We derive a new expectation maximization algorithm for policy optimization in  linear Gaussian Markov decision processes, where the reward function is  parameterised in terms of a flexible mixture of Gaussians. This approach  exploits both analytical tractability and numerical optimization. Consequently,  on the one hand, it is more flexible and general than closed-form solutions,  such as the widely used linear quadratic Gaussian (LQG) controllers. On the  other hand, it is more accurate and faster than optimization methods that rely  on approximation and simulation. Partial analytical solutions (though costly)  eliminate the need for simulation and, hence, avoid approximation error. The  experiments will show that for the same cost of computation, policy  optimization methods that rely on analytical tractability have higher value  than the ones that rely on simulation.",
        "bibtex": "@InProceedings{pmlr-v5-hoffman09a,\n  title = \t {An Expectation Maximization Algorithm for Continuous Markov Decision Processes with Arbitrary Reward},\n  author = \t {Hoffman, Matthew and Freitas, Nando and Doucet, Arnaud and Peters, Jan},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {232--239},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/hoffman09a/hoffman09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/hoffman09a.html},\n  abstract = \t {We derive a new expectation maximization algorithm for policy optimization in  linear Gaussian Markov decision processes, where the reward function is  parameterised in terms of a flexible mixture of Gaussians. This approach  exploits both analytical tractability and numerical optimization. Consequently,  on the one hand, it is more flexible and general than closed-form solutions,  such as the widely used linear quadratic Gaussian (LQG) controllers. On the  other hand, it is more accurate and faster than optimization methods that rely  on approximation and simulation. Partial analytical solutions (though costly)  eliminate the need for simulation and, hence, avoid approximation error. The  experiments will show that for the same cost of computation, policy  optimization methods that rely on analytical tractability have higher value  than the ones that rely on simulation.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/hoffman09a/hoffman09a.pdf",
        "supp": "",
        "pdf_size": 903537,
        "gs_citation": 42,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1235002785238205577&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "Univ. of British Columbia, Computer Science; Univ. of British Columbia, Computer Science; Univ. of British Columbia, Computer Science; Max Planck Inst. for Biol. Cybernetics",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca;tuebingen.mpg.de",
        "email": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca;tuebingen.mpg.de",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of British Columbia;Max Planck Institute for Biological Cybernetics",
        "aff_unique_dep": "Computer Science;Biological Cybernetics",
        "aff_unique_url": "https://www.ubc.ca;https://www.biologie.kit.edu",
        "aff_unique_abbr": "UBC;MPIBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Canada;Germany"
    },
    {
        "id": "242db6dd61",
        "title": "An Information Geometry Approach for Distance Metric Learning",
        "site": "https://proceedings.mlr.press/v5/wang09c.html",
        "author": "Shijun Wang; Rong Jin",
        "abstract": "Metric learning is an important problem in machine learning and  pattern recognition. In this paper, we propose a framework for  metric learning based on information geometry. The key idea is to  construct two kernel matrices for the given training data: one is  based on the distance metric and the other is based on the assigned  class labels. Inspired by the idea of information geometry, we  relate these two kernel matrices to two Gaussian distributions, and  the difference between the two kernel matrices is then computed by  the Kullback-Leibler (KL) divergence between the two Gaussian  distributions. The optimal distance metric is then found by  minimizing the divergence between the two distributions. Based on  this idea, we present two metric learning algorithms, one for linear  distance metric and the other for nonlinear distance with the  introduction of a kernel function. Unlike many existing algorithms  for metric learning that require solving a non-trivial optimization  problem and therefore are computationally expensive when the data  dimension is high, the proposed algorithms have a closed-form  solution and are computationally more efficient. Extensive  experiments with data classification and face recognition show that  the proposed algorithms are comparable to or better than the  state-of-the-art algorithms for metric learning.",
        "bibtex": "@InProceedings{pmlr-v5-wang09c,\n  title = \t {An Information Geometry Approach for Distance Metric Learning},\n  author = \t {Wang, Shijun and Jin, Rong},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {591--598},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/wang09c/wang09c.pdf},\n  url = \t {https://proceedings.mlr.press/v5/wang09c.html},\n  abstract = \t {Metric learning is an important problem in machine learning and  pattern recognition. In this paper, we propose a framework for  metric learning based on information geometry. The key idea is to  construct two kernel matrices for the given training data: one is  based on the distance metric and the other is based on the assigned  class labels. Inspired by the idea of information geometry, we  relate these two kernel matrices to two Gaussian distributions, and  the difference between the two kernel matrices is then computed by  the Kullback-Leibler (KL) divergence between the two Gaussian  distributions. The optimal distance metric is then found by  minimizing the divergence between the two distributions. Based on  this idea, we present two metric learning algorithms, one for linear  distance metric and the other for nonlinear distance with the  introduction of a kernel function. Unlike many existing algorithms  for metric learning that require solving a non-trivial optimization  problem and therefore are computationally expensive when the data  dimension is high, the proposed algorithms have a closed-form  solution and are computationally more efficient. Extensive  experiments with data classification and face recognition show that  the proposed algorithms are comparable to or better than the  state-of-the-art algorithms for metric learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/wang09c/wang09c.pdf",
        "supp": "",
        "pdf_size": 920582,
        "gs_citation": 54,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1314592954932193725&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Dept. of Radiology and Imaging Sciences, National Institutes of Health; Dept. of Computer Science and Engineering, Michigan State University",
        "aff_domain": "; ",
        "email": "; ",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "National Institutes of Health;Michigan State University",
        "aff_unique_dep": "Dept. of Radiology and Imaging Sciences;Dept. of Computer Science and Engineering",
        "aff_unique_url": "https://www.nih.gov;https://www.msu.edu",
        "aff_unique_abbr": "NIH;MSU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d9aa6cd1e1",
        "title": "Choosing a Variable to Clamp",
        "site": "https://proceedings.mlr.press/v5/eaton09a.html",
        "author": "Frederik Eaton; Zoubin Ghahramani",
        "abstract": "In this paper we propose an algorithm for  approximate inference on graphical models  based on belief propagation (BP). Our algorithm  is an approximate version of Cutset  Conditioning, in which a set of variables is  instantiated to make the rest of the graph  singly connected. We relax the constraint  of single-connectedness, and select variables  one at a time for conditioning, running belief  propagation after each selection. We consider  the problem of determining the best variable  to clamp at each level of recursion, and  propose a fast heuristic which applies backpropagation  to the BP updates. We demonstrate  that the heuristic performs better than  selecting variables at random, and give experimental  results which show that it performs  competitively with existing approximate inference  algorithms.",
        "bibtex": "@InProceedings{pmlr-v5-eaton09a,\n  title = \t {Choosing a Variable to Clamp},\n  author = \t {Eaton, Frederik and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {145--152},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/eaton09a/eaton09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/eaton09a.html},\n  abstract = \t {In this paper we propose an algorithm for  approximate inference on graphical models  based on belief propagation (BP). Our algorithm  is an approximate version of Cutset  Conditioning, in which a set of variables is  instantiated to make the rest of the graph  singly connected. We relax the constraint  of single-connectedness, and select variables  one at a time for conditioning, running belief  propagation after each selection. We consider  the problem of determining the best variable  to clamp at each level of recursion, and  propose a fast heuristic which applies backpropagation  to the BP updates. We demonstrate  that the heuristic performs better than  selecting variables at random, and give experimental  results which show that it performs  competitively with existing approximate inference  algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/eaton09a/eaton09a.pdf",
        "supp": "",
        "pdf_size": 515626,
        "gs_citation": 50,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14875812933601418682&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Department of Engineering, University of Cambridge, UK; Department of Engineering, University of Cambridge, UK",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Cambridge",
        "aff_unique_dep": "Department of Engineering",
        "aff_unique_url": "https://www.cam.ac.uk",
        "aff_unique_abbr": "Cambridge",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "bc7f55aaba",
        "title": "Chromatic PAC-Bayes Bounds for Non-IID Data",
        "site": "https://proceedings.mlr.press/v5/ralaivola09a.html",
        "author": "Liva Ralaivola; Marie Szafranski; Guillaume Stempfel",
        "abstract": "PAC-Bayes bounds are among the most accurate generalization bounds for classifiers learned with IID data, and it    is particularly so for margin classifiers.  However, there are many    practical cases where the training data show some dependencies and    where the traditional IID assumption does not apply. Stating    generalization bounds for such frameworks is therefore of the utmost    interest, both from theoretical and practical standpoints. In this work, we propose the first \u2013\u00a0to the best of our knowledge\u00a0\u2013  PAC-Bayes generalization bounds for classifiers trained on data    exhibiting dependencies. The approach undertaken to establish our    results is based on the decomposition of a so-called dependency    graph that encodes the dependencies within the data, in sets of    independent data, through the tool of graph fractional covers.  Our    bounds are very general, since being able to find an upper bound on    the (fractional) chromatic number of the dependency graph is    sufficient to get new PAC-Bayes bounds for specific settings. We show how our results can be used to derive bounds for bipartite    ranking and windowed prediction on sequential data.",
        "bibtex": "@InProceedings{pmlr-v5-ralaivola09a,\n  title = \t {Chromatic PAC-Bayes Bounds for Non-IID Data},\n  author = \t {Ralaivola, Liva and Szafranski, Marie and Stempfel, Guillaume},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {416--423},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/ralaivola09a/ralaivola09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/ralaivola09a.html},\n  abstract = \t {PAC-Bayes bounds are among the most accurate generalization bounds for classifiers learned with IID data, and it    is particularly so for margin classifiers.  However, there are many    practical cases where the training data show some dependencies and    where the traditional IID assumption does not apply. Stating    generalization bounds for such frameworks is therefore of the utmost    interest, both from theoretical and practical standpoints. In this work, we propose the first \u2013\u00a0to the best of our knowledge\u00a0\u2013  PAC-Bayes generalization bounds for classifiers trained on data    exhibiting dependencies. The approach undertaken to establish our    results is based on the decomposition of a so-called dependency    graph that encodes the dependencies within the data, in sets of    independent data, through the tool of graph fractional covers.  Our    bounds are very general, since being able to find an upper bound on    the (fractional) chromatic number of the dependency graph is    sufficient to get new PAC-Bayes bounds for specific settings. We show how our results can be used to derive bounds for bipartite    ranking and windowed prediction on sequential data.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/ralaivola09a/ralaivola09a.pdf",
        "supp": "",
        "pdf_size": 1296318,
        "gs_citation": 19,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8803821254271785416&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Laboratoire d\u2019Informatique Fondamentale de Marseille; Laboratoire d\u2019Informatique Fondamentale de Marseille; Laboratoire d\u2019Informatique Fondamentale de Marseille",
        "aff_domain": "lif.univ-mrs.fr;lif.univ-mrs.fr;lif.univ-mrs.fr",
        "email": "lif.univ-mrs.fr;lif.univ-mrs.fr;lif.univ-mrs.fr",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Laboratoire d\u2019Informatique Fondamentale de Marseille",
        "aff_unique_dep": "Laboratoire d\u2019Informatique Fondamentale",
        "aff_unique_url": "https://lifm.univ-mrs.fr",
        "aff_unique_abbr": "LIFM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "90cca3ec2a",
        "title": "Clusterability: A Theoretical Study",
        "site": "https://proceedings.mlr.press/v5/ackerman09a.html",
        "author": "Margareta Ackerman; Shai Ben-David",
        "abstract": "We investigate measures of the clusterability of data sets. Namely, ways to define how \u2018strong\u2019 or \u2018conclusive\u2019 is the clustering structure of a given data set. We address this issue with generality, aiming for conclusions that apply regardless of any particular clustering algorithm or any specific data generation  model.    We survey several notions of clusterability that have been discussed in the literature, as well as propose a new notion of data clusterability.    Our comparison of these notions reveals that, although they all attempt to evaluate the same intuitive property, they are pairwise inconsistent.    Our analysis discovers an interesting phenomenon; Although most of the common clustering tasks are NP-hard, finding a close-to-optimal clustering for well-clusterable data sets is easy (computationally). We prove instances of this general claim with respect to the various clusterability notions that we discuss.    Finally, we investigate how hard it is to determine the clusterability value of a given data set. In most cases, it turns out that this is an NP-hard problem.",
        "bibtex": "@InProceedings{pmlr-v5-ackerman09a,\n  title = \t {Clusterability: A Theoretical Study},\n  author = \t {Ackerman, Margareta and Ben-David, Shai},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {1--8},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/ackerman09a/ackerman09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/ackerman09a.html},\n  abstract = \t {We investigate measures of the clusterability of data sets. Namely, ways to define how \u2018strong\u2019 or \u2018conclusive\u2019 is the clustering structure of a given data set. We address this issue with generality, aiming for conclusions that apply regardless of any particular clustering algorithm or any specific data generation  model.    We survey several notions of clusterability that have been discussed in the literature, as well as propose a new notion of data clusterability.    Our comparison of these notions reveals that, although they all attempt to evaluate the same intuitive property, they are pairwise inconsistent.    Our analysis discovers an interesting phenomenon; Although most of the common clustering tasks are NP-hard, finding a close-to-optimal clustering for well-clusterable data sets is easy (computationally). We prove instances of this general claim with respect to the various clusterability notions that we discuss.    Finally, we investigate how hard it is to determine the clusterability value of a given data set. In most cases, it turns out that this is an NP-hard problem.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/ackerman09a/ackerman09a.pdf",
        "supp": "",
        "pdf_size": 811743,
        "gs_citation": 181,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2012144705321976317&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "University of Waterloo; University of Waterloo",
        "aff_domain": "uwaterloo.ca;cs.uwaterloo.ca",
        "email": "uwaterloo.ca;cs.uwaterloo.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Waterloo",
        "aff_unique_dep": "",
        "aff_unique_url": "https://uwaterloo.ca",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "d87c98a5a2",
        "title": "Coherence Functions for Multicategory Margin-based Classification Methods",
        "site": "https://proceedings.mlr.press/v5/zhang09a.html",
        "author": "Zhihua Zhang; Michael Jordan; Wu-Jun Li; Dit-Yan Yeung",
        "abstract": "Margin-based classification methods are typically devised based on a majorization-minimization procedure, which approximately solves an  otherwise intractable minimization problem defined with the 0-l loss. However, extension of such methods from the binary classification setting to the more general multicategory setting turns out to be non-trivial. In this paper, our focus is to devise margin-based classification methods that can be seamlessly applied to both settings, with the binary setting simply as a special case. In particular, we propose a new majorization loss function that we call the coherence function, and then devise a new multicategory margin-based boosting algorithm based on the coherence function. Analogous to deterministic annealing, the coherence function is characterized by a temperature factor. It is closely related to the multinomial log-likelihood function and its limit at zero temperature corresponds to a multicategory hinge loss function.",
        "bibtex": "@InProceedings{pmlr-v5-zhang09a,\n  title = \t {Coherence Functions for Multicategory Margin-based Classification Methods},\n  author = \t {Zhang, Zhihua and Jordan, Michael and Li, Wu-Jun and Yeung, Dit-Yan},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {647--654},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/zhang09a/zhang09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/zhang09a.html},\n  abstract = \t {Margin-based classification methods are typically devised based on a majorization-minimization procedure, which approximately solves an  otherwise intractable minimization problem defined with the 0-l loss. However, extension of such methods from the binary classification setting to the more general multicategory setting turns out to be non-trivial. In this paper, our focus is to devise margin-based classification methods that can be seamlessly applied to both settings, with the binary setting simply as a special case. In particular, we propose a new majorization loss function that we call the coherence function, and then devise a new multicategory margin-based boosting algorithm based on the coherence function. Analogous to deterministic annealing, the coherence function is characterized by a temperature factor. It is closely related to the multinomial log-likelihood function and its limit at zero temperature corresponds to a multicategory hinge loss function.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/zhang09a/zhang09a.pdf",
        "supp": "",
        "pdf_size": 574101,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4038146345295857093&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "568507f1b6",
        "title": "Convex Perturbations for Scalable Semidefinite Programming",
        "site": "https://proceedings.mlr.press/v5/kulis09a.html",
        "author": "Brian Kulis; Suvrit Sra; Inderjit Dhillon",
        "abstract": "Many important machine learning problems are modeled and solved via semidefinite programs; examples include metric learning, nonlinear embedding, and certain clustering problems. Often, off-the-shelf software is invoked for the associated optimization, which can be inappropriate due to excessive computational and storage requirements.  In this paper, we introduce the use of convex perturbations for solving semidefinite programs (SDPs), and for a specific perturbation we derive an algorithm that has several advantages over existing techniques: a) it is simple, requiring only a few lines of Matlab, b) it is a first-order method, and thereby scalable, and c) it can easily exploit the structure of a given SDP (e.g., when the constraint matrices are low-rank, a situation common to several machine learning SDPs). A pleasant byproduct of our method is a fast, kernelized version of the large-margin nearest neighbor metric learning algorithm.  We demonstrate that our algorithm is effective in finding fast approximations to large-scale SDPs arising in some machine learning applications.",
        "bibtex": "@InProceedings{pmlr-v5-kulis09a,\n  title = \t {Convex Perturbations for Scalable Semidefinite Programming},\n  author = \t {Kulis, Brian and Sra, Suvrit and Dhillon, Inderjit},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {296--303},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/kulis09a/kulis09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/kulis09a.html},\n  abstract = \t {Many important machine learning problems are modeled and solved via semidefinite programs; examples include metric learning, nonlinear embedding, and certain clustering problems. Often, off-the-shelf software is invoked for the associated optimization, which can be inappropriate due to excessive computational and storage requirements.  In this paper, we introduce the use of convex perturbations for solving semidefinite programs (SDPs), and for a specific perturbation we derive an algorithm that has several advantages over existing techniques: a) it is simple, requiring only a few lines of Matlab, b) it is a first-order method, and thereby scalable, and c) it can easily exploit the structure of a given SDP (e.g., when the constraint matrices are low-rank, a situation common to several machine learning SDPs). A pleasant byproduct of our method is a fast, kernelized version of the large-margin nearest neighbor metric learning algorithm.  We demonstrate that our algorithm is effective in finding fast approximations to large-scale SDPs arising in some machine learning applications.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/kulis09a/kulis09a.pdf",
        "supp": "",
        "pdf_size": 351597,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=264827902673752917&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "UC Berkeley EECS and ICSI, Berkeley, CA 94720; MPI for Biological Cybernetics, 72076 T\u00fcbingen, Germany; Department of Computer Sciences, University of Texas at Austin, Austin, TX 78712",
        "aff_domain": "; ; ",
        "email": "; ; ",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "University of California, Berkeley;Max Planck Institute for Biological Cybernetics;University of Texas at Austin",
        "aff_unique_dep": "Electrical Engineering and Computer Sciences;;Department of Computer Sciences",
        "aff_unique_url": "https://www.berkeley.edu;https://www.biological-cybernetics.de;https://www.utexas.edu",
        "aff_unique_abbr": "UC Berkeley;MPIBC;UT Austin",
        "aff_campus_unique_index": "0;1;2",
        "aff_campus_unique": "Berkeley;T\u00fcbingen;Austin",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "70723f1191",
        "title": "Covariance Operator Based Dimensionality Reduction  with Extension to Semi-Supervised Settings",
        "site": "https://proceedings.mlr.press/v5/kim09a.html",
        "author": "Minyoung Kim; Vladimir Pavlovic",
        "abstract": "We consider the task of dimensionality reduction for regression (DRR) informed by real-valued multivariate labels. The problem is often treated as a regression task where the goal is to find a low dimensional representation of the input data that preserves the statistical correlation with the targets. Recently, Covariance Operator Inverse Regression (COIR) was proposed as an effective solution that exploits the covariance structures of both input and output. COIR addresses known limitations of recent DRR techniques and allows a closed-form solution without resorting to explicit output space slicing often required by existing IR-based methods. In this work we provide a unifying view of COIR and other DRR techniques and relate them to the popular supervised dimensionality reduction methods including the canonical correlation analysis (CCA) and the linear discriminant analysis (LDA). We then show that COIR can be effectively extended to a semi-supervised learning setting where many of the input points lack their corresponding multivariate targets. A study of benefits of proposed approaches is presented on several important regression problems in both fully-supervised and semi-supervised settings.",
        "bibtex": "@InProceedings{pmlr-v5-kim09a,\n  title = \t {Covariance Operator Based Dimensionality Reduction  with Extension to Semi-Supervised Settings},\n  author = \t {Kim, Minyoung and Pavlovic, Vladimir},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {280--287},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/kim09a/kim09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/kim09a.html},\n  abstract = \t {We consider the task of dimensionality reduction for regression (DRR) informed by real-valued multivariate labels. The problem is often treated as a regression task where the goal is to find a low dimensional representation of the input data that preserves the statistical correlation with the targets. Recently, Covariance Operator Inverse Regression (COIR) was proposed as an effective solution that exploits the covariance structures of both input and output. COIR addresses known limitations of recent DRR techniques and allows a closed-form solution without resorting to explicit output space slicing often required by existing IR-based methods. In this work we provide a unifying view of COIR and other DRR techniques and relate them to the popular supervised dimensionality reduction methods including the canonical correlation analysis (CCA) and the linear discriminant analysis (LDA). We then show that COIR can be effectively extended to a semi-supervised learning setting where many of the input points lack their corresponding multivariate targets. A study of benefits of proposed approaches is presented on several important regression problems in both fully-supervised and semi-supervised settings.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/kim09a/kim09a.pdf",
        "supp": "",
        "pdf_size": 2112709,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8312026571324518664&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "fddf1deac9",
        "title": "Data Biased Robust Counter Strategies",
        "site": "https://proceedings.mlr.press/v5/johanson09a.html",
        "author": "Michael Johanson; Michael Bowling",
        "abstract": "The problem of exploiting information about the environment while still being robust to inaccurate or incomplete information arises in many domains. Competitive imperfect information games where the goal is to maximally exploit an unknown opponent\u2019s weaknesses are an example of this problem.  Agents for these games must balance two objectives.  First, they should aim to exploit data from past interactions with the opponent, seeking a best-response counter strategy.  Second, they should aim to minimize losses since the limited data may be misleading or the opponent\u2019s strategy may have changed, suggesting an opponent-agnostic Nash equilibrium strategy.  In this paper, we show how to partially satisfy both of these objectives at the same time, producing strategies with favorable tradeoffs between the ability to exploit an opponent and the capacity to be exploited.  Like a recently published technique, our approach involves solving a modified game; however the result is more generally applicable and even performs well in situations with very limited data.  We evaluate our technique in the game of two-player, Limit Texas Hold\u2019em.",
        "bibtex": "@InProceedings{pmlr-v5-johanson09a,\n  title = \t {Data Biased Robust Counter Strategies},\n  author = \t {Johanson, Michael and Bowling, Michael},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {264--271},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/johanson09a/johanson09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/johanson09a.html},\n  abstract = \t {The problem of exploiting information about the environment while still being robust to inaccurate or incomplete information arises in many domains. Competitive imperfect information games where the goal is to maximally exploit an unknown opponent\u2019s weaknesses are an example of this problem.  Agents for these games must balance two objectives.  First, they should aim to exploit data from past interactions with the opponent, seeking a best-response counter strategy.  Second, they should aim to minimize losses since the limited data may be misleading or the opponent\u2019s strategy may have changed, suggesting an opponent-agnostic Nash equilibrium strategy.  In this paper, we show how to partially satisfy both of these objectives at the same time, producing strategies with favorable tradeoffs between the ability to exploit an opponent and the capacity to be exploited.  Like a recently published technique, our approach involves solving a modified game; however the result is more generally applicable and even performs well in situations with very limited data.  We evaluate our technique in the game of two-player, Limit Texas Hold\u2019em.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/johanson09a/johanson09a.pdf",
        "supp": "",
        "pdf_size": 680389,
        "gs_citation": 82,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15216551894955825663&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Department of Computing Science, University of Alberta, Edmonton, Alberta, Canada; Department of Computing Science, University of Alberta, Edmonton, Alberta, Canada",
        "aff_domain": "cs.ualberta.ca;cs.ualberta.ca",
        "email": "cs.ualberta.ca;cs.ualberta.ca",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Alberta",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.ualberta.ca",
        "aff_unique_abbr": "UAlberta",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Edmonton",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "8108ace4e1",
        "title": "Deep Boltzmann Machines",
        "site": "https://proceedings.mlr.press/v5/salakhutdinov09a.html",
        "author": "Ruslan Salakhutdinov; Geoffrey Hinton",
        "abstract": "We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.",
        "bibtex": "@InProceedings{pmlr-v5-salakhutdinov09a,\n  title = \t {Deep Boltzmann Machines},\n  author = \t {Salakhutdinov, Ruslan and Hinton, Geoffrey},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {448--455},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/salakhutdinov09a.html},\n  abstract = \t {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer \u201cpre-training\u201d phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf",
        "supp": "",
        "pdf_size": 553762,
        "gs_citation": 3213,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12383446974530989248&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 28,
        "aff": "Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto",
        "aff_domain": "cs.toronto.edu;cs.toronto.edu",
        "email": "cs.toronto.edu;cs.toronto.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Toronto",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.utoronto.ca",
        "aff_unique_abbr": "U of T",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Toronto",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "3ad16efff2",
        "title": "Deep Learning using Robust Interdependent Codes",
        "site": "https://proceedings.mlr.press/v5/larochelle09a.html",
        "author": "Hugo Larochelle; Dumitru Erhan; Pascal Vincent",
        "abstract": "We investigate a simple yet effective method to introduce inhibitory and  excitatory interactions between units in the layers of a deep neural  network classifier. The method is based on the greedy layer-wise procedure  of deep learning algorithms and extends the denoising autoencoder of (Vincent et al., 2008) by adding asymmetric  lateral connections between its hidden coding units, in a manner that is much simpler and  computationally more efficient than previously proposed approaches. We present experiments on two character recognition problems which show for  the first time that lateral connections can significantly improve the  classification performance of deep networks.",
        "bibtex": "@InProceedings{pmlr-v5-larochelle09a,\n  title = \t {Deep Learning using Robust Interdependent Codes},\n  author = \t {Larochelle, Hugo and Erhan, Dumitru and Vincent, Pascal},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {312--319},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/larochelle09a/larochelle09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/larochelle09a.html},\n  abstract = \t {We investigate a simple yet effective method to introduce inhibitory and  excitatory interactions between units in the layers of a deep neural  network classifier. The method is based on the greedy layer-wise procedure  of deep learning algorithms and extends the denoising autoencoder of (Vincent et al., 2008) by adding asymmetric  lateral connections between its hidden coding units, in a manner that is much simpler and  computationally more efficient than previously proposed approaches. We present experiments on two character recognition problems which show for  the first time that lateral connections can significantly improve the  classification performance of deep networks.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/larochelle09a/larochelle09a.pdf",
        "supp": "",
        "pdf_size": 1030455,
        "gs_citation": 46,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12028133223499698784&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "69fef150dd",
        "title": "Distilled sensing: selective sampling for sparse signal recovery",
        "site": "https://proceedings.mlr.press/v5/haupt09a.html",
        "author": "Jarvis Haupt; Rui Castro; Robert Nowak",
        "abstract": "A selective sampling methodology called Distilled Sensing (DS) is proposed for recovering sparse signals in noise.  DS exploits the fact that it is often easier to rule out locations that do not contain signal than it is to detect the locations of non-zero signal components.  We formalize this observation and use it to devise a sequential selective sensing strategy that focuses sensing/measurement resources towards the signal subspace.  This adaptivity in sensing results in rather surprising gains in sparse signal recovery compared to non-adaptive sensing.  We show that exponentially weaker sparse signals can be recovered via DS compared with conventional non-adaptive sensing.",
        "bibtex": "@InProceedings{pmlr-v5-haupt09a,\n  title = \t {Distilled sensing: selective sampling for sparse signal recovery},\n  author = \t {Haupt, Jarvis and Castro, Rui and Nowak, Robert},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {216--223},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/haupt09a/haupt09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/haupt09a.html},\n  abstract = \t {A selective sampling methodology called Distilled Sensing (DS) is proposed for recovering sparse signals in noise.  DS exploits the fact that it is often easier to rule out locations that do not contain signal than it is to detect the locations of non-zero signal components.  We formalize this observation and use it to devise a sequential selective sensing strategy that focuses sensing/measurement resources towards the signal subspace.  This adaptivity in sensing results in rather surprising gains in sparse signal recovery compared to non-adaptive sensing.  We show that exponentially weaker sparse signals can be recovered via DS compared with conventional non-adaptive sensing.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/haupt09a/haupt09a.pdf",
        "supp": "",
        "pdf_size": 920870,
        "gs_citation": 58,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7535930177645003799&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "966e226b80",
        "title": "Dual Temporal Difference Learning",
        "site": "https://proceedings.mlr.press/v5/yang09a.html",
        "author": "Min Yang; Yuxi Li; Dale Schuurmans",
        "abstract": "Recently, researchers have investigated novel dual representations  as a basis for dynamic programming and reinforcement learning algorithms.  Although the convergence properties of classical dynamic programming  algorithms have been established for dual representations, temporal  difference learning algorithms have not yet been analyzed.  In this paper,  we study the convergence properties of temporal difference learning using  dual representations.  We contribute significant progress by proving the  convergence of dual temporal difference learning with eligibility traces.  Experimental results suggest that the dual algorithms seem to demonstrate  empirical benefits over standard primal algorithms.",
        "bibtex": "@InProceedings{pmlr-v5-yang09a,\n  title = \t {Dual Temporal Difference Learning},\n  author = \t {Yang, Min and Li, Yuxi and Schuurmans, Dale},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {631--638},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/yang09a/yang09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/yang09a.html},\n  abstract = \t {Recently, researchers have investigated novel dual representations  as a basis for dynamic programming and reinforcement learning algorithms.  Although the convergence properties of classical dynamic programming  algorithms have been established for dual representations, temporal  difference learning algorithms have not yet been analyzed.  In this paper,  we study the convergence properties of temporal difference learning using  dual representations.  We contribute significant progress by proving the  convergence of dual temporal difference learning with eligibility traces.  Experimental results suggest that the dual algorithms seem to demonstrate  empirical benefits over standard primal algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/yang09a/yang09a.pdf",
        "supp": "",
        "pdf_size": 511041,
        "gs_citation": 5,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10571831378701568792&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "08c5b456fe",
        "title": "Efficient graphlet kernels for large graph comparison",
        "site": "https://proceedings.mlr.press/v5/shervashidze09a.html",
        "author": "Nino Shervashidze; SVN Vishwanathan; Tobias Petri; Kurt Mehlhorn; Karsten Borgwardt",
        "abstract": "State-of-the-art  graph kernels do not scale to large graphs with hundreds of nodes and thousands of edges. In this article we propose to compare graphs by counting graphlets, i.e., subgraphs with $k$ nodes where $k \\in \\{ 3, 4, 5 \\}$. Exhaustive enumeration of all graphlets being prohibitively expensive, we introduce two theoretically grounded speedup schemes, one based on sampling and the second one specifically designed for bounded degree graphs. In our experimental evaluation, our novel kernels allow us to efficiently compare large graphs that cannot be tackled by existing graph kernels.",
        "bibtex": "@InProceedings{pmlr-v5-shervashidze09a,\n  title = \t {Efficient graphlet kernels for large graph comparison},\n  author = \t {Shervashidze, Nino and Vishwanathan, SVN and Petri, Tobias and Mehlhorn, Kurt and Borgwardt, Karsten},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {488--495},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/shervashidze09a/shervashidze09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/shervashidze09a.html},\n  abstract = \t {State-of-the-art  graph kernels do not scale to large graphs with hundreds of nodes and thousands of edges. In this article we propose to compare graphs by counting graphlets, i.e., subgraphs with $k$ nodes where $k \\in \\{ 3, 4, 5 \\}$. Exhaustive enumeration of all graphlets being prohibitively expensive, we introduce two theoretically grounded speedup schemes, one based on sampling and the second one specifically designed for bounded degree graphs. In our experimental evaluation, our novel kernels allow us to efficiently compare large graphs that cannot be tackled by existing graph kernels.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/shervashidze09a/shervashidze09a.pdf",
        "supp": "",
        "pdf_size": 783218,
        "gs_citation": 1386,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14274177394684905099&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": "MPI for Biological Cybernetics + MPI for Developmental Biology, T\u00fcbingen, Germany; Department of Statistics, Purdue University, West Lafayette, IN, USA; Institute for Computer Science, LMU M\u00fcnchen, M\u00fcnchen, Germany; MPI for Informatics, Saarbr\u00fccken, Germany; MPI for Biological Cybernetics + MPI for Developmental Biology, T\u00fcbingen, Germany",
        "aff_domain": "; ; ; ; ",
        "email": "; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2;3;4;0+1",
        "aff_unique_norm": "Max Planck Institute for Biological Cybernetics;Max Planck Institute for Developmental Biology;Purdue University;LMU M\u00fcnchen;Max Planck Institute for Informatics",
        "aff_unique_dep": "Biological Cybernetics;Developmental Biology;Department of Statistics;Institute for Computer Science;Informatics",
        "aff_unique_url": "https://www.biological-cybernetics.de;https://www.mpi-biochem.mpg.de;https://www.purdue.edu;https://www.lmu.de;https://mpi-inf.mpg.de",
        "aff_unique_abbr": "MPIBC;MPI for Dev Bio;Purdue;LMU;MPII",
        "aff_campus_unique_index": "1;2;3;4;1",
        "aff_campus_unique": ";T\u00fcbingen;West Lafayette;M\u00fcnchen;Saarbr\u00fccken",
        "aff_country_unique_index": "0+0;1;0;0;0+0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "a22ff33edd",
        "title": "Estimating Tree-Structured Covariance Matrices via Mixed-Integer Programming",
        "site": "https://proceedings.mlr.press/v5/bravo09a.html",
        "author": "Hector Corrada Bravo; Stephen Wright; Kevin Eng; Sunduz Keles; Grace Wahba",
        "abstract": "We present a novel method for estimating tree-structured covariance matrices directly from observed  continuous data. A representation of these classes of matrices as linear combinations  of rank-one matrices indicating object partitions is used to formulate estimation as  instances of well-studied numerical optimization problems.    In particular, our estimates are based on projection, where the covariance estimate  is the nearest tree-structured covariance matrix to an observed sample covariance matrix.  The problem is posed as a linear or quadratic mixed-integer program (MIP) where  a setting of the integer variables in the MIP specifies a set of tree topologies of the structured  covariance matrix. We solve these problems to optimality using efficient and robust existing MIP solvers.    We present a case study in phylogenetic analysis of expression in yeast gene families and a comparison using simulated data to distance-based tree estimating procedures.",
        "bibtex": "@InProceedings{pmlr-v5-bravo09a,\n  title = \t {Estimating Tree-Structured Covariance Matrices via Mixed-Integer Programming},\n  author = \t {Bravo, Hector Corrada and Wright, Stephen and Eng, Kevin and Keles, Sunduz and Wahba, Grace},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {41--48},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/bravo09a/bravo09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/bravo09a.html},\n  abstract = \t {We present a novel method for estimating tree-structured covariance matrices directly from observed  continuous data. A representation of these classes of matrices as linear combinations  of rank-one matrices indicating object partitions is used to formulate estimation as  instances of well-studied numerical optimization problems.    In particular, our estimates are based on projection, where the covariance estimate  is the nearest tree-structured covariance matrix to an observed sample covariance matrix.  The problem is posed as a linear or quadratic mixed-integer program (MIP) where  a setting of the integer variables in the MIP specifies a set of tree topologies of the structured  covariance matrix. We solve these problems to optimality using efficient and robust existing MIP solvers.    We present a case study in phylogenetic analysis of expression in yeast gene families and a comparison using simulated data to distance-based tree estimating procedures.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/bravo09a/bravo09a.pdf",
        "supp": "",
        "pdf_size": 712455,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11027434596133783445&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": ";;;;",
        "aff_domain": ";;;;",
        "email": ";;;;",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "da5f6f110f",
        "title": "Estimation Consistency of the Group Lasso and its Applications",
        "site": "https://proceedings.mlr.press/v5/liu09a.html",
        "author": "Han Liu; Jian Zhang",
        "abstract": "We extend the $\\ell_2$-consistency result of (Meinshausen and Yu 2008) from the Lasso to  the group Lasso. Our main theorem shows that the group Lasso  achieves estimation consistency under a mild condition and an asymptotic upper bound on the number of selected variables can be obtained.  As a result, we can apply the nonnegative garrote procedure to the group Lasso result to obtain an estimator which is simultaneously  estimation  and variable selection consistent.  In particular,  our setting allows both the number of groups and the number of variables per  group increase and thus is applicable to high-dimensional problems.  We also provide   estimation consistency analysis for a version of the sparse additive models with increasing dimensions. Some finite-sample results are also reported.",
        "bibtex": "@InProceedings{pmlr-v5-liu09a,\n  title = \t {Estimation Consistency of the Group Lasso and its Applications},\n  author = \t {Liu, Han and Zhang, Jian},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {376--383},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/liu09a/liu09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/liu09a.html},\n  abstract = \t {We extend the $\\ell_2$-consistency result of (Meinshausen and Yu 2008) from the Lasso to  the group Lasso. Our main theorem shows that the group Lasso  achieves estimation consistency under a mild condition and an asymptotic upper bound on the number of selected variables can be obtained.  As a result, we can apply the nonnegative garrote procedure to the group Lasso result to obtain an estimator which is simultaneously  estimation  and variable selection consistent.  In particular,  our setting allows both the number of groups and the number of variables per  group increase and thus is applicable to high-dimensional problems.  We also provide   estimation consistency analysis for a version of the sparse additive models with increasing dimensions. Some finite-sample results are also reported.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/liu09a/liu09a.pdf",
        "supp": "",
        "pdf_size": 1327724,
        "gs_citation": 85,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4518578371153699904&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA 15213; Department of Statistics, Purdue University, West Lafayette, IN, 47907-2066",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Carnegie Mellon University;Purdue University",
        "aff_unique_dep": "Machine Learning Department;Department of Statistics",
        "aff_unique_url": "https://www.cmu.edu;https://www.purdue.edu",
        "aff_unique_abbr": "CMU;Purdue",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Pittsburgh;West Lafayette",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ec8c3ab256",
        "title": "Exact and Approximate Sampling by Systematic Stochastic Search",
        "site": "https://proceedings.mlr.press/v5/mansinghka09a.html",
        "author": "Vikash Mansinghka; Daniel Roy; Eric Jonas; Joshua Tenenbaum",
        "abstract": "We introduce _adaptive sequential rejection sampling_, an  algorithm for generating exact samples from high-dimensional, discrete  distributions, building on ideas from classical AI search. Just as  systematic search algorithms like A* recursively build complete  solutions from partial solutions, sequential rejection sampling  recursively builds exact samples over high-dimensional spaces from  exact samples over lower-dimensional subspaces. Our algorithm recovers  widely-used particle filters as an approximate variant without  adaptation, and a randomized version of the directed arc consistency  algorithm with backtracking when applied to deterministic problems. In  this paper, we present the mathematical and algorithmic underpinnings  of our approach and measure its behavior on ferromagnetic Isings and  other probabilistic graphical models, obtaining exact and approximate  samples in a range of situations.",
        "bibtex": "@InProceedings{pmlr-v5-mansinghka09a,\n  title = \t {Exact and Approximate Sampling by Systematic Stochastic Search},\n  author = \t {Mansinghka, Vikash and Roy, Daniel and Jonas, Eric and Tenenbaum, Joshua},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {400--407},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/mansinghka09a/mansinghka09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/mansinghka09a.html},\n  abstract = \t {We introduce _adaptive sequential rejection sampling_, an  algorithm for generating exact samples from high-dimensional, discrete  distributions, building on ideas from classical AI search. Just as  systematic search algorithms like A* recursively build complete  solutions from partial solutions, sequential rejection sampling  recursively builds exact samples over high-dimensional spaces from  exact samples over lower-dimensional subspaces. Our algorithm recovers  widely-used particle filters as an approximate variant without  adaptation, and a randomized version of the directed arc consistency  algorithm with backtracking when applied to deterministic problems. In  this paper, we present the mathematical and algorithmic underpinnings  of our approach and measure its behavior on ferromagnetic Isings and  other probabilistic graphical models, obtaining exact and approximate  samples in a range of situations.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/mansinghka09a/mansinghka09a.pdf",
        "supp": "",
        "pdf_size": 1929033,
        "gs_citation": 16,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9330696502788926612&as_sdt=400005&sciodt=0,14&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "0d4c28be27",
        "title": "Exploiting Probabilistic Independence for Permutations",
        "site": "https://proceedings.mlr.press/v5/huang09b.html",
        "author": "Jonathan Huang; Carlos Guestrin; Xiaoye Jiang; Leonidas Guibas",
        "abstract": "Permutations are ubiquitous in many real  world problems, such as voting, rankings  and data association.   Representing  uncertainty over permutations is  challenging, since there are n!  possibilities. Recent Fourier-based  approaches can be used to provide a compact representation  over low-frequency components of the distribution. Though polynomial,  the complexity of these representations grows very rapidly, especially  if we want to maintain reasonable estimates for peaked distributions.   In this paper, we first characterize the notion of probabilistic independence   for  distribution over permutations.   We then present a method for factoring distributions into  independent components in the Fourier domain,  and use our algorithms to decompose large  problems into much smaller ones.  We demonstrate that our method provides very significant improvements  in terms of running time, on real tracking data.",
        "bibtex": "@InProceedings{pmlr-v5-huang09b,\n  title = \t {Exploiting Probabilistic Independence for Permutations},\n  author = \t {Huang, Jonathan and Guestrin, Carlos and Jiang, Xiaoye and Guibas, Leonidas},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {248--255},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/huang09b/huang09b.pdf},\n  url = \t {https://proceedings.mlr.press/v5/huang09b.html},\n  abstract = \t {Permutations are ubiquitous in many real  world problems, such as voting, rankings  and data association.   Representing  uncertainty over permutations is  challenging, since there are n!  possibilities. Recent Fourier-based  approaches can be used to provide a compact representation  over low-frequency components of the distribution. Though polynomial,  the complexity of these representations grows very rapidly, especially  if we want to maintain reasonable estimates for peaked distributions.   In this paper, we first characterize the notion of probabilistic independence   for  distribution over permutations.   We then present a method for factoring distributions into  independent components in the Fourier domain,  and use our algorithms to decompose large  problems into much smaller ones.  We demonstrate that our method provides very significant improvements  in terms of running time, on real tracking data.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/huang09b/huang09b.pdf",
        "supp": "",
        "pdf_size": 1245144,
        "gs_citation": 23,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17363149974852531475&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "ef9533e7e2",
        "title": "Factorial Mixture of Gaussians and the Marginal Independence Model",
        "site": "https://proceedings.mlr.press/v5/silva09b.html",
        "author": "Ricardo Silva; Zoubin Ghahramani",
        "abstract": "Marginal independence constraints play an important role in learning  with graphical models. One way of parameterizing a model of marginal  independencies is by building a latent variable model where two  independent observed variables have no common latent source. In sparse  domains, however, it might be advantageous to model the marginal  observed distribution directly, without explicitly including latent  variables in the model. There have been recent advances in Gaussian  and binary models of marginal independence, but no models with  non-linear dependencies between continuous variables has been proposed  so far. In this paper, we describe how to generalize the Gaussian  model of marginal independencies based on mixtures, and how to learn  parameters. This requires a non-standard parameterization and raises  difficult non-linear optimization issues.",
        "bibtex": "@InProceedings{pmlr-v5-silva09b,\n  title = \t {Factorial Mixture of Gaussians and the Marginal Independence Model},\n  author = \t {Silva, Ricardo and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {520--527},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/silva09b/silva09b.pdf},\n  url = \t {https://proceedings.mlr.press/v5/silva09b.html},\n  abstract = \t {Marginal independence constraints play an important role in learning  with graphical models. One way of parameterizing a model of marginal  independencies is by building a latent variable model where two  independent observed variables have no common latent source. In sparse  domains, however, it might be advantageous to model the marginal  observed distribution directly, without explicitly including latent  variables in the model. There have been recent advances in Gaussian  and binary models of marginal independence, but no models with  non-linear dependencies between continuous variables has been proposed  so far. In this paper, we describe how to generalize the Gaussian  model of marginal independencies based on mixtures, and how to learn  parameters. This requires a non-standard parameterization and raises  difficult non-linear optimization issues.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/silva09b/silva09b.pdf",
        "supp": "",
        "pdf_size": 558014,
        "gs_citation": 7,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1863474130116331834&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Department of Statistical Science, University College London; Department of Engineering, University of Cambridge",
        "aff_domain": "stats.ucl.ac.uk;eng.cam.ac.uk",
        "email": "stats.ucl.ac.uk;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;University of Cambridge",
        "aff_unique_dep": "Department of Statistical Science;Department of Engineering",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.cam.ac.uk",
        "aff_unique_abbr": "UCL;Cambridge",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "London;Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "6d6dd3e1b2",
        "title": "Gaussian Margin Machines",
        "site": "https://proceedings.mlr.press/v5/crammer09a.html",
        "author": "Koby Crammer; Mehryar Mohri; Fernando Pereira",
        "abstract": "We introduce Gaussian Margin Machines   (GMMs), which maintain a Gaussian distribution over weight vectors for binary classification. The learning algorithm for these machines   seeks the least informative distribution that will   classify the training data correctly with high   probability. One formulation can be expressed   as a convex constrained optimization problem   whose solution can be represented linearly   in terms of training instances and their inner   and outer products, supporting kernelization.   The algorithm has a natural PAC-Bayesian   generalization bound. A preliminary evaluation   on handwriting recognition data shows that our   algorithm improves over SVMs for the same   task.   methods, we maintain a distribution over alternative weight   vectors, rather than committing to a single specific one. However, these distributions are not derived by Bayes? rule.   Instead, they represent our knowledge of the weights given constraints imposed by the training examples. Specifically, we use a Gaussian distribution over weight vectors with   mean and covariance parameters that are learned from the training data. The learning algorithm seeks for a distribution with a small Kullback-Leibler (KL) divergence from a   \ufb01xed isotropic distribution, such that each training example is correctly classified by a strict majority of the weight   vectors. Conceptually, this is a large-margin probabilistic   principle, instead of the geometric large margin principle   in SVMs.   The learning problem for GMMs can be expressed as a   convex constrained optimization, and its optimal solution",
        "bibtex": "@InProceedings{pmlr-v5-crammer09a,\n  title = \t {Gaussian Margin Machines},\n  author = \t {Crammer, Koby and Mohri, Mehryar and Pereira, Fernando},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {105--112},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/crammer09a/crammer09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/crammer09a.html},\n  abstract = \t {We introduce Gaussian Margin Machines   (GMMs), which maintain a Gaussian distribution over weight vectors for binary classification. The learning algorithm for these machines   seeks the least informative distribution that will   classify the training data correctly with high   probability. One formulation can be expressed   as a convex constrained optimization problem   whose solution can be represented linearly   in terms of training instances and their inner   and outer products, supporting kernelization.   The algorithm has a natural PAC-Bayesian   generalization bound. A preliminary evaluation   on handwriting recognition data shows that our   algorithm improves over SVMs for the same   task.   methods, we maintain a distribution over alternative weight   vectors, rather than committing to a single specific one. However, these distributions are not derived by Bayes? rule.   Instead, they represent our knowledge of the weights given constraints imposed by the training examples. Specifically, we use a Gaussian distribution over weight vectors with   mean and covariance parameters that are learned from the training data. The learning algorithm seeks for a distribution with a small Kullback-Leibler (KL) divergence from a   \ufb01xed isotropic distribution, such that each training example is correctly classified by a strict majority of the weight   vectors. Conceptually, this is a large-margin probabilistic   principle, instead of the geometric large margin principle   in SVMs.   The learning problem for GMMs can be expressed as a   convex constrained optimization, and its optimal solution}\n}",
        "pdf": "http://proceedings.mlr.press/v5/crammer09a/crammer09a.pdf",
        "supp": "",
        "pdf_size": 1095590,
        "gs_citation": 30,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7680668396393713264&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "f4d29d024a",
        "title": "Group Nonnegative Matrix Factorization for EEG Classification",
        "site": "https://proceedings.mlr.press/v5/lee09a.html",
        "author": "Hyekyoung Lee; Seungjin Choi",
        "abstract": "Given EEG data measured from several subjects under the same condition,  our goal is to estimate common task-related bases in a linear model  that capture intra-subject variations as well as inter-subject variations.  Such bases capture the common phenomenon in a group data, which is known as group analysis.  In this paper we present a method of nonnegative matrix factorization (NMF) that  is well suited to analyze EEG data of multiple subjects.  The method is referred to as group nonnegative matrix factorization (GNMF)  where we seek task-related common bases reflecting both intra-subject and  inter-subject variations, as well as bases involving individual characteristics.  We compare GNMF with NMF and some modified NMFs, in a task of learning spectral features  from EEG data. Experiments on BCI competition data indicate that GNMF improves  the EEG classification performance. In addition, we also show that GNMF is useful  in a task of subject-to-subject transfer where the prediction for an unseen subject  is performed based on a linear model learned from different subjects in the same group.",
        "bibtex": "@InProceedings{pmlr-v5-lee09a,\n  title = \t {Group Nonnegative Matrix Factorization for EEG Classification},\n  author = \t {Lee, Hyekyoung and Choi, Seungjin},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {320--327},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/lee09a/lee09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/lee09a.html},\n  abstract = \t {Given EEG data measured from several subjects under the same condition,  our goal is to estimate common task-related bases in a linear model  that capture intra-subject variations as well as inter-subject variations.  Such bases capture the common phenomenon in a group data, which is known as group analysis.  In this paper we present a method of nonnegative matrix factorization (NMF) that  is well suited to analyze EEG data of multiple subjects.  The method is referred to as group nonnegative matrix factorization (GNMF)  where we seek task-related common bases reflecting both intra-subject and  inter-subject variations, as well as bases involving individual characteristics.  We compare GNMF with NMF and some modified NMFs, in a task of learning spectral features  from EEG data. Experiments on BCI competition data indicate that GNMF improves  the EEG classification performance. In addition, we also show that GNMF is useful  in a task of subject-to-subject transfer where the prediction for an unseen subject  is performed based on a linear model learned from different subjects in the same group.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/lee09a/lee09a.pdf",
        "supp": "",
        "pdf_size": 704582,
        "gs_citation": 109,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5018644694043778954&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "2f8d3b915e",
        "title": "Handling Sparsity via the Horseshoe",
        "site": "https://proceedings.mlr.press/v5/carvalho09a.html",
        "author": "Carlos M. Carvalho; Nicholas G. Polson; James G. Scott",
        "abstract": "This paper presents a general, fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals, and is therefore closely related to widely used approaches for sparse Bayesian learning, including, among others, Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justifed theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives.",
        "bibtex": "@InProceedings{pmlr-v5-carvalho09a,\n  title = \t {Handling Sparsity via the Horseshoe},\n  author = \t {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {73--80},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/carvalho09a.html},\n  abstract = \t {This paper presents a general, fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals, and is therefore closely related to widely used approaches for sparse Bayesian learning, including, among others, Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justifed theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf",
        "supp": "",
        "pdf_size": 872434,
        "gs_citation": 805,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13257095951240867393&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "9bedff932f",
        "title": "Hash Kernels",
        "site": "https://proceedings.mlr.press/v5/shi09a.html",
        "author": "Qinfeng Shi; James Petterson; Gideon Dror; John Langford; Alex Smola; Alex Strehl; S. V. N. Vishwanathan",
        "abstract": "We propose hashing to facilitate efficient kernels. This generalizes    previous work using sampling and we show a principled way to compute    the kernel matrix for data streams and sparse feature    spaces. Moreover, we give deviation bounds from the exact kernel    matrix. This has applications to estimation on strings and graphs.",
        "bibtex": "@InProceedings{pmlr-v5-shi09a,\n  title = \t {Hash Kernels},\n  author = \t {Shi, Qinfeng and Petterson, James and Dror, Gideon and Langford, John and Smola, Alex and Strehl, Alex and Vishwanathan, S. V. N.},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {496--503},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/shi09a/shi09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/shi09a.html},\n  abstract = \t {We propose hashing to facilitate efficient kernels. This generalizes    previous work using sampling and we show a principled way to compute    the kernel matrix for data streams and sparse feature    spaces. Moreover, we give deviation bounds from the exact kernel    matrix. This has applications to estimation on strings and graphs.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/shi09a/shi09a.pdf",
        "supp": "",
        "pdf_size": 828544,
        "gs_citation": 138,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8072756426463748536&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 5,
        "aff": "Australian National University and NICTA, Canberra, Australia; Australian National University and NICTA, Canberra, Australia; Department of Computer Science, Academic College of Tel-Aviv-Ya\ufb00o, Israel; Yahoo! Research, New York, NY and Santa Clara, CA, USA; Yahoo! Research, New York, NY and Santa Clara, CA, USA; Yahoo! Research, New York, NY and Santa Clara, CA, USA; Department of Statistics, Purdue University, IN, USA",
        "aff_domain": "; ; ; ; ; ; ",
        "email": "; ; ; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;2;2;2;3",
        "aff_unique_norm": "Australian National University;Academic College of Tel-Aviv-Ya\ufb00o;Yahoo! Research;Purdue University",
        "aff_unique_dep": ";Department of Computer Science;;Department of Statistics",
        "aff_unique_url": "https://www.anu.edu.au;https://www.acultyaffo.ac.il;https://research.yahoo.com;https://www.purdue.edu",
        "aff_unique_abbr": "ANU;;Yahoo! Res;Purdue",
        "aff_campus_unique_index": "0;0;2;2;2;3",
        "aff_campus_unique": "Canberra;;New York;Indiana",
        "aff_country_unique_index": "0;0;1;2;2;2;2",
        "aff_country_unique": "Australia;Israel;United States"
    },
    {
        "id": "6421390c29",
        "title": "Infinite Hierarchical Hidden Markov Models",
        "site": "https://proceedings.mlr.press/v5/heller09a.html",
        "author": "Katherine Heller; Yee Whye Teh; Dilan Gorur",
        "abstract": "In this paper we present the Infinite Hierarchical Hidden Markov Model (IHHMM), a nonparametric generalization of Hierarchical Hidden Markov Models (HHMMs). HHMMs have been used for modeling sequential data in applications such as speech recognition, detecting topic transitions in video and extracting information from text. The IHHMM provides more flexible modeling of sequential data by allowing a potentially unbounded number of levels in the hierarchy, instead of requiring the specification of a fixed hierarchy depth.  Inference and learning are performed efficiently using Gibbs sampling and a modified forward-backtrack algorithm. We show encouraging demonstrations of the workings of the IHHMM.",
        "bibtex": "@InProceedings{pmlr-v5-heller09a,\n  title = \t {Infinite Hierarchical Hidden Markov Models},\n  author = \t {Heller, Katherine and Teh, Yee Whye and Gorur, Dilan},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {224--231},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/heller09a/heller09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/heller09a.html},\n  abstract = \t {In this paper we present the Infinite Hierarchical Hidden Markov Model (IHHMM), a nonparametric generalization of Hierarchical Hidden Markov Models (HHMMs). HHMMs have been used for modeling sequential data in applications such as speech recognition, detecting topic transitions in video and extracting information from text. The IHHMM provides more flexible modeling of sequential data by allowing a potentially unbounded number of levels in the hierarchy, instead of requiring the specification of a fixed hierarchy depth.  Inference and learning are performed efficiently using Gibbs sampling and a modified forward-backtrack algorithm. We show encouraging demonstrations of the workings of the IHHMM.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/heller09a/heller09a.pdf",
        "supp": "",
        "pdf_size": 956456,
        "gs_citation": 62,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16666781371348099571&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Engineering Department, University of Cambridge, Cambridge, UK; Gatsby Unit, University College London, London, UK; Gatsby Unit, University College London, London, UK",
        "aff_domain": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "email": "gatsby.ucl.ac.uk;gatsby.ucl.ac.uk;gatsby.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "University of Cambridge;University College London",
        "aff_unique_dep": "Engineering Department;Gatsby Unit",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Cambridge;UCL",
        "aff_campus_unique_index": "0;1;1",
        "aff_campus_unique": "Cambridge;London",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "6b328a0a09",
        "title": "Inverse Optimal Heuristic Control for Imitation Learning",
        "site": "https://proceedings.mlr.press/v5/ratliff09a.html",
        "author": "Nathan Ratliff; Brian Ziebart; Kevin Peterson; J. Andrew Bagnell; Martial Hebert; Anind K. Dey; Siddhartha Srinivasa",
        "abstract": "Imitation learning is an increasingly important tool for both developing  automatic decision making systems as well as for learning to predict  decision-making and behavior by observation. Two basic approaches are common:  the first, which we here term behavioral cloning (BC)\\citeBehavioralCloning,ALVINN,DAVE, treats the imitation learning problem  as a straightforward one of supervised learning (e.g. classification) where the  goal is to map observations to controls.  Secondly, the notion of inverse optimal control (IOC) \\citeBoydIOC,ng00irl,Abbeel04c,mmp06 for  modeling   such decision making behavior has gained prominence as it allows for learned   decision-making that reasons sequentially and over a long horizon.  Unfortunately, such inverse optimal control methods rely upon the ability to   efficiently solve a planning problem and suffer from the usual \u201ccurse of  dimensionality\u201d when the state space gets large. This paper presents a novel  approach to imitation learning that we call Inverse Optimal Heuristic Control (IOHC) which capitalizes on the strengths of both paradigms by  allowing long-horizon, planning style reasoning in a low dimensional space,  while enabling a high dimensional additional set of features to guide overall  action selection.  We frame this combined problem as one of optimization, and  although the resulting objective function is actually non-convex, we are able  to provide convex upper and lower bounds to optimize as surrogates. Further,  these bounds, as well as our empirical results, show that the objective   function is nearly convex and leads to improved performance on a set of  imitation learning problems including turn prediction of drivers as well as  predicting the likely paths taken by pedestrians in an office environment.",
        "bibtex": "@InProceedings{pmlr-v5-ratliff09a,\n  title = \t {Inverse Optimal Heuristic Control for Imitation Learning},\n  author = \t {Ratliff, Nathan and Ziebart, Brian and Peterson, Kevin and Bagnell, J. Andrew and Hebert, Martial and Dey, Anind K. and Srinivasa, Siddhartha},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {424--431},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/ratliff09a/ratliff09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/ratliff09a.html},\n  abstract = \t {Imitation learning is an increasingly important tool for both developing  automatic decision making systems as well as for learning to predict  decision-making and behavior by observation. Two basic approaches are common:  the first, which we here term behavioral cloning (BC)\\citeBehavioralCloning,ALVINN,DAVE, treats the imitation learning problem  as a straightforward one of supervised learning (e.g. classification) where the  goal is to map observations to controls.  Secondly, the notion of inverse optimal control (IOC) \\citeBoydIOC,ng00irl,Abbeel04c,mmp06 for  modeling   such decision making behavior has gained prominence as it allows for learned   decision-making that reasons sequentially and over a long horizon.  Unfortunately, such inverse optimal control methods rely upon the ability to   efficiently solve a planning problem and suffer from the usual \u201ccurse of  dimensionality\u201d when the state space gets large. This paper presents a novel  approach to imitation learning that we call Inverse Optimal Heuristic Control (IOHC) which capitalizes on the strengths of both paradigms by  allowing long-horizon, planning style reasoning in a low dimensional space,  while enabling a high dimensional additional set of features to guide overall  action selection.  We frame this combined problem as one of optimization, and  although the resulting objective function is actually non-convex, we are able  to provide convex upper and lower bounds to optimize as surrogates. Further,  these bounds, as well as our empirical results, show that the objective   function is nearly convex and leads to improved performance on a set of  imitation learning problems including turn prediction of drivers as well as  predicting the likely paths taken by pedestrians in an office environment.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/ratliff09a/ratliff09a.pdf",
        "supp": "",
        "pdf_size": 2421975,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15928679318742339592&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 18,
        "aff": "Robotics Institute, MLD, CSD, Carnegie Mellon University, Pittsburgh, PA 15213; Robotics Institute, MLD, CSD, Carnegie Mellon University, Pittsburgh, PA 15213; Robotics Institute, MLD, CSD, Carnegie Mellon University, Pittsburgh, PA 15213; Robotics Institute, MLD, CSD, Carnegie Mellon University, Pittsburgh, PA 15213; Robotics Institute, MLD, CSD, Carnegie Mellon University, Pittsburgh, PA 15213; Robotics Institute, MLD, CSD, Carnegie Mellon University, Pittsburgh, PA 15213; Intel Research, Pittsburgh, PA 15213",
        "aff_domain": ";;;;;;",
        "email": ";;;;;;",
        "github": "",
        "project": "",
        "author_num": 7,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0;0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;Intel",
        "aff_unique_dep": "Robotics Institute;Intel Research",
        "aff_unique_url": "https://www.cmu.edu;https://www.intel.com",
        "aff_unique_abbr": "CMU;Intel",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "Pittsburgh",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2f77b61d1d",
        "title": "Kernel Learning by Unconstrained Optimization",
        "site": "https://proceedings.mlr.press/v5/li09a.html",
        "author": "Fuxin Li; Yunshan Fu; Yu-Hong Dai; Cristian Sminchisescu; Jue Wang",
        "abstract": "We study the problem of learning a kernel matrix from an  apriori kernel and training data. In this context, we propose a new unconstrained convex optimization formulation, with an  arbitrary convex second-order differentiable loss function on kernel  entries and a LogDet divergence term for regularization. Since the  number of variables is of order $O(n^2)$, the computational cost of  standard Newton and quasi-Newton methods for learning a kernel  matrix is prohibitive. Here an operator form Hessian is  used to develop an $O(n^3)$ trust-region inexact Newton method,  where the Newton direction is computed using several conjugate  gradient steps on the Hessian operator equation. On the uspst  dataset, our algorithm can handle 2 million optimization variables  within one hour. Experiments are shown for both linear (Mahalanobis)  metric learning and for kernel learning. The convergence rate, speed  and performance of several loss functions and algorithms are  discussed.",
        "bibtex": "@InProceedings{pmlr-v5-li09a,\n  title = \t {Kernel Learning by Unconstrained Optimization},\n  author = \t {Li, Fuxin and Fu, Yunshan and Dai, Yu-Hong and Sminchisescu, Cristian and Wang, Jue},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {328--335},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/li09a/li09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/li09a.html},\n  abstract = \t {We study the problem of learning a kernel matrix from an  apriori kernel and training data. In this context, we propose a new unconstrained convex optimization formulation, with an  arbitrary convex second-order differentiable loss function on kernel  entries and a LogDet divergence term for regularization. Since the  number of variables is of order $O(n^2)$, the computational cost of  standard Newton and quasi-Newton methods for learning a kernel  matrix is prohibitive. Here an operator form Hessian is  used to develop an $O(n^3)$ trust-region inexact Newton method,  where the Newton direction is computed using several conjugate  gradient steps on the Hessian operator equation. On the uspst  dataset, our algorithm can handle 2 million optimization variables  within one hour. Experiments are shown for both linear (Mahalanobis)  metric learning and for kernel learning. The convergence rate, speed  and performance of several loss functions and algorithms are  discussed.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/li09a/li09a.pdf",
        "supp": "",
        "pdf_size": 636978,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8881419840075651729&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Institute of Numerical Simulation, University of Bonn; LSEC, Institute of Computational Mathematics, Chinese Academy of Sciences; LSEC, Institute of Computational Mathematics, Chinese Academy of Sciences; Institute of Numerical Simulation, University of Bonn; CSIS, Institute of Automation, Chinese Academy of Sciences",
        "aff_domain": "ins.uni-bonn.de; ; ; ; ",
        "email": "ins.uni-bonn.de; ; ; ; ",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0;1",
        "aff_unique_norm": "University of Bonn;Chinese Academy of Sciences",
        "aff_unique_dep": "Institute of Numerical Simulation;Institute of Computational Mathematics",
        "aff_unique_url": "https://www.uni-bonn.de;http://www.cas.cn",
        "aff_unique_abbr": ";CAS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0;1",
        "aff_country_unique": "Germany;China"
    },
    {
        "id": "555c10e935",
        "title": "Lanczos Approximations for the Speedup of Kernel Partial Least Squares Regression",
        "site": "https://proceedings.mlr.press/v5/kramer09a.html",
        "author": "Nicole Kramer; Masashi Sugiyama; Mikio Braun",
        "abstract": "The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is quadratic in the number of examples. However, the necessity of obtaining sensitivity measures as degrees of freedom for model selection or confidence intervals for more detailed analysis requires cubic runtime, and thus constitutes  a computational bottleneck in real-world data analysis. We propose a novel algorithm for KPLS which not only computes (a) the fit, but also (b) its approximate degrees of freedom and (c) error bars in quadratic runtime. The algorithm exploits a close connection between Kernel PLS and the Lanczos algorithm for approximating the eigenvalues of symmetric matrices, and uses this approximation to compute the trace of powers of the kernel matrix in quadratic runtime.",
        "bibtex": "@InProceedings{pmlr-v5-kramer09a,\n  title = \t {Lanczos Approximations for the Speedup of Kernel Partial Least Squares Regression},\n  author = \t {Kramer, Nicole and Sugiyama, Masashi and Braun, Mikio},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {288--295},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/kramer09a/kramer09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/kramer09a.html},\n  abstract = \t {The runtime for Kernel Partial Least Squares (KPLS) to compute the fit is quadratic in the number of examples. However, the necessity of obtaining sensitivity measures as degrees of freedom for model selection or confidence intervals for more detailed analysis requires cubic runtime, and thus constitutes  a computational bottleneck in real-world data analysis. We propose a novel algorithm for KPLS which not only computes (a) the fit, but also (b) its approximate degrees of freedom and (c) error bars in quadratic runtime. The algorithm exploits a close connection between Kernel PLS and the Lanczos algorithm for approximating the eigenvalues of symmetric matrices, and uses this approximation to compute the trace of powers of the kernel matrix in quadratic runtime.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/kramer09a/kramer09a.pdf",
        "supp": "",
        "pdf_size": 913291,
        "gs_citation": 9,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11965192961764506189&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Machine Learning Group, Berlin Institute of Technology, Germany; Department of Computer Science, Tokyo Institute of Technology, Japan; Machine Learning Group, Berlin Institute of Technology, Germany",
        "aff_domain": "cs.tu.berlin.de;cs.titech.ac.jp;cs.tu-berlin.de",
        "email": "cs.tu.berlin.de;cs.titech.ac.jp;cs.tu-berlin.de",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Berlin Institute of Technology;Tokyo Institute of Technology",
        "aff_unique_dep": "Machine Learning Group;Department of Computer Science",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.titech.ac.jp",
        "aff_unique_abbr": "TU Berlin;Titech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berlin;",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "Germany;Japan"
    },
    {
        "id": "f09c51c067",
        "title": "Large-Margin Structured Prediction via Linear Programming",
        "site": "https://proceedings.mlr.press/v5/wang09d.html",
        "author": "Zhuoran Wang; John Shawe-Taylor",
        "abstract": "This paper presents a novel learning algorithm for structured classification problems, where the task is to predict multiple and interacting labels (multilabel) for an input object. The maximum margin separation between the correct multilabels and the incorrect ones is formulated as a linear program.  Instead of explicitly writing out the entire problem with an exponentially large  constraint set, the linear program is solved iteratively via column generation. In this case, the process of generating most violated constraints is equivalent to searching for highest-scored misclassified incorrect multilabels, which can be easily achieved by decoding the structure based on current estimations.  In addition, we also explore the integration of the column generation and the extragradient method for linear programming to gain further efficiency. Compared to previous works on large-margin structured prediction, this framework has advantages in handling arbitrary structures and larger-scale  problems. Experimental results on part-of-speech tagging and statistical machine translation tasks are reported, demonstrating the competitiveness of the proposed approach.",
        "bibtex": "@InProceedings{pmlr-v5-wang09d,\n  title = \t {Large-Margin Structured Prediction via Linear Programming},\n  author = \t {Wang, Zhuoran and Shawe-Taylor, John},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {599--606},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/wang09d/wang09d.pdf},\n  url = \t {https://proceedings.mlr.press/v5/wang09d.html},\n  abstract = \t {This paper presents a novel learning algorithm for structured classification problems, where the task is to predict multiple and interacting labels (multilabel) for an input object. The maximum margin separation between the correct multilabels and the incorrect ones is formulated as a linear program.  Instead of explicitly writing out the entire problem with an exponentially large  constraint set, the linear program is solved iteratively via column generation. In this case, the process of generating most violated constraints is equivalent to searching for highest-scored misclassified incorrect multilabels, which can be easily achieved by decoding the structure based on current estimations.  In addition, we also explore the integration of the column generation and the extragradient method for linear programming to gain further efficiency. Compared to previous works on large-margin structured prediction, this framework has advantages in handling arbitrary structures and larger-scale  problems. Experimental results on part-of-speech tagging and statistical machine translation tasks are reported, demonstrating the competitiveness of the proposed approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/wang09d/wang09d.pdf",
        "supp": "",
        "pdf_size": 561243,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6314963107335478159&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Department of Computer Science, University College London, London, WC1E 6BT, United Kingdom; Department of Computer Science, University College London, London, WC1E 6BT, United Kingdom",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "376d333ccd",
        "title": "Latent Force Models",
        "site": "https://proceedings.mlr.press/v5/alvarez09a.html",
        "author": "Mauricio \u00c1lvarez; David Luengo; Neil D. Lawrence",
        "abstract": "Purely data driven approaches for machine learning present difficulties when data is scarce relative to the complexity of the model or when the model is forced to extrapolate. On the other hand, purely mechanistic approaches  need to identify and specify all the interactions in the problem at hand (which  may not be feasible) and still leave the issue of how to parameterize the system. In this paper, we present a hybrid approach using Gaussian processes and differential equations to combine data driven modeling with a physical model of the system. We show how different, physically-inspired, kernel functions  can be developed through sensible, simple, mechanistic assumptions about the underlying system. The versatility of our approach is illustrated with three case studies from computational biology, motion capture and geostatistics.",
        "bibtex": "@InProceedings{pmlr-v5-alvarez09a,\n  title = \t {Latent Force Models},\n  author = \t {\u00c1lvarez, Mauricio and Luengo, David and Lawrence, Neil D.},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {9--16},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/alvarez09a/alvarez09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/alvarez09a.html},\n  abstract = \t {Purely data driven approaches for machine learning present difficulties when data is scarce relative to the complexity of the model or when the model is forced to extrapolate. On the other hand, purely mechanistic approaches  need to identify and specify all the interactions in the problem at hand (which  may not be feasible) and still leave the issue of how to parameterize the system. In this paper, we present a hybrid approach using Gaussian processes and differential equations to combine data driven modeling with a physical model of the system. We show how different, physically-inspired, kernel functions  can be developed through sensible, simple, mechanistic assumptions about the underlying system. The versatility of our approach is illustrated with three case studies from computational biology, motion capture and geostatistics.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/alvarez09a/alvarez09a.pdf",
        "supp": "",
        "pdf_size": 875992,
        "gs_citation": 250,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2251426177898918666&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science, University of Manchester, Manchester, UK, M13 9PL; Dep. Teor\u00b4\u0131a de Se\u02dcnal y Comunicaciones, Universidad Carlos III de Madrid, 28911 Legan\u00b4es, Spain; School of Computer Science, University of Manchester, Manchester, UK, M13 9PL",
        "aff_domain": "cs.man.ac.uk;ieee.org;cs.man.ac.uk",
        "email": "cs.man.ac.uk;ieee.org;cs.man.ac.uk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Manchester;Universidad Carlos III de Madrid",
        "aff_unique_dep": "School of Computer Science;Department of Signal Theory and Communications",
        "aff_unique_url": "https://www.manchester.ac.uk;https://www.uc3m.es",
        "aff_unique_abbr": "UoM;UC3M",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Manchester;Legan\u00e9s",
        "aff_country_unique_index": "0;1;0",
        "aff_country_unique": "United Kingdom;Spain"
    },
    {
        "id": "b6f1800629",
        "title": "Latent Variable Models for Dimensionality Reduction",
        "site": "https://proceedings.mlr.press/v5/zhang09b.html",
        "author": "Zhihua Zhang; Michael I. Jordan",
        "abstract": "Principal coordinate analysis (PCO), as a duality of principal component analysis (PCA), is also a classical method for explanatory data analysis.  In this paper we propose a probabilistic PCO by using a normal latent variable model in which   maximum likelihood estimation and an expectation-maximization algorithm are respectively devised to calculate the configurations of objects in a low-dimensional Euclidean space. We also devise probabilistic formulations for kernel PCA which is a nonlinear extension of PCA.",
        "bibtex": "@InProceedings{pmlr-v5-zhang09b,\n  title = \t {Latent Variable Models for Dimensionality Reduction},\n  author = \t {Zhang, Zhihua and Jordan, Michael I.},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {655--662},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/zhang09b/zhang09b.pdf},\n  url = \t {https://proceedings.mlr.press/v5/zhang09b.html},\n  abstract = \t {Principal coordinate analysis (PCO), as a duality of principal component analysis (PCA), is also a classical method for explanatory data analysis.  In this paper we propose a probabilistic PCO by using a normal latent variable model in which   maximum likelihood estimation and an expectation-maximization algorithm are respectively devised to calculate the configurations of objects in a low-dimensional Euclidean space. We also devise probabilistic formulations for kernel PCA which is a nonlinear extension of PCA.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/zhang09b/zhang09b.pdf",
        "supp": "",
        "pdf_size": 588558,
        "gs_citation": 8,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8244357483196406931&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "College of Comp. Sci. and Tech., Zhejiang University, Hangzhou, Zhejiang 310027, China; Departments of EECS and Statistics, University of California, Berkeley, Berkeley, CA 94720, USA",
        "aff_domain": "cs.zju.edu.cn;cs.berkeley.edu",
        "email": "cs.zju.edu.cn;cs.berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Zhejiang University;University of California, Berkeley",
        "aff_unique_dep": "College of Computer Science and Technology;Departments of EECS and Statistics",
        "aff_unique_url": "http://www.zju.edu.cn;https://www.berkeley.edu",
        "aff_unique_abbr": "ZJU;UC Berkeley",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Hangzhou;Berkeley",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "12b6e3d88a",
        "title": "Latent Wishart Processes for Relational Kernel Learning",
        "site": "https://proceedings.mlr.press/v5/li09b.html",
        "author": "Wu-Jun Li; Zhihua Zhang; Dit-Yan Yeung",
        "abstract": "In this paper, we propose a novel relational kernel learning model based on  latent Wishart processes (LWP) to learn the kernel function for relational data.  This is done by seamlessly integrating the relational information and the input attributes into the kernel learning process.  Through extensive experiments on diverse real-world applications, we demonstrate that our LWP model can give very promising performance in practice.",
        "bibtex": "@InProceedings{pmlr-v5-li09b,\n  title = \t {Latent Wishart Processes for Relational Kernel Learning},\n  author = \t {Li, Wu-Jun and Zhang, Zhihua and Yeung, Dit-Yan},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {336--343},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/li09b/li09b.pdf},\n  url = \t {https://proceedings.mlr.press/v5/li09b.html},\n  abstract = \t {In this paper, we propose a novel relational kernel learning model based on  latent Wishart processes (LWP) to learn the kernel function for relational data.  This is done by seamlessly integrating the relational information and the input attributes into the kernel learning process.  Through extensive experiments on diverse real-world applications, we demonstrate that our LWP model can give very promising performance in practice.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/li09b/li09b.pdf",
        "supp": "",
        "pdf_size": 809558,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1444086768816001924&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 14,
        "aff": "Dept. of Comp. Sci. and Eng., Hong Kong Univ. of Sci. and Tech., Hong Kong, China; College of Comp. Sci. and Tech., Zhejiang University, Zhejiang 310027, China; Dept. of Comp. Sci. and Eng., Hong Kong Univ. of Sci. and Tech., Hong Kong, China",
        "aff_domain": "cse.ust.hk;cs.zju.edu.cn;cse.ust.hk",
        "email": "cse.ust.hk;cs.zju.edu.cn;cse.ust.hk",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Zhejiang University",
        "aff_unique_dep": "Department of Computer Science and Engineering;College of Computer Science and Technology",
        "aff_unique_url": "https://www.ust.hk;http://www.zju.edu.cn",
        "aff_unique_abbr": "HKUST;ZJU",
        "aff_campus_unique_index": "0;1;0",
        "aff_campus_unique": "Hong Kong;Zhejiang",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "8307df44a4",
        "title": "Learning Exercise Policies for American Options",
        "site": "https://proceedings.mlr.press/v5/li09d.html",
        "author": "Yuxi Li; Csaba Szepesvari; Dale Schuurmans",
        "abstract": "Options are important instruments in modern finance. In this paper, we investigate reinforcement learning (RL) methods\u2014in particular, least-squares policy iteration (LSPI)\u2014for the problem of learning exercise policies for American options. We develop finite-time bounds on the performance of the policy obtained with LSPI and compare LSPI and the fitted Q-iteration algorithm (FQI) with the Longstaff-Schwartz method (LSM), the standard least-squares Monte Carlo algorithm from the finance community. Our empirical results show that the exercise policies discovered by LSPI and FQI gain larger payoffs than those discovered by LSM, on both real and synthetic data. Furthermore, we find that for all methods the policies learned from real data generally gain similar payoffs to the policies learned from simulated data. Our work shows that solution methods developed in machine learning can advance the state-of-the-art in an important and challenging application area, while demonstrating that computational finance remains a promising area for future applications of machine learning methods.",
        "bibtex": "@InProceedings{pmlr-v5-li09d,\n  title = \t {Learning Exercise Policies for American Options},\n  author = \t {Li, Yuxi and Szepesvari, Csaba and Schuurmans, Dale},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {352--359},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/li09d/li09d.pdf},\n  url = \t {https://proceedings.mlr.press/v5/li09d.html},\n  abstract = \t {Options are important instruments in modern finance. In this paper, we investigate reinforcement learning (RL) methods\u2014in particular, least-squares policy iteration (LSPI)\u2014for the problem of learning exercise policies for American options. We develop finite-time bounds on the performance of the policy obtained with LSPI and compare LSPI and the fitted Q-iteration algorithm (FQI) with the Longstaff-Schwartz method (LSM), the standard least-squares Monte Carlo algorithm from the finance community. Our empirical results show that the exercise policies discovered by LSPI and FQI gain larger payoffs than those discovered by LSM, on both real and synthetic data. Furthermore, we find that for all methods the policies learned from real data generally gain similar payoffs to the policies learned from simulated data. Our work shows that solution methods developed in machine learning can advance the state-of-the-art in an important and challenging application area, while demonstrating that computational finance remains a promising area for future applications of machine learning methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/li09d/li09d.pdf",
        "supp": "",
        "pdf_size": 483924,
        "gs_citation": 67,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11454682938708874205&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "41b02a980d",
        "title": "Learning Low Density Separators",
        "site": "https://proceedings.mlr.press/v5/ben-david09a.html",
        "author": "Shai Ben-David; Tyler Lu; David Pal; Miroslava Sotakova",
        "abstract": "We define a novel, basic, unsupervised learning problem - learning  the lowest density homogeneous hyperplane separator of an  unknown probability distribution.  This task is relevant to several  problems in machine learning, such as semi-supervised learning and  clustering stability.  We investigate the question of existence of a  universally consistent algorithm for this problem. We propose two  natural learning paradigms and prove that, on input unlabeled random  samples generated by any member of a rich family of distributions,  they are guaranteed to converge to the optimal separator for that  distribution. We complement this result by showing that no learning  algorithm for our task can achieve uniform learning rates (that are  independent of the data generating distribution).",
        "bibtex": "@InProceedings{pmlr-v5-ben-david09a,\n  title = \t {Learning Low Density Separators},\n  author = \t {Ben-David, Shai and Lu, Tyler and Pal, David and Sotakova, Miroslava},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {25--32},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/ben-david09a/ben-david09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/ben-david09a.html},\n  abstract = \t {We define a novel, basic, unsupervised learning problem - learning  the lowest density homogeneous hyperplane separator of an  unknown probability distribution.  This task is relevant to several  problems in machine learning, such as semi-supervised learning and  clustering stability.  We investigate the question of existence of a  universally consistent algorithm for this problem. We propose two  natural learning paradigms and prove that, on input unlabeled random  samples generated by any member of a rich family of distributions,  they are guaranteed to converge to the optimal separator for that  distribution. We complement this result by showing that no learning  algorithm for our task can achieve uniform learning rates (that are  independent of the data generating distribution).}\n}",
        "pdf": "http://proceedings.mlr.press/v5/ben-david09a/ben-david09a.pdf",
        "supp": "",
        "pdf_size": 491995,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9769278807118137788&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada; Departement of Computer Science, University of Aarhus, Denmark",
        "aff_domain": "cs.uwaterloo.ca;cs.uwaterloo.ca;cs.uwaterloo.ca;daimi.au.dk",
        "email": "cs.uwaterloo.ca;cs.uwaterloo.ca;cs.uwaterloo.ca;daimi.au.dk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Waterloo;University of Aarhus",
        "aff_unique_dep": "David R. Cheriton School of Computer Science;Departement of Computer Science",
        "aff_unique_url": "https://uwaterloo.ca;https://www.au.dk",
        "aff_unique_abbr": "UWaterloo;AU",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Waterloo;",
        "aff_country_unique_index": "0;0;0;1",
        "aff_country_unique": "Canada;Denmark"
    },
    {
        "id": "8720d361ec",
        "title": "Learning Sparse Markov Network Structure via Ensemble-of-Trees Models",
        "site": "https://proceedings.mlr.press/v5/lin09a.html",
        "author": "Yuanqing Lin; Shenghuo Zhu; Daniel Lee; Ben Taskar",
        "abstract": "Learning the sparse structure of a general  Markov network is a hard problem. One of the  main difficulties is the computation of its generally  intractable partition function. To circumvent  this difficulty, this paper proposes to learn  the network structure using an ensemble-of-trees  (ET) model. The ET model was first introduced  by Meila and Jaakkola [1], and it approximates  a Markov network using a mixture of all possible  (super-exponentially many) spanning trees.  The advantage of the ET model is that, although  it needs to sum over super-exponentially many  trees, its partition function as well as data likelihood  can be computed in a closed form. Furthermore,  since the ET model tends to represent  a Markov network using as small number  of trees as possible, it provides a natural regularization  for finding a sparse network structure.  Our simulation results show that the proposed  ET approach is able to accurately recover  the true Markov network connectivity and significantly  outperform the state-of-art approaches  for both discrete and continuous random variable  networks. Furthermore, we also demonstrate the  usage of the ET model for discovering the network  of words from blog posts.",
        "bibtex": "@InProceedings{pmlr-v5-lin09a,\n  title = \t {Learning Sparse Markov Network Structure via Ensemble-of-Trees Models},\n  author = \t {Lin, Yuanqing and Zhu, Shenghuo and Lee, Daniel and Taskar, Ben},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {360--367},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/lin09a/lin09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/lin09a.html},\n  abstract = \t {Learning the sparse structure of a general  Markov network is a hard problem. One of the  main difficulties is the computation of its generally  intractable partition function. To circumvent  this difficulty, this paper proposes to learn  the network structure using an ensemble-of-trees  (ET) model. The ET model was first introduced  by Meila and Jaakkola [1], and it approximates  a Markov network using a mixture of all possible  (super-exponentially many) spanning trees.  The advantage of the ET model is that, although  it needs to sum over super-exponentially many  trees, its partition function as well as data likelihood  can be computed in a closed form. Furthermore,  since the ET model tends to represent  a Markov network using as small number  of trees as possible, it provides a natural regularization  for finding a sparse network structure.  Our simulation results show that the proposed  ET approach is able to accurately recover  the true Markov network connectivity and significantly  outperform the state-of-art approaches  for both discrete and continuous random variable  networks. Furthermore, we also demonstrate the  usage of the ET model for discovering the network  of words from blog posts.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/lin09a/lin09a.pdf",
        "supp": "",
        "pdf_size": 465636,
        "gs_citation": 18,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6104087372252919605&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4b62223d59",
        "title": "Learning Thin Junction Trees via Graph Cuts",
        "site": "https://proceedings.mlr.press/v5/dafna09a.html",
        "author": "Shahaf Dafna; Carlos Guestrin",
        "abstract": "Structure learning algorithms usually focus on  the compactness of the learned model. However,  compact representation does not imply the existence  of tractable inference algorithms: both exact  and approximate inference may still be NP-hard.  This focus on compactness leads to learning  good models that require approximate inference  techniques, thus reducing their prediction  quality. In this paper, we propose a method for  learning an attractive class of models: bounded treewidth  junction trees. Those models permit  both compact representation of probability distributions  and efficient exact inference.  Our method uses a new global criterion to construct  the tree. Our criterion, based on a Bethe  approximation of the likelihood, transforms the  structure learning problem into an intuitive graph  theoretical task.  We present an efficient randomized algorithm  with theoretical guarantees for finding good separators.  We recursively apply this procedure to  obtain a thin junction tree. Our extensive empirical  evaluation demonstrates the benefit of applying  exact inference using our models to answer  queries. We also extend our technique to learn  low tree-width conditional random fields, and  demonstrate significant improvements over state  of the art block-L1 regularization techniques.",
        "bibtex": "@InProceedings{pmlr-v5-dafna09a,\n  title = \t {Learning Thin Junction Trees via Graph Cuts},\n  author = \t {Dafna, Shahaf and Guestrin, Carlos},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {113--120},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/dafna09a/dafna09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/dafna09a.html},\n  abstract = \t {Structure learning algorithms usually focus on  the compactness of the learned model. However,  compact representation does not imply the existence  of tractable inference algorithms: both exact  and approximate inference may still be NP-hard.  This focus on compactness leads to learning  good models that require approximate inference  techniques, thus reducing their prediction  quality. In this paper, we propose a method for  learning an attractive class of models: bounded treewidth  junction trees. Those models permit  both compact representation of probability distributions  and efficient exact inference.  Our method uses a new global criterion to construct  the tree. Our criterion, based on a Bethe  approximation of the likelihood, transforms the  structure learning problem into an intuitive graph  theoretical task.  We present an efficient randomized algorithm  with theoretical guarantees for finding good separators.  We recursively apply this procedure to  obtain a thin junction tree. Our extensive empirical  evaluation demonstrates the benefit of applying  exact inference using our models to answer  queries. We also extend our technique to learn  low tree-width conditional random fields, and  demonstrate significant improvements over state  of the art block-L1 regularization techniques.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/dafna09a/dafna09a.pdf",
        "supp": "",
        "pdf_size": 2555241,
        "gs_citation": 55,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12172777930054113396&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "195562ad6c",
        "title": "Learning a Parametric Embedding by Preserving Local Structure",
        "site": "https://proceedings.mlr.press/v5/maaten09a.html",
        "author": "Laurens van der Maaten",
        "abstract": "The paper presents a new unsupervised dimensionality reduction technique, called parametric t-SNE, that learns a parametric mapping between the high-dimensional data space and the low-dimensional latent space. Parametric t-SNE learns the parametric mapping in such a way that the local structure of the data is preserved as well as possible in the latent space.  We evaluate the performance of parametric t-SNE in experiments on two datasets, in which we compare it to the performance of two other unsupervised parametric dimensionality reduction techniques. The results of experiments illustrate the strong performance of parametric t-SNE, in particular, in learning settings in which the dimensionality of the latent space is relatively low.",
        "bibtex": "@InProceedings{pmlr-v5-maaten09a,\n  title = \t {Learning a Parametric Embedding by Preserving Local Structure},\n  author = \t {van der Maaten, Laurens},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {384--391},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/maaten09a/maaten09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/maaten09a.html},\n  abstract = \t {The paper presents a new unsupervised dimensionality reduction technique, called parametric t-SNE, that learns a parametric mapping between the high-dimensional data space and the low-dimensional latent space. Parametric t-SNE learns the parametric mapping in such a way that the local structure of the data is preserved as well as possible in the latent space.  We evaluate the performance of parametric t-SNE in experiments on two datasets, in which we compare it to the performance of two other unsupervised parametric dimensionality reduction techniques. The results of experiments illustrate the strong performance of parametric t-SNE, in particular, in learning settings in which the dimensionality of the latent space is relatively low.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/maaten09a/maaten09a.pdf",
        "supp": "",
        "pdf_size": 4737171,
        "gs_citation": 809,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4352665295520260441&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "TiCC, Tilburg University",
        "aff_domain": "gmail.com",
        "email": "gmail.com",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Tilburg University",
        "aff_unique_dep": "TiCC",
        "aff_unique_url": "https://www.tilburguniversity.edu",
        "aff_unique_abbr": "Tilburg U",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Netherlands"
    },
    {
        "id": "fbe19117b9",
        "title": "Learning the Switching Rate by Discretising Bernoulli Sources Online",
        "site": "https://proceedings.mlr.press/v5/rooij09a.html",
        "author": "Steven Rooij; Tim Erven",
        "abstract": "The expert tracking algorithm Fixed-Share  depends on a parameter alpha, called the switching rate. If the final number of outcomes $T$ is known in advance, then the switching rate can be learned with regret $\\frac{1}{2} \\log T + O(1)$ bits. The current fastest method that  achieves this, Learn-alpha, is based on optimal  discretisation of the Bernoulli distributions into $O(\\sqrt{T})$ bins and runs in $(T\\sqrt{T})$ time; however the exact locations of these points  have to be determined algorithmically.    This paper introduces a new discretisation  scheme with the same regret bound for  known $T$, that specifies the number and positions of the discretisation points explicitly. The scheme is especially useful when $T$ is not known in advance: a new fully online algorithm, Refine-Online, is presented, which  runs in $O(T \\sqrt{T} \\log T)$ time and achieves a  regret of $\\frac{1}{2} \\log 3 \\log T + O(\\log \\log T)$ bits.",
        "bibtex": "@InProceedings{pmlr-v5-rooij09a,\n  title = \t {Learning the Switching Rate by Discretising Bernoulli Sources Online},\n  author = \t {Rooij, Steven and Erven, Tim},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {432--439},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/rooij09a/rooij09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/rooij09a.html},\n  abstract = \t {The expert tracking algorithm Fixed-Share  depends on a parameter alpha, called the switching rate. If the final number of outcomes $T$ is known in advance, then the switching rate can be learned with regret $\\frac{1}{2} \\log T + O(1)$ bits. The current fastest method that  achieves this, Learn-alpha, is based on optimal  discretisation of the Bernoulli distributions into $O(\\sqrt{T})$ bins and runs in $(T\\sqrt{T})$ time; however the exact locations of these points  have to be determined algorithmically.    This paper introduces a new discretisation  scheme with the same regret bound for  known $T$, that specifies the number and positions of the discretisation points explicitly. The scheme is especially useful when $T$ is not known in advance: a new fully online algorithm, Refine-Online, is presented, which  runs in $O(T \\sqrt{T} \\log T)$ time and achieves a  regret of $\\frac{1}{2} \\log 3 \\log T + O(\\log \\log T)$ bits.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/rooij09a/rooij09a.pdf",
        "supp": "",
        "pdf_size": 624381,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10329782764961971607&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Centre for Mathematical Sciences, Cambridge CB3 0WB, United Kingdom; Centrum Wiskunde & Informatica (CWI), Science Park 123, P.O. Box 94079, 1090 GB Amsterdam, The Netherlands",
        "aff_domain": "statslab.cam.ac.uk;cwi.nl",
        "email": "statslab.cam.ac.uk;cwi.nl",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Cambridge;Centrum Wiskunde & Informatica",
        "aff_unique_dep": "Centre for Mathematical Sciences;",
        "aff_unique_url": "https://www.maths.cam.ac.uk;https://www.cwi.nl",
        "aff_unique_abbr": "Cambridge;CWI",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Cambridge;Amsterdam",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United Kingdom;Netherlands"
    },
    {
        "id": "cbccfb39e7",
        "title": "Locally Minimax Optimal Predictive Modeling with Bayesian Networks",
        "site": "https://proceedings.mlr.press/v5/silander09a.html",
        "author": "Tomi Silander; Teemu Roos; Petri Myllym\u00e4ki",
        "abstract": "We propose an information-theoretic approach for predictive modeling with Bayesian networks. Our approach is based on the minimax optimal Normalized Maximum Likelihood (NML) distribution, motivated by the MDL principle. In particular, we present a parameter learning method which, together with a previously introduced NML-based model selection criterion, provides a way to construct highly predictive Bayesian network models from data. The method is parameter-free and robust, unlike the currently popular Bayesian marginal likelihood approach which has been shown to be sensitive to the choice of prior hyperparameters. Empirical tests show that the proposed method compares favorably with the Bayesian approach in predictive tasks.",
        "bibtex": "@InProceedings{pmlr-v5-silander09a,\n  title = \t {Locally Minimax Optimal Predictive Modeling with Bayesian Networks},\n  author = \t {Silander, Tomi and Roos, Teemu and Myllym\u00e4ki, Petri},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {504--511},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/silander09a/silander09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/silander09a.html},\n  abstract = \t {We propose an information-theoretic approach for predictive modeling with Bayesian networks. Our approach is based on the minimax optimal Normalized Maximum Likelihood (NML) distribution, motivated by the MDL principle. In particular, we present a parameter learning method which, together with a previously introduced NML-based model selection criterion, provides a way to construct highly predictive Bayesian network models from data. The method is parameter-free and robust, unlike the currently popular Bayesian marginal likelihood approach which has been shown to be sensitive to the choice of prior hyperparameters. Empirical tests show that the proposed method compares favorably with the Bayesian approach in predictive tasks.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/silander09a/silander09a.pdf",
        "supp": "",
        "pdf_size": 520275,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16832054629862412080&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "47483f1787",
        "title": "MCMC Methods for Bayesian Mixtures of Copulas",
        "site": "https://proceedings.mlr.press/v5/silva09a.html",
        "author": "Ricardo Silva; Robert Gramacy",
        "abstract": "Applications of copula models have been increasing in number in recent  years. This class of models provides a modular parameterization of  joint distributions: the specification of the marginal distributions  is parameterized separately from the dependence structure of the  joint, a convenient way of encoding a model for domains such as  finance. Some recent advances on how to specify copulas for arbitrary  dimensions have been proposed, by means of mixtures of decomposable  graphical models. This paper introduces a Bayesian approach for  dealing with mixtures of copulas which, due to the lack of prior  conjugacy, raise computational challenges. We motivate and present  families of Markov chain Monte Carlo (MCMC) proposals that exploit the  particular structure of mixtures of copulas. Different algorithms are  evaluated according to their mixing properties, and an application in  financial forecasting with missing data illustrates the usefulness of  the methodology.",
        "bibtex": "@InProceedings{pmlr-v5-silva09a,\n  title = \t {MCMC Methods for Bayesian Mixtures of Copulas},\n  author = \t {Silva, Ricardo and Gramacy, Robert},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {512--519},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/silva09a/silva09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/silva09a.html},\n  abstract = \t {Applications of copula models have been increasing in number in recent  years. This class of models provides a modular parameterization of  joint distributions: the specification of the marginal distributions  is parameterized separately from the dependence structure of the  joint, a convenient way of encoding a model for domains such as  finance. Some recent advances on how to specify copulas for arbitrary  dimensions have been proposed, by means of mixtures of decomposable  graphical models. This paper introduces a Bayesian approach for  dealing with mixtures of copulas which, due to the lack of prior  conjugacy, raise computational challenges. We motivate and present  families of Markov chain Monte Carlo (MCMC) proposals that exploit the  particular structure of mixtures of copulas. Different algorithms are  evaluated according to their mixing properties, and an application in  financial forecasting with missing data illustrates the usefulness of  the methodology.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/silva09a/silva09a.pdf",
        "supp": "",
        "pdf_size": 505590,
        "gs_citation": 21,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=18088942735723418870&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Statistical Science, University College London; Statistical Laboratory, University of Cambridge",
        "aff_domain": "stats.ucl.ac.uk;statslab.cam.ac.uk",
        "email": "stats.ucl.ac.uk;statslab.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University College London;University of Cambridge",
        "aff_unique_dep": "Department of Statistical Science;Statistical Laboratory",
        "aff_unique_url": "https://www.ucl.ac.uk;https://www.cam.ac.uk",
        "aff_unique_abbr": "UCL;Cambridge",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "London;Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "9d60b84603",
        "title": "Markov Topic Models",
        "site": "https://proceedings.mlr.press/v5/wang09b.html",
        "author": "Chong Wang; Bo Thiesson; Chris Meek; David Blei",
        "abstract": "We develop Markov topic models (MTMs), a novel family of generative graphical models that can learn topics simultaneously from multiple corpora, such as papers from different conferences. We apply Gaussian (Markov) random fields to model the correlations of different corpora.  MTMs capture both the internal topic structure within each corpus and the relationships between topics across the corpora.  We derive an efficient estimation procedure with variational expectation-maximization.  We study the performance of our models on a corpus of abstracts from six different computer science conferences.  Our analysis reveals qualitative discoveries that are not possible with traditional topic models, and improved quantitative performance over the state of the art.",
        "bibtex": "@InProceedings{pmlr-v5-wang09b,\n  title = \t {Markov Topic Models},\n  author = \t {Wang, Chong and Thiesson, Bo and Meek, Chris and Blei, David},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {583--590},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/wang09b/wang09b.pdf},\n  url = \t {https://proceedings.mlr.press/v5/wang09b.html},\n  abstract = \t {We develop Markov topic models (MTMs), a novel family of generative graphical models that can learn topics simultaneously from multiple corpora, such as papers from different conferences. We apply Gaussian (Markov) random fields to model the correlations of different corpora.  MTMs capture both the internal topic structure within each corpus and the relationships between topics across the corpora.  We derive an efficient estimation procedure with variational expectation-maximization.  We study the performance of our models on a corpus of abstracts from six different computer science conferences.  Our analysis reveals qualitative discoveries that are not possible with traditional topic models, and improved quantitative performance over the state of the art.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/wang09b/wang09b.pdf",
        "supp": "",
        "pdf_size": 847735,
        "gs_citation": 71,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9304852074998486683&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 12,
        "aff": "Computer Science Dept., Princeton University; Microsoft Research; Microsoft Research; Computer Science Dept., Princeton University",
        "aff_domain": "princeton.edu;microsoft.com;microsoft.com;princeton.edu",
        "email": "princeton.edu;microsoft.com;microsoft.com;princeton.edu",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Princeton University;Microsoft",
        "aff_unique_dep": "Computer Science Department;Microsoft Research",
        "aff_unique_url": "https://www.princeton.edu;https://www.microsoft.com/en-us/research",
        "aff_unique_abbr": "Princeton;MSR",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Princeton;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e2886e65db",
        "title": "Matching Pursuit Kernel Fisher Discriminant Analysis",
        "site": "https://proceedings.mlr.press/v5/diethe09a.html",
        "author": "Tom Diethe; Zakria Hussain; David Hardoon; John Shawe-Taylor",
        "abstract": "We derive a novel sparse version of Kernel Fisher Discriminant Analysis (KFDA) using an approach based on Matching Pursuit (MP). We call this algorithm Matching Pursuit Kernel Fisher Discriminant Analysis (MPKFDA). We provide generalisation error bounds analogous to those constructed for the Robust Minimax algorithm together with a sample compression bounding technique. We present experimental results on real world datasets, which show that MPKFDA is competitive with the KFDA and the SVM on UCI datasets, and additional experiments that show that the MPKFDA on average outperforms KFDA and SVM in extremely high dimensional settings.",
        "bibtex": "@InProceedings{pmlr-v5-diethe09a,\n  title = \t {Matching Pursuit Kernel Fisher Discriminant Analysis},\n  author = \t {Diethe, Tom and Hussain, Zakria and Hardoon, David and Shawe-Taylor, John},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {121--128},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/diethe09a/diethe09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/diethe09a.html},\n  abstract = \t {We derive a novel sparse version of Kernel Fisher Discriminant Analysis (KFDA) using an approach based on Matching Pursuit (MP). We call this algorithm Matching Pursuit Kernel Fisher Discriminant Analysis (MPKFDA). We provide generalisation error bounds analogous to those constructed for the Robust Minimax algorithm together with a sample compression bounding technique. We present experimental results on real world datasets, which show that MPKFDA is competitive with the KFDA and the SVM on UCI datasets, and additional experiments that show that the MPKFDA on average outperforms KFDA and SVM in extremely high dimensional settings.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/diethe09a/diethe09a.pdf",
        "supp": "",
        "pdf_size": 765588,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15681886387808817043&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "Centre for Computational Statistics and Machine Learning, Department of Computer Science, University College London, UK, WC1E 6BT; Centre for Computational Statistics and Machine Learning, Department of Computer Science, University College London, UK, WC1E 6BT; Centre for Computational Statistics and Machine Learning, Department of Computer Science, University College London, UK, WC1E 6BT; Centre for Computational Statistics and Machine Learning, Department of Computer Science, University College London, UK, WC1E 6BT",
        "aff_domain": "cs.ucl.ac.uk;cs.ucl.ac.uk;cs.ucl.ac.uk;cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk;cs.ucl.ac.uk;cs.ucl.ac.uk;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "072915465f",
        "title": "Maximum Entropy Density Estimation with Incomplete Presence-Only Data",
        "site": "https://proceedings.mlr.press/v5/huang09a.html",
        "author": "Bert Huang; Ansaf Salleb-Aouissi",
        "abstract": "We demonstrate a generalization of Maximum Entropy Density Estimation that elegantly handles incomplete presence-only data. We provide a formulation that is able to learn from known values of incomplete data without having to learn imputed values, which may be inaccurate. This saves the effort needed to perform accurate imputation while observing the principle of maximum entropy throughout the learning process. We provide analysis and examples of our algorithm under different settings of missing data.",
        "bibtex": "@InProceedings{pmlr-v5-huang09a,\n  title = \t {Maximum Entropy Density Estimation with Incomplete Presence-Only Data},\n  author = \t {Huang, Bert and Salleb-Aouissi, Ansaf},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {240--247},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/huang09a/huang09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/huang09a.html},\n  abstract = \t {We demonstrate a generalization of Maximum Entropy Density Estimation that elegantly handles incomplete presence-only data. We provide a formulation that is able to learn from known values of incomplete data without having to learn imputed values, which may be inaccurate. This saves the effort needed to perform accurate imputation while observing the principle of maximum entropy throughout the learning process. We provide analysis and examples of our algorithm under different settings of missing data.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/huang09a/huang09a.pdf",
        "supp": "",
        "pdf_size": 474505,
        "gs_citation": 14,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12172597989667015521&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Science Department, Columbia University; Center for Computational Learning Systems, Columbia University",
        "aff_domain": "cs.columbia.edu;ccls.columbia.edu",
        "email": "cs.columbia.edu;ccls.columbia.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Columbia University",
        "aff_unique_dep": "Computer Science Department",
        "aff_unique_url": "https://www.columbia.edu",
        "aff_unique_abbr": "Columbia",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "New York;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "3d359b13b3",
        "title": "Multi-Manifold Semi-Supervised Learning",
        "site": "https://proceedings.mlr.press/v5/goldberg09a.html",
        "author": "Andrew Goldberg; Xiaojin Zhu; Aarti Singh; Zhiting Xu; Robert Nowak",
        "abstract": "We study semi-supervised learning when the data consists of multiple intersecting manifolds.  We give a finite sample analysis to quantify the potential gain of using unlabeled data in this multi-manifold setting.  We then propose a semi-supervised learning algorithm that separates different manifolds into decision sets, and performs supervised learning within each set.  Our algorithm involves a novel application of Hellinger distance and size-constrained spectral clustering.  Experiments demonstrate the benefit of our multi-manifold semi-supervised learning approach.",
        "bibtex": "@InProceedings{pmlr-v5-goldberg09a,\n  title = \t {Multi-Manifold Semi-Supervised Learning},\n  author = \t {Goldberg, Andrew and Zhu, Xiaojin and Singh, Aarti and Xu, Zhiting and Nowak, Robert},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {169--176},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/goldberg09a/goldberg09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/goldberg09a.html},\n  abstract = \t {We study semi-supervised learning when the data consists of multiple intersecting manifolds.  We give a finite sample analysis to quantify the potential gain of using unlabeled data in this multi-manifold setting.  We then propose a semi-supervised learning algorithm that separates different manifolds into decision sets, and performs supervised learning within each set.  Our algorithm involves a novel application of Hellinger distance and size-constrained spectral clustering.  Experiments demonstrate the benefit of our multi-manifold semi-supervised learning approach.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/goldberg09a/goldberg09a.pdf",
        "supp": "",
        "pdf_size": 2669058,
        "gs_citation": 209,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3155253520731987550&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Computer Sciences Dept., University of Wisconsin-Madison; Computer Sciences Dept., University of Wisconsin-Madison; Applied and Computational Math, Princeton University; Computer Sciences Dept., University of Wisconsin-Madison; Elec. and Computer Engineering, University of Wisconsin-Madison",
        "aff_domain": "cs.wisc.edu;cs.wisc.edu;princeton.edu;cs.wisc.edu;ece.wisc.edu",
        "email": "cs.wisc.edu;cs.wisc.edu;princeton.edu;cs.wisc.edu;ece.wisc.edu",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "University of Wisconsin-Madison;Princeton University",
        "aff_unique_dep": "Computer Sciences Dept.;Department of Applied and Computational Mathematics",
        "aff_unique_url": "https://www.wisc.edu;https://www.princeton.edu",
        "aff_unique_abbr": "UW-Madison;Princeton",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Madison;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "6e3e93e045",
        "title": "Network Completion and Survey Sampling",
        "site": "https://proceedings.mlr.press/v5/hanneke09a.html",
        "author": "Steve Hanneke; Eric P. Xing",
        "abstract": "We study the problem of learning the topology of an undirected network  by observing a random subsample.  Specifically, the sample is chosen by  randomly selecting a fixed number of vertices, and for each we are allowed  to observe all edges it is incident with.  We analyze a general formalization  of learning from such samples, and derive confidence bounds on the number  of differences between the true and learned topologies, as a function of  the number of observed mistakes and the algorithm\u2019s bias.  In addition to this  general analysis, we also analyze a variant of the problem under a stochastic  block model assumption.",
        "bibtex": "@InProceedings{pmlr-v5-hanneke09a,\n  title = \t {Network Completion and Survey Sampling},\n  author = \t {Hanneke, Steve and Xing, Eric P.},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {209--215},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/hanneke09a/hanneke09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/hanneke09a.html},\n  abstract = \t {We study the problem of learning the topology of an undirected network  by observing a random subsample.  Specifically, the sample is chosen by  randomly selecting a fixed number of vertices, and for each we are allowed  to observe all edges it is incident with.  We analyze a general formalization  of learning from such samples, and derive confidence bounds on the number  of differences between the true and learned topologies, as a function of  the number of observed mistakes and the algorithm\u2019s bias.  In addition to this  general analysis, we also analyze a variant of the problem under a stochastic  block model assumption.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/hanneke09a/hanneke09a.pdf",
        "supp": "",
        "pdf_size": 571422,
        "gs_citation": 53,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8386333318961236797&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Machine Learning Department, Carnegie Mellon University; Machine Learning Department, Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;cs.cmu.edu",
        "email": "cs.cmu.edu;cs.cmu.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Carnegie Mellon University",
        "aff_unique_dep": "Machine Learning Department",
        "aff_unique_url": "https://www.cmu.edu",
        "aff_unique_abbr": "CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c0658d326c",
        "title": "Non-Negative Semi-Supervised Learning",
        "site": "https://proceedings.mlr.press/v5/wang09a.html",
        "author": "Changhu Wang; Shuicheng Yan; Lei Zhang; Hongjiang Zhang",
        "abstract": "The contributions of this paper are three-fold. First, we present a general formulation for reaping the benefits from both non-negative data factorization and semi-supervised learning, and the solution naturally possesses the characteristics of sparsity, robustness to partial occlusions, and greater discriminating power via extra unlabeled data. Then, an efficient multiplicative updating procedure is proposed along with its theoretic justification of the algorithmic convergency. Finally, the tensorization of this general formulation for non-negative semi-supervised learning is also briefed for handling tensor data of arbitrary order. Extensive experiments compared with the state-of-the-art algorithms for non-negative data factorization and semi-supervised learning demonstrate the algorithmic properties in sparsity, classification power, and robustness to image occlusions.",
        "bibtex": "@InProceedings{pmlr-v5-wang09a,\n  title = \t {Non-Negative Semi-Supervised Learning},\n  author = \t {Wang, Changhu and Yan, Shuicheng and Zhang, Lei and Zhang, Hongjiang},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {575--582},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/wang09a/wang09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/wang09a.html},\n  abstract = \t {The contributions of this paper are three-fold. First, we present a general formulation for reaping the benefits from both non-negative data factorization and semi-supervised learning, and the solution naturally possesses the characteristics of sparsity, robustness to partial occlusions, and greater discriminating power via extra unlabeled data. Then, an efficient multiplicative updating procedure is proposed along with its theoretic justification of the algorithmic convergency. Finally, the tensorization of this general formulation for non-negative semi-supervised learning is also briefed for handling tensor data of arbitrary order. Extensive experiments compared with the state-of-the-art algorithms for non-negative data factorization and semi-supervised learning demonstrate the algorithmic properties in sparsity, classification power, and robustness to image occlusions.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/wang09a/wang09a.pdf",
        "supp": "",
        "pdf_size": 1226511,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=694740185233550610&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 8,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "833bf197b9",
        "title": "Novelty detection: Unlabeled data definitely help",
        "site": "https://proceedings.mlr.press/v5/scott09a.html",
        "author": "Clayton Scott; Gilles Blanchard",
        "abstract": "In machine learning, one formulation of the novelty detection problem is  to build a detector based on a training sample consisting of only nominal  data. The standard (inductive) approach to this problem has been to  declare novelties where the nominal density is low, which reduces the  problem to density level set estimation. In this paper, we consider the  setting where an unlabeled and possibly contaminated sample is also  available at learning time. We argue that novelty detection is naturally  solved by a general reduction to a binary classification problem. In  particular, a detector with a desired false positive rate can be achieved  through a reduction to Neyman-Pearson classification. Unlike the inductive  approach, our approach yields detectors that are optimal (e.g.,  statistically consistent) regardless of the distribution on novelties.  Therefore, in novelty detection, unlabeled data have a substantial impact  on the theoretical properties of the decision rule.",
        "bibtex": "@InProceedings{pmlr-v5-scott09a,\n  title = \t {Novelty detection: Unlabeled data definitely help},\n  author = \t {Scott, Clayton and Blanchard, Gilles},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {464--471},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/scott09a/scott09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/scott09a.html},\n  abstract = \t {In machine learning, one formulation of the novelty detection problem is  to build a detector based on a training sample consisting of only nominal  data. The standard (inductive) approach to this problem has been to  declare novelties where the nominal density is low, which reduces the  problem to density level set estimation. In this paper, we consider the  setting where an unlabeled and possibly contaminated sample is also  available at learning time. We argue that novelty detection is naturally  solved by a general reduction to a binary classification problem. In  particular, a detector with a desired false positive rate can be achieved  through a reduction to Neyman-Pearson classification. Unlike the inductive  approach, our approach yields detectors that are optimal (e.g.,  statistically consistent) regardless of the distribution on novelties.  Therefore, in novelty detection, unlabeled data have a substantial impact  on the theoretical properties of the decision rule.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/scott09a/scott09a.pdf",
        "supp": "",
        "pdf_size": 806633,
        "gs_citation": 92,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5415830074552181179&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "University of Michigan; Fraunhofer FIRST.IDA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Michigan;Fraunhofer Institute for Software and Systems Engineering",
        "aff_unique_dep": ";FIRST.IDA",
        "aff_unique_url": "https://www.umich.edu;https://www.first.ida.fraunhofer.de/",
        "aff_unique_abbr": "UM;Fraunhofer FIRST.IDA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "019c9f443a",
        "title": "On Partitioning Rules for Bipartite Ranking",
        "site": "https://proceedings.mlr.press/v5/clemencon09a.html",
        "author": "Stephan Clemencon; Nicolas Vayatis",
        "abstract": "The purpose of this paper is to investigate the  properties of partitioning scoring rules in the  bipartite ranking setup. We focus on ranking rules based on scoring functions. General sufficient conditions for the AUC consistency of scoring functions that are constant on cells of a partition of the feature space are provided.  Rate bounds are obtained for cubic  histogram scoring rules under mild smoothness  assumptions on the regression function.  In this setup, it is shown how to penalize the  empirical AUC criterion in order to select a  scoring rule nearly as good as the one that  can be built when the degree of smoothness  of the regression function is known.",
        "bibtex": "@InProceedings{pmlr-v5-clemencon09a,\n  title = \t {On Partitioning Rules for Bipartite Ranking},\n  author = \t {Clemencon, Stephan and Vayatis, Nicolas},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {97--104},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/clemencon09a/clemencon09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/clemencon09a.html},\n  abstract = \t {The purpose of this paper is to investigate the  properties of partitioning scoring rules in the  bipartite ranking setup. We focus on ranking rules based on scoring functions. General sufficient conditions for the AUC consistency of scoring functions that are constant on cells of a partition of the feature space are provided.  Rate bounds are obtained for cubic  histogram scoring rules under mild smoothness  assumptions on the regression function.  In this setup, it is shown how to penalize the  empirical AUC criterion in order to select a  scoring rule nearly as good as the one that  can be built when the degree of smoothness  of the regression function is known.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/clemencon09a/clemencon09a.pdf",
        "supp": "",
        "pdf_size": 1004500,
        "gs_citation": 20,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=9811647516127470863&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "LTCI - Telecom/CNRS UMR 5141, Telecom ParisTech, France; CMLA - CNRS UMR 8536, ENS Cachan & UniverSud, France",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Telecom ParisTech;ENS Cachan",
        "aff_unique_dep": "LTCI - Telecom/CNRS UMR 5141;CMLA - CNRS UMR 8536",
        "aff_unique_url": "https://www.telecom-paristech.fr;",
        "aff_unique_abbr": "Telecom ParisTech;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "France"
    },
    {
        "id": "72cd3743a1",
        "title": "Online Inference of Topics with Latent Dirichlet Allocation",
        "site": "https://proceedings.mlr.press/v5/canini09a.html",
        "author": "Kevin Canini; Lei Shi; Thomas Griffiths",
        "abstract": "Inference algorithms for topic models are typically designed to be run over an entire collection of documents after they have been observed. However, in many applications of these models, the collection grows over time, making it infeasible to run batch algorithms repeatedly. This problem can be addressed by using online algorithms, which update estimates of the topics as each document is observed. We introduce two related Rao-Blackwellized online inference algorithms for the latent Dirichlet allocation (LDA) model \u2013 incremental Gibbs samplers and particle filters \u2013 and compare their runtime and performance to that of existing algorithms.",
        "bibtex": "@InProceedings{pmlr-v5-canini09a,\n  title = \t {Online Inference of Topics with Latent Dirichlet Allocation},\n  author = \t {Canini, Kevin and Shi, Lei and Griffiths, Thomas},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {65--72},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/canini09a/canini09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/canini09a.html},\n  abstract = \t {Inference algorithms for topic models are typically designed to be run over an entire collection of documents after they have been observed. However, in many applications of these models, the collection grows over time, making it infeasible to run batch algorithms repeatedly. This problem can be addressed by using online algorithms, which update estimates of the topics as each document is observed. We introduce two related Rao-Blackwellized online inference algorithms for the latent Dirichlet allocation (LDA) model \u2013 incremental Gibbs samplers and particle filters \u2013 and compare their runtime and performance to that of existing algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/canini09a/canini09a.pdf",
        "supp": "",
        "pdf_size": 895460,
        "gs_citation": 331,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6942754969824936224&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Computer Science Division, University of California, Berkeley, CA 94720; Helen Wills Neuroscience Institute, University of California, Berkeley, CA 94720; Department of Psychology, University of California, Berkeley, CA 94720",
        "aff_domain": "cs.berkeley.edu;berkeley.edu;berkeley.edu",
        "email": "cs.berkeley.edu;berkeley.edu;berkeley.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of California, Berkeley",
        "aff_unique_dep": "Computer Science Division",
        "aff_unique_url": "https://www.berkeley.edu",
        "aff_unique_abbr": "UC Berkeley",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Berkeley",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d52ac2b838",
        "title": "Optimizing Costly Functions with Simple Constraints: A Limited-Memory Projected Quasi-Newton Algorithm",
        "site": "https://proceedings.mlr.press/v5/schmidt09a.html",
        "author": "Mark Schmidt; Ewout Berg; Michael Friedlander; Kevin Murphy",
        "abstract": "An optimization algorithm for minimizing a smooth function over a    convex set is described.  Each iteration of the method computes a    descent direction by minimizing, over the original constraints, a    diagonal plus low-rank quadratic approximation to the function.  The    quadratic approximation is constructed using a limited-memory    quasi-Newton update.  The method is suitable for large-scale    problems where evaluation of the function is substantially more    expensive than projection onto the constraint set.  Numerical    experiments on one-norm regularized test problems indicate that the    proposed method is competitive with state-of-the-art methods such as    bound-constrained L-BFGS and orthant-wise descent. We further show    that the method generalizes to a wide class of problems, and     substantially improves on state-of-the-art methods for problems such    as learning the structure of Gaussian graphical models and Markov random    fields.",
        "bibtex": "@InProceedings{pmlr-v5-schmidt09a,\n  title = \t {Optimizing Costly Functions with Simple Constraints: A Limited-Memory Projected Quasi-Newton Algorithm},\n  author = \t {Schmidt, Mark and Berg, Ewout and Friedlander, Michael and Murphy, Kevin},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {456--463},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/schmidt09a/schmidt09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/schmidt09a.html},\n  abstract = \t {An optimization algorithm for minimizing a smooth function over a    convex set is described.  Each iteration of the method computes a    descent direction by minimizing, over the original constraints, a    diagonal plus low-rank quadratic approximation to the function.  The    quadratic approximation is constructed using a limited-memory    quasi-Newton update.  The method is suitable for large-scale    problems where evaluation of the function is substantially more    expensive than projection onto the constraint set.  Numerical    experiments on one-norm regularized test problems indicate that the    proposed method is competitive with state-of-the-art methods such as    bound-constrained L-BFGS and orthant-wise descent. We further show    that the method generalizes to a wide class of problems, and     substantially improves on state-of-the-art methods for problems such    as learning the structure of Gaussian graphical models and Markov random    fields.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/schmidt09a/schmidt09a.pdf",
        "supp": "",
        "pdf_size": 997749,
        "gs_citation": 345,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=914529143347341991&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": "Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia; Department of Computer Science, University of British Columbia",
        "aff_domain": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "email": "cs.ubc.ca;cs.ubc.ca;cs.ubc.ca;cs.ubc.ca",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of British Columbia",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ubc.ca",
        "aff_unique_abbr": "UBC",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Vancouver",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "86a02484bc",
        "title": "PAC-Bayes Analysis Of Maximum Entropy Classification",
        "site": "https://proceedings.mlr.press/v5/shawe-taylor09a.html",
        "author": "John Shawe-Taylor; David Hardoon",
        "abstract": "We extend and apply the PAC-Bayes theorem to the analysis of maximum entropy learning by considering maximum entropy classification. The theory introduces a multiple sampling technique that controls an effective margin of the bound. We further develop a dual implementation of the convex optimisation that optimises the bound. This algorithm is tested on some simple datasets and the value of the bound compared with the test error.",
        "bibtex": "@InProceedings{pmlr-v5-shawe-taylor09a,\n  title = \t {PAC-Bayes Analysis Of Maximum Entropy Classification},\n  author = \t {Shawe-Taylor, John and Hardoon, David},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {480--487},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/shawe-taylor09a/shawe-taylor09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/shawe-taylor09a.html},\n  abstract = \t {We extend and apply the PAC-Bayes theorem to the analysis of maximum entropy learning by considering maximum entropy classification. The theory introduces a multiple sampling technique that controls an effective margin of the bound. We further develop a dual implementation of the convex optimisation that optimises the bound. This algorithm is tested on some simple datasets and the value of the bound compared with the test error.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/shawe-taylor09a/shawe-taylor09a.pdf",
        "supp": "",
        "pdf_size": 457168,
        "gs_citation": 22,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17485349430660243403&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Centre for Computational Statistics and Machine Learning, Department of Computer Science, University College London, UK, WC1E 6BT; Centre for Computational Statistics and Machine Learning, Department of Computer Science, University College London, UK, WC1E 6BT",
        "aff_domain": "cs.ucl.ac.uk;cs.ucl.ac.uk",
        "email": "cs.ucl.ac.uk;cs.ucl.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University College London",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ucl.ac.uk",
        "aff_unique_abbr": "UCL",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "London",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "d8880095a3",
        "title": "PAC-Bayesian Generalization Bound for Density Estimation with Application to Co-clustering",
        "site": "https://proceedings.mlr.press/v5/seldin09a.html",
        "author": "Yevgeny Seldin; Naftali Tishby",
        "abstract": "We derive a PAC-Bayesian generalization bound for density estimation.  Similar to the PAC-Bayesian generalization bound for classification, the  result has the appealingly simple form of a tradeoff between empirical  performance and the KL-divergence of the posterior from the  prior. Moreover, the PAC-Bayesian generalization bound for  classification can be derived as a special case of the bound for density  estimation.    To illustrate a possible application of our bound we derive a  generalization bound for co-clustering. The bound provides a criterion  to evaluate the ability of co-clustering to predict new co-occurrences,  thus introducing a supervised flavor to this traditionally unsupervised task.",
        "bibtex": "@InProceedings{pmlr-v5-seldin09a,\n  title = \t {PAC-Bayesian Generalization Bound for Density Estimation with Application to Co-clustering},\n  author = \t {Seldin, Yevgeny and Tishby, Naftali},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {472--479},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/seldin09a/seldin09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/seldin09a.html},\n  abstract = \t {We derive a PAC-Bayesian generalization bound for density estimation.  Similar to the PAC-Bayesian generalization bound for classification, the  result has the appealingly simple form of a tradeoff between empirical  performance and the KL-divergence of the posterior from the  prior. Moreover, the PAC-Bayesian generalization bound for  classification can be derived as a special case of the bound for density  estimation.    To illustrate a possible application of our bound we derive a  generalization bound for co-clustering. The bound provides a criterion  to evaluate the ability of co-clustering to predict new co-occurrences,  thus introducing a supervised flavor to this traditionally unsupervised task.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/seldin09a/seldin09a.pdf",
        "supp": "",
        "pdf_size": 863820,
        "gs_citation": 26,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=2816141394562994551&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": "School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel; School of Computer Science and Engineering, The Hebrew University of Jerusalem, Israel",
        "aff_domain": "cs.huji.ac.il;cs.huji.ac.il",
        "email": "cs.huji.ac.il;cs.huji.ac.il",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Hebrew University of Jerusalem",
        "aff_unique_dep": "School of Computer Science and Engineering",
        "aff_unique_url": "http://www.huji.ac.il",
        "aff_unique_abbr": "HUJI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Jerusalem",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "c5d12c50ea",
        "title": "Particle Belief Propagation",
        "site": "https://proceedings.mlr.press/v5/ihler09a.html",
        "author": "Alexander Ihler; David McAllester",
        "abstract": "The popularity of particle filtering for inference in Markov chain models defined over random variables with very large or continuous domains makes it natural to consider sample\u2013based versions of belief propagation (BP) for more general (tree\u2013structured or loopy) graphs. Already, several such algorithms have been proposed in the literature.  However, many questions remain open about the behavior of particle\u2013based BP algorithms, and little theory has been developed to analyze their performance. In this paper, we describe a generic particle belief propagation (PBP) algorithm which is closely related to previously proposed methods.  We prove that this algorithm is consistent, approaching the true BP messages as the number of samples grows large. We then use concentration bounds to analyze the finite-sample behavior and give a convergence rate for the algorithm on tree\u2013structured graphs. Our convergence rate is $O(1/\\sqrt{n})$ where n is the number of samples, independent of the domain size of the variables.",
        "bibtex": "@InProceedings{pmlr-v5-ihler09a,\n  title = \t {Particle Belief Propagation},\n  author = \t {Ihler, Alexander and McAllester, David},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {256--263},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/ihler09a/ihler09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/ihler09a.html},\n  abstract = \t {The popularity of particle filtering for inference in Markov chain models defined over random variables with very large or continuous domains makes it natural to consider sample\u2013based versions of belief propagation (BP) for more general (tree\u2013structured or loopy) graphs. Already, several such algorithms have been proposed in the literature.  However, many questions remain open about the behavior of particle\u2013based BP algorithms, and little theory has been developed to analyze their performance. In this paper, we describe a generic particle belief propagation (PBP) algorithm which is closely related to previously proposed methods.  We prove that this algorithm is consistent, approaching the true BP messages as the number of samples grows large. We then use concentration bounds to analyze the finite-sample behavior and give a convergence rate for the algorithm on tree\u2013structured graphs. Our convergence rate is $O(1/\\sqrt{n})$ where n is the number of samples, independent of the domain size of the variables.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/ihler09a/ihler09a.pdf",
        "supp": "",
        "pdf_size": 545381,
        "gs_citation": 187,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13280936143452198089&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 15,
        "aff": "Dept. of Computer Science, University of California, Irvine; Toyota Technological Institite, Chicago",
        "aff_domain": "ics.uci.edu;tti-c.org",
        "email": "ics.uci.edu;tti-c.org",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of California, Irvine;Toyota Technological Institute at Chicago",
        "aff_unique_dep": "Dept. of Computer Science;",
        "aff_unique_url": "https://www.uci.edu;https://www.tti-chicago.org",
        "aff_unique_abbr": "UCI;TTI-Chicago",
        "aff_campus_unique_index": "0;1",
        "aff_campus_unique": "Irvine;Chicago",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "a3bf15b619",
        "title": "Probabilistic Models for Incomplete Multi-dimensional Arrays",
        "site": "https://proceedings.mlr.press/v5/chu09a.html",
        "author": "Wei Chu; Zoubin Ghahramani",
        "abstract": "In multiway data, each sample is measured by multiple sets of  correlated attributes. We develop a probabilistic framework for  modeling structural dependency from partially observed  multi-dimensional array data, known as pTucker. Latent components  associated with individual array dimensions are jointly retrieved  while the core tensor is integrated out. The resulting algorithm  is capable of handling large-scale data sets. We verify the  usefulness of this approach by comparing against classical models  on applications to modeling amino acid fluorescence, collaborative  filtering and a number of benchmark multiway array data.",
        "bibtex": "@InProceedings{pmlr-v5-chu09a,\n  title = \t {Probabilistic Models for Incomplete Multi-dimensional Arrays},\n  author = \t {Chu, Wei and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {89--96},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/chu09a/chu09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/chu09a.html},\n  abstract = \t {In multiway data, each sample is measured by multiple sets of  correlated attributes. We develop a probabilistic framework for  modeling structural dependency from partially observed  multi-dimensional array data, known as pTucker. Latent components  associated with individual array dimensions are jointly retrieved  while the core tensor is integrated out. The resulting algorithm  is capable of handling large-scale data sets. We verify the  usefulness of this approach by comparing against classical models  on applications to modeling amino acid fluorescence, collaborative  filtering and a number of benchmark multiway array data.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/chu09a/chu09a.pdf",
        "supp": "",
        "pdf_size": 842213,
        "gs_citation": 162,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11529001130862965232&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Yahoo! Labs.; Dept. of Engineering, Univ. of Cambridge",
        "aff_domain": "yahoo-inc.com;eng.cam.ac.uk",
        "email": "yahoo-inc.com;eng.cam.ac.uk",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Yahoo!;University of Cambridge",
        "aff_unique_dep": "Yahoo! Labs;Department of Engineering",
        "aff_unique_url": "https://yahoo.com;https://www.cam.ac.uk",
        "aff_unique_abbr": "Yahoo!;Cambridge",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "92ce7d78d3",
        "title": "Relational Topic Models for Document Networks",
        "site": "https://proceedings.mlr.press/v5/chang09a.html",
        "author": "Jonathan Chang; David Blei",
        "abstract": "We develop the relational topic model (RTM), a model of documents and the links between them. For each pair of documents, the RTM models their link as a binary random variable that is conditioned on their contents. The model can be used to summarize a network of documents, predict links between them, and predict words within them. We derive efficient inference and learning algorithms based on variational methods and evaluate the predictive performance of the RTM for large networks of scientific abstracts and web documents.",
        "bibtex": "@InProceedings{pmlr-v5-chang09a,\n  title = \t {Relational Topic Models for Document Networks},\n  author = \t {Chang, Jonathan and Blei, David},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {81--88},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/chang09a/chang09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/chang09a.html},\n  abstract = \t {We develop the relational topic model (RTM), a model of documents and the links between them. For each pair of documents, the RTM models their link as a binary random variable that is conditioned on their contents. The model can be used to summarize a network of documents, predict links between them, and predict words within them. We derive efficient inference and learning algorithms based on variational methods and evaluate the predictive performance of the RTM for large networks of scientific abstracts and web documents.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/chang09a/chang09a.pdf",
        "supp": "",
        "pdf_size": 804457,
        "gs_citation": 776,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=14527180070501111568&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "Department of Electrical Engineering, Princeton University; Department of Computer Science, Princeton University",
        "aff_domain": "princeton.edu;cs.princeton.edu",
        "email": "princeton.edu;cs.princeton.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Princeton University",
        "aff_unique_dep": "Department of Electrical Engineering",
        "aff_unique_url": "https://www.princeton.edu",
        "aff_unique_abbr": "Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0705959f7b",
        "title": "Relative Novelty Detection",
        "site": "https://proceedings.mlr.press/v5/smola09a.html",
        "author": "Alex Smola; Le Song; Choon Hui Teo",
        "abstract": "Novelty detection is an important tool for unsupervised data   analysis. It relies on finding regions of low density within which   events are then flagged as novel. By design this is dependent on the   underlying measure of the space. In this paper we derive a   formulation which is able to address this problem by allowing for a   reference measure to be given in the form of a sample from an   alternate distribution. We show that this optimization problem can   be solved efficiently and that it works well in practice.",
        "bibtex": "@InProceedings{pmlr-v5-smola09a,\n  title = \t {Relative Novelty Detection},\n  author = \t {Smola, Alex and Song, Le and Teo, Choon Hui},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {536--543},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/smola09a/smola09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/smola09a.html},\n  abstract = \t {Novelty detection is an important tool for unsupervised data   analysis. It relies on finding regions of low density within which   events are then flagged as novel. By design this is dependent on the   underlying measure of the space. In this paper we derive a   formulation which is able to address this problem by allowing for a   reference measure to be given in the form of a sample from an   alternate distribution. We show that this optimization problem can   be solved efficiently and that it works well in practice.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/smola09a/smola09a.pdf",
        "supp": "",
        "pdf_size": 2421893,
        "gs_citation": 89,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15677435166152533942&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "75fb8bcac7",
        "title": "Residual Splash for Optimally Parallelizing Belief Propagation",
        "site": "https://proceedings.mlr.press/v5/gonzalez09a.html",
        "author": "Joseph Gonzalez; Yucheng Low; Carlos Guestrin",
        "abstract": "As computer architectures move towards parallelism we must build a    new theoretical understanding of parallelism in machine learning.    In this paper we focus on parallelizing message passing inference    algorithms in graphical models. We develop a theoretical    understanding of the limitations of parallelism in belief    propagation and bound the optimal achievable running parallel    performance on a certain class of graphical models.  We demonstrate    that the fully synchronous parallelization of belief propagation is    highly inefficient.  We provide a new parallel belief propagation    which achieves optimal performance on a certain class of graphical    models.  Using two challenging real-world problems, we empirically    evaluate the performance of our algorithm. On the real-world    problems, we find that our new algorithm achieves near linear    performance improvements and out performs alternative parallel    belief propagation algorithms.",
        "bibtex": "@InProceedings{pmlr-v5-gonzalez09a,\n  title = \t {Residual Splash for Optimally Parallelizing Belief Propagation},\n  author = \t {Gonzalez, Joseph and Low, Yucheng and Guestrin, Carlos},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {177--184},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/gonzalez09a/gonzalez09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/gonzalez09a.html},\n  abstract = \t {As computer architectures move towards parallelism we must build a    new theoretical understanding of parallelism in machine learning.    In this paper we focus on parallelizing message passing inference    algorithms in graphical models. We develop a theoretical    understanding of the limitations of parallelism in belief    propagation and bound the optimal achievable running parallel    performance on a certain class of graphical models.  We demonstrate    that the fully synchronous parallelization of belief propagation is    highly inefficient.  We provide a new parallel belief propagation    which achieves optimal performance on a certain class of graphical    models.  Using two challenging real-world problems, we empirically    evaluate the performance of our algorithm. On the real-world    problems, we find that our new algorithm achieves near linear    performance improvements and out performs alternative parallel    belief propagation algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/gonzalez09a/gonzalez09a.pdf",
        "supp": "",
        "pdf_size": 932682,
        "gs_citation": 184,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3230136626567307140&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "4bac04c82d",
        "title": "Reversible Jump MCMC for Non-Negative Matrix Factorization",
        "site": "https://proceedings.mlr.press/v5/zhong09a.html",
        "author": "Mingjun Zhong; Mark Girolami",
        "abstract": "We present a fully Bayesian approach to Non-Negative Matrix Factorisation (NMF) by developing a Reversible Jump Markov Chain Monte Carlo (RJMCMC) method which provides full posteriors over the matrix components. In addition the NMF model selection issue is addressed, for the first time, as our RJMCMC procedure provides the posterior  distribution over the matrix dimensions and therefore the number of components in the NMF model. A comparative analysis is provided with the Bayesian Information Criterion (BIC) and model selection employing estimates of the marginal likelihood. An illustrative synthetic example is provided using blind mixtures of images. This is then followed by a large scale study of the recovery of component spectra  from multiplexed Raman readouts. The power and flexibility of the Bayesian methodology and the proposed RJMCMC procedure to objectively assess  differing model structures and infer the corresponding plausible component spectra for this complex data is demonstrated convincingly.",
        "bibtex": "@InProceedings{pmlr-v5-zhong09a,\n  title = \t {Reversible Jump MCMC for Non-Negative Matrix Factorization},\n  author = \t {Zhong, Mingjun and Girolami, Mark},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {663--670},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/zhong09a/zhong09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/zhong09a.html},\n  abstract = \t {We present a fully Bayesian approach to Non-Negative Matrix Factorisation (NMF) by developing a Reversible Jump Markov Chain Monte Carlo (RJMCMC) method which provides full posteriors over the matrix components. In addition the NMF model selection issue is addressed, for the first time, as our RJMCMC procedure provides the posterior  distribution over the matrix dimensions and therefore the number of components in the NMF model. A comparative analysis is provided with the Bayesian Information Criterion (BIC) and model selection employing estimates of the marginal likelihood. An illustrative synthetic example is provided using blind mixtures of images. This is then followed by a large scale study of the recovery of component spectra  from multiplexed Raman readouts. The power and flexibility of the Bayesian methodology and the proposed RJMCMC procedure to objectively assess  differing model structures and infer the corresponding plausible component spectra for this complex data is demonstrated convincingly.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/zhong09a/zhong09a.pdf",
        "supp": "",
        "pdf_size": 940988,
        "gs_citation": 29,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1068583661845672260&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 11,
        "aff": "Department of Computing Science, University of Glasgow, Glasgow, G12 8QQ, Scotland UK; Department of Computing Science, University of Glasgow, Glasgow, G12 8QQ, Scotland UK",
        "aff_domain": "dcs.gla.ac.uk;dcs.gla.ac.uk",
        "email": "dcs.gla.ac.uk;dcs.gla.ac.uk",
        "github": "",
        "project": "http://www.dcs.gla.ac.uk/inference/",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Glasgow",
        "aff_unique_dep": "Department of Computing Science",
        "aff_unique_url": "https://www.gla.ac.uk",
        "aff_unique_abbr": "UofG",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Glasgow",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "2b882d57fa",
        "title": "Sampling Techniques for the Nystrom Method",
        "site": "https://proceedings.mlr.press/v5/kumar09a.html",
        "author": "Sanjiv Kumar; Mehryar Mohri; Ameet Talwalkar",
        "abstract": "The Nystrom method is an efficient technique to generate low-rank matrix   approximations and is used in several large-scale learning applications. A   key aspect of this method is the distribution according to which columns are   sampled from the original matrix.  In this work, we present an analysis of   different sampling techniques for the Nystrom method. Our analysis includes   both empirical and theoretical components. We first present novel experiments   with several real world datasets, comparing the performance of the Nystrom   method when used with uniform versus non-uniform sampling distributions. Our   results suggest that uniform sampling without replacement, in addition to being   more efficient both in time and space, produces more effective approximations.   This motivates the theoretical part of our analysis which gives the first   performance bounds for the Nystrom method precisely when used with uniform   sampling without replacement.",
        "bibtex": "@InProceedings{pmlr-v5-kumar09a,\n  title = \t {Sampling Techniques for the Nystrom Method},\n  author = \t {Kumar, Sanjiv and Mohri, Mehryar and Talwalkar, Ameet},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {304--311},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/kumar09a/kumar09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/kumar09a.html},\n  abstract = \t {The Nystrom method is an efficient technique to generate low-rank matrix   approximations and is used in several large-scale learning applications. A   key aspect of this method is the distribution according to which columns are   sampled from the original matrix.  In this work, we present an analysis of   different sampling techniques for the Nystrom method. Our analysis includes   both empirical and theoretical components. We first present novel experiments   with several real world datasets, comparing the performance of the Nystrom   method when used with uniform versus non-uniform sampling distributions. Our   results suggest that uniform sampling without replacement, in addition to being   more efficient both in time and space, produces more effective approximations.   This motivates the theoretical part of our analysis which gives the first   performance bounds for the Nystrom method precisely when used with uniform   sampling without replacement.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/kumar09a/kumar09a.pdf",
        "supp": "",
        "pdf_size": 334273,
        "gs_citation": 177,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10021186359052931698&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 17,
        "aff": "Google Research; Courant Institute and Google Research; Courant Institute, NYU",
        "aff_domain": "google.com;cs.nyu.edu;cs.nyu.edu",
        "email": "google.com;cs.nyu.edu;cs.nyu.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Google;Courant Institute;New York University",
        "aff_unique_dep": "Google Research;Courant Institute;Courant Institute",
        "aff_unique_url": "https://research.google;https://courant.nyu.edu;https://www.courant.nyu.edu",
        "aff_unique_abbr": "Google Research;Courant;NYU",
        "aff_campus_unique_index": "0;2",
        "aff_campus_unique": "Mountain View;;New York",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "c30f9c178d",
        "title": "Semi-Supervised Affinity Propagation with Instance-Level Constraints",
        "site": "https://proceedings.mlr.press/v5/givoni09a.html",
        "author": "Inmar Givoni; Brendan Frey",
        "abstract": "Recently, affinity propagation (AP) was introduced  as an unsupervised learning algorithm for  exemplar based clustering. Here we extend the  AP model to account for semi-supervised clustering.  AP, which is formulated as inference in  a factor-graph, can be naturally extended to account  for `instance-level\u2019 constraints: pairs of  data points that cannot belong to the same cluster  (cannot-link), or must belong to the same cluster  (must-link). We present a semi-supervised AP algorithm  (SSAP) that can use instance-level constraints  to guide the clustering. We demonstrate  the applicability of SSAP to interactive image  segmentation by using SSAP to cluster superpixels  while taking into account user instructions regarding  which superpixels belong to the same object.  We demonstrate SSAP can achieve better  performance compared to other semi-supervised  methods.",
        "bibtex": "@InProceedings{pmlr-v5-givoni09a,\n  title = \t {Semi-Supervised Affinity Propagation with Instance-Level Constraints},\n  author = \t {Givoni, Inmar and Frey, Brendan},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {161--168},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/givoni09a/givoni09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/givoni09a.html},\n  abstract = \t {Recently, affinity propagation (AP) was introduced  as an unsupervised learning algorithm for  exemplar based clustering. Here we extend the  AP model to account for semi-supervised clustering.  AP, which is formulated as inference in  a factor-graph, can be naturally extended to account  for `instance-level\u2019 constraints: pairs of  data points that cannot belong to the same cluster  (cannot-link), or must belong to the same cluster  (must-link). We present a semi-supervised AP algorithm  (SSAP) that can use instance-level constraints  to guide the clustering. We demonstrate  the applicability of SSAP to interactive image  segmentation by using SSAP to cluster superpixels  while taking into account user instructions regarding  which superpixels belong to the same object.  We demonstrate SSAP can achieve better  performance compared to other semi-supervised  methods.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/givoni09a/givoni09a.pdf",
        "supp": "",
        "pdf_size": 1004371,
        "gs_citation": 116,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17688534279976799321&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "bd719cf02f",
        "title": "Sequential Learning of Classifiers for Structured Prediction Problems",
        "site": "https://proceedings.mlr.press/v5/roth09a.html",
        "author": "Dan Roth; Kevin Small; Ivan Titov",
        "abstract": "Many classification problems with structured outputs can be regarded as a set of interrelated sub-problems where constraints  dictate valid variable assignments.  The standard approaches to these problems include either independent learning of individual classifiers for each of the sub-problems or joint learning of the entire set of classifiers with the constraints enforced during learning.  We propose an intermediate approach where we learn these classifiers in a sequence using previously learned classifiers to guide learning of the next classifier by enforcing constraints between their outputs. We provide a theoretical motivation to explain why this learning protocol is expected to outperform both alternatives when individual problems have different \u2018complexity\u2019. This analysis motivates an algorithm for choosing a preferred order of classifier learning. We evaluate our technique on artificial experiments and on the entity and relation identification problem where the proposed method  outperforms both joint and independent learning.",
        "bibtex": "@InProceedings{pmlr-v5-roth09a,\n  title = \t {Sequential Learning of Classifiers for Structured Prediction Problems},\n  author = \t {Roth, Dan and Small, Kevin and Titov, Ivan},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {440--447},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/roth09a/roth09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/roth09a.html},\n  abstract = \t {Many classification problems with structured outputs can be regarded as a set of interrelated sub-problems where constraints  dictate valid variable assignments.  The standard approaches to these problems include either independent learning of individual classifiers for each of the sub-problems or joint learning of the entire set of classifiers with the constraints enforced during learning.  We propose an intermediate approach where we learn these classifiers in a sequence using previously learned classifiers to guide learning of the next classifier by enforcing constraints between their outputs. We provide a theoretical motivation to explain why this learning protocol is expected to outperform both alternatives when individual problems have different \u2018complexity\u2019. This analysis motivates an algorithm for choosing a preferred order of classifier learning. We evaluate our technique on artificial experiments and on the entity and relation identification problem where the proposed method  outperforms both joint and independent learning.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/roth09a/roth09a.pdf",
        "supp": "",
        "pdf_size": 792480,
        "gs_citation": 13,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16952544925807644701&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": "Dept. of Computer Science, Univ. of Illinois at U-C; Dept. of Computer Science, Univ. of Illinois at U-C; Dept. of Computer Science, Univ. of Illinois at U-C",
        "aff_domain": "illinois.edu;illinois.edu;illinois.edu",
        "email": "illinois.edu;illinois.edu;illinois.edu",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://illinois.edu",
        "aff_unique_abbr": "UIUC",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Urbana-Champaign",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "e32beb8ad9",
        "title": "Sleeping Experts and Bandits with Stochastic Action Availability and Adversarial Rewards",
        "site": "https://proceedings.mlr.press/v5/kanade09a.html",
        "author": "Varun Kanade; H. Brendan McMahan; Brent Bryan",
        "abstract": "We consider algorithms for selecting actions in order to maximize rewards chosen by an adversary, where the set of actions available on any given round is selected stochastically. We present the first polynomial-time no-regret algorithms for this setting. In the full-observation (experts) version of the problem, we present an exponential-weights algorithm that achieves regret $\\mathcal{O}(\\sqrt{T \\log n})$, which is the best possible. For the bandit setting (where the algorithm only observes the reward of the action selected), we present a no-regret algorithm based on follow-the-perturbed-leader. This algorithm runs in polynomial time, unlike the EXP4 algorithm which can also be applied to this setting. Our algorithm has the interesting interpretation of solving a geometric experts problem where the embedding in which rewards are linear is never explicitly constructed. We argue that this adversarial-reward, stochastic availability formulation is important in practice, as assuming stationary stochastic rewards is unrealistic in many domains.",
        "bibtex": "@InProceedings{pmlr-v5-kanade09a,\n  title = \t {Sleeping Experts and Bandits with Stochastic Action Availability and Adversarial Rewards},\n  author = \t {Kanade, Varun and McMahan, H. Brendan and Bryan, Brent},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {272--279},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/kanade09a/kanade09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/kanade09a.html},\n  abstract = \t {We consider algorithms for selecting actions in order to maximize rewards chosen by an adversary, where the set of actions available on any given round is selected stochastically. We present the first polynomial-time no-regret algorithms for this setting. In the full-observation (experts) version of the problem, we present an exponential-weights algorithm that achieves regret $\\mathcal{O}(\\sqrt{T \\log n})$, which is the best possible. For the bandit setting (where the algorithm only observes the reward of the action selected), we present a no-regret algorithm based on follow-the-perturbed-leader. This algorithm runs in polynomial time, unlike the EXP4 algorithm which can also be applied to this setting. Our algorithm has the interesting interpretation of solving a geometric experts problem where the embedding in which rewards are linear is never explicitly constructed. We argue that this adversarial-reward, stochastic availability formulation is important in practice, as assuming stationary stochastic rewards is unrealistic in many domains.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/kanade09a/kanade09a.pdf",
        "supp": "",
        "pdf_size": 927027,
        "gs_citation": 74,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=7794402247164202158&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 14,
        "aff": "Georgia Tech; Google Inc.; Google Inc.",
        "aff_domain": "cc.gatech.edu;google.com;google.com",
        "email": "cc.gatech.edu;google.com;google.com",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": "Georgia Institute of Technology;Google",
        "aff_unique_dep": ";Google",
        "aff_unique_url": "https://www.gatech.edu;https://www.google.com",
        "aff_unique_abbr": "Georgia Tech;Google",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "d502270ec2",
        "title": "Spanning Tree Approximations for Conditional Random Fields",
        "site": "https://proceedings.mlr.press/v5/pletscher09a.html",
        "author": "Patrick Pletscher; Cheng Soon Ong; Joachim Buhmann",
        "abstract": "In this work we show that one can train Conditional Random Fields of intractable graphs effectively and efficiently by considering a mixture of random spanning trees of the underlying graphical model. Furthermore, we show how a maximum-likelihood estimator of such a training objective can subsequently be used for prediction on the full graph. We present experimental results which improve on the state-of-the-art. Additionally, the training objective is less sensitive to the regularization than pseudo-likelihood based training approaches. We perform the experimental validation on two classes of data sets where structure is important: image denoising and multilabel classification.",
        "bibtex": "@InProceedings{pmlr-v5-pletscher09a,\n  title = \t {Spanning Tree Approximations for Conditional Random Fields},\n  author = \t {Pletscher, Patrick and Ong, Cheng Soon and Buhmann, Joachim},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {408--415},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/pletscher09a/pletscher09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/pletscher09a.html},\n  abstract = \t {In this work we show that one can train Conditional Random Fields of intractable graphs effectively and efficiently by considering a mixture of random spanning trees of the underlying graphical model. Furthermore, we show how a maximum-likelihood estimator of such a training objective can subsequently be used for prediction on the full graph. We present experimental results which improve on the state-of-the-art. Additionally, the training objective is less sensitive to the regularization than pseudo-likelihood based training approaches. We perform the experimental validation on two classes of data sets where structure is important: image denoising and multilabel classification.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/pletscher09a/pletscher09a.pdf",
        "supp": "",
        "pdf_size": 820895,
        "gs_citation": 28,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5187224087450116487&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 16,
        "aff": "Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "5faedcad54",
        "title": "Sparse Probabilistic Principal Component Analysis",
        "site": "https://proceedings.mlr.press/v5/guan09a.html",
        "author": "Yue Guan; Jennifer Dy",
        "abstract": "Principal component analysis (PCA) is a  popular dimensionality reduction algorithm.  However, it is not easy to interpret which of  the original features are important based on  the principal components. Recent methods improve interpretability by sparsifying PCA through adding an $L_1$ regularizer. In this paper,  we introduce a probabilistic formulation  for sparse PCA. By presenting sparse PCA  as a probabilistic Bayesian formulation, we  gain the benefit of automatic model selection.  We examine three different priors for achieving  sparsification: (1) a two-level hierarchical prior equivalent to a Laplacian distribution and consequently to an $L_1$ regularization, (2) an inverse-Gaussian prior, and (3) a Jeffrey\u2019s prior. We learn these models by applying variational inference. Our experiments verify that indeed our sparse probabilistic model  results in a sparse PCA solution.",
        "bibtex": "@InProceedings{pmlr-v5-guan09a,\n  title = \t {Sparse Probabilistic Principal Component Analysis},\n  author = \t {Guan, Yue and Dy, Jennifer},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {185--192},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/guan09a/guan09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/guan09a.html},\n  abstract = \t {Principal component analysis (PCA) is a  popular dimensionality reduction algorithm.  However, it is not easy to interpret which of  the original features are important based on  the principal components. Recent methods improve interpretability by sparsifying PCA through adding an $L_1$ regularizer. In this paper,  we introduce a probabilistic formulation  for sparse PCA. By presenting sparse PCA  as a probabilistic Bayesian formulation, we  gain the benefit of automatic model selection.  We examine three different priors for achieving  sparsification: (1) a two-level hierarchical prior equivalent to a Laplacian distribution and consequently to an $L_1$ regularization, (2) an inverse-Gaussian prior, and (3) a Jeffrey\u2019s prior. We learn these models by applying variational inference. Our experiments verify that indeed our sparse probabilistic model  results in a sparse PCA solution.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/guan09a/guan09a.pdf",
        "supp": "",
        "pdf_size": 829791,
        "gs_citation": 141,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13275657316239767030&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": "Electrical and Computer Engineering Dept., Northeastern University, Boston, MA 02115, USA; Electrical and Computer Engineering Dept., Northeastern University, Boston, MA 02115, USA",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Northeastern University",
        "aff_unique_dep": "Electrical and Computer Engineering Dept.",
        "aff_unique_url": "https://www.northeastern.edu",
        "aff_unique_abbr": "NU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Boston",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "140b5658d1",
        "title": "Speed and Sparsity of Regularized Boosting",
        "site": "https://proceedings.mlr.press/v5/xi09a.html",
        "author": "Yongxin Xi; Zhen Xiang; Peter Ramadge; Robert Schapire",
        "abstract": "Boosting algorithms with $l_1$ regularization are of interest because $l_1$ regularization leads to sparser composite classifiers. Moreover, Rosset et al. have shown that for separable data, standard $l_p$ regularized loss minimization results in a margin maximizing classifier in the limit as regularization is relaxed. For the case $p=1$, we extend these results by obtaining explicit convergence bounds on the regularization required to yield a margin within prescribed accuracy of the maximum achievable margin. We derive similar rates of convergence for the $\\epsilon$-AdaBoost algorithm, in the process providing a new proof that $\\epsilon$-AdaBoost is margin maximizing as $\\epsilon$ converges to $0$. Because both of these known algorithms are computationally expensive, we introduce a new hybrid algorithm, AdaBoost+$L_1$, that combines the virtues of AdaBoost with the sparsity of $l_1$ regularization in a computationally efficient fashion. We prove that the algorithm is margin maximizing and empirically examine its performance on five datasets.",
        "bibtex": "@InProceedings{pmlr-v5-xi09a,\n  title = \t {Speed and Sparsity of Regularized Boosting},\n  author = \t {Xi, Yongxin and Xiang, Zhen and Ramadge, Peter and Schapire, Robert},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {615--622},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/xi09a/xi09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/xi09a.html},\n  abstract = \t {Boosting algorithms with $l_1$ regularization are of interest because $l_1$ regularization leads to sparser composite classifiers. Moreover, Rosset et al. have shown that for separable data, standard $l_p$ regularized loss minimization results in a margin maximizing classifier in the limit as regularization is relaxed. For the case $p=1$, we extend these results by obtaining explicit convergence bounds on the regularization required to yield a margin within prescribed accuracy of the maximum achievable margin. We derive similar rates of convergence for the $\\epsilon$-AdaBoost algorithm, in the process providing a new proof that $\\epsilon$-AdaBoost is margin maximizing as $\\epsilon$ converges to $0$. Because both of these known algorithms are computationally expensive, we introduce a new hybrid algorithm, AdaBoost+$L_1$, that combines the virtues of AdaBoost with the sparsity of $l_1$ regularization in a computationally efficient fashion. We prove that the algorithm is margin maximizing and empirically examine its performance on five datasets.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/xi09a/xi09a.pdf",
        "supp": "",
        "pdf_size": 966562,
        "gs_citation": 36,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4109766614910141620&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 11,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "02b7a1cf3d",
        "title": "Statistical and Computational Tradeoffs in Stochastic Composite Likelihood",
        "site": "https://proceedings.mlr.press/v5/dillon09a.html",
        "author": "Joshua Dillon; Guy Lebanon",
        "abstract": "Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. We prove the consistency of the estimators, provide formulas for their asymptotic variance and computational complexity, and discuss experimental results in the context of Boltzmann machines and conditional random fields. The theoretical and experimental studies demonstrate the effectiveness of the estimators in achieving a predefined balance between computational complexity and statistical accuracy.",
        "bibtex": "@InProceedings{pmlr-v5-dillon09a,\n  title = \t {Statistical and Computational Tradeoffs in Stochastic Composite Likelihood},\n  author = \t {Dillon, Joshua and Lebanon, Guy},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {129--136},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/dillon09a/dillon09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/dillon09a.html},\n  abstract = \t {Maximum likelihood estimators are often of limited practical use due to the intensive computation they require. We propose a family of alternative estimators that maximize a stochastic variation of the composite likelihood function. We prove the consistency of the estimators, provide formulas for their asymptotic variance and computational complexity, and discuss experimental results in the context of Boltzmann machines and conditional random fields. The theoretical and experimental studies demonstrate the effectiveness of the estimators in achieving a predefined balance between computational complexity and statistical accuracy.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/dillon09a/dillon09a.pdf",
        "supp": "",
        "pdf_size": 616138,
        "gs_citation": 17,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=17290065518345498067&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "College of Computing, Georgia Institute of Technology; College of Computing, Georgia Institute of Technology",
        "aff_domain": "gatech.edu;cc.gatech.edu",
        "email": "gatech.edu;cc.gatech.edu",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "College of Computing",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Atlanta",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "485381a809",
        "title": "Structure Identification by Optimized Interventions",
        "site": "https://proceedings.mlr.press/v5/busetto09a.html",
        "author": "Alberto Giovanni Busetto; Joachim Buhmann",
        "abstract": "We consider the problem of optimal experimental design in structure identification. Whereas standard approaches simply minimize Shannon\u2019s entropy of the estimated parameter posterior, we show how to select between alternative model configurations, too. Our method specifies the intervention that makes an experiment capable of determining whether or not a particular configuration hypothesis is correct. This is performed by a novel clustering technique in approximated Bayesian parameter estimation for non-linear dynamical systems. The computation of the perturbation that minimizes the effective number of clusters in the belief state is constrained by the increase of the expected Kullback-Leibler divergence between the parameter prior and the posterior. This enables the disambiguation of persisting alternative explanations in cases where standard design systematically fails. Its applicability is illustrated with a biochemical Goodwin model, showing correct identification between multiple kinetic structures. We expect that our approach will prove useful especially for complex structures with reduced observability and multimodal posteriors.",
        "bibtex": "@InProceedings{pmlr-v5-busetto09a,\n  title = \t {Structure Identification by Optimized Interventions},\n  author = \t {Busetto, Alberto Giovanni and Buhmann, Joachim},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {57--64},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/busetto09a/busetto09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/busetto09a.html},\n  abstract = \t {We consider the problem of optimal experimental design in structure identification. Whereas standard approaches simply minimize Shannon\u2019s entropy of the estimated parameter posterior, we show how to select between alternative model configurations, too. Our method specifies the intervention that makes an experiment capable of determining whether or not a particular configuration hypothesis is correct. This is performed by a novel clustering technique in approximated Bayesian parameter estimation for non-linear dynamical systems. The computation of the perturbation that minimizes the effective number of clusters in the belief state is constrained by the increase of the expected Kullback-Leibler divergence between the parameter prior and the posterior. This enables the disambiguation of persisting alternative explanations in cases where standard design systematically fails. Its applicability is illustrated with a biochemical Goodwin model, showing correct identification between multiple kinetic structures. We expect that our approach will prove useful especially for complex structures with reduced observability and multimodal posteriors.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/busetto09a/busetto09a.pdf",
        "supp": "",
        "pdf_size": 540074,
        "gs_citation": 12,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=11477749029493075736&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Department of Computer Science, ETH Zurich, Zurich, Switzerland+Life Science Zurich PhD Program on Systems Biology of Complex Diseases; Competence Center for Systems Physiology and Metabolic Diseases, Zurich, Switzerland",
        "aff_domain": "inf.ethz.ch;inf.ethz.ch",
        "email": "inf.ethz.ch;inf.ethz.ch",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "ETH Zurich;University of Zurich;Meta",
        "aff_unique_dep": "Department of Computer Science;Life Science Zurich PhD Program;Competence Center for Systems Physiology and Metabolic Diseases",
        "aff_unique_url": "https://www.ethz.ch;https://www.unizh.ch;",
        "aff_unique_abbr": "ETHZ;UZH;",
        "aff_campus_unique_index": "0+0;0",
        "aff_campus_unique": "Zurich",
        "aff_country_unique_index": "0+0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "ecc494192c",
        "title": "Supervised Spectral Latent Variable Models",
        "site": "https://proceedings.mlr.press/v5/bo09a.html",
        "author": "Liefeng Bo; Cristian Sminchisescu",
        "abstract": "We present a probabilistic structured prediction method for learning input-output dependencies where correlations between outputs are modeled as low-dimensional manifolds constrained by both geometric, distance preserving output relations,and predictive power of inputs. Technically this reduces to learning a probabilistic, input conditional model, over latent (manifold) and output variables using an alternation scheme. In one round, we optimize the parameters of an input-driven manifold predictor using latent targets given by preimages (conditional expectations) of the current manifold-to-output model. In the next round, we use the distribution given by the manifold predictor in order to maximize the probability of the outputs with an additional, implicit distance preserving constraint on the manifold. The resulting Supervised Spectral Latent Variable Model (SSLVM) combines the properties of probabilistic geometric manifold learning (accommodates geometric constraints corresponding to any spectral embedding method including PCA, ISOMAP or Laplacian Eigenmaps), with the additional supervisory information to further constrain it for predictive tasks. We demonstrate the superiority of the method over baseline PPCA + regression frameworks and show its potential in difficult realworld computer vision benchmarks designed for the reconstruction of three-dimensional human poses from monocular image sequences.",
        "bibtex": "@InProceedings{pmlr-v5-bo09a,\n  title = \t {Supervised Spectral Latent Variable Models},\n  author = \t {Bo, Liefeng and Sminchisescu, Cristian},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {33--40},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/bo09a/bo09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/bo09a.html},\n  abstract = \t {We present a probabilistic structured prediction method for learning input-output dependencies where correlations between outputs are modeled as low-dimensional manifolds constrained by both geometric, distance preserving output relations,and predictive power of inputs. Technically this reduces to learning a probabilistic, input conditional model, over latent (manifold) and output variables using an alternation scheme. In one round, we optimize the parameters of an input-driven manifold predictor using latent targets given by preimages (conditional expectations) of the current manifold-to-output model. In the next round, we use the distribution given by the manifold predictor in order to maximize the probability of the outputs with an additional, implicit distance preserving constraint on the manifold. The resulting Supervised Spectral Latent Variable Model (SSLVM) combines the properties of probabilistic geometric manifold learning (accommodates geometric constraints corresponding to any spectral embedding method including PCA, ISOMAP or Laplacian Eigenmaps), with the additional supervisory information to further constrain it for predictive tasks. We demonstrate the superiority of the method over baseline PPCA + regression frameworks and show its potential in difficult realworld computer vision benchmarks designed for the reconstruction of three-dimensional human poses from monocular image sequences.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/bo09a/bo09a.pdf",
        "supp": "",
        "pdf_size": 890104,
        "gs_citation": 11,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=10865360033694957807&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 8,
        "aff": "Toyota Technological Institute at Chicago; University of Bonn",
        "aff_domain": "tti-c.org; sminchisescu.ins.uni-bonn.de",
        "email": "tti-c.org; sminchisescu.ins.uni-bonn.de",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;University of Bonn",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tti-chicago.org;https://www.uni-bonn.de/",
        "aff_unique_abbr": "TTI Chicago;UBonn",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;1",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "e58d7f9d32",
        "title": "The Block Diagonal Infinite Hidden Markov Model",
        "site": "https://proceedings.mlr.press/v5/stepleton09a.html",
        "author": "Thomas Stepleton; Zoubin Ghahramani; Geoffrey Gordon; Tai-Sing Lee",
        "abstract": "The Infinite Hidden Markov Model (IHMM) extends hidden Markov models to have a countably infinite number of hidden states (Beal et al., 2002; Teh et al., 2006). We present a generalization of this framework that introduces block-diagonal structure in the transitions between the hidden states. These blocks correspond to \u201csub-behaviors\u201d exhibited by data sequences. In identifying such structure, the model classifies, or partitions, sequence data according to these sub-behaviors in an unsupervised way. We present an application of this model to artificial data, a video gesture classification task, and a musical theme labeling task, and show that components of the model can also be applied to graph segmentation.",
        "bibtex": "@InProceedings{pmlr-v5-stepleton09a,\n  title = \t {The Block Diagonal Infinite Hidden Markov Model},\n  author = \t {Stepleton, Thomas and Ghahramani, Zoubin and Gordon, Geoffrey and Lee, Tai-Sing},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {552--559},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/stepleton09a/stepleton09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/stepleton09a.html},\n  abstract = \t {The Infinite Hidden Markov Model (IHMM) extends hidden Markov models to have a countably infinite number of hidden states (Beal et al., 2002; Teh et al., 2006). We present a generalization of this framework that introduces block-diagonal structure in the transitions between the hidden states. These blocks correspond to \u201csub-behaviors\u201d exhibited by data sequences. In identifying such structure, the model classifies, or partitions, sequence data according to these sub-behaviors in an unsupervised way. We present an application of this model to artificial data, a video gesture classification task, and a musical theme labeling task, and show that components of the model can also be applied to graph segmentation.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/stepleton09a/stepleton09a.pdf",
        "supp": "",
        "pdf_size": 2063911,
        "gs_citation": 25,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=6327809327414982188&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 19,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "3138f7626f",
        "title": "The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training",
        "site": "https://proceedings.mlr.press/v5/erhan09a.html",
        "author": "Dumitru Erhan; Pierre-Antoine Manzagol; Yoshua Bengio; Samy Bengio; Pascal Vincent",
        "abstract": "Whereas theoretical work suggests that deep architectures might be more efficient at representing highly-varying functions, training deep architectures  was unsuccessful until the recent advent of algorithms based on unsupervised pre-training. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments confirm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization, the positive  effect of pre-training in terms of optimization and its role as a kind of regularizer. We show the influence of architecture depth, model capacity, and number of training examples.",
        "bibtex": "@InProceedings{pmlr-v5-erhan09a,\n  title = \t {The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training},\n  author = \t {Erhan, Dumitru and Manzagol, Pierre-Antoine and Bengio, Yoshua and Bengio, Samy and Vincent, Pascal},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {153--160},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/erhan09a/erhan09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/erhan09a.html},\n  abstract = \t {Whereas theoretical work suggests that deep architectures might be more efficient at representing highly-varying functions, training deep architectures  was unsuccessful until the recent advent of algorithms based on unsupervised pre-training. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments confirm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization, the positive  effect of pre-training in terms of optimization and its role as a kind of regularizer. We show the influence of architecture depth, model capacity, and number of training examples.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/erhan09a/erhan09a.pdf",
        "supp": "",
        "pdf_size": 954480,
        "gs_citation": 664,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5399752359162851171&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 18,
        "aff": "DIRO, Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, Qu\u00e9bec, Canada+Google, Mountain View, California, USA; DIRO, Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, Qu\u00e9bec, Canada; DIRO, Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, Qu\u00e9bec, Canada; Google, Mountain View, California, USA; DIRO, Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, Qu\u00e9bec, Canada",
        "aff_domain": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;google.com;iro.umontreal.ca",
        "email": "iro.umontreal.ca;iro.umontreal.ca;iro.umontreal.ca;google.com;iro.umontreal.ca",
        "github": "",
        "project": "",
        "author_num": 5,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;0;0;1;0",
        "aff_unique_norm": "Universit\u00e9 de Montr\u00e9al;Google",
        "aff_unique_dep": "DIRO;Google",
        "aff_unique_url": "https://www.umontreal.ca;https://www.google.com",
        "aff_unique_abbr": "UdeM;Google",
        "aff_campus_unique_index": "0+1;0;0;1;0",
        "aff_campus_unique": "Montr\u00e9al;Mountain View",
        "aff_country_unique_index": "0+1;0;0;1;0",
        "aff_country_unique": "Canada;United States"
    },
    {
        "id": "1fcb785a17",
        "title": "Tighter and Convex Maximum Margin Clustering",
        "site": "https://proceedings.mlr.press/v5/li09c.html",
        "author": "Yu-Feng Li; Ivor W. Tsang; Jame Kwok; Zhi-Hua Zhou",
        "abstract": "Maximum margin principle  has been successfully applied to many  supervised and semi-supervised problems in machine learning.  Recently, this principle was extended for clustering, referred to as  Maximum Margin Clustering (MMC) and achieved promising performance  in recent studies. To avoid the problem of local minima, MMC can be  solved globally via convex semi-definite programming (SDP)  relaxation. Although many efficient approaches have been proposed to  alleviate the computational burden of SDP, convex MMCs are still not  scalable for medium data sets. In this paper, we propose a novel  convex optimization method, LG-MMC, which maximizes the margin of  opposite clusters via label generation. It can be shown that LG-MMC  is much more scalable than existing convex approaches. Moreover, we  show that our convex relaxation is tighter than state-of-art convex  MMCs. Experiments on eighteen UCI datasets and MNIST dataset show  significant improvement over existing MMC algorithms.",
        "bibtex": "@InProceedings{pmlr-v5-li09c,\n  title = \t {Tighter and Convex Maximum Margin Clustering},\n  author = \t {Li, Yu-Feng and Tsang, Ivor W. and Kwok, Jame and Zhou, Zhi-Hua},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {344--351},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/li09c/li09c.pdf},\n  url = \t {https://proceedings.mlr.press/v5/li09c.html},\n  abstract = \t {Maximum margin principle  has been successfully applied to many  supervised and semi-supervised problems in machine learning.  Recently, this principle was extended for clustering, referred to as  Maximum Margin Clustering (MMC) and achieved promising performance  in recent studies. To avoid the problem of local minima, MMC can be  solved globally via convex semi-definite programming (SDP)  relaxation. Although many efficient approaches have been proposed to  alleviate the computational burden of SDP, convex MMCs are still not  scalable for medium data sets. In this paper, we propose a novel  convex optimization method, LG-MMC, which maximizes the margin of  opposite clusters via label generation. It can be shown that LG-MMC  is much more scalable than existing convex approaches. Moreover, we  show that our convex relaxation is tighter than state-of-art convex  MMCs. Experiments on eighteen UCI datasets and MNIST dataset show  significant improvement over existing MMC algorithms.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/li09c/li09c.pdf",
        "supp": "",
        "pdf_size": 947354,
        "gs_citation": 150,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=1302013814157029856&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 22,
        "aff": "National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210093, China; School of Computer Engineering, Nanyang Technological University, Singapore 639798; Department of Computer Science and Engineering, Hong Kong University of Science & Technology, Hong Kong; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210093, China",
        "aff_domain": "lamda.nju.edu.cn;ntu.edu.sg;cse.ust.hk;lamda.nju.edu.cn",
        "email": "lamda.nju.edu.cn;ntu.edu.sg;cse.ust.hk;lamda.nju.edu.cn",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Nanjing University;Nanyang Technological University;Hong Kong University of Science & Technology",
        "aff_unique_dep": "National Key Laboratory for Novel Software Technology;School of Computer Engineering;Department of Computer Science and Engineering",
        "aff_unique_url": "http://www.nju.edu.cn;https://www.ntu.edu.sg;https://www.ust.hk",
        "aff_unique_abbr": "Nanjing U;NTU;HKUST",
        "aff_campus_unique_index": "0;1;2;0",
        "aff_campus_unique": "Nanjing;Singapore;Hong Kong SAR",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "2557538b54",
        "title": "Tractable Bayesian Inference of Time-Series Dependence Structure",
        "site": "https://proceedings.mlr.press/v5/siracusa09a.html",
        "author": "Michael Siracusa; John Fisher III",
        "abstract": "We consider the problem of Bayesian inference over graphical structures describing the interactions among multiple vector time-series. A directed temporal interaction model is presented which assumes a fixed dependence structure among time-series. Using a conjugate prior over this model\u2019s structure and parameters, we focus our attention on characterizing the exact posterior uncertainty in the structure given data. The model is extended via the introduction of a dynamically evolving latent variable which indexes dependence structures over time. Performing inference using this model yields promising results when analyzing the interaction of multiple tracked moving objects.",
        "bibtex": "@InProceedings{pmlr-v5-siracusa09a,\n  title = \t {Tractable Bayesian Inference of Time-Series Dependence Structure},\n  author = \t {Siracusa, Michael and III, John Fisher},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {528--535},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/siracusa09a/siracusa09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/siracusa09a.html},\n  abstract = \t {We consider the problem of Bayesian inference over graphical structures describing the interactions among multiple vector time-series. A directed temporal interaction model is presented which assumes a fixed dependence structure among time-series. Using a conjugate prior over this model\u2019s structure and parameters, we focus our attention on characterizing the exact posterior uncertainty in the structure given data. The model is extended via the introduction of a dynamically evolving latent variable which indexes dependence structures over time. Performing inference using this model yields promising results when analyzing the interaction of multiple tracked moving objects.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/siracusa09a/siracusa09a.pdf",
        "supp": "",
        "pdf_size": 579482,
        "gs_citation": 27,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=16188727108280460902&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 9,
        "aff": "CSAIL, Massachusetts Institute of Technology, Cambridge, MA 02139; CSAIL, Massachusetts Institute of Technology, Cambridge, MA 02139",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "Computer Science and Artificial Intelligence Laboratory (CSAIL)",
        "aff_unique_url": "https://www.csail.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "076bd54901",
        "title": "Tractable Search for Learning Exponential Models of Rankings",
        "site": "https://proceedings.mlr.press/v5/mandhani09a.html",
        "author": "Bhushan Mandhani; Marina Meila",
        "abstract": "We consider the problem of learning the Generalized Mallows (GM) model  of [Fligner and Verducci, 1986], which represents a probability distribution over all  possible permutations (aka rankings) of a given set of objects. The  training data consists of a set of permutations. This problem  generalizes the well known rank aggregation problem. Maximum  Likelihood estimation of the GM model is NP-hard. An exact but  inefficient search-based method was recently proposed for this  problem. Here we introduce the first non-trivial heuristic function  for this search. We justify it theoretically, and show why it is  admissible in practice. We experimentally demonstrate its  effectiveness, and show that it is superior to existing techniques for  learning the GM model. We also show good performance of a family of  faster approximate methods of search.",
        "bibtex": "@InProceedings{pmlr-v5-mandhani09a,\n  title = \t {Tractable Search for Learning Exponential Models of Rankings},\n  author = \t {Mandhani, Bhushan and Meila, Marina},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {392--399},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/mandhani09a/mandhani09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/mandhani09a.html},\n  abstract = \t {We consider the problem of learning the Generalized Mallows (GM) model  of [Fligner and Verducci, 1986], which represents a probability distribution over all  possible permutations (aka rankings) of a given set of objects. The  training data consists of a set of permutations. This problem  generalizes the well known rank aggregation problem. Maximum  Likelihood estimation of the GM model is NP-hard. An exact but  inefficient search-based method was recently proposed for this  problem. Here we introduce the first non-trivial heuristic function  for this search. We justify it theoretically, and show why it is  admissible in practice. We experimentally demonstrate its  effectiveness, and show that it is superior to existing techniques for  learning the GM model. We also show good performance of a family of  faster approximate methods of search.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/mandhani09a/mandhani09a.pdf",
        "supp": "",
        "pdf_size": 486522,
        "gs_citation": 56,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=13962317431489865132&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 6,
        "aff": "Dept. of Computer Science, University of Washington, Seattle USA 98195; Dept. of Statistics, University of Washington, Seattle USA 98195",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Washington",
        "aff_unique_dep": "Department of Computer Science",
        "aff_unique_url": "https://www.washington.edu",
        "aff_unique_abbr": "UW",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Seattle",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "76004852db",
        "title": "Tree Block Coordinate Descent for MAP in Graphical Models",
        "site": "https://proceedings.mlr.press/v5/sontag09a.html",
        "author": "David Sontag; Tommi Jaakkola",
        "abstract": "A number of linear programming relaxations have been proposed for finding most likely settings of the variables (MAP) in large probabilistic models. The relaxations are often succinctly expressed in the dual and reduce to different types of reparameterizations of the original model. The dual objectives are typically solved by performing local block coordinate descent steps. In this work, we show how to perform block coordinate descent on spanning trees of the graphical model. We also show how all of the earlier dual algorithms are related to each other, giving transformations from one type of reparameterization to another while maintaining monotonicity relative to a common objective function. Finally, we quantify when the MAP solution can and cannot be decoded directly from the dual LP relaxation.",
        "bibtex": "@InProceedings{pmlr-v5-sontag09a,\n  title = \t {Tree Block Coordinate Descent for MAP in Graphical Models},\n  author = \t {Sontag, David and Jaakkola, Tommi},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {544--551},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/sontag09a/sontag09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/sontag09a.html},\n  abstract = \t {A number of linear programming relaxations have been proposed for finding most likely settings of the variables (MAP) in large probabilistic models. The relaxations are often succinctly expressed in the dual and reduce to different types of reparameterizations of the original model. The dual objectives are typically solved by performing local block coordinate descent steps. In this work, we show how to perform block coordinate descent on spanning trees of the graphical model. We also show how all of the earlier dual algorithms are related to each other, giving transformations from one type of reparameterization to another while maintaining monotonicity relative to a common objective function. Finally, we quantify when the MAP solution can and cannot be decoded directly from the dual LP relaxation.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/sontag09a/sontag09a.pdf",
        "supp": "",
        "pdf_size": 1226282,
        "gs_citation": 97,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=4422838295483564948&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 13,
        "aff": ";",
        "aff_domain": ";",
        "email": ";",
        "github": "",
        "project": "",
        "author_num": 2,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "d5f6cfe779",
        "title": "Tree-Based Inference for Dirichlet Process Mixtures",
        "site": "https://proceedings.mlr.press/v5/xu09a.html",
        "author": "Yang Xu; Katherine Heller; Zoubin Ghahramani",
        "abstract": "The Dirichlet process mixture (DPM) is a widely used model for clustering and for general nonparametric Bayesian density estimation. Unfortunately, like in many statistical models, exact inference in a DPM is intractable, and  approximate methods are needed to perform efficient inference. While most attention in the literature has been placed on Markov chain Monte Carlo (MCMC), variational Bayesian (VB) and collapsed variational methods, Heller and Ghahramani (2005) recently introduced a novel class of approximation for DPMs based on Bayesian hierarchical clustering (BHC). These tree-based combinatorial approximations efficiently sum over exponentially many ways of partitioning the data and offer a novel lower bound on the marginal likelihood of the DPM. In this paper we make the following contributions: (1) We show empirically that the BHC lower bounds are substantially tighter than the bounds given by VB and by collapsed variational methods on synthetic and real datasets. (2) We also show that BHC offers a more accurate predictive performance on these datasets. (3) We further improve the tree-based lower bounds with an algorithm that efficiently sums contributions from alternative trees. (4) We present a fast approximate method for BHC. Our results suggest that our combinatorial approximate inference methods and lower bounds may be useful not only in DPMs but in other models as well.",
        "bibtex": "@InProceedings{pmlr-v5-xu09a,\n  title = \t {Tree-Based Inference for Dirichlet Process Mixtures},\n  author = \t {Xu, Yang and Heller, Katherine and Ghahramani, Zoubin},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {623--630},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/xu09a/xu09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/xu09a.html},\n  abstract = \t {The Dirichlet process mixture (DPM) is a widely used model for clustering and for general nonparametric Bayesian density estimation. Unfortunately, like in many statistical models, exact inference in a DPM is intractable, and  approximate methods are needed to perform efficient inference. While most attention in the literature has been placed on Markov chain Monte Carlo (MCMC), variational Bayesian (VB) and collapsed variational methods, Heller and Ghahramani (2005) recently introduced a novel class of approximation for DPMs based on Bayesian hierarchical clustering (BHC). These tree-based combinatorial approximations efficiently sum over exponentially many ways of partitioning the data and offer a novel lower bound on the marginal likelihood of the DPM. In this paper we make the following contributions: (1) We show empirically that the BHC lower bounds are substantially tighter than the bounds given by VB and by collapsed variational methods on synthetic and real datasets. (2) We also show that BHC offers a more accurate predictive performance on these datasets. (3) We further improve the tree-based lower bounds with an algorithm that efficiently sums contributions from alternative trees. (4) We present a fast approximate method for BHC. Our results suggest that our combinatorial approximate inference methods and lower bounds may be useful not only in DPMs but in other models as well.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/xu09a/xu09a.pdf",
        "supp": "",
        "pdf_size": 814131,
        "gs_citation": 15,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=693548461414180219&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 15,
        "aff": ";;",
        "aff_domain": ";;",
        "email": ";;",
        "github": "",
        "project": "",
        "author_num": 3,
        "track": "main",
        "status": "Poster"
    },
    {
        "id": "727cb49144",
        "title": "Variable Metric Stochastic Approximation Theory",
        "site": "https://proceedings.mlr.press/v5/sunehag09a.html",
        "author": "Peter Sunehag; Jochen Trumpf; S.V.N. Vishwanathan; Nicol Schraudolph",
        "abstract": "We provide a variable metric stochastic approximation theory. In doing so, we provide a convergence theory for a large class of online variable metric methods including the recently introduced online versions of the BFGS algorithm and  its limited-memory LBFGS variant. We also discuss the implications of our results in the areas of elicitation of properties of distributions using prediction markets and in learning from expert advice.",
        "bibtex": "@InProceedings{pmlr-v5-sunehag09a,\n  title = \t {Variable Metric Stochastic Approximation Theory},\n  author = \t {Sunehag, Peter and Trumpf, Jochen and Vishwanathan, S.V.N. and Schraudolph, Nicol},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {560--566},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/sunehag09a/sunehag09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/sunehag09a.html},\n  abstract = \t {We provide a variable metric stochastic approximation theory. In doing so, we provide a convergence theory for a large class of online variable metric methods including the recently introduced online versions of the BFGS algorithm and  its limited-memory LBFGS variant. We also discuss the implications of our results in the areas of elicitation of properties of distributions using prediction markets and in learning from expert advice.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/sunehag09a/sunehag09a.pdf",
        "supp": "",
        "pdf_size": 699046,
        "gs_citation": 47,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=15235292368575643978&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 17,
        "aff": "NICTA, Locked Bag 8001, Canberra ACT 2601, Australia + Dept. of Information Engr., Australian National Univ., Canberra, Australia; Dept. of Information Engr., Australian National Univ., Canberra, Australia; Statistics Department, Purdue University, West Lafayette, IN; adaptive tools AG, Canberra ACT 2602, Australia",
        "aff_domain": "nicta.com.au;anu.edu.au;stat.purdue.edu;adaptivetools.com",
        "email": "nicta.com.au;anu.edu.au;stat.purdue.edu;adaptivetools.com",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0+1;1;2;3",
        "aff_unique_norm": "NICTA;Australian National University;Purdue University;adaptive tools AG",
        "aff_unique_dep": ";Department of Information Engineering;Statistics Department;",
        "aff_unique_url": ";https://www.anu.edu.au;https://www.purdue.edu;",
        "aff_unique_abbr": ";ANU;Purdue;",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Canberra;West Lafayette",
        "aff_country_unique_index": "0+0;0;1;0",
        "aff_country_unique": "Australia;United States"
    },
    {
        "id": "397ee7db7c",
        "title": "Variational Bridge Regression",
        "site": "https://proceedings.mlr.press/v5/armagan09a.html",
        "author": "Artin Armagan",
        "abstract": "Here we obtain approximate Bayes inferences through variational methods when an exponential power family type prior is specified for the regression coefficients to mimic the characteristics of the Bridge regression. We accomplish this through hierarchical modeling of such priors. Although the mixing distribution is not explicitly stated for scale normal mixtures, we obtain the required moments only to attain the variational distributions for the regression coefficients. By choosing specific values of hyper-parameters (tuning parameters) present in the model, we can mimic the model selection performance of best subset selection in sparse underlying settings. The fundamental difference between MAP, maximum a posteriori, estimation and the proposed method is that, here we can obtain approximate inferences besides a point estimator. We also empirically analyze the frequentist properties of the estimator obtained. Results suggest that the proposed method yields an estimator that performs significantly better in sparse underlying setups than the existing state-of-the-art procedures in both n>p and p>n scenarios.",
        "bibtex": "@InProceedings{pmlr-v5-armagan09a,\n  title = \t {Variational Bridge Regression},\n  author = \t {Armagan, Artin},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {17--24},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/armagan09a/armagan09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/armagan09a.html},\n  abstract = \t {Here we obtain approximate Bayes inferences through variational methods when an exponential power family type prior is specified for the regression coefficients to mimic the characteristics of the Bridge regression. We accomplish this through hierarchical modeling of such priors. Although the mixing distribution is not explicitly stated for scale normal mixtures, we obtain the required moments only to attain the variational distributions for the regression coefficients. By choosing specific values of hyper-parameters (tuning parameters) present in the model, we can mimic the model selection performance of best subset selection in sparse underlying settings. The fundamental difference between MAP, maximum a posteriori, estimation and the proposed method is that, here we can obtain approximate inferences besides a point estimator. We also empirically analyze the frequentist properties of the estimator obtained. Results suggest that the proposed method yields an estimator that performs significantly better in sparse underlying setups than the existing state-of-the-art procedures in both n>p and p>n scenarios.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/armagan09a/armagan09a.pdf",
        "supp": "",
        "pdf_size": 504044,
        "gs_citation": 52,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=8107078916112917050&as_sdt=5,33&sciodt=0,33&hl=en",
        "gs_version_total": 7,
        "aff": "Department of Statistical Science, Duke University",
        "aff_domain": "stat.duke.edu",
        "email": "stat.duke.edu",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "Duke University",
        "aff_unique_dep": "Department of Statistical Science",
        "aff_unique_url": "https://www.duke.edu",
        "aff_unique_abbr": "Duke",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United States"
    },
    {
        "id": "36588b9f6a",
        "title": "Variational Inference for the Indian Buffet Process",
        "site": "https://proceedings.mlr.press/v5/doshi09a.html",
        "author": "Finale Doshi; Kurt Miller; Jurgen Van Gael; Yee Whye Teh",
        "abstract": "The Indian Buffet Process (IBP) is a nonparametric prior for latent feature models in which observations are influenced by a combination of several hidden features.  For example, images may be composed of several objects or sounds may consist of several notes.  Latent feature models seek to infer what these latent features from a set of observations. Current inference methods for the IBP have all relied on sampling.  While these methods are guaranteed to be accurate in the limit, in practice, samplers for the IBP tend to mix slow.  We develop a deterministic variational method for the IBP.  We provide theoretical guarantees on its truncation bounds and demonstrate its superior performance for high dimensional data sets.",
        "bibtex": "@InProceedings{pmlr-v5-doshi09a,\n  title = \t {Variational Inference for the Indian Buffet Process},\n  author = \t {Doshi, Finale and Miller, Kurt and Gael, Jurgen Van and Teh, Yee Whye},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {137--144},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/doshi09a/doshi09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/doshi09a.html},\n  abstract = \t {The Indian Buffet Process (IBP) is a nonparametric prior for latent feature models in which observations are influenced by a combination of several hidden features.  For example, images may be composed of several objects or sounds may consist of several notes.  Latent feature models seek to infer what these latent features from a set of observations. Current inference methods for the IBP have all relied on sampling.  While these methods are guaranteed to be accurate in the limit, in practice, samplers for the IBP tend to mix slow.  We develop a deterministic variational method for the IBP.  We provide theoretical guarantees on its truncation bounds and demonstrate its superior performance for high dimensional data sets.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/doshi09a/doshi09a.pdf",
        "supp": "",
        "pdf_size": 1180005,
        "gs_citation": 189,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=12982039394924101433&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 16,
        "aff": "Engineering Department, Cambridge University, Cambridge, UK; Computer Science Division, University of California, Berkeley, Berkeley, CA; Engineering Department, Cambridge University, Cambridge, UK; Gatsby Unit, University College London, London, UK",
        "aff_domain": "; ; ; ",
        "email": "; ; ; ",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "Cambridge University;University of California, Berkeley;University College London",
        "aff_unique_dep": "Engineering Department;Computer Science Division;Gatsby Unit",
        "aff_unique_url": "https://www.cam.ac.uk;https://www.berkeley.edu;https://www.ucl.ac.uk",
        "aff_unique_abbr": "Cambridge;UC Berkeley;UCL",
        "aff_campus_unique_index": "0;1;0;2",
        "aff_campus_unique": "Cambridge;Berkeley;London",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "9c9c8a5f9f",
        "title": "Variational Learning of Inducing Variables in Sparse Gaussian Processes",
        "site": "https://proceedings.mlr.press/v5/titsias09a.html",
        "author": "Michalis Titsias",
        "abstract": "Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs  are defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler divergence between  the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.",
        "bibtex": "@InProceedings{pmlr-v5-titsias09a,\n  title = \t {Variational Learning of Inducing Variables in Sparse Gaussian Processes},\n  author = \t {Titsias, Michalis},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {567--574},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/titsias09a.html},\n  abstract = \t {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs  are defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler divergence between  the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf",
        "supp": "",
        "pdf_size": 585990,
        "gs_citation": 2233,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=5933990210955605124&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 9,
        "aff": "School of Computer Science, University of Manchester, UK",
        "aff_domain": "cs.man.ac.uk",
        "email": "cs.man.ac.uk",
        "github": "",
        "project": "",
        "author_num": 1,
        "track": "main",
        "status": "Poster",
        "aff_unique_index": "0",
        "aff_unique_norm": "University of Manchester",
        "aff_unique_dep": "School of Computer Science",
        "aff_unique_url": "https://www.manchester.ac.uk",
        "aff_unique_abbr": "UoM",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Manchester",
        "aff_country_unique_index": "0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "5bea2c1bfc",
        "title": "Visualization Databases for the Analysis of Large Complex Datasets",
        "site": "https://proceedings.mlr.press/v5/guha09a.html",
        "author": "Saptarshi Guha; Paul Kidwell; Ryan P. Hafen; William S. Cleveland",
        "abstract": "Comprehensive visualization that preserves the information in a large complex dataset requires a visualization database (VDB): many displays, some with many pages, and with one or more panels per page. A single display using a specific display method results from partitioning the data into subsets, sampling the subsets, and applying the method to each sample, typically one per panel. The time of the analyst to generate a display is not increased by choosing a large sample over a small one. Displays and display viewers can be designed to allow rapid scanning. Often, it is not necessary to view every page of a display. VDBs, already successful just with off-the-shelf tools, can be greatly improved by research that rethinks all of the areas of data visualization in the context of VDBs.",
        "bibtex": "@InProceedings{pmlr-v5-guha09a,\n  title = \t {Visualization Databases for the Analysis of Large Complex Datasets},\n  author = \t {Guha, Saptarshi and Kidwell, Paul and Hafen, Ryan P. and Cleveland, William S.},\n  booktitle = \t {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {193--200},\n  year = \t {2009},\n  editor = \t {van Dyk, David and Welling, Max},\n  volume = \t {5},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},\n  month = \t {16--18 Apr},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v5/guha09a/guha09a.pdf},\n  url = \t {https://proceedings.mlr.press/v5/guha09a.html},\n  abstract = \t {Comprehensive visualization that preserves the information in a large complex dataset requires a visualization database (VDB): many displays, some with many pages, and with one or more panels per page. A single display using a specific display method results from partitioning the data into subsets, sampling the subsets, and applying the method to each sample, typically one per panel. The time of the analyst to generate a display is not increased by choosing a large sample over a small one. Displays and display viewers can be designed to allow rapid scanning. Often, it is not necessary to view every page of a display. VDBs, already successful just with off-the-shelf tools, can be greatly improved by research that rethinks all of the areas of data visualization in the context of VDBs.}\n}",
        "pdf": "http://proceedings.mlr.press/v5/guha09a/guha09a.pdf",
        "supp": "",
        "pdf_size": 385869,
        "gs_citation": 35,
        "gs_cited_by_link": "https://scholar.google.com/scholar?cites=3466617206969739960&as_sdt=2005&sciodt=0,5&hl=en",
        "gs_version_total": 10,
        "aff": ";;;",
        "aff_domain": ";;;",
        "email": ";;;",
        "github": "",
        "project": "",
        "author_num": 4,
        "track": "main",
        "status": "Poster"
    }
]